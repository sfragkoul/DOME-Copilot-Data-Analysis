_id/$oid	dataset/availability	dataset/provenance	dataset/redundancy	dataset/splits	dataset/done	dataset/skip	evaluation/availability	evaluation/comparison	evaluation/confidence	evaluation/measure	evaluation/method	evaluation/done	evaluation/skip	model/availability	model/duration	model/interpretability	model/output	model/done	model/skip	optimization/algorithm	optimization/config	optimization/encoding	optimization/features	optimization/fitting	optimization/meta	optimization/parameters	optimization/regularization	optimization/done	optimization/skip	user/$oid	publication/pmid	publication/pmcid	publication/updated	publication/authors	publication/journal	publication/title	publication/doi	publication/year	publication/done	publication/skip	publication/tags	public	created/$date	updated/$date	uuid	reviewState	shortid	update	__v	score	provenance_source	annotation_matches_publication_metadata	annotation_matches_shortid
63a25db2e8edf6ce46f6e84b	Yes, The MSI data have been deposited to the ProteomeXchange Consortium (http://proteomecentral.proteomexchange.org) via the iProX partner repository with the dataset identifier PXD038876.	DESI-MSI experiments of whole mouse kidney and renal tumor specimens. Two MSI datasets and their corresponding H&E microscopy images.  The MSI data have been deposited to the ProteomeXchange Consortium (http://proteomecentral.proteomexchange.org) via the iProX partner repository with the dataset identifier PXD038876.	Not applicable 	Not applicable 	4.0	0.0	No	Yes, our method was compared with two internal validation measures: (1) segmentation was evaluated by how closely it resembled the low-dimension overview of the high‚Äêdimensional molecular content of MSI data obtained by nonlinear dimension reduction techniques (here we used UMAP). The resemblance between the UMAP image and the segmentation maps was measured by applying a Canny edge detector to both and computing their edge correlation.  (2) the Davies‚ÄìBouldin index (DBI) was used to find the optimal segmentation for MSI data. DBI measures the ratio of within-cluster distances (the measure of intra-cluster compactness) to between-cluster distances (the measure of inter-cluster separation), so a smaller DBI indicates better-defined clusters and thus supposedly better segmentation. 	not applicable 	Cohen's kappa score between MSI- and Histology-segmentation results	A multimodal fusion-based method was proposed in our manuscript. Visual evaluation by an expert was also performed.	4.0	1.0	Yes, available at https://github.com/guoang4github/ROIforMSI/ (Licence: GPL-3)	less than 1 min	Not applicable	Clustering 	4.0	0.0	Spectral Clustering algorithm with  n_clusters=3 to 8, affinity='cosine' or 'nearest neighbors', n_components=5, assign_labels='kmeans'	Not applicable 	"An inner layer (conv5-block32-concat) of DenseNet 201 was used as our extractor of choice to encode the stain color and morphology of a 2D H&E image tile into a set of informative histomorphological features (HF). To stay in keeping with ImageNet, our input tiles had to be resized to [224, 224, 3] through bilinear interpolation and each color channel had to be centered to zero . Eventually, the output arrays after the concatenation operation at the conv5_block32 layer of DenseNet201 were 2-D globally average-pooled, Min-Max scaled, and reshaped as a ""histomorphological feature (HF) spectrum"" of 1,920 variables."	HF data cube had 1920 histomorphological features. For the MSI data of whole kidney, the number of m/z variables was 87. For the tumor data, the number of m/z variables was 89. Feature selection was performed to include only the ions that had a stronger signal in the foreground tissue areas than the background glass substrates.	Kmeans	No	n_clusters were selected objectively by a multimodal fusion method proposed in our manuscript, the affinity and n_components were empirically configured to produce visually less fragmented regions 	Not applicable 	7.0	1.0	63992de57417daebc3c00985	37039115.0	PMC10087011	02/02/2026 19:46:52	Guo A, Chen Z, Li F, Luo Q.	GigaScience	Delineating regions of interest for mass spectrometry imaging by multimodally corroborated spatial segmentation.	10.1093/gigascience/giad021	2023	0.0	0.0		1.0	2022-12-21T01:13:22.874Z	2026-02-02T19:46:52.000Z	8c0c94cd-3172-4f65-92b0-b997742324e0	undefined	ck60ijuxvo				DOME_JSON	Match	Match
643fee35bcc9ba89a8f2c763	Training and test datasets were as split as possible, and the same goes for training and validation. Datasets are all available on FigShare and can also be re-created easily from the publicly available PRIDE datasets.	"The datasets have been sourced from PRIDE, the largest repository for proteomics based MS datasets.
The overarching datasets consists of 151M datapoints, from which multiple smaller datasets have been extracted from 750K to 2M datapoints randomly sampled.
Due to using pre-existing data only, some data points have been used in previous papers (which is also key part of the manuscript)"	"The datasets were split randomly based on sequence and raw files. This means that the hold-out test dataset was split based on entire projects from PRIDE to make sure the test set were at heterogenous from the training set as possible. Then we split training and validation based on unique sequences in the remaining datasets. This also means that a perfect 70:20:10 split is not possible, but that's what we aimed for when randomly splitting. 
This is normally how the split is done in the field."	"Large datasets was split into 12 smaller datasets randomly based on queries. Splits ranged from 750K to 2M in size, and may have overlaps.
For training and testing each dataset was split into training, validation and hold-out test datasets at a ~70:20:10 split ratio.
The distribution of datapoints is roughly normally distributed for most subsets, except for the m/z range filter ones, where the nature of the split will cause the dataset to be largely located at the normally lower populated ends on the normal distributions."	4.0	0.0	"All models, code and output files are available on FigShare:
https://figshare.com/articles/dataset/Rehfeldt_et_al_data_reference/21680669"	Since the field has no standard metric, methods or benchmarking datasets we did not test to other models or datasets. 	No. We did compare to a dataset of random retention times to show that our modes outperform random guessing, but we cannot compare to other models or metrics as no such baseline exists in the field.	We used the TimeDelta metric which was available in the python package here: https://dlomix.readthedocs.io/en/latest/_modules/dlomix/eval/rt_eval.html#TimeDeltaMetric	hold-out datasets randomly sampled to ensure as unbiased testing as possible	5.0	0.0	"Yes the source code for our testing is here: https://gitlab.com/tjobbertjob/ms-review-paper
with the underlying code being available here: https://gitlab.com/roettgerlab/ms2ai"	between 2-10 hours for training. Minutes for testing	"The model architecture is explained with code to see the entire model. We also published all weights for all trained models.
However, the nature of NNs makes them hard to interpret, but we made everything as transparent as possible."	regression	4.0	0.0	We used a state-of-the-art neural network in the field and thus did not construct a NN ourselves. Our manuscript does not focus on the optimization of the network but a comparison of the underlying data. We used the PROSIT retention time model.	"All parameters, splits, seeds and so on are available in the source code
https://gitlab.com/tjobbertjob/ms-review-paper"	The retention time was normalized for each file between the first and last identified peptides to mimic, the otherwise unachievable, iRT computations performed in the original PROSIT paper.	1. The sequence of the peptide.	As our training points are >> our features, we tested first on a dataset of random retention times based on which we can compare if the model actually learns the data. We then trained and refined 12+ models and they were comparable to each other. While we cannot rule out under-fitting, its unlikely that the model was capable of achieving better results. 	"No
"	The only parameters we changed were the epochs, as well as adding an early stopping patience. All other parameters were as described in the original paper of the same model. We changed these parameters as the default epochs was not enough time to converge our training.	Due to the computation needs to train and refine so many models, we choose to add an early stopping of 20 epochs to the model. This was partially to avoid overfitting, and partially to reduce the wasted computation time. No other regularization methods were added by us.	8.0	0.0	643fe8a892c76639b81bfe5f	37983748.0	PMC10659119	02/02/2026 19:46:52	Rehfeldt TG, Krawczyk K, Echers SG, Marcatili P, Palczynski P, R√∂ttger R, Schw√§mmle V.	GigaScience	Variability analysis of LC-MS experimental factors and their impact on machine learning.	10.1093/gigascience/giad096	2023	0.0	0.0		1.0	2023-04-19T13:35:49.122Z	2026-02-02T19:46:52.000Z	399608f3-8b79-4df1-ac09-1a9aa6e1947d	undefined	3glxd6418e				DOME_JSON	Match	Match
645aa34160bf612a3caabc1b	https://github.com/ZhenjiangFan/DAG-deepVASE	We used a simulation data set, two public data, and one access-controlled data. Our simulation data are downloadable from our project website (https://github.com/ZhenjiangFan/DAG-deepVASE). TCGA breast invasive carcinoma (BRCA) data were downloaded from https://tcga.xenahubs.net, available under BRCA cohort, under gene expression RNAseq section, on IlluminaHiSeq (n=1,218) TCGA Hub. It consists of the gene expression RNAseq dataset (dataset ID: TCGA.BRCA.sampleMap/HiSeqV2) and the clinical phenotype dataset (dataset ID: TCGA.BRCA.sampleMap/BRCA_clinicalMatrix). To investigate the dietary effect of the human gut microbiome, we downloaded a cross-sectional data of 98 healthy volunteers from https://noble.gs.washington.edu/proj/DeepPINK/ that preprocessed the data set.	all datasets are without overlap, kept coincident for each trial of the algorithms.	We used 10 fold cross validation in all the data sets. 	4.0	0.0	No	We compared our method to threeexisting methods, causalMGM, DAG-GNN, and NOTEAR.	better timing and accuracy	Sensivitity and specificity	10-fold cross validation	4.0	1.0	https://github.com/ZhenjiangFan/DAG-deepVASE	Within 6 hours for mid-sized samples.	we developed the first computational method that explicitly learns nonlinear causal relations and estimates the effect size using a deep-neural network approach coupled with the knockoff framework.	Model classification.	4.0	0.0	The initial weights for the hidden layer are generated using the Glorot normal initializer, which uses ùêø1‚àíùëüùëíùëîùë¢ùëôùëéùëüùëñùëßùëéùë°ùëñùëúùëõ with the regularization parameter set to O(‚àö(2logp/n)). To train this model, mean of squares of errors (MSE) is used to calculate the loss in comparison with the response on the output layer. To train the model‚Äôs parameters with respect to the loss function, we used a stochastic gradient descent method called ‚ÄúAdam optimization‚Äù. 	No	All the inputs have been normalized into the range [-1, 1] for fairness.	We used a simulation data set, two public data, and one access-controlled data. Our simulation data are downloadable from our project website (https://github.com/ZhenjiangFan/DAG-deepVASE). TCGA breast invasive carcinoma (BRCA) data were downloaded from https://tcga.xenahubs.net, available under BRCA cohort, under gene expression RNAseq section, on IlluminaHiSeq (n=1,218) TCGA Hub. It consists of the gene expression RNAseq dataset (dataset ID: TCGA.BRCA.sampleMap/HiSeqV2) and the clinical phenotype dataset (dataset ID: TCGA.BRCA.sampleMap/BRCA_clinicalMatrix). To investigate the dietary effect of the human gut microbiome, we downloaded a cross-sectional data of 98 healthy volunteers from https://noble.gs.washington.edu/proj/DeepPINK/ that preprocessed the data set.	We used linear fit.	No	"Parameters	Value
Activation function	Rectified linear unit (ReLU)
Initial weight values	Glorot normal intializer
Regularization	ùêø1‚àíùëüùëíùëîùë¢ùëôùëéùëüùëñùëßùëéùë°ùëñùëúùëõ
Optimization	Adam optimization
Loss function	Mean of squares of errors (MSE)
FDR control rate	0.05
"	We used ùêø1‚àíùëüùëíùëîùë¢ùëôùëéùëüùëñùëßùëéùë°ùëñùëúùëõ with the regularization parameter set to O(‚àö(2logp/n)).	6.0	2.0	6454174d92c76639b843f17b	37395630.0	PMC10316696	02/02/2026 19:46:52	Fan Z, Kernan KF, Sriram A, Benos PV, Canna SW, Carcillo JA, Kim S, Park HJ.	GigaScience	Deep neural networks with knockoff features identify nonlinear causal relations and estimate effect sizes in complex biological systems.	10.1093/gigascience/giad044	2023	0.0	0.0		1.0	2023-05-09T19:47:13.177Z	2026-02-02T19:46:52.000Z	9b42e80b-19bd-49d8-b06e-3b8302ceb152	undefined	ngjci1cyqx				DOME_JSON	Match	Match
64662eaa60bf612a3caabc6b	NCBI Public Dataset. . All data are available at GEO, with human brain data access number: GSE67835; Chung et al. dataset accession code: GSE75688; Hrvatin dataset GEO accession code: GSE59739; Romanov et al. dataset GEO accession code: GSE74672; Deng et al. dataset GEO accession code: GSE45719; mouse pancreatic islet data GEO accession code: GSE84133. Human pancreatic islet data are available at GEO or EMBL-EBI database with accession codes GSE81076, GSE85241, GSE86469, E-MTAB-5061, and GSE84133.	Homo sapiens and Mouse data	Data does not overlap	No	3.0	1.0	Yes.We will publish it on GitHub	Compare with other algorithms	No	ARI-Adjusted Rand Index, RI-Rand Index, NMI-Normalized Mutual Information, MI-Mutual Information	 Independent dataset	4.0	1.0	Links can be provided separately	No	Transparent.Neural Topic inherit the advantages of the topic model and is highly interpretable	No	2.0	2.0	Adam	Yes.We will publish it on GitHub	Normalization processing	The expression of each gene in a single cell	No	No	Around 200	No	5.0	3.0	6465d2d592c76639b8669c75	38000911.0	PMC10673642	02/02/2026 19:46:52	Qi Y, Han S, Tang L, Liu L.	GigaScience	Imputation method for single-cell RNA-seq data using neural topic model.	10.1093/gigascience/giad098	2023	0.0	0.0		1.0	2023-05-18T13:56:58.509Z	2026-02-02T19:46:52.000Z	0ba2ada5-240f-4c3a-87ad-b1db60315cb9	undefined	krz8m7iufj				DOME_JSON	Match	Match
65d7e78d1502715bfe53c582	"The data set(s) supporting the results and all other code used in this article are available in the ‚ÄòVisual Physiology Opsin Database‚Äô GitHub repository, (10.5281/zenodo.10667840 or https://github.com/VisualPhysiologyDB/visual-physiology-opsin-db/). All data and code is covered under a GNU General Public License (Version 3), in accordance with Open Source Initiative (OSI)-policies.  DOME-ML (Data, Optimisation, Model, and Evaluation in Machine Learning) annotation, supporting the current study, is available through DOME Wizard.
"	The source of the data is from a database we compiled, the Visual Physiology Opsin Database (VPOD) (10.5281/zenodo.10667840 or https://github.com/VisualPhysiologyDB/visual-physiology-opsin-db/). VPOD_1.0 is a new database, available on GitHub, that currently includes all heterologously expressed animal opsins. We refer to a subset of the database with only heterologous data as VPOD_het_1.0, although for version 1.0, this is synonymous with the entire database. VPOD_het_1.0 relies on 68 publications, mainly primary sources, with dates ranging from the 1980‚Äôs to 2023. The database contains opsin sequences and phenotype data from 166 unique species (counting 35 reconstructed ancestors), including fishes, amphibians, reptiles, mammals, crustaceans, and bivalves. Altogether, VPOD_het_1.0 contains 864 unique opsin sequences and corresponding Œªmax values.	"We used naming conventions that include versioning to improve reproducibility and reliability of individual datasets and models. For example, one subset combines ultraviolet and SWS opsins, which we named VPOD_uss_het_1.0 (n=280). Our convention is to name the subset (in this case USS = ‚ÄòUltraviolet and Short-wave Sensitive‚Äô opsins); name the source of phenotype data (heterologous = het), and record the version number of the dataset (1.0). We also created subsets for medium- and long-wave sensitive opsins (VPOD_mls_het_1.0)(n=91) and all rod (Rh1) and rod-like (Rh2) opsins (VPOD_rod_het_1.0)(n=352). Other subsets use species taxonomy, one for vertebrates (VPOD_vert_het_1.0)(n=721) and another for invertebrates (VPOD_inv_het_1.0)(n=143). For taxonomic subsets, we considered all sequences from phylum Chordata as ‚Äòvertebrates‚Äô and the rest as 'invertebrates‚Äô. Another subset excludes all mutant opsin sequences, called ‚Äòwild-types‚Äô (VPOD_wt_het_1.0)(n=318). A final named subset is the whole data set (VPOD_wds_het_1.0)(n=864). 
We used naming conventions that include versioning to improve reproducibility and reliability of individual datasets and models. For example, one subset combines ultraviolet and SWS opsins, which we named VPOD_uss_het_1.0 (n=280). Our convention is to name the subset (in this case USS = ‚ÄòUltraviolet and Short-wave Sensitive‚Äô opsins); name the source of phenotype data (heterologous = het), and record the version number of the dataset (1.0). We also created subsets for medium- and long-wave sensitive opsins (VPOD_mls_het_1.0)(n=91) and all rod (Rh1) and rod-like (Rh2) opsins (VPOD_rod_het_1.0)(n=352). Other subsets use species taxonomy, one for vertebrates (VPOD_vert_het_1.0)(n=721) and another for invertebrates (VPOD_inv_het_1.0)(n=143). For taxonomic subsets, we considered all sequences from phylum Chordata as ‚Äòvertebrates‚Äô and the rest as 'invertebrates‚Äô. Another subset excludes all mutant opsin sequences, called ‚Äòwild-types‚Äô (VPOD_wt_het_1.0)(n=318). A final named subset is the whole data set (VPOD_wds_het_1.0)(n=864). 
"	For model training, a whole dataset it submitted to the algorithm training pipeline and it is randomly sampled to split the data such  that 80% is used for training, and 20% is used for model validation. The amount of data points in these splits/sets are dependent on the subset of data used to train the models. As mentioned earlier, VPOD_het_1.0 contains 864 unique opsin sequences and corresponding Œªmax values. From this we created eleven data subsets with varying levels of taxonomic and gene family inclusivity to test which factors most impact the reliability/performance of ML methods.	4.0	0.0	Yes, this all available on our GitHub repository, (10.5281/zenodo.10667840 or https://github.com/VisualPhysiologyDB/visual-physiology-opsin-db/).	We didn't compare to an existing publicly available ML methods. However, we compared performance of ML models to phylogenetic imputation, which estimates phenotypes using phylogenetic information. Phylogenetic imputation uses maximum likelihood (we will not abbreviate maximum likelihood as ML to avoid confusion with machine learning), assuming Brownian Motion to predict missing phenotypes using a phylogenetic tree, assuming more closely related species or sequences have more similar phenotypes. We randomly removed 50 opsin sequences, and their corresponding Œªmax values from each of the training datasets used to train our ML models (with the exception of the smaller MWS/LWS and invertebrate datasets, in which we only removed 15), then estimated the removed Œªmax values using phylogenetic imputation. We used the phylogenetic imputation sub-module of the phytools R package for performing imputation. We compared imputed and actual Œªmax using regression. 	No	The default performance metrics for regression and classification that deepBreaks uses are Mean Absolute Error (MAE) and F-score. The default list of metrics that deepBreaks reports are provided in the documentation provide predefined custom metrics or a set of metrics from the scikit-learn library in python.	"For model evaluation and comparison, deepBreaks by default uses a 10-fold cross-validation approach and ranks
the models based on their average cross-validation score.
As we described in the methods of the main text ""we created eleven data subsets with varying levels of taxonomic and gene family inclusivity (Table 1) to test which factors most impact the reliability/performance of ML methods. We used naming conventions that include versioning to improve reproducibility and reliability of individual datasets and models. For example, one subset combines ultraviolet and SWS opsins, which we named VPOD_uss_het_1.0. Our convention is to name the subset (in this case USS = ‚ÄòUltraviolet and Short-wave Sensitive‚Äô opsins); name the source of phenotype data (heterologous = het), and record the version number of the dataset (1.0). We also created subsets for medium- and long-wave sensitive opsins (VPOD_mls_het_1.0) and all rod (Rh1) and rod-like (Rh2) opsins (VPOD_rod_het_1.0). Other subsets use species taxonomy, one for vertebrates (VPOD_vert_het_1.0) and another for invertebrates (VPOD_inv_het_1.0). For taxonomic subsets, we considered all sequences from phylum Chordata as ‚Äòvertebrates‚Äô and the rest as 'invertebrates‚Äô. Another subset excludes all mutant opsin sequences, called ‚Äòwild-types‚Äô (VPOD_wt_het_1.0). A final named subset is the whole data set (VPOD_wds_het_1.0). 
Using various subsets of data, we performed a number of experiments to better understand the performance of ML models in predicting Œªmax. First, to better understand how training data relate to model performance, R2 , and training data size, we gradually increased the size of training datasets, using the WDS, Vertebrate, WT, and Rod subsets separately, by adding between 15-50 randomly selected sequences at a time, repeating the process three times per data split (Table S1). We then analyzed the fit between the size of training data sets (x-axis) and model performance (y-axis), comparing six non-linear models with AIC to find the model that best explains the observed variation (Figure S2). Second, to understand if ML could predict known phenotypic changes due to experimental mutations, we queried the top performing WT model (which lacks data from artificially mutated sequences) using all experimentally mutated opsins to predict their phenotypes. We plotted these results using matplotlib [49] to visualize characteristics of poorly predicted outliers (e.g., taxonomic bias or sensitivity to mutations which caused large shifts in Œªmax from the WT). Third, we examined the ability of our models to predict Œªmax of thirty invertebrate opsins not in VPOD_1.0 because they are only known from physiological studies (Table S3, Figure S4). Here, we collected data both characterized by single-cell microspectrophotometry (MSP) or electroretinogram methods and with expression localized to cell-type by in-situ-hybridization (ISH), to link Œªmax to a specific opsin (the sequences and metadata can be found in ‚Äòmsp_erg_raw.txt‚Äô and ‚Äòmsp_erg_meta.tsv‚Äô, while the resulting predictions can be found under the ‚Äòmsp_tests‚Äô folder on our GitHub repository). Finally, we directly compared predictive capabilities of models trained on different data subsets by randomly selecting and removing the same 25 wild-type ultraviolet or short-wave sensitive opsins from the training data of the WDS, Vertebrate, WT, and UVS/SWS models before training and querying the model with those same sequences following training (Table S3, Figure S5). ""
"	4.0	1.0	As mentioned earlier, all software, code and supporting data are available on our GitHub repository (10.5281/zenodo.10667840 or https://github.com/VisualPhysiologyDB/visual-physiology-opsin-db/). All data and code (software) is covered under a GNU General Public License (Version 3), in accordance with Open Source Initiative (OSI)-policies. 	A single representative prediction, all the way to several hundred predictions, takes only a matter of seconds to complete on a normal desktop PC.  Additionally, training the models can completed in a matter of minutes on a desktop PC as well.  	"All models used are generally interpretable, primarily due to the nature of the feature selection process, treating each position on an amino acid sequence as a feature. 
For interpreting the contribution of sequence positions to the predictive models, we use the feature importance, coefficients, and weights as different algorithms have different kinds of
output. For xgboost and lightgbm the reported feature importance represents the number of times a feature appears in a tree. For AdaBoost, random forest, decision tree, extra tree, and
gradient boosting the importance of a feature is its Gini importance which is computed as the
normalized total reduction of the criterion brought by that feature."	The models used for our paper were all regression, but deepBreaks provides the option to also train classification models in an identical manner. 	4.0	0.0	We then trained various ML models employing a custom version of deepBreaks, an ML tool designed for exploring genotype-phenotype associations. For continuous phenotypes, deepBreaks fit models using twelve different existing ML linear regression algorithms including, Ridge Regression, Lasso Regression, Bayesian Regression, Lasso Least Angle Regression, Huber Regressor, Extremely Randomized Trees (Extra Trees), Extreme Gradient Boosting (xgboost), Light Gradient Boosting Machine (lightgbm), Random Forest, Decision Tree, and AdaBoost. deepBreaks takes aligned genotype data (DNA, RNA, Amino Acid) and some measure(s) of corresponding continuous or categorical phenotype data as input to train ML models. 	Yes, they are likely available in the scikit-learn documentation. 	The deepBreaks pipeline for preprocessing starts with dropping columns in the dataset that contain missing values over a certain threshold. The default threshold is 80% of the number of samples.  Dropping the zero-entropy (constant) features from the dataset, is the next step.   deepBreaks uses one-hot encoding to convert amino acid sequences into numerical values. One consequence of this encoding is any amino acids at a given position in the alignment, which are not present at that position in any training data, will be treated equivalently as unseen. For example, cases of only A and V at a highly conserved site in the training set that are presented with a sequence with T at that site will be considered as no A and no V. The models cannot distinguish the input whether it's T or other unseen amino acids at that site.  	"The number of input features is variable depending on the length of the sequences following sequences alignment and the many preprocessing and feature selection steps. deepBreaks uses the Kruskal-Wallis tests (for continuous phenotypes)  to reduce the number of positions in the training data set and drop the redundant ones. This statistical test is used to assess the significance of each position by running tests on all the positions against the phenotype one by one. Those features where the p-value of their test against the phenotype is less than a threshold (default p-value = 0.25) will be dropped. A list of all features and their test p-values are provided a report to the user. Then, since deepBreaks considers each position in the sequences as a feature of our training dataset,  it checks for collinearity between our predictive variables, as it can cause issues for parameter estimation. To check for the relationship between
"	We did not focus on ruling out overfitting or underfitting, however these features may be present in the scikit-learn backend. 	No	For all of the above-mentioned models deepBreaks by default uses the default hyperparameters from the scikit-learn library in python and a grid search (expandable by user preference) parameter set that is provided in the documentation.	We did not directly impliment any regularization measures, however I know certain models like the Light Gradient Boosted Machine and Xtreme Gradient Boosted Machine algorithms have regularization terms/parameters to limit overfitting.  That said, more information on this can be found in the scikit-learn documentation or in the corresponding literature for each machine learning algorithm type. 	7.0	1.0	65d6a5c492c76639b881cd51	39460934.0	PMC11512451	02/02/2026 19:46:52	Frazer SA, Baghbanzadeh M, Rahnavard A, Crandall KA, Oakley TH.	GigaScience	Discovering genotype-phenotype relationships with machine learning and the Visual Physiology Opsin Database (VPOD).	10.1093/gigascience/giae073	2024	0.0	0.0		1.0	2024-02-23T00:32:13.005Z	2026-02-02T19:46:52.000Z	00e4d4f8-2dd6-452a-b27c-72fe9296b09d	undefined	88n4lxv68p				DOME_JSON	Match	Match
65e843f41502715bfe53cd1e	"All the mentioned datasets are selected from previous studies and are publicly available.
Datasets are both mentioned through referencing to other studies and accessible through the URL: https://github.com/rdk/p2rank-datasets"	"Training: CHEN11‚Äîa dataset of 251 proteins harboring 476 ligands introduced in LBS prediction benchmarking study.

Optimization and validation: JOINED‚Äîconsists of structures from several smaller datasets used in previous studies (B48/U48, B210, DT198, ASTEX) joined into one larger dataset. you can find the details below:
B48/U48‚ÄîDatasets that contain a set of 48 proteins in a bound and unbound state.
B210‚Äîa benchmarking dataset of 210 proteins in bound state.
DT198‚Äîa dataset of 198 drug-target complexes.
ASTEX‚ÄîAstex Diverse set is a collection of 85 proteins that was introduced as a benchmarking dataset for molecular docking methods."	"Datasets for training and validation purposes are selected arbitrarily.
Testing dataset is not mentioned.
Only training set is mentioned to be non-redundant without explicit mention on the cutout identity percentage."	"2 data splits were utilized:
Training: CHEN11 dataset composed of 251 data points.
Optimization and validation: JOINED dataset composed of an overall  541 data points.
All the data points are positive examples of protein-ligand complexes."	4.0	0.0	No	The method was compared to other publicly available methods in the 3 mentioned metrics.	"The confidence intervals are only reported for ""Average prediction time per protein"".
While the method is reported to perform best in the first 2 mentioned metrics, which is not the case in terms of prediction time, the difference is not substantial."	"Instead of the conventional performance metrics, 3 reported metrics are as follows:
1. Identification success rate [%] measured by DCCcriterion (distance from pocket center to closest ligand atom) with 4 √Ö threshold.
2. Average total number of binding sites predicted per protein by on a given dataset.
3. Average time required for prediction on a single protein."	The model was evaluated on two independent datasets. Disjunct with data points present in training and validation set.	4.0	1.0	"The software and the source code is publicly available through the provided link to the GitHub repository of the software.
It is a stand-alone executable software, they currently have a web server, but at the time of publication the web server was under development.
URL: https://github.com/rdk/p2rank
Licence: MIT "	It is mentioned that it requires under 1 s for prediction on one protein.	"The model has 200 trees, each grown with no depth limit using 6 features, this makes it challenging to interpret due to the sheer number of trees and their depth.
However, it is still possible to obtain feature importance scores."	It is a regression model outputting numbers between 0 and 1.	4.0	0.0	"The used algorithm is a Random Forest Regression model.
The model is not novel."	The model is accessible through the URL of the GitHub repository.	The model takes vectors of 35 numerical features as input.	"Every vector is composed of 35 numerical features.
No mention on feature selection."	"The clear number of parameters is not mentioned.
No assessment on over/under fitting was mentioned."	The algorithm is not a meta-predictor.	"Beside the hyper-parameters of Random Forest, the algorithm uses other parameters and ""cut-of's"", ""thresholds"", ""protrusion radius"", are explicitly mentioned.
It is mentioned that only hyper-parameters are optimized on a separate dataset from training set.
The method of optimization is not mentioned."	A validation set was used to optimize the parameters before training on the training set.	8.0	0.0	65e73fdb92c76639b8e309f3	30109435.0	PMC6091426	02/02/2026 19:46:52	Kriv√°k R, Hoksza D.	Journal of cheminformatics	P2Rank: machine learning based tool for rapid and accurate prediction of ligand binding sites from protein structure.	10.1186/s13321-018-0285-8	2018	0.0	0.0		1.0	2024-03-06T10:22:44.950Z	2026-02-02T19:46:52.000Z	9bd31f10-4479-4940-810c-3e6a4bcfb1e3	undefined	iblymem5cf				DOME_JSON	Match	Match
66193ab11502715bfe53da89	Yes, in the published paper.	The dataset, consisting of 300 annotated corneocyte nanotexture images, is being introduced for the first time in this paper.	Random sampling from the pool of 1000 images and balanced subsets between train, validation, and test sets.	An 80% training set (N = 240 images), a 10% validation set (N = 30 images), and a 10% test set (N = 30 images).	4.0	0.0	Yes, in the published paper.	YOLOv8 and YOLOv9. YOLOv9 achieves a reduction of 49% in the number of parameters and 43% in computational workload compared to YOLOv8, while still demonstrating a 0.6% improvement in detection accuracy.	Yes, intersection over union (IoU) was applied to evaluate the precision of the bounding boxes.	Mean average precision (mAP): mAP@50 and mAP@50-95.	The model performance was evaluated on the annotated test set.	5.0	0.0	Yes, in the published paper.	The inference time ranges from 34.8 to 85.2 ms, depending on the model complexity, and was evaluated using a NVIDIA Tesla T4 GPU.	Black box, deep convolutional neural network.	Output consists of detected objects with bounding boxes (object detection).	4.0	0.0	YOLOv8 and YOLOv9, the state-of-the-art real-time object detectors.	Yes, in the published paper.	Each image has a resolution of 512 x 512 pixels and was preprocessed using contrast enhancement algorithms. Further details can be found in the paper.	Corneocyte nanotexture features with bounding boxes.	Application of a batch normalization after the convolution layers but before the nonlinear activation function, in order to reduce overfitting and improve the generalization ability of the model.	No, all raw data.	The parameters range from 3.2 to 68.2 million in YOLOv8 models and from 2 to 57.3 million in YOLOv8 models, varying with their neural network architectures.	The validation set was applied to prevent overfitting and to search for the optimal training epoch.	8.0	0.0	66193ab192c76639b89a4b9c	39657103.0	PMC11629979	02/02/2026 19:46:52	Wang JH, Pereda J, Du CW, Chu CY, Christensen MO, Kezic S, Jakasa I, Thyssen JP, Satheesh S, Hwu EE.	GigaScience	Stratum corneum nanotexture feature detection using deep learning and spatial analysis: a noninvasive tool for skin barrier assessment.	10.1093/gigascience/giae095	2024	0.0	0.0		1.0	2024-04-12T13:44:17.986Z	2026-02-02T19:46:52.000Z	33af8be6-b9bb-43ea-ad92-6605078237a4	undefined	z6ubsl6wyp				DOME_JSON	Match	Match
66213f37ded6e7820f749fde	No, the data is not released.	The source of data is UK Biobank, it has been used in various papers and recognized by the community. We have selected 5861 cases(see criteria details in the manuscripts) and 5861 carefully matched health controls.	A shuffle-split resampling scheme was used to subdivide the data.	The data is split into 100 stratified training (80%) and validation (20%) splits. The number of data points varies based on different diagnostic groups (see more details in the supplemental file). The distributions of the data types have not been plotted.	4.0	0.0	No	No	No	The performance metrics are reported by comparing the resulting 100 classification accuracies for each shuffle split against chance level (0.5).	We use cross-validation for method evaluation.	2.0	3.0	The source code is provided in https://github.com/tyo8/WAPIAW3 with MIT license.	The execution time for a single prediction is around 1 hour when using a high-performance computing cluster, this can vary depending on the feature set and diagnostic groups used.	The model is black box as it's using random forest.	The model is classification.	4.0	0.0	The main algorithm we used is random forest classification. The support vector classification and K-nearest neighbor classification are used for comparison between different machine learning algorithms.	The hyperparameter configurations are available in https://github.com/tyo8/WAPIAW3/classification_model/model_specification.py .	The categorical demographic and behavioral data is encoded by UK Biobank. Data were preprocessed by a standard scaler (i.e., z-scoring) before entry into the dimension reduction and predictive estimator pipelines.	We have 2 structural MRI feature sets, 16 resting state functional MRI feature sets, and 1 sociodemographic feature set (see more details in the supplements file).Feature selection was not performed.	To control overfitting, we (a) selected model hyperparameters in held-out sets and (b) trained 100 realizations of the model on shuffle-split data. The total number of parameters and hyperparameters across all investigated models is 365; no single model's number of parameters or hyperparameters (100-140) exceeded the number of training points in the smallest group (250).	No.	"For random forest classification, there are 5 parameters including number of PCs(rank/100, rank/32, rank/10, rank/3.2, full rank), impurity criterion (fixed as ""Gini""), maximum tree depth(5, 10, 20, 40, full depth), fraction of features(1, 5, log2, sqrt, complete) for split and number of trees. The parameters are selected using grid-search, except the number of trees are fixed to 250 based on previous publications. This search was performed over a grid of 125 total hyperparameters.
For support vector classification, there are 3 parameters including number of PCs(rank/100, rank/32, rank/10, rank/3.2, full rank), Margin regularization coefficient C(10^(-3),10^(-2),10^(-1),1,10^1,10^2,10^3), Kernel function(linear, rbf, sigmoid, poly). The parameters are selected using grid-search, which was performed over a grid of 140 total hyperparameters.
For K-nearest neighbor classification, there are 4 parameters including Number of PCs(rank/100, rank/32, rank/10, rank/3.2, full rank), Number of neighbors(1, 5, 11, 18, 27), Neighbor weights(‚Äòuniform‚Äô, ‚Äòdistance‚Äô), Distance metric(‚ÄòL1‚Äô, ‚ÄòL2‚Äô, ‚Äòcosine‚Äô). The parameters are selected using grid-search, which was performed over a grid of 150 total hyperparameters."	We did not employ regulariazation via pre-convergence early stopping.	7.0	1.0	66213f37b2aaa82f502f52d5	39931027.0	PMC11811528	02/02/2026 19:46:52	Easley T, Luo X, Hannon K, Lenzini P, Bijsterbosch J.	GigaScience	Opaque ontology: neuroimaging classification of ICD-10 diagnostic groups in the UK Biobank.	10.1093/gigascience/giae119	2025	0.0	0.0		1.0	2024-04-18T15:41:43.382Z	2026-02-02T19:46:52.000Z	2c0f9bb8-4a98-4182-97e4-992de146b7ac	undefined	e0eaxjld0h				DOME_JSON	Match	Match
670e363261d57eb8bca695a3	Data are public, can be obtained from iDNA-MS(http://lin-group.cn/server/iDNA-MS/), as well as MuLan-Methyl(https://github.com/husonlab/mulan-methyl).	"Data source: iDNA-MS (Lv, Hao, et al. ""iDNA-MS: an integrated computational tool for detecting DNA modification sites in multiple genomes."" Iscience 23.4 (2020): 100991.)
Data are in classes,the data statistics for positive samples and negative samples in both training dataset and test dataset can be found in supplementary file of published paper.
"	"The training and test set are split by iDNA-MS, they are independent.
"	"The ratio of training and test set is 1:1.
Validation set is generated by sampling 20% training dataset.
The distribution of data types in the training and test sets is same."	4.0	0.0	Yes, it's available in the MuLan-Methyl manuscript and its supplementary files.	Yes, MuLan-Methyl is compared with iDNA-ABF and iDNA-ABT on the iDNA-MS independent dataset, MuLan-Methyl outperforms 13 out of 17 sub-dataset.	Performance metrics of our study doesn't have confidence intervals.	"MuLan-Methyl is evaluated by the following evaluation metrics: AUC, Accuracy, F1-score, Recall, and AUPR.
"	We evaluated MuLan-Methyl on the iDNA-MS independent test dataset.	5.0	0.0	The source code is released (https://github.com/husonlab/mulan-methyl). A web server implementing the MuLan-Methyl approach is freely accessible at http://ab.cs.uni-tuebingen.de/software/mulan-methyl/.	In average one second.	MuLan-Methyl is interpretable by utilizing the self-attention mechanism of transformer architecture. For example, attention score assigned to each tokens of each sample which predicted positive are used to discover methylation motifs.	It's classification model.	4.0	0.0	"MuLan-Methyl is an ensemble framework consists of five transformer-based language models.
"	Yes, all the hyper-parameters are reported in MuLan-Methyl manuscript and its corresponding Github repository(https://github.com/husonlab/mulan-methyl).	The input of MuLan-Methyl is processed by converting each sample' DNA sequences and its taxonomic lineages into a description. Each processed sample is further encoded by tokenizer of each language models.	Two features of each sample are used as input, one is sample's DNA sequence, another is its taxonomic lineage. No feature selection is performed.	p is much larger than f, Overfitting is ruled out since the loss value of both training process and validation process has similar changing trends.	The model doesn't use data from other ML algorithms as input.	"Here is the models' parameter:
Model Number of parameters
MuLan-Methyl-BERT 105,242,882
MuLan-Methyl-DistilBERT 62,714,114
MuLan-Methyl-ALBERT 11,045,122
MuLan-Methyl-XLNet 111,934,466
MuLan-Methyl-ELECTRA 29,336,578"	Yes, early stopping on validation dataset is conducted for preventing overfitting.	8.0	0.0	670e3632908d74f9cb330467	37489753.0	PMC10367125	02/02/2026 19:46:52	Zeng W, Gautam A, Huson DH.	GigaScience	MuLan-Methyl-multiple transformer-based language models for accurate DNA methylation prediction.	10.1093/gigascience/giad054	2023	0.0	0.0		1.0	2024-10-15T09:30:26.463Z	2026-02-02T19:46:52.000Z	8ac43578-356a-49dc-a0a1-f2bfd3a887e0	undefined	bgldxl71jp				DOME_JSON	Match	Match
670e389461d57eb8bca695c1	These data are released in the Zenodo public repository: https://zenodo.org/record/7582233 with a license CC0 1.0.	"The data (drug-disease pairs) used for training our KGML-xDTD model were collected from four high-quality and NLP-derived data sources: MyChem Data, SemMedDB Data, NDF-RT Data, and RepoDB Data. All of these are public datasets; please see a more detailed description in our paper ""KGML-xDTD: A Knowledge Graph-based Machine Learning Framework for Drug Treatment Prediction and Mechanism Description"".

The data are categorized into classes, with 21,437 positive cases and 33,189 negative cases.

The data are in classes
  "	"We split the data into training, validation, and test sets where the drug-disease pairs of each unique drug are randomly split according to a ratio of 8:1:1. For example, let‚Äôs say drugA has 10 known diseases that it treats (e.g., drugA-disease1, . . . , drugA-disease10), 8 pairs are randomly split into the training set, 1 pair is to the validation set, 1 pair to the test set. 

The drug-disease pairs in the training set and test set are independent, with no overlap. 

There is no such drug class distribution analysis reported in the previous dataset that is similar to ours.


"	"They are 26,552 negative cases and 17,149 positive cases in the training set.
There are 3,318 negative cases and 2,143 positive cases in the validation set.
There are 3,319 negative cases and 2,145 positive cases in the validation set.

The drug class distribution in the training set and test set are similar. We plotted their distribution in Figure 7. in the paper."	4.0	0.0	If you follow the evaluation instruction on Github: https://github.com/chunyuma/KGML-xDTD, you can easily reproduce the evaluation results reported in the paper.	The KGML-xDTD has been compared with the comprehensive start-of-the-art ML/DL models based on the same dataset we collected from four public datasets mentioned in the Data Chapter.	For the ranking metrics (e.g., MRR and Hit@K), we generate 10 sets of random drug-disease pairs (each with 1,000 pairs) independently and calculate the mean and standard deviation of the ranking-based metrics outcomes, which are superior to other baselines. 	The accuracy, Macro F1 score, MRR, Hit@K are reported as performance metrics, They are all common evaluation metrics used for the goal of drug repurposing prediction based on the biomedical knowledge graph.	The model is evaluated based on an independent test set.	5.0	0.0	Yes, the source code released on Github: https://github.com/chunyuma/KGML-xDTD with the MIT license. Please follow the instruction on the GitHub to download and run the executable model.	A single representative prediction ran on a desktop PC needs only 409 ms.	The model itself is a black box but we utilize a novel knowledge-graph path-based method to explain the drug repurposing prediction result.	The model is classification.	4.0	0.0	"The KGML-xDTD model framework of two modules: a drug repurposing prediction (DRP) module that combines the advantages of GraphSAGE and a Random Forest model, and a Mechanism of Action (MOA) prediction module that utilizes an adversarial actor-critic reinforcement learning (RL) model. 

This model framework combines the existing start-of-the-art models with the biologically meaningful ""demonstration paths"", and achieves better performance than the known alternatives."	Yes, they are reported in the supplemenatary material in the paper.	Each node in the biomedical knowledge graph is encoded into 512 numeric embedding through the PubMedBERT model and GraphSage. For each drug-disease pair, their embeddings are concatenated and used as input for a Random Forest model to predict drug repurposing.	The knowledge graph node features are generated via the PubMedBERT model and GraphSage. The dimension of each node feature embedding is 512. There is no manual feature selection needed for this case. The random forest model selects the features by itself. Yes, the model selection via random forest is performed using the training set only.	The training data is larger than p but the underfitting problem was ruled out via the setting of using maximum depth of 35 and the number of tress of 200 in the random forest.	No, the model uses the training data that are collected by ourselves. The training data is independent of the test data.	There are 9 hyper-parameters used in unsupervised GraphSAGE embedding training which are selected manually while 2 hyper-parameters are used in the random forest which are selected via grid search algorithm.  The remaining hyper-parameters are set to default values.	There are a few overfitting prevention techniques used in the KGML-xDTD model which includes subsampling and feature selection by the random forest model, early stopping using a validation set in the reinforcement learning model.	8.0	0.0	670e3894908d74f9cb330ede	37602759.0	PMC10441000	02/02/2026 19:46:52	Ma C, Zhou Z, Liu H, Koslicki D.	GigaScience	KGML-xDTD: a knowledge graph-based machine learning framework for drug treatment prediction and mechanism description.	10.1093/gigascience/giad057	2023	0.0	0.0		1.0	2024-10-15T09:40:36.120Z	2026-02-02T19:46:52.000Z	3cd05870-401b-496e-9b88-edc18d906026	undefined	zg83kd0vmv				DOME_JSON	Match	Match
670e39e161d57eb8bca695c5	"All UCI datasets are available via https://archive.ics.uci.edu/. 
We also provide a clear instruction together with our supporting material such as code on the following GitHub repository: https://github.com/juaml/ConfoundLeakage/tree/main.
The clinical data can be obtained upon request from the original authors as described in the supporting material.
Our repo uses the GNU Affero General Public License v3.0. 

The sensitive real-world clinical dataset is restricted and can only be requesting it from PeakProfiling GmbH with certain restrictions. See paper for more details."	"We used two sources of data: I) The widely used and recognized UCI Machine Learning Repository, and II) a real-world clinical dataset. 
I) UCI data: For classification dataset we subsampled the data to have balanced classes (the corresponding n after subsampling shown in parenthesis) :
1) Income (Adult) n=32561 (15682,  equal Npos and Nneg)
2) Bank Marketing n=41188 (9280,  equal Npos and Nneg)
3) Heart n=297 (274,  equal Npos and Nneg)
4) Blood Transfusion n=748 (356,  equal Npos and Nneg)
5) Breast Cancer n=569 (424,  equal Npos and Nneg)
6) Student Performance n=649 | used for regression
7) Abalone n=4177 | used for regression
8) Concrete Compressive Strength n=1030 | used for regression
9) Residential Building n=372 | used for regression
10) Real Estate n=414 | used for regression

II) real-world clinical dataset is restricted, but was already used by von Polier at al. 2021
n=1045 (126, equal Npos and Nneg)"	As we used a (nested) cross-validation and thus the training and test sets are independent of each other. To account for the dependence structure across CV repeats when comparing CV outcomes of two ML models, we used the Bayesian ROPE approach which is specifically designed for this purpose.  	All data splits for the main analyses were performed using repeated k-fold cross-validation (k=5, repeats=10). In case of classification we used a stratified folds to preserve distributions of the data in training and test sets. If hyperparameter tuning was needed, we uses a nested cross-validation with the outer loop used for model assessment as described above and an inner 5-fold CV was used for model selection. We did not plot each of these distributions for our 11 datasets.	4.0	0.0	The code is available here: https://github.com/juaml/ConfoundLeakage under GNU Affero General Public License v3.0	We compare the performance of each ML algorithm with and without confound removal. We do not make any claims of performing better on the data than other algorithms. Still, we included a baseline model (dummy classifier/regressor) to evaluate performance not informed by the feature target relationship.	We report statistically meaningful results using the Bayesian ROPE approach. We also add standard deviations over repeats which could convey similar information as confidence intervals.	We report predictive r¬≤ for regression tasks and AUCROC for classification tasks. They are representative of what is common and reommended in the literature.	We employed nested cross validation scheme. Train/test split (or simulating new out-of-sample data) was performed only in simulated data or examples and was used to aid understanding of the concepts and results presented in the paper.	5.0	0.0	The code is available under the GNU Affero General Public License v3.0 on GitHub: https://github.com/juaml/ConfoundLeakage	Prediction on new data takes less than a second on a standard deskop PC.	The models used were all standatd models with some degree of interpretability, e.g. we visualized decision trees in Figure 1 and 3. Furthermore, we looked into feature importance of a random forest model in Figure 4.	We used both classification and regression given the different datasets.	4.0	0.0	"We only used the following standard ML algorithms. 
1) Linear/Logistic Regression 
2) Support Vector Machine with RBF kernel
3) Support Vector Machine with linear kernel
4) Decision Tree
5) Random Forests
6) Multi Layer Perceptron
7) Dummy/Baseline Model
"	URL: https://github.com/juaml/ConfoundLeakage/blob/main/leakconfound/leakconfound/experiments/helper_func.py. License: GNU Affero General Public License v3.0	Categorical features were one-hot encoded.  All continuous features were z-score standardized. Depending on the experimental condition we performed confound removal using confound regression with or without shuffling the features. All preprocessing steps were cross-validation consistent to avoid any data leakage.	"No feature selection was used. 
For the restricted data we followed instructions given by von Polier to follow best practices described in von Polier et al. 2021.
List of Feature numbers:
1) Income (Adult) f=14
2) Bank Marketing f=20
3) Heart f=13
4) Blood Transfusion f=4
5) Breast Cancer f=10
6) Student Performance f=30
7) Abalone  f=8
8) Concrete Compressive Strength f=8
9) Residential Building f=107
10) Real Estate f=6
11) real-world clinical dataset f=616"	We observe the same behavior for different datasets with different n/p reatios. Only the real world clinical dataset had p>n. Still we observe the same behaviour over different models as in the other datasets. We explicitly focused in CV for generalization estimates and did not analyse over- or under-fitting.	The input to our models is only the available features and no meta-predictions were used.	"The parameters of the models were according the corresponding model class as defined in sci-kit learn library.
The hyperparameters were selected using a grid-search within a nested cross-validation scheme. 
Here is a list of hyperparameters to chose from given the used ML algorithm: 
1) Logistic Regression : max_iter=100000, c=np.geomspace(1e-2, 1e2, 25)
2) RBF Support Vector Machine : c=np.geomspace(1e-2, 1e2, 25)
3) Linear Support Vector Machine:c=np.geomspace(1e-2, 1e2, 25), max_iter = 1000
4) Decision Tree: None
5) Random Forest : n_estimators=500
6) Multi Layered Perceptron : hidden_layer_size= [[32], [64], [128], [256]] (max_iter = 100000 for classification)
7) Dummy/Baseline Model: strategy='mean' for regression and strategy='prior' for classification"	As we used nested cross-validation we can rule out overoptimistic final results with a reasonable confidence. 	8.0	0.0	670e39e1908d74f9cb331505	37776368.0	PMC10541796	02/02/2026 19:46:52	Hamdan S, Love BC, von Polier GG, Weis S, Schwender H, Eickhoff SB, Patil KR.	GigaScience	Confound-leakage: confound removal in machine learning leads to leakage.	10.1093/gigascience/giad071	2023	0.0	0.0		1.0	2024-10-15T09:46:09.031Z	2026-02-02T19:46:52.000Z	4a182f3e-96e9-4a16-9429-878ae0565956	undefined	1scfw6g7s4				DOME_JSON	Match	Match
670e3b6e61d57eb8bca695c9	All the images used for training, validation, and testing are available at Zenodo with the DOI: https://doi.org/10.5281/zenodo.7555467 (Creative Commons ‚Äî CC0 license).	To generate the image datasets required for a DL model, we conducted a spheroid gel contraction assay using 5000 Smooth Muscle Cells (SMCs) and Human Embryonic Kidney (HEK) cells per collagen spheroid. After the collagen droplet had polymerized, we changed the medium and transferred the plates to an Incucyte Live-Cell Analysis System, which captured images of the spheroids every hour for a duration of 24 hours. Additionally, we employed a ZEISS Axio Vert.A1 Inverted Microscope to manually capture images of the spheroids at specific time points. By utilizing both methods, we were able to collect a diverse range of spheroid images, resulting in a robust dataset for our DL model. We obtained a total of 530 images from the Incucyte system and 432 images from the microscope.	The training, validation, and test sets are independent of each other. Furthermore, the spheroids in the test dataset underwent different treatment compared to those in the training and validation datasets. For the test dataset, the spheroids were cultured in smooth muscle cell medium and Dulbecco's Modified Eagle Medium (DMEM) with 0.5% and 1% FBS. 	"<!DOCTYPE html>
<html>
<head>
  <title>Dataset Information</title>
</head>
<body>
  
  <h4>Incucyte System Dataset:</h4>
  <ul>
    <li>Training dataset: 336 images</li>
    <li>Validation dataset: 144 images</li>
    <li>Test dataset: 50 images</li>
  </ul>

  <h4>Microscope Dataset:</h4>
  <ul>
    <li>Training dataset: 265 images</li>
    <li>Validation dataset: 117 images</li>
    <li>Test dataset: 50 images</li>
  </ul>
</body>
</html>"	4.0	0.0	No.	No.	No.	To evaluate the performance of the trained models on spheroid segmentation, we used the Average Precision (AP) evaluation metric. 	The evaluation method employed for Mask R-CNN involved assessing the model's performance based on object detection accuracy and instance segmentation quality using Intersection over Union (IoU).	2.0	3.0	The source code for SpheroScan is available at https://github.com/FunctionalUrology/SpheroScan with GNU GENERAL PUBLIC LICENSE.	The prediction module exhibits linear runtime complexity, taking approximately 1 second per image to mask the spheroids. The run-time performance evaluation was conducted on a Red Hat server equipped with 16 Central Processing Unit (CPU) cores and 64 GB of Random-Access Memory (RAM).	We utilized the Mask R-CNN model, which is recognized as a black box model itself.	No. Mask RCNN is a object detection and semantic segmentation model.	4.0	0.0	For spheroid detection and segmentation, we employed a state-of-the-art deep learning model known as Mask Regions with Convolutional Neural Networks (R-CNN).	In this study, we used the Mask R-CNN model for instance segmentation and tuned several of its parameters to fit the specific problem and the dataset we were working with. The backbone of the model was a ResNet-50 feature pyramid network, and we initialised the model with weights from a pre-trained COCO instance segmentation model. The batch size for training was set to 4, and the base learning rate was set to 0.00025. The RoIHead batch size was 256, and we used a single output class (for spheroids). We trained the model for a total of 1000 iterations. In addition to these specified parameters, we used the default values for all other parameters of the Mask R-CNN model.	An experienced researcher in the spheroid assay manually annotated the images from Incucyte and microscopes using the VGG Image Annotator. 	Not applicable.	Not applicable.	No, our model does not utilize data from other algorithms.	In this study, we used the Mask R-CNN model for instance segmentation and tuned several of its parameters to fit the specific problem and the dataset we were working with. The backbone of the model was a ResNet-50 feature pyramid network, and we initialised the model with weights from a pre-trained COCO instance segmentation model. The batch size for training was set to 4, and the base learning rate was set to 0.00025. The RoIHead batch size was 256, and we used a single output class (for spheroids). We trained the model for a total of 1000 iterations. In addition to these specified parameters, we used the default values for all other parameters of the Mask R-CNN model.	No.	7.0	1.0	670e3b6e908d74f9cb331be6	37889008.0	PMC10603766	02/02/2026 19:46:52	Akshay A, Katoch M, Abedi M, Shekarchizadeh N, Besic M, Burkhard FC, Bigger-Allen A, Adam RM, Monastyrskaya K, Gheinani AH.	GigaScience	SpheroScan: a user-friendly deep learning tool for spheroid image analysis.	10.1093/gigascience/giad082	2023	0.0	0.0		1.0	2024-10-15T09:52:46.103Z	2026-02-02T19:46:52.000Z	127575b1-dda0-4c00-9f22-520eb402fc63	undefined	d9uiis9a39				DOME_JSON	Match	Match
670e3c5061d57eb8bca695ce	Raw data and preprocessed data are all accessible at https://gitlab.com/sysbiobig/mlonmicrobiome . You can check raw data in the folder-original_data, and preprocessed data by each step in the folder-Count_Preprocessing and folder-Preprocessing. (This part is described at Section 2 and Code Availability Statement)	"Dataset is originated from four different public datasets in Qiita. Each dataset is open to the public from references: 1. Lloyd-Price et al (published 2019, citation#=1233), 2. Flores et al. (published 2014, citation #=377), 3. Halfvarsona et al. (published 2017, citation #=825), 4. McDonald et al. (published 2018, citation #=476). A Total of 2140 samples are used for the experiment. (citation # is referred at 2022/12/22) (This part is described at Section 2.1)
Among 1569 samples which have class state, 702 samples are positive and the rest are negative. Number of positives and negatives is similar, and various performance indexes are calculated to avoid class imbalance problems. (This part is described at Section 2)"	Partitioning is done by splitting the training and the independent test (never used for preprocessing, feature selection, parameter tuning) and, internally to the training set, by bootstrapping to maintain the class balance and random sorting. (This part is described at Section 2)	The ratio between training and test set was 80:20. Separate validation set was not used. To maintain the distribution of data types, bootstrapping was used. 	4.0	0.0	The source code containing evaluation files is open to the public at https://gitlab.com/sysbiobig/mlonmicrobiome with GNU General Public License. 	To validate the performance improvement/consistence, we compare with public, simple algorithm which is Linear SVM based RFE without any special preprocessing method. 	To prove the low variability of performance, the standard deviation of performance is written together with average in Supplementary Tables. Variability is rounded to two decimal places. 	To validate the evaluation, we check various performance indexes (MCC, AUC, accuracy, PPV, NPV, sensitivity, specificity) and stability indexes (rank based stability measurement-number of common features, Pearson Correlation, Canberra Distance, Bray-Curtis Dissimilarity). Evaluation is done by test dataset obtained from following process: merging benchmark datasets and splitting into training dataset and test dataset by random sort(bootstrapping). 	As illustrated in Fig.1, we divided the entire dataset into Ensemble Dataset 1 and Ensemble Dataset 2. And each ensemble dataset was divided to training set and test set. With this pipeline, we are able to evaluate the entire pipeline in two independent separate dataset. 	5.0	0.0	The source code is open to the public at https://gitlab.com/sysbiobig/mlonmicrobiome with GNU General Public License. 	The major execution time was consumed by RFE stage, and we recorded the execution time in the source code. In each bootstrap, RFE with mapping by Bray-curtis similarity matrix required around 3~4 minutes. 	We reported interpretation of model in Figure 5 by SHapley Additive exPlanations (SHAP). 	We used regressor to predict between healthy and IBD samples. 	4.0	0.0	Eight different types of prediction algorithms, namely logistic regression, linear SVM, random forest, XGBoost, Perceptron, and Multi-Layer Perceptron (MLP) with 1, 2 or 3 hidden layers, were used to classify samples in IBD vs. healthy using the features selected within the RFE phase. There was no new ML algorithm. 	We reported how hyperparameters were tuned in 2.5 Classification Model Algorithms, and reported the code at https://gitlab.com/sysbiobig/mlonmicrobiome with GNU General Public License. 	As explained in Section 2.1 Data, we divided the abundance profiles by the geometric mean in each data source and took the log (base 2) of the ratios, and then min-max scaler was applied.	We selected a core set of features with 283 taxa at the species level and 220 at the genus level. Feature selection was done by RFE, which is a core part of our paper. RFE was done only by training dataset. 	In this experiment, f was large so that overfitting should be considered (f>100). In this aspect, we propose stable feature selection method to prevent overfitting. 	Before applying the eight different types of prediction algorithms, Recursive Feature Elimination (RFE) by Linear SVM was used. RFE stage was done solely by training dataset, and test dataset was used only for evaluating the performance of eight different prediction algorithms.	Grid search is used to tune major parameters (learning rate, regularization parameter). Grid search is done solely on the training dataset in cross validation. As all models are built from scikit-learn library 1.0.2, definitions of parameters are all released in the library page and also summarized in Methods. (This part is described at Section 2.2 and 2.5.1)	We applied RFE with mapping transformation to prevent overfitting. Also, regularization parameter was tuned for applicable machine learning algorithm(logistic regression, linear SVM, XGBoost, MLP regressor).	8.0	0.0	670e3c50908d74f9cb331fd1	37882604.0	PMC10600917	02/02/2026 19:46:52	Lee Y, Cappellato M, Di Camillo B.	GigaScience	Machine learning-based feature selection to search stable microbial biomarkers: application to inflammatory bowel disease.	10.1093/gigascience/giad083	2023	0.0	0.0		1.0	2024-10-15T09:56:32.676Z	2026-02-02T19:46:52.000Z	3572dc07-ae4e-497f-b7b7-dc66981fbe02	undefined	16cyggfthl				DOME_JSON	Match	Match
670e60fa61d57eb8bca695f6	"Yes. The anonymized point cloud data are available from Vipond et al. ""Multiparameter persistent homology landscapes identify immune cell spatial patterns in tumors"" https://doi.org/10.1073/pnas.2102166118 

https://github.com/MultiparameterTDAHistology/SpatialPatterningOfImmuneCells

our pipeline is available at
https://github.com/dioscuri-tda/ecp_experiments/tree/main/immune_cells

"	"Anonymized point cloud data from Vipond et al. ""Multiparameter persistent homology landscapes identify immune cell spatial patterns in tumors"" https://doi.org/10.1073/pnas.2102166118 .
"	"We repeated the procedure outlined in the original paper, described in sectin 3D of the supplementary materials. 

For each pair of cell type we make a randomized 80/20 training/test split with stratification. The process was repeated 100 time"	"We repeated the procedure outlined in the original paper, described in section 3D of the supplementary materials.

There is a total of 1574 data points belonging to 3 different cell types. No separate validation set is provided."	4.0	0.0	https://github.com/dioscuri-tda/ecp_experiments MIT license	we compare our results to the one in the original article by Vipond et al.	confidence intervals are not present in Vipond et al. , we provide ours in our GitHUb. Our results are statistically significant.	Average accuracy on the test set.	cross-validation	5.0	0.0	https://github.com/dioscuri-tda/ecp_experiments MIT license	Seconds on a consumer-grade laptop.	The model is interpretable, but interpretation goes beyond the scope of this work.	Classification.	4.0	0.0	"We repeated the procedure outlined in the original paper, described in section 3D of the supplementary materials.
Vipond et al. ""Multiparameter persistent homology landscapes identify immune cell spatial patterns in tumors"" https://doi.org/10.1073/pnas.2102166118 

Quoting from the above source:

We test the ability of the MPH-landscape to distinguish the cell types in each tumor. For each pair of cell types we make a randomized 80/20 training/test split, and evaluate the classification accuracy of 3 classifiers (Linear Discriminant Analysis, LDA, Regularised Linear Discriminant Analysis, rLDA, and regularised Quadratic Discriminant Analysis, rQDA) on the test data. Repeating this process 100 times we attain average pairwise classification accuracies."	https://github.com/dioscuri-tda/ecp_experiments/tree/main/immune_cells MIT license	We used the Vipond et al.'s code to re-generate the same Vietoris-Rips and bifiltered Vietoris-Rips complexes from the provided pointclouds. We then computed ECC (radius only) and ECP (radius and codensity) for each complex and used them as input for the same LDA, rLDA and rQDA classifiers using the same train-test split procedure.	Both ECCs and ECPs where converted to vectors of lenght 51 by sampling them on a grid.	By comparing train and test accuracies 	No.	 Default hyperparameters from the scikit-learn implementations, as in the original study.	no.	7.0	1.0	670e60fa908d74f9cb33bb76	37966428.0	PMC10646871	02/02/2026 19:46:52	D≈Çotko P, Gurnari D.	GigaScience	Euler characteristic curves and profiles: a stable shape invariant for big data problems.	10.1093/gigascience/giad094	2023	0.0	0.0		1.0	2024-10-15T12:32:58.438Z	2026-02-02T19:46:52.000Z	2f8fe6a8-a0b4-40aa-93a9-3a83e53fde3f	undefined	x0nks31fro				DOME_JSON	Match	Match
670e64f661d57eb8bca695fa	The datasets we have utilized are publicly available datasets. All the relevant details can be found in the manuscript.	"
In this project, we utilized four different publicly available datasets that are well recognized in the domain. The first dataset consisted of mRNA data obtained from a study on Chronic Lymphocytic Leukemia (CLL) patients, measuring their transcriptome profiles. The second dataset was collected from a cervical cancer study that analyzed the expression levels of 714 miRNAs in human samples. The third and fourth datasets were obtained from The Cancer Genome Atlas (TCGA) and included mRNA and miRNA sequencing data from patients with Breast Invasive Carcinoma (BRCA). 
  
<!DOCTYPE html>
<html>
<head>
<style>
table {
  text-align: center;
}
th {
  text-align: center;
}
</style>
</head>
<body>

<table>

  <tr>
    <th>Dataset</th>
    <th>Data type</th>
    <th>Number of Samples</th>
    <th>Number of Features</th>
    <th>Target Class ratio</th>
  </tr>
  <tr>
    <td>CLL</td>
    <td>mRNA</td>
    <td>136</td>
    <td>5000</td>
    <td>Male (n=82): Female (n=54)</td>
  </tr>
  <tr>
    <td>Cervical cancer</td>
    <td>miRNA</td>
    <td>58</td>
    <td>714</td>
    <td>Normal (n=29): Tumor (n=29)</td>
  </tr>
  <tr>
    <td>TCGA-BRCA</td>
    <td>miRNA</td>
    <td>1207</td>
    <td>1404</td>
    <td>Normal (n=104): Tumor (n=1104)</td>
  </tr>
  <tr>
    <td>TCGA-BRCA</td>
    <td>mRNA</td>
    <td>1219</td>
    <td>5520</td>
    <td>Normal (n=113): Tumor (n=1106)</td>
  </tr>

</table>

</body>
</html>

"	"We maintained a distinct test/validation dataset to evaluate the model's performance on TCGA datasets. The original datasets were randomly split, ensuring that each class was proportionally represented within the dataset. Approximately 70% of the data was allocated to the training dataset, while the remaining 30% was assigned to the test dataset.

To assess the model's performance on the CLL and cervical cancer datasets, we employed the Repeated (10) Stratified K-fold (3) Cross-Validation method. This approach allowed us to thoroughly evaluate the model by repeatedly dividing the data into folds, ensuring that each fold maintained a balanced distribution of classes.
"	"We maintained a distinct test/validation dataset to evaluate the model's performance on TCGA datasets. The original datasets were randomly split, ensuring that each class was proportionally represented within the dataset. Approximately 70% of the data was allocated to the training dataset, while the remaining 30% was assigned to the test dataset.

To assess the model's performance on the CLL and cervical cancer datasets, we employed the Repeated (10) Stratified K-fold (3) Cross-Validation method. This approach allowed us to thoroughly evaluate the model by repeatedly dividing the data into folds, ensuring that each fold maintained a balanced distribution of classes.
"	4.0	0.0	Not applicable.	We compared the performance of our trained models with a dummy classifier, which is a classifier that makes random predictions. Typically, it is expected that the trained models will outperform the dummy classifier. This serves as a baseline comparison to assess the effectiveness and superiority of our trained models. By comparing the performance metrics of our models against the random predictions of the dummy classifier, we can evaluate the added value and efficacy of our trained models in making accurate predictions.	No.	We computed 7 evaluation metrics, including precision, recall, F1 score, and area under the curve (AUC), for each trained model across all datasets.This comprehensive approach enabled us to gain a thorough understanding of the model's performance from multiple angles and perspectives. 	We consistently maintained an independent test set for TCGA dataset to evaluate the model's performance and identify potential issues of overfitting or underfitting. To mitigate the risk of overfitting in the CLL and cervical datasets, we utilized the Repeated Stratified K-fold Cross-Validation method.	4.0	1.0	The source code for MLcps is available at https://github.com/FunctionalUrology/MLcps with GNU GENERAL PUBLIC LICENSE.	 It is not applicable to current project.	 We have used multiple ML classification algorithms, most of which are black box models. For example, SVM (Support Vector Machine) is considered a black box model, but features like support vectors and feature importance can offer some interpretability.	 Classification.	4.0	0.0	We have utilized 8 classification algorithms for each dataset in order to classify the classes within each dataset.	We did not perform any hyperparameter tuning for the trained models. 	For the CLL patients dataset, we specifically utilized the top 5,000 most variable mRNAs, excluding genes from the Y chromosome, as input for the machine learning pipeline. In the case of the BRCA mRNA dataset, our focus was solely on differentially expressed genes identified by edgeR, using a threshold of FDR ‚â§ 0.001 and logFC > ¬± 2. As for the cervical cancer dataset, we utilized it in its original form without making any modifications.	No feature selection was performed on any of the datasets.	We consistently maintained an independent test set for TCGA dataset to evaluate the model's performance and identify potential issues of overfitting or underfitting. To mitigate the risk of overfitting in the CLL and cervical datasets, we utilized the Repeated Stratified K-fold Cross-Validation method.	No. 	The majority of the trained models were utilized with their default parameters.	No.	7.0	1.0	670e3b6e908d74f9cb331be6	38091508.0	PMC10716825	02/02/2026 19:46:52	Akshay A, Abedi M, Shekarchizadeh N, Burkhard FC, Katoch M, Bigger-Allen A, Adam RM, Monastyrskaya K, Gheinani AH.	GigaScience	MLcps: machine learning cumulative performance score for classification problems.	10.1093/gigascience/giad108	2023	0.0	0.0		1.0	2024-10-15T12:49:58.625Z	2026-02-02T19:46:52.000Z	e8f2de3b-25bc-4a43-9af4-fc31a7f05d40	undefined	vdqf1t5lyj				DOME_JSON	Match	Match
670e65ee61d57eb8bca695fe	"The data are all publicly available, as they were all released with previous publications. Please refer the ""Data Availability"" section of the manuscript for details."	"The data were all from public repositories released with previous publications. Please refer the ""Data Availability"" section of the manuscript for details."	When we do train/test split, all splits are random.	"There are over ten different experiments in this work. In general, K% of the data were held out for testing, while (100-K)% were used during training. Among this (100-K)%, 85%*(100-K)% were used in the training loops, while 15%*(100-K)% were used in the validation loops (e.g., for the purpose of early stoping determination). In most cases, K = 20. But in some cases, due to limited amount of public data, we can only hold out smaller amount of data; otherwise, the training set would be too small to be effective. There are also a few experiments, we just followed the original split from the published dataset. Please refer the ""Data Availability"" section of the manuscript for details."	4.0	0.0	All models are released https://zenodo.org/records/10034416, which can be used to reproduce raw outputs	No	No	"The objective of this paper is to introduce a geneic machien learning tool, not for a specific scientific study. The evaluation metric varies from application to application. Please refer the ""Results"" section of the manuscript for details."	evaluated on hold-out sets	3.0	2.0	yes, open source python package. https://github.com/MMV-Lab/mmv_im2im (MIT license)	Generally about a few seconds, depending on the size of the image and the specific model and application.	in general, deep neural networks are not fully interpretable	The objective of this paper is to introduce a machine learning tool, not for a specific scientific question. The model can be regression or classification depending on specific image-to-image transformation application. For labelfree, denoising, modality transfer, synthetic image generation, the models are regression; for other segmentation tasks, the models are classification.	4.0	0.0	Neural network. Four types of methods were used, conditional GAN, cycle GAN, FCN, and embedding based instance segmentation network.	yes, https://github.com/MMV-Lab/mmv_im2im/tree/main/paper_configs	Usually, the images need to go through intensity normalization before feeding into the neural network. Different experiments may use different intensity normalization methods.	Inputs are images	We demonstration examples of various regularization methods in different experiments, such as weigh decay and early stopping, etc.. The objective of this paper is to introduce a generic machine learning tool, not for a specific scientific question. Users can configure their own way to control overfitting and under fitting, depending on their own data.	No	"We used different neural networks for different applications. Please refer the ""Results"" section of the manuscript for details. The goal is not to show the best model. Instead, it is to highlight the flexibility of our tool, where users can test differnet models without changing any line of code. The objective of this paper is to introduce a generic machine learning tool, not for a specific scientific question."	yes, we used weight decay in the optimizer and early stopping monitoring the validation loss. (Again, the objective of this paper is to introduce a generic machine learning tool, not for a specific scientific question. One highlight of this tool is that users can easily configure different regularization methods, e.g., different early stopping criteria, different optimizer, adding additional regularization term in the loss, etc.)	7.0	1.0	6435665292c76639b80869bd	38280188.0	PMC10821710	02/02/2026 19:46:52	Sonneck J, Zhou Y, Chen J.	GigaScience	MMV_Im2Im: an open-source microscopy machine vision toolbox for image-to-image transformation.	10.1093/gigascience/giad120	2023	0.0	0.0		1.0	2024-10-15T12:54:06.607Z	2026-02-02T19:46:52.000Z	b2f6bf1e-bccb-4250-98a3-0b71335847dc	undefined	9izph0m2we				DOME_JSON	Match	Match
670e670661d57eb8bca69602	The datasets we have utilized are publicly available datasets. All the relevant details can be found in the manuscript.	"
In this project, we utilized six different publicly available datasets that are well recognized in the domain. The first dataset consisted of mRNA data obtained from a study on Chronic Lymphocytic Leukemia (CLL) patients, measuring their transcriptome profiles. The second dataset was collected from a cervical cancer study that analyzed the expression levels of 714 miRNAs in human samples. The third and fourth datasets were obtained from The Cancer Genome Atlas (TCGA) and included mRNA and miRNA sequencing data from patients with Breast Invasive Carcinoma (BRCA). 
  <br>
  <br>


The fifth dataset consists of scRNA-seq data obtained from peripheral blood mononuclear cells (PBMCs) that were sequenced using 10√ó chromium technology. Among all the cell populations described in this study, we specifically utilized the scRNA datasets of CD8+ na√Øve, CD14+, and CD16+ monocytes (n=1500) with the goal of identifying distinct markers for each of these cell populations. The sixth dataset used in this study was the widely recognized Glass Identification dataset obtained from the University of California Irvine (UCI) ML repository.
  
<!DOCTYPE html>
<html>
<head>
<style>
table {
  text-align: center;
}
th {
  text-align: center;
}
</style>
</head>
<body>

<table>

  <tr>
    <th>Dataset</th>
    <th>Data type</th>
    <th>Number of Samples</th>
    <th>Number of Features</th>
    <th>Target Class ratio</th>
  </tr>
  <tr>
    <td>CLL</td>
    <td>mRNA</td>
    <td>136</td>
    <td>5000</td>
    <td>Male (n=82): Female (n=54)</td>
  </tr>
  <tr>
    <td>Cervical cancer</td>
    <td>miRNA</td>
    <td>58</td>
    <td>714</td>
    <td>Normal (n=29): Tumor (n=29)</td>
  </tr>
  <tr>
    <td>TCGA-BRCA</td>
    <td>miRNA</td>
    <td>1207</td>
    <td>1404</td>
    <td>Normal (n=104): Tumor (n=1104)</td>
  </tr>
  <tr>
    <td>TCGA-BRCA</td>
    <td>mRNA</td>
    <td>1219</td>
    <td>5520</td>
    <td>Normal (n=113): Tumor (n=1106)</td>
  </tr>
  <tr>
    <td> PBMC </td>
    <td> scRNA-seq </td>
    <td>1500</td>
    <td>500</td>
    <td> CD8 Naive (n=500 cells) : CD14 Monocytes (n=500 cells) : CD16 Monocytes (n=500 cells) )</td>
  </tr>

  <tr>
    <td>Glass Identification</td>
    <td>Oxide content (i.e., Na, Fe, K, etc)</td>
    <td>214</td>
    <td>10</td>
    <td>Glass 1 (70), Glass 2 (76), Glass 3 (17), Glass 5 (12), Glass 6 (10), Glass 7 (29)</td>
  </tr>
</table>

</body>
</html>

"	We retained an independent dataset to evaluate the performance of the model for each dataset. The original datasets were divided randomly but in a stratified manner, with 70% allocated to the training dataset and 30% allocated to the test dataset.	We kept a separate test/validation dataset to assess the model's performance on each dataset. The initial datasets were randomly divided, but in a way that preserved proportional representation of each class in the given dataset, with 70% assigned to the training and 30% assigned to the test dataset.	4.0	0.0	All the trained models and corresponding performance results are available on (https://doi.org/10.5281/zenodo.8073635) under the Creative Commons - CC0 license.	We compared the performance of our trained models with a dummy classifier, which is a classifier that makes random predictions. Typically, it is expected that the trained models will outperform the dummy classifier. This serves as a baseline comparison to assess the effectiveness and superiority of our trained models. By comparing the performance metrics of our models against the random predictions of the dummy classifier, we can evaluate the added value and efficacy of our trained models in making accurate predictions.	No.	We computed more than 10 evaluation metrics, including precision, recall, F1 score, and area under the curve (AUC), for each trained model across all datasets. This comprehensive approach enabled us to gain a thorough understanding of the model's performance from multiple angles and perspectives	We consistently maintained an independent test set for all datasets to evaluate the model performance accurately. In addition, we employed multiple model evaluation techniques such as repeated stratified k-fold cross-validation, nested cross-validation, and others. These techniques allow for robust and reliable assessment of the models, taking into account the variance and potential bias in the data. By utilizing such methods, we aimed to obtain a comprehensive understanding of the model's performance across different folds and iterations, enhancing the reliability of our results.	4.0	1.0	The source code for MLme is available at https://github.com/FunctionalUrology/MLme with GNU GENERAL PUBLIC LICENSE.	It is not applicable to current project.	We have used multiple ML classification algorithms, some of which are black box models. For example, SVM (Support Vector Machine) is considered a black box model, but features like support vectors and feature importance can offer some interpretability. In contrast, kNN (K-Nearest Neighbors) is generally interpretable as it relies on nearest neighbors and distance metrics.	Classification.	4.0	0.0	We have utilized over 15 classification algorithms for each dataset in order to classify the classes within each datasets.	We did not perform any hyperparameter tuning for the trained models. However, all the trained models and corresponding performance results are available on Zenodo (https://doi.org/10.5281/zenodo.8073635) under the Creative Commons - CC0 license.	For the CLL patients dataset, we exclusively utilized the top 5,000 most variable mRNAs, excluding genes from the Y chromosome, as input for ML pipeline. As for the BRCA mRNA dataset, we concentrated solely on differentially expressed genes identified by edgeR, with a threshold of FDR ‚â§ 0.001 and logFC > ¬± 2.  For the PBMC dataset, we utilized only 500 highly variable genes across the three cell populations. For all other datasets, we employed them as they were originally available without any modifications.	To evaluate MLme, we utilized datasets ranging from 10 features to 5000 features. Feature selection was performed on all the datasets, but we always maintained an independent dataset solely for evaluating the model's performance. It's important to note that this independent dataset was not used for any kind of preprocessing or model training.	We consistently maintained an independent test set for all datasets in order to evaluate the model's performance and determine if there was any overfitting or underfitting. Additionally, we employed cross-validation methods and feature selection techniques to mitigate the risk of overfitting. These measures helped ensure a more reliable assessment of the model's generalization capabilities and minimize the influence of biased or noisy data.	The ML pipeline employed in this project incorporates several preprocessing steps, including data resampling and feature selection. These steps are followed by model training and evaluation. To prevent any potential data leakage, we perform data preparation within cross validation folds. Additionally, we maintain an independent test dataset to ensure accurate and unbiased evaluation.	The majority of the trained models were utilized with their default parameters.	We employed cross-validation methods and feature selection techniques to mitigate the risk of overfitting. These measures helped ensure a more reliable assessment of the model's generalization capabilities and minimize the influence of biased or noisy data.	8.0	0.0	670e3b6e908d74f9cb331be6	38206587.0	PMC10783149	02/02/2026 19:46:52	Akshay A, Katoch M, Shekarchizadeh N, Abedi M, Sharma A, Burkhard FC, Adam RM, Monastyrskaya K, Gheinani AH.	GigaScience	Machine Learning Made Easy (MLme): a comprehensive toolkit for machine learning-driven data analysis.	10.1093/gigascience/giad111	2024	0.0	0.0		1.0	2024-10-15T12:58:46.423Z	2026-02-02T19:46:52.000Z	bb9e3878-75dc-4120-88f4-0a98f0195ec1	undefined	bxlfk5g20v				DOME_JSON	Match	Match
670e6a0961d57eb8bca6960a	The data is accessible via GigaDB and the supplemental material.	The source of the dataset is INfrastructure for a PHAge REference Database (INPHARED) provided by https://github.com/RyanCook94/inphared on Aug 1st, 2022. There are 3,659 bacteriophage genomes in this dataset. We curated 1912 tailspike protein sequences and 200732 non-tailspike protein sequences from this dataset. This data has not been used in previous papers.	To train and validate the SpikeHunter the manually curated set of tailspike proteins was first clustered into 20,274 clusters at 30% identity using CD-HIT. The dataset was divided into training, validation, and testing datasets in a ratio of 3:1:1 using the StratifiedGroupKFold function in the Scikit-learn python package, with two key objectives in mind: 1) ensuring the absence of overlaps among protein clusters, and 2) maintaining consistent ratios of positive to negative samples across the splits. As a result of this process, the training, validation and test sets are independent.	The dataset was divided into training, validation, and testing datasets in a ratio of 3:1:1. The training set includes 122,506 proteins (comprising 1,023 positive samples and 121,483 negative samples belonging to 12,170 clusters), a validation set 40,838 proteins (comprising 343 positive samples and 40,495 negative samples belonging to 4,054 clusters), and a test set 39,300 proteins (comprising 546 positive samples and 38,754 negative samples belonging to 4,050 clusters).	4.0	0.0	Yes. The data is accessible via GigaDB and the supplemental material.	No.	No.	Evaluation of the model on the testing dataset demonstrated an F1-score of 0.994, precision of 0.991, recall of 0.996, specificity of 1.000, a Mathew‚Äôs correlation coefficient (MCC) of 0.994, and the area under the precision-recall curve of 0.994.	It was evaluated by an independent test dataset.	3.0	2.0	Yes, the source code is released at https://github.com/nlm-irp-jianglab/SpikeHunter.	3 days with two NVIDIA v100x GPUs on a high-performance computing cluster.	It is a black box.	The model is a classification that outputs probabilities to indicate whether each sequence corresponds to a tailspike protein or not.	4.0	0.0	This is a deep-learning model that utilizes the pre-trained transformer protein language model ESM-2 (esm2_t33_650M_UR50D) as an embedding layer. This embedding layer is then linked to a neural network comprising three fully connected layers with 1280, 568, and 2 nodes, respectively. The output layer employs a softmax activation function to produce probabilities, indicating whether each sequence corresponds to a tailspike protein or not.	Yes. The deep-learning model trained in this study is provided at https://figshare.com/articles/online_resource/SpikeHunter_trained_model_pth_file/23577051.	Phage protein sequences were tokenized and transformed into numerical vectors using the batch_converter function in the ESM Python package (https://github.com/facebookresearch/esm).	Since the sequences are embedded as 1280-length representations using a pre-trained transformer protein language model ESM-2 during training, the number of input features before embedding varies depending on the lengths of protein sequences, while the number of features after the embedding is 1280. No feature selection strategy was performed.	The model's parameter count is approximately eight times the number of training data points. Early stopping was adopted to stop training once the model performance was no longer improved on the validation dataset for three consecutive epochs to avoid overfitting.	This model is not a meta-predictor.	800698	Early stopping was adopted to stop training once the model performance was no longer improved on the validation dataset for three consecutive epochs to avoid overfitting.	8.0	0.0	652556ed92c76639b86e085d	38649301.0	PMC11034027	02/02/2026 19:46:52	Yang Y, Dufault-Thompson K, Yan W, Cai T, Xie L, Jiang X.	GigaScience	Large-scale genomic survey with deep learning-based method reveals strain-level phage specificity determinants.	10.1093/gigascience/giae017	2024	0.0	0.0		1.0	2024-10-15T13:11:37.812Z	2026-02-02T19:46:52.000Z	1261ec25-2b6d-4a8a-aca6-a50c235a4737	undefined	dfyn1yvtz3				DOME_JSON	Match	Match
670e6eef61d57eb8bca6960e	"Yes
1. Mouse olfactory bulb: 
a. The 10x Geomics Visium dataset can be download from: https://www.10xgenomics.com/resources/datasets/adult-mouse-olfactory-bulb-1-standard-1
b. The Stereo-seq datasets can be download from: https://db.cngb.org/stomics/mosta/download/
2. Human dorsolateral prefrontal cortex (DLPFC): the 10x Geomics Visium dataset and annotation file can be download from: https://zenodo.org/record/6925603#.YuM5WXZBwuU
3. Mouse hippocampal dataset: the Slide-seq datasets can be download from: https://singlecell.broadinstitute.org/single_cell/study/SCP815/highly-sensitive-spatial-transcriptomics-at-near-cellular-resolution-with-slide-seqv2#study-summary, https://singlecell.broadinstitute.org/single_cell/study/SCP354/slide-seq-study#study-summary, and https://singlecell.broadinstitute.org/single_cell/study/SCP948/robust-decomposition-of-cell-type-mixtures-in-spatial-transcriptomics#study-summary, respectively.
4. Mouse embryonic brain: the Stereo-seq datasets can be download from: https://db.cngb.org/stomics/mosta/download/"	publication	No	We did not split the data and used all datasets for model training and testing.	3.0	1.0	No	benchmarking method: PRECAST, GraphST, SCALEX, Harmony, Combat, BBKNN, Scanorama, MNN	No	"F1 score of local inverse Simpson's index, adjusted rand index, Moran's I index
Downstream bioinformation analysis, differential expression analysis, GO enrichment analysis, trajectory inference analysis, et al."	We use different dataset to evaluate our method, which include measured by different sequencing platforms, different time-series, different regions, et al.	3.0	2.0	github1: https://github.com/zhangchao162/Spatialign.git github2: https://github.com/STOmics/Spatialign.git pypi: https://pypi.org/project/spatialign/ tutorial: https://spatialign-tutorials.readthedocs.io/en/latest/index.html	base on the input dataset	early stopping	No, our model output a latent embedding and reconstructed representation, respectively.	4.0	0.0	We first implement a self-supervised contrastive learning architecture (Deep graph infomax framework) for dimensional reduction while simultaneously propagating neighbouring spatil context between spots/cells. And we employ an across-domain adaptation technique to align joint embeddings. 	 No	The collected datasets were be saved as '*.h5ad' format, and also includes two-dimensional spatial coordinates for each spot/cell. The dataformat can reference: https://anndata.readthedocs.io/en/latest/  In the preprocessing step, the raw gene expression matrices were first filtered according to criteria 'min_gene' smaller than 20 and 'min_cell' smaller than 20  for each data using SCANPY (version: 1.9.1), and followed by normalization and log transformation of individual spots.  In our algorithm, spatiAlign, we just set the 'is_norm_log' to True.	base on input data list, we choose common genes as input	No	No	"There are 17 instantiated model parameters. Details as follows,
:param data_path: List of input dataset path.
:param min_genes: Minimum number of genes expressed required for a cell to pass filtering, default 20.
:param min_cells: Minimum number of cells expressed required for a gene to pass filtering, default 20.
:param batch_key: The batch annotation to :attr:`obs` using this key, default, 'batch'.
:param is_norm_log: Whether to perform 'sc.pp.normalize_total' and 'sc.pp.log1p' processing, default, True.
:param is_scale: Whether to perform 'sc.pp.scale' processing, default, False.
:param is_hvg: Whether to perform 'sc.pp.highly_variable_genes' processing, default, False.
:param is_reduce: Whether to perform PCA reduce dimensional processing, default, False.
:param n_pcs: PCA dimension reduction parameter, valid when 'is_reduce' is True, default, 100.
:param n_hvg: 'sc.pp.highly_variable_genes' parameter, valid when 'is_reduce' is True, default, 2000.
:param n_neigh: The number of neighbors selected when constructing a spatial neighbor graph. default, 15.
:param is_undirected: Whether the constructed spatial neighbor graph is undirected graph, default, True.
:param latent_dims: The number of embedding dimensions, default, 100.
:param is_verbose: Whether the detail information is print, default, True.
:param seed: Random seed.
:param gpu: Whether the GPU device is using to train spatialign.
:param save_path: The path of alignment dataset and saved spatialign.

There are 7 training model parameters. Details as follows,
:param lr: Learning rate, default, 1e-3.
:param max_epoch: The number of maximum epochs, default, 500.
:param alpha: The momentum parameter, default, 0.5
:param patient: Early stop parameter, default, 15.
:param tau1: Instance level and pseudo prototypical cluster level contrastive learning parameters, default, 0.2
param tau2: Pseudo prototypical cluster entropy parameter, default, 1.
:param tau3: Cross-batch instance self-supervised learning parameter, default, 0.5
"	early stopping	6.0	2.0	64ffbbb492c76639b801efb4	39028588.0	PMC11258913	02/02/2026 19:46:52	Zhang C, Liu L, Zhang Y, Li M, Fang S, Kang Q, Chen A, Xu X, Zhang Y, Li Y.	GigaScience	spatiAlign: an unsupervised contrastive learning model for data integration of spatially resolved transcriptomics.	10.1093/gigascience/giae042	2024	0.0	0.0		1.0	2024-10-15T13:32:31.707Z	2026-02-02T19:46:52.000Z	dfddd412-f46b-447b-8ab6-a45bfeaf37c2	undefined	345ml05xtn				DOME_JSON	Match	Match
670e70a761d57eb8bca69612	"Single-class simulated dataset: Yes, published on https://github.com/BioSok/OmadaSimulatedDatasets. 

Multi-class simulated dataset: Yes, published on https://github.com/BioSok/OmadaSimulatedDatasets. 

Pan-cancer dataset: These is public data and the Breast cancer found in https://www.cbioportal.org/study/summary?id=brca_tcga_pan_can_atlas_2018, Colorectal cancer found in https://www.cbioportal.org/study/summary?id=coadread_tcga_pan_can_atlas_2018 and Lung cancer found in https://www.cbioportal.org/study/summary?id=luad_tcga_pan_can_atlas_2018.

PAH dataset: The data can be found in the EGA (the European Genome-phenome Archive) database under accession code EGAS000010055326562 (https://ega-archive.org/studies/EGAS00001005532) . Restricted access, needs application.

GUSTO dataset: The data can be found in the GEO database with accession number GSE182409."	"Single-class simulated dataset: Simulated by using specific mean and standard deviation. This dataset only contains one class. The dataset hasn't been used before as it was generated from one of Omada's functions.

Multi-class simulated dataset: Simulated by using specific means and standard deviations. This dataset contains five classes of almost identical sizes (72,72,72,72,71). The dataset hasn't been used before as it was generated from one of Omada's functions.

Pan-cancer dataset:  Sourced from https://pubmed.ncbi.nlm.nih.gov/32025007/ and contains 3 classes: Breast cancer found in https://www.cbioportal.org/study/summary?id=brca_tcga_pan_can_atlas_2018, Colorectal cancer found in https://www.cbioportal.org/study/summary?id=coadread_tcga_pan_can_atlas_2018 and Lung cancer found in https://www.cbioportal.org/study/summary?id=luad_tcga_pan_can_atlas_2018. These data have been used in several publications and are widely used for cancer studies.

PAH dataset: The transcriptomic data can be found in the EGA (the European Genome-phenome Archive) database under accession code EGAS000010055326562 (https://ega-archive.org/studies/EGAS00001005532) . Restricted access, needs application. This dataset has no defined classes. It has been used by https://www.nature.com/articles/s41467-021-27326-0.

GUSTO dataset: This dataset can be found in the GEO database with accession number GSE182409. There are no classes."	No test sets for any dataset.	"Single-class simulated dataset: The dataset contains 100 datapoints. No test set as this was used for unsupervised learning methods.

Multi-class simulated dataset: The dataset contains 359 datapoints. No test set as this was used for unsupervised learning methods.

Pan-cancer dataset:  Each of the three classes have the following datapoints: breast (n=1084), lung (n=566) and colorectal (n=594). No test set as this was used for unsupervised learning methods.

PAH dataset: The dataset contains 359 datapoints. No test set as this was used for unsupervised learning methods.

GUSTO dataset: The dataset contains 238 datapoints. No test set as this was used for unsupervised learning methods."	4.0	0.0	No, but the p-values of the biological validation can be found in https://www.nature.com/articles/s41467-021-27326-0.	There are no methods that address the same questions. 	The statistical significance was calculated based on biological differences and not ML metrics. 	As this is unsupervised learning the performance was justified biologically.	Independent dataset.	5.0	0.0	The source code can be found: https://github.com/BioSok/omada and is also published as a package in https://www.bioconductor.org/packages/release/bioc/html/omada.html.	Under a minute for a dataset for few hundred datapoints/features.	Black box.	Unsupervised clustering.	4.0	0.0	Three (unsupervised) widely-used clustering algorithms were used: spectral, k-means and hierarchical. 	No.	The data were in numeric form representing gene counts (FPKM or TPM depeding on the dataset). The processing consists of arcise transformation.	Each dataset used a different number of features ranging from 100 to 25,955. Feature selection was performed as a step of Omada using the average cluster stability for different feature subsets after they were ranked by expression variance.	This was unsupervised learning.	The input does not consist of other ML algorithm results.	In this model, Spectral and k-means clustering allows one parameter (kernel) and hierarchical allows 2 parameters (distance measure and linkage). These parameters were selected to as they provide the most influence towards the clustering results.	This was unsupervised learning.	7.0	1.0	670e70a7908d74f9cb33fe7c	38991852.0	PMC11238428	02/02/2026 19:46:52	Kariotis S, Tan PF, Lu H, Rhodes CJ, Wilkins MR, Lawrie A, Wang D.	GigaScience	Omada: robust clustering of transcriptomes through multiple testing.	10.1093/gigascience/giae039	2024	0.0	0.0		1.0	2024-10-15T13:39:51.375Z	2026-02-02T19:46:52.000Z	49b4a023-592f-4a04-bad2-827e519896e0	undefined	iv50u2ycn5				DOME_JSON	Match	Match
670e716961d57eb8bca69616	The training script and test data for this article will be uploaded to the website's database after publication. Currently, you can use and view the MOBFinder tool on Github: https://github.com/MOBFinder	All plasmid genomes were downloaded from NCBI database.	When training the four MOB classification models suitable for different length ranges, the training and testing sets were randomly split in a 7:3 ratio. There was no overlap between the training and testing sets; they were independent of each other.	When training plasmid DNA word vectors based on the Skip-gram language model, MOBFinder utilized a total of 37,139 plasmid genomes. For training the four MOB classification models, this study obtained the complete plasmid genomes from the NCBI database again and annotated them with MOB typing using the relaxase database. Subsequently, 67,925 plasmid genomes determined by MOB typing were used to simulate plasmid assembly fragments from metagenomic datasets for training the MOB classification models	4.0	0.0	 The evaluation script for this tool will be uploaded to the relevant database after the article is published. You can access the MOBFinder tool from Github: https://github.com/FengTaoSMU/MOBFinder.	As there are currently no benchmark datasets for metagenomic plasmid assembly fragments, this study constructed one from scratch and compared it with existing tools like MOB-suite and MOBscan.	The evaluation metrics used in this study range from 0 to 1, with 0 indicating the lowest performance and 1 indicating the highest. MOBFinder's performance surpasses existing tools significantly, with an overall accuracy at least 59% higher than MOB-suite and at least 61% higher than MOBscan.	This tool uses F1 score, balanced accuracy, harmonic mean, and AUC to evaluate the model. These metrics are commonly used in other similar algorithms, such as PPR-Meta and PlasTrans.	Independent dataset.	5.0	0.0	The source code of this tool will be uploaded to the corresponding database after the article is published. You can access the MOBFinder tool from Github: https://github.com/FengTaoSMU/MOBFinder.	According to our test results, MOBFinder takes approximately 5 to 18 minutes to predict on the test set, depending on the length and quantity of input data.	Black box.	Classification.	4.0	0.0	The machine learning algorithms used in this study are the Skip-gram language model and Random Forest. Both of these methods are well-known in machine learning. Currently, methods used for MOB typing annotation rely on relaxase-based approaches such as MOB-suite and MOBscan. However, in metagenomic plasmid assembly fragments, relaxase sequences are often missing or incomplete. As a result, most plasmid assembly fragments are annotated as non-transferable plasmids. Due to differences in sequence features and host range among different MOB types, this study utilized the Skip-gram language model to digitally encode the biological patterns and features of metagenomic plasmid assembly fragments, resulting in significant performance improvements for MOBFinder. With a training dataset of up to 990,000 entries, Random Forest was used to prevent overfitting in training the MOB classification models. The combined use of these two algorithms effectively enhanced the performance of predicting MOB types in metagenomic plasmid assembly fragments. In some cases, F1 and AUC scores reached as high as 0.99, almost perfect.	According to our test results, MOBFinder takes approximately 5 to 18 minutes to predict on the test set, depending on the length and quantity of input data.	"1. In word vector training, this study generated overlapping ""words"" of 4-mers from plasmid genomes and assigned a random numeric vector to each unique ""word"". These vectors were then inputted into a two-layer neural network for training, learning the probability of occurrence of 10 preceding and succeeding ""words"". The two-layer neural network consists of a hidden layer with 100 neurons. Finally, 100-dimensional word vectors corresponding to 256 DNA 4-mer ""words"" were outputted. 2. Training of the MOB classification model: This study utilized known relaxases of MOB types to perform MOB typing on plasmid genomes, selecting plasmid genomes with confirmed MOB types based on their scores to construct benchmark datasets. For each MOB type of plasmid genome, the training and testing sets were divided in a 7:3 ratio. The training set for each MOB type randomly generated 90,000 simulated metagenomic plasmid assembly fragments, while the testing set for each MOB type randomly generated 500 simulated metagenomic plasmid assembly fragments. All fragments were encoded using the generated 4-mer word vectors, with the average 100-dimensional word vector calculated as input for training the Random Forest model."	This tool did not conduct feature selection; instead, it used 100 word vector features trained by the skip-gram model as input.	The number of each MOB type in the training dataset is the same, and random forest was used to prevent overfitting.	No.	"1. The skip-gram model consists of two layers of neural networks: a hidden layer and an output layer, with 100 neurons in the hidden layer. For each input 4-mer ""word"", it predicts the probability of occurrence of 10 preceding and succeeding words in its context.
2. The random forest model uses 500 trees."	Random forest.	7.0	1.0	651ac93d92c76639b850254d	39101782.0	PMC11299106	02/02/2026 19:46:52	Feng T, Wu S, Zhou H, Fang Z.	GigaScience	MOBFinder: a tool for mobilization typing of plasmid metagenomic fragments based on a language model.	10.1093/gigascience/giae047	2024	0.0	0.0		1.0	2024-10-15T13:43:05.591Z	2026-02-02T19:46:52.000Z	bf403e75-6baf-4278-bf96-1469c78c65e0	undefined	4y1myuozde				DOME_JSON	Match	Match
670e76ea61d57eb8bca6961b	"The data is available following the original publication*. 

* Doron S, Melamed S, Ofir G, Leavitt A, Lopatina A,Keren M, et al. Systematic discovery of antiphage defense systems in the microbial pangenome. Science 2018;359(6379):eaar4120. https://www.science.org/doi/abs/10.1126/science.aar4120."	"Our dataset consists of 21196 unique validated samples available due to a previous publication*. The dataset we used was imbalanced for types, consisting of 1263 Durantia samples, 3723 Gajiba samples,1529 Hachiman samples, 6882 Wadjet samples, 637 Lamassu samples, 2807 Septu samples, 647 Shedu samples, 1097 Thoeris samples, 745 Kiwa samples, 1866 Zorya samples. Hence there were only roughly one-tenth of the samples of Lamassu than for Wadjet.  

* Doron S, Melamed S, Ofir G, Leavitt A, Lopatina A,Keren M, et al. Systematic discovery of antiphage defense systems in the microbial pangenome. Science 2018;359(6379):eaar4120. https://www.science.org/doi/abs/10.1126/science.aar4120."	Training and test set were insured to be independent, by controlling for sequence homology using BLAST.	We separate our dataset in a stratified training, test, and validation split, using a 10-fold CV split to ensure enough training data samples are available for subclasses with few samples.	4.0	0.0	Additional evaluation files are part of the gigascience database.	We compare our method to PADLOC on a set of phages, bacteria and archea.	The confidence of the class assignments is discussed in the publication. Additional wet-lab experiments will be needed to verify the quality.	We supply representative metrics ROC-AUC and AUPRC togther with BLAST allignment scores, aswell as classification scores for single classes.	We use cross-validation and ensure the independence of our training/test dataset using BLAST.	5.0	0.0	The source code can be found in our github repro under the free MIT licence: https://github.com/SvenHauns/Deepdefense	On an older laptop (cpu run only, Intel(R) Core(TM) i5-8350U CPU @ 1.70GHz) after first loading the model a simple prediction can be made in 0.60 seconds.	The model is mostly a black box model. The output of the model is more interpretable than it would usually be the case, since we make use of methods to increase the model calibration.	classification	4.0	0.0	"To optimize our algorithm we choose BOHB (bayesian optimization with hyperband)*

*Falkner S, Klein A, Hutter F. BOHB: Robust and Efficient Hyperparameter Optimization at Scale. CoRR 2018;abs/1807.01774.http://arxiv.org/abs/1807.01774."	Details of the optimization schedule and hyperparameter configurations can be found in the publication.	data was encoded in a one-hot-vector fashion using the aminoacids of the protein	The input consists primarily of the proteins, ecnoded in a one-hot fashion, additionally we supply information characterizing any proteins as used by a previous publication. No further feature selection was performed.	Overfitting was controlled by using a validation set with an early stopping procedure. 	does not use a meta-predictor	The current deeplearning model has roughly 11700000 learnable parameter. The size of the model was subject of the optimization processed and hence also optimized. 	We use early stopping using a validation set to prevent overfitting. Additionally a dropout rate was discovered using the described optimization mechanism. 	8.0	0.0	670e76ea908d74f9cb3419bb	39388605.0	PMC11959188	02/02/2026 19:46:52	Hauns S, Alkhnbashi OS, Backofen R.	GigaScience	Deepdefense: annotation of immune systems in prokaryotes using deep learning.	10.1093/gigascience/giae062	2024	0.0	0.0		1.0	2024-10-15T14:06:34.688Z	2026-02-02T19:46:52.000Z	440c11f3-f064-40d7-9b1d-5d29591896b4	undefined	zn00kk9s7i				DOME_JSON	Match	Match
670e781961d57eb8bca6961f	The splits for the training data are published in the corresponding github repository (https://github.com/tovaroe/GBMatch_CNN, GPL 3.0).	"The training data is derived from the publication ""The DNA methylation landscape of glioblastoma disease progression shows extensive heterogeneity in time and space"" (https://doi.org/10.1038/s41591-018-0156-x) and contains a total of 276 cases, 189 of which have information on the transcriptional subtype.

"	The external validation set is completely independent, as it is derived from TCGA.	"For model selection, the above mentioned 276 or 189 cases, respectively, were split into 5 equally sized fold for 5-fold CV, with similar distribution of transcriptional subtypes and survival, respectively.

The external validation data is derived from TCGA, consists of 178 cases and can be accessed via cBioPortal (https://www.cbioportal.org) and/or the GDC Data Portal (https://portal.gdc.cancer.gov). In contrast to the training dataset, the transcriptional subtypes were determined by RNAseq, but the overall distribution is similar, albeit with slightly more classical cases and fewer mesenchymal cases. Due to the strict selection critera for the study underlying the training dataset, the overall survival in the TCGA dataset was slightly lower."	4.0	0.0	Evaluation results are covered in the manuscript. the Model output for the external TCGA validation dataset is available via the github page: https://github.com/tovaroe/GBMatch_CNN (GPL 3.0)	NA / no benchmark datasets available.	NA / no other methods for comparison available	"For prediction of the transcriptional subtype: accuracy, AUC.
For prediction of the survival: median overall survial and logrank test after groupint into high-risk and low-risk groups."	5-fold CV and independent external test dataset	5.0	0.0	The source code is released via github: https://github.com/tovaroe/GBMatch_CNN (GPL 3.0)	Depending on the slide scan size, a single prediction requires a few minutes on a workstation desktop PC.	The model is semi-interpretable. While the prediction of single image tiles is a black box, the mapping of tile prediction onto the whole slide scans allows for interpretability (discussed extensively in the manuscript).	Both	4.0	0.0	We used a pre-trained Xception CNN as the backbone with an additional layer for TS/survival prediction and fine-tuning.	Available via the github-page in the config.py (https://github.com/tovaroe/GBMatch_CNN)	Image tiles were used as input for the ML algorithm, the preprocessing was performed according to the keras implementation of the Xception model.	Image tiles (512x512 px) were used as input, no additional feature selection was performed.	Overfitting/Underfitting was ruled out by keeping track of the validation set error in 5-fold-CV during hyperparameter selection; And by fixing the pre-trained model parameters and only fine-tuning the last few layers.	Not applicable	No additional parameters werde set in the pre-trained Xception model. For prediction of survival, a 1-neuron layer was added as the ultimate layer, and for prediction of the transcriptional subtype, a 3-neuron layer was added.	Dropout (0.25)	8.0	0.0	670e7819908d74f9cb341ef3	39185700.0	PMC11345537	02/02/2026 19:46:52	Roetzer-Pejrimovsky T, Nenning KH, Kiesel B, Klughammer J, Rajchl M, Baumann B, Langs G, Woehrer A.	GigaScience	Deep learning links localized digital pathology phenotypes with transcriptional subtype and patient outcome in glioblastoma.	10.1093/gigascience/giae057	2024	0.0	0.0		1.0	2024-10-15T14:11:37.681Z	2026-02-02T19:46:52.000Z	19954d39-0d13-4f99-9e5d-0624ccd5b638	undefined	2bpmmu8yav				DOME_JSON	Match	Match
67333632a96b586fdb27231e	You can access to the curated RefSeq dataset with the link http://zenodo.org/records/14005015 (dataset.tar.gz), which is licensed under the CC0 license.	The data is all curated from the NCBI RefSeq database. There are 173,666, 99,945, 28,081 data points for the three GO categories, MF, BP, and CC, repectively (three sub-tasks). The minimum positive points of each label is set to 50. Because we formulate the plasmid protein function prediction as a multi-class, multi-label classification on 377 labels (172 MF labels + 174 BP labels + 31 CC labels), we can't show all the detailed number of positive/negative points for the 377 binary classifications here. However, you can check more detailed information from the Supplementary Figure S3 in the manuscript. Because we manually curated the dataset based on the RefSeq database, the protein sequences and their GO annotations are very reliable.	As described above, the generated test set can serve as a novel protein set compared to the training set. Specficially, the minimum normalized edit distances between the training set and the test set for most of the labels are larger than 0.7 (as shown in Supplementary Figure S4 in the manuscript).	There are three sub-tasks for our tool, namely the GO term classifications on the MF, BP, and CC categories, respectively. The detailed training/validation/test ratios for the three sub-tasks are 99,806/56,491/17,369, 60,143/29,768/10,034, and 21,228/4,045/2,808. You might find that the splitting ratio is not fixed, because we curated this dataset by simulating the novel protein function prediction scenario. Specifically, for each GO category, we allocated 10% of the most recently released proteins with GO annotations as the test set. Additionally, we ensure that the novel test set significantly differs from the training set in terms of protein sequences. Therefore, among the remaining 90% annotated proteins, those lacking significant alignments (E-value>1e-3) to the test set were assigned to the training set, while others were assigned to the validation set. This splitting strategy poses significant challenges for both PlasGO. Because of the large number of the binary classifications (377), we can't plot all the distributions. However, the data type distributions are roughly consistent for the training, validation, and test sets.	4.0	0.0	Yes, you can check the dataset used for benchmark experiments via the Zenodo repository [14005015] at http://zenodo.org/records /14005015.	Because the RefSeq test set we curated is to simulate the novel protein function prediction scenario, we didn't include any alignment-based tools for benchmarking, such as Diamond and BLASTP. Additionally, we conducted the benchmarking experiments with six learning-based state-of-the-art tools, including DeepGOPlus, PFresGO, TALE, DeepSeq, TM-Vec, and CaLM. For a fair comparison, we retrained all of them using our curated RefSeq dataset (training/validation/test). The results showed that PlasGO performed the best on all three GO categories (Figure 6 in the manuscript).	No. The two evaluation metrics do not relate to confidence intervals. For more details, please refer to the CAFA challanges.	"We used the two commonly used metrics in the CAFA challenge, namely the protein-centric Fmax and the term-centric AUPR. For more details of the two metrics, please refer to the ""Experimental setup"" section in the manuscript."	We conducted a series of rigorous experiments to evaluate the performance of PlasGO. All of them can be checked in the manuscript's Results part. Specifically, they include 1) the performance on the novel RefSeq test set, 2) two ablation studies for validating PlasGO‚Äôs design rationale, 3) visualization of the PlasGO embeddings for interpreting the BERT module, 4) identification of elusive GO term labels to improve the precision of PlasGO, 5) labels of different frequencies and confidence scores which show the performance stability on different labels and the effectiveness of the learned confidence scores, 6) application: automatic GO prediction for unannotated plasmid-encoded proteins in RefSeq, which pre-annotates high-confidence GO terms for all available plasmid proteins (678,197), 7) case study: annotations for two well-studied conjugative plasmids associated with AMR.	5.0	0.0	Yes. You can refer to the GitHub repository https://github.com/Orin-beep/PlasGO for most recent updates. Licensed under the MIT license.	The running speed of PlasGO is very fast. For the 365 plasmid proteins in the example data, it only took 1m48s for the GO term prediction on one NVIDIA GeForce RTX 2080 Ti. 	The model is interpretable. We mainly interpreted the global BERT module of PlasGO by comparing the raw embeddings generated from the ProtTrans model and the contextualized embeddings from the global BERT module. You can check the results from Figure 7 in the manuscript. The results showed thats suggests that PlasGO effectively captures the latent features associated with plasmid-specific functions.	Classification.	4.0	0.0	We employed deep learning in our tool. PlasGO consists of three sub-modules, a pre-trained protein language model, a global BERT model, and a classifier module incorporating a self-attention confidence weighting mechanism. The core part of PlasGO is the BERT model to learn the global context within plasmid sentences. We used the BERT model because we formuated plasmids as a language defined on the protein token set, and BERT is one of the state-of-the-art language models.	Yes, you can check PlasGO's default models (models.tar.gz) via the Zenodo repository [14005015] at http://zenodo.org/records/14005015, which is licensed under the CC0 license.	We employed the powerful foundation protein languange model, ProtTrans, to encode our input protein sequences. It can generate biologically meaningful embedding for each plasmid-encoded protein. Then, the raw input embeddings can greatly improve the performance of GO term prediction through transfer learning. 	"As described in V.3, the only input is the plasmid protein amino acid (AA) sequences. Then, ProtTrans is employed to generate the raw embeddings for each protein (preprocessing). By capturing the
semantic meaning of individual AA tokens and their contextual relationships within the protein sequence, ProtTrans will generate per-protein embeddings (1024 dimensions) as the input to the subsequent BERT module."	We have 89,835 plasmid sentences in total for training the simplified global BERT model with token classification task. As shown above, the simplest BERT model (CC) have hyperparameter of hidden_size=512, head_num=8, and layer_num=2. The corpus size (89,835) is sufficient to fit the BERT module. In addition, there are several overfitting strategies in the PlasGO models, which will be listed in V.7.	No. There is no data from other ML algorithms as input to PlasGO.	The parameter number is 13340504 for MF model, 13698908 for BP model, and 6891070 for CC model. Actually, the three models are all simple BERT models with hidden_size = 512, head_num = 8. Besides, we used 4 Transformer layers for MF and BP, and 2 Transformer layers for CC, considering the relatively smaller dataset size and label size for CC.	Yes. 1) Dropout layers adopted on both the classifier FC layer and also the attention blocks in the BERT model. 2) Validation set is employed to prevent overfitting. 3) Layer Normalization is employed for each Transformer block and also the token embedding layer. 4) A rank regularization method is utilized in the classifier module to promote the ability of the model to distinguish low-confidence and high-confidence predictions. For more details, please check the Supplementary Section S1.  	8.0	0.0	668fc3487089c469b454be8b	39704702.0	PMC11659980	02/02/2026 19:46:52	Ji Y, Shang J, Guan J, Zou W, Liao H, Tang X, Sun Y.	GigaScience	PlasGO: enhancing GO-based function prediction for plasmid-encoded proteins based on genetic structure.	10.1093/gigascience/giae104	2024	0.0	0.0		1.0	2024-11-12T11:04:18.047Z	2026-02-02T19:46:52.000Z	07fc5bdf-9ef7-423d-9bf5-acb545419ac3	undefined	pugj4ycv1x				DOME_JSON	Match	Match
6735577ca96b586fdb272404	Our data and scripts can be downloaded from the following link: https://github.com/LPH-BIG/scGraph2Vec and https://doi.org/10.5281/zenodo.12092871.	"scGraph2Vec is a deep-learning framework for generating gene embeddings that are highly informative by incorporating single-cell data patterns of neighbor genes and communities. The datasets used for training are the gene-by-cell matrix from public available scRNA-seq data and the interaction network from the BioGRID database (release v.4.4.210, https://downloads.thebiogrid.org/File/BioGRID/Release-Archive/BIOGRID-4.4.210/BIOGRID-ALL-4.4.210.mitab.zip). The gene-by-cell matrix contained an average of 66,395 cells (range 2,638 to 287,269) and 24,064 genes (range 13,714 to 33,694). The interaction network contained 977,356 interactions among 19,752 genes. The scRNA-seq datasets were previously published and can be freely available. Their PubMed IDs and data links are as follows: 
(1) Brain: PMID 29227469, https://db.cngb.org/HCL/;
(2) Heart: PMID 32403949, https://singlecell.broadinstitute.org/single_cell/study/SCP498/transcriptional-and-cellular-diversity-of-the-human-heart;
(3) Kidney: PMID 31604275, https://www.kidneycellatlas.org/;
(4) Liver: PMID 30348985, https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE115469;
(5) Lung: PMID 33208946, https://hlca.ds.czbiohub.org/;
(6) PBMC: 10X genomics, https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.1.0/pbmc3k."	We referred to the classic data division method when training the VGAE model (https://arxiv.org/pdf/1611.07308v1). The dataset was split according to the positive and negative edges of the interaction network, and the training set and test set are independent. The positive edges (i.e., the element values of the matrix being 1) of the initial adjacency matrix were split by 90%, 5%, and 5% respectively and the same number of negative edges (i.e., the element values of the matrix being 0) were randomly selected to form the training set, validation set, and test set, respectively.	The gene-by-cell matrix contained an average of 66,395 cells (range 2,638 to 287,269) and 24,064 genes (range 13,714 to 33,694). The interaction network contained 977,356 interactions among 19,752 genes. The gene-by-cell matrix were adjusted to be consistent with the nodes of the interaction network. We deleted the genes that were not annotated in the interaction network and used 0 to fill in the node features that lacked feature values. The positive edges (i.e., the element values of the matrix being 1) of the initial adjacency matrix were split by 90%, 5%, and 5% respectively and the same number of negative edges (i.e., the element values of the matrix being 0) were randomly selected to form the training set, validation set, and test set, respectively.	4.0	0.0	Scripts that were used to perform the evaluation are available (https://github.com/LPH-BIG/scGraph2Vec).	Yes, we adopt a unified benchmark framework to compare scGraph2Vec with other gene embedding calculation methods, including scVI, LDVAE, siVAE, scVAE, scGNN, scapGNN, scETM, SAUCIE, and SIMBA.	scGraph2Vec had the highest silhouette coefficients, implying the best module partitioning.	For link prediction, we use AUC and AP indicators for evaluation. For community detection, we use silhouette coefficient for evaluation.	Cross-validation. Additionlly, the models were tested on data from difference tissues. 	5.0	0.0	Yes, the source code is publicly available on GitHub (https://github.com/LPH-BIG/scGraph2Vec).	Using a 64-core CPU in parallel, the total computation time was about 50 minutes, the peak memory usage was 64 gigabytes.	We demonstrate that gene embedding is biologically interpretable and can reveal functional gene modules representing general or tissue-specific cellular processes. For example, genes within modules are topologically tightly connected and tend to cluster housekeeping genes and cell type-specific genes.	The model is used for unsupervised learning in tasks of link prediction and community detection.	4.0	0.0	We chose Variational Graph Autoencoder (VGAE) because it can well integrate graph and node features. Its efficiency and accuracy have been demonstrated in existing single-cell embedding methods.	Yes, this information can be found in the Github (https://github.com/LPH-BIG/scGraph2Vec).	For each scRNA-seq dataset, we filtered for genes with a non-zero expression value in at least 3 cells and for cells with at least 200 expressed genes. The originally downloaded count data were transformed to log (counts per million + 1) values and scaled by all cells. 	On average, each scRNA-seq datasets contained 66,395 cells/features (ranging from 2,638 to 287,269) and 24,064 genes/nodes (ranging from 13,714 to 33,694). We used all features after quality control of single-cell data.	Our features are sufficient to train p, therefore over and underfitting should not be an issue.	No, this question is not applicable to our work.	The model has four hyperparameters specific to scGraph2Vec and some general parameters associated with GAE/VGAE. We carried out a hyperparameter sweep to examine the key hyperparameters of scGraph2Vec and the best hyperparameters were determined by using a combination of assessment parameters, including the silhouette coefficient, the number of clusters, and the running time. 	We chose to add add early stopping and dropouts to prevent overfitting.	8.0	0.0	6735577cdd85bd59b3a35cbb	39704704.0	PMC11659981	02/02/2026 19:46:52	Lin S, Jia P.	GigaScience	scGraph2Vec: a deep generative model for gene embedding augmented by graph neural network and single-cell omics data.	10.1093/gigascience/giae108	2024	0.0	0.0		1.0	2024-11-14T01:50:52.614Z	2026-02-02T19:46:52.000Z	298f41cd-8564-43b1-a79c-b58f5e6568b6	undefined	3c9pv95v5g				DOME_JSON	Match	Match
6736ef894040aef5ee81ef98	"released as:
Proteomics: http://central.proteomexchange.org/cgi/GetDataset?ID=PXD043300
RNAseq (mouse): https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE234245
snRNAseq (mouse): https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE234243
(sn)RNAseq (human): https://ega-archive.org/studies/EGAS00001007318
OpenModificationSearch:  https://figshare.com/articles/dataset/dx_doi_org_10_6084_m9_figshare_6025748/6025748
"	"direct experiment
- 51 sporadic ALS patients and 50 control subjects
- four transgenic mouse models of C9orf72-, SOD1-, TDP-43-, and FUS-ALS"	n.a.	n.a.	4.0	0.0	n.a.	no, since the method was not developed by us. 	n.a.	n.a.	The expression of differentially expressed proteins was compared to the expression of the proteins without imputation.	5.0	0.0	yes, https://github.com/imsb-uke/MAXOMOD_Pipeline	n.a.	The model is interpretable, by checking the branching rules, but interpretability was not used.	Imputation	4.0	0.0	Random Forest using missForest R package. It is not a new algorithm and often used for this task.	default parameters were used and the source code is provided.	low-abundance proteins detected in less than 50% of samples were filtered out	All features, depending on the dataset were used as input. No further feature selection was performed.	out-of-bag (OOB) imputation error estimate is used	no	maxiter = 10, ntree = 100, which were the default parameters.	no	8.0	0.0	6736ef89dd85bd59b3ab1779	39693632.0	PMC11653894	02/02/2026 19:46:52	Hausmann F, Caldi Gomes L, H√§nzelmann S, Khatri R, Oller S, Gebelin M, Parvaz M, Tzeplaeff L, Pasetto L, Zhou Q, Zelina P, Edbauer D, Pasterkamp RJ, Rehrauer H, Schlapbach R, Carapito C, Bonetto V, Bonn S, Lingor P.	GigaScience	A dataset profiling the multiomic landscape of the prefrontal cortex in amyotrophic lateral sclerosis.	10.1093/gigascience/giae100	2024	0.0	0.0		1.0	2024-11-15T06:51:53.814Z	2026-02-02T19:46:52.000Z	ec52c8bd-a5de-40f6-9733-86b6b1bca22e	undefined	po7edt8oqz				DOME_JSON	Match	Match
67504d6469b4c4f264235694	Data is re-splited from other publication and also available via GitHub link.	The dataset is collected from the previous publication DeepFRI (https://github.com/flatironinstitute/DeepFRI) and HEAL (https://github.com/ZhonghuiGu/HEAL), and then we split them according to our methodology.	Both datasets meet the criterion that the sequence identity between samples from different sets is below 30%.	"1) PDBch training set: 29304, validation set: 3660, test set 3665;
2) AFch training set: 34135, validation set : 3881, test set 3981."	4.0	0.0	might be part of the GitHub repo.	We compared it with several baselines.	compare performance between methods.	AUPR and Fmax.	Independent dataset and several experiments.	5.0	0.0	Yes via GitHub.	About two hours for traning.	Hard to interpretable.	Multi-label classification.	4.0	0.0	GCN and Graph Transformer were employed for representation learning, while adversarial learning was utilized for domain alignment.	No	The paper proposed a novel model based on Graph Transformer to acquire graph-level embeddings that capture spatial semantics.	The protein features including protein sequence, distance map between residues, residue embeddings extracted from protein language model are used as input.	The number of parameters is reasonable.	No, the model used raw data as input.	About 8 million1 parameters are used.	 Yes, we implemented early stopping using a validation set. If the validation loss increased consistently for several epochs, training would be halted.	7.0	1.0	661e294a92c76639b8a6ac73	39657158.0	PMC11734293	02/02/2026 19:46:52	Fu Y, Gu Z, Luo X, Guo Q, Lai L, Deng M.	GigaScience	Learning a generalized graph transformer for protein function prediction in dissimilar sequences.	10.1093/gigascience/giae093	2024	0.0	0.0		1.0	2024-12-04T12:39:00.931Z	2026-02-02T19:46:52.000Z	5d9e7df0-7107-499a-ae55-79c125fde132	undefined	ccm469wdz5				DOME_JSON	Match	Match
676008c869b4c4f264235cc6	The data are released in the https://github.com/SkywalkerLuke 	The data sources include multiple databases, specifically IEDB, CEDAR, VDJdb, ImmunoCode, dbpepneo2.0, and NEPdb. These datasets can be downloaded from Google Drive and have been submitted along with the source code of TransHLA, including both training and testing data.	Independent	The ratio of the training set, validation set, and test set is 7:1:2. For HLA-II epitopes, there are 312,245 positive samples and 312,245 negative samples. For HLA-I epitopes, there are 459,442 positive samples and 459,442 negative samples. The document mentions that an independent validation set was indeed used, but it does not provide the specific number of samples in the validation set. To ensure the rationality of data distribution, the researchers examined the frequency distribution of HLA alleles that bind to peptides in the IEDB database. The results show that the frequency distribution of HLA alleles in the training set, validation set, and test set exhibits highly similar characteristics (see Figure 2 and Supplementary Figure 5). Therefore, it can be concluded that the distribution of data types in the training set and test set is similar. The document notes that the distribution of data types for the training set, validation set, and test set has all been visualized. Specifically, Figure 2 shows the distribution of major HLA-I alleles across different datasets, while Supplementary Figure 5 displays the distribution of the top 50 alleles for HLA-DR, HLA-DQ, and HLA-DP.	4.0	0.0	No	Yes, a comparison was performed against current SOTA software on benchmark datasets, as well as against simpler baselines. Details of these comparisons can be found in Table 3 and Table 4 of the paper.	Our model significantly improves performance, particularly in specificity, and this improvement is statistically significant.	The performance metrics reported include AUC-ROC, AUC-PRC, accuracy, F1-score, Matthews correlation coefficient, recall, precision, and specificity. These metrics provide a comprehensive and representative analysis, which is consistent with standards in the literature.	The method was evaluated using an independent dataset by splitting the data into training, validation, and test sets in a 7:1:2 ratio. This ensures that the evaluation is based on data that the model has not seen during training.	4.0	1.0	https://github.com/SkywalkerLuke/TransHLA, MIT license	Using an NVIDIA 3080 GPU and a batch size of 128, it takes approximately 1200 seconds to output around 180,000 predictions.	Black Box	classification	4.0	0.0	TransHLA employs a deep learning machine learning algorithm, specifically combining the Transformer architecture with Residue Convolutional Neural Networks (Residue CNN) to predict epitopes presented by HLA molecules. The novelty of this algorithm lies in its utilization of cutting-edge deep learning techniques, such as Transformers and ESM2 embeddings, to enhance prediction accuracy and efficiency. This new approach was chosen over more traditional models due to its higher specificity and sensitivity in identifying immunogenic epitopes and novel epitopes. Moreover, by merging all epitopes into a single positive sample class and using negative samples in a 1:1 ratio during training, this method learns unified epitope features, enabling better differentiation between epitopes and regular peptide sequences. This not only improves prediction accuracy but also significantly reduces the need for chemical experiments, lowering costs, and demonstrating greater robustness and reliability in antigen selection tasks across diverse populations.	Yes, the hyperparameter configurations, optimization schedule, model files, and optimization parameters are reported. You can find this information at the following URL: https://github.com/SkywalkerLuke/TransHLA. The repository includes details about the licensing as well.	The input data consists of sequences. The sequences are encoded using the ESM2 tokenizer, which converts characters into numerical representations. Then, these numerical representations are passed through ESM2 to generate embeddings. This process leverages the advanced modeling capabilities of ESM2 to create high-dimensional representations of the peptide sequences, which are essential for effective machine learning model training.	We use 2 pretrained embedding features (sequence embedding, structure) as input. We do not use feature selection, and we use the deep learning framework for abstract feature extraction.	The number of parameters is quite large, categorizing it as a large model. However, since a portion of these parameters comes from the pre-trained model, which has effectively extracted features in advance, this helps mitigate both overfitting and underfitting. During downstream training, we employed an early stopping mechanism and established conditions for when early stopping should commence, allowing us to save the best-performing model. This approach helps ensure that the model does not overfit to the training data while still maintaining sufficient capacity to learn effectively.	No, the model does not use data from other machine learning algorithms as input. The training data for the initial predictors and the meta-predictor are not clearly stated to be independent of the test data for the meta-predictor. Therefore, it cannot be confirmed that they are independent.	The model uses approximately 700 million parameters. Of these, 650 million parameters are derived from the ESM2 pre-trained model. The remaining parameters are determined based on the size of the ESM2 embeddings and the number of layers in the model.	Yes, overfitting prevention techniques were used. Specifically, we employed a validation set along with an early stopping mechanism. This approach helps monitor the model's performance and stops training when improvements on the validation set cease, effectively preventing overfitting.	8.0	0.0	676005bbdd85bd59b37ec404	40036690.0	PMC11878767	02/02/2026 19:46:52	Lu T, Wang X, Nie W, Huo M, Li S.	GigaScience	TransHLA: a Hybrid Transformer model for HLA-presented epitope detection.	10.1093/gigascience/giaf008	2024	0.0	0.0		1.0	2024-12-16T11:02:32.002Z	2026-02-02T19:46:52.000Z	a5882fb4-b99f-42de-bcf3-a479ebdff246	undefined	peuywb6nkx				DOME_JSON	Match	Match
6760e79869b4c4f264235d59	Yes, the data and the splits are part of the gigascience database.	The data was taken from the National Center for Biotechnology Information (NCBI) and the Integrated Microbial Genomes & Microbiomes - Viruses (IMGVR) databases. The corresponding GigaScience database contains the original data, including accession IDs	To ensure the independence of the training and test sets, we first delete duplicates in the dataset. Then, we use BLAST to remove strongly homologous sequences and create different test subsets to evaluate our models	The exact training/test splits are recorded and are part of the GigaScience database. Originally, the test data comprised 20% of the total data (565,760 original sequences) and was later downsampled for different homology conditions. To ensure a fair comparison, we created multiple test sets using different sequence homologies and report the performance on each one of them. Additionally, we use a nested 5-fold cross-validation (CV) to properly assess the quality of our optimization in the HVSeeker-protein setting (98,720 phage sequences and 122,366 bacteria sequences). Hence, during optimization, HVSeeker-protein uses another 20% of the data for validation. The different data splits used for optimization are also part of the GigaScience database.	4.0	0.0	All evaluation files, including previous test-splits and models used to achieve the performance are supplied as part of the gigascience dataset. 	We compare our method against other SOTA methods (Seeker, Rnn-VirSeeker, DeepVirFinder, PPR-Meta ) and an additional HMM baseline. The scripts used for this evaluation are part of the gigascience database. 	HVSeeker is shown to outperform other methods on various comprehensive datasets and on different performance metrics. 	We use precision, recall, accuracy and F1-score, ROC-AUC  to fairly compare different methods. 	For optimization runs we used a nested 5-fold CV, ensuring fair assessment of our results due to independent test datasets. When testing HVSeeker-DNA we additionally used subsets of our original independent test-set of different homology conditions to challenge our models. In the last step we test HVSeeker against a novel dataset based on the infant gut metagenome, showing it's utility in a practical setting.	5.0	0.0	The source code is released on github and can be found in the gigascience database aswell (MIT license included). To make it easier to run the scripts, we also supply a conda environment with instructions for easy installation. 	Executing a simple forward pass with one sequence only takes around 0.0366 seconds on a standard machine (cpu only). Prediction time is therfore neglegtible. 	As it is the case with most deep-learning methods, interpretaing the classification results is unfortunately not directly possible. 	The model performs classification. 	4.0	0.0	HVSeeker is based on established machine learning (ML) algorithms, such as LSTM and Transformer, which are known to work well with sequences and have been adapted to our specific problem	The model files, optimization schedule and paramter and hyperparameter configurations are reported and can be found in the gigascience database. 	HVSeeker relies on one-hot encoding but merges data in different ways, such as padding (cycling through the sequence until it reaches the required length), contig assembly (combining multiple shorter sequences to generate a longer sequence, then splitting it into subsequences of 1000 bp), and sliding window (using a window of 1000 bp to slide over the input DNA sequence in 100 bp steps)	Both versions of HVSeeker rely only on the sequence, the input features are therfore the nucleotides/amino acids of the input sequence. No further feature selection was performed.	We evaluate our models during training using a validation set to achieve an optimal fit of the paramters.	HVSeeker-Protein uses input from a pre-trained version of ProtBert. Since ProtBert was previously trained on a masked language task, it had not encountered a classification task prior to this study.	HVSeeker contains 1026152 trainable parameter. The number of parameter is a result of adapting the model to the training process. 	We use early stopping using a validation set and dropout to prevent overfitting to the training data. You can find details in the provided scripts. 	8.0	0.0	6760e76ddd85bd59b3841ff8	40372723.0	PMC12080225	02/02/2026 19:46:52	Al-Najim A, Hauns S, Tran VD, Backofen R, Alkhnbashi OS.	GigaScience	HVSeeker: a deep-learning-based method for identification of host and viral DNA sequences.	10.1093/gigascience/giaf037	2025	0.0	0.0		1.0	2024-12-17T02:53:12.396Z	2026-02-02T19:46:52.000Z	40ca4b51-f205-4b23-93ed-a95042d470e7	undefined	igr5x3a1vs				DOME_JSON	Match	Match
6761352369b4c4f264235d9d	"The annotated data are freely available in the open repository under CC0 licence:
https://zenodo.org/records/8240994

The full collection of the images used to verify the pipeline, including raw images and metadata, are available at the following links.:
AT023: 10.5281/zenodo.13853394
AT024: 10.5281/zenodo.13856248
AT025: 10.5281/zenodo.13856317
"	"This dataset contains annotated images of mature inflorescences, collected from experiments conducted on the Multiparent Advanced Generation Inter-Cross (MAGIC) population for Arabidopsis. These 2D RGB images are in png format, consist of branches of stems with fruits attached. The designated image-based tasks are for quantitative trait extraction for Arabidopsis fruits, and has been used to develop Cascade Mask R-CNN for instance segmentation. The annotations for an image consist of segmentation masks for individual instance of siliques and have been store in COCO format in json files, with only one class of object of interest, i.e. siliques. The number of silique object could vary from none to hundreds. 

This annotation dataset was curated for our AI pipeline for Arabidopsis fruit phenotyping, and has not be used in other work yet. "	The datasets were split randomly form two independent datasets: a training set and a test set without any shared examples between the two. 	"The annotated image dataset has been randomly split into two sets: training (44 images, with 2,288 annotated siliques) and test (11 images, with 652 silique segmentations).
The distributions of the training and test set are similar as they are from the same experiment and imaged under the same condition by a flatbed scanner. 

For the verification of the pipeline using QTL analysis, the full dataset collection (in total of over 7000 images) has been utilised. 
"	4.0	0.0	All direct segmentation output can be reproduced using the docker-containerised pipeline with open source code freely available under the license of GPL 3.0. The perfermance is also reported in the associated paper, freely available under CC0 license. All the additional evaluation of the model outputs via QTL analysis, including identified loci based on deep learning phenotypic data and known genes (from bioinformatic database and literature), are available on the Github Repository https://github.com/kieranatkins/silique-detector under Open Source Licenses. 	No direct comparison on instance segmentation model is available at the moment, as this is a newly curated dataset for our AI pipeline development. Simpler alternative traits can be extracted using other easier to extract traits, by e.g. semantic segmentation algorithms, however it would not be a precise measurement on the silique morphology, as we hope for this work. 	Experiments for comparison to alternative methods have not been available. However, this development aims to demonstrate the potential of using AI based pipeline to for large scale fruit phenotyping, and we have done so by verifying the model-derived phenotypic outputs through QTL analysis. 	"We evaluated the trained model on an independent test set of 11 annotated images containing 652 siliques, using standard performance metrics for object detection and segmentation. These include Average Precision and Average Recall (with an IoU threshold ranging from 0.5 to 0.95 in increments of 0.05), as well as Precision at IoU thresholds of 0.5 and 0.75.

For independent verification of the instance silique segmentation model, the entire image collection (over 7000 images) was examined through qualitative inspection and QTL analysis to identify links between variability in model-derived fruit morphology phenotypes and the genome. Significant loci were then cross-referenced with known genes and relevant literature."	Original model output were first validated using independent test dataset (of 11 images), followed by applying the model on a much larger dataset of over 7000 images with qualitative visual checking and further QTL analysis on the model derived phenotypic data. 	5.0	0.0	The entire pipeline, docker containerised, including pre-trained models, model inferences, phenotypic data extraction and evaluation, is available at https://github.com/kieranatkins/silique-detector/ under GPL 3.0 license. Instructions have been given to deploy the pipeline with or without GPU, including the use of images from docker-hub. 	Only a few seconds or less are required to inference on the model to generate the segmentation predictions using a single desktop PC with CPU only, it is recommended to run on machine with larger RAM (e.g. 32 GB or above) is recommended. 	This model is a blackbox neural network model, however it's outputs (detected silique instances and corresponding masks) can be plotted overlaying the original images for visual inspection. Examples are given on the associated paper, and also users can easily run through the docker containerised pipeline and generate images with model prediction examples. 	Direct model outputs for the instance segmentation problem include detected bounding boxes and segmentation masks for siliques and their associated confidence scores.	4.0	0.0	We chose Cascade Mask R-CNN for our instance segmentation tasks due to its multi-stage approach, which progressively refines detection, offering higher accuracy and improved handling of scale and occlusion compared to single-stage methods like YOLACT, which prioritise real-time processing over precision. While two-stage methods, including Mask R-CNN, provide a strong balance between detection precision and efficiency, additional integration of attention mechanisms, such as transformer architectures, can further enhance spatially aware segmentation but often require more computational resources and training data. Cascade Mask R-CNN‚Äôs ability to refine detections iteratively makes it particularly suitable for the challenges in agricultural imagery, such as dense scenes and varied scales.	All hyperparameter configurations are available on GitHub respository: https://github.com/kieranatkins/silique-detector/  under GPL-3.0 license	The input images are resized by a factor of 0.9, reducing their dimensions from 5100x3600 to 4590x3240, to better fit within the GPU 's RAM (NVIDIA A100 48 GB) during model training.	We opted for end-to-end deep learning model architecture where features are automatically learned from the data. 	We initialise the weights without any parameter freezing to allow the model to adapt to the largely different domain of our dataset.   The model was trained for 36 epochs, most of the settings are set as default values, using stochastic gradient descent with a learning rate of 0.01. These hyper-parameters were chosen based on the internal validation (randomly split from the training set) performance, i.e. to allow the model to converge well without overfitting. We included an initial linear learning rate warm-up for the first 100 iterations, starting at 0.00001, and ending at the learning rate of 0.01. This was chosen to stop the sharp initial updates from the initial COCO weights, which can lead to instability \citep{glimer21}. The learning rate was set to decay by a factor of 0.1 at epochs 24 and 33 to allow for fine-tuning later in the training. Training took place across 4 GPUs. Batch size was set to 1 image for each GPU. 	No	Underlying Cascade Mask R-CNN, we used a RegNetX 1.6GF backbone \cite{radosavovic20} with a feature pyramid network. We initialised the network with weights pre-trained on COCO dataset. 	Stochastic gradient descent also included a momentum value of 0.9 and a weight decay value of 0.0001. 	7.0	1.0	6724ee06dd85bd59b34c490d	39937596.0	PMC11816797	02/02/2026 19:46:52	Atkins K, Garz√≥n-Mart√≠nez GA, Lloyd A, Doonan JH, Lu C.	GigaScience	Unlocking the power of AI for phenotyping fruit morphology in Arabidopsis.	10.1093/gigascience/giae123	2024	0.0	0.0		1.0	2024-12-17T08:24:03.566Z	2026-02-02T19:46:52.000Z	551c6108-80ff-4445-bc1b-81db6dfb2396	undefined	a8q3rb7qrv				DOME_JSON	Match	Match
6765dbb869b4c4f264235ef1	Data is provided to reviewer of journal and will be released publicly after successful publication.	previous Publication + own annotation (segmentation)	Training and test sets independent (different wells, i.e. different spheroids)	"883 images for training 
108 images for validation
104 images for testing"	4.0	0.0	see publication and repo	"Comparison to four recent deep-learning segmentations of tumor spheroid images (SpheroScan(Incucyte+Microscope), AnaSP, SpheroidJ)
Comparison to Otsu Tresholding"	Yes. Yes.	"JCD = 1-IoU, Relative diameter deviation, relative circularity deviation, Ambiguous Spheroid Fraction, Invalid Spheroid Fraction
Set is representative for radiated tumor spheroids of head and neck cancer."	independent dataset (hold-out test dataset) + novel experiments	5.0	0.0	https://igit.informatik.htw-dresden.de/aagef650/spheroidsegdedeb	it takes about 1.8 seconds to segment one image on the CPU (Intel(R) Core(TM) i7-4770). When the segmentation is performed serially on the GPU (NVIDIA GeForce RTX 3080) the U-Net needs only 0.03 seconds per image and the HRNet 0.08 seconds.	Black box	point-wise classification (segmentation)	4.0	0.0	"Fully convolutional neuronal networks, U-Net and HRNet
no new ML algorithm"	Yes. See publication and Data Availability statement in preprint https://doi.org/10.48550/arXiv.2405.01105	Images are converted from 16-bi to 8-bit images to ensure compatibility with the employed libraries FastAi and SemTorch, the effect on depth is negligible as only a small fraction of the 16-bit range is utilized during imaging.	No extra feature selection.	Validation during training was used (Train-validation-test-split technique) to avoid overfitting as well as data augmentation (vertical flip, horizontal flip, or rotation by 180 degree	"No data from other ML algorithms used as input.
For transfer learning ImageNet data set is used J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, Imagenet: A large-scale hierarchical image database, in: 2009 IEEE conference on computer vision and pattern recognition, Ieee, 2009, pp. 248‚Äì255.
"	Used architecture (resnet-34) has 21797672 weights, 63.5 million parameters. Hyperparameters were optimized.	validation-based early stopping with the Jaccard coefficient as underlying metric	8.0	0.0	6765dbb8dd85bd59b399b946	40331344.0	PMC12056507	02/02/2026 19:46:52	Streller M, Michl√≠kov√° S, Ciecior W, L√∂nnecke K, Kunz-Schughart LA, Lange S, Voss-B√∂hme A.	GigaScience	Image segmentation of treated and untreated tumor spheroids by fully convolutional networks.	10.1093/gigascience/giaf027	2025	0.0	0.0		1.0	2024-12-20T21:03:52.252Z	2026-02-02T19:46:52.000Z	193c93af-4b27-43b5-9d66-f8a672e5aa5e	undefined	0g2i3j2hzg				DOME_JSON	Match	Match
678141b269b4c4f2642361f7	"Access to public data:  Human Connectome Project - Young Adult Study
Public data website: https://www.humanconnectome.org/study/hcp-young-adult
Register for access: https://db.humanconnectome.org/app/template/Login.vm (simple sign-on)
Access to restricted data:
https://www.humanconnectome.org/study/hcp-young-adult/document/restricted-data-usage includes link to e-access application form.
Data : WU-Minn HCP 1200 Subjects Data Release, for which the following reference manual applies: https://www.humanconnectome.org/storage/app/media/documentation/s1200/HCP_S1200_Release_Reference_Manual.pdf"	The Human Connectome Project's Young Adult dataset was used for functional MRI data. This dataset is widely used and well established in MRI analysis.	Our dataset was split fold-wise into test and train sets randomly. Since the dataset contains some data from siblings, we took care not to split siblings into corresponding train/test pairs.	We used internal cross-validation (k-fold validation) with a k of 128. We also used a large number of subjects varied between 999 and 1077 according to the respective analysis. Distribution of data types did thus not differ between train and test sets.	4.0	0.0	Does not apply.	Does not apply.	We used permutation-based analysis to confirm the statistical validity of the results. No comparable models predicting a similar measure with similar features exists.	Pearson's r of predicted vs observed BMI is reported. This dataset contains data from young and predominantly white adults, a table with basic demographic information is provided in the paper.	Internal cross-validation was used.	5.0	0.0	Source code is released under an open-source (GPLv3) license and is available in a git repository: https://codeberg.org/~tobac/hcp-suite	The prediction process itself takes a few seconds, model building and prediction using 128 CPUs on a computing cluster takes 5 to 10 minutes.	It is interpretable as the edges used to predict BMI are documented and, in fact, interpreted in the paper.	This is a regression model.	4.0	0.0	"The ML algorithm used is a form of linear regression. Connectome-based predictive modeling (CPM) was used as described in [1]. CPM uses simple correlation described by Pearson's r between features (edges of a connectome) and a to-be-predicted measure (in this case, BMI) in the train set. From negative and positive correlations between features and the measure a general linear model is constructed, which is then used to predict the test set's measure.

[1] Shen X, Finn ES, Scheinost D, et al. Using connectome-based predictive modeling to predict individual behavior from brain connectivity. Nat Protoc. 2017;12(3):506-518. doi:10.1038/nprot.2016.178
"	Python code with which the analysis was performed is available under a GPLv3 license and can be accessed through a git respository at https://codeberg.org/tobac/hcp-suite As part of the published paper, a Jupyter notebook is provided with intermediate files (i.e. connectivity matrices)	A preprocessed version of the Human Connectome Project's (HCP) Young Adults dataset was used. While a comprehensive and detailed description has been published elsewhere [1], a concise description of the steps involved is as follows: In a first step, correction for distortions related to gradient nonlinearity (which is more pronounced in the HCP‚Äôs scanner setup) was applied with a FreeSurfer software package. The FSL software‚Äôs FLIRT method was then used to correct for head motion. Grand-mean intensity normalization was performed on the fMRI time series. One of the keystones of fMRI studies is reliable intersubject comparability, which requires translating a subject‚Äôs physical space into a common standard space (‚Äùregistering‚Äù). The HCP addressed this fundamental issue via multimodal registering, i.e. using a variety of imaging modalities to reliably and automati- cally identify anatomical or functional landmarks in each subject‚Äôs 3D data and align them accordingly in what they call grayordi- nate space, a derivate of Montreal Neurological Institute (MNI) space, in which only matter of interest (i.e. gray matter) is preserved. Building on work described in [2], Glasser et al. [3] developed a multi-modal and mapped areal-feature-based (dubbed ‚ÄùMSMAll‚Äù) registration method, which uses myelin maps, resting-state brain networks, visuotopic maps, and a subcortical region of interest for inter-subject alignment (see the supplemental methods of [3] for implementation details).  The analysis is a parcellated analysis. For the cerebral cortex, HCP-MMP1.0 (Human Connectome Project Multi- Modal Parcellation version 1.0, [3]) by Glasser et al. was used. They delineated 180 parcels per hemisphere (360 in total) by using the overlap of four areal feature maps, one for each modality (cortical thickness, relative myelin content, tfMRI, rsfMRI). Subcortical parcels were provided by Tian et al. [4]. They relied on subcortical-to-cortical connectivity derived from the HCP‚Äôs rsfMRI data to delineate 27 subcortical parcels per hemisphere (54 in total) along connectivity gradients, i.e. sufficiently stark changes in func tional connectivity. Finally, cerebellar parcels came from a HCP-based study which clustered neighboring cerebellar voxels into 100 parcels by means of similarity of their rsfMRI time-series [5].  [1] Glasser MF, Sotiropoulos SN, Wilson JA, Coalson TS, Fis- chl B, Andersson JL, et al. The Minimal Preprocessing Pipelines for the Human Connectome Project. NeuroImage 2013 Oct;80:105‚Äì124 [2] Robinson EC, Jbabdi S, Glasser MF, Andersson J, Burgess GC, Harms MP, et al. MSM: A New Flexible Framework for Multimodal Surface Matching. NeuroImage 2014 Oct;100:414‚Äì426. [3] Glasser, Coalson TS, Robinson EC, Hacker CD, Harwell J, Yacoub E, et al. A Multi-Modal Parcellation of Human Cerebral Cortex. Nature 2016 Aug;536(7615):171‚Äì178. [4] Tian Y, Margulies DS, Breakspear M, Zalesky A. Topographic Organization of the Human Subcortex Unveiled with Functional Connectivity Gradients. Nature Neuroscience 2020 Nov;23(11):1421‚Äì1432. [5] Ren Y, Guo L, Guo CC. A Connectivity-Based Parcellation Im- proved Functional Representation of the Human Cerebellum. Scientific Reports 2019 Dec;9(1):9115.	Features in CPM correspond to the edges of the connectome used to predict the measure in question. Since we used a 513 x 513 connectivity matrix (of which 99 x 513 nodes contained values of interest and were used as features), the number of features was comparatively high. They were selected with p < 0.05 as a thresholding mechanism and only selected in the training set. Feature selection and prediction was strictly separated.	Internal cross-validation and a regularization technique (see below) were used to counteract overfitting.	No data from other ML algorithms was used. 	Does not readily apply.	Ledroit-Wolf [1] regularization was used. [1] Ledoit O, Wolf M. A Well-Conditioned Estimator for Large- Dimensional Covariance Matrices. Journal of Multivariate Analysis 2004 Feb;88(2):365‚Äì411.	8.0	0.0	678141b2dd85bd59b30d3f8b	40072905.0	PMC11899596	02/02/2026 19:46:52	Bachmann T, Mueller K, Kusnezow SNA, Schroeter ML, Piaggi P, Weise CM.	GigaScience	Cerebellocerebral connectivity predicts body mass index: a new open-source Python-based framework for connectome-based predictive modeling.	10.1093/gigascience/giaf010	2025	0.0	0.0		1.0	2025-01-10T15:50:10.510Z	2026-02-02T19:46:52.000Z	17faa1b9-e9c6-4278-996f-1b5f48afc444	undefined	pj6nprkh27				DOME_JSON	Match	Match
67869f6969b4c4f2642362ec	All the data is publicly available and the SRA IDs can be found in our repository.	The data is a widely used curatedMetagenomicData set. Our manually curated subset consists of 1720 healthy and 2878 unhealthy individuals.	The split was done randomly within the validation set. For the train set, a leave-one-cohort-out approach was applied and its size depended on the size of the left out cohort.	We used a leave-one-cohort-our approach to test our method. For the training and validation within each fold, a 5-fold cross validation with shuffling was used.	4.0	0.0	Yes, they can be found in our repository.	The comparison was done to best-to-date methods on well established datasets.	Yes, the score is a probability of a sample being healthy.	AUC and accuracy.	Leave-one-cohort-out approach was applied. The results were compared to other, best-to-date methods.	5.0	0.0	Yes, the complete scripts can be found in our repository.	<1min on a standard PC.	The model is a well-established random forest classifier.	Classification.	4.0	0.0	Random forest.	Only default parameters were used. Model files can be found in our repository.	The data was not encoded - the input parameters were defined based on our metagenomic results.	The input features were defined based on our metagenomic results, using only healthy data. In total, 6 input features were used.	A random forest was used to  reduce this risk. We used a minimal number of input features - only the most relevant ones. In addition, we ensured that the number of classifiers was not below 100. Finally, we applied leave-one-cohort-out validation.	No, only one model was used.	A default number of estimators was used.	Leave-one-cohort-out approach was used.	8.0	0.0	67866fb4dd85bd59b3218217	40117176.0	PMC11927397	02/02/2026 19:46:52	Zieli≈Ñska K, Udekwu KI, Rudnicki W, Frolova A, ≈Åabaj PP.	GigaScience	Healthy microbiome-moving towards functional interpretation.	10.1093/gigascience/giaf015	2025	0.0	0.0		1.0	2025-01-14T17:31:21.822Z	2026-02-02T19:46:52.000Z	bf0e30ce-692c-4127-9fc3-74309d095ec4	undefined	4cstv1dfjm				DOME_JSON	Match	Match
678e57475a9877fd97a7d570	"The data used to train and test our model is available on GigaDB, which is the repository used to host data and tools associated with articles in GigaScience Press. 
This section will be updated after publication (e.g., link to GigaDB repository). "	Whole-genome sequencing dataset of the CHM13 cell line (generated by the T2T Consortium) was processed using in-house scripts to obtain the STR flanking sequence and its associated sequencing accuracy. The data is numerical and consists of 39,594 (Ax10 STR), 7,660 (ATx10 STR), and 5,916 (ACx10 STR) data points. This data has not been used in any other paper. 	The training and test sets are independent: data points to be used as the training set were selected first, and the remaining data points were employed as the test set. 	Training-validation ratio of 9:1 was used for each group of STR (e.g., 6894:766 for ATx10 STR). Separate validation set was not used. The distribution of data types in the training and test sets are very similar and they have been randomly selected using Python Pandas Package. 	4.0	0.0	The source code is available under the GigaDB repository. The link will be provided after publication.	Unfortunately, comparisons were not performed, as they are not the scope of our study.	P-values of the obtained Pearson correlation indicated statistical significance. No other metrics were used.	Pearson correlation values were presented.	We evaluated the results through a separate analysis using an orthogonal method (Uniform Manifold Approximation and Projection analyses), which is available in our paper. We tested the findings made through the ML model in an independent dataset, by measuring the detection of a biological process. Furthermore, to ensure that the ML model was learning from the data points and not overfitted, we tested the ML model using different DNA sequences as inputs as a sanity check. 	5.0	0.0	The source code is available under the GigaDB repository. The link will be provided after publication.	Prediction is almost instant, in both a high-performing computing cluster and a M2 MacBook Air. 	The model is black box.	Regression.	4.0	0.0	A sequential neural network featuring a 1D convolutional layer with 48 filters and a kernel size of 2, followed by a flattening layer and two dense layers with 120 and 40 nodes was implemented using TensorFlow, (Mart√≠n Abadi et al., 2015). The algorithm is not new, but was chosen as our goal was to simply check if there is any association between STR flanking sequences and their associated sequencing accuracy.	No such parameters were used.	Flanking sequences (string consisting of A,T,G,C) were one-hot encoded to be used as input.	Flanking sequences that are 12 letters long were one-hot-encoded, creating a 4x12 matrix per sequence. Feature selection was not performed. 	p is indeed larger than the number of training points (74,553 vs. 39,594), but overfitting was mitigated through early stopping. 	No meta-predictors are present in our model.	74,553 parameters are used in the model. Different number of parameters (e.g., by adjusting kernel sizes, filters) were experimented, and the model that gave the best results were chosen. We note that changing the parameters did not drastically improve prediction results.	Early stopping was used (tensorflow.keras.callbacks.EarlyStopping) to prevent overfitting. 	8.0	0.0	678e5747edf7b65039c2911e	40094553.0	PMC11912559	02/02/2026 19:46:52	Park G, An H, Luo H, Park J.	GigaScience	NanoMnT: an STR analysis tool for Oxford Nanopore sequencing data driven by a comprehensive analysis of error profile in STR regions.	10.1093/gigascience/giaf013	2025	0.0	0.0		1.0	2025-01-20T14:01:43.955Z	2026-02-02T19:46:52.000Z	1d337347-11fd-47d2-a055-8025164a2ad4	undefined	hjnyjl40c9				DOME_JSON	Match	Match
678e91805a9877fd97a7d58b	"More info on dataset: https://github.com/CraigMyles/SurGen-Dataset
Download data here: https://doi.org/10.6019/S-BIAD1285"	Samples were provided via Lothian BioResource and digitised in St Andrews. The dataset includes diverse cancer types and patient demographics, detailed in the associated labels.csv files. Further information can be accessed at https://www.ebi.ac.uk/biostudies/bioimages/studies/S-BIAD1285 and https://github.com/CraigMyles/SurGen-Dataset	60:20:20 Train, Val, Test. Cases are not duplicated or leaked across splits.	60:20:20 Train, Val, Test. Stratified over various labels. Full details in paper.	4.0	0.0	Yes, within the paper.	Results were evaluated to similar results using partitions of SurGen dataset for purpose of benchmarking.	We explored the effects of thresholding on sensitivity and specificity to optimise model performance metrics. Confidence intervals can be produced for AUROC, but were not the primary focus of this study.	AUROC	Hold-out test set.	5.0	0.0	https://github.com/CraigMyles/SurGen-Dataset	Utilising a single V100, x1 sample can be evaluated and predicted upon in 0.084 seconds	Deep learning model.	Classification	4.0	0.0	Weakly supervised multiple instance learning with rich features extracted from histopathology pre-trained foundation models.	Exact commands for reproducing the experiments can be found in the reproducibility section of the GitHub repository: https://github.com/CraigMyles/SurGen-Dataset/blob/main/reproducibility/README.md.	Foundation models and/or ImageNet pretained ResNet models. Encoding is flexible.	Again this varies depending on the approach. Generally, 768-1024-dim feature vectors. The counts depend on the area of tissue within each whole slide image.  Experiments can be exactly recreated using the reproducibilty documentation within https://github.com/CraigMyles/SurGen-Dataset	Pipeline utilises pretrained models ranging from 25M params to 0.3B params. There are 1020 samples. 	Model accepts data from any feature format. Models can be loaded on the fly from hugging face or loaded from a model checkpoint.	Model parameters varies depending on hyperparameter selection.	Models trained to 200 epoch but with checkpointing at highest validation AUROC score.	8.0	0.0	678e9180edf7b65039c365f5	41128428.0	PMC12569769	02/02/2026 19:46:52	Myles C, Um IH, Marshall C, Harris-Birtill D, Harrison DJ.	GigaScience	SurGen: 1020 H&amp;E-stained whole-slide images with survival and genetic markers.	10.1093/gigascience/giaf086	2025	0.0	0.0		1.0	2025-01-20T18:10:08.752Z	2026-02-02T19:46:52.000Z	3b8bfed3-46c9-4f3e-9609-96467b80651b	undefined	vuknweu17e				DOME_JSON	Match	Match
678fa69f5a9877fd97a7d5dc	The training dataset is available on zenodo (https://zenodo.org/records/10940194) with a CC-BY 4.0 licence. 	"The dataset consists of 155 primary and 155 metastatic melanoma manually selected ROIs, scanned at 40√ó magnification (0.22 ¬µm/px) with a size of 1024 √ó 1024 pixels. For these ROIs, annotations of both tissue and nuclei are supplied, as well as a context ROI of 5120 √ó 5120 pixels centered around the ROI. Annotations were created by a medical expert and checked and corrected by a dermatopathologist.  All cases were digitized in a large melanoma referral center, however 76 cases are revisions or consultations originating from other treatment hospitals. Annotations are in the .GeoJSON format, making annotations easily visualizable with the opensource pathology image viewer Qupath. 

The following nuclei categories were used:  tumor, stroma, vascular endothelium, histiocyte, melanophage, lymphocyte, plasma cell, neutrophil, apoptotic and epithelium and the following tissue categories were used: tumor, stroma, epidermis, necrosis, blood vessel, and background. A total of 147 739 nuclei are manually annotated. "	The sets were split randomly. As each sample is a histological specimen from a different patient the training and test dataset are independent. 	"The PUMA dataset contains 103 primary melanoma ROIs and 103 metastatic melanoma ROIs in the training set, with a total of 97,429 nuclei. 
The test set includes 52 primary melanoma ROIs and 52 metastatic melanoma ROIs, comprising 50,490 nuclei. 

In the manuscript the distribution of metastatic sites, nuclei and tissue annotations is plotted in the data description paragraph. "	4.0	0.0	https://github.com/tueimage/PUMA-challenge-eval-track1, https://github.com/tueimage/PUMA-challenge-eval-track2	"Yes, a comparison with publicly available methods has been performed (NN192, Hover-Net and Hover-NeXt trained on the PanNuke dataset). 
No publicly available method exists able at segmenting skin tissue on H&E stained histopathology images. "	The results are bootstrapped for generation of confidence intervals. This is not performed in the final public evaluation code for the PUMA challenge due to reducing computational burden. The difference in performance between the used models is statistically significant. 	"For the calculation of the F1 score, the center distance between the predicted nuclei and the ground truth nuclei was used. For each ground truth nucleus, predictions within a 15 pixel (3.3 Œºm) were identified. This radius is smaller than the average size of lymphocytes, which form the smallest nuclei in the dataset. Matching was performed based on the highest predictive score (if available) or the shortest distance. After matching, the ground truth was censored until all ground truth nuclei were either matched or classified as a false negative. Using the identified true positives, false positives, and false negatives, precision (all correct predictions divided by all predictions) and recall (all correct predictions divided by all ground truth nuclei) were calculated. The class F1 score was computed as the harmonic mean of precision and recall, ranging from 0 to 1. Finally to compare models, micro F1  (aggregation of TP, FP, and FN over all classes, followed by F1 score calculation) and Average F1  (the average of class F1 scores) were calculated [40]. Results are shown with a 95% confidence interval which is calculated through bootstrapping the samples. 

Both models and intra- and interobserver agreement were evaluated using the DICE score. The DICE score is a harmonic mean between 0 and 1, in which 1 is a perfect segmentation prediction, and 0 is no correct prediction. This can result in inflated high average DICE scores if a tissue class is only present in a few samples, as the DICE score is 1 in the case of a correct absent prediction. To accommodate this, we calculated not only the average DICE score over all samples but also the micro average DICE. This is the DICE score for all predictions concatenated along one axis, resulting in a prediction mask of 1024 √ó 96.256 pixels. Results are shown with a 95% confidence interval which is generated through bootstrapping the sample results. 

Metrics are in line with literature (Maier-Hein, L., Reinke, A., Godau, P. et al. Metrics reloaded: recommendations for image analysis validation. Nat Methods 21, 195‚Äì212 (2024). https://doi.org/10.1038/s41592-023-02151-z) "	Independent dataset	5.0	0.0	https://github.com/tueimage/PUMA-challenge-baseline-track1, https://github.com/tueimage/PUMA-challenge-baseline-track2	Less than 30 seconds	The model outputs segmentation annotations, which are fully interpretable by pathologists. 	Segmentation	4.0	0.0	"In this study, two algorithm classes are used. The first class consists of models for nuclei instance segmentation, while the second class includes semantic segmentation models for medical image analysis. The nuclei instance segmentation algorithms offer advantages over classic models due to post-processing techniques like watershed segmentation, which help separate densely packed nuclei. Additionally, these algorithms can be applied to whole slide images instead of tiles. 

For tissue annotation semantic segmentation, two models were used. The first is nnU-Net, which is designed for supervised medical image segmentation. The second is Mask2Former2, a transformer segmentation algorithm which can be used for supervised semantic segmentation. For this experiment the backbone was replaced by a foundation model that is better able at feature extraction from histopathology images.


"	The original training code of the models used was not adjusted. Weights for the baseline solution of the PUMA challenge are supplied on Zenodo (https://zenodo.org/records/13881999). 	Annotations are supplied in the .geoJSON format. For training of Hover-Net and Hover-Next the .geoJSON and .TIFF images were recoded to numpy array masks. For tissue segmentation the .geoJSON were encoded to .PNG masks. 	No explicit feature selection was performed. The models used the raw input from the images without reducing or selecting specific features. Therefore, the entire dataset, including both tissue and nuclei information, was used for training without separate feature selection steps.	In this study, due to the use of different models, it‚Äôs hard to directly compare p (parameters) and f (input features) with the number of training points. The models were used to establish a baseline without detailed tuning. Overfitting was countered by 5-fold cross-validation and the use of an independent test set, though specific model-related techniques were not addressed.	No	The models in this research used default configurations for the number of parameters. The goal was to establish a baseline for the dataset, providing a reference point for the PUMA challenge. The community is then invited to improve upon these baseline models in future work. No manual tuning or selection of the number of parameters (p) was performed.	To decrease the risk of overfitting we used data-augmentation and 5-fold cross validation to define a training and validation set in the original training set. We took the mean of these models on the independent test set. 	7.0	1.0	678fa631edf7b65039c6cf91	39970004.0	PMC11837757	02/02/2026 19:46:52	Schuiveling M, Liu H, Eek D, Breimer GE, Suijkerbuijk KPM, Blokx WAM, Veta M.	GigaScience	A novel dataset for nuclei and tissue segmentation in melanoma with baseline nuclei segmentation and tissue segmentation benchmarks.	10.1093/gigascience/giaf011	2024	0.0	0.0		1.0	2025-01-21T13:52:31.149Z	2026-02-02T19:46:52.000Z	59a6767b-171a-443e-933d-91ad24c967f7	undefined	45xadyd7wx				DOME_JSON	Match	Match
6790cf555a9877fd97a7d6c6	"Human raw data and basecall datasets (FAF04090, FAF09968, FAB42828) are available at https://github.com/nanopore-wgs-consortium/NA12878/blob/master/Genome.md; 
bacterial raw data and lambda phage data are available at https://github.com/marcpaga/nanopore_benchmark/tree/main/download; 
ONT chunk dataset are available at https://cdn.oxfordnanoportal.com/software/analysis/bonito/example_data_dna_r9.4.1_v0.zip."	"Human raw data and basecall datasets (FAF04090, FAF09968, FAB42828) are available at https://github.com/nanopore-wgs-consortium/NA12878/blob/master/Genome.md; bacterial raw data and lambda phage data are available at https://github.com/marcpaga/nanopore_benchmark/tree/main/download; ONT chunk dataset are available at https://cdn.oxfordnanoportal.com/software/analysis/bonito/example_data_dna_r9.4.1_v0.zip.
This data has been used in previous papers."	"Human data/ont chunk data: split into train/test randomly.
bacterial data: The training dataset consists of 50 individual species genomes, consisting of 30 Klebsiella pneumoniae genomes, 10 genomes of other species of Enterobacteriaceae, and 10 genomes from other families of Proteobacteria. The test dataset is composed of nine species, including three Klebsiella pneumoniae, Shigella sonnei, Serratia marcescens, Haemophilus haemolyticus, Acinetobacter pittii, Stenotrophomonas maltophilia, and Staphylococcus aureus. 
Training and test sets are independent."	"Human data: train/test: 80,000/5,000 reads
bacterial data: train/test: 40,000/5,000 reads
ont chunk data: train/dev: 1.22 million /20,000 chunks"	4.0	0.0	No	"Compare with Bonito-Transformer, Bonito-LSTM, CATCaller, and SACall
"	No	"SqueezeCall-L achieved the highest identity rate, at 93.97%, on average. 
"	It was evaluated by an independent test dataset.	3.0	2.0	https://github.com/ProfessionalFarmer/SqueezeCall/tree/main	"3 days with 4 NVIDIA A100 GPUs on a high-performance computing cluster.
"	It is a black box.	Sequence labeling	4.0	0.0	An end-to-end Squeezeformer-based model for accurate nanopore basecalling	https://github.com/ProfessionalFarmer/SqueezeCall/tree/main	Reads were sliced in non-overlapping chunks of 3,600 data points, then fed into the end-to-end model	"Use only raw electrical current.
No feature selection strategy was performed."	The model's parameter count is approximately 6 times the number of training data points. Early stopping was adopted to stop training once the model performance was no longer improved on the validation dataset for three consecutive epochs to avoid overfitting.	This model is not a meta-predictor.	"95M parameters
"	Early stopping was adopted to stop training once the model performance was no longer improved on the validation dataset for three consecutive epochs to avoid overfitting.	8.0	0.0	6790cf55edf7b65039cac271	40008031.0	PMC11851125	02/02/2026 19:46:52	Zhu Z.	GigaByte (Hong Kong, China)	SqueezeCall: nanopore basecalling using a Squeezeformer network.	10.46471/gigabyte.148	2025	0.0	0.0		1.0	2025-01-22T10:58:29.334Z	2026-02-02T19:46:52.000Z	a1cf004b-28b3-4aa7-8710-3b350e3bdbda	undefined	ujqs1yayo1				DOME_JSON	Match	Match
67a24e6275e2c69534578d2b	"All of the data is publicly available via this link: [TO ADD]
We also have supporting scripts for executing NeRFStudio models on this data on this Github Repo link: https://github.com/Lewis-Stuart-11/3D-Plant-View-Synthesis"	"We used a robot system to capture data of a set of different wheat plants, captured over a series of different timesteps. All individual data captured of each plant, at each timestep, was encapsulated into their own directories. 
The main data consists of RGB images and depth maps of each plant, captured from positions equidistance around the centre of each plant. Each position for each image was recorded and stored as a transform file and COLMAP point cloud, which ensures that these images can be utilised for 3D reconstruction using Neural Radiance Fields, 3D Gaussian Splatting or other supported models/frameworks.
Approximately 320 images and depth maps were captured for each plant instance- this would vary depending on if the robot setup failed to reach some of the positions. For each of the plant instances, we also include all model weights and training parameters, which can be executed using NeRFStudio.
This dataset is novel and is introduced along with the supported manuscript. "	We chose to implement a 8:1 split between our training and evaluation (test) images. These images were added to each set sequentially during capturing, meaning that after 8 training images were captured, the next was added to the evaluation set. Our capturing process takes images around the plant in a linear path, meaning that the evaluation images were evenly distributed around the plant (resulting in accurate evaluation results). These splits are similar to other multi-view image datasets (such as the original NeRF Dataset).	"For each plant, there are around 35 images used for evaluation (284 images for training). These datasets were distributed evenly based on our capturing process (see IV.3).
Images assigned to the training set or evaluation sets are labelled with '_train' or '_eval' in each file name. Furthermore, separate transform files have been generated for both training and evaluation images. Validation sets are not required for training view synthesis models (at least in this instance)."	4.0	0.0	Yes, these are available on our dataset which is available via this link: [TO ADD].	No, we did not compare this to other datasets, since we are not providing that these NeRFStudio models are more efficient than other publicly available models. Rather, we compared the accuracy of point cloud generated via these models to point clouds generated using Structure from Motion (SfM)	We showed that these methods produce better 3D reconstructions than other standard 3D reconstruction methods such as Structure from Motion (SfM)	"The metrics we used were PSNR, LPIPS and SSIM. These were generated using NeRFStudio's evaluation functionality and can be compared to other standardised models.
We also include a new metric we call PSNR Masked, which only calculates the PSNR of pixels included in a mask (which excludes the background). We provide scripts for calculating this on our Github, which can be accessed via this link: https://github.com/Lewis-Stuart-11/3D-Plant-View-Synthesis "	For evaluation, we rendered a set of images using these models, and compared these to the ground truth images. The evaluation images were not included during training, ensuring that our evaluations effectively measure how accurate these reconstructions are.	5.0	0.0	We utilised NeRFStudio for training on our dataset, which is available via this link: https://docs.nerf.studio/index.html. We also offer a scripts for easily running these models on our dataset, which is available via this link: https://github.com/Lewis-Stuart-11/3D-Plant-View-Synthesis	"For a Desktop PC rendering a 1920x1080 image:
3DGS model: ~0.05 seconds
NeRF model: 2.0 seconds
These times will vary based on the specifications of the computer running the model, as well as the size of the image to be rendered"	We used two models for training, 3D Gaussian Splatting and Neural Radiance Fields, both supported by NeRFStudio. Neural Radiance Field models utilise a neural network to represent a given scene, meaning that these models are black box. However, 3D Gaussian Splatting models utilise gradient descent to position, reshape and recolour a set of Gaussians to fit a complex scene. Since these Gaussians are explicit data structure, this is an interpretable model	The model predicts an RGB image of a scene from a given camera position (view synthesis). These models can also generate point clouds and meshes.	4.0	0.0	"We used two methods for training on our dataset: Neural Radiance Fields and 3D Gaussian Splatting. Both of these methods are utilised for performing view synthesis, which allow novel views to be generated that are not included in the original training set. These models also perform 3D reconstruction.
NeRFStudio, a framework that offers functionality for training data on view synthesis models, was used for all training and evaluation of the dataset. We utilised two of their methods for training:
- NeRFacto: an implementation of NeRF that includes enhancements from various other established models
- Splatfacto: an implementation of Gaussian Splatting with some optimisations
Both of these methods offer state-of-the-art performance compared to other popular models, while also have a large range of functionality such as a real-time renderer, evaluation functionality and exportation capabilities"	Yes, our parameter configurations and weights are publicly available on our dataset, that can be accessed using this link: [TO ADD]	The data was not encoded before training	Roughly 320 RGB images were used for these models. 284 images were used for training.	View synthesis models, at least in the way we utilised them with this dataset, are designed to reconstruct a specific scene. Hence, these models are meant to overfit based on the images of the scene.	No data from other ML algorithms were used	Each model has a large number of parameters (roughly 100). These are all the default values generated when running these NeRFStudio models. These parameters are stored in a config file for each trained model on our dataset	No, as we did not need to prevent overfitting	8.0	0.0	67a24e62edf7b650391b303a	40139909.0	PMC11945317	02/02/2026 19:46:52	Stuart LAG, Wells DM, Atkinson JA, Castle-Green S, Walker J, Pound MP.	GigaScience	High-fidelity wheat plant reconstruction using 3D Gaussian splatting and neural radiance fields.	10.1093/gigascience/giaf022	2025	0.0	0.0		1.0	2025-02-04T17:29:06.321Z	2026-02-02T19:46:52.000Z	97203c6d-6673-4317-a6ba-2b9d79f1d75d	undefined	swv7bfezxi				DOME_JSON	Match	Match
67af37c51f0965481b0c6de1	"1. ABIDE: Available, URL: https://osf.io/hc4md Creative Common CC BY-SA 3.0 License
2. HCP: Available under request. Data and custom license available at https://www.humanconnectome.org/ 
3. IXI: Available, URL: https://zenodo.org/records/11635168 Creative Common CC BY-SA 3.0 License
4. BCW: Available, URL: https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data Creative Common CC BY-NC-SA 4.0 License"	"Four openly available datasets have been used in our study:

1. Autism Brain Imaging Data Exchange (ABIDE). 866 participants (Autism spectrum disorder = 402; Neurotypical controls = 464). Preprocessed data published on October 30, 2018.

2. Human Connectome Project (HCP). 999 healthy participants. HCP1200 released on March 01, 2017

3. Information Extraction from Images (IXI). 600 structural MRI images of healthy individuals.

4. Breast Cancer Wisconsin (BCW). 569 samples for 30 diagnostic features computed from digitized images of fine needle aspirates (FNA) of breast masses (malignant = 357, benign = 212). Released on October 31, 1995."	The splits are determined based on the chosen splitting strategy, since our focus is not on the prediction outcome itself but rather on comparing the performance of the fixed splitting strategies with the adaptive one. Redundancy is naturally reduced by training a different model on each sample size.	"Important: We use our proposed method (AdaptiveSplit) to find the optimal sample splitting, compared to more common, fixed, splitting strategies. 

1. for the ABIDE dataset:

Sample size budget: 400, 442, 489, 542, 599

AdaptiveSplit: 
41% training, 59 % test (with 400 total samples)
49% training, 51 % test (with 442 total samples) 
59% training, 41 % test (with 489 total samples)
71% training, 29 % test (with 542 total samples)
82% training, 18 % test (with 599 total samples)

Pareto Split: 80% train, 20% test (on all sample size budgets)
90/10 Split: 90% train, 10% test (on all sample size budgets)
Half Split: 50% train, 50% test (on all sample size budgets)

2. for the HCP dataset:

Sample size budget: 242, 272, 305, 343, 384

AdaptiveSplit: 
44% training, 56 % test (with 242 total samples)
53% training, 47 % test (with 272 total samples) 
64% training, 36 % test (with 305 total samples)
76% training, 24 % test (with 343 total samples)
89% training, 11 % test (with 384 total samples)

Pareto Split: 80% train, 20% test (on all sample size budgets)
90/10 Split: 90% train, 10% test (on all sample size budgets)
Half Split: 50% train, 50% test (on all sample size budgets)

3. for the IXI dataset:

Sample size budget: 49, 65, 86, 113, 150

AdaptiveSplit: 
21% training, 79 % test (with 49 total samples)
25% training, 75 % test (with 65 total samples) 
40% training, 60 % test (with 86 total samples)
61% training, 39 % test (with 113 total samples)
89% training, 11 % test (with 150 total samples)

Pareto Split: 80% train, 20% test (on all sample size budgets)
90/10 Split: 90% train, 10% test (on all sample size budgets)
Half Split: 50% train, 50% test (on all sample size budgets)

4. for the BCW dataset:

Sample size budget: 49, 65, 86, 113, 150

AdaptiveSplit: 
21% training, 79 % test (with 49 total samples)
25% training, 75 % test (with 65 total samples) 
40% training, 60 % test (with 86 total samples)
61% training, 39 % test (with 113 total samples)
89% training, 11 % test (with 150 total samples)

Pareto Split: 80% train, 20% test (on all sample size budgets)
90/10 Split: 90% train, 10% test (on all sample size budgets)
Half Split: 50% train, 50% test (on all sample size budgets)"	4.0	0.0	A dedicated repository with analysis code is present at https://github.com/gallg/AdaptiveSplitAnalysis	We compare Ridge Regression with AdaptiveSplit against Ridge with fixed splitting methods: Pareto Split, Half Split and 90/10 Split	we run each model on 100 different permutation of the data for each sample size budget. Final results are shown with using SEM as confidence intervals (See Fig.3 of the manuscript)	"1. negative mean squared error for regression tasks
2. accuracy for classification tasks
3. p-values to evaluate the statistical significance"	cross-validation, novel experiments	5.0	0.0	The maintained version of the software, for general use, is available in the following GitHub repository: https://github.com/pni-lab/adaptivesplit	Depends on the adaptivesplit package configuration file. The program allows a fast mode to speed up computation but lowering precision.	The models are simple enough to allow intepretation. However, interpretation of the models is not important for our study	We fit a Ridge model for each dataset. We have two regression tasks (for the HCP and IXI datasets) and two classification tasks (for the ABIDE and BCW datasets)	4.0	0.0	"RidgeCV, as implemented in Scikit-Learn
https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html"	configuration files and analysis scripts with used parameters are present in the following GitHub repository: https://github.com/pni-lab/AdaptiveSplitAnalysis	All features are standard scaled.	All the features present in each dataset.	"We use only the default ""alphas"" parameter from RidgeCV. Overfitting is ruled out by cross validation."	No	"On each dataset we fit a simple Ridge Regressor (HCP, IXI) / Classifier (ABIDE, BCW) with a default alphas parameter:

RidgeCV scoring = ‚Äúneg_mean_absolute_error‚Äù (for regression datasets: HCP, IXI); ‚Äúaccuracy‚Äù (for classification datasets: ABIDE, BCW)
RidgeCV alphas = ‚Äú(0.1, 1.0, 10.0)‚Äù

The Adaptivesplit package includes a configuration file with parameters that define the ""stopping rule"".
Depending on the ""stopping rule"", the adaptivesplit algorithm finds the optimal splitting strategy to define the boundaries between what can be used as training data and what can be used as external validation. This is extremely useful during prospective data acquisition. 
When the algorithm finds the optimal splitting strategy it stops (this is why we talk about ""stopping rule"").

AdaptiveSplit parameters:

min_training_sample_size = 0
max_training_sample_size = inf
target_power = 0.8
alpha = 0.05
min_score = -inf
min_relevant_score = 0
min_validation_sample_size = 12

window_size= 6
step = 1
cv = 5

bootstrap_samples = 100
power_bootstrap_samples = 1
n_jobs = -1
scoring = ‚Äúneg_mean_squared error‚Äù (regression); ‚Äúaccuracy‚Äù (classification)
total_sample_size = highest sample size budget (depends on each dataset)"	L2 regularization, as implemented in scikit-learn's Ridge Regression	7.0	1.0	67a5eb0eedf7b6503929386c	40366867.0	PMC12077397	02/02/2026 19:46:52	Gallitto G, Englert R, Kincses B, Kotikalapudi R, Li J, Hoffschlag K, Bingel U, Spisak T.	GigaScience	External validation of machine learning models-registered models and adaptive sample splitting.	10.1093/gigascience/giaf036	2025	0.0	0.0	machine learning, predictive modelling, pre-registration, external validation, adaptive splitting	1.0	2025-02-14T12:32:05.015Z	2026-02-02T19:46:52.000Z	d4ff70aa-eecf-4aa3-b8d9-80c1cc82b56b	undefined	0p6q20kd4b				DOME_JSON	Match	Match
67bf7c281f0965481b0c7294	The data is not publicly available.	"The source of the data is a publication. The data consists of 285 images (divided into 4 subgroups of different types of histological staining) for classification (pixel level segmentation of tissue from whole slide images). Each image consists of 640x640 pixels (409600 pixels). Each pixel is either categorized as 'tissue' or 'background'. The whole dataset consists of 116¬†736¬†000 pixels (85284725 pixels are 'tissue', 31 451 275 pixels are 'background'). In relative terms: 73.1% (std: 0.04) of all pixels are tissue area, and 26.9% (std: 0.04) are background area. The four staining subgroups were distributed as follows:

Hematoxylin and Eosin: 32 images
Hematoxylin, Erythrosine, and Saffron: 160 images
Masson's Trichrome staining: 32
Immunohistochemistry: 61

This dataset has not been used by other papers."	"In the final model the sets were split randomly (although with stratification to ensure class balance). The only factor that could be considered ""dependent"" between images is the type of histological staining. Other datasets with tissue annotation typically only include one type of stain (often hematoxylin and eosin, e.g., ORCA: https://sites.google.com/unibas.it/orca/home and CAMELYON16: https://camelyon16.grand-challenge.org/) and focus not on segmenting tissue from background, but provide tissue annotations for segmentation areas of medical interest within the tissue area. Therefore, they are not useful for our method which is evaluated to work on different types of staining."	"Since the task is tissue segmentation from images, we applied data splitting at an image level. The model was validated with two strategies to highlight robustness (explanation at ""Evaluation.Evaluation Method)

The final model used by the registration algorithm was trained on a 80/20 split (stratified split to ensure that distributions are similar). No hyperparameter optimization was conducted because the structure of the deep learning model was predefined. "	4.0	0.0	Raw evaluation files have not been made available.	We compared our method to a simpler baseline method (Otsu's thresholding) by performing the same preprocessing steps to the tissue image but using Otsu' thresholding instead of our model.	We have performed a one-sided Wilcoxon signed-rank test and achieved a p value < 0.0001. Therefore, we make the claim that our method is better than the baseline.	"The reported performance metrics are Hausdorff distance (69.159), Dice Similarity Coefficient (0.979), Precision (0.982), and Recall (0.997).

These are metrics used in the field of Digital Pathology when working in the field of segmentation tissue regions of interest. For instance, see https://doi.org/10.3389/fmed.2022.971873 and https://doi.org/10.3390/cancers15030762."	"The method was evaluated in two rounds of validation before final training:
1. Stratified Kfold cross validation with a 80/20 split with stratification for each type of staining.
2. Leave-one-tissuetype-out validation (train the model on 3 staining types and leave one staining type out) with a split according to the distribution noted in IV.1. This was done for analyzing the robustness. Example: When testing ""leave-one-tissuetype-out validation"" on immunohistochemistry, 224 images from the other 3 staining groups were used as training and the 61 images of immunohistochemistry were used as a test set.

The final model was trained on a 80/20 stratified split using cross validation."	5.0	0.0	The software is released under the MIT license and can be executed via Python API. It is used a preprocessing step in a registration pipeline. Tutorials for running the registration (and specifically the segmentation portion of the registration) can be found here: https://github.com/mwess/GreedyFHist/blob/master/examples/notebooks/pairwise.ipynb. Example data is available.	The model takes around 1.06 seconds on a desktop PC.	The model is based on a deep learning architecture and is therefore a black box model.	The model performs tissue segmentation which is a form of classification.	4.0	0.0	The ML algorithm is a deep learning architecture based on the pre-existing YOLO8 model (https://yolov8.com/) for segmentation. It was chosen because it is able to perform fast inference on a CPU which complements the rest of our CPU based registration algorithm.	The trained model is incorporated into our registration algorithm, GreedyFHist (https://github.com/mwess/GreedyFHist) and available under the MIT license. Default parameters were used to train the model.	Preprocessing: Images were resampled to a resolution of 640 x 640 and converted to grayscale. Spurious noise from the background of the image was removed using a method by Chambolle et al., that performs total variation denoising. Then we used the YOLO8 model to perform tissue segmentation on the image. After segmentation small artifacts were removed from mask and masks were rescaled to the input images original size.	Each input image consists of 640x640 (409600) pixel. No feature selection was done.	Since we train the model for pixel level classification our training points were 116¬†736¬†000 pixels and  3 157 200 parameters (ratio of 0.027). We evaluated our segmentation model with stratified kfold cross validation and leave-one-tissuetype-out validation before training the final model. All models performed similarly. 	No data from other ML algorithms is used as input.	The model was predefined with 3 157 200 parameters. By default, adjusting the number of parameters was not possible and therefore not done.	During training, image augmentation is used to generalize the dataset (Augmentation parameters can be found here: https://docs.ultralytics.com/modes/train/#train-settings). Additional augmentation, mosaic augmentation (https://www.analyticsvidhya.com/blog/2023/12/mosaic-data-augmentation/) is used as a means of improving the models robustness. This feature was disabled in the last 10 epochs of training (epochs 90 - 100). Because of that, no early stopping was used.	8.0	0.0	67bf7c28edf7b650399238cf	40366868.0	PMC12077394	02/02/2026 19:46:52	Wess M, Andersen MK, Midtbust E, Guillem JCC, Viset T, St√∏rkersen √ò, Krossa S, Rye MB, Tessem MB.	GigaScience	Spatial integration of multi-omics data from serial sections using the novel Multi-Omics Imaging Integration Toolset.	10.1093/gigascience/giaf035	2025	0.0	0.0	Image Segmentation, Digital Pathology	1.0	2025-02-26T20:40:08.523Z	2026-02-02T19:46:52.000Z	bd56ff04-ef56-4cbf-bf9b-a110989b9967	undefined	vsx18unv9t				DOME_JSON	Match	Match
67d977ca478cfd7e6983b59c	"Yes, the data are on a data repository, http://doi.org/20.500.11850/697773
Additionally, the data are in convenient form for ML tasks prepared as Hugging face dataset, https://huggingface.co/datasets/mikeboss/FIP1"	"1. Genetic marker data for the GABI wheat panel genotypes originates from a public dataset, https://doi.org/10.5061/dryad.n02v6wwzc
2. Multi-environment trial data for the GABI wheat panel genotypes originate from a public dataset, https://doi.org/10.5447/ipk/2022/18
3. Multi-year trial data for the FIP field phenotyping platform origin from direct experiments
4. Weather data for German environments were extracted from the Climate Data Center (CDC) of the German Weather Service
5. Weather data for the French environments were extracted from Meteo France SYNOP database

The multi-environment trial data comprise 12192 data points for GABI resp. 6996 data points for FIP."	Training and test sets were split to exclude genotypes and environments, resulting in three independent test sets, unseen genotypes, unseen environments, and unseen genotypes in unseen environments. This corresponds to the gold standard of evaluation genotype-environment-interaction models.	"For the FIP set, the splits are done for unseen genotypes and unseen environments in parallel, resulting in 1 training set and 3 test sets:
- training: 829 genotypes
- test (unseen genotype): 30 genotypes
- test (unseen environment): 48 genotypes
- test (unseen genotypes in unseen environment): 30 genotypes

For the GABI set, the splits are done for unseen genotypes in unseen GABI environments, resulting in 1 training set and 1 test set:
- training: 316 genotypes
- test: 60 genotypes

No validation set was used, as no hyperparameter tuning was performed.
The train/test splits and how genotypes and environments are shared among data points are plotted in Figure 5 of the corresponding manuscript.
"	4.0	0.0	yes, on https://gitlab.ethz.ch/crop_phenotyping/fip-1.0-data-set-traits	No, the method itself is public and established. Three established models were compared, with GBLUP (id) the baseline, and two more advanced ones, GBLUP (diag) and random regression.	Yes, the metrics are calculated per environment and the mean and var provided.	"Pearson's correlation (accuracy) and RMSE (bias).
Yes, those are the two standard metrics for genomic prediction models"	Both cross-validation and independent test set.	5.0	0.0	Model training was done in R, which is a public statistical programming language.	10 min to 1 hour.	Interpretable. Marker effects can be extracted, and coefficients to environmental covariates interpreted.	Regression	4.0	0.0	"All models were general linear mixed model by residual maximum likelihood (REML). The algorithm is well-established and implemented in several R-packages.
We used the R package ASReml-R."	Yes, all settings of the GBLUP model are mentioned in the manuscript.	Marker data were encoded as similiarity matrix using the Yang algorithm. Independent variables were centered and normalized.	18846 SNP markers and 9 environmental covariates were used. No feature selection was performed.	The 18846 SNP markers were reduced to a similarity matrix (Kinship, Yang algorithm), so that n > p.	No	p is between 2314 (id model) and 6346 (random regression). The model equation (e.g., interaction effects) directly determines p.	No.	6.0	2.0	67339149dd85bd59b399c87e	40498535.0	PMC12153353	02/02/2026 19:46:52	Roth L, Boss M, Kirchgessner N, Aasen H, Aguirre-Cuellar BP, Akiina PPA, Anderegg J, Castillo JG, Chen X, Corrado S, Cybulski K, Keller B, G√∂bel Kortstee S, Kronenberg L, Liebisch F, Nousi P, Oppliger C, Perich G, Pfeifer J, Yu K, Storni N, Tschurr F, Treier S, Volpi M, Zellweger H, Zumsteg O, Hund A, Walter A.	GigaScience	The FIP 1.0 Data Set: Highly resolved annotated image time series of 4,000 wheat plots grown in 6 years.	10.1093/gigascience/giaf051	2025	0.0	0.0		1.0	2025-03-18T13:40:26.714Z	2026-02-02T19:46:52.000Z	d79d2bb8-c33e-4cd8-ae80-9eda4e1e4da6	undefined	mj3s5cptmp				DOME_JSON	Match	Match
67da3599478cfd7e6983b608	"PBMC 10k dataset downloaded from https://support.10xgenomics.com/single-cell-multiome-atac-gex/datasets/1.0.0/pbmc_granulocyte_sorted_10k.
SHARE-seq dataset downloaded from https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE140203.
NeurlPS dataset downloaded from GSE194122.
Greenleaf2021 downloaded from GSE162170.
"	"The data used in this article are all from public datasets on the Internet, which are explained in detail below:
PBMC 10k dataset contains 11909 cells (data points), collected from PBMC, downloaded from https://support.10xgenomics.com/single-cell-multiome-atac-gex/datasets/1.0.0/pbmc_granulocyte_sorted_10k. Each cell in this data contains two batches of RNA and ATAC values, and the data contains a total of 20 cell types. The task performed on this data in this article is multimodal integration, so it is not a classification task, and there is no division into positive or negative samples.

SHARE-seq dataset contains 32231 cells (data points), collected from skin late anagen (mouse), downloaded from https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE140203. The data comes from the article ‚ÄòChromatin potential identified by shared single-cell profiling of RNA and chromatin‚Äô. Each cell in this data contains two batches of RNA and ATAC values, and the data contains a total of 22 cell types. The task performed on this data in this article is multimodal integration, so it is not a classification task, and there is no division into positive or negative samples.

NeurlPS dataset contains 42492 cells (data points), collected from bone marrow, downloaded from GSE194122. The data comes from the article ‚ÄòA sandbox for prediction and integration of DNA, RNA, and proteins in single cells‚Äô. This data is used for cross-modal prediction tasks in the article, so it is a regression task and there is no division into positive or negative samples.

Greenleaf2021 dataset contains 4733 cells (data points), collected from brain cortex, downloaded from GSE162170. The data comes from the article ‚ÄòChromatin and gene-regulatory dynamics of the developing human cerebral cortex at single-cell resolution‚Äô. The task performed on this data in this article is multimodal integration, so it is not a classification task, and there is no division into positive or negative samples.
"	In the cross-modal prediction task, we conducted benchmark experiments on the PBMC 10k and NeurlPS datasets. For the PBMC 10k dataset, we randomly divide cells into training (n=9527) and testing (n=2382) sets (bootstrapping five times). For the NeurlPS dataset, we compare with the top five winners from the modality prediction task (ATAC-GEX subtask) using the official settings, datasets, and guidelines from the multimodal single-cell data integration competition of NeurIPS 2021.	In the multimodal integration task, the dataset is not divided into training and test sets because we use unsupervised learning. In the cross-modal prediction task, we use 80% of the cells as the training set and the remaining 20% of the cells as the test set. The data distribution of the training set and the test set is consistent.	4.0	0.0	The evaluation code is shown in https://github.com/melobio/Attune. The evaluation results reported in this paper are available in Supplementary_Table.xlsx.	We benchmark with GLUE, uniPort, Seurat, Cobolt, MinNet, scJoint, MultiVI, sciCAN, Babel and Polarbear on PBMC 10k dataset and SHARE-seq dataset.	The performance metrics have confidence intervals and it can claim that the method is superior to others. We benchmark against other multimodal integration methods on PBMC 10k and SHARE-seq dataset (repeated five times with different random seeds). We benchmark against other cross-modal prediction methods on PBMC 10K and NeurIPS dataset (bootstrapping five times).	Integration endeavors are assessed through an array of metrics, including mean average precision (MAP), cell type ASW, neighbor consistency (NC), Seurat alignment score (SAS), Batch ASW, graph connectivity (GC), biology conservation, omics mixing, overall integration score and FOSCTTM. Most of which have been widely embraced and validated in previous single-cell integration tasks, such as graph connectivity (GC) in the NeurIPS 2021 competition and average silhouette width (ASW) in scJoint and FOSCTTM in MMD-MA.	"To demonstrate the performance of Attune, we benchmark it against other multimodal integration methods (repeated five times with different random seeds) on matched scRNA-seq and scATAC-seq datasets, such as 10x Multiome and SHARE-seq, with several established metrics, most of which have been widely embraced and validated in previous single-cell integration tasks, such as graph connectivity (GC) in the NeurIPS 2021 competition and average silhouette width (ASW) in scJoint. We also quantify the alignment performance between modalities using the Fraction Of Samples Closer Than the True Match (FOSCTTM) on both datasets.
In cross-modal prediction, we compare it against state-of-the-art cross-modal prediction methods such as BABEL and Polarbear, using the 10x Multiome dataset (PBMC10k, n=11,909). We randomly divide cells into training (n=9527) and testing (n=2382) sets (bootstrapping five times), and evaluate the method using gene-wise Pearson correlation and gene-wise Spearman‚Äôs correlation.
"	5.0	0.0	The source code is available on Github at https://github.com/melobio/Attune. The license is GPL-3.0.	On the PBMC 10k dataset (11909 cells), pre-training takes 30 minutes, cross-modal prediction takes 15 minutes, and inference of gene-peak interaction takes 1 hour. The above time is calculated on a single Quadro RTX 6000 24GB	By employing transformer‚Äôs cross-attention mechanism, we can interpretable reveal regulatory interaction of genes and construct gene-peak interaction. Attune employs contrastive learning as a pretraining procedure followed by a transformer decoder to model the relationship between peaks and genes. The transformer is a deep neural network structure for sequence modeling. The self-attention mechanism establishes attention connections between each token in a sequence, so the embedding of each token contains implicit context. Meanwhile, the cross-attention mechanism establishes attention connections between tokens in two sequences, enabling the model to extract the dependency between tokens in the two sequences. Both self-attention and cross-attention are adopted in our model. Self-attention captures intra-modality interaction, such as gene-gene relationships, while cross-attention models inter-modality interaction, which is the peak-gene relationship.	Attune is an unsupervised pre-training model, which consists of two asymmetric teacher-student networks, as well as two modules dedicated to downstream tasks: the cross-modal prediction module and the inference of gene-peak interaction module. The cross-modal prediction module is regression task and the inference of gene-peak interaction module is classification task.	4.0	0.0	"The ML algorithm reported in this paper belongs to an unsupervised multimodal pre-training algorithm, which is a cross-modal contrastive learning pre-training model designed to capture the interactions between different modalities.
In the context of single-cell multi-modal data, this ML algorithm is innovative in that it can capture the interactions between different modalities. By maximizing the agreement between modalities within a cell on the hypersphere. It can also places representations of distinct modalities into a shared feature space, enabling the modeling of interactions between peaks and genes. These capabilities are not available in other known algorithms.
"	Hyperparameter configurations and optimization strategies and model files and optimization parameters are described in detail in https://github.com/melobio/Attune. 	The Attune model takes expression (gene count) and accessibility (peak count) matrices from matched multimodal scRNA-seq and scATAC-seq as input data. For scRNA-seq data, genes expressed in fewer than 5% of cells were filtered out. We used SCANPY to normalize each cell count to 10,000 read counts before the logarithm. Additionally, sex chromosome genes were removed, and 2000 highly variable genes (HVGs) were selected based on the experiments depicted in Figure 2d providing a reasonable compromise. For scATAC-seq data, peaks accessed in fewer than 5% of cells were filtered out. And peaks from sex chromosomes were also filtered out. The normalized expression and accessibility matrices were encoded in the TensorFlow Record (TF-record) format. The scRNA-seq data was encapsulated in one TF-record file, with 'gene index' and 'gene count' fields, while the scATAC-seq data was encapsulated in another file, with 'peak index' and 'peak count' fields.	For scRNA-seq data, there are 2000 features as input, and for scATAC-seq data, there are more than 20,000 features as input. Based on the experiments depicted in Figure 2d and Table S5 providing a reasonable compromise. This choice allows us to achieve satisfactory performance while managing the computational cost and ensuring the overall stability and efficiency of the model. It performed using the training set and testing set.	Attune leverages separate teacher-student networks to learn cell representations from scRNA-seq and scATAC-seq respectively, through cross-modal contrastive learning. Contrast learning is implemented by the explicit comparison of the d-dimensional embedding (where d‚Äâ=‚Äâ128 by default) of a cell on two modalities on a unit hypersphere.  Positive sample pairs are created by taking two modal representations of a cell and pulling them together, while negative samples are created by taking different cells and widening the distance between them. The above process is quantified using NT-Xent loss (the normalized temperature-scaled cross-entropy loss). To avoid underfitting of the model, we set the minimum batch size to 32. As the number of training iterations increases, the NT-Xent loss will gradually decrease. To avoid overfitting of the model, we set the maximum epoch to 20.	The data input to the model is raw data and has not been processed by other ML algorithms. It is clear that training data of initial predictors and meta-predictor are independent of test data for the meta-predictor.	The parameter that needs to be defined in the model is the dimension of embedding, and we set the dimension of the embedding to 128. Comparative experiments are detailed in Table S6.	To avoid overfitting of the model, we set the maximum epoch to 20.	8.0	0.0	67da3599edf7b65039fddc2c	41105015.0	PMC12532322	02/02/2026 19:46:52	Yang Y, Xie C, He Q, Yang M.	GigaScience	Cross-modal contrastive learning decodes developmental regulatory features through chromatin potential analysis.	10.1093/gigascience/giaf053	2024	0.0	0.0		1.0	2025-03-19T03:10:17.104Z	2026-02-02T19:46:52.000Z	e3e1fed9-0700-4740-a8f9-61afceb18d98	undefined	q1loj0zi07				DOME_JSON	Match	Match
67f66706822ee03b50b6c44a	"All the datasets are public available at https://doi.org/10.5281/zenodo.14447515. However, as it was suffled and split at random, the splits were not saved.
The license is CC0."	"There are 3 main sources: 
- Group A: the original source is https://doi.org/10.1016/j.soilbio.2021.108472. 
- Grouo B: the original source is https://github.com/knights-lab/MLRepo/tree/master
- MGnify: https://www.ebi.ac.uk/metagenomics/about, ids: MGYS00000916, MGYS00001160, MGYS00000580, MGYS00001175, MGYS00001188, and MGYS00001255.

But all the sources processed and used during the work are public available at https://doi.org/10.5281/zenodo.14447515, under the folder ""Data"".

All values ‚Äã‚Äãare ""True Values"" meaning no data augmentation was used

Yes, all datasets have been previously used by other papers and recognized by the community."	"The tests were performed using a suffle train-test split ten times. 

For the cross-studies the test sets were completly independent, since train and test were colledted from different projects.

for the non cross-studies, the distribution was the same as previously published ML datasets.

"	"Since it is a regression, all tests were performed using a suffle train-test split ten times. 

The exception was the cross-studies testing, which employed two (or four, depending on the experiment) datasets in a way that a model was trained on the entire dataset and then used to predict the second dataset. This procedure is repeated for every dataset.

For the non cross-studies, all the distributions are public available at suplemmentary material 1. 

The distribution was not plotted in other cases, but it can be done by the tool, without the use of external libraries.

"	4.0	0.0	No. However, the tool displays its own metrics, such as the p-value, and a 'hold-out' validation using a box plot.	Yes, the datasets were benchmarked using four recently published and freely available tools. Yes, the CLR-LASSO.	All the p-values are reported by CODARFE, and all much smaller than 0.01. 	Rooted mean squared error for the train-test split, and R¬≤ adjusted and Rooted mean squared error. 	"Shuffle train test split with ten repetitions.

For cross-studies , each dataset acts as an independent dataset."	5.0	0.0	Yes. there are 5 different ways to run the tool. All available at https://github.com/alerpaschoal/CODARFE under Apache License Version 2.0, January 2004	For a small dataset (less than 10,000 columns), the duration is less than 10 minutes, and it is linear to the number of columns (features).	The model is interpretable, since it is a regresison and the weights for each feature are visible. The program can also be used to plot a heat mat of feature relevance.	Regression	4.0	0.0	The used ML algorithm are Huber Regression and Random forest regression.	"Yes, they are all public available under the section ""Material and Methods"" on https://doi.org/10.1101/2024.07.18.604052 under a CC-BY-NC-ND 4.0 International license"	The Center-Log-Ratio was used to remove the compositional effect.	The input dataset determines the number of features. The number of features is chosen by combining the Huber Regression with Recursive Feature Elimination.	Yes, p is much larger than the number off points. The overfitting was ruled out by combining four criteriato evaluate each set of features generated during the Recursive Feature Elimination: the R2 adjusted was chosen to determine the correlation strength with the target variable while selecting a concise number of predictors due to its penalization effect, collaborating to reduce the type 1 error during the fit process; BIC was chosen to differentiate models with similar R2 and with different predictor quantities, and it also helps keep low type 1 error due its penalization related to the number of predictors; Rooted Mean Squared Error (RMSE) was used in cross-validation to reduce overfitting,  and lastly, the p-value of the F-test is used to ensure the statistical significance of the selected predictors. All metrics were normalized using the ‚Äúminmax‚Äù method and summed to create the model‚Äôs score. This occours at most 100 times and the best score is selected.	No.	Using a grid-search the ‚Äúepsilon‚Äù parameter, determining outlier influence, was set to 2, and the ‚Äúalpha‚Äù value, controlling regularization, was set at 0.0003 for the Huber regresion, and  ‚Äún_estimators = 160‚Äù and ‚Äúcriterion = poisson‚Äù for the Random Forest.	Yes, mainly by the Rooted Mean Squared Error (RMSE) that was used in cross-validation to reduce overfitting during the model score creation. (See Fitting)	7.0	1.0	67f66706edf7b6503972514d	40549543.0	PMC12365963	02/02/2026 19:46:52	Barbosa MC, Marques da Silva JF, Alves LC, Finn RD, Paschoal AR.	GigaScience	CODARFE: Unlocking the prediction of continuous environmental variables based on microbiome.	10.1093/gigascience/giaf055	2025	0.0	0.0		1.0	2025-04-09T12:24:38.735Z	2026-02-02T19:46:52.000Z	ebc6003e-1a01-4548-a865-3c38d0225d4e	undefined	oym6non3hy				DOME_JSON	Match	Match
680a5787ea60466a7ca5ab9e	"UK Biobank data are only available after registration and payment of access fee:
https://www.ukbiobank.ac.uk/enable-your-research/apply-for-access
The data IDs of the data used cannot be shared, since UK Biobank creates unique codes for each application separately. Our application ID is 42577."	"UK Biobank: https://www.ukbiobank.ac.uk
Subset of n=3046 where two time points were available (about 2 years apart)"	"Test and training data are from the same database (UK Biobank), but are independent
Random splits while considering similar age distribution between test and training data"	"10-fold validation (no separate validation set used)
Distribution of age was adapted between test and training data"	4.0	0.0	No	No	No	Mean absolute error (MAE) which is quite common	cross-validation with 10-fold	2.0	3.0	https://github.com/ChristianGaser/BrainAGE (Apache 2.0 license)	~100s on a desktop PC	Gaussian Process Regression (GPR) is somewhat interpretable, but it's not as transparent as simple linear models. It's often seen as a semi-black-box model.	Regression	4.0	0.0	Gaussian Process Regression	GPR code is available here: https://github.com/ChristianGaser/BrainAGE	Preprocessing of MRI data using CAT12 following these instructions: https://github.com/ChristianGaser/BrainAGE/blob/master/README.md	Feature reduction using PCA (based on singular value decomposition) to all the models using n‚Äì1 PCA components (n = minimum of voxel number or sample size)	GPR has quite low number of p	No meta-prediction	Hyperparameters: 100 for the constant mean function and -1 for the likelihood function	No	7.0	1.0	680a5602ede910531f9afe54	40407124.0	PMC12099614	02/02/2026 19:46:52	Luders E, Poromaa IS, Barth C, Gaser C.	GigaScience	A Case for estradiol: younger brains in women with earlier menarche and later menopause.	10.1093/gigascience/giaf060	2025	0.0	0.0	Gaussian Process Regression, BrainAGE	1.0	2025-04-24T15:23:51.786Z	2026-02-02T19:46:52.000Z	560cc584-7cbe-4759-8707-05e634023ab1	undefined	rpt86qjsd5				DOME_JSON	Match	Match
681db701ec6931461ae8ab0e	"1. CellBinDB
   - Accession: https://zenodo.org/records/15110639
   - License: CC BY-NC-SA 4.0
2. IEEE_TMI_2019
   - Accession: https://doi.org/10.5281/zenodo.1174342
   - License: CC-BY-4.0
3. Lizard
   - Accession: https://www.kaggle.com/datasets/aadimator/lizard-dataset
   - License: CC BY-NC-SA 4.0"	"The data used in this project comes from the dataset CellBinDB published in this article, as well as two public datasets IEEE_TMI_2019 and Lizard.
CellBinDB contains 1044 images, IEEE_TMI_2019 contains 50 images, and Lizard contains 238 images.
All the datasets mentioned above are publicly accessible. The 2 public datasets have been utilized in prior research and are well-recognized within the scientific community."	"The training set consists of the CellBinDB dataset, and the test sets consist of two separate datasets: IEEE_TMI_2019 and Lizard.
The training set and the test sets are completely independent. There is no overlap between these datasets in terms of image content or source, ensuring that the models are evaluated on entirely unseen data during testing."	"For our model fine-tuning process:
Training Set: We used the CellBinDB dataset, which contains 1,044 images.
Test Sets: Two separate datasets were used for testing:
IEEE_TMI_2019: Contains 50 images.
Lizard: Contains 238 images.

No separate validation set was used during the fine-tuning of both models (Cellpose and MEDIAR).

Regarding the distribution of data types:
The training set includes images from four different staining types (DAPI, H&E, ssDNA, and mIF), while both test sets only contain images stained with H&E. This difference in staining types between the training and test sets allows us to evaluate the generalization capability of our models on unseen staining types.
In our paper, we have included T-SNE plots that visualize the feature distributions of the datasets. These plots demonstrate that the features in CellBinDB are broadly distributed and cover characteristics from most publicly available datasets. This broad coverage explains why fine-tuning models on CellBinDB improves their performance on third-party datasets they have not seen before, thereby validating the utility and comprehensiveness of our dataset.
"	4.0	0.0	https://github.com/STOmics/cs-benchmark (MIT license)	The comparison was made between the model's performance before and after fine-tuning on our dataset (CellBinDB). Specifically, we evaluated the improvement in F1 scores post fine-tuning. No direct comparison with publicly available methods or simpler baselines was performed.	No.	The F1 score is reported as the primary performance metric. This metric is widely used in segmentation tasks and is representative of commonly used metrics in the literature, especially for evaluating precision and recall balance.	The method was evaluated on independent test sets (IEEE_TMI_2019 and Lizard). The primary focus was on comparing the improvement in F1 scores between the models fine-tuned on CellBinDB and those not fine-tuned.	4.0	1.0	CellBinDB: https://github.com/STOmics/cs-benchmark (MIT license); Cellpose: https://github.com/MouseLand/cellpose (BSD-3-Clause license); MEDIAR: https://github.com/Lee-Gihun/MEDIAR (MIT license)	Each image takes a few seconds on a consumer laptop.	The model is interpretable, but interpretation goes beyond the scope of this work. Our work focuses on fine-tuning existing models and observing whether performance improves.	The models used (Cellpose and MEDIAR) are for instance segmentation, not classification or regression. They are designed to identify and segment individual objects (such as cells) within images.	4.0	0.0	"The models used in this study are Cellpose and MEDIAR. Both models are designed for image segmentation tasks, specifically tailored for biomedical image analysis.
Neither Cellpose nor MEDIAR is a new algorithm; both have been previously published and are well-established in the field."	The default configuration of the original study was used. Cellpose: https://github.com/MouseLand/cellpose (BSD-3-Clause license); MEDIAR: https://github.com/Lee-Gihun/MEDIAR (MIT license)	The raw images were used directly without additional encoding. Both models are designed to process raw image data.	We use microscope images as input. Instead of using feature selection, we use a deep learning framework for abstract feature extraction.	By comparing train and test accuracies.	The input to the models consists of the original images from the datasets without any intermediate processing or outputs from other ML algorithms.	Default hyperparameters from the scikit-learn implementations, as in the original study.	No. Manually select training epochs based on experience.	8.0	0.0	676a1780dd85bd59b3ab6ad7	40552981.0	PMC12206155	02/02/2026 19:46:52	Shi C, Fan J, Deng Z, Liu H, Kang Q, Li Y, Guo J, Wang J, Gong J, Liao S, Chen A, Zhang Y, Li M.	GigaScience	CellBinDB: a large-scale multimodal annotated dataset for cell segmentation with benchmarking of universal models.	10.1093/gigascience/giaf069	2025	0.0	0.0	Cell Segmentation	1.0	2025-05-09T08:04:17.793Z	2026-02-02T19:46:52.000Z	7073b8b2-c3a7-41eb-b7ae-4a22ca7e5ce9	undefined	etom5gtugj				DOME_JSON	Match	Match
6825ac7aec6931461ae8af2b	The original data sets are publicly available at https://www.humanconnectome.org/ for HCP and https://www.neuroinfo.org/gsp/ for GSP, which can be accessed via data use agreements. The preprocessed data sets are available at https://doi.org/10.5281/zenodo.10050233 for HCP and https://doi.org/10.5281/zenodo.10050234 for GSP.	We used two datasets, HCP and GSP, from public databases. HCP Npos 960 and Nneg 960, GSP Npos 1570 and Nneg 1570. The datasets have been used in previous published papers.	Each subject contribute two hemispheres. We divided our subjects into two groups: for 50% of the subjects, we used their left hemispheres for training, and for the remaining 50%, we used their right hemispheres. The hemispheres not selected for training were then used for testing.	For each dataset, 50% data points used for training and 50% data points used for testing.	4.0	0.0	The code for baselines and generated figures are available at: https://github.com/shuo-zhou/GSDA-Lateralization. Sample baseline model files are available on Hugging Face Model Hub: https://huggingface.co/shuo-zhou/brain_network_lateralization. Both are provided under the MIT license.	We compared our method with standard logistic regression and univarite t-test.	Accuracy and group specificity index are reported with confidence intervals. Our new method obtained the desired results, whereas the baseline methods did not.	Classification accuracy and group specificity index derived from accuracy.	Repeated training-test split.	5.0	0.0	The source code is available at: https://github.com/shuo-zhou/GSDA-Lateralization under MIT license.	Around 30 seconds on a standard CPU machine for fitting one model.	Our models are linear and interpretable. The coefficients of a first-order model represent the strength of lateralization for the corresponding brain network connections.	Classification.	4.0	0.0	Group-specific discriminant analysis. This algorithm is new. It focuses on capturing group-specific patterns, while the patterns learned by standard classifiers 	Yes. Sample configurations are availabe on the GitHub repo: https://github.com/shuo-zhou/GSDA-Lateralization. Sample model files are available on Hugging Face Model Hub: https://huggingface.co/shuo-zhou/brain_network_lateralization. Both are provided under the MIT license.	Left brain hemispheres are labeled as 0, and right brain hemispheres are labeled as 1. For GSDA-Logit, sex is utilized as the grouping factor in the experiments, encoding males as 0 and females as 1	7503 features. No feature selection.	In our experiments, the ratio of p/(Npos + Nneg) is between 5 and 8. We repeat the train-test split 1,000 times to demonstrate that the models are not overfitting.	In our second-order classification, the models obtained from the first-order classification (left vs. right) are used as input data.	Our model is linear, the number of parameters is the same as the number of features: 7503	We use L2 regularization to prevent overfitting.	8.0	0.0	66d5a08e7089c469b4d96acf	40884801.0	PMC12398281	02/02/2026 19:46:52	Zhou S, Luo J, Jiang Y, Wang H, Lu H, Gong G.	GigaScience	Group-specific discriminant analysis enhances detection of sex differences in brain functional network lateralization.	10.1093/gigascience/giaf082	2025	0.0	0.0		1.0	2025-05-15T08:57:30.116Z	2026-02-02T19:46:52.000Z	ad84e397-0491-46f4-9241-6498ccc495b3	undefined	3znup161pu				DOME_JSON	Match	Match
685504d91d1a98db540a923e	Yes. All datasets used in this study, including RNA and disease information, known associations, and the 5-fold cross-validation splits, are publicly available. The data will be archived and assigned a DOI in GigaDB upon publication. In addition, key data files and preprocessing scripts are available in the GitHub repository. All data will be shared under an open data license to ensure reusability and transparency.	"The raw data were collected from publicly available and well-established biological databases. miRNA‚Äìdisease associations were obtained from HMDD v4.0, with sequence data from miRBase. lncRNA and circRNA associations were sourced from LncRNADisease v3.0, with circRNA sequences from circBase and lncRNA sequences from GENCODE and NONCODE. piRNA‚Äìdisease associations were collected from piRDisease v1.0, and piRNA sequences from piRBase and piRNAdb. Disease terms were standardized using Disease Ontology (DOID) identifiers.

The dataset is structured as a binary classification task, where positive samples are known RNA‚Äìdisease associations and negative samples are constructed using the spy technique. The total number of positive samples is in the tens of thousands, and negative samples are generated in equal size per fold during cross-validation. These databases are widely used and recognized in the ncRNA research community, although the exact combination and integration presented in this study is novel."	The dataset was randomly split using 5-fold cross-validation. For each fold, training and test sets were strictly separated with no overlapping RNA‚Äìdisease pairs. This independence was enforced using binary mask matrices that explicitly define which entries are used for training or testing. No additional redundancy reduction (e.g., sequence similarity filtering) was performed. The distribution of positive and negative samples is consistent with previous studies in the ncRNA‚Äìdisease association domain, and no significant imbalance or bias was observed.	Each fold of the dataset contains approximately 10,000 positive samples for training, along with an equal number of reliable negative samples selected using the spy technique. The test set in each fold includes around 3,000 positive samples and an equal number of randomly sampled negatives, resulting in roughly 6,000 test samples per fold. No separate validation set was used; all evaluation was performed using 5-fold cross-validation. The distributions of data types (RNA categories and disease classes) were kept consistent across folds through random stratified splitting. While data distribution plots were not included, care was taken to ensure balanced representation in both training and test sets.	4.0	0.0	Yes. The raw evaluation outputs, including prediction scores, evaluation scripts, and fold-wise performance logs, are available in the GitHub repository and will be archived in GigaDB upon publication. The evaluation code includes metrics computation, result aggregation, and visualization tools. All resources are released under an OSI-compliant open-source license (e.g., MIT).	Yes. The proposed method was compared against both publicly available methods and simpler baseline models using the same dataset and evaluation protocol. The comparisons were conducted using 5-fold cross-validation to ensure fairness and reproducibility. The results demonstrated that our model achieved superior performance across multiple metrics, confirming its effectiveness over existing alternatives.	Performance metrics were averaged over 5-fold cross-validation, and standard deviations were reported to indicate variability. While formal confidence intervals were not computed, the observed performance improvements over baselines were consistent across all folds. This consistency suggests that the model's superiority is statistically meaningful, although no formal statistical significance tests (e.g., p-values) were conducted.	We report AUC (Area Under the ROC Curve), AUPR (Area Under the Precision-Recall Curve), and Ranking Index (RI) as performance metrics. These metrics are widely used in ncRNA‚Äìdisease association prediction tasks and are well-suited for imbalanced binary classification problems. AUC and AUPR measure the classifier‚Äôs ability to distinguish positive and negative associations, while RI evaluates the ranking quality of predicted associations. This set of metrics is representative and consistent with standard practices in recent biomedical machine learning literature.	The method was evaluated using 5-fold cross-validation. Known RNA‚Äìdisease associations were randomly partitioned into five folds, and in each run, one fold was used for testing while the remaining four were used for training. Reliable negative samples were constructed separately for each fold using the spy technique. No associations from the test set were used during training, ensuring strict separation. Evaluation metrics were averaged across all folds to assess generalization performance.	5.0	0.0	Yes, the full source code is publicly available. It includes training, evaluation, and prediction scripts for the PanGIA model. However, no executable, web server, virtual machine, or container instance is currently provided. The source code is hosted on GitHub at https://github.com/qiankunzizairen/PanGIA and will also be archived in GigaDB upon publication. It is released under an OSI-compliant open-source license (e.g., MIT).	A single forward pass to generate predictions for all RNA‚Äìdisease pairs takes approximately 1 second on an NVIDIA A100 GPU. On a CPU, the same operation may take 15‚Äì30 seconds depending on matrix size. During training, each epoch takes around 8‚Äì12 seconds on A100. Generating predictions and saving ranked results for one fold takes less than 1 minute in total.	The model incorporates both black-box and interpretable components. While deep neural networks like HAN and MMoE are generally considered black-box models, our architecture allows for some interpretability. Specifically, the gating mechanisms in the Multi-gate Mixture-of-Experts (MMoE) reveal which expert networks are activated for each RNA type, offering insights into task-specific patterns. In addition, the cross-task attention weights highlight how different RNA types influence each other. These components provide interpretable signals that can be visualized and analyzed to better understand the learned biological relationships.	The model performs binary classification. For each RNA‚Äìdisease pair, it predicts the probability of association, which is then thresholded to classify the pair as associated or not. The output is a score between 0 and 1 for each pair.	4.0	0.0	"The algorithm belongs to the class of graph neural networks (GNNs) and multi-task learning architectures. Specifically, we use a Multi-gate Mixture-of-Experts (MMoE) framework combined with a Heterogeneous Graph Attention Network (HAN), designed to learn shared and task-specific representations across multiple RNA types (lncRNA, circRNA, miRNA, piRNA). While MMoE and HAN are existing models, this combination is novel in the context of heterogeneous ncRNA‚Äìdisease association prediction.

We chose this architecture because it allows information sharing across related tasks while maintaining specialization through expert selection. Compared to single-task or flat GNN baselines (e.g., GCN, GAT), this structure improved performance and interpretability across all RNA types."	Yes. All hyperparameter configurations, optimization settings, and model files (including trained weights and training scripts) are publicly available in the GitHub repository and will be archived in GigaDB upon acceptance. The GitHub repository includes the training script with command-line arguments for hyperparameters, and configuration values are documented in the README. Trained model files (.pt) and cross-validation scores are included in the `scores/` directory. The code is released under an OSI-compliant open-source license (e.g., MIT or Apache 2.0).	The ncRNA‚Äìdisease data were represented as a heterogeneous graph with multiple relation types (miRNA‚Äìdisease, lncRNA‚Äìdisease, circRNA‚Äìdisease, piRNA‚Äìdisease). Each node type (RNA, disease) was assigned initial feature vectors. For RNAs, one-hot encodings of sequence types or statistical features (e.g., k-mer counts) were used where applicable; for diseases, semantic embeddings derived from the DOID ontology were used. All features were normalized. The adjacency matrix was binarized (1 for known association, 0 otherwise), and reliable negative samples were generated using the spy technique. Masks were used to define training and test regions in the matrix.	Each RNA and disease node is represented by a fixed-length feature vector. For RNAs, the input feature dimension is typically 64 or 128 depending on configuration; for diseases, features were projected to the same dimension using a linear layer. In total, the feature dimension per node is f = 128. No explicit feature selection was performed. All features were retained and used as input. The input features were derived from domain knowledge (e.g., sequence encodings, ontology embeddings) and not learned from the training set alone.	Yes, the number of trainable parameters (p ‚âà 1‚Äì2 million) is larger than the number of training points in each fold, and the feature dimension (f ‚âà 128) is also relatively high. To reduce overfitting risk, we applied multiple strategies: dropout layers were used throughout the model (including attention and expert modules), early layers were regularized, and 5-fold cross-validation was used for evaluation. Additionally, no test data was used during training, and performance was monitored across folds to ensure consistency. The model showed stable generalization across folds, indicating overfitting was effectively controlled.	No, the model does not use the outputs of other machine learning algorithms as input. All predictions are directly generated from the original input features (node attributes and edge structure) within a unified end-to-end architecture. There is no meta-predictor stage, and no stacking or ensembling of separate models was performed. The 5-fold cross-validation ensures strict separation between training and test data.	The model has approximately 1.2 to 2.0 million trainable parameters, depending on the chosen hidden dimensions, number of experts, and expert layer size. The total parameter count includes the HAN encoder, expert networks, gating layers, and attention modules. The values of hyperparameters (e.g., hidden_dim, expert_dim, num_experts, num_heads) were manually selected based on validation performance using 5-fold cross-validation. No exhaustive grid search was conducted; instead, we evaluated a small set of reasonable configurations based on prior work and model capacity considerations.	Yes, multiple regularization techniques were applied to prevent overfitting. Dropout layers were used in both the expert networks and attention modules. L2 regularization was applied implicitly via the optimizer. Additionally, the model‚Äôs multi-task structure (MMoE) encourages parameter sharing and reduces the risk of overfitting to any single RNA type. While early stopping was not explicitly applied, 5-fold cross-validation was used to evaluate generalization, and no test data was used during training.	8.0	0.0	6852b52ae5963d3a24a8dce1	41105014.0	PMC12532321	02/02/2026 19:46:52	Liu X, L√º X, Chen Q, Sun J, Zhao T, Zhu Y.	GigaScience	PanGIA: A universal framework for identifying association between ncRNAs and diseases.	10.1093/gigascience/giaf123	2025	0.0	0.0	Heterogeneous Graph Attention Network, Mixture-of-Experts, ncRNA‚ÄìDisease Association	1.0	2025-06-20T06:51:05.438Z	2026-02-02T19:46:52.000Z	985784d3-cb96-48c8-b833-db4fca62053d	undefined	yi6cmalcqe				DOME_JSON	Match	Match
688fc2386163a9cb4ff9f33f	Yes, the datasets used in this study have been made publicly available for reproducibility and further research. They are hosted on GitHub at https://github.com/wenmm/EssSubgraph/tree/main/data. The data are shared under the GPL v3 license.	The protein-protein interaction (PPI) network was obtained from publicly available databases such as STRING (source of string_net.txt), which is widely recognized and used in bioinformatics research. Node features were generated by performing PCA on a processed TCGA gene expression dataset (cancer_full_expression_pc50.csv) that was compiled and curated by previous studies from the TCGA public resource. Class labels indicating essential (positive) and non-essential (negative) genes were obtained from a summary file from DepMap, a widely used database. The dataset includes 2,299 essential genes (Npos) and 10,723 non-essential genes (Nneg). It is designed for classification tasks and does not contain real-valued outputs for regression. While the cancer_full_expression_pc50.csv feature file was generated in this study, it is from direct transformation of TCGA expression matrix, which is widely used in the cancer research community.	The dataset was split using stratified 5-fold cross-validation to ensure independence between training and test sets in each fold. Specifically, genes were partitioned so that no gene appeared in both training and test sets within the same fold, thus maintaining non-overlapping sets. Similar to our approach, other gene classification methods such as EMOGI and MTGCN (both used as benchmark methods in this study) also employ cross-validation strategies to split data into training and test sets. Cross-validation provides a robust framework for performance evaluation by ensuring that each sample is tested while maintaining independence between training and testing data within each fold. This approach is widely used in the community for gene-level prediction tasks, especially when datasets are limited in size or exhibit class imbalance. Therefore, our use of stratified cross-validation aligns with established practices in similar studies.	"We employed a stratified 5-fold cross-validation strategy to evaluate model performance. 80% of the data were allocated for training and 20% for testing. Within the training subset, 90% was used for final training and 10% for validation, which was utilized for model tuning and early stopping. Although the original label data consist of 2,299 essential genes (Npos) and 10,723 non-essential genes (Nneg), the actual number of genes involved in training varies depending on the protein-protein interaction (PPI) network, as not all genes are present in every network. For instance, in the STRING network, only a subset of genes is retained after filtering based on network inclusion.

Given that non-essential genes outnumber essential genes by more than fourfold, and that the number of available genes varies across networks, direct training would suffer from a class imbalance problem, potentially resulting in poor predictive performance on the minority (essential) class. To mitigate this, we adopted a resampling strategy whereby, for each training, validation, and test split, all essential genes were included alongside randomly selected non-essential genes at a 1:4 ratio. Using the STRING network as an example, this procedure yielded 6,048 genes in the training set, 972 in the validation set, and 1,680 in the test set. Because the splits were dynamically generated in each fold, we did not plot the distributions explicitly, but the stratification process guarantees that the class balance is consistent across training, validation, and test sets."	4.0	0.0	Yes, the raw evaluation files‚Äîincluding assignments for comparison and baselines, statistical code, and data‚Äîare available. They can be found at https://github.com/wenmm/EssSubgraph/tree/main/baseline. We have provided a README file there with instructions on how to run these baseline methods.	We did not evaluate our method on publicly available benchmark datasets, as existing essential gene prediction methods such as XGEP and DeepHE typically use their own specific datasets. However, we applied our method to the dataset used by XGEP and achieved superior performance compared to their results. Additionally, we included comparisons to simpler baseline models to demonstrate the improvements brought by our approach. Although we did not use publicly available benchmark datasets, we performed comparisons on the same dataset with several graph-based models including GCN, GAT, EMOGI, and MTGCN. Additionally, we compared our method against traditional approaches such as DeepWalk and SVM. These comparisons demonstrate the superior performance of our method across multiple baselines.	We used 5-fold cross-validation and conducted 10 rounds, obtaining 50 sets of data. The average performance was shown as a heatmap, and for the data with perturbations, we created a line plot with error bars. The data underlying the line plot with error bars showed statistical significance of the superior performance. 	We report commonly used classification performance metrics, including Area Under the Receiver Operating Characteristic Curve (AUROC) and Precision-Recall Area Under the Curve (PRAUC). These metrics are widely accepted in the literature for evaluating binary classification tasks. Our reported metrics are consistent with those used in comparable studies such as EMOGI and MTGCN (both used as benchmark methods in this study), facilitating direct comparison and demonstrating the competitiveness of our approach.	The method was evaluated using stratified 5-fold cross-validation to ensure robust assessment of model performance. 	5.0	0.0	Yes, the source code was released in github (https://github.com/wenmm/EssSubgraph/tree/main), we provide methods to run the algorithm using both Docker containers and Conda environments. The Docker image and Conda environment files are available in our GitHub repository, released under the GPL v3 license.	Each epoch takes approximately 1‚Äì2 seconds to complete.  Experiments were run on a workstation with an AMD Ryzen Threadripper 3960X 24-core CPU, an NVIDIA GeForce RTX 2080 Ti GPU, and 256 GB of RAM.	The graph neural network (GNN) models used in this study are not inherently interpretable and require additional methods to enhance their interpretability.	The model is a classification model. It is designed to predict whether a given gene is essential or non-essential, which is a binary classification task.	4.0	0.0	The machine learning approach used in this study falls within the class of graph neural networks (GNNs). While graph neural networks themselves are not new, they possess previously unexplored advantages for node classification tasks, especially when integrating multi-omics data and protein-protein interaction networks. This makes GNNs particularly well-suited for essential gene prediction, as they effectively capture both network topology and complex biological features that other methods struggle to model.	Yes, the parameters are reported in the script (https://github.com/wenmm/EssSubgraph/blob/main/EssSubgraph.py). The code is released under the GPL v3 license.	The protein-protein interaction (PPI) network was represented as a graph where nodes correspond to genes and edges represent interactions. Node features were derived from gene expression data, which were first processed by principal component analysis (PCA) performed on TCGA gene expression (gene-sample) matrix curated by previous studies. After PCA, we applied min-max scaling to normalize node feature values to a [0, 1] range. For each gene, its expression-based feature vector was combined with its topology in the PPI network to provide the input for the graph neural network. Label data (essential vs. non-essential) were encoded as binary classes (1 for essential, 0 for non-essential). Prior to training, data splits were performed using stratified cross-validation, ensuring balanced representation of classes. 	We evaluated the test set PRAUC across different PCA dimensions and selected ùëì= 50 features as it achieved the best performance. Given that benchmark methods EMOGI and MTGCN both use 64 features and the performance difference between 50 and 64 features is minimal, we chose 50 from a model performance analysis perspective to provide a clear rationale for our selection.	In our study, the number of trainable parameters p is moderately higher than the number of training samples. For example, under the STRING network setting, the training set contains around 6,000 genes. We only use 50 node features so our model does not fall into the regime where f > 100. To prevent overfitting, we applied several strategies, including stratified cross-validation, early stopping based on validation performance, dropout, and L2 regularization. These techniques help the model generalize effectively. Conversely, to avoid underfitting, we tuned the model‚Äôs complexity (e.g., number of layers and hidden channels). The consistent results across validation folds suggest that the model's capacity is appropriate for the task.	Our model does not use predictions from other machine learning algorithms as input.	There are 95,361 trainable parameters in the model. They are trained with gradient descent and a loss function defined by the deviation from experimental data. The model also includes a set of commonly used hyperparameters such as the number of layers, hidden dimensions, dropout rate, learning rate, L2 regularization weight, and whether to use batch normalization. These settings were kept consistent with previous related studies to ensure fair comparison, and were not extensively tuned for performance optimization.	Yes, we used stratified cross-validation, early stopping based on validation set, dropout, and L2 regularization.	8.0	0.0	688fc238e5963d3a24f1720b	41134033.0	PMC12690466	02/02/2026 19:46:52	Wen H, Carpenter S, McGinnis K, Nelson A, Smith K, Hong T.	GigaScience	EssSubgraph improves performance and generalizability of mammalian essential gene prediction with large networks.	10.1093/gigascience/giaf136	2025	0.0	0.0	Subgraph, Biological Networks, Essential Genes, CRISPR	1.0	2025-08-03T20:10:32.696Z	2026-02-02T19:46:52.000Z	26ed83f5-f80b-484e-ae9e-b89b7020bd80	undefined	05q1xlke1j				DOME_JSON	Match	Match
65f598d51502715bfe53d2d8	The data supporting the results and software in this article are available in the ‚ÄòVirHost‚Äô GitHub repository, (https://github.com/GreyGuoweiChen/VirHost). All data and code is covered under a MIT License. The analytic code is accessible from the GigaDB.	We collected 6,735 RNA viruses from Virus-Host Database and 126,417 records with host annotations from NCBI GenBank, both of which are widely used database. After the careful data preprocessings, including de-replication and removal of ambiguous host annotations, we finally got 14500 RNA viruses, spanning 30 virus orders and 12 host groups, including Invertebrate, Viridiplantae, Fungi, Bacteria, Primates, Rodentia, Artiodactyla, Carnivora, Other mammalia, Aves, Reptilia, and Fish. We also evaluated our method in a set of newly-identified RNA viruses, whose hosts are derived based on experimental evidence. The set includes 126 RNA viruses and were collected from various hosts, including Plant, Invertebrate, Fungi, and Fish. The dataset is complied from 20 publications.	We evalaute the model by cross-validation, leave-one-genus-out experiment and recently-identified viruses, which are not included in the training data. In all datasets, the training and test sets are independent. Before the evaluation, we remove the data redundancy to be less than 90% coverage and 80% identity. In leave-one-genus-out experiment and recently-identified viruses, the sequence identity between training and test set is even lower. 	"We first evalute the learning architecture by stratified 5-fold cross validation. The viruses in corresponding virus orders are stratified splited into 5 folds by their labels, with 4 folds as training set and 1 as test set. We evaluated the models on each fold and average the results. There is no duplicate samples between the training and test datasets. 
Besides, we evaluated the models by conducting leave-one-genus-out experiments. Specifically, we used one genus as test set and viruses from other genuses as training set to assess the generalizability of the models. There are 448 virus genera in total. This experiment is a strong demonstration of the generality of the model; no viruses of the same genus are distributed in both the training and test data. "	4.0	0.0	They are availble in the GigaDB and supplementary information.	Yes, we set the alignment-based method, BLASTN, as the baseline model. We then further comared VirHost with various available learning-based methods and validated the outperforming accuracy of VirHost.	We evaluated all the benchmark models across virus orders by their accuracy distribution. VirHost outperformed any other models from the average accuracy. We also tested confidence of the superior accuracy by one-sided Wilcoxon test, which gave p-value smaller then 0.005	We used accuracy, precision, and prediction rate and the performance metrics in this study. These metrics are widely used in previous studies, like DOI:10.1126/science.aap9072 and doi: 10.1093/bioinformatics/btab33 .	We extensively assessed the models by stratified 5-fold cross evaluation, leave-one-genus-out experiment, and recently-identified novel RNA viuses.	5.0	0.0	It can be access by https://github.com/GreyGuoweiChen/VirHost	It takes around 5 seconds to run the test cases (3 viruses) on a normal desktop PC.	The models are black box models. But we provide the evidence and different confidence level of our predictions to users.	We build hierarchical classification models for each virus order.	4.0	0.0	We designated XGBoost as the learning architecture out of its superiors accuracy and training efficiency. To validate our choice, we conducted an evaluation of several learning architectures, encompassing XGBoost, Gradient Boosting Decision Tree (GBDT), Random Forest (RF), Support Vector Machine with RBF kernel (SVM), Logistic Regression (LR), K-Nearest Neighbors (KNN), and Gaussian Naive Bayes (GNB). Using the scikit-learn and xgboost package, default parameters were employed to train these models, and their performance was assessed using accuracy as the evaluation metric.	"All of the model configuration can be easily access from the scikit-learn and xgboost documentation. If you wanna check the parameters of our models, you may load the models by pickle package and used the command ""model.get_xgb_params()"""	We encode the query sequences by their genomic traits and sequence homology. Specifically, we encode the sequences into a (137+m) dimensional vector as the input to models, where m is the number of host labels in the corresponding virus order and model layer. The genomic traits is vectorized as a 137-dimensional vector. To encode the m feature, we first group the reference viruses by their host labels, and then align the query virus against the reference viruses. The highest similarity score against each group is kept to be the corresponding feature value.	Two groups of feature are used as input, 137 genomic traits and the viral sequence homology. Although we did feature selection, we simply compared the feature contribution and model accuracy between the 137 genomic tratis and the codon pairs groups. This is done in the cross validation experiment. For a fair comparison, we removed all of the codon pair scores without using any relatively important ones, which will not result in the data leakage.	We did not focus on ruling out overfitting or underfitting. However, it is worth noting that the parameter count of XGBoost is typically lower than that of neural networks. Additionally, XGBoost incorporates L2 regularization by default, which helps mitigate the risk of overfitting.	No.	For all of the above-mentioned models, we use the default hyperparameters from scikit-learn and xgboost library to train.	XGBoost incorporates L2 regularization by default, which helps mitigate the risk of overfitting. Besides, we adopted the early stop strategy by evaluating the prediction probability of our models. We set probability cutoff for each model to distinguish predictions with low confidence.	7.0	1.0	65f410ad92c76639b816bef8	39172545.0	PMC11340644	02/02/2026 19:46:52	Chen G, Jiang J, Sun Y.	GigaScience	RNAVirHost: a machine learning-based method for predicting hosts of RNA viruses through viral genomes.	10.1093/gigascience/giae059	2024	0.0	0.0		1.0	2024-03-16T13:04:21.998Z	2026-02-02T19:46:52.000Z	e2ca04b9-38fe-40d6-9fd9-5c8355f4402d	undefined	ta01kc1au8				DOME_JSON	Match	Match
660304301502715bfe53d658	Yes, available at the tool website. No licence specified 	Data for training and testing were extracted from Protein Data Bank (PDB). Npos = 20636/26798/87200 (short/medium/long range residue contacts). Nneg = NA. Dataset firstly introduced in this study.	Redundancy between training and testing set is set to at most 25% pairswise sequence identity. 	Training set: 517 proteins. Test set: 98 proteins. No validation set has been used. 	4.0	0.0	No	Comparison has been perfomed using the test introduced in this study. No baseline approaches tested	No	accuracy in contact prediction at different distance ranges (short, medium and long range contacts)	independent dataset	3.0	2.0	Source code and stanalone version available at the tool website.  License is not specified	not reported	model is black box	classification	3.0	1.0	Naive Bayes combined with neural networks. The algorithm is not new.	Not available	Overall 717 sequence-based features are adopted, including MSA-derived, secondary structure (predicted), solvation (predicted), sequence separation between pairs of residues.	517 features. no feature selection apparently performed.	number of parameters, though not precisely estimated, is much lower than number of data points. Unclear how potential underfitting has been handled.	Yes, it adopts predictions obtained by other contact map prediction methods. It is not clear whether training data used for initial predictor are independent from test data of the meta predictor.	Unclear	unclear	7.0	1.0	6603042f92c76639b849e69f	28369334.0	PMC5860114	02/02/2026 19:46:52	He B, Mortuza SM, Wang Y, Shen HB, Zhang Y.	Bioinformatics (Oxford, England)	NeBcon: protein contact map prediction using neural network training coupled with na√Øve Bayes classifiers.	10.1093/bioinformatics/btx164	2017	0.0	0.0		1.0	2024-03-26T17:21:51.940Z	2026-02-02T19:46:52.000Z	2783c6b5-d21f-4ac6-a776-4235d6631552	undefined	6jexahy331				DOME_JSON	Match	Match
66030aaa1502715bfe53d65c	No	Data are extracted from different databases (TAIR, AtGenExpress Consortium, NASCArrays). Data are in classes. Npos and Nneg are not reported.	Random split has been adopted for cross-validation. No redundancy check performed.	Training set: 11553 data points, No test nor validation sets have been used.  Performance is scored using cross-validation only.	3.0	1.0	No	no comparison with other approaches perfomed	no confidence interval reported. No statistical significance over baselines has been computed	ROC curve	cross validation	4.0	1.0	No	not stated	Model is partially interpretable, since classifier paramenters can be used to assess how gene expression data related to gene response to stress.	classification	2.0	2.0	Different supervised and unsupervised learning algorithms: logistic regression, linear discriminant analysis, quadratic discriminant analysis, naive Bayes, k-nearest neighbors, PCA. No new algorithm developed.	No	Gene expression data	290 features. no feature selection performed.	p is much lower than the number of features. Unclear how potential underfitting was handled	No	Unclear, approximately in the range of the number of features	Not adopted	6.0	2.0	6603042f92c76639b849e69f	17888165.0	PMC2213690	02/02/2026 19:46:52	Lan H, Carson R, Provart NJ, Bonner AJ.	BMC bioinformatics	Combining classifiers to predict gene function in Arabidopsis thaliana using large-scale gene expression measurements.	10.1186/1471-2105-8-358	2007	0.0	0.0		1.0	2024-03-26T17:49:30.048Z	2026-02-02T19:46:52.000Z	600b20de-7c70-41af-ad39-33121af090ef	undefined	ysqyy92zyr				DOME_JSON	Match	Match
6603fdf31502715bfe53d6df	Yes, data available at GitHub	Data taken from literature. Data are in classes. Npos=1735, Nneg=1735. Dataset not previuosly used for ML applications.	Overall redundancy is set to 40% protein pairwise sequence identity at most. 	Training set: 2775 data points. Testing set: 695 data points. No separate validation set used. Distributions of data are consistent between training and testing.	4.0	0.0	No	comparison with other methods performed on the benchmark dataset introduced in this study. No baseline method included in the benchmark	No	standard mesures for classification: accuracy, sensitivity, specificity, F1 and MCC	cross-validation and independent dataset	3.0	2.0	yes, source code is available at GitHub. No License is present.	not reported	model is black box	classification	3.0	1.0	Dimensionality reduction (PCA) followed by classifier (SVM). The algorithm is not new. 	no	Peptides are represented using: spaced dipeptide composition, physicochemical featurs, one-hot encoding	f=614 before PCA, f=100 after PCA. 	Npos+Nneg>p. No specific strategies to rule out underfitting	No	p=100 (SVM)	No overfitting prevention strategies seem to have been applied	7.0	1.0	6603042f92c76639b849e69f	33225896.0	PMC7682087	02/02/2026 19:46:52	Liu X, Wang L, Li J, Hu J, Zhang X.	BMC genomics	Mal-Prec: computational prediction of protein Malonylation sites via machine learning based feature integration : Malonylation site prediction.	10.1186/s12864-020-07166-w	2020	0.0	0.0		1.0	2024-03-27T11:07:31.027Z	2026-02-02T19:46:52.000Z	7cb19427-d4cb-4038-8225-3fabb2f4f585	undefined	2b9h7u472x				DOME_JSON	Match	Match
66040f611502715bfe53d6e7	yes (URL)	Data taken from SCOP database (release 1.69). Multi-class data where each class corresponds to a SCOP superfamily. Number of superfamilies is 74 and 1458 in in Set A  and Set B, respectively.  Number of proteins is 643 and 2182 in Set A and Set B, respectively.	Testing set should be independent, given that split has been perfomed at the level of SCOP superfamilies	Dataset A: N_train=543 (proteins), N_test=110 (proteins) ; Dataset B: N_train=1740 (proteins), N_test=442 (proteins)	4.0	0.0	yes, through the author website	Yes, with a single external tool (AutoSCOP)	not reported	Per-class error rate	cross-validation and independent test set	4.0	1.0	yes, thrugh the author website	not reported	blackbox	multi-class classification	3.0	1.0	Combined kNN and SVM. 	yes, hardcoded into the source code	Protein primary sequence. Profile kernel is used for SVM	Unclear	Cannot be determined	No	Not clearly declared.	No  	6.0	2.0	6603042f92c76639b849e69f	18808707.0	PMC2561051	02/02/2026 19:46:52	Melvin I, Weston J, Leslie CS, Noble WS.	BMC bioinformatics	Combining classifiers for improved classification of proteins from sequence or structure.	10.1186/1471-2105-9-389	2008	0.0	0.0		1.0	2024-03-27T12:21:53.450Z	2026-02-02T19:46:52.000Z	804c2b2b-e663-4d3a-8042-7d2a35ad2122	undefined	vwy8sti3pw				DOME_JSON	Match	Match
66041e5d1502715bfe53d70a	No	Data is extracted from the PDB. Data are in classes (alpha-helix vs other), however, it is not clear ho many alpha-helix residues (positive example) are present within the 300 proteins in the dataset. Dataset previsouly used in other studies and recognized by the community (no logner available)	All proteins in the dataset are non-homologous all-alpha proteins. Unclear how redundancy reduction is performed.	Traning set: 150 proteins; Testing set: 150 proteins. Distributions of data classess in training and testing not shown. No validation set adopted. 	3.0	1.0	Not available	Cmparison with other approaches is missing. No baseline method is considered	Not reported	accuracy and segment-overlap value for alpha helix (SOVa)	Repeated random traing/test split	5.0	0.0	No	No	Model is interpreatable, since paramenters learning by the SVM corresponds to interpretable paramenters of an energy function	binary classification	2.0	2.0	support vector machine classifyer. The algorithm is not new.	No	Residue are encoded with different propensity scale for a residues to by in alpha-helix or coil	f=380. No feature selection performed. 	p is much lower than number of training points. Not clear how potential undefitting has been ruled out	No	p=380	No regularization applied. Validation set not used.	6.0	2.0	6603042f92c76639b849e69f	17570862.0	PMC1892091	02/02/2026 19:46:52	Gassend B, O'Donnell CW, Thies W, Lee A, van Dijk M, Devadas S.	BMC bioinformatics	Learning biophysically-motivated parameters for alpha helix prediction.	10.1186/1471-2105-8-s5-s3	2007	0.0	0.0		1.0	2024-03-27T13:25:49.790Z	2026-02-02T19:46:52.000Z	b863eb51-d9ae-4fc0-bfd4-006db90d1631	undefined	qx3ex71jye				DOME_JSON	Match	Match
6613fe1f1502715bfe53d929	Yes. Supporting Information.	"Source of positive miRNAs: TarBase (Papadopoulos et al., 2009) and miRecords database (Xiao et al., 2009, Nucleic Acids Res., 37, D155‚ÄìD158, Nucleic Acids Res., 37, D105‚ÄìD110)  
A set of 289 miRNA transcript pairs (positive examples), and 289 negative examples were used as training dataset (total training data points npos + nneg =578).  
The positive data had been used previously in other papers. The negative data are novel, and have been identified by the Authors by means of a complex analysis. A subset of the negative examples has been validated experimentally."	Not reported	The SVM model with RBF kernel was generated by a 10-fold cross-validation on the training dataset.  The distribution of + and - in each data split is not mentioned, but it can be deduced to have been half and half.	4.0	0.0	I could not find them.	Ten other methods (DIANA-microT, Micro Inspector, miRanda, MirTarget2, NBmiRTar, PicTar, PITA, RNA22, RNAhybrid, TargetScan) have been applied by the Authors to the same test data, reaching the conclusion that the better performance of their own method was probably due to the better negative training set they were able to individuate.	No confidence intervals are reported for any performance measure.	Sensitivity, specificity (ROC curves), ACA (Average Classwise Accuracy) and MCC (Matthews Correlation Coefficient).	Independent dataset, experimentally validated, composed of 246 miRNA‚Äìtranscript pairs, of  which 187 were positive, 59 negative. 	5.0	0.0	SVM software is standard.  TargetMiner is available as an online tool at https://www.isical.ac.in/~bioinfo_miu/targetminer20.htm.     Executables are given .  Data files of test sets are available.	Not reported	"SVM is generally considered black box. However, some ante hoc interpretability is manifested in the feature generation step, which was based on miRNA / 3'-UTR mRNA hybridization characteristics, such as seed length and identification, thermodynamic duplex stability, combinatorial effect of miRNAs and multiple target sites in the 3-UTRs of target mRNA, importance of miRNA‚Äôs outseed segment to determine the target specificity, analysis of evolutionary conserved target sites.  Post-hoc analysis of single features effectiveness was not performed.  Indeed, that is in agreement with the Author's statement, ""Since our objective in this article is to demonstrate the utility of systematically identifying the negative examples, we have kept the feature selection part simple. A more sophisticated approach is expected to improve the performance and will be considered in the future""."	Binary classification (target or non-target genes for miRNAs).	4.0	0.0	Support Vector Machine (SVM). 	TargetMiner is available as an online tool at https://www.isical.ac.in/~bioinfo_miu/targetminer20.htm.       The optimized parameter values for C and Œ≥ are given in text.  Both the 90 and 30 feature sets are explicitly reported in the Supplementary Materials, together with the identification numbers of the 578 data points.  	kmer for seed region, base pairing frequencies in the nearby regions.	Two SVM models were build, based on the 90 and 30 (top 30 F-score) features, respectively. The two SVM classifiers are referred to as TargetMiner* and TargetMiner.  The selection of the 30 top features was based on an F-score, which indicated the discriminating power of a given feature between positive and negative examples. The F-score was calculated based on the training set only.	The number of features was less than 100, but not small (30 or 90), compared to the number of training points (n = 578). 	No.	The two standard RBF SVM hyperparameters C and Œ≥ were determined by a grid search with a 10-fold cross-validation on the training dataset. 	The parameter C was standardly used to control the tradeoff between training error and margin. 	7.0	1.0	65faa4f792c76639b82bab29	19692556.0		02/02/2026 19:46:52	Bandyopadhyay S, Mitra R.	Bioinformatics (Oxford, England)	TargetMiner: microRNA target prediction with systematic identification of tissue-specific negative examples.	10.1093/bioinformatics/btp503	2009	0.0	0.0		1.0	2024-04-08T14:24:31.559Z	2026-02-02T19:46:52.000Z	b05d78bd-b870-43e2-8313-2c6e8cc7a91a	undefined	vqpw9fqmuu				DOME_JSON	Match	Match
6634f1d6ded6e7820f74a18b	No	"The source of the data is 250,000 randomly selected molecules from ZINC
This subset was already used by Go¬¥mez-Bombarelli et al. (2018).
Pairs of molecules were constructed to satisfy the following criteria: 
(i) identical heavy atom count and counts of specific heavy atoms (C, N, O, S, Cl, F)
(ii) high similarity in property-space
(iii) low structural similarity"	Test sets are independent from the training data.	"Authors used two different test sets, DUD-E with 102 targets and DEKOIS 2.0 with 80 targets, and prepared different size training sets accordingly.
This resulted in a training set of 131 199 pairs for DUD-E and 103 170 for DEKOIS 2.0.
Authors selected 1000 pairs for model validation and used the remainder to train the model."	3.0	1.0	No	No	Yes, confidence intervals are reported. The generated decoys by the model improved the Deviation from Optimal Embedding (DOE) score by an average of 81% and 66%, respectively, decreasing from 0.166 to 0.032 for DUD-E and from 0.109 to 0.038 for DEKOIS 2.0.	The reported performance metrics include the DOE (deviation from optimal embedding) score, doppelganger score, and AUC ROC from predictive models and virtual screening performance.	The model was evaluated on independent SBVS datasets DUD-E and DEKOIS 2.0.	3.0	2.0	No	No	No mention was made on interpretability. The model appears to be a black box.	It is a generative model.	2.0	2.0	"It is deep learning method using graph neural networks.
It is not a novel algorithm."	No	The data were encoded as graphs representing molecules. The model was trained using pairs of molecules, framing decoy generation as a multimodal graph-to-graph translation problem.	No	No	It is not a meta-predictor.	No	No	3.0	5.0	65e73fdb92c76639b8e309f3	33532838.0	PMC8352508	02/02/2026 19:46:52	Imrie F, Bradley AR, Deane CM.	Bioinformatics (Oxford, England)	Generating property-matched decoy molecules using deep learning.	10.1093/bioinformatics/btab080	2021	0.0	0.0		1.0	2024-05-03T14:16:54.445Z	2026-02-02T19:46:52.000Z	56d33311-36d5-4be6-9a3f-2b732a2b4c63	undefined	fbkgc98sns				DOME_JSON	Match	Match
6634f285ded6e7820f74a19a	"Available from the corresponding author on ""reasonable request""."	"The authors used structures in PDB as the main data source.
They clustered the structures based on different sequence identity thresholds. The resulted structure dataset consists of:
10173 (30% sequence identity), 14064 (50% sequence identity), and 17607 structures (90% sequence identity)."	The authors trained their model on 3 redundancy-reduced datasets with 3 different sequence identity thresholds (30%, 50%, and 90%).	"For training the model, the authors used a random split 5-fold cross-validation.
The proportion of training and test data points used in cross-validation was not reported."	4.0	0.0	No	The model was compared to a similar method called SPIN.	Standard deviation was reported. The results are not significantly better than the only method that was compared to (SPIN) with 3% improvement.	"The authors used the accuracy of top-k predictions from the model as the evaluation metric.
Top-K accuracy: if the native amino acid is within the top-K predictions (K amino acids that have the highest probabilities), the prediction is considered correct."	"1. The model was solely evaluated on 3 proteins that are as follows:
An all-Œ± protein (PDB ID 2B8I60), an all-Œ≤ protein (PDB ID 1HOE61), and a mixed Œ±Œ≤ protein (PDB ID 2IGD).
2. The model was evaluated and compared to another method on a data set of 50 proteins."	4.0	1.0	No	No	No mention on interpretability was made. The model appears to be a black box.	Ultimately, it is a classifier method that outputs the potential residue types at a target position based on the probability of 20 amino acids for that specific position.	2.0	2.0	The algorithm used is a classic neural network. 	No	Data was encoded by extracting features from protein residues and their N closest neighboring residues.	"The following input features were reported:
1. Cos and sin values of backbone dihedrals
2. Total solvent accessible surface area (SASA) of backbone atoms
3. Three-type secondary structure
4. CŒ±-CŒ± distance to the central residue.
5. Unit vectors from the central residue to the neighbor residues
6. Number of backbone-backbone hydrogen bonds."	No	It is not a meta-predictor.	"The following parameters were reported:
1. Activation function: ReLU for all layers.
2. Loss function: Categorical cross entropy.
3. Optimization: SGD with Nesterov momentum (0.9) and learning rate of 0.01.
4. Batch size: 40,000.
5. Sample weighting: Adjusted based on residue type abundance
6. Epochs: 1000"	No	5.0	3.0	65e73fdb92c76639b8e309f3	29679026.0	PMC5910428	02/02/2026 19:46:52	Wang J, Cao H, Zhang JZH, Qi Y.	Scientific reports	Computational Protein Design with Deep Learning Neural Networks.	10.1038/s41598-018-24760-x	2018	0.0	0.0		1.0	2024-05-03T14:19:49.660Z	2026-02-02T19:46:52.000Z	a2eb5814-8cb1-4c0d-a2aa-96fda79bcc2d	undefined	rejm27trh8				DOME_JSON	Match	Match
6634f443ded6e7820f74a1ac	The PROTACs data used in this study are available in the public database of PROTAC-DB (http://cadd.zju.edu.cn/protacdb/).	"The source of the data is PROTAC molecules that are either present on PROTACS-DB, an online database, or gathered from ""other public sources"".
The size of the whole dataset used is 2832.
It was further labeled as 988 positive and 1844 samples."	"Data was split randomly and no mention on redundancy reduction was made.
The whole dataset is considered to be small in size and, to my knowledge, no previous ML dataset does exist for this purpose."	"For optimization purpose, they randomly split the data into the training set, validation set, and test set at a ratio of 8:1:1.
After optimization, they used a 8:2 ratio for training:test.
Data was split randomly and no mention was made on the distribution of the negatives/positives different divisions."	4.0	0.0	No	Only comparison to SVM and RF models (developed by authors) was done.	No	Average accuracy and AUROC are the reported performance metrics.	"The model was eventually evaluated on a test set, after optimization on a validation set in another experiment beforehand.
Authors further validated the model by using a batch recently reported PROTACs."	3.0	2.0	They used a validation set to optimize hyperparameters. The source code of DeepPROTACs and associated data preparation scripts are available at github (https://github.com/fenglei104/ DeepPROTACs) 98. The final DeepPROTACs model is also provided.	No	No mention on interpretability of the model was made. Due to many layers of input encoding and using neural networks, the model appears to be a black box.	It is a binary classifier.	3.0	1.0	"The algorithm is a Multi Layer Perceptron (Neural Networks).
Authors compared the algorithm with SVM and RF, and they got better results with Neural Networks."	They used a validation set to optimize hyperparameters. The source code of DeepPROTACs and associated data preparation scripts are available at github (https://github.com/fenglei104/ DeepPROTACs) 98. The final DeepPROTACs model is also provided.	"Authors analyzed each PROTAC molecule from through 5 aspects: POI Pocket, E3 Pocket, Warhead, E3 ligand, and Linker. The first 4 are preprocessed and encoded by Graph Convolution and Max Pooling layers. As mentioned,  to process the data regarding the SMILES format of the ""linker"" part in PROTAC molecules, they used embeddings followed by bidirectional LSTM and a fully connected layer."	"For each aspect of the PROTAC molecule (5), 64 features were extracted to be fed into the main neural networks at the end.
No mention was made on feature selection"	The number of reported parameters is not bigger than training points. Authors used over-sampling method, compared to normal sampling and under-sampling, to address the imbalance in the data.	"As a part of data encoding, to process the data regarding the SMILES format of the ""linker"" part in PROTAC molecules, they used embeddings followed by bidirectional LSTM and a fully connected layer."	"Overall 17 parameters were reported by Authors.
Parameters were optimized on validation set."	Authors used over-sampling method, compared to normal sampling and under-sampling, to address the imbalance in the data.	8.0	0.0	65e73fdb92c76639b8e309f3	36414666.0	PMC9681730	02/02/2026 19:46:52	Li F, Hu Q, Zhang X, Sun R, Liu Z, Wu S, Tian S, Ma X, Dai Z, Yang X, Gao S, Bai F.	Nature communications	DeepPROTACs is a deep learning-based targeted degradation predictor for PROTACs.	10.1038/s41467-022-34807-3	2022	0.0	0.0		1.0	2024-05-03T14:27:15.320Z	2026-02-02T19:46:52.000Z	2a384a79-d63e-4e6d-9930-10b3a40c2718	undefined	l5alam5gs2				DOME_JSON	Match	Match
6634f47fded6e7820f74a1b0	No	"3 manually built dataset pairs were used by authors to construct prediction models for drug-likeness.
Dataset pair | Number of positive | Number of negative | Total
WDI/ACD | 38,260 | 288,540 | 326,800
MDDR/ZINC | 171,850 | 199,220 | 371,070
WORLDDRUG/ZINC | 3,380 | 199,220 | 202,600

MDDR (MACCS-II Drug Data Report [MDDR], 2004), WDI (Li et al., 2007), ACD (Li et al., 2007), ZINC (Irwin et al., 2012; Sterling and Irwin, 2015), ZINC WORLD DRUG (Sterling and Irwin, 2015)"	"Authors randomly split the datasets.
The training and test sets are not independent.
They removed the duplicates appearing in both sets.
"	"Authors randomly split the datasets on the proportion of 9:1 as training set and ""validation"" (apparently used as the test set).
No mention on stratified sampling to ensure the same distribution of positive and negative classes in the splits."	3.0	1.0	No	Accuracy was compared to a SVM model built by Li et al (Li et al., 2007).	No	All models were evaluated by five indexes. The Accuracy, Specificity, Sensitivity, Matthews correlation coefficient (MCC), and area under the receiver operating characteristic curve (AUC).	"They used a 5 fold cross-validation.
The evaluation was not performed on an independent set.
"	3.0	2.0	No	No	No mention on interoperability. It appears to be a black box.	It is a classifier model.	2.0	2.0	Algorithm is an autoencoder neural network.	No	Authors used 2D descriptors to encode the molecules. Molecules after preprocessing were calculated by MOLD2 (Hong et al., 2008), resulting a descriptor matrix of ‚àº700 descriptors per molecule.	‚àº700 descriptors per molecule.	The exact number of parameters not reported. As a strategy against overfitting, authors tried to optimized the weight of the positive and negative sample loss of the logarithmic likelihood loss function.	It is not a meta-predictor.	"Reported table of the hyperparameters:
Initializer = TruncatedNormal
Number of hidden layers = 1
Number of hidden layer nodes = 512
L2 Normalization term = 1e-4
Dropout rate = 0.14
Activation = Relu
Batch size = 128
Optimizer = Adam
Loss = mse for AE, binary crossentropy for classifier"	"As a strategy against overfitting, authors tried to optimized the weight of the positive and negative sample loss of the logarithmic likelihood loss function. They also used early stopping based on classification ACC on the ""test"" set."	7.0	1.0	65e73fdb92c76639b8e309f3	30538725.0	PMC6277570	02/02/2026 19:46:52	Hu Q, Feng M, Lai L, Pei J.	Frontiers in genetics	Prediction of Drug-Likeness Using Deep Autoencoder Neural Networks.	10.3389/fgene.2018.00585	2018	0.0	0.0		1.0	2024-05-03T14:28:15.854Z	2026-02-02T19:46:52.000Z	8106fd96-ea13-4cb8-84f9-4ac85aba9559	undefined	6qvzmhfton				DOME_JSON	Match	Match
6634f4c2ded6e7820f74a1b4	All data used for training and evaluation is publicly available through the PDB (https://www.rcsb.org/). 	Protein Data Bank was used as the source data for training, validation, and test.	"Authors adopted a time split strategy to create an independent test set.
The data was clustered using a 1‚Äâ√ó‚Äâ10‚àí3 hhblits E-value for proteins and 80% sequence identity for RNA molecules."	"For training and validation sets, A dataset was constructed considering all PDB structures published at or before 30 April 2020, consisted of 7,396 RNA chains and 23,583 Protein-NA complexes.
For Test set, all structures published to the PDB from May 2020 or later, consisted of 91 complexes with one protein molecule plus a single RNA chain or DNA duplex, 43 cases with a single RNA chain and 106 cases with more than one protein chain or more than a single RNA chain or DNA duplex."	4.0	0.0	No	The predictions of the model were compared to AlphaFold (protein structure prediction method), Hdock (protein‚ÄìDNA docking method), and methods for RNA prediction such as DeepFoldRNA, FARFAR2, and AIchemy_RNA.	Confidence metrics were used and reported tailored to different tasks.	Local Distance Difference Test (lDDT),  fraction of native contacts (FNAT), mean interface predicted aligned error (PAE), CAPRI metrics, RMS, and plDDT.	The model was evaluated on an independent test set.	4.0	1.0	Source code and a link to the training weights have been made available at https://github.com/uw-ipd/RoseTTAFold2NA. The model can be used through a conda environment.	No	The model appears to be a black box as a deep neural network with 67M parameteres.	It is a structure prediction model that has some resemblance to a regressor in the sense that it predicts the spatial arrangement of atoms.	3.0	1.0	The model is a neural network-based approach adopted from RoseTTAFold.	Source code and a link to the training weights have been made available at https://github.com/uw-ipd/RoseTTAFold2NA.	The data is encoded as sequence (1D), residue pair (2D) and structural (3D) representations of protein‚Äìnucleic acid complexes. This makes it compatible with the three-track architecture of RoseTTAFoldNA.	The model simultaneously refines three representations of a biomolecular system: sequence (1D), residue-pair distances (2D) and cartesian coordinates (3D).	p is much larger than the number of training points.	It is not a meta-predictor.	The model has ‚àº67‚ÄâM parameters and made up of 36 total layers.	To improve generalizability of protein‚ÄìDNA interactions, authors added a few ways of ‚Äòrandomizing‚Äô inputs during training. They also performed negative training.	8.0	0.0	65e73fdb92c76639b8e309f3	37996753.0	PMC10776382	02/02/2026 19:46:52	Baek M, McHugh R, Anishchenko I, Jiang H, Baker D, DiMaio F.	Nature methods	Accurate prediction of protein-nucleic acid complexes using RoseTTAFoldNA.	10.1038/s41592-023-02086-5	2023	0.0	0.0		1.0	2024-05-03T14:29:22.628Z	2026-02-02T19:46:52.000Z	a80c9c2f-d151-4ef4-8aaf-28dfedcf8b7c	undefined	nsqtgf6bjv				DOME_JSON	Match	Match
6634f526ded6e7820f74a1b8	The data and associated scripts available at https://github.com/HannesStark/EquiBind.	"Authors used protein-ligand complexes from PDBBind in a time split manner.
The newest version, PDBBind v2020, contains 19,443 protein-ligand complexes with 3,890 unique receptors and 15,193 unique ligands.
"	A time split strategy was adopted by the authors.	"Of the 19 119 preprocessed complexes, 1512 were discovered in 2019 or later. From these, they randomly sampled 125 unique proteins and collected all new complexes containing them (363) to create the final test set.
From the remaining complexes that are older than 2019, we remove those with ligands contained in the test set, giving 17,347 complexes for training and validation. These are divided into 968 validation complexes, which share no ligands with the remaining 16,379 train complexes."	4.0	0.0	Code to reproduce results with the provided model weights is available at https://github.com/HannesStark/ EquiBind.	"The model was compared to similar software and methods including QVINA, GNINA, SMINA, and GLIDE.
Also the combinations of the model with SMINA and QVINA was included in the benchmark."	MEAN, MED, 25TH, 50TH, 75TH, % below 5 and 2 AÀö was reported for the mentioned evaluation metrics. While the standard model performs relatively well, in most metrics the best results are obtained when EQUIBIND is combined with SMINA for fine-tuning.	Authos used the ligand root mean square deviation (L-RMSD), the centroid distance, and the KabschRMSD as evaluation metics.	The evaluation was carried out on an independent test set in two experiments: 1. Flexible blind self-docking and 2. Blind re-docking	5.0	0.0	The model and its source code is available at https://github.com/HannesStark/ EquiBind.	"In ""Flexible blind self-docking"" experiment, Standard EQUIBIND takes on average 0.16 second on 16 CPU cores and 0.04 second on a 6GB GTX 1060 GPU to process a task."	No mention was made on the interoperability of the model. However, due to the complexity of the model, it appears to be a black box.	The model is close to a regression model by predicting the coordinates of the ligand-protein binding site alongside the bond angles and lengths of the ligand molecule.	4.0	0.0	It is a geometric deep learning model.	Code to reproduce results or perform fast docking with the provided model weights is available at https://github.com/HannesStark/ EquiBind.	Both input molecules (ligand & receptor) are represented to the model as spatial k-nearest neighbor (k-NN) graphs.	"For the Œ±-carbons in the receptor graph, authors used the residue type as a feature. The edges have two attributes.

In the ligand, the edges have features that are encoded in the same fashion as for the receptor. Meanwhile, the atoms have the following features: atomic number; chirality; degree; formal charge; implicit valence; the number of connected hydrogens; the number of radical electrons; hybridization type; whether or not it is in an aromatic ring; in how many rings it is; and finally, 6 features for whether or not it is in a ring of size 3, 4, 5, 6, 7, or 8."	p is not larger than the number of training points. 	The model combines Graph Matching Networks and E(3)-Equivariant Graph Neural Networks.	"Overall 15 parameters were reported.
Authors used search space strategy to obtain a strong performance on the validation set."	"Authors optimized the model using ""Adam"" and did early stopping with patience of 150 epochs based on the percentage of predicted validation set complexes with an RMSD better than 2 A."	8.0	0.0	65e73fdb92c76639b8e309f3			02/02/2026 19:46:52	Hannes St√§rk, Octavian-Eugen Ganea, Lagnajit Pattanaik, Regina Barzilay, Tommi Jaakkola	arXiv	EQUIBIND: Geometric Deep Learning for Drug Binding Structure Prediction	https://doi.org/10.48550/arXiv.2202.05146	2022	0.0	0.0		1.0	2024-05-03T14:31:02.390Z	2026-02-02T19:46:52.000Z	242800d0-70de-4409-9b2f-bff40afacc32	undefined	skg7vifrfy				DOME_JSON	Match	Match
6638a35eb30933003cc215bd	The code and network weights of Merizo are available at https://github.com/psipred/Merizo	"The PDB chains and domain annotations used for training were accessed from version 4.3 of the CATH database.
The final training and testing set contained 17,287 and 663 chains respectively.
To fine-tune the model, 7502 and 1195 AFDB-human models were used for the training and testing sets, respectively"	Further redundancy filtering with CD-HIT39 was performed to cluster targets which had a sequence identity of greater than 99%.	The authors devised a training-test split which did not overlap at the CATH homologous superfamily (H) level  to better gauge performance on folds that the network has not seen before.	4.0	0.0	No	"The benchmark compares the accuracy of domain assignments by Merizo against those produced by four recently published methods including DeepDom, a CNN-based method from Eguchi et al (referred to as Eguchi-CNN), SWORD and UniDoc.
They also included four baseline measures, including scoring ECOD assignments against CATH (where ECOD assignments are treated as a prediction result), and three random assignment methods prefixed with‚ÄôRandom‚Äô, where the domain count is estimated according to the Domain Guess by Size method."	Yes, confidence intervals are reported. Merizo is the most performant method on the CATH-663 set when scoring by IoU. It is followed closely by UniDoc, which has a wider distribution.	"They scored predictions based on (1) how well the residues in a predicted domain overlap with a true domain, measured via the intersect-over-union (IoU) between residues in the predicted and
ground-truth domain, and (2) how precise the predicted domain boundaries are, when assessed using the Matthews Correlation Coefficient (MCC)."	"They evaluated the model on a test split which did not overlap at the CATH homologous superfamily (H) level with the training set.
No mention was made of cross-validation."	4.0	1.0	It will be incorporated into the PSIPRED workbench at http://bioinf.cs.ucl.ac.uk/psipred/	Average time per target (second) on GPU: 0.112	The model is a deep neural network with 37 M parameters, rendering it a black box.	The model has elements of both classification and regression tasks.	4.0	0.0	"The model is a deep neural network.
It is not a novel algorithm."	The model is accessible through https://github.com/psipred/Merizo	The model takes three inputs: a single representation, pairwise representation and backbone frames. The single representation is produced by one-hot encoding the primary sequence into 20 amino acid classes and then projected into 512 feature dimensions. For the pairwise representation, authors used the pairwise distance map derived from alpha carbons, directly embedded into 32 feature dimensions as continuous values using a linear layer. Finally, the Euclidean backbone frames are calculated from each residue ‚Äúframe‚Äù (N-CA-C atoms) via Gram-Schmidt orthogonalization.	The model takes 3 inputs with varying feature dimensions.	P is much larger than the number of training points. The authors devised a training-test split which did not overlap at the CATH homologous superfamily (H) level  to better gauge performance on folds that the network has not seen before.	It is not a meta-predictor.	The model is a small encoder-decoder network with approximately 37 M parameters (20.4 M in the encoder and 16.8 M in the decoder).	No	7.0	1.0	65e73fdb92c76639b8e309f3	38114456.0	PMC10730818	02/02/2026 19:46:52	Lau AM, Kandathil SM, Jones DT.	Nature communications	Merizo: a rapid and accurate protein domain segmentation method using invariant point attention.	10.1038/s41467-023-43934-4	2023	0.0	0.0		1.0	2024-05-06T09:31:10.078Z	2026-02-02T19:46:52.000Z	5231538c-a267-4ece-a6dd-484c57f5f1d8	undefined	nfj5rdqggm				DOME_JSON	Match	Match
6638a38fb30933003cc215c1	The data used for training and testing are available at https://services.healthtech.dtu.dk/service.php?DeepLoc-2.0	"The authors used 3 different datasets for different purposes.
1. Localization dataset for training purpose, obtained from SwissProt:
Cytoplasm =  9870 
Nucleus =  9720
Extracellular =  3301 
Mitochondrion =  2590 
Cell membrane =  4187 
Endoplasmic reticulum (ER) =  2180 
Plastid =  1047 
Golgi apparatus =  1279 
Lysosome/Vacuole =  1496 
Peroxisome =  304

2. Localization dataset for testing purpose, obtained from The Human Protein Atlas:
Nucleus =  893
Cytoplasm =  562
Cell membrane =  287
Mitochondrion =  196
Golgi apparatus =  86
Endoplasmic reticulum (ER)  =  77

3. Sorting signals dataset, curated from literature:
Signal Peptides (SP)  =  1011
Transmembrane domains (TM)  =  260 
Mitochondrial transit peptide (MT)  =  242 
Chloroplast transit peptide (CH)  =  90 
Thylakoidal lumen composite transit peptide (TH) 42 
Nuclear localization signal (NLS) 148 
Nuclear export signal (NES) 100 
Peroxisome targeting signal (PTS) 127"	"Authors ensured that each pair of train and test fold does not share sequences that have global sequence identity greater than 30%.
"	"1 and 3 datasets are used in a 5-fold cross-validation after homology based partitioning.  
dataset 2 is completely used for evaluation purpose."	4.0	0.0	No	The model was compared to other similar methods including YLoc+, DeepLoc 1.0,  Fuel-mLoc, and LAProtT5	Standard deviations are reported.	"Metrics for quantifying the performance in prediction of subcellular localization:
Number of predicted labels, Accuracy, Jaccard, MicroF1, MacroF1, Matthews Correlation Coefficient

For quantifying the relevance of attention to the sorting signals:
Importance in signal: Total attention mass present within the signal.
Signal over background: The average attention value within the signal over the average value outside the signal.
Metric Entropy: The entropy of the attention normalized by the information length of the protein. 
KL-Divergence: Distributional dissimilarity between the signal and attention"	An independent dataset (3) was used for the evaluation purpose.	4.0	1.0	It is available through a webserver: https://services.healthtech.dtu.dk/services/DeepLoc-2.0/	"Based on the language model used, webserver estimated time per sequence are as follows:

Short sequences (Average length: 104) for ESM1b and ProtT5 respectively:
Model load time (s) 11.07 and 26.80 
Prediction time (s / seq) 0.83 3.93
Plot time (s / seq) 2.38 2.57

Long sequences (Average length: 400) for ESM1b and ProtT5 respectively:
Model load time (s) 11.09 26.09
Prediction time (s / seq) 3.29 7.33
Plot time (s / seq) 7.94 7.97"	After encoding the sequences with protein language models, an interpretable attention pooling mechanism is used to produce sequence representations.	The model is a multi-label classification model.	4.0	0.0	"The prediction stage consists of two multi-layer perceptron (MLP) classifier heads.
The first head is trained along with the learnable vector from the attention step for the ten-class multi-label subcellular localization task.
A second head is trained after freezing the rest of the parameters for the nineclass sorting signal prediction task"	No	Then using an interpretable attention pooling mechanism a sequence representation is produced. The two prediction heads then utilize this representation to predict multiple labels for both the 10-type subcellular localization and 9-type sorting signal prediction tasks.	No	Discrete-Cosine Transform was used to regularize the attention-pooling layer.	"DeepLoc 2.0 uses a transformer-based protein language model to encode the input amino acid sequence.
For this purpose, they evaluated three publicly available transformer models:
The 12-layer ESM model with 84M parameters
The 33-layer ESM model with 650M parameters
The 3B parameter ProtT5-XL-UniRef50model"	No	No	4.0	4.0	65e73fdb92c76639b8e309f3	35489069.0	PMC9252801	02/02/2026 19:46:52	Thumuluri V, Almagro Armenteros JJ, Johansen AR, Nielsen H, Winther O.	Nucleic acids research	DeepLoc 2.0: multi-label subcellular localization prediction using protein language models.	10.1093/nar/gkac278	2022	0.0	0.0		1.0	2024-05-06T09:31:59.998Z	2026-02-02T19:46:52.000Z	c4675347-1a80-45e3-b55c-1e43e2e94e91	undefined	63a7xx35gw				DOME_JSON	Match	Match
6638a3d4b30933003cc215c5	All datasets are available from the SBGrid data repository https://data.sbgrid.org/dataset/843/	"4 different datasets were used in two different applications of the model, all of them obtained from other publications.

Application 1: scoring of docking models 
BM5 = 3,592,600 models generated from 142 dimers using HADDOCK
CAPRI = 16,666 models generated from 13 protein dimers by over 40 different research teams using a variety of software

Application 2: binary classification of biological and crystal dimers in 50/50 proportions 
MANY = 5739 dimers
DC = 161 dimers"	"The splits have been done randomly by sklearn StratifiedKFold tool in an independent way.
No mention on redundancy reduction by pairwise identity."	"BM5 = 10% test, 90% training and evaluation in a 10-fold cross validation that each fold is splitted 80-20 for training and evaluation, respectively.
CAPRI  = completely used to benchmark the trained model on BM5 with other software
MANY = 80-20 for training and evaluation respectively
DC = completely used for test purpose from the best model trained on MANY"	4.0	0.0	No	in both applications, the model was compared and benchmarked to other software, including their own previous model.	Confidence intervals are reported for AUC's.	"Area under the ROC Curve (AUC), MSE, R2, Accuracy, TP rate, and TN rate were reported in Application 1.
Accuracy, Specificity, Sensitivity, and Precision metrics were reported in Application 2."	"Application 1: The model was trained with cross-validation on BM5 dataset and tested on CAPRI dataset.
Application 2: The model was trained on MANY dataset without cross-validation, and tested on DC dataset."	4.0	1.0	DeepRank-GNN can be installed using PyPi package manager after installation of dependencies. The source code is available on GitHub: https://github.com/DeepRank/Deeprank-GNN	"Average time of graph generation step per model (second) = 0,65
Average scoring time per model (second) = 2,8E-02 "	Due to the complexity of data pre-processing and the model itself, it appears be a black box or hardly interpretable at least.	The model can be used in both classification and regression tasks. 	4.0	0.0	"The model is a Graph Neural Network.
It is not a novel model."	Available on GitHub: https://github.com/DeepRank/Deeprank-GNN	The graph representation of a PPI is split into two sub-graphs, i.e. the internal graph connecting atoms from the same protein and the external graph connecting atoms from distinct proteins. The two sub-graphs are sequentially passed to two consecutive convolution/activation/pooling layers. The two final graph representations are flattened using the mean value of each feature and merged before applying two fully connected layers.	"48 features.
No mention on feature selection."	No	No	No	No	4.0	4.0	65e73fdb92c76639b8e309f3	36420989.0	PMC9805592	02/02/2026 19:46:52	R√©au M, Renaud N, Xue LC, Bonvin AMJJ.	Bioinformatics (Oxford, England)	DeepRank-GNN: a graph neural network framework to learn patterns in protein-protein interfaces.	10.1093/bioinformatics/btac759	2023	0.0	0.0		1.0	2024-05-06T09:33:08.069Z	2026-02-02T19:46:52.000Z	2a48fdcb-6313-45be-a84a-d0c8a9843cb5	undefined	u122uk9z7j				DOME_JSON	Match	Match
6638a402b30933003cc215c9	No	"The authors used in-house (Bayer) datasets related to 10 ADMET endpoints
1. LogD (pH7.5) (LOD) = 76,548 
2. LogD (pH2.3) (LOA) = 236,280
3. Membrane affinity (LOM) = 64,506 
4. Human serum albumin binding (LOH) = 61,398 
5. Melting point (LMP) = 90,589 
6. Solubility (DMSO)  (LOO) = 38,841 
7. Solubility (powder) (LOP) = 2334 
8. Solubility (nephelometry) (LON )= 88,301 
9. Solubility (DMSO not fully dissolved) (LOX) = 7392 
10. Solubility (no assay annotation) (LOQ) = 50,016"	"Models were evaluated in both a cross-validation and a separate test set fashion.
Different splitting strategies were used that includes:
1. Cluster split using k-means to cluster the compounds (K = 10)
2. Random splits ensuring that each fold contains representatives from each task
3. Time splits with no cross-validation based on measurement dates, in which later measurements used as test sets"	"Different splitting strategies were used independently and only the test size for time split dataset is reported:
LOD = 32,794
LOA = 46,481
LOM = 197
LOH = 614
LMP = 55
LOO = 22,803
LOP = 935"	3.0	1.0	No	"The model was compared to simpler models developed by authors, including:
Random Forest
Fully-Connected Single Task Network
Fully-Connected Multitask Network
Single Task Graph Convultional Network"	Standard deviations are reported in supplementary materials.	The performance of such regression models is evaluated by the coefficient of determination r2 (which measures the concordance between predicted and experimental values) and the Spearman correlation coefficient rho (which measures the ranking capabilities of the models).	"The model was evaluated a in cross-validation fashion for random and cluster split strategies.
In time split method, the model that was trained on earlier measurements is evaluated on recent measurements."	4.0	1.0	The code to train multitask graph convolutional networks is available on github: https://github.com/fmonta/mtnngc_admet	No	The model is a black box.	The model is a regression model for predicting 7 ADMET endpoints.	3.0	1.0	Multiple models were used and compared, but the main model that was focused by the authors is a Multitask Graph Convolutional Neural Network	The code to train multitask graph convolutional networks is available on github: https://github.com/fmonta/mtnngc_admet	The authors used molecular graphs and 75 simple atomic descriptors as initial node features.	"75 atomic features for each atom in a molecule.
No explanation on feature selection."	No	The model is not a meta-predictor	"The authors used the implementation of the Duvenaud algorithm in DeepChem v.1.2.1 and kept the architecture and hyperparameters suggested by the authors for ADMET predictions.
(Duvenaud, D.; Maclaurin, D.; Aguilera-Iparraguirre, J.; G√≥mez-Bombarelli, R.; Hirzel, T.; Aspuru-Guzik, A.; Adams, R.P. Convolutional Networks on Graphs for Learning Molecular Fingerprints. In Proceedings of the Advances in Neural Information Processing Systems 28 (NIPS 2015), Montreal, QC, Canada, 7‚Äì12 December 2015)"	cross validation was performed for random splits and cluster splits.	7.0	1.0	65e73fdb92c76639b8e309f3	31877719.0	PMC6982787	02/02/2026 19:46:52	Montanari F, Kuhnke L, Ter Laak A, Clevert DA.	Molecules (Basel, Switzerland)	Modeling Physico-Chemical ADMET Endpoints with Multitask Graph Convolutional Networks.	10.3390/molecules25010044	2019	0.0	0.0		1.0	2024-05-06T09:33:54.347Z	2026-02-02T19:46:52.000Z	34249f84-1197-4a65-8ab2-a298da42e32a	undefined	3iham0l32l				DOME_JSON	Match	Match
6638a437b30933003cc215cd	"The used dataset is publicly available through the URL: https://huggingface.co/datasets/nferruz/UR50_2021_04
No information on licence was reported."	"The model was trained and evaluated on UniRef50 database (version 2021_04) with 49,874,565 data points.
The dataset has been previously used in many other studies."	"The split has been done randomly.
The evaluation set is a 10% independent exclusion from UniRef50. The rest 90% was used for training.
UniRef50 is a 50% identity clustered database of UniProt KB. "	"They have used a 90 - 10 (44.9 million and 4.9 million) split corresponding to training and evaluation set respectively.
No feature has been reported to be taken into account for splitting the data. "	4.0	0.0	No	"The model was not compared to similar methods as it is quite novel.
At one point, the generations by the model were compared to randomly generated sequences as the simplest baseline."	Only MD calculations for measuring structure stability have confidence intervals.	"The generations made by the model were compared to a randomly selected natural dataset of 10,000 data points (sequences) in terms of following measures:
1) globular domains % 
2) Ordered content % 
3) Alpha-helical content %
4) Beta-sheet content and %
5) Coil content 
6) Structure stability using MD calculations"	The model was evaluated based on the comparison of the model's generations to random naturally occurring samples in terms of sequence and structural properties.	4.0	1.0	The model is freely available and the code and documentation are available here: https:// huggingface.co/docs/transformers/main_classes/trainer	"""ProtGPT2 generates sequences in a matter of seconds""."	The model is a black box due to sheer number of 738 million parameters.	It is a generative model.	4.0	0.0	"The model is an autoregressive Transformer.
The model is not novel."	The model weights are publicly available in the HuggingFace repository: https://huggingface.co/nferruz/ProtGPT2 and Zenodo: https://doi.org/10.5281/zenodo.6796843 [https://zenodo.org/record/ 6796843#.YswB9XbMIVA]	BPE tokenizer was used to train the vocabulary of the dataset. BPE is a sub-word tokenization algorithm.	"The results shown in this work correspond to a model trained with a block size of 512 tokens.
No feature selection step was involved."	p is much larger than f. The model was fit in a way that the generations most resemble the ones that naturally occur. 	The model is not a meta-predictor.	"The final model is a decoder-only architecture of 36 layers and 738 million parameters.
The architecture matches that of the previously released GPT2-large Transformer."	No	7.0	1.0	65e73fdb92c76639b8e309f3	35896542.0	PMC9329459	02/02/2026 19:46:52	Ferruz N, Schmidt S, H√∂cker B.	Nature communications	ProtGPT2 is a deep unsupervised language model for protein design.	10.1038/s41467-022-32007-7	2022	0.0	0.0		1.0	2024-05-06T09:34:47.725Z	2026-02-02T19:46:52.000Z	fa0099b4-d7bf-44c1-bee9-04ac2176a4d9	undefined	3a79km88l8				DOME_JSON	Match	Match
6638eae3b30933003cc215e9	All data sets and code are available at GitHub: https://github.com/tbwxmu/SAMPN.	"The lipophilicity data was obtained from CHEMBL3301361 by AstraZeneca, comprising 4200 molecules.
Aqueous solubility data, sourced from the online chemical database and modeling environment (OCHEM), includes 1311 experimental records."	"The splits were performed randomly.
For the initial data preprocessing, duplicate molecules were removed so that each chemical structure in the data was unique."	"The training set comprises 80% of the data, while the test set contains 10%.
Additionally, a separate validation set, constituting 10% of the data, was employed for parameter selection.
Dataset distributions are provided in supplementary material."	4.0	0.0	All data sets and code are available at GitHub: https://github.com/tbwxmu/ SAMPN.	In each task, authors built RF, MPN, SAMPN, multiMPN and multi-SAMPN models to explore the relationship between the target property and the molecular structure. 	Standard deviations are reported for each of the used metrics.	"Multiple metrics were used to evaluate the performance of the model: mean absolute error (MAE), root mean squared error (RMSE), mean squared error (MSE),
coefcient of determination (R2) and Pearson correlation coefcient (PC)."	The model was evaluated through a 10-fold cross-validation that uses 10% of the data as the test set.	5.0	0.0	All data sets and code are available at GitHub: https://github.com/tbwxmu/ SAMPN.	No	The attention mechanism of the model indicates the degree to which each atom of the molecule contributes to the property of interest, and these results are visualized. Hence, it is interpretable.	It is a regressor model.	3.0	1.0	In principle it is an explainable graph neural network. It is not a novel model and was mainly adopted from Deepchem MPN (message passing network).	All data sets and code are available at GitHub: https://github.com/tbwxmu/ SAMPN.	The SMILES representations of molecules are converted into directed graphs before training the model. The graphs consisted of nodes and edges, where the number of nodes equals the number of atoms, and edges are always double the number of bonds (bidirectional).	"Following features for nodes and edges were reported by the authors:

Attribute | Description | Dimension 
**Node** 
Atom type  | All currently known chemical elements | 118       
Degree | Number of heavy atom neighbors  | 6         
Formal charge  | Charge assigned to an atom (-2, -1, 0, 1, 2) | 5         
Chirality label | R, S, unspecified, and unrecognized type of chirality | 4         
Hybridization | sp, sp2, sp3, sp3d, or sp3d2 | 5         
Aromaticity | Aromatic atom or not | 1         

**Edge** 
Bond type | Single, double, triple, or aromatic | 4         
Ring | Whether the bond is in a ring | 1         
Bond stereo | Nature of the bond‚Äôs stereochemistry (none, any, Z, E, cis, or trans) | 6         "	The used dataset is fairly small, considering that ANN is used as the model algorithm and it easily overfit on small datasets. Authors adopted a 10-fold cross validation strategy to improve generalization.	It is not a meta-predictor.	"Hyperparameters were tuned via grid search using the Hyperopt package (v0.1.2).
RMSE on the validation set guided the search for optimal hyperparameter combinations.

Following parameters and their associated range were reported by the authors.
Activation function = Tanh, ELU, LeakyReLU, ReLU, PReLU, SELU
Steps of message passing = 2‚Äì6 
Graph embedding size = 32‚Äì512 
Dropout rate = 0.0‚Äì0.4 
Layers of fully connected network = 1‚Äì3"	Authors adopted a 10-fold cross validation strategy to improve generalization.	8.0	0.0	65e73fdb92c76639b8e309f3	33431047.0	PMC7035778	02/02/2026 19:46:52	Tang B, Kramer ST, Fang M, Qiu Y, Wu Z, Xu D.	Journal of cheminformatics	A self-attention based message passing neural network for predicting molecular lipophilicity and aqueous solubility.	10.1186/s13321-020-0414-z	2020	0.0	0.0		1.0	2024-05-06T14:36:19.663Z	2026-02-02T19:46:52.000Z	321f38fb-6765-48e6-8db4-12e72da58b47	undefined	dq30wdbow2				DOME_JSON	Match	Match
664b2a16b30933003cc21843	"""Availability of data and materials
Publicly available datasets were analyzed in this study. This data can be found here: http://lab.malab.cn/~acy/iTTCA."""	"A non-redundant dataset of 592 tumor T cell antigens (positive samples, ca 60%) and 393 tumor T cell antigens (negative samples, ca. 40%) was used (n=985).  It was an already existing dataset (generated and used in Charoenkwan P, et al (2020) Anal Biochem. 599:113747, PMID: 32333902).  
Given that 80% of the samples were randomly selected as the training dataset, the latter had 788 data points (470 positive, 318 negative),"	"The Authors state that the dataset from (Charoenkwan P, et al (2020) Anal Biochem. 599:113747, PMID: 32333902) was not redundant.  However, the latter Authors, in describing the building of the datatset, only state that ""Duplicate peptides were removed"", and do not mention any analysis of degree of pairwise identity."	80% of the 985 samples were randomly selected as the training dataset and the remaining 20% of samples were taken as the independent test dataset.          Tenfold cross-validation of the training set was used during models optimization.      To balance the positive and negative samples in the training set, the Authors used the integrated resampling technique SMOTE-Tomek, a combination of over- and under-sampling methods: synthetic minority over-sampling technique (SMOTE) (Chawla NV, et al. (2002) J Artif Intell Res.16:321‚Äì57) and Tomek‚Äôs links (Tomek) (Tomek I. Two modifcations of CNN. (1976) IEEE Trans Syst Man Cybern.SMC6(11):769‚Äì72.). Such hybrid-sampling approach, according to the Authors, can simultaneously avoid the shortcomings of overftting and loss of key information caused by SMOTE and Tomek, respectively.	4.0	0.0	Not available	"The Authors compared iTTCA-RF with iTTCA-Hybrid (published by Charoenkwan P, et al (2020) Anal Biochem. 599:113747, PMID: 32333902) on the same training andd testing datasets.
The BACC, AUC, Sn, Sp and MCC metrics of the Authors' model on the training set were between 3.6% and 9.0% higher than those of iTTCA-Hybrid, respectively. The same metrics on the independent test were between 0.8% and 4.6% higher.  "	No confidence intervals are reported for any performance measure. Therefore, the statistic significance of the Author's claim can not be calculated.	Balanced Accuracy, AUC, Specifity, Sensitivity, Matthews Correlation Coefficient. 	Independent dataset (n=197 data points, of which 122 positive, 75 negative). 	5.0	0.0	The online prediction server was made freely accessible at http://lab.malab.cn/~acy/iTTCA.   All test classifiers were applied as in the Scikit-learn implementation.	Not reported.	The MRMD feature selection method selected a subset of features that were strongly correlated with the class label and have low redundancy between features.  However, no analysis of a possible interpretability have been reported in the publication.    	Binary classification (TTCA or not TTCA) (TTCA = tumor T cell antigens).  	4.0	0.0	The best performing machine learning algorithm was searched among the following 6 standard classifers: random forest (RF), support vector machine (SVM), adaboost (AB), logistic regression (LR), bagging, and gradient boosting machine (GBM), all of them in the implementation of the scikit-learn package.  The best performer turned out to be Random Forest (RF).	The training and testing datasets are freely accessible at http://lab.malab.cn/~acy/iTTCA.	The protein sequences (T-cell epitopes) were preliminarily encoded using 4 kinds of feature extraction methods: global protein sequence descriptors (GPSD), grouped amino acid and peptide composition (GAAPC), pseudo amino acid composition (PAAC), and adaptive skip dipeptide composition (ASDC).   The iLearn tool package was used to generate the 4 type of sequence features.	The following two-step feature selection technique to search for the optimal feature subset was applied:  First, the maximum relevance maximum distance (MRMD) algorithm was used to analyze the feature importance of the involved vectors. Then, after application of the incremental feature selection (IFS) strategy, different feature subsets were generated for optimization for each considered classifcation algorithms.      The best performance model was finally constructed using the top 263 selected features.	The number of features was 263, at risk of overfitting. The somewhat lower performance metrics on the test set, relative to the training set, could indicate some overfitting in the model optimization procedure.   The Author's did not claim to have ruled out overfitting, but they have taken some preventive measures, namely the SMOTE-Tomek for balancing the numerosity of the training positive and negative sets and an accurate feature selection strategies.	No.	The number of paramenters was the standard one for each of the 6 classifiers. The hyper-parameters were optimized using grid search, with the search ranges presented in the Supplements. 	As mentioned, the Authors have taken some preventive measures: the SMOTE-Tomek for balancing the numerosity of the training positive and negative sets and an accurate feature selection strategies.	7.0	1.0	65faa4f792c76639b82bab29	34706730.0	PMC8554859	02/02/2026 19:46:52	Jiao S, Zou Q, Guo H, Shi L.	Journal of translational medicine	iTTCA-RF: a random forest predictor for tumor T cell antigens.	10.1186/s12967-021-03084-x	2021	0.0	0.0		1.0	2024-05-20T10:46:46.631Z	2026-02-02T19:46:52.000Z	9ef704d3-b4f4-461f-aa23-03871338dc04	undefined	y38upwlhxf				DOME_JSON	Match	Match
664b2a4db30933003cc21847	"""Data Availability Statement -- Publicly available datasets were analyzed in this study. This data can be found here: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE110357."""	The datasets were obtained from the study of Pastushenko et al (2018) at https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE110357. The number of data points (mouse Tumor Cells (TCs), each one with single-cell gene expression profiles) was 383.  The datapoints of the 1¬∞ dataset were 71 epithelial YFP + Epcam + skin squamous cell carcinoma TCs and the datapoints of the 2¬∞ dataset were 312 mesenchymal-like YFP + Epcam ‚àí skin squamous cell carcinoma TCs.   Epithelial YFP + Epcam + TCs and mesenchymal-like YFP + Epcam ‚àí TCs represent different EMT (Epithelial-to-Mesenchymal Transition states.   Gene expression differences may reveal the cascade mechanisms of tumor migration and invasion.	Not applicable.	10 fold cross validation.   To balance the two datasets, so that the numbers of epithelial tumor and mesenchymal TCs were equal, the tool ‚ÄúSMOTE‚Äù (Synthetic Minority Over-samplingTEchnique) in Weka was used, to generate new samples in the class of epithelial TC.   	4.0	0.0	Not available	No comparisons.	No confidence intervals for MCC were reported.	MCC (Matthews Correlation Coefficient).	Cross-validation.   There were no independent datasets or novel experiments.	5.0	0.0	The algorithms and software packages used in this publication were all standard or already published.	No reported.	"According to the Authors, ""SVM and RF are ‚Äúblackbox‚Äù methods. [...] RIPPER can generate interpretable classification rules.""    Ante-hoc techniques for transparency in SVM and RF were not applied, since feature selection was performed in a completely automatic manner.  Post-hoc analysis was partially interpretable, since several of the selected features were genes already known to be involved in the Epithelial-to-Mesenchymal Transition.  In addition, GO term and KEGG pathway enrichment results were consistent with the known differences between epithelial and mesenchymal cells."	Binary classification (the optimal set of genes (features) should be able to clearly separate Epithelial from Mesenchymal cell status).  	4.0	0.0	Standard Random Forest (RF) and Support Vector Machine (SVM) algorithms were used.   In addition, repeated incremental pruning to produce error reduction (RIPPER) was applied to produce classification rules for classifying samples from different TCs.  	Not reported.	Each TC was encoded with the expression levels of 49,585 genes.	"
Boruta feature selection (Kursa and Rudnicki (2010) J. Statist. Softw. Artic. 36, 1‚Äì13) and minimum redundancy maximum relevance (mRMR) method (Peng et al (2005) IEEE Transact. Patt. Anal. Mach. Intel. 27, 1226‚Äì1238) were used, to evaluate the importance of each feature. Key features were then selected, and fed into the incremental feature selection (IFS) with supervised classifiers to identify the optimal gene signatures for screening different TCs.   The IFS was run with SVM, RIPPER, and RF, respectively.   The optimal numbers of features turned out to be 169 (SVM), 159 (RF), 38 (RIPPER).
"	After SMOTE application, the number of datapoints was 624.    The Authors first used Boruta to select relevant features, resulting in 237 features (genes). After mRMR and IFS, the subsets of features with the optimal classification performance were obtained.  The numbers of features were 169 (SVM), 159 (RF), 38 (RIPPER).     Given the very high Accuracy (>= 0.979) and MCC (>= 0.934) values, obtained with all three classifiers, the methods seem to be at risk of overfitting.   While the high number of features could be a reason for the probable overfitting in SVM and RF, the RIPPER model might eventually have had different sources of overfitting.      	No.	The authors do not mention parameters numbers different from standard.  	Not reported.	7.0	1.0	65faa4f792c76639b82bab29	33584803.0	PMC7876317	02/02/2026 19:46:52	Yu X, Pan X, Zhang S, Zhang YH, Chen L, Wan S, Huang T, Cai YD.	Frontiers in genetics	Identification of Gene Signatures and Expression Patterns During Epithelial-to-Mesenchymal Transition From Single-Cell Expression Atlas.	10.3389/fgene.2020.605012	2021	0.0	0.0		1.0	2024-05-20T10:47:41.132Z	2026-02-02T19:46:52.000Z	85fd4f8b-595f-4be3-8d57-2d351b44e1ce	undefined	t7m3dsf9ni				DOME_JSON	Match	Match
664b2a75b30933003cc2184b	The training dataset has been taken from a cited publication (Warner DJ, et al (2012) Drug Metab Dispos Biol Fate Chem 40:2332‚Äì2341), which however was not accessible without a fee.  One of the test sets in published, in the unprocessed form, in Pedersen JM, et al (2013) Toxicol Sci 136:328‚Äì343.	The training dataset comprised 408 compounds:113 BSEP (bile salt export pump) inhibitors and 295 non-inhibitors, derived from a previous publication (Warner DJ, et al (2012) Drug Metab Dispos Biol Fate Chem 40:2332‚Äì2341	A chemical space network (CSN) was constructed and analyzed in order to assess the structural similarity shared by the compounds of the inhibitor and non-inhibitor groups.  The majority of the nodes did not have a connection, indicating a high structural diversity in the training dataset.  A similar analysis showed a high structural diversity also for one of the test sets.	For all model, tenfold internal cross-validation was applied.  Distribution of data point in the splits is not reported.	4.0	0.0	Not available.	A comparison between the present structure-based models to a previous published method, by the same research group, that applied ligand-based models, was performed.  A comparison to a model, obtained using the scoring probability functions only, was performed. 	No confidence intervals of performance metrics are given.	Sensitivity, specificity, accuracy, G-mean, Matthews Correlation Coefficient, AUC-ROC.	Two independent test sets were used, containing 166 compounds (44 inhibitors and 122 noninhibitors), and 638 compounds (248 inhibitors and 390 non-inhibitors), respectively.  The first set was taken from the work of Pedersen JM, et al (2013) Toxicol Sci 136:328‚Äì343.  The second dataset (unpublished) was provided by AstraZeneca within the framework of the IMI project eTOX. (http://www.etoxproject.eu). Both studies provide in vitro inhibition data on human BSEP.	5.0	0.0	The open source software WEKA (version 3.7.10) was used.	"Not reported.
"	The input features were relatively transparent, being based on the quality of the docking poses and on two main reasonable physicochemical properties of the compounds.  The addition of the latters did improve the performance, which was in agreement with expectation.	Binary classification (BSEP inhibitor or non-inhibitor).  	4.0	0.0	The open source software WEKA (version 3.7.10) was used for building binary classification models. The standard machine learning classifiers, J48, Random Forest, REPTree, LibSVM, and Naive Bayes, were used with the default parameters along with tenfold internal cross-validation.	Not available.	The initial dataset was curated according to the following steps: (1) removal of inorganic compounds using Instant JChem v.5.3, 2010, ChemAxon (http://www.chemaxon.com); (2) analysis and removal of mixtures formed by two or more large molecules; (3) deletion of organometallic compounds using MOE 2011.10 15; (4) identification and removal of compounds containing special atoms such as selenium or tellurium by means of an in-house MOE SVL script; (5) normalization of chemotypes using the ChemAxon‚Äôs Standardizer with the following settings: clean 2D, aromatize, mesomerize, neutralize, tautomerize and all transform options; (6) identification and elimination of nonunique structures using MOE; (7) deletion of compounds having permanent charges.    Different docking runs (using the docking software GOLD) and docking fitness functions were applied to the preprocessed training set.  Prior probability distributions for the inhibitor and non-inhibitor classes were calculated.	The input for the set of the 5 ML models was a combination of the Xscore(ChemScore) scoring function, obtained by the GOLD docking runs, combined with physicochemical properties as descriptors for the training set. The physicochemical properties were MW (Molecular Weight) and log(P) (where P is the octanol-water partition coefficient of the compound).   No feature selection is mentioned.	The standard number of parameters in the applied classifiers was much lower than the training points.  The possibility of under-fitting was not mentioned.	"No.
"	The standard parameters in WEKA for the 5 chosen classifiers were used.  Their number is in the order of less than a dozen for each classifier. 	Not mentioned in text.	8.0	0.0	65faa4f792c76639b82bab29	28527154.0	PMC5487762	02/02/2026 19:46:52	Jain S, Grandits M, Richter L, Ecker GF.	Journal of computer-aided molecular design	Structure based classification for bile salt export pump (BSEP) inhibitors using comparative structural modeling of human BSEP.	10.1007/s10822-017-0021-x	2018	0.0	0.0		1.0	2024-05-20T10:48:21.402Z	2026-02-02T19:46:52.000Z	4fb79024-ab54-4e22-889b-73335801c3bc	undefined	2g7fzlkala				DOME_JSON	Match	Match
665a01aa37ea6fa797a6bd7d	"Data relevant to the ML methods:
The PCR data was the determination of the presence of 22 equally weighted genetic factors covering virulence genes, R-type and phylogroup across 272 isolates. This data such as gene names, amplicon size in bp length and sequence 5‚Ä≤‚Äì3‚Ä≤ are available in tables in the text - Tables 1, 2 and 3. 

Data not relevant to the ML methods:
The WGS sequences can be found on European Nucleotide Archive (EMBL-EBI) with project accession number PRJEB11876 (ERP013295) and on the NCBI database.
ENA: http://www.ebi.ac.uk/ena/data/view/PRJEB11876
NCBI: https://www.ncbi.nlm.nih.gov/bioproject/PRJEB11876/  "	"Avian pathogenic Escherichia coli (APEC) isolates were the primary data source and were collected through direct experimentation. 

272 APEC isolates were collected from the  UK (173), Germany (69) and Italy (30). These 272 isolates were then genetically characterised using multiplex polymerase chain reactions (PCRs) targeting 22 equally weighted factors covering virulence genes, R-type and phylogroup. This data was used for their ML approaches. 

95 of the 272 isolates were further analysed using Whole Genome Sequencing (WGS) to create whole genomes. This data was not used for their ML appraoches."	Data was not split. No training data used due to unsupervised classification ML model used.	No - no data split as unsupervised classification method used without labelled data included. 	4.0	0.0	No	No 	No cross compaarison to other methods. From the text: Strong factor associations were reported (confidence 0.99):  1). ompT (protease) ==‚Äâ>‚ÄâhylP (haemolysin) 2). IroN (siderophore)‚Äâ+‚ÄâompT (proteases) ==‚Äâ>‚Äâhlyp (haemolysin) 3). ompT (proteases)‚Äâ+‚ÄâsitA (cell adhesion‚Äìmetal ions binding==‚Äâ>‚Äâhlyp (haemolysisn).  Significant factor associations were reported (confidence above 0.95): 4). IroN (siderophore)‚Äâ+‚ÄâsitA (cell adhesion‚Äìmetal ions binding) ==‚Äâ>‚ÄâhlyP (haemolysisn conf:(0.96) 5). cva/cvi (bacteriocin immunity) ==‚Äâ>‚ÄâhlyP (haemolysisn) conf:(0.96) 6). hlyP (haemolysisn)‚Äâ+‚ÄâsitA (cell adhesion metal ions binding) ==‚Äâ>‚Äâiron (siderophore) conf:(0.95) 7). hlyp(haemolysisn)‚Äâ=‚Äâ1 sitA (cell adhesion‚Äìmetal ions binding) ==‚Äâ>‚ÄâompT (proteases) conf:(0.95).	Apriori is an unsupervised learning algorithm, the concept of performance measures in the traditional sense (like accuracy or precision) doesn't directly apply. 	"The Apriori ML algorithm itself doesn't directly benefit from techniques like cross-validation typically used in supervised learning. 

The 22 equally weighted factors covering virulence genes, R-type and phylogroup predicted factor associations were used in a standard ML Apriori algorithm implementation. This is a novel dataset."	4.0	1.0	No, it seems no novel code was produced or shared if it was produced for the analysis. No GitHub or code repository linked. The primary ML related software used for the analysis is identified as the 'machine learning and data mining software WEKA' which is referenced in the text. (https://dl.acm.org/doi/10.1145/1656274.1656278)	Information not provided for the text on the ML section. It is unlikely a HPC machine was needed to run the analysis as the software was released in the early 2000s and uses a user friendly graphical user interface.  	Information and tutorials are available on the Apriori ML algorithm of the Weka software in online tutorials e.g. https://www.tutorialspoint.com/weka/weka_association.htm . As the default parameters were used, it is not a complete black box. However, the exact availability of the Weka tool's underlying code is not clear or easily available from the paper, e.g. no GitHub link. 	Classification model.	4.0	0.0	"To determine virulence factor associations the in the PCR isolates data they used machine learning and data mining software WEKA. Within WEKA the Apriori ML algorithm was applied to the data.

This Apriori ML algorithm is not new, first proposed in 1994."	Apriori ML algorithm in WEKA software was used, leaving all the parameters on the default settings. 	No clear preprocessing was undertaken beyond standard data structuring for use in the ML alogrithm. 22 equally weighted factors covering virulence genes, R-type and phylogroup were checked for their presence in each of the 272 isolates. These DNA data virulence factors are available in spreadsheet like tables in the paper.	None - not applicable to Apriori ML model. They use transactional datasets, in this case the 22 PCR virulence factors are the constituents of the transactional dataset in relation to each of the 272 isolates.	No - unsupervised ML method, used to classify and no fitting is used for this. 	"No - Apriori ML alogrithm does not use meta-predictions as an input. 

While Apriori doesn't use meta-predictors, other algorithms for association rule learning might leverage them:
SEuqential Miner (SEQUOIA)or Constraint-based association rule learning."	"Provided a standard Apriori ML algorithm was implemented as indicated in the text, here are some of the key parameters that would be used:
Minimum Support 
Minimum Confidence 

Also sometimes used by the Apriori ML algorithm are:
Maximum Length 
Lift
"	No - not applicable for this unsupervised ML method. No regularistaion is used.	8.0	0.0	665a01aa7089c469b4646267	27875980.0	PMC5120500	02/02/2026 19:46:52	Cordoni G, Woodward MJ, Wu H, Alanazi M, Wallis T, La Ragione RM.	BMC genomics	Comparative genomics of European avian pathogenic E. Coli (APEC).	10.1186/s12864-016-3289-7	2016	0.0	0.0		1.0	2024-05-31T16:58:18.981Z	2026-02-02T19:46:52.000Z	4871c35e-fd51-4036-abbc-783f8f4ca99a	undefined	23rrtkglve				DOME_JSON	Match	Match
665d0af137ea6fa797a6bda1	"Training data:
-Positive class, disease causing indel variant data was sourced from the Human Gene Mutation Database (HGMD), professional version 2017. http://www.hgmd.cf.ac.uk

-Negative class, putatively neutral indel variant data was sourced from the Genome Aggregation Database (gnomAD) databases. https://gnomad.broadinstitute.org/

Test data:
-1. Somatic test set: Catalogue Of Somatic Mutations In Cancer (COSMIC) genome-wide screen data set (v85) & DataBase of Cancer Driver InDels (dbCID). 

-2. De novo test set: REACH Project & Simons Simplex Collection.

The publicly available data of the study derived from the above databases are available at http://mutpred.mutdb.org/ - training data: http://mutpred.mutdb.org/wo_exclusive_hgmd_mp2_training_data.txt

Also references in the text to other publications regarding some of the data sources. "	"Data source: from databases - both pay to access (HGMD professional) and publicly available ones (COSMIC/gnomAD/+). 

Data type: DNA - genetic variants, grouped as standard insertions and deletions (indels) and complex indels.
Class positive: disease causing sequence-retaining insertion, deletion, and complex indel variants. Unclear but likely: 1,296.
Class negative: putatively neutral insertion/deletion variants. Unclear but likely: 2,392.

Determination of exact number of training and test data points is difficult to ascertain due to poor alignment with numeric figures provided in the text to that of Table 2 which contains this information. The Table 2 itself is also difficult to interpret due to a second set of bracketed numbers beside each initial figure provided. Neither sets of numbers align exactly to the text and no clear key/description provided. 

Dataset reuse/community recognised: data used is a subset of avilable data from reputable international databases. Large variety of databases leveraged and combined for training and test data."	"How were the sets split?: 
For the training data:
-Validation set (25%): A quarter of the training data is used to fine-tune the model during training (resilient propagation method).
-Cross-validation (10-fold): The training data is divided into 10 folds. The model is trained on 9 folds and tested on the remaining one, repeated 10 times, to assess generalisability (AUC-ROC).

Are the training and test sets independent?: 
Yes, indpendent test and training sets used.

How does the distribution compare to previously published ML datasets?:
MutPred-Indel is compared to three existing methods: DDIG-in, VEST-Indel, and CADD  but it uses the training data from the current study, not entirely independent datasets. The paper notes a 'paucity' of such data."	"Training data set comprised of:
-5606 single residue deletions
-1033 single residue insertions
-2427 multi-residue insertions
-3052 multi-residue deletions
-1253 complex indel variants

Test data sets: 
2x test data sets were used to test the models classification efficacy.
1. Somatic mutation test sets: consisting of two sets of putatively damaging cancer causing somatic variants derived from information in COSMIC AND dbCID databases. Text seems to indicate 576 test data points based on the final sentence '(n = 576)' but not clear or explictly stated/documented in text.  

2. De novo mutation test sets: consisting of non-frameshifting insertion/deletion variants curated from families affected by Autism Spectrum Disorder (ASD). Data sourced from REACH Project (2650 families) and the Simons Simplex Collection (SSC). Final test set after filtering is performed is composed of 1217 candidate de novo indels in 827 offspring (506 cases, 321 controls).

Separate validation set used, and if yes, how large was it?: 
The model training utilised the resilient propagation method and 25% of training data set aside for the validation."	4.0	0.0	No - no link/files /statistical code relating to the raw evaluation beyond text details.	"Yes, the evaluation of MutPred-Indel included comparisons to both simpler baselines and publicly available methods.

Simpler baseline: authors compared MutPred-Indel to MutPred2. They achieved this by simulating deletions and insertions using MutPred2 and showed that MutPred-Indel outperformed MutPred2 in this scenario (AUC of 0.903 vs 0.797)

Comparison to publicly available methods: authors also compared MutPred-Indel to other indel prediction methods like VEST-Indel and DDIG. MutPred-Indel achieved the highest AUC (0.897) compared to VEST-Indel (0.875) and DDIG-in (0.869). "	Confidence intervals are not explicitly mentioned in the text. No clear indication of t-tests/other approaches to determine statistically significant method improvements beyond the direct AUC comparisons.	"The area under the receiver operating characteristic (ROC) curve is detailed and plotted in the evalaution section.

The paper notes the model: 'shows strong performance in cross-validation with the area under the ROC curve (AUC) of 0.908'. Figure 5 details the performance ROC curves under various comparisons such as against other prediction models such as 'CADD'."	"Various evaluation methods were employed such as:
10-fold cross-validation, per-protein and per-cluster cross-validation and comparison of models with different feature sets."	5.0	0.0	Source code GitHub repository for MutPred2: https://github.com/vpejaver/mutpred2 - MIT License. Primary resource website of the software (http://mutpred.mutdb.org/) offers  live web service for analyses and downloadable executable.	The standalone model executable can be used for genome-scale data sets. To install and run MutPred2, you will need about 50 GB of hard disk space and at least 4 GB RAM. No further information avaialble regarding prediction run times beyond this.	Somewhat of a black box from the text alone. In the text there are reasonable descriptions of the model but given the lack of publicly available model & code (eg: in Huggingface/other model hosting/no GitHub) and exact training & test datasets. It is somewhat of a blackbox if attempting to reproduce and interpret the exacts of the model given its complex ML algorithm nature being that of an ensemble of neural networks from the text alone. However, there is a GitHub online that can be found for the model and related datasets so this helps greatly with the model interpretability in conjunction with the text. However, it should be explictly linked for readers. The Matlab related files and info also not available on GitHub due to licensing.	Classification - used to predict pathogenicity of indels	4.0	0.0	The ML algorithm used in the paper is an ensemble of bagged two-layer feed-forward neural networks.	No, not clearly available from the text or software website. Regarding parameters reporting, the text does note: 'Each pathogenicity predictor was developed with the Matlab 2016b Neural Network Toolbox as an ensemble of one hundred bagged two-layer feed-forward neural networks, where the following training parameters were not varied between alternative models'.	Preprocessing was undertaken via Two-sample t-test and principal component analysis (PCA). Encoding used and detailed under feature engineering section such as numerical encoding.	Precise number of input features not explicitly stated in main publication text. The structural and functional features of protein sequences are detailed in Table 3 of the text and fall into 5x categories with approximately 57 noted across these categories. 	Provided that the feature information and inferred parameter infromation (not easy to discern from text), the risk of overfitting is not very likely due to N >> p and the validation set. For underfitting this is mitigated by minimal feature reduction and 10-fold cross-validation employed, but this is dependent on significance of the features and risk grows if the features used are not significant enough. 	No	Without the precise feature count the number of parameters cannot be exactly determined. However, if to use the estimated number of features from Table 3 in the text (57 features from total of 5x categories), it could be inferred that with a network architecture of a two-layer feed-forward neural network with 10 hidden units would be 591.	Yes, as the text mentions using a validation set, specifically retaining 25% of the training data being set aside for validation.	7.0	1.0	665a01aa7089c469b4646267	31199787.0	PMC6594643	02/02/2026 19:46:52	Pagel KA, Antaki D, Lian A, Mort M, Cooper DN, Sebat J, Iakoucheva LM, Mooney SD, Radivojac P.	PLoS computational biology	Pathogenicity and functional impact of non-frameshifting insertion/deletion variation in the human genome.	10.1371/journal.pcbi.1007112	2019	0.0	0.0		1.0	2024-06-03T00:14:41.897Z	2026-02-02T19:46:52.000Z	ffea624d-972d-4b3e-85bb-4708bb5c26a1	undefined	5bz7gjjsah				DOME_JSON	Match	Match
666ca94f37ea6fa797a6c1f9	"Yes, all the data in the dataset that was used has been documented in the Zenodo repository. The original source literature dataset remains available in the Europe PMC database. Biodata Resource Inventory Code Release: https://zenodo.org/records/10105162

EPMC database: https://europepmc.org/

EPMC licensing policy: https://plus.europepmc.org/user-guide#Licensing_policies"	"Data source: Europe PMC - a database for depositing life science literature was the primary data source for the study.  

The life science literature used was: titles and correpsonding abstracts.

Dataset community recognition: this was not an established dataset but created for the purpose of this study using a subset of literature data available in Europe PMC.

Npos/Nneg info: the study kept the entries where two curators agreed on the article classification label 
-Total positive or negative, n = 1,587
-Total positive: 478
-Total negative: 1,109"	"Test and training sets were independent.

Distribution of whole data set: 70% training, 15% validation, 15% test. This aligns to general reccomended redundancy splits for ML model training, fine tuning and evaluation uses.
"	"Whole data set:
1634 literature records total. This was reduced to 1,587 (data points were kept from origianl 1,634 where 2x curators agreed on classification of pos/neg)
These were dsitributed into: 70% training, 15% validation, 15% test (hold-out).

Data points - breakdown of the initial 1,587 items of literature data collected:
-Training set: 1,110 data points
-Test set: 239 data points (+ 238 validation data points)

Was a separate validation set used, and if yes, how large was it?: yes, validation set used = 238 data points

Are the distributions of data types in both training and test sets plotted?: no - but data cleaning done through curator validation on all npos/nneg idenitfied.

The above information on the data splits can be viewed in an overview table in the paper: Table 1. Training dataset splits for the article classification task - https://doi.org/10.1371/journal.pone.0294812.t001"	4.0	0.0	Yes - several supplmentary files well detail the evaluation. Code: https://zenodo.org/records/10105162 / 	"Yes, a camparison to manually curated database regisrties was undertaken as deatailed in section 2.9.2 'Comparison with existing registries.' This utilised data from re3data.org and FAIRsharing regsitries using their APIs. Limitations of the benchmark datasets noted in the text.

This was not a simpler ML model baseline as this work was novel, but instead against manually curated datasets containing the same fields the model was extracting with NER.

The authors initially expected greater overlap with re3data.org and FAIRSharing, but only 536/3112 (17.2%) inventory resources were identified in these two registries; similarly, the majority of life science resources within both re3data (975/1189, 80.5%) and FAIRsharing (1161/1640, 70.8%) are not found in their inventory."	Confidence scores used but not confidence interval ranges. The use of the models was novel and no comparable  method was deployed in the past to draw direct comparison to these ML methods. However, manual curator validation was used as a baseline as described in text: to determine a threshold probability of < 0.978 as ‚Äúlow-scoring,‚Äù where 0.978 was the average probability for names determined by a curator to have been correctly predicted in the 10% random sample of the 468 articles that were manually reviewed.	"Both the perfomance of the article classification task and that of the NER task were measured to ensure best model performance.

Pre-trained BERT model selection for article classification: 
-Performance measured to selected best BERT model for the NLP tasks. 
-15 total evaluated and noted in table 3 (and cited)
-'S(upplementary)5 Table. Article classification' measures BERT model performance and documents this in relation to F1-score, Precision & Recall. 

Pre-trained BERT model selection for NER model performance:
-Performance measured to selected best BERT model for the NER tasks.
-15 total evaluated and noted in table 3 (and cited)
-'S(upplementary)6 Table. NER model performance.' measure BERT model performance and documents this in relation to F1-score, Precision & Recall. 

Mid way evaluation undertaken also to manually assess precision on a 10% random sample (n = 468) of the NER tasks. "	Independent dataset used for validation. 15% of whole dataset held out to validate the model performance.	5.0	0.0	Source code available: Yes - https://zenodo.org/records/10105162. Algorithms and models can be run using the 2x Snakemake reusable pipelines and also an iPython notebook (ipynb) file is available for use Google Colab/other notebook deployment platforms. 	"The text directly references computational need for a GPU to re-run the model training. They do not make exact/estimate statements referring to execution times but explictly state a Google Colab instance with a GPU utlising their iPython notebooks (ipynb) provided would be sufficient. This would indicate a desktop equipped with a GPU would suffice rather than there being a requirement for access to HPC resources and several GPUs. To run  

Potentially this information is more explicitly noted in the well documented supplementary materials code release: https://zenodo.org/records/10105162."	Highly interpretable - the authors made expert efforts to avoid having a black box model publication. For example all code and data & configs, etc are all available on Zenodo repository and expertly documented for ease of interpretability: https://zenodo.org/records/10105162. All test and training data available and clear, linked from text to relevant repositories. Explicit study design noted in text to have been created with open science and reproducibility in mind.	Classification model 	4.0	0.0	"Two machine learning models described in the text were fine-tuned to automate the process of: 
-1: classifying research articles 
-2: extracting mentions of biodata resources from those predicted to describe a biodata resource

These were based on pre-trained BERT (Bidirectional Encoder Representations from Transformers) models.

Pre-trained BERT models leverage deep learning transformer architectures - neural networks utilised for NLP tasks. Not novel algorithm."	Hyperparameters detailed in 'S4 Table. Hyperparameters used for model fine-tuning for article classification and NER tasks.' - Zenodo PDF: https://doi.org/10.1371/journal.pone.0294812.s008	Data was preprocessed for use by the pre-trained BERT models independently by two curators. Article titles and abstracts were reviewed to classify them as either describing a biodata resource (pos) or not describing a biodata resource(neg). Text strings were tokenized for BERT models using specific BERT tokenizers described in text.	"Not applicable as pre-trained BERT models were used. These do not use a single set of features that you directly extract and feed into the model like traditional machine learning methods. BERT learns features based on the pre-training it was subject to. 

Related info on tokenization described above with regards to BERT model input features during the model fine tuning and training for its two tasks NER and classification."	Exact p and f unkown and not detailed in text due to pre-trained BERT models in used. However, the authors used early stopping, a validation set, pre-trained BERT models with cross-comparison across 15 of these, and reported good performance on unseen test data. This in total contributes to a strong likelihood that the authors effectively addressed both overfitting and underfitting concerns. 	No - data is sourced from direct Europe PMC API query of data as detailed in text, and is not inputting meta-predictions to the models.	"Number of parameters (p) are used in the model?: 
-Not directly mentioned but findable based on the BERT model types being well detailed. 
-Pre-trained BERT models were used, so model parameters not directly detailed in text as a result but can be found from: Hugging Face Library (https://huggingface.co/docs/transformers/en/index) where you can find the number of parameters for a pre-trained model by using the model.config.num_parameters() function.

Hyperparameters detailed in 'S4 Table. Hyperparameters used for model fine-tuning for article classification and NER tasks.'"	Yes - overfitting prevention techniques were employed.  Early stopping was undertaken using a validation set. Max of 10 epochs but early stopping and use of highest precision.	8.0	0.0	665a01aa7089c469b4646267	38015968.0	PMC10684096	02/02/2026 19:46:52	Imker HJ, Schackart KE, Istrate AM, Cook CE.	PloS one	A machine learning-enabled open biodata resource inventory from the scientific literature.	10.1371/journal.pone.0294812	2023	0.0	0.0		1.0	2024-06-14T20:34:23.344Z	2026-02-02T19:46:52.000Z	5f5a87ee-e6b1-441d-a425-0abbc75bb322	undefined	31tebn1dq4				DOME_JSON	Match	Match
666f4a9e37ea6fa797a6c20d	"Yes - data available: http://ftp.tue.mpg.de/ebio/projects/ResMiCo/
This is on the: MPI for Biology FTP server, author noted choice due to size of datasets."	"An existing database was the primary source of the data - Release 202 of the Genome Taxonomy Database (GTDB).
-18,000 reference genomes from the database were selected for further use.
-From this database the authors created a synthetic dataset comprised of bacterial and archaeal genomes with the simulation software 'Metagenome read simulation of multiple synthetic communities' (https://github.com/nick-youngblut/MGSIM).
-Illumina ART read simulator was used to generate paired-end Illumina reads of length 100 or 150 using either the default ‚ÄúIllumina HiSeq 2500‚Äù error profile or the ‚ÄúHiSeq2500L150R1/2‚Äù error profile used in CAMISIM.
-Information detailed with parameters of the simulation in 'Table 1. Parameter values used in the simulation pipeline.'"	"The sets were split from the original 18,000 using a family taxonomic level split to divide these into training and test.

For redundancy reduction, max 50 genomes per species were included to avoid overfitting.

Random genome selection was used to divide the test and training in alignment with the 50 genomes max per species. 

"	"18,000 total reference genomes selected from GTDB. Of these:
-Test: 9000 reference genomes used 
-Training: 9000 reference genomes used

10% of the training dataset was used as a validation set for the model selection."	4.0	0.0	Large degree of the evaluation is noted throughout the text & figures or in S1 - Supplementary Material ResMiCo: increasing the quality of metagenome-assembled genomes with deep learning (https://journals.plos.org/ploscompbiol/article/file?type=supplementary&id=10.1371/journal.pcbi.1011001.s001). Statistical code for evaluation not clearly avaialble linked from text or within shared GitHub repository.	"The model was compared to other state of the art models for this prediction task:
-metaMIC  
-DeepMAsED  
-ALE 

The model was also compared to various benchmark datasets:
-CAMI datasets (gut, oral, skin)
-Mock communities (BMock12, MBARC-26)"	Confidence intervals are not explictly mentioned in the text. If taking the other models as the baseline for camparison the reported AUPRC & AUROC are noted to outperform these throughout the text. More information for the confidence would be useful beyond comparisons already noted for the datasets and other models.	"Various model performances measure metrics were reported in the text eg:
-Area Under the Precision-Recall Curve (AUPRC)
-Area Under the ROC Curve (AUROC)
-Precision and Recall"	"Independent datasets were used to evalaute the model. 
For example the text notes that two CAMI datasets that simulate non-human biomes: CAMI-marine and CAMI-plant-associated were used to evaluate the model and corresponding AUPRC & AUROC shared."	5.0	0.0	Main ResMiCo GitHub (python package and snakemake pipeline): https://github.com/leylabmpi/ResMiCo - MIT license. Dataset simulation pipeline used available in Snakemake: https://doi.org/10.1093/bioinformatics/bts480 . Dockerfiles noted in the repo but no VMs/web server. Direct pip install possible from a command line and information in GitHub software repo but limitations noted for incompatibility with apple Mac machines due to chipset.  	"There is a section dedicated to benchmarking the ResMiCo model resource requirements in the materials and methods.
-Using the CAMI gut dataset, ResMiCo ran > 2x faster with a GPU versus a CPU (108 ¬± 0.7 versus 38.7 ¬± 10.3 contigs per second).
-They further note the reccomendation to use multiple GPUs for training the model on larger datasets.
-While it is feasible to run the model with a CPU, this is at a much slower rate: 140,000 contigs in 1 hour with a single CPU."	Moderately interpretable. GitHub available with model code & datasets hosted online. Important info detailed in paper but not precisely straight forward on all aspects. Tutorials available which are helpful to reuse the model: https://github.com/leylabmpi/ResMiCo/wiki/ResMiCo-SM-tutorial. However, this is a lightweight tutorial page and more information could be provided given the complex nature of the model used. More clarity around the large number of datasets mentioned in the evaluation would be helpful.	Regression	4.0	0.0	"Deep Residual Neural Network (ResNet) 
-Not a new algorithm "	The list of optimised hyperparameters and the attempted values are provided in Table B in S1 Text. (https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1011001#pcbi.1011001.s001). Unclear if further information available and this is unlikely based on the GitHub which is functionally in place to share reuse of the model vs considerations of sharing precise configuration information.  	Data encoding (0-1) detailed in supplemetary data 'Table A. The full list of positional features computed by ResMiCo pipeline'. The fifth column states preprocessing applied to the features of the table as used for the data encoding for the model: standardisation (Std), normalisation (Nrm), and one-hot encoding (Onh).	"14 features as noted in 'Fig 3. Feature ranked by their importance.'.

Feature selection was performed and noted in a dedicated paper subsection. "	Yes, p much larger than number of training points. Trainable Parameters (559,441) vs. Features (14).	No	ResMiCo model has 562,573 parameters, of which 559,441 are trainable.	Early stopping likely used based on text information but not explicitly written in the text. Class imbalance handling and data augmentation approaches also noted in the text which can help prevent overfitting a model.	7.0	1.0	665a01aa7089c469b4646267	37126495.0	PMC10174551	02/02/2026 19:46:52	Mineeva O, Danciu D, Sch√∂lkopf B, Ley RE, R√§tsch G, Youngblut ND.	PLoS computational biology	ResMiCo: Increasing the quality of metagenome-assembled genomes with deep learning.	10.1371/journal.pcbi.1011001	2023	0.0	0.0		1.0	2024-06-16T20:27:10.713Z	2026-02-02T19:46:52.000Z	e9d0a4a1-31d9-4782-937b-885197f55f14	undefined	hipgaatgji				DOME_JSON	Match	Match
6670865737ea6fa797a6c28a	Preprocessed data linked in a Google drive: https://drive.google.com/drive/folders/19K5DxFVpjozpd1rKnBvZZzDmnPp1-Ldw & also off of the main GitHub (https://github.com/maslov-group/FUN-PROSE/blob/main/README.md).	"3x RNA-seq datasets used:
1. RNA-seq data on N. crassa (wild type and gene-deletion mutants) growing on different carbon sources [44]
2. S. cerevisiae RNA-seq data for 28 analog sensitive kinase alleles across 12 different conditions (stresses and different media) [45]
3. I. orientalis RNA-seq data for growth in different media conditions (YPD+glucose and lignocellulosic extracts) [45]

Corresponding datasets from publications cited in the text:
[44.] Wu VW, Thieme N, Huberman LB, Dietschmann A, Kowbel DJ, Lee J, et al. The regulatory and transcriptional landscape associated with carbon utilization in a filamentous fungus. Proceedings of the National Academy of Sciences. 2020;117(11):6003‚Äì6013.

[45.] Mace K, Krakowiak J, El-Samad H, Pincus D. Multi-kinase control of environmental stress responsive transcription. PloS one. 2020;15(3):e0230246. pmid:32160258
View ArticlePubMed/NCBIGoogle Scholar"	"Gene condition splits for each of the 3x different data sets corresponding to each of the fungi species are noted as:
-70 % training
-10% validation 
-20% test  

Model seems to have been trained on each of the 3x corresponding data sets which contains RNA-seq expression data under different conditions. "	"Three different datasets divided into train/test set splits for use in creating the model, each described in supplementary figures 1-3:
1. S1 Fig. Vizualizing the Saccharomyces cerevisiae dataset.
2. S2 Fig. Vizualizing the Neurospora crassa dataset.
3. S3 Fig. Vizualizing the Issatchenkia orientalis dataset.

Author's first split the gene-condition data into train, validation and test sets by randomly withholding 10% of the elements for the validation set and 20% for the test set.

Difficult to discern exact training and test set data points from the text and figures relating to the publication. It appears that relating to each dataset there are:
S. cerevisiae = 6645 genes
N. crassa = 9725 genes
I. orientalis = 4925 genes 
However, it is not very clear."	4.0	0.0	Not available based on text, evaluation not heavily focused on in text.	"The evaluation of the model performance seems to be against that of the 3x distinct datasets used for the study to create the model.
-S. cerevisiae dataset (noted under data section)
-N. crassa dataset (noted under data section)
-I. orientalis dataset (noted under data section)

No other models cross compared or simpler baseline to evaluate the model. Poorly described evaluation."	No	"Pearson correlation coefficient reported for model use across the 3x datasets used:
-0.85 for S. cerevisiae
-0.72 for N. crassa
-0.81 for I. orientalis

Not many other performance measures in the text/supplementary files. "	"Independent data sets used to evaluate FUN-PROSE model.

The authors noted use of several previously published RNA-seq datasets for different fungal species to evaluate the model. Although, this is contradictory as it seems these 3x datasets were used to create the model in the first place. Somewhat unclear the barriers between create the model and datasets used to evaluate the model."	4.0	1.0	GitHub repository: https://github.com/maslov-group/FUN-PROSE. Primarily in python and jupyter notebooks. No containers present in the repo or other run method support/variations. 	"Text notes models were trained on an NVIDIA V100 GPU with 16GB of RAM using automatic mixed-precision training. 
Also further notes that there was model training on a NVIDIA GeForce GTX 1080 Ti GPU.

Execution time metrics not stated in text."	Not very interpretable - comes across as a black box from the text and formatting. The datasets used are confusing and poorly described eg: hosted in Google drive, and from cited publications. Three different fungi species were used for creating the model and not very accessible without combing trhough the text to understand the underlying RNA-seq data within each of these or their splits/data points. No clear tables to detail the splits of test/training/validation although some percents provided. Almost 20 figures in the paper when including supplementary data and very divided information points across the whole paper.  Parameters and features of the model poorly described and very little info on the fitting/evalution of the model.	Regression	4.0	0.0	"Convolutional neural network (CNN) - not a new algorithm.
-Composed of two convolutional layers"	Hyperparameters described directly in a subsection of the paper: 'Hyperparameter optimization and model training'. This does not have exact hyperparameter configurations, optimization schedule, model files, and optimization parameters reported. However, certain GitHub files in the code repo seem to contain further hyperparameter information such as 'hyperparameter-search.py' (https://github.com/maslov-group/FUN-PROSE/blob/main/hyperparameter-search.py) 	Data sources and preprocessing subsection includes info on the utisation of log-transformed RNA-seq expression levels and also the use of one-hot encoded promoter sequences later in other sections.	"Not explicitly noted in the text.

256 convolutional filters noted, therefore likely hundreds/thousands of features in the model as there is potential for multiple features to be extracted per filter."	Fitting poorly described. Unknown parameters vs features from text - cannot determine exact figures for these. Fitting not touched on in the text at all. Only comparisons of pearson correlations across use in different datasets.	No	"Specific CNN layer parameters are noted to be described in table 1 - but primarily hyperparameters instead are actually in the table.
-Titled 'Table 1.  Configuration search space for hyperparameter optimization and best hyperparameters identified in the space.'
-Parameters of the model partially described and it does not seem as if the weights/biases etc are included in one of the many figures/tables clearly. Cannot determine accurate number from text or their selection. "	Early stopping was performed: 60 epochs and training was stopped early if the validation correlation coefficient did not improve for 5 epochs in a row.	7.0	1.0	665a01aa7089c469b4646267	37971967.0	PMC10653424	02/02/2026 19:46:52	Nambiar A, Dubinkina V, Liu S, Maslov S.	PLoS computational biology	FUN-PROSE: A deep learning approach to predict condition-specific gene expression in fungi.	10.1371/journal.pcbi.1011563	2023	0.0	0.0		1.0	2024-06-17T18:54:15.621Z	2026-02-02T19:46:52.000Z	3badf5a7-cfb0-4138-813b-299c482e3bf4	undefined	3cfsqm95ys				DOME_JSON	Match	Match
667203b937ea6fa797a6c2ef	"Datasets used for training & test available on the GitHub repository within labelled CSV files. GitHub: https://github.com/BV-BRC-dependencies/zou-plasmid-prediction.

The original data sourced from BV-BRC (https://www.bv-brc.org/) should also still be avaialble provided no changes have occured to the data since the original study publication."	"Data source is from the database: PAThosystems Resource Integration Center (PATRIC).
-Note: this database is now called the Bacterial and Viral Bioinformatic Resource Center (BV-BRC).

The data type is DNA, plasmids and chromosomes.

The study classed its data as follows:
‚Äúplasmid‚Äù as positive class, Npos = 10,654 
‚Äúchromosome‚Äù as the negative class, Nneg = 10,584

Novel dataset was compiled from publicly available data - this dataset is not community recognised/re-used but the overall database content would be. "	"The dataset (exempting the holdout set) was split 7:2:1 ratio for model training, testing, and validation, respectively.

Yes - independent training and test as noted from use of hold out set."	"10,654 plasmid genomes with lengths greater than 2kb.
-plasmid dataset contained a total of 1,258 species and 485 genera
10,584 bacterial chromosomes with lengths greater than 10kb. 
-chromosomal dataset contained 2,212 species and 906 genera. 

Prior to building models: 1,000 plasmid and 1,000 chromosomal sequences were separated from the dataset to create a holdout set."	4.0	0.0	No - the evaluation is not clearly available based on GitHub repository content or linked from text.	"The different models cross-compared for the classification task could be considered baselines from lower (eg: logistic regression) to higher complexity (eg: neural network) models.

Further, the text does compare the models to existing classification methods described in previous publications:
-PlasClass
-PlasFlow
These were evaluated on the same datasets used in this study."	Confidence intervals are reported for the models performances throughout the text. 	"For classifying plasmid and chromosome sequences using 6-mers as features, the following model metrics were reported in the text:

-Accuracy
-F1 score
-Precision 
-Recall "	"10-fold cross-validation was employed for model evaluation.
"	5.0	0.0	Github containing model source code available: https://github.com/BV-BRC-dependencies/zou-plasmid-prediction. Python code primarily, no interactive notebooks. No docker/container deployment linked or clearly available. Repository license not clear.	"Clear equations for helping with the estimation of neural network memory usage are present. However, no exact compute requirements for model training or time to do so. Additionally, no run time info for use on datasets.

Text notes GPUs plural for model needs - it could be presumed more than one GPU is needed. 2x GPUs are mentioned at another point in the text and improved performance in relation to this."	Moderately interpretable - but variable by the 4x model types which makes this harder to discern interpretability. Low information on exact parameters and features in the text and somewhat confusing as there is mutliple models. However, the GitHub with data is available and model source code. Although no containers/environment control tooling/code provided. The simpler model types (e.g. random forest) are more interpretable but the more complex ones (e.g. neural network) are somewhat more complex. The authors did a decent job for describing the various DOME areas.	Classification	4.0	0.0	"4x model types were noted to have been used for the model generations based on text:

1. Logistic regression 
2. Random forest   
3. eXtreme Gradient Boosting - XGBoost   
4. Neural network

None of these models are novel"	Hyperparameters detailed in the text. The model GitHub linked may include more configuration information but unclear.  	The data encoding was through k-merisation which involved creating numerical representations of the categorical data (DNA sequences) for the machine learning models.  Likely one-hot encoding but hard to determine from text as not explictily written..	"Features are somewhat described in the text but exact feature figures not explicitly included in relation the the 4x model types used. However, some of these can be inferred based on the text.

Some more information can be extrapolated from the text based on the features below:
-K-mer size: the text mentions using k-mers of size 6 (6-mers) for feature extraction.
-Subsequence length: the model used two subsequence lengths - 2kb and 5kb.

4x nucleotides (A, T, G, C) means a 6-mer can have 4^6 = 4096 possible combinations.
The text notes they only considered the ""canonical"" k-mer - the lexicographically highest version (e.g., ""AAAAAA"" instead of ""TTTTTT""). This reduces the number of features by half (assuming even distribution of nucleotides).

Therefore, the model features for 6-mers could be inferred as (4^6 / 2) = 2048. However, given the exact numbers are not stated for the models in text this may not be true."	Unclear model fittings - there is not a lot of information in the text on the precise number of parameters or features used to generate the 4x models.	No	"Exact parameters for the 4x models are not clearly stated in the text. 

1. Logistic regression = unkown, but uses a single weight vector with a dimension equal to the number of features.

2. Random forest   = 200 (based on 200 decision trees noted in the final model.)

3. eXtreme Gradient Boosting - XGBoost  = unkown, little information in text

4. Neural network = unkown, not explicitly stated in text
-However, information for the neural network parameters can be inferred to be large as it has 7 layers with specific numbers of neurons.
-Text notes text number of neurons in each layer (256, 256, 128, 128, 32, 10, and 1).
-Hyperparameter tuning & computational Resources seem to have been considered for parameter selection.
"	10-fold cross-validation was one of the key approaches technique used to prevent overfitting. For the neural network specifically two other approaches were mentioned in the text to avoid overfitting: drop-out layers (these were applied to the first 4x layers to prevent co-adaptation and encourage generalisable features.) and L2 weight regularisation (model used a value of 0.0001 to penalise large weights to prevent overfitting.).	7.0	1.0	665a01aa7089c469b4646267	36525447.0	PMC9757591	02/02/2026 19:46:52	Zou X, Nguyen M, Overbeek J, Cao B, Davis JJ.	PloS one	Classification of bacterial plasmid and chromosome derived sequences using machine learning.	10.1371/journal.pone.0279280	2022	0.0	0.0		1.0	2024-06-18T22:01:29.043Z	2026-02-02T19:46:52.000Z	7a30fa4d-704a-4379-b242-310bf0bf9d57	undefined	x9vqohdvu0				DOME_JSON	Match	Match
66732d6237ea6fa797a6c33c	Only publications listed - by searching these publications the data only seems to be available for one of the two and linked from its text as supplementary data files (Ravel et al. in 2011 [8]:). Data splits not available. No licenses or URL.	"Data source: publications

Data type: 16S rRNA seq data (for understanding microbial communities - microbiome)

The data used consists of two different datasets drawn from studies published by:
-Ravel et al. in 2011 [8]:
Ravel J, Gajer P, Abdo Z, Schneider GM, Koenig SSK, et al. (2011) Vaginal microbiome of reproductive-age women. Proceedings of the National Academy of Science USA 108: 4680‚Äì4687.

-Srinivasan et al. in 2012 [9]:
Srinivasan S, Hoffman NG, Morgan MT, Matsen FA, Fiedler TL, et al. (2012) Bacterial communities in women with bacterial vaginosis: high resolution phylogenetic analysis reveal relationships of microbiota to clinical criteria. PLoS One 7: 6."	No information relating to the redundancy between data splits evident in the text.	"Data splits are not mentioned in the text.

Dataset sizes & data within:
-Ravel et al. dataset: 396 samples - included only asymptomatic participants
-Srinivasan et al. dataset: 220 samples - included women with and without a BV diagnosis."	4.0	0.0	No availability of evaluation files, code, etc.	"No comparison was made to any publicly available methods.

No benchmark datasets used or against simpler baselines compared to unless considering the complexity variances between the 3x models to be a simpler baseline varying by model complexity."	No confidence intervals or values noted in the text. 	"The performance measures used to evaluate the accuracy of the 3x models were:
-The accuracy of the 3x models at performing the correct classification (above 90% for Nugent score BV + above 80% for Amsel criteria BV.)
-The receiver operator curves (ROCs) of the 3x models in 'Figure 2. A comparison of the classification accuracies for each machine learning technique'"	"10-fold cross-validation was noted as the primary evaluation method used in the text.

Additionally, the authors compare the 3x models on two datasets but these seem to be the same ones used to train the models. No clarity on splits or redundancy of the model data for the evalutaion vs training."	5.0	0.0	No - no clear links to a code repository or GitHub from the publication.	"The different classification techniques varied widely in computational time: 

-Logistic regression + random forests were noted as relatively quick to run, usually completing in less than an hour on a single laptop. 

-Gaussian process classifier is noted to have taken several hours longer to run."	Black box - missing a lot of key information. No GitHub linked for model code and no links to training/test data. Very poor use of the DOME related info breakdowns and 3x models poorly described overall in text making the interpretability black box like.	Classification	4.0	0.0	"Text notes used of three machine learning algorithm types.

ML algorithms: 
1. Gaussian process classifier
2. Random forests
3. Logistic regression 

Not new algorithms.

Chose these types as some are more efficient than others at the classification needed."	No - not reported in text and no links to configurations.	Not explicitly noted in text. Data will likely have been encoded for Microbial Taxa & BV status.	"Features noted to be algorithm specific across each of the 3x models created.

Table 1. This table shows the fifteen most important features identified by the different classifiers.
-Based on this it can be inferred that the models had at least 15x features but does not list all or the total number.

Feature selection seems to have been done as the author notes that they ranked the features by their apparent importance to each model."	Potentially models were overfit. No exact p or f numbers to help determination. There are low dataset sizes noted in text (396 and 220). However, the text notes some overfitting prevention methods such as cross-validation and model complexity penalties.	No	"For the gaussian process classifier, Table 2. lists the parameter values used.
-6x parameters noted in the table and corresponding values.

For the random forest model,  the author notes use of R package 'randomForest' on default parameters. 

For the logistic regression model, the author notes use of R package 'glmnet' on default parameters. "	Ten-fold cross validation was the primary technique employed to prevent overfitting.	7.0	1.0	665a01aa7089c469b4646267	24498380.0	PMC3912131	02/02/2026 19:46:52	Beck D, Foster JA.	PloS one	Machine learning techniques accurately classify microbial communities by bacterial vaginosis characteristics.	10.1371/journal.pone.0087830	2014	0.0	0.0		1.0	2024-06-19T19:11:30.981Z	2026-02-02T19:46:52.000Z	1dcb40c7-3c20-484d-9f45-bcf91cfd4d17	undefined	ats2zi61i5				DOME_JSON	Match	Match
66735a8837ea6fa797a6c341	"Data was sourced from the Stanford HIV drug resistance database (http://hivdb.stanford.edu/).
However, the exact sequences and phenotype/genotype data used is not listed or avaialble in text/supplementary data. So the data used is not available nor the splits."	"Database - Stanford HIV drug resistance database (http://hivdb.stanford.edu/).

The data used is de-identified genotype-phenotype datasets: 23,000 protease gene sequences and 23,000 reverse transcriptase gene sequences."	"Some information in text on the splits.

Data is split by 5-fold cross-validation, this uses four folds for training and the remaining unseen fold for testing in each iteration.

However, the level of redundacy reduction (if any performed) is not clear in the text."	"Unclear if whole dataset points = 
-46,000 total (23,000 protease seqs & 23,000 reverse transcriptase seqs) 
or 
-23,000 total (containing a mix of protease seqs and reverse transcriptase seqs.)
The former is stated at first but the splits noted later in the text align with the latter.

Text notes for the data splits:
-Training dataset: 18,400 protease seqs and reverse transcriptase seqs
-Testing dataset: 4,600 protease seqs and reverse transcriptase seqs
Unclear the distibution of these.

Distibutions unclear from text and not available in any figure/tables/files."	4.0	0.0	Not available.	Comparison not undertaken.	No confidence intervals reported. '88% ¬± 7.1% improvement' is noted but unclear if specific confidence metrics.	"Some performance metrics of the model are reported. However, these are not compared to the literature or other previously published models.

1. Accuracy percentages: provide an overall measure of how well the model performs by detailing the percentage of correct predictions.

2. Positive Predictive Value (PPV) and Negative Predictive Value (NPV): metrics for these were reported and indicate the probability of a positive/negative prediction being correct.

3. Statistical Significance (aZ-score): aZ-score greater than 1.98 reported. This would indicate a p-value less than 0.05 and a statistically significant result. "	 5-fold cross-validation performed for model evaluation.	5.0	0.0	None available or linked. No GitHub, code repository available.	No information on execution time or compute requirements.	Black box. Next to no required information for model interpretation available. No datasets, no code, no parameters, features, data splits. Even the exact model algorithm in use is unclear beyond the fact it is a supervised machine learning model that performs classification.	Classification	4.0	0.0	"The algorithm used is a supervised machine learning algorithm for classification.
-However, the exact model type not mentioned or clear.
-Text refers to the model used as 'ANRS', a computer based interpretation algorithm was built by the French ANRS (Agence Nationale de Recherches sur le SIDA; National Agency for AIDS Research). ANRS classifies ARV resistance according to three levels: susceptible, intermediate, and resistant. ‚ÄòSusceptible‚Äô indicates that a particular ARV drug will be effective against HIV; ‚Äòintermediate‚Äô indicates that the ARV drug is partially effective; and if the ARV is not effective at all, it is classified ‚Äòresistant‚Äô.
-Further online searches do not clearly yield information on this model."	Configuration not available.	No information in text on how the genotype seqs or phenotype data was encoded for the model.	"Unclear from text - not explicitly stated. Text suggests after feature selection the top 10 features were selected for the model.

Yes - feature selection was performed and noted in text. The text mentions using ReliefF, MODTree filtering, FCBF filtering, and CFS filtering. These feature selection methods were used to identify the most relevant features (gene mutations) from the data for the ML task of predicting HIV drug resistance."	Overfitting less likely due to 5-fold cross-validation and limited features in use. Underfitting may be possible but low info in text not clear to determine the fit likelihood. Parameter numbers unknown.	No	Not mentioned in text - difficult to infer/extrapolate as model type is also not specified.	5-fold cross-validation technique was in use to help prevent overfitting. No others clearly noted in text.	7.0	1.0	665a01aa7089c469b4646267	29181236.0	PMC5688026	02/02/2026 19:46:52	Singh Y.	Healthcare informatics research	Machine Learning to Improve the Effectiveness of ANRS in Predicting HIV Drug Resistance.	10.4258/hir.2017.23.4.271	2017	0.0	0.0		1.0	2024-06-19T22:24:08.516Z	2026-02-02T19:46:52.000Z	297b4cc8-9937-41b5-bfb3-7469fedc1fbe	undefined	otyepo566r				DOME_JSON	Match	Match
667846cb37ea6fa797a6c4b0	"Two DNA methylation datasets GSE88929 and GSE102177 with clinical information were downloaded from the GEO database (http://www.ncbi.nlm.nih.gov/geo/). Splits are noted in the text and use of the dataset data points for test, training and validation.

Table S3 provided in supplementary data contains the sample information of the training set and testing set in the GSE88929 datasets."	"Data source: GEO database

Datat type: DNA methylation data

Acronym note: Gestational diabetes mellitus (GDM)

Two DNA methylation datasets GSE88929 and GSE102177 with clinical information were downloaded from the GEO database (http://www.ncbi.nlm.nih.gov/geo/). Both datasets were measured by the Illumina HumanMethylation450 BeadChip assays. 

-GSE88929 dataset: contained 68 umbilical cord blood samples from the newborns of mothers with GDM and 64 controls without GDM [12]. 

-GSE102177 dataset: consisted of the peripheral blood samples from 18 fullsibling pairs that were exposed to different conditions of intrauterine hyperglycemia (GDM pregnancy or non-GDM pregnancy). Therefore, there were 18 samples with exposure to maternal GDM and 18 controls without exposure to GDM in the GSE102177 dataset [23]."	"Little information on redundancy reduction methods for the data splits in the text.

Likely based on the text, the test and training were kept separate."	"Authors randomly separated the samples from GSE88929 into the training set and testing set, containing 66 samples.

Table S3 provided in the supplementary data contains the sample information of the training set and testing set in the GSE88929 datasets.

Data points:
-Test: 66 samples.
-Training: 66 samples.

The samples from GSE102177 were instead used as an independent validation set. This contained: 18 samples."	4.0	0.0	No 	No - no clear indication of comparison to other methods using benchmark datasets. Authors did note this was a novel usage of SVM for this classification task meaning there may not have been other methods to compare to.	No confidence intervals noted in the text related to the SVM model performance.	"Limited performance measures reported in the text.

Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) was used as the primary performance metric for the SVM model in the GDM study.

Reported Values: AUC values for three datasets:
-Training Set (GSE88929): AUC = 0.8138
-Testing Set (GSE88929): AUC = 0.7576
-Independent Validation Set (GSE102177): AUC = 0.6667

Additional performance measure metrics like accuracy, precision, recall, or F1-score could also be used and are absent from the text/related files."	Independent dataset used to validate the model. This was the GSE102177 dataset kept independent for model evaluation and contained 18 samples and 18 controls. 	5.0	0.0	No source code released or GitHub/ code repository available from the text.	No contextual SVM ML model execution time information in text.	Black box - no model source code linked in a repository (No GitHub/Zenodo/other). Dataset info for the training and test set is available in supplementary file. Less clear for separate valdation dataset. Very poor model optimisation information in the text/suppl. files which is also heavily contributing to black box model nature.  	Classification	4.0	0.0	"Support vector machine (SVM) ML algorithm - this is not a new algorithm.

Why chosen: 
-Previous SVM model usage success for prognosis and diagnosis of disease, such as diabetes mellitus.
-Previous successful use and application of SVMs for GDM research.
-Novelty of usage for CpG methylation biomarkers."	No - no configuration available. Little info in text on these model optimisation info points.	No clear information of how the SVM input data was encoded. Œ≤-values of CpG sites from the data seem to have been used but no further information on how these were encoded.	"Input features, (f) = 6 

Œ≤-values of CpG sites used as features: text notes using the Œ≤-values of 6 CpG sites for model development. These Œ≤-values directly represent the features that were used in the model for classification.

Yes: feature selection was performed to determine these 6x."	Fitting not clearly addressed in the text, very little information on optimising the fit to avoid over-/under- fitting case for the SVM model. Parameter size unkown but most likely somewhat larger than features used. Small number of features used (f=6). Training set size is not very large, only 66. 	No	"Parameters used for the SVM ML model are unclear.

In the simplest case there was potentially 6 parameters used based on the 6x CpG sites noted and their Œ≤-values for the model. However, given the model type there it is very likely there were more than 6x parameters used. e.g. kernel function or tuned hyperparameters."	None clearly mentioned in text if used. Validation data set noted but not in context of use for early stopping.	7.0	1.0	665a01aa7089c469b4646267	34104645.0	PMC8162250	02/02/2026 19:46:52	Liu Y, Geng H, Duan B, Yang X, Ma A, Ding X.	BioMed research international	Identification of Diagnostic CpG Signatures in Patients with Gestational Diabetes Mellitus <i>via</i> Epigenome-Wide Association Study Integrated with Machine Learning.	10.1155/2021/1984690	2021	0.0	0.0		1.0	2024-06-23T16:01:15.774Z	2026-02-02T19:46:52.000Z	725073a0-1dc7-4be3-b693-04fd39944947	undefined	3ahkdhcjev				DOME_JSON	Match	Match
670e68a561d57eb8bca69606	The data splits are created using reproducible code you can find  in our GitHub repository under: https://github.com/juaml/julearn_paper/ with a Attribution-NonCommercial-ShareAlike 4.0 International licence.	"All data we use is recognized by the community and was used by it before. As we only do replication examples our analyses and data is by design recognized. 

Replication 1 Data:   562 data points
Replication 2 Data:  498 data points  (291 controls, 207 after balancing)
Replication 3 Data: 368 data points "	The splits were created using K-Fold cross-validation. This makes training and test set independent on the level of each iteration. 	"Replication 1: Repeated K-Fold Cross-Validation with 5 repeats and 5 equal splits (80% training)
Replication 2: Repeated K-Fold Cross-Validation with 60 repeats and 2 splits (50% training) following the work to be replicated
Replication 3: Used different cross-validation schemas for different subexperiments out of the following options: 
Leave-One-Out (1 data point for testing) or Repeated  K-Fold Cross-Validation with 10 repeats and 10 equal splits (90% training)

When applying hyperparameter tuning training is spitted using another 5 Fold Cross-Validation."	4.0	0.0	All code is available here:  https://github.com/juaml/julearn_paper/ (Attribution-NonCommercial-ShareAlike 4.0 International licence). This includes information about what comparisons are made and how we got to the presented results. 	We do not claim any improvements over previous methods. Therefore we only performed comparisons also performed in the replicated work. 	The replication examples were able to replicate previous work. Where needed we also show significance and measurements of confidence, i.e. Replication 1 & 2	"We used common sets of metrics given the literature. Names of scores refer to the in julearn used names: 
Replication 1:  [
    ""neg_mean_absolute_error"",
    ""neg_mean_squared_error"",
    ""r2"",
]

Replication 2: Performed standard training, scoring with accuracy. Reported mean age of misclassified for corrected and uncorrected models

Replication 3:  [""neg_mean_absolute_error"",
    ""neg_mean_squared_error"",
    ""neg_root_mean_squared_error"",
    ""neg_median_absolute_error"",
    ""r2"",
    ""r_corr""]"	We used nested cross-validation. Therefore cross-validation.	5.0	0.0	Yes our examples are released here: https://github.com/juaml/julearn_paper/ (Attribution-NonCommercial-ShareAlike 4.0 International licence) and the actual software is released here https://github.com/juaml/julearn (GNU Affero General Public License)		Models used range in their interpretabilty, but all of them are reasonably interpretable using common methods like permutation importance. Some havea direct interpretation of weights such as gaussian models. As we do not aim to gain any new evidence interpretability of the models is not relevant for this work.	"Replication 1 Models are regression 
Replication 2 Models are classification
Replication 3 Models are regression"	3.0	1.0	"We propose software compatible with scikit-learn standard. It allows users to use any ML algorithm class compatible with that standard. 
Furthermore, we illustrated or software using multiple examples (including 3 replications).
Here we used the following algorithm classes: 
SVM, RVR, Gaussian Models and unsupervised methods like: PCA & CBPM. 

There are no newly proposed ML algorithms."	This information ist included in our GitHub repository under: https://github.com/juaml/julearn_paper/ with a Attribution-NonCommercial-ShareAlike 4.0 International licence	PCA, Z-Scoring, Feature Selection, Confound Regression	"All preprocessing steps including feature selection were trained only on the training set in a CV consistent way.
Variance thresholding was used in Replication Example 1.
CBPM thresholds significantly correlated features with the target and was used in Example 3."	Our analyses are replications of previous research following there setup as we only want to show that our software is able to reproduce previous research. Therefore we know that we at least fitted as well as previous research. Overfitting was ruled out by our regigorous nested cross-validation setupts. As mentioned before we used feature selection or PCA to reduce the number of features if needed to decrease the p. 	No meta-predictions were used.	"Using notation of Hyperparameter=ListOfParameters
CV -> Cross-Validation

Replication Example 1:
RVR 1 - using CV: kernel=[""linear"", ""poly""], degree=[1, 2] and Model 2 using CV: kernel=[""linear"", ""rbf"", ""poly""], C=[0.01, 0.1]

Replication Example 2: 
SVM - using CV: C=np.arange(0.1, 4, 0.2)

Replication Example 3: 
CBPM - using manual combinations documented in open source code: 
corr_signs = [""pos"", ""neg"", ""posneg""]
significance_threshold = [0.01, 0.05, 0.10 p]
"	Maninly nested cross-validation.	8.0	0.0	670e68a5908d74f9cb33dfa4	38496213.0	PMC10940896	02/02/2026 19:46:52	Hamdan S, More S, Sasse L, Komeyer V, Patil KR, Raimondo F, Alzheimer‚Äôs Disease Neuroimaging Initiative.	GigaByte (Hong Kong, China)	Julearn: an easy-to-use library for leakage-free evaluation and inspection of ML models.	10.46471/gigabyte.113	2024	0.0	0.0		1.0	2024-10-15T13:05:41.358Z	2026-02-02T19:46:52.000Z	567d717d-1210-4ce3-92f1-effeabb4d133	undefined	z5ym87wvmo				DOME_JSON	Match	Match
670e790061d57eb8bca69627	Yes, all the data are released in the PhageGE github example data (https://github.com/JinxinMonash/PhageGE/tree/main/Example%20data).	The dataset was selected from Mavrich, T., Hatfull, G. Bacteriophage evolution differs by host, lifestyle and genome. Nat Microbiol 2, 17112 (2017). There are 604 positive (temperate) and 453 negative cases (lytic) for the whole dataset. 	The training and test sets are independent using a 60:40 split.	634 genomes were included in the training set while 423 genomes were included in the test set. We applied cross-validation to tune model hyper-parameters, where the training set was randomly split into individual training and validation sets.	4.0	0.0	We shared the benchmark dataset in the phageGE github under the example data folder.	Our manuscript include the comparison of the performance of PhageGE and other methods.	We do provided classification accuracy of each compared method across both the training and test datasets. Based on the comparison, the performance of phageGE is superior to others. 	Our manuscript include the comparison of the performance of PhageGE and other methods.	Cross-validation, independent dataset	5.0	0.0	Yes, the source code is released in the PhageGE github page. And it has been incorporated in our phageGE webserver.	Seconds on HPC cluster	Black box 	Classification	4.0	0.0	Random forest classifier 	Data and related files for the development of final model have been shared in phageGE github (https://github.com/JinxinMonash/PhageGE/tree/main/Example%20data)	Conserved Domain Database (11/2023) for protein domains that mechanistically involved in lysogeny were collected and manually curated. In the meantime, each genome sequence in the training set, a list of all possible 6-frame translation sequences for all genomes were generated with rhmmer package. 	477 features (protein domains) were collected initially. Testing set was not used for feature selection, pre-processing steps or parameter tuning.	We optimised the initial data collection (from CDD) strategy to limit the possibility of over-fitting. We also performed the comparison of the incorrect predictions of training and testing.		"‚Äòbootstrap‚Äô (True, False), ‚Äòclass_weight‚Äô (balanced, balanced_subsample), ‚Äòmin_samples_leaf‚Äô , ‚Äòn_estimators‚Äô , and ‚Äòmax_depth‚Äô. 
GridSearchCV were used to evaluate all possible parameters or their combinations.
"	We limited the amount of initial features (protein domains from CDD) from the data collection.	8.0	0.0	670e7885908d74f9cb3420e6	39320317.0	PMC11423353	02/02/2026 19:46:52	Zhao J, Han J, Lin YW, Zhu Y, Aichem M, Garkov D, Bergen PJ, Nang SC, Ye JZ, Zhou T, Velkov T, Song J, Schreiber F, Li J.	GigaScience	PhageGE: an interactive web platform for exploratory analysis and visualization of bacteriophage genomes.	10.1093/gigascience/giae074	2024	0.0	0.0		1.0	2024-10-15T14:15:28.964Z	2026-02-02T19:46:52.000Z	d77983e0-5279-4379-b608-8032a2990b09	undefined	09r03h9clm				DOME_JSON	Match	Match
6780005469b4c4f26423614a	GEO (public upon acceptance)	The DNA methylation data of the CD34+/CD38‚àí/Lin‚àí cord blood hematopoietic stem cells  (uHSCs) was directly obtained from a multi-ethnic cohort of 72 pregnant women (Npos = 34 obese; Nneg = 38 non-obese) from Kapiolani Medical Center for Women and Children in Honolulu, Hawaii (collected between 2016 and 2018). 	The data was randomly split into 8:2 train/test ratio. Training and test sets were independent.	A total of 72 samples were split into 8:2 train/test ratio, with 59 samples in the training set, 13 samples in the testing set. No separate validation set was used. The distribution of samples in train and test are balanced. 	4.0	0.0	Codes are available in https://github.com/lanagarmire/COBRE_methyl with GPL-3.0 license	No	No, only one-time prediction performance metric was reported.	Balanced accuracy, accuracy, F1 score	5-fold cross-validation on obesity data. Prediction on external independent TCGA datatsets.	4.0	1.0	Code released in https://github.com/lanagarmire/COBRE_methyl with GPL-3.0 license.	Run time 0.005 second on HPC.	This random forest obesity model offers some interpretability features based on feature importance scores. For example, cgxxxxxxxx has the highest feature importance in obesity classification.	Classification	4.0	0.0	Random forest model was used for normal vs obesity classification.	All codes are available in https://github.com/lanagarmire/COBRE_methyl with GPL-3.0 license	Methylation beta values were directly used used after preprocessing (batch removal, normalization, etc.).	61 predictors were used in the model. They were selected from top obesity-related pathways.	No. P is smaller than sample size.	No. The model directly use methylation data as input.	Based on default tuning strategy, p was implicitly selected based on the heuristic sqrt(predictors) = 7.81025.		8.0	0.0	677eef3ddd85bd59b303c812	40388307.0	PMC12087453	02/02/2026 19:46:52	Du Y, Benny PA, Shao Y, Schlueter RJ, Gurary A, Lum-Jones A, Lassiter CB, AlAkwaa FM, Tiirikainen M, Towner D, Ward WS, Garmire LX.	GigaScience	Multiomics analysis of umbilical cord hematopoietic stem cells from a multiethnic cohort of Hawaii reveals the intergenerational effect of maternal prepregnancy obesity and risks for cancers.	10.1093/gigascience/giaf039	2025	0.0	0.0		1.0	2025-01-09T16:59:00.197Z	2026-02-02T19:46:52.000Z	0c113f48-b8de-4364-9535-6a6d64345ca7	undefined	0hgj0n9qo2				DOME_JSON	Match	Match
67c59b331f0965481b0c74f2	"Data availability section of paper includes the data sources.

GitHub links the exact data used: 
For genetics:
UniRef90: v2020_01 - https://ftp.uniprot.org/pub/databases/uniprot/previous_releases/release-2020_01/uniref/
MGnify: v2018_12 - http://ftp.ebi.ac.uk/pub/databases/metagenomics/peptide_database/2018_12/
Uniclust30: v2018_08 - http://wwwuser.gwdg.de/~compbiol/uniclust/2018_08/
BFD: only version available - https://bfd.mmseqs.com/

For structural templates:
PDB: (downloaded 2020-05-14) - no link in GitHub
PDB70: 2020-05-13 - http://wwwuser.gwdg.de/~compbiol/data/hhsuite/databases/hhsuite_dbs/old-releases/pdb70_from_mmcif_200513.tar.gz

Mirrored Databases
The following databases have been mirrored by DeepMind, and are available with reference to the following:
BFD (unmodified), by Steinegger M. and S√∂ding J., available under a Creative Commons Attribution-ShareAlike 4.0 International License.
BFD (modified), by Steinegger M. and S√∂ding J., modified by DeepMind, available under a Creative Commons Attribution-ShareAlike 4.0 International License. See the Methods section of the AlphaFold proteome paper for details.
Uniref30: v2021_03 (unmodified), by Mirdita M. et al., available under a Creative Commons Attribution-ShareAlike 4.0 International License.
MGnify: v2022_05 (unmodified), by Mitchell AL et al., available free of all copyright restrictions and made fully and freely available for both non-commercial and commercial use under CC0 1.0 Universal (CC0 1.0) Public Domain Dedication.

All input data are freely available from public sources - summary listing:
Structures from the PDB were used for training and as templates (https://www.wwpdb.org/ftp/pdb-ftp-sites; for the associated sequence data and 40% sequence
clustering see also https://ftp.wwpdb.org/pub/pdb/derived_data/ and https://cdn.rcsb.org/resources/sequence/clusters/bc-40.out). 
Training used a version of the PDB downloaded 28/08/2019, while CASP14 template search used a version downloaded 14/05/2020. Template search also used the PDB70 database, downloaded 13/05/2020 (https://wwwuser.gwdg.de/~compbiol/data/hhsuite/databases/hhsuite_dbs/).
For MSA lookup at both training and prediction time, they used UniRef90 v2020_01 (https://ftp.ebi.ac.uk/pub/databases/uniprot/previous_releases/
release-2020_01/uniref/), BFD (https://bfd.mmseqs.com), Uniclust30 v2018_08 (https://wwwuser.gwdg.de/~compbiol/uniclust/2018_08/), and MGnify clusters
v2018_12 (https://ftp.ebi.ac.uk/pub/databases/metagenomics/peptide_database/2018_12/).
Uniclust30 v2018_08 was further used as input for constructing a distillation structure dataset.
"	"Data source: Several databases

Databases:
-Worldwide Protein Databank (PDB) for protein structures. Downloaded  28 August 2019.  
-UniProt data: used to construct BFD (below)
-Big Fantastic Database (BFD): 65,983,866 families represented as multiple sequence alignments (MSA) and hidden Markov models (HMMs) covering 2,204,359,010 protein
-Uniref9067 v.2020_01, BFD, Uniclust3036 v.2018_08 and MGnify6 v.2018_12 - for MSA lookup at both training and prediction time.
-Uniclust3036 v.2018_08 to construct a distillation structure dataset - for sequence distillation.
-CASP14 template search used the PDB70 database. Downloaded 13 May 2020. 

Total data points: variable across input data type and not clearly disclosed in paper - could be inferred from supplementary data files linked on paper or the GitHub linked data files. 
Pos v Neg data classes: not applicable.

Dataset use by community: databases used are well recognised and the gold standard for the data types used. The evaluation data for CASP is also a standardised set of data for comparative benchmarking of protein structure prediction ML models."	"How were the sets split?: testing data from CASP14 is a completely unseen and diverse test data set used for benchmarking protein structure prediction.  This data is not allowed to be used for training.

Are the training and test sets independent?: yes 

How was this enforced:  pairwise sequence identity was taken into consideration.  Clustering sequences to avoid similar sequence identities in test considered. Authors address issue of sequence identity in biological data sets and sequence similarity is carefully controlled.

How does the distribution compare to previously published ML datasets?: using standard CASP14 rules this aligns to all other models participating in the competition. CASP has long history of addressing distribution of proteins in test.  "	"Data splits:
-Training: figures not explicitly stated in paper - can be inferred from various input file availability. Exact figures unclear also in supplementary material.

-Test: figures not explicitly stated paper - can be inferred from file availability and CAPS14 competition details. 

-Validation: yes, validation set used - to select the best model during training they monitor lDDT-CŒ± performance on a validation set of targets collected from CAMEO over 3 months period (2018-12-14 to 2019-03-09).  Exact validation figure of CAMEO targets not stated in text and unclear from supplementary data file. 

Distributions: yes - the distributions are different. Much wider range of proteins used in the training phase vs smaller test set from CASP14 containing previously unseen proteins. However, CASP14 test set purposely designed to provide diverse range of structure to predict for its competition. 

Distribution plotting available: not available in paper or supplementary file."	4.0	0.0	Not explicitly provided - but enough content in GitHub contains data to reproduce the evaluation using CASP14 methods. Some CASP14 assessment files: https://predictioncenter.org/download_area/CASP14/.  	Yes - comparison was made against other methods in CASP14 competition, all publicly available. 	Yes - confidence intervals used in main text reporting of accuracy performance metrics. Statistical significance exists backing the method is superior to others across many performance measures and details provided by evaluation in the CASP14 competition.  	"In the main paper primarily accuracy is reported as the main performance measure. Figure 2 for example contains accuracy graphs.

In CASP14 assessment the performance measures can be found detailed here: https://predictioncenter.org/casp12/doc/help.html

Details on some of the main protein structure prediction performance measures:
Accuracy
Global Distance Test - Total Score
Root Mean Square Deviation 
TM-score 
Global Local Distance Difference Test 
Contact Score

Machine learning specific model performance measures:
AUC-ROC (Area Under the Receiver Operating Characteristic Curve): Used for assessing contact and distance predictions (evaluating classification of residue-residue contacts).
AUC-PR (Area Under the Precision-Recall Curve): Used alongside AUC-ROC for imbalanced data in contact prediction.
Pearson Correlation Coefficient: Measures correlation between predicted and actual residue-residue distances.
Spearman Rank Correlation: Evaluates ranking consistency between predicted and actual distances.
PPV (Positive Predictive Value): Measures precision in predicting contacts.
Precision & Recall: Assesses accuracy of long-range contact predictions.

Alphafold 2 measure results in CASP14: https://predictioncenter.org/casp14/results.cgi?groups_id=205&submit=Submit"	"Independent dataset from CASP14.
-
Independent datasets: AlphaFold 2 was evaluated using CASP14 and further validated with CAMEO, ensuring that the evaluation was on data not seen during training.
Cross-validation: The model used an exponential moving average (EMA) to select the best performing model during training.
Novel experiments and metrics: AlphaFold 2's predictions were evaluated using metrics like lDDT-CŒ±, RMSD, and pLDDT to assess the accuracy and confidence of structure predictions.
Ablation studies: Performed to understand the impact of various components of the model.
Real-world application testing: AlphaFold 2 was tested on proteins with no known homologs to evaluate its ability to predict structures de novo.
"	5.0	0.0	Yes: paper has a clear 'Code availability section'. GitHub software availability: https://github.com/google-deepmind/alphafold . Apache 2.0 License. Docker image w/ instructions provided in GitHub & Google Deepmind provided server also to use it: https://alphafoldserver.com/welcome (requires login).	"Training execution time:
Takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days when training the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, hence the model uses 128 TPU v3 cores. 
HPC & GPU/TPUs necessary.

Prediction inference time: 
The full time to make a structure prediction varies  depending on the length of the protein. Representative timings for the neural network using a single model on V100 GPU are 4.8 min with 256 residues, 9.2 min with 384 residues and 18 h at 2,500 residues. 
GPU/TPUs necessary for good speeds.

More information on inference prediction times from GitHub: https://github.com/google-deepmind/alphafold?tab=readme-ov-file#note-on-casp14-reproducibility
The table reports prediction runtimes for proteins of various lengths. All runtimes are from a single A100 NVIDIA GPU. 
Table data example: 100 AA residues = 4.9 seconds to predict etc. (up to 5,000 residues)
"	Moderately interpretable in spite of deep learning approach. Great efforts have been made to make the model interpretable despite it being a deep neural network method. The paper contains a dedicated section 'Interpreting the neural network'. This section has related Fig 4a. where the different contributing components are assessed to understand their contribution to the model's protein prediction accuracy.  All code and data as well are available and well documented to further support interpretability of the model. Some limitations surrounding lack of exact parameters/features numbers information summarised and info hidden deep in supplementary files/GitHub.  	Mixed model output leveraging both regression and classification at different stages for final protein structure prediction.	4.0	0.0	"Mix of classical and novel ML algorithms applied for AlphaFold 2 model:
Deep Neural Network (DNN) approaches:
-Transformer-based model: Evoformer (novel). Chosen for ability to consider both simultaneously multiple sequence alignments (MSAs) and pairwise amino acid residue representations to support the structure prediction.
-Graph Neural Network (GNN) - used in structure module of the model"	Most configuration information available on GitHub: https://github.com/google-deepmind/alphafold. Some config info also reported in the supplementary materials file. However, not all specifics are fully available for the configurations - this has led to initiatives such as OpenFold to provide an AlphaFold re-implementation with all relevant details made available for users - https://github.com/aqlaboratory/openfold/tree/main. AlphaFold Model files: Available for download on the AlphaFold GitHub repository. License: Open-source under the Apache 2.0 License with specific restrictions for commercial use.	From supplementary files most info present on encoding methods. Protein sequences: One-hot encoding & Learned embeddings (captures biochemical properties). Multiple Sequence Alignment (MSA): Integer encoding. Residue-Residue Interactions: pair representation encoding information about the relation between the residues. 	"Exact feature number not explicitly written in text.

Supplementary files contain feature information: Table 1 Input features to the model. Difficult to infer exact feature number from this due to complexity of the model. "	Fitting not mentioned in main text but info related to how it is addressed in supplementary materials file. Exact number of parameters and training points not disclosed makes it difficult to comment on fitting. However, available files in GitHub could support determining the fit.	No	"Unclear from the paper an exact figure for the parameters used in the model. Information also unclear from supplementary information.
Supplementary materials note: concrete values for these parameters are given in the training details (subsection 1.11). However, no figures noted explicitly stating parameters used for the model.
GitHub linked file contains various AlphaFold model parameters: https://storage.googleapis.com/alphafold/alphafold_params_2022-12-06.tar . After examining the file it would appear there are approx 90 million parameters. This would need to be further verified and confirmed with the authors. "	For evaluation they employed use of an exponential moving average of the trained parameters with the decay 0.999. To select the best model during training they monitored lDDT-CŒ± performance on a validation set of targets collected from CAMEO over 3 months period (2018-12-14 to 2019-03-09).  Drop out and loss functions  also used. Most information in supplementary materials.	7.0	1.0	665a01aa7089c469b4646267	34265844.0	PMC8371605	02/02/2026 19:46:52	Jumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronneberger O, Tunyasuvunakool K, Bates R, ≈Ω√≠dek A, Potapenko A, Bridgland A, Meyer C, Kohl SAA, Ballard AJ, Cowie A, Romera-Paredes B, Nikolov S, Jain R, Adler J, Back T, Petersen S, Reiman D, Clancy E, Zielinski M, Steinegger M, Pacholska M, Berghammer T, Bodenstein S, Silver D, Vinyals O, Senior AW, Kavukcuoglu K, Kohli P, Hassabis D.	Nature	Highly accurate protein structure prediction with AlphaFold.	10.1038/s41586-021-03819-2	2021	0.0	0.0	Protein, Structural biology, AlphaFold	1.0	2025-03-03T12:06:11.420Z	2026-02-02T19:46:52.000Z	1edf8b75-cb6e-41a1-947c-e2da6a879bb0	undefined	qew9jmdocf				DOME_JSON	Match	Match
67c5b98a1f0965481b0c75bb	The datasets, the source codes and the trained model are available on https://github.com/biomed-AI/LMDisorder.	"The dataset comprises 4229 protein sequences (DM4229), with 72 fully disordered chains from DisProt v5.0 and 4157 high-resolution X-ray crystallography structures from PDB.
Additionally, there are also four independent test datasets: SL329, DisProt228, Mobi9230, and DisProt452,"	"The datasets were split into training, validation, and testing sets randomly.
To ensure independence between training and test sets, redundancy reduction was enforced by using a sequence similarity cutoff of <25%, as determined by BLASTClust."	"The training set consists of 2700 proteins, the validation set 300 proteins, and the testing set 1229 proteins (DM1229).
There are also four independent test datasets: SL329, DisProt228, Mobi9230, and DisProt452."	4.0	0.0	No	"The model was compared with seven single-sequence-based methods (ESpritz-N, ESpritz-D, ESpritz-X, IUPred2A-short, IUPred2A-long and Spot-Disorder-Single) on DM1229 and SL329. 
In addition, it was compared to NetSurfP-3.0, which is based on ESM-1b language model. "	No	Authors employed the area under the receiver operating characteristic curve (AUCROC), precision (Pr), sensitivity (Se), specificity (Sp), the area under the precision-recall curve (AUCPR), Matthews correlation coefficient (MCC), and the weighted score Sw (Sw = sensitivity + specificity ‚Äì 1) to evaluate the performance of the model.	"Authors evaluated the model on a testing set of 1229 proteins (DM1229), in addition to four independent test datasets SL329, DisProt228,
Mobi9230 and DisProt452."	3.0	2.0	The datasets, the source codes and the trained model are available on https://github.com/biomed-AI/LMDisorder.	For a protein with 500 residues it takes 1 second on an Nvidia GeForce RTX 3090 GPU.	No mention was made on interpretability. The model appears to be a black box due to its complexity.	It is a binary classifier model.	4.0	0.0	"The model is comprised of transformer networks and a fully connected layer (ANN).
It is not a novel algorithm."	The datasets, the source codes and the trained model are available on https://github.com/biomed-AI/LMDisorder.	First, the protein sequence is inputted into the pretrained language model ProtTrans to yield the sequence embedding, which is augmented by Gaussian noise to avoid overfitting.	Authors extracted the hidden states from the last layer of the ProtTrans encoder as sequence features, which is an n √ó 1024 matrix (n is the sequence length).	The exact number of total parameters not reported. To rule out overfitting, early stopping was used with validation set and additionally, a dropout rate of 0.3 was applied.	Yes, the model uses embeddings generated by unsupervised pretrained language models, namely ProtTrans (ProtT5-XL-U50) and ESM-1b.	"The number of total parameters is not explicitly stated but authors reported following parameters:
2-layer transformer network
128 hidden units
Hyperparameters: h = 4, Œµ = 0.05
Batch size: 12
Dropout rate: 0.3
Adam optimizer with learning rate of 3 √ó 10^(-4)
Binary cross-entropy loss function
Implemented with PyTorch 1.7.1

p was selected through searching all hyperparameters through a grid search."	 Yes, early stopping was used with validation set. Additionally, a dropout rate of 0.3 was applied.	8.0	0.0	65e73fdb92c76639b8e309f3	37204193.0		02/02/2026 19:46:52	Song Y, Yuan Q, Chen S, Chen K, Zhou Y, Yang Y.	Briefings in bioinformatics	Fast and accurate protein intrinsic disorder prediction by using a pretrained language model.	10.1093/bib/bbad173	2023	0.0	0.0	CAID, Critical Assessment of Protein Intrinsic Disorder	1.0	2025-03-03T14:15:38.119Z	2026-02-02T19:46:52.000Z	48c51541-34cd-47ad-a8b8-111095cd92e0	undefined	gd9a2ccysk				DOME_JSON	Match	Match
67c5bb071f0965481b0c75dc	Not splits, but the source data was collected by parsing the publicly available DisProt repository (https://www.disprot.org/).	"745 experimentally annotated proteins from the DisProt 7.0 database.
DisProt has been used frequently by other communities and publications in the field of disorder prediction."	"Data split was done randomly.
Test dataset was reduced to 176 proteins that share <25% sequence similarity to the training proteins."	"Training dataset: 445 proteins
Validation dataset: 100 proteins
Test dataset: 200 proteins"	4.0	0.0	No	The model was compared to other disorder prediction methods, such as IUPred, Espritz, and Spot-Disorder.	Confidence intervals were not reported. The results indicate superior performance compared to other similar methods and software.	F1-score, MCC, and ROC-AUC are reported as performance metrics.	The evaluation is based on the performance of the model on the test dataset.	4.0	1.0	Authors released the code for flDPnn at https://gitlab.com/sina.ghadermarzi/fldpnn.	5 to 10 s per protein.	No mention was made on interpretability. The models seems to be a black box.	It is a classifier.	4.0	0.0	"Deep feedforward neural network. 
It is not a novel algorithm."	Authors released the code for flDPnn at https://gitlab.com/sina.ghadermarzi/fldpnn.	A protein sequence is analyzed by other previously mentioned algorithms to create sequence profiles, the sequences profiles are encoded and passed to the main ML models created by the authors.	The profiles are encoded into three feature sets.	No	"Multiple predictors are used to generate sequence profiles that are later fed into the ML model.
IUPred, PSIPRED, DisoRDPbind, DFLpred, fMoRFpred."	"The exact number of parameters not reported.
Authors empirically selected the hyper-parameters including number of layers and the number of nodes in the hidden layer by a grid search."	Authors clustered the data points using CD-HIT (sequence similarity clustering technique) to prevent overfitting.	7.0	1.0	65e73fdb92c76639b8e309f3	34290238.0	PMC8295265	02/02/2026 19:46:52	Hu G, Hu G, Katuwawala A, Wang K, Wu Z, Ghadermarzi S, Gao J, Kurgan L.	Nature communications	flDPnn: Accurate intrinsic disorder prediction with putative propensities of disorder functions.	10.1038/s41467-021-24773-7	2021	0.0	0.0	CAID, Critical Assessment of Protein Intrinsic Disorder	1.0	2025-03-03T14:21:59.439Z	2026-02-02T19:46:52.000Z	c2e4770f-5db5-45b6-8ce2-dc01b58f2a03	undefined	pxq6fqown7				DOME_JSON	Match	Match
67d02e3e478cfd7e6983b11a	"Dataset released from link hosted on Nature paper: https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.3300/MediaObjects/41587_2015_BFnbt3300_MOESM62_ESM.zip . Available in sub-folder 'data'. 

ENCODE ChIP-seq experiments from Uniform Peaks of Transcription Factor ChIP-seq from ENCODE. Analysis available at
http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeAwgTfbs
Uniform/

Also data should be available from author's website for Deepbind but response time error on 3rd March 2025 indicates it is no longer accessible: http://tools.genes.toronto.edu/deepbind/"	"Data source: databases & previous publications

DNA & RNA data used for the model was formed from thousands of public datasets, including data types: Protein binding microarrays (PBM), RNAcompete, ChIP-seq,CLIP-seq and HT-SELEX experiments.

Pos vs Neg - classes: not applicable

Dataset community recognition: yes - some test data from the revised DREAM5 TF-DNA Motif Recognition Challenge which is community recognised."	"How were the sets split? :
-RNAcompete probe data: is split into nearly equally sized sets. Approx 50% training and 50% test. 
-ENCODE ChIP-seq data sets: trained on 500 odd-numbered peaks, and then evaluated the models on the top 500 even-numbered peaks
-HT-SELEX TF binding models: no clear numeric test and training split noted"	"Total data points: vary across the Deepbind models. Not clearly listed in paper. Inference possible from supplementary and data files.

RNA-binding protein (RBP) models: 
RNAcompete probe data is split into nearly equally sized sets called SetA (training) and SetB (test). No exact figure provided.
1/3 of SetA is used as holdout data. 

ENCODE ChIP-seq TF binding models:
506 in vivo ENCODE ChIP-seq data sets
Training: top 500 odd-numbered peaks
Test: top 500 even-numbered peaks 

HT-SELEX TF binding models:
1786 HT-SELEX data sets provided by Jolma et al. (citation 34)
HT-SELEX data for 303 human DNA-binding domains, 84 mouse DNA-binding domains and 151 full-length human transcription factors - representing 411 distinct transcription factors
No clear numeric test and training split noted

DREAM5 transcription factor (TF) binding models:
Training:
TFs 1‚Äì33 are trained on HK probes.
TFs 34‚Äì66 are trained on ME probes.
Test Set:
TFs 1‚Äì33 are tested on ME probes.
TFs 34‚Äì66 are tested on HK probes."	4.0	0.0	The code for performing the experiments is available for download in the Nature Supplementary Software. Licensing statment: 'Your access to and use of the downloadable code (the ‚ÄúCode‚Äù) contained in this Supplementary Software is subject to a non-exclusive, revocable, non-transferable, and limited right to use the Code for the exclusive purpose of undertaking academic, governmental, or not-for-profit research. Use of the Code or any part thereof for commercial or clinical purposes is strictly prohibited in the absence of a Commercial License Agreement from Deep Genomics. (info@deepgenomics.com)'	"Yes authors compared against publicly available methods - compared against DREAM5 TF-DNA Motif Recognition Challenge by Weirauch et al. publication which evaluates 26 existing ML method algorithms that were trained on PBM measurements, for example FeatureREDUCE, BEEML-PBM, MatrixREDUCE, RankMotif and Seed-and-Wobble. 

Author's tested DeepBind under the same conditions and outperformed all 26 methods (Fig. 3a). DeepBind also ranked first among 15 teams when submitted to the online DREAM5 evaluation script (Supplementary Table 1)."	None mentioned directly in text. Figure 3: Quantitative performance on various types of held-out experimental test data - does have plots with confidence intervals e.g. for AUCs (e). Results directly evaluated against standardized evaluation DREAM5 which aids claiming statistical superiority.	"Largely reported in Figure 3: Quantitative performance on various types of held-out experimental test data. This Figure text notes specific links to supplementary data files with  further info.

Also noted of performance measures: 
-Pearson correlation between the predicted and actual probe intensities,
-Spearman correlation
-Values from area under the receiver operating characteristic (ROC) curve (AUC) computed by setting high-intensity probes as positives and the remaining probes as negatives. 
-DREAMS PBM score
-Wilcoxon one-sided signed-rank test & related p-values
-Mann-Whitney U test

Yes representative compared to literature as these are used in other evaluations such as the Weirauch et al evaluation."	"Methods were evaluated using:
-3-fold cross validation on the training set.
-Independent dataset


"	5.0	0.0	The model software is available in the Nature Biotechnology hosted Supplementary Software file link as a zip file containing a folder within labelled 'code' - https://www.nature.com/articles/nbt.3300#MOESM62. This link provided linked to text is no longer working but should also provide an executable file: http://tools.genes.toronto.edu/deepbind/.  Licensing statment: 'Your access to and use of the downloadable code (the ‚ÄúCode‚Äù) contained in this Supplementary Software is subject to a non-exclusive, revocable, non-transferable, and limited right to use the Code for the exclusive purpose of undertaking academic, governmental, or not-for-profit research. Use of the Code or any part thereof for commercial or clinical purposes is strictly prohibited in the absence of a Commercial License Agreement from Deep Genomics. (info@deepgenomics.com)'	"GPU & CPU noted for training and inference. 
-GeForce GTX TITAN 
-Intel Core i7 3.5GHz
 
RNA-binding protein (RBP) models:
-On 244 RNAcompete RBPs, the median running time of MatrixREDUCE was nearly 15 minutes and the mean was nearly 110 minutes on Intel(R) Xeon(R) CPU E5-4650 @ 2.70 GHz. Moreover, the 10th and 90th percentile were nearly 10 minutes and two hours, respectively. The maximum time running for a single run was nearly three days.

ENCODE ChIP-seq TF binding models:
-The median and mean running time of MEME-ChIP (only the MEME module) were nearly 16 minutes on Intel(R) Xeon(R) CPU E5-4650 @ 2.70 GHz. The standard deviation of running times was nearly 100 seconds. Note that MEME-ChIP has a default maximum number of input sequences of 600, and maximum total number of input bases of 100,000. The median number of input sequences for each ChIP-seq data set is 17,000. Considering MEME‚Äôs ùí™((ùëõùëô) 2ùë§) running time26 (ùëõ, ùëô, ùë§ denote number and length of the input sequences, and motif width, respectively), we do not report results for MEME-ChIP using ‚Äúall remaining‚Äù peaks because even with 10,000 peaks the program did not terminate after 24 hours of CPU time.

HT-SELEX TF binding models:
-For MEME-1K the median and mean run time were nearly three minutes and the standard deviation was 165 seconds. However, for MEME-5K, the mean and median were nearly 12 hours and the standard deviation was 115 minutes. The 90th percentile of running times was more than 13  hours (for 30nt and 40nt sequences)."	Model is moderately interpretable: good efforts to detail the methods in the 28 page supplementary information file. Some limitations given the models nature as a deep neural network (CNN). This lowers interpretability and explainable AI approaches not implemented to breakdown the neural network layers and understand exactly how the model works. Data and code also shared which aids the interpretability as well as good details provided on hyperparameters and other such elements which aid interpreting the model.	"Classification & Regression
-Binding scores can be real-valued measurements or binary class label"	4.0	0.0	"Deep Convolutional Neural Network (CNN)

Not a new ML algorithm."	Hyperparameters:  Supplementary Table 10. Parameters noted in main supplementary text file section: 2.1 Calibration parameters. Model files available from Nature methods file download link in 'Supplementary Software' - https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.3300/MediaObjects/41587_2015_BFnbt3300_MOESM62_ESM.zip . Licensing statment: 'Your access to and use of the downloadable code (the ‚ÄúCode‚Äù) contained in this Supplementary Software is subject to a non-exclusive, revocable, non-transferable, and limited right to use the Code for the exclusive purpose of undertaking academic, governmental, or not-for-profit research. Use of the Code or any part thereof for commercial or clinical purposes is strictly prohibited in the absence of a Commercial License Agreement from Deep Genomics. (info@deepgenomics.com)'	Not described in main text but is detailed in supplementary file. The encoding is one hot encoding with padding for the sequence data representation for the model. 	"Number of features (f): 1192  - noted in main supplementary file section: 9.3 DeepFind model structure

Yes feature selection performed.

Feature selection:  features extraction based on the wild type and mutant DNA sequences. Then additional relevant features were added such as distance to the nearest transcription start site (TSS) and conservation-based features from PhyloP and PhastCons tracks, which measure evolutionary conservation of the genomic regions. These feature additions were selected after beiing checked against the AUC performance which improved when included. "	 p to f exact numbers unclear to infer fit. Authors note in supplementary file that the calibration phase automatically chooses enough regularization (weight decay, learning rate, momentum, dropout, early stopping) to mitigate the over-fitting problem for the final model.	No	"Total parameters not explicitly mentioned in main text. Information on parameters configuration in main supplementary text file section: 2.1 Calibration parameters, noted with hyperparameters in python config file.

p selection:  authors note use of deep learning techniques (citations 13,14,15,16) to infer model parameters and to optimize algorithm settings. "	Authors incorporated several regularizers used by the deep learning community, including: dropout, weight decay and early stopping (Supplementary Notes, sec. 2).	7.0	1.0	665a01aa7089c469b4646267	26213851.0		02/02/2026 19:46:52	Alipanahi B, Delong A, Weirauch MT, Frey BJ.	Nature biotechnology	Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning.	10.1038/nbt.3300	2015	0.0	0.0	RNA, Binding	1.0	2025-03-11T12:36:14.882Z	2026-02-02T19:46:52.000Z	9bf5db4f-637c-4a66-87d6-66788cf9a54e	undefined	jqi03r1d0i				DOME_JSON	Match	Match
6836c0b81d1a98db540a8619	"https://zenodo.org/records/10554304
License: Creative Commons Zero v1.0 Universal"	"ColoPola dataset is Colorectal cancer Polarimetric images dataset

The dataset consists of 572 slices (specimens) with 20,592 images, 284 slices of which were designated as cancer samples and 288 as normal samples.
Each sample has 36 polarimetric images (i.e., HH, HV, HP, HM, HR, HL, VH, VV, VP, VM, VR, VL, PH, PV, PP, PM, MH, MV, MP, MR, ML, MM, RH, RV, RP, RM, RR, RL, LH, LV, LP, LM, LR, and LL).
Each folder in the ColoPola dataset consists of 36 polarimetric images. Each image is 1280x1024 pixels in size and was created in the TIF file format (HH.tif, HV.tif, ..., LL.tif). "	The dataset is divided by sample slices, not images.	"Type of sample	Specimen (Sample slices)	Image	Training	Validation     Testing
Normal	                              288	                        10,368	    184	    46	        58
Cancer	                              284	                        10,224	     181	    46	        57
Total	                              572	                         20,592	     365	    92	       115
"	4.0	0.0	We provided in manuscript.	We trained and compared five models. 	We applied for two statistical test to validate the results of five models.	Accuracy, Precision, Recall and F1score	The models were evaluated on testing set (unseen data).	5.0	0.0	https://github.com/haile493/Colorectal-cancer-detection-using-ColoPola-dataset	2s	Black box models	Binary classification	4.0	0.0	Binary class: Colorectal cancer and Healthy	https://github.com/haile493/Colorectal-cancer-detection-using-ColoPola-dataset	Crop the image from 1280x1024 to 900x900 pixels, then resize to 224x224 pixels.	The input data dimensions were defined as 900√ó900√ó36, representing the width, height, and red channel value of 36 polarimetric images. Subsequently, the input images were resized to 224√ó224√ó36 for processing.	Due to p‚â´(Npos+Nneg), we developed two small models and fine-tuned three pretrained ones, using techniques like early stopping to prevent overfitting.	"No.
The results of models are evaluated on the testing set, independently from the training set and validation set."		Early stopping was utilized	7.0	1.0	6836c0b8e5963d3a24940cba	41100285.0	PMC12530094	02/02/2026 19:46:52	Pham TT, Vo QH, Nguyen TV, Nguyen TH, Phan QH, Le TH.	GigaScience	ColoPola: A polarimetric imaging dataset for colorectal cancer detection.	10.1093/gigascience/giaf120	2025	0.0	0.0	Colorectal cancer, polarimetric image, Deep learning	1.0	2025-05-28T07:52:24.172Z	2026-02-02T19:46:52.000Z	fb606ec7-8535-442a-8082-cfe2396831fd	undefined	futlrtl5w4				DOME_JSON	Match	Match
6854f0d71d1a98db540a923a	"Yes, all the datasets are publicly available. 

Simulation 1 ‚Äì https://rsinghlab.github.io/SCOT/data/
Simulation 2 ‚Äì https://rsinghlab.github.io/SCOT/data/
Simulation 3 ‚Äì https://rsinghlab.github.io/SCOT/data/
Simulation 4 ‚Äì https://rsinghlab.github.io/SCOT/data/
SNAREseq ‚Äì https://rsinghlab.github.io/SCOT/data/
scGEM ‚Äì https://rsinghlab.github.io/SCOT/data/
PBMC 1 ‚Äì https://figshare.com/s/ff75c0bfecd89468dc8d?file=44698216
PBMC 2 ‚Äì https://drive.google.com/drive/folders/1AH1h2zDnuowdV3gSm2RjDgmPfHgmsk7I
"	"We evaluated our method on four widely used, gold-standard multi-omics benchmarks:

scGEM ‚Äì Jointly profiles gene expression and DNA methylation, covering 177 cells √ó 34 gene-expression features and 177 cells √ó 27 methylation features.

SNARE-seq ‚Äì Links chromatin accessibility to gene expression, producing an ATAC matrix of 1,047 cells √ó 19 peaks and an RNA matrix of 1,047 cells √ó 10 genes.

PBMC 10X (Dataset 1) ‚Äì Human peripheral blood mononuclear cells, comprising 9,378 cells with 130,417 scATAC peaks and 15,417 scRNA genes.

PBMC 10X (Dataset 2) ‚Äì A complementary PBMC set with 11,259 cells, 28,307 scATAC peaks, and 11,942 scRNA genes.

We also evaluated our method on four Simulated benchmarks:

Simulation 1 ‚Äì Branch manifold
2-D branching structure embedded into 1,000-Dimension with 300 points.

Simulation 2 ‚Äì Swiss-roll manifold
3-D Swiss roll embedded into 2,000-Dimension with 300 points.

Simulation 3 ‚Äì Circular frustum manifold
3-D frustum embedded into 1,000-Dimension with 300 points.

Simulation 4 ‚Äì Splatter RNA-seq
Synthetic scRNA-seq counts (5,000 cells; 50 or 500 genes).
"	"Consistent with prior literature, we evaluate the entire dataset without creating separate train/test splits. GROTIA is assessed in two modes:

Semi-supervised ‚Äì hyper-parameters are tuned with the aid of cell-type labels.

Fully unsupervised ‚Äì hyper-parameters are tuned solely by minimizing the model‚Äôs loss function, with no label information used."	"Consistent with prior literature, we evaluate the entire dataset without creating separate train/test splits. GROTIA is assessed in two modes:

Semi-supervised ‚Äì hyper-parameters are tuned with the aid of cell-type labels.

Fully unsupervised ‚Äì hyper-parameters are tuned solely by minimizing the model‚Äôs loss function, with no label information used."	4.0	0.0	Yes, the result is replicable.	The method is compared to six advanced methods.	No, they don't.	We report FOSCTTM and label-transfer accuracy (LTA) because together they capture the two complementary aspects of multimodal alignment emphasized in recent literature. FOSCTTM is a pairwise metric: it asks whether each individual cell‚Äôs true counterpart in the other modality is indeed its nearest neighbour in the joint space, providing a fine-grained measure of geometric alignment that is sensitive to global distortions. LTA, in contrast, is a group-level metric: it checks how well discrete biological labels propagate across modalities, reflecting the preservation of local neighbourhood structure that underlies downstream analyses such as clustering or trajectory inference. hese two metrics are standard in state-of-the-art studies on single-cell alignment (e.g. SCOT, UNIPORT) and are widely regarded as jointly representative of alignment quality.	The method was assessed in two distinct evaluation modes, each paired with two quantitative metrics across eight datasets.	5.0	0.0	Yes, it is publicly available (https://github.com/PennShenLab/GROTIA).	It is 15 minutes for a 10k points dataset.	Yes, it is interpretable. We compute partial derivatives of the RBF kernel embeddings with respect to each gene‚Äôs expression and then weight these by the projection matrix to obtain a contribution score for every gene‚Äìdimension pair. Ranking these scores identifies dimension-specific ‚Äúsignature genes‚Äù‚Äîthose whose expression changes most strongly reposition cells in the low-dimensional space. 	Neither.	4.0	0.0	Yes, GROTIA is a novel algorithm. It performs global alignment with optimal transport while preserving local structure through graph regularization, delivering superior accuracy with high computational efficiency.	Yes, it is in the github under MIT license	All datasets were pre-processed following the protocols detailed in their original publications.	The PCA (dimension 50) result of the pre-processed data has been used as input. No feature selection is performed.	No, it is not.	No, it doesn't.	"Our model has just three hyper-parameters:

Latent dimension d ‚Äì we use 5 or 8 and find GROTIA is robust to either choice.

Nearest neighbours k ‚Äì fixed at 5 to build the graph Laplacian that preserves local structure.

Loss weights p and q ‚Äì

p balances the two modalities (searched over {1, 0.1, 0.01, 0.001}).

q strengthens local-geometry preservation (searched over {1 √ó 10‚Åª¬≥, 10‚Åª‚Å¥, 10‚Åª‚Åµ, 10‚Åª‚Å∂}).

We keep p > q so that local structure is maintained while still achieving good cross-modal alignment."	Training employs early stopping: if the loss fails to improve for 50 consecutive epochs, optimization terminates.	8.0	0.0	6854f0d7e5963d3a24c87f7e			02/02/2026 19:46:52	Zexuan Wang, Qipeng Zhan, Shu Yang, Zhuoping Zhou, Mengyuan Kan,Tianhuan Zhai and Li Shen	GigaScience	An Interpretable Graph-Regularized Optimal Transport Framework for Diagonal Single-Cell Integrative Analysis	https://doi.org/10.1101/2024.10.30.621072	2025	0.0	0.0	single cell	1.0	2025-06-20T05:25:43.289Z	2026-02-02T19:46:52.000Z	f5bbd7f1-ddf7-461a-874c-c30711b9b177	undefined	79ooxhtwjr				DOME_JSON	Match	Match
68701eb21d1a98db540a9ca8	Yes. The metadate of each dataset are uploaded to GigaScience private User DropBox area, which will be release after the publication of the paper. And the raw data can be downloaded from publicly accessible cryo-electron microscopy databases EMDB with the metadate.	"The dataset used by CryoDataBot was compiled from publicly accessible cryo-electron microscopy databases, including the Electron Microscopy Data Bank (EMDB) and Protein Data Bank (PDB), supplemented by atomic models from peer-reviewed publications.

Total for the point in each class:
        Label-0: 13650927191
        Label-1: 264684349
        Label-2: 105272253
        Label-3: 260789461
        Label-4: 537589210
        Total: 14819262464"		"We use 42,060*64*64*64 data points for training and a separate 10,516*64*64*64 data points for validation.

A total of 20 cryoEM maps were used as the independent test set. Each map was subdivided into 3D subvolumes of size 64√ó64√ó64, enabling voxel-level evaluation."	3.0	1.0	Evaluation files are not currently available, but they will be released upon publication under an open-source license (MIT or Apache 2.0) via GitHub.	The U-Net model is for testing the dataset.	No.	F1 score, precision and recall. Yes.	Independent dataset	4.0	1.0	Yes and yes. The source code of CryoDataBot is available at https://github.com/t00shadow/CryoDataBot under the MIT license.	"The time varies among different input size of cryoEM map.

Using NVIDIA RTX 4090, the whole process for an input of a large size cryoEM map with 440A*440A*440A is ~50s.
The prediction part will cost 15s and the rest time is spent on data normalization, output data generation, and so on."	The 19-layer 3D U-Net model used in CryoDataBot is partially interpretable. While deep learning models like U-Net are inherently complex, the outputs and intermediate activations offer useful insights:  Segmentation Masks: The model generates 3D segmentation maps of biomolecular structures which can be directly visualized and overlaid on cryoEM density maps, making predictions interpretable for structural biologists.  Voxel-level Confidence Scores: Output channels provide probability values for each voxel, allowing users to assess prediction certainty.  Feature Map Analysis: Intermediate convolutional layers can be visualized to understand which regions and patterns the model focuses on.  Comparative Validation: Predicted structures can be compared with ground truth atomic models using RMSD and FSC scores, supporting model interpretability through quantitative metrics.  These examples demonstrate that while the model‚Äôs internal logic may be non-transparent, its behavior and predictions can be meaningfully interpreted and evaluated in practical applications.	Classification	4.0	0.0	"In the testing of CryoDataBot-generated dataset, we utilizes supervised deep learning algorithms, primarily based on convolutional neural networks (CNNs). Specifically, it integrates U-Net architectures.

The U-Net is not new but tailored for 3D cryoEM map segmentation and feature recognition."	Yes. The hyperparameter configurations, optimization schedule are in method part of the submitted paper. The model files have been uploaded to GigaScience private User DropBox area, which will be release after the publication of the paper.	Map Normalization: Cryo-EM density maps were normalized to a consistent voxel intensity range to eliminate dataset-dependent variance and facilitate model convergence.  Resampling & Voxel Size Standardization: All maps were resampled to a fixed voxel size (e.g., 1 √Ö/voxel) to ensure spatial consistency across inputs.  Segmentation & Label Encoding: Atomic models were converted into labeled volumetric masks. Each atom type or residue class was encoded using categorical encoding schemes suitable for the supervised learning tasks.  Input Format Transformation: CryoEM maps were formatted as 3D tensors (e.g., N √ó 64 √ó 64 √ó 64) compatible with deep learning frameworks such as PyTorch.	One. No.	We have used two groups of training set (control and experimental), including multi voxel files of size 64*64*64 (~260,000). Experimental training dataset contains ~50,000 voxel files, including 1e10 training points, which is much larger than the number of ps. We verified that the model did not underfit by ensuring convergence in training and strong performance on held-out validation data.	No.	We used a 19-layer 3D U-Net models, including a total number of params as 90,298,821.	Batch normalization and early stopping using a validation set	7.0	1.0	68701eb2e5963d3a24c31fc6	41124016.0	PMC12596181	02/02/2026 19:46:52	Xu Q, Wu L, Rebelo M, Feng S, Yu X, Farheen F, Kihara D, Zhou ZH.	GigaScience	CryoDataBot: a pipeline to curate cryoEM datasets for AI-driven structural biology.	10.1093/gigascience/giaf127	2025	0.0	0.0	Structural Biology, Dataset Generation for AI, Automated Pipeline	1.0	2025-07-10T20:12:34.195Z	2026-02-02T19:46:52.000Z	694f1516-5631-4198-a4df-b02afc4c5709	undefined	zg8nv733ej				DOME_JSON	Match	Match
6937d8dedef4ff8a77671d2f	"The datasets used in this study can be accessed in the following links:
MGH Olink proteomics: https://info.olink.com/mgh-covid-study-overview-page?hsCtaTracking=fff99a2a-81c1-4e4a-a70d-6922d26503b4
Mayo Olink Proteomics: https://www.thelancet.com/journals/landig/article/PIIS2589-7500(22)00112-1/fulltext#supplementaryMaterial
Stanford Olink proteomics: https://datadryad.org/stash/dataset/doi:10.5061/dryad.9cnp5hqmn
Single-cell MGH Villani group and Blish: https://www.covid19cellatlas.org/index.patient.html"	"1st scenarios - Proteomics: Three Olink proteomics count matrices and their corresponding clinical metadata:
MGH 305 samples [225 no-severe COVID-19, 80 severe COVID-19] -- Training dataset
Mayo 449 samples [181 no-severe COVID-19, 268 severe COVID-19] -- Validation dataset
Stanford 64 samples [40 no-severe COVID-19, 24 severe COVID-19] -- Independed testing dataset

2nd scenarios - scRNAseq
MGH 298 samples [216 no-severe COVID-19, 76 severe COVID-19] -- Training dataset [exclude those samples that exist in scRNAseq dataset] -- Training dataset
scMGH 6665 samples (single-cell samples) [5144 no-severe COVID-19, 1521 severe COVID-19] -- Validation dataset
Blish 6500 samples (single-cell samples) [2609 no-severe COVID-19, 3891 severe COVID-19] -- Independed testing dataset"	The training-validation-testing datasets were independed	"MGH -- Training
Mayo -- Validation
Stanford -- Testing
scMGH -- Validation
Blish -- Testing"	4.0	0.0	-	-	-	"AUC and F1 score

1st model - Proteomics:
AUC = 0.96, F1score =  0.89

2nd model - scRNAseq:
AUC = 0.99, F1score = 0.975"	Cross-validation and an independent dataset	5.0	0.0	github link: https://github.com/BiodataAnalysisGroup/APNet	"Proteomics:
Estimated time for Griding ~1-2 days
Estimated time for Training ~ 3-4 hours

scRNAseq
Estimated time for Griding ~ 3-4 days
Estimated time for Training ~ 5-6 hours
"	Interpretable ‚Äì in the article, the authors extensively explore the interpretation of the learned [model/representations/features]	Classification	4.0	0.0	PASNet (Pathway-Associated Sparse Network) that utilize biological priors from pathway databases, enhancing both predictive performance and pathway-level interpretability	The model configuration is available at the repository: https://github.com/BiodataAnalysisGroup/APNet	PASNet is applied to activity data, where each feature represents the activity level of a gene and is typically encoded as a continuous numerical value. To incorporate biological knowledge, genes are mapped to relevant pathways using established databases. This pathway information is then used in PASNet to impose sparse connections in the network, allowing the model to capture pathway-specific patterns in the data.	"1st model - Proteomics:
To reduce noise and focus on the most relevant features, we retained the common statistically significant drivers present in both three proteomic datasets, as these shared drivers are expected to participate in the pathways identified in the enrichment analysis. In total, 250 features were used for model training.

2nd model - scRNAseq:
To reduce noise and focus on the most relevant features, we retained the common statistically significant drivers present in both three datasets [MGH proteomics, scMGH - Blish scRNAseq], as these shared drivers are expected to participate in the pathways identified in the enrichment analysis. In total, 223 features were used for model training."	-	The algorithm used the activity matrices that extracted from the NetBID2 (part of the workflow), alongside with a table indicating whether the drivers participated in such of pathways from pathway enrichment with top 30 pathways from Gene Ontology (GO), Reactome, KEGG, and Wikipathway.	"Learning Rate (LR) and L2 regularization (L2), through griding search

1st model - Proteomics
Optimal learning rate (LR = 0.05) and L2 regularization (L2 = 0.0003)

2nd model - scRNAseq
Optimal learning rate (LR = 0.007) and L2 regularization (L2 = 0.0003)"	-	8.0	0.0	692d3e71e5963d3a24fdf895	39921901.0	PMC11897427	02/02/2026 19:46:52	Gavriilidis GI, Vasileiou V, Dimitsaki S, Karakatsoulis G, Giannakakis A, Pavlopoulos GA, Psomopoulos F.	Bioinformatics (Oxford, England)	APNet, an explainable sparse deep learning model to discover differentially active drivers of severe COVID-19.	10.1093/bioinformatics/btaf063	2025	0.0	0.0	COVID-19, Deep Learning, APNet, Neural Networks, proteomics	1.0	2025-12-09T08:07:58.973Z	2026-02-02T19:46:52.000Z	c53401a2-834f-468c-95f6-d843cc73bb50	undefined	c06c4e2tsl				DOME_JSON	Match	Match
6953e753def4ff8a776723ab	Pre-processed data has been made available on the OSF pre-registration site: https://osf.io/gztmd/	"Data were collected as part of standard clinical care by team members of the CHOP site of the US CP Early Detection and Intervention Network in a REDcap database between May 2019 and December 2023. This included the secure uploading of iPad- or iPhone-recorded videos, General Movement Assessments (GMA) scores and demographic information.  Exclusions were applied to intubated patients, those under the influence of sedation medications, within a week post-operative, on ECMO support, or diagnosed with myelomeningocele. For all infants who were between 10-20 weeks post-term age (corrected for preterm birth, if applicable) at the time of a clinic visit, and whose parents or legal guardians agreed to video recording for clinical care, clinicians captured a 1-2 minute video of the infant lying in supine.  

Data classes were defined according to the GMA scores: 
1 - Fidgety (Nneg = 826) 
2 - Absent Fidgety (Npos = 99) 
3 - Atypical Fidgety (excluded = 6) 

The full dataset comprised 1053 infants. 122 were then excluded for meeting one or more exclusion criteria listed above. Six infants with a GMA score of 3 (‚Äúatypical fidgety‚Äù) were also excluded, since there were not enough infants in this group for model training/testing, and movement patterns differ from those of ‚Äúabsent fidgety‚Äù infants. 

"	"The true holdout set (n = 186) was randomly sampled from the dataset, stratified by GMA score, prior to any algorithm development, and pre-registered on OSF. 

"	The full dataset comprised 1053 infants. 122 were then excluded for meeting one or more exclusion criteria listed above. Six infants with a GMA score of 3 (‚Äúatypical fidgety‚Äù) were also excluded, since there were not enough infants in this group for model training/testing, and movement patterns differ from those of ‚Äúabsent fidgety‚Äù infants. The remaining 931 videos were split into an analysis set (739) and a lock box holdout set (186) (Figure 1, Figure 2). The analysis set was further split into train/val/test sets (648, 93, 186), each of which had a 10-12% representation of the ‚Äúabsent fidgety‚Äù movement type. The splits were stratified to preserve the ratios of boy/girl infants, as well as age, and race/ethnicity. There was a total recording duration of 60-120 s per infant.	4.0	0.0	All model outputs and CV scores are available. 	AutoML already tests simple baselines to ensure they do not out-perform the selected model. Additionally, the model was tested against another available model for predicting risk from infant video (STAM).	Performance metrics are reported with associated cross-validation scores. The goal of the paper was not the claim that the result is superior to that of others, but rather to show that the result persists even when applying strict rigorous ML approaches. 	Two standard measures are report: AUC-ROC which is a standard measure used in the machine learning community, and Preicision-Recall (PR)-AUC, which is more interpretable within the medical literature as it relates to the sensitivity/specificity measures that are widely used. 	Two evaluation strategies were used. First, 10 times 5-folds cross validation were performed on the train/test data to ensure that model performance was not reliant on any one specific data split. Finally, the model was evaluated on a lock-box, pre-registered held out test set of participant data to show generalizability. 	5.0	0.0	Yes, the source code is available on GitHub (https://github.com/KordingLab/predict-scores-from-video) under an MIT License. A container has not yet been released. 	prediction requires approximately 30 seconds on a standard machine. 	The model is interpretable as it is possible to inspect the weights assigned to each of the 38 movement features included in the model. Additional, permutation testing was carried out to allow for intuitive visualization of feature importance. 	The model is a binary classifier. 	4.0	0.0	"
AutoSklearn-2.0 employs a meta-learning approach to automate the selection and tuning of well-established machine learning algorithms like decision trees, random forests, and support vector machines. The final model selected was Stochastic Gradient Descent. "	All hyper parameters are available in the stored model details, which are available on the pre-registration site: 	All data used for training was numerical, and pre-processed to remove missing values. All other parameters of the data pre-processing and encoding were determined by AutoSklearn 	38 features were used as input. No feature selection was performed - features were determined by medical specialists and pre-registered in advance of any data collection or analysis. 	The number of training datapoints (> 600) is much larger than the number of features (38). Additionally, AutoML mitigates the risk of overfitting by abstracting away any feature and model parameter selection. Moreover, the trained model was evaluated on a lock-box holdout set of data after pre-registration to determine generalizability. 	The model does not use data from other ML algorithms as input. The inputs to the classifier are the features derived from the pose-estimate time-series data, which rely on ViTPose-H. 	p was selected by the AutoML optimizer based on the classification algorithm selected. The search space was restricted to choosing the best model as opposed to an ensemble, in this case SGD, so p corresponds to the weights given to each of the 38 features. 	AutoML was the primary approach used to prevent the risk of overfitting. 	8.0	0.0	6786955bdd85bd59b3220ba6	40666343.0	PMC12262795	02/02/2026 19:46:52	Melanie Segado, Laura A. Prosser, Andrea F. Duncan, Michelle J. Johnson, Konrad P. Kording	GigaScience	A Pre-Registered, Open Pipeline for Early Cerebral Palsy Risk Assessment from Infant Videos	https://doi.org/10.1101/2024.11.06.24316844	2025	0.0	0.0		1.0	2025-12-30T14:53:07.781Z	2026-02-02T19:46:52.000Z	92032042-129d-4213-925c-1d7ea27b360a	undefined	qtvakphr68				DOME_JSON	Match	Match
67c5b54a1f0965481b0c756c	All the data is available in the Github repository: https://github.com/YihePang/DisoFLAG/tree/main . They are also available on Zenodo: https://zenodo.org/records/10361856. The statistics about data are provided in the supplementary materials.	DisProt 9.3 is used to extract proteins with functional annotations. 1 long sequence DP00072 (>30000) is removed. 925 sequences were obtained. Then sequence redundancy using CD-HIT was performed with 25% sequence similarity. The clusters were randomly divided into 5 sets, 3 of which were used in training (589 sequences), 1 used for validation (148 sequences) and 1 used for testing (188 sequences). An additional test set was obtained from the newly updated proteins in DisProt version 9.4 (98 sequences).	Dataset was redundancy reduced with 25% sequence similarity using CD-HIT and clusters were randomly divided in 5 sets. 	DisProt 9.3 is used to extract proteins with functional annotations. 1 long sequence DP00072 (>30000) is removed. 925 sequences were obtained. Then sequence redundancy was performed with 25% sequence similarity suing CD-HIT. The clusters were randomly divided into 5 sets, 3 of which were used in training (589 sequences), 1 used for validation (148 sequences) and 1 used for testing (188 sequences). An additional test set was obtained from the newly updated proteins in DisProt version 9.4 (98 sequences).	4.0	0.0	They are available in the supplementary materials, and the github repository.	Comparison was done with fIDPnn, DeepDISOBind, DisoRDPbind, ANCHOR-2, MoRFchibi-Light, SPOT-MoRF, and MoRFchibi-Web . 	It is provided in the supplementary materials. 	AUROC, AUPR, F-max, MCC, and BACC are reported, which align with established criteria of CAID. 	Validation was performed on independent set. 	5.0	0.0	software is available at : https://github.com/YihePang/DisoFLAG. webserver is available at: http://bliulab.net/DisoFLAG/	Not reported. 	Since the model uses protein language models, GRU layers, and GCN layer, it could be treated as a black box. 	The DisoFLAG outputs the real-valued propensity score results for disorder and disordered functions.	4.0	0.0	The model contains a Bi-GRU layer, Attention mechanism, GRU layer, Graph-based interaction units and Graph Convolutional Network is used. The model is trained in a supervised manner. 	Everything is reported in the supplementary materials.	The residue embeddings for each protein are used as provided by ProtT5. The protein dataset has been redundancy reduced with 25% sequence similarity.  	The embeddings provided by ProtT5 protein language model (pLM) are used as input, which are of dimension 1024 for each residue.  The performance of DisoFLAG is also evaluated using different protein representations including ProtBERT (pLM), position-specific scoring matrix (PSSM) and amino acid one-hot encoding. 		The model uses the residue representations (embeddings) provided by the ProtT5 protein language model (pLM). 	The number of parameters  for each model component re reported in the supplementary materials. 	Dropout is used. 	7.0	1.0	66c495857089c469b477b4ce	38166858.0	PMC10762911	02/02/2026 19:46:52	Pang Y, Liu B.	BMC biology	DisoFLAG: accurate prediction of protein intrinsic disorder and its functions using graph-based interaction protein language model.	10.1186/s12915-023-01803-y	2024	0.0	0.0	Critical Assessment of Protein Intrinsic Disorder, CAID	1.0	2025-03-03T13:57:30.407Z	2026-02-02T19:46:52.000Z	858d7c50-a50b-46b5-829e-53b10d340f84	undefined	ip5bv2erho				DOME_JSON	Match	Match
67f6a78d822ee03b50b6c47c	The datasets, including annotations of the DNA-, RNA- and protein-interacting IDRs, are freely available at https://www.csuligroup.com/DeepDISOBind/.	DisProt Database is used. 	data is redundancy reduced at 30% sequence similarity with CD-HIT. 	train = 238, validation=118, test=394	4.0	0.0	Details are provided in the paper and Supplementary materials. 	The method is compared toDisoRDPbind, MTDsites, ProNA2020, ANCHOR2, MoRFChibiLight, SCRIBER, BindN+, NCBRPred, TargetDNA, RNABindRPlus	Statistical significance tests are performed and reported in the paper. 	F1 score, sensitivity, and AUC are reported which are common metrics used in CAID. 	The method was tested on independent dataset. 	5.0	0.0	 DeepDISOBind is available at https://www.csuligroup.com/DeepDISOBind/	The execution time is available in the CAID2 and CAID3 results.	The method uses CNN and FNN. 	DeepDISOBind and other related tools produce putative propensities for the disordered DNA-, RNA- and protein-binding interactions for each residue in the input protein sequences. These real-valued propensities are accompanied by binary predictions, i.e. residues are classified as either DNA-/RNA-/protein-interacting or non-DNA-/RNA-/protein-interacting.	4.0	0.0	Convolutional Neurual Networks and Feed Forward Neurual Networks are used. 	The details are availanble in the supplementary materials available at http://bib.oxfordjournals.org/	the profile includes 33 dimensions: 20 for one-hot encoding of sequence +3 sequence embedding values +5 RAAP values +3 secondary structure predictions +2 disorder predictions. They use sliding windows to predict the interaction propensity for the residues in the middle of the windows. We pad the windows at the sequence termini with zeros.	the profile includes 33 dimensions: 20 for one-hot encoding of sequence +3 sequence embedding values +5 RAAP values +3 secondary structure predictions +2 disorder predictions		Predictions from SPOT-disorder-single are used.  	The hyperparameters were tuned on the validation set. 	They use early stopping, and dropout. more details are provided in the paper.	7.0	1.0	66c495857089c469b477b4ce	34905768.0		02/02/2026 19:46:52	Zhang F, Zhao B, Shi W, Li M, Kurgan L.	Briefings in bioinformatics	DeepDISOBind: accurate prediction of RNA-, DNA- and protein-binding intrinsically disordered residues with deep multi-task learning.	10.1093/bib/bbab521	2021	0.0	0.0	Critical Assessment of Protein Intrinsic Disorder, CAID	1.0	2025-04-09T16:59:57.353Z	2026-02-02T19:46:52.000Z	73b73978-8b38-4c08-9976-a83788639e70	undefined	apf7wp9txp				DOME_JSON	Match	Match
691f33aadef4ff8a77670d89	The MALDI-TOF spectra used in this project can be obtained by downloading the DRIAMS dataset from https://doi.org/10.5061/dryad.bzkh1899q. The accompanying metadata includes information on the hospital and year of data collection, necessary to reproduce the splits.	"The publicly available DRIAMS dataset [1] consists of MALDI-TOF mass spectrometry data collected from patients in the Swiss healthcare system. This dataset includes data from four distinct diagnostic laboratories, collected from 2015 to 2018: University Hospital Basel (designated as DRIAMS-A), Cantonal Hospital Basel-Land (DRIAMS-B), Cantonal Hospital Aarau (DRIAMS-C), and the Viollier laboratory service provider (DRIAMS-D). The MALDI-TOF mass spectra used in this project were originally published as part of the study by Weis et al. [2]. This dataset is widely recognized within the microbiology community and has been utilized in several subsequent studies.

[1] Weis, C., et al. ""DRIAMS: database of resistance information on antimicrobials and MALDI-TOF mass spectra."" Dryad, dataset (2021).
[2] Weis et al., ""Direct antimicrobial resistance prediction from clinical MALDI-TOF mass spectra using machine learning"" (Nature Medicine, 28(1), 2022, pp. 164‚Äì174)"	To prevent data leakage, each data split was designed so that no pathogen appeared in both the training and test sets. In each split, the test set was drawn under a different condition: a different hospital in the hospital zero-shot setting, and a different year in the year zero-shot setting.	"In total, 631,167 observations were leveraged: 11,610 in A2015; 138,132 in A2016; 179,334 in A2017; 123,558 in A2018; 32,377 in B2018, 47,586 in C2018; and 98,570 in D2018. 
Two main data splits were created: 
1. Hospital zero-shot split: Select data from the DRIAMS dataset collected in 2018. Let i, j ‚àà {A, B, C, D} represent the hospitals in the dataset. Train the model on data from hospital i. Test the model on data from hospital j, where j!= i.
2. Year zero-shot split: Select data from the DRIAMS A. Let i, j ‚àà {2015, 2016, 2017, 2018} represent the year of data collection. Train the model on data from year i. Test the model on data from year j, where j!= i.
"	4.0	0.0	The raw evaluation files are not available.	"We use as baseline, the best model of Visona et al.  [1], which utilizes a 6000-dimensional binned MALDI-TOF mass spectra vector representation for pathogens and Morgan fingerprints for antimicrobial representations.

[1] Vison√†, Giovanni, et al. ""Multimodal learning in clinical proteomics: enhancing antimicrobial resistance prediction models with chemical information."" Bioinformatics 39.12 (2023): btad717.
"	We provide error bars representing standard deviation across training sets.	We report three standard classification metrics for imbalanced data: area under the precision-recall curve (AUPRC), balanced accuracy, and Matthews Correlation Coefficient (MCC).	The method was evaluated under two zero-shot settings: hospital zero-shot and year zero-shot, using four data splits for each scenario.	5.0	0.0	The source code used to train the models and generate the results presented in this study is publicly available at https://github.com/DianeDuroux/ZeroShotAMR. The project is released under the GNU General Public License v3.0 (GPL-3.0).	Inference for a single spectrum-antimicrobial combination takes less than 1 second to run on a desktop PC.	The model is black box.	The model is a classifier.	4.0	0.0	"In Visona et al. [1], three primary models are explored: (1) encodings derived by applying principal component analysis to mass spectra and chemical fingerprints, which are concatenated and used as input for a logistic regression model; (2) joint representations generated by Siamese networks, subsequently utilized as input for logistic regression to predict resistance; and (3) a classification Multi-Layer Perceptron (MLP) with Residual Skip-Connections. Building on the findings of Visona et al. [1], which identified the MLP with Residual Skip-Connections [2] as achieving the highest performance, we trained this network to predict the probability of resistance for antimicrobial-spectrum pairs.

[1] Vison√†, Giovanni, et al. ""Multimodal learning in clinical proteomics: enhancing antimicrobial resistance prediction models with chemical information."" Bioinformatics 39.12 (2023): btad717.
[2] Szegedy, Christian, et al. ""Inception-v4, inception-resnet and the impact of residual connections on learning."" Proceedings of the AAAI conference on artificial intelligence. Vol. 31. No. 1. 2017."	The code used to train the models (including the hyperparameter configurations) and generate the results presented in this study is publicly available at https://github.com/DianeDuroux/ZeroShotAMR. Pretrained model weights can be accessed via the GigaDB repository. The project is released under the GNU General Public License v3.0 (GPL-3.0).	"Data bias exists within DRIAMS-A due to one particular workstation (HospitalHygene), where samples were captured using a different medium and skewed heavily toward resistant samples. Following Weis et al. [1], we excluded data from this workstation to mitigate potential biases. The spectra were binned using a bin size of 3 m/z units. The encodings defined in the section above were used. [1] Weis et al., ""Direct antimicrobial resistance prediction from clinical MALDI-TOF mass spectra using machine learning"" (Nature Medicine, 28(1), 2022, pp. 164‚Äì174)"	Different encoding strategies are evaluated (as described above), each resulting in a different number of input features. None of the encoding techniques leverage the resistance/susceptibility outcome during feature construction. The total number of features varies from 1,280 (MAE embeddings of dimension 512 combined with Molformer embeddings of dimension 768) to 30,160 (6000-dimensional spectra combined with 24,160-dimensional SELFIES encodings).	The number of parameters is larger than the number of training points. Overfitting was addressed through early stopping, dropout, weight decay, and validation monitoring (see below)	"We compare two pathogen representations: binned MALDI-TOF mass spectra and encodings generated by masked autoencoders (MAEs). 

We compare three types of antimicrobial representations: traditional molecular fingerprints, encodings generated by Molformer transformer-based model for molecular representation learning [1], and SELF-referencing embedded strings (Selfies) [2].

[1] Ross, Jerret, et al. ""Large-scale chemical language representations capture molecular structure and properties."" Nature Machine Intelligence 4.12 (2022): 1256-1264.
[2] Krenn, Mario, et al. ""Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation."" Machine Learning: Science and Technology 1.4 (2020): 045024."	The input projections encodes the MALDI TOF spectra and the antimicrobials encodings into 512-dimensional vectors, which are then concatenated. The model consists of 5 residual blocks, including ReLU activation, a linear layer of dimensionality 1024, a dropout layer with probability 0.2, and a BatchNorm layer. The ResMLP is trained with early stopping with a patience parameter of 10 epochs using an Adam optimiser with a learning rate of 3‚àó10‚àí4 and a weight decay of 10‚àí5. With the 6000-dimensional binned MALDI-TOF mass spectra and Morgan molecular fingerprints, the ResMLP model has 218M trainable parameters.	Early stopping was employed to halt training if the validation loss did not improve in consecutive epochs, preventing the model from over-optimizing on the training set. Additional regularization was introduced via dropout within residual blocks (with a probability of 0.2) and weight decay (L2 regularization with a factor of 1e-5) to discourage overly complex models. The best model was selected based on validation performance, ensuring generalizability.	8.0	0.0	68f216dbe5963d3a2463ff6a	41553357.0		02/02/2026 19:46:52	Duroux D, Meyer PP, Vison√° G, Beerenwinkel N.	GigaScience	Generalizable machine learning models for rapid antimicrobial resistance prediction in unseen healthcare settings.	10.1093/gigascience/giaf156	2025	0.0	0.0	AMR, Generalizability, Zero shot	1.0	2025-11-20T15:28:42.879Z	2026-02-02T19:46:52.000Z	678d736a-e3e6-4f2f-ae45-f5b5ce7b6c69	undefined	nfny9oz269				DOME_JSON	Match	Match
67c33bfa1f0965481b0c738e	the data are available at https://aiupred.elte.hu/statistics 	"The transformer neural network was trained to predict positional energies calculated using the IUPred energy function for 17‚Äâ282 non-redundant structures derived from the PISCES database (version: PISCES_cullpdb_pc40.0_res0.0‚Äì2.2_len40-10000_R0.25_Xray_d2022_05_22_chains18454). From this dataset, sequences which could not be mapped to Uniprot sequences, with length below 30, and those with transmembrane regions (based on the PDBTM database) were filtered. The dataset consists of 4‚Äâ276‚Äâ509 positions, which were split into two parts for training and testing, containing 80% and 20% of the positions, respectively. 
For translating the estimated energies into disorder tendencies, a Neural Network is used. For training and testing, the DisProt database (version 2023-06) was used as the positive dataset (divided in a 80‚Äì20 ratio for training and testing), after filtered for redundancy using the CD-Hit algorithm. As a negative dataset, a set of globular domains comprises sequence regions which encode single structural domains with determined monomeric structures in the PDB (4549 protein regions) and splitted into two parties in 80‚Äì20 ratio, as as used in IUPred2A. For validation, the CAID1-PDB and CAID2-PDB datasets were used (each entry that is already part of DisProt was removed manually)."	The datasets were splitted into training and test set, and for validations 2 other datasets were used: CAID2-PDB and CAID1-PDB. datasets were reduced using CD-HIT.	for all the datasets used, they were split into 80-20 ration for training and testing. 	4.0	0.0	They are available at https://aiupred.elte.hu/statistics.	Comparisons are available in the paper and at https://aiupred.elte.hu/statistics.	Yes	AUROC, Runtime on Human proteome	The method was evaluated on CAID2 dataset.	5.0	0.0	Yes data and webserver are available at https://aiupred.elte.hu/statistics.	for analysing human proteome, it took1.5 hours on CPU (3 proteins per second) and 3 min on GPU (100 proteins per second on Nvidia 1080).	The model consists of two parts, a transformer network following by a feed forward network. Therefore it is generally considered a black box due to its complex, high-dimensional representations and non-linear transformations. However, interpretability can be improved using techniques like attention weight analysis, gradient-based attribution (e.g., Integrated Gradients), and probing learned embeddings.	Classification	4.0	0.0	Feed forward neural networks and Transformer network.	The information is available in supplementary material.	redundancy reduction and filtering out sequences that couldn't be mapped to UniProt with length below 30 and transmembrane regions.	The input of the model (transformer network) is the amino acid sequence.		No.		Dropout and other techniques in neural network training were used.	5.0	3.0	66c495857089c469b477b4ce	38747347.0	PMC11223784	02/02/2026 19:46:52	Erd≈ës G, Doszt√°nyi Z.	Nucleic acids research	AIUPred: combining energy estimation with deep learning for the enhanced prediction of protein disorder.	10.1093/nar/gkae385	2024	0.0	0.0	Critical Assessment of Protein Intrinsic Disorder, CAID	1.0	2025-03-01T16:55:22.183Z	2026-02-02T19:46:52.000Z	72248b6d-2b5f-4f07-8095-db261c26882a	undefined	goldx6bwjb				DOME_JSON	Match	Match
67d800a4478cfd7e6983b4d6	The Link to all the datasets used are provided in table1 in the paper. 	The training data of three predictors are merged: SETH, IDR-CRF, flDPnn. The data come from DisProt, MobiDB, CheZod, PDB, CAID1-Disorder, CAID2-Disorder	Redundancy reduction was performed using CD-HIT.	"Train = 6892 proteins (12.55%)  , Validation = 100 proteins(32.61%), Test = 1807 proteins (18.09%), Total = 8799 proteins (14.66%)
The percentages indicate proportion of disordered residues. "	4.0	0.0		Comparison with 40 other predictors were reported in the supplementary materials.	Nor reported.	MCC, F1-score, AP,  and AUROC are reported, which align with the metrics in CAID.  	"The model has been tested on CheZOD, MxD set , CAID1 Disorder set and CAID2 Disorder-PDB set .
The urls for each dataset is available in Table1 in the paper."	4.0	1.0	The code is fully reproducible in CodeOcean: doi.org/10.24433/CO.7350682.v1	Not reported.	The model uses U-Net with Convolutional Attention architecture, so it could be considered as a black-box.	The model outputs a probability for each residue being disorder. The probability is converted to binary state by a threshold of 0.5.	4.0	0.0	The model uses U-Net with Convolutional Attention architecture. It is trained in a supervised manner. 	All the details are reported in the paper. The trained models are available in the CodeOcean platform: doi.org/10.24433/CO.7350682.v1	 The model uses the embeddings from ProtT5 protein language model, which provides a vector of 1024 dimension for each residue. Features were standardized. 	ProtT5 protein language model provides vectors of dimension 1024. Features were standardized. 		The model uses the embeddings from ProtT5 protein language model, which is trained in a self-supervised manner. 	628710 parameters are reported for DisorderUnetLM model.	dropout and early stopping were used.  	7.0	1.0	66c495857089c469b477b4ce	39708500.0		02/02/2026 19:46:52	Kotowski K, Roterman I, Stapor K.	Computers in biology and medicine	DisorderUnetLM: Validating ProteinUnet for efficient protein intrinsic disorder prediction.	10.1016/j.compbiomed.2024.109586	2025	0.0	0.0	CAID, Critical Assessment of Protein Intrinsic Disorder, Disorder Prediction	1.0	2025-03-17T10:59:48.498Z	2026-02-02T19:46:52.000Z	86f72203-2bec-4f8e-9551-1c392d37d852	undefined	znnn9k8683				DOME_JSON	Match	Match
67f65b4d822ee03b50b6c446	"Details of data ids and train-test-validation splits are available at ""additional files"": https://doi.org/10.1016/j.ijbiomac.2024.137665"	They obtain 8657 non-redundant, high-resolution protein sequences from the PDB database (August 09, 2019). Proteins were filtered so that their sequence identity is lower than 25 % and they are longer than 51 residues. Of the 8657 non-redundant PDB chains, 5997 (called ‚ÄúIDR chain‚Äù, see Additional file 2) have at least one IDR and 2660 (called ‚ÄúSTR chain‚Äù, see Additional file 3) are fully structured.	Proteins were filtered so that their sequence identity is lower than 25 % and they are longer than 51 residues.	"They randomly divided 5997 IDR chains into 2 datasets: one (called Train5400, see Additional file 4) includes 5400 chains, which will be used to make a training set together with 2660 STR chains, and the other includes 597 chains, which constitute a validation dataset (called Val597, see Additional file 5). Val597 dataset contains 151,203 residues and 950 disordered regions in 597 chains. Of 151,203 residues, 12,014 (7.95 %) residues are positives and 139,189 (92.05 %) residues are negatives. They use another two validation sets: one is CASP10 dataset (see Additional file 6) which was released in IDR category of CASP10 and the other is Val200 set (see Additional file 7), which consists of 200 protein chains with only the IDRs of at least ten residues, extracted from Val597 dataset.
"	4.0	0.0	Details are provided in the paper. 	The method is compared to all the methods of CAID1 and CAID2.	Details are provided in the paper. 	AUC and AUPR are reported in the paper, which aligns with the common metrics in CAID. 	The model was tested on CAID1's Disorder-PDB, CAID2 Disorder-PDB, CAID2-NoX-PDB and CAID2-Xray-PDB	5.0	0.0	PredIDR can be freely used through the CAID Prediction Portal available at https://caid.idpcentral.org/portal or downloaded as a Singularity container from https://biocomputingup.it/shared/caid-predictors/	runtime of PredIDR is available in the CAID2 paper, along with the runtimes of all the predictors in CAID2.  	Convolutional Neural Network is used, so it is not interpretable. 	They used only one output node to predict the state of a residue where the output is 1 if the residue is disordered and 0 if predicted as ordered, so it's a binary classification.	4.0	0.0	Convolutional Neural Network is used. 	Details are provided in the paper. 	They used a sliding window. All features in the sliding window are combined to form a feature tensor. The size of the tensor for each position in the window is 32 consisting of 21 frequency values (one for each amino acid type plus the gap symbol) as provided by the MSA profile, 8-dimensional vector for the 8-state secondary structure (SS8), one value for the predicted solvent accessibility (ACCDEC) and 2 values indicating whether the position is outside the sequence boundaries, i.e. before N-terminus or after C-terminus. In our experiments, they used a window of 15 residues, therefore, the size of the feature tensor for a single residue is 32 by 15.	represent each protein residue with three different features, evolutionary profile, secondary structure and solvent accessibility		They do not use data from other predictors. 		They trained the convolutional neural network twenty times and the ten top-performing networks were chosen based on the performance in Val597 dataset and combined as an ensemble by averaging the predictions from ten top-performing neural networks. The outputs were then averaged. 	6.0	2.0	66c495857089c469b477b4ce	39571839.0		02/02/2026 19:46:52	Han KS, Song SR, Pak MH, Kim CS, Ri CP, Del Conte A, Piovesan D.	International journal of biological macromolecules	PredIDR: Accurate prediction of protein intrinsic disorder regions using deep convolutional neural network.	10.1016/j.ijbiomac.2024.137665	2025	0.0	0.0	Critical Assessment of Protein Intrinsic Disorder, CAID	1.0	2025-04-09T11:34:37.678Z	2026-02-02T19:46:52.000Z	c785d045-cf25-4881-88bc-058bdb127494	undefined	stp0kvhixm				DOME_JSON	Match	Match
67f6a7bc822ee03b50b6c480	"Details of dataset are available in ""additional files""."	They obtained 11,065 non-redundant protein sequences from Protein Data Bank (PDB) and Database of Protein Disorder (DisProt). These proteins have sequence identity <25 % among them and are longer than 50 residues. These include 8657 X-ray crystallography structures from PDB database (August 09, 2019) and 2408 protein sequences from DisProt database (DisProt release 2023_06).	Training proteins have sequence identity <25 % among them.	The validation set is the CAID1's Disorder-PDB dataset, except that 3 sequences DP01985, DP02003 and DP02027 were removed, along with 18 other proteins that were not in DisProt anymore. This results in 625 sequences. 	4.0	0.0	Details are provided in the paper and additional files.  	Comparison with CAID2 methods is available. 	Not reported.	AUC, AUPRC, MCC, DC_mae are reported, which are common metrics established in CAID. 	The test set is CAID2's Disorder-PDB. The dataset contains 286,999 residues and 493 disordered regions in 348 chains. Of 286,999 residues, 37,072 (12.92 %) are positives, 93,805 (32.68 %) are negatives and 156,122 (54.40 %) are excluded. They also used two additional test sets created in our previous study PredIDR, CAID2-X-ray-PDB and CAID2-NoX-PDB, to inspect the predictor ability to detect two different flavors of disorder.	5.0	0.0	PredIDR2 series can be freely used through the CAID Prediction Portal available at https://caid.idpcentral.org/portal or downloaded as a Singularity container from https://biocomputingup.it/shared/caid-predictors/	Execution time is reported in the CAID prediction portal:  https://caid.idpcentral.org/challenge/results	The model is a CNN, so it could be treated as a blackbox, although there are some methods for CNN interpretablility. 	They used only one output node to predict the state of a residue where the output is 1 if the residue is disordered and 0 if it is ordered.	4.0	0.0	Convolutional Neural Network is used. 	Details are reported in the paper. 	The encoding schema is the same as PredIDR. 	PredIDR2-Prof_Art and PredIDR2-Prof_Rnd (called PredIDR2-Prof series) use the same input features as PredIDR: sequence profile, eight-state secondary structure and 20-class solvent accessibility. PredIDR2-Seq_Art and PredIDR2-Seq_Rnd (called PredIDR2-Seq series) also use three input features. In addition to two features employed in PredIDR: eight-state secondary structure and 20-class solvent accessibility, they use protein sequence rather than sequence profile for the purpose of providing faster speed than PredIDR2-Prof series		No predictions from other methods is used. 		Details are reported in the paper. An Ensemble smoothing is used. 	6.0	2.0	66c495857089c469b477b4ce	40054813.0		02/02/2026 19:46:52	Han KS, Kim HK, Kim MH, Pak MH, Pak SJ, Choe MM, Kim CS.	International journal of biological macromolecules	PredIDR2: Improving accuracy of protein intrinsic disorder prediction by updating deep convolutional neural network and supplementing DisProt data.	10.1016/j.ijbiomac.2025.141801	2025	0.0	0.0	Critical Assessment of Protein Intrinsic Disorder, CAID	1.0	2025-04-09T17:00:44.537Z	2026-02-02T19:46:52.000Z	2d7d6d05-355b-4ead-9fe9-327d0e6040dc	undefined	kkkb6kyxxz				DOME_JSON	Match	Match
6803f55eea60466a7ca5a843	Datasets are available at https://github.com/DagmarIlz/SETH	They used 2 datasets for training and testing available from ODiNPred, CheZOD1325 (training, 1325 proteins) and CheZOD117 (testing, 117 proteins). The train set was further redundancy reduced, and it contained 1174 proteins. 	The training set (CheZOD1325) was redundancy reduced against the test set using MMseqs2 at 20% PIDE(percentage pairwise sequence identity). The CheZOD training data was already constructed such that all protein pairs had less than 50% PIDE.	"training set = 1174 proteins (Positive = 28% , Negative = 72%)
test set = 117 proteins (Positive = 69% , Negative = 31%)"	4.0	0.0	The details are provided in the paper and supplementary materials. 	The method is compared to AUCPred, DISOPRED2, DISOPRED3, DynaMine, ESpritz, GlobPlot, IUPred, MFDp2. RONN, SPOT-disorder, ODiNPred	The confidence intervals at 95% are calculated. 	The AUC is reported. 	The method was tested on CheZOD117 dataset.	5.0	0.0	SETH is freely publicly available at: https://github.com/Rostlab/SETH.	On a machine with one RTX A6000 GPU with 48GB RAM, predicting the nuances of disorder for each residue of the entire human proteome (20,352 proteins) from the individual protein sequences took approximately 23 min. For Swiss-Prot [566,969 proteins ], it took approximately 7 hours. SETH could predict disorder for approximately 10‚Äì20 proteins in 1 s, depending on the protein length. Even on smaller GPUs such as a single NVIDIA GeForce RTX 3060 with 12 GB vRAM, computing predictions for the human proteome still took only an hour. On an AMD Ryzen 5 5500U CPU, performing predictions for our test set CheZOD117 (average protein length 112) only took 12 min, showing that for small sets a GPU is not even necessary.	SETH uses a 2 layer CNN and in a separate model a linear regression, with AI explainability methods like SHAP analysisi could be interpretable. 	The model predicts continuous values as CheZOD scores are, and uses threshold 8 to convert them to binary states.	4.0	0.0	They used linear regression, logistic regression and Convolutional Neural Networks.	Details are provided in the supplementary materials. 	The embeddings from ProtT5  protein language model are 1024 dimensional real-valued vectors per residue in a protein. 	Input features using ProtT5 pLM is 1024.		The use embeddings from ProtT5 protein language models.	The parameters were selected with validation.		6.0	2.0	66c495857089c469b477b4ce	36304335.0	PMC9580958	02/02/2026 19:46:52	Ilzh√∂fer D, Heinzinger M, Rost B.	Frontiers in bioinformatics	SETH predicts nuances of residue disorder from protein embeddings.	10.3389/fbinf.2022.1019597	2022	0.0	0.0	Critical Assessment of Protein Intrinsic Disorder, CAID	1.0	2025-04-19T19:11:26.846Z	2026-02-02T19:46:52.000Z	cd94316f-0bde-4f29-96df-a0bda4ab824f	undefined	0pkzn35pom				DOME_JSON	Match	Match
6804afc6ea60466a7ca5a84d	All data, the source code, and the trained model are available via GitHub (https:// github. com/Rostlab/ bindPredict). More details are provided in the supplementary materials.	Protein sequences with binding annotations were extracted from BioLiP9 . BioLiP provides bind-ing annotations for residues based on structural information from the Protein Data Bank (PDB), i.e., proteins for which several PDB structures with different identifiers exist may have multiple binding annotations. BioLiP annotated 104,733 structures with high enough resolution and binding annotations which could be mapped to 14,894 sequences in UniProt.	The BioLiP set was clustered to remove redundancy using UniqueProt with an HVAL < 0 (corresponding to no pair of proteins in the data set having over 20% pairwise sequence identity over 250 aligned residues	"After the redundancy reduction, the final set of 1314 proteins was split into a development set with 1014 proteins (called DevSet1014 with 13,999 binding residues, 156,684 non-binding residues; Supplementary Table S12) used for optimizing model weights and hyperparameters (after another random split into training and validation), and test set with 300 proteins (TestSet300 with 5869 binding residues, 56,820 non-binding residues;
Supplementary Table S12)"	4.0	0.0	Details are provided in the paper and in the supplementary materials.	Several variation of model were compared together.	Yes all the metrics have confidence intervals and they are statitically significant.	precision, recall, negative precision, negative recall, AUC, F1score , MCC are provided.	The model was tested on independent test set.	5.0	0.0	All data, the source code, and the trained model are available via GitHub (https:// github. com/ Rostlab/ bindPredict)		Since the model is a shallow CNN, it could be interpretable with AI explainability methods.	bindEmbed21DL predicts residues binding to 3 classes: metal ions, nucleic acids (DNA and RNA), and/or regular small molecules	3.0	1.0	The model is a 2 layer Convolutional Neural Network, combined with Homology based transfer with MMseqs2.	Details are provided in the paper and supplementary materials.	The input features are embeddings from ProtT5 language model, a vector of 1024 dimension per residue in a protein.	1024		The model uses embedding from PotT5 protein language model.	The parameters were selected with validation set.	Dropout with 70% rate is used. 	7.0	1.0	66c495857089c469b477b4ce	34903827.0	PMC8668950	02/02/2026 19:46:52	Littmann M, Heinzinger M, Dallago C, Weissenow K, Rost B.	Scientific reports	Protein embeddings and deep learning predict binding residues for various ligand classes.	10.1038/s41598-021-03431-4	2021	0.0	0.0	Critical Assessment of Protein Intrinsic Disorder, CAID	1.0	2025-04-20T08:26:46.191Z	2026-02-02T19:46:52.000Z	e067bb61-a631-44bd-ae1c-01a9f9eac5e9	undefined	eiujukgxuw				DOME_JSON	Match	Match
6908ddc5def4ff8a776704fc	Training and test data are available in the web server: https://services.healthtech.dtu.dk/services/NetStart-2.0/	"RefSeq-assembled genomes and corresponding annotation data from NCBI‚Äôs Eukaryotic Genome Annotation Pipeline Database. 
The final dataset, distributed across five partitions, contains 9 912 708 sequences, of which 1 162 194 are positive (having a translation initiation sites) and 8 750 514 are negative (not having a translation initiation site). "	Data was partitioned using GraphPart (10.1093/nargab/lqad088). The authors used MMseqs2 for alignment and chose a pairwise identity threshold of 50% (10.1038/nbt.3988).	Data was stratified on a per-species basis and partitioned using GraphPart (10.1093/nargab/lqad088).	4.0	0.0	No	Comparisons to dedicated TIS Transformer (10.1093/nargab/lqad021) and ab initio gene finders AUGUSTUS (10.1093/nar/gki458) and the recently developed Tiberius (10.1093/bioinformatics/btae685)	No	MCC. The set seems to be larger and more diverse than datasets in the literature.	Cross-validation and an independent dataset.	3.0	2.0	The model is available as a web server (https://services.healthtech.dtu.dk/services/NetStart-2.0/) and Python package (https://github.com/lsandvad/netstart2). The licence is: CC BY-NC-ND 4.0	Seconds.	Interpretable - in the article authors delve a lot into the interpretation of the learned 	classification	4.0	0.0	a deep learning metamodel trained on three types of concatened embeddings. Beside two custom embeddings, it also uses a pretrained protein language model ESM-2 (general class: transformer).	The model configuration is available at the repository: https://github.com/lsandvad/netstart2/tree/main/src/model_config. The licence is: CC BY-NC-ND 4.0	One-hot encoding of nucleic acids	Three types of inputs: species, nucleic acid sequence, amino acid sequence.		The model uses pre-learned nucleic acid and species embeddings and ESM-2 embeddings		Performance on the non-homologous test set	6.0	2.0	64a80caa92c76639b8082168	40830753.0	PMC12366053	02/02/2026 19:46:52	Nielsen LS, Pedersen AG, Winther O, Nielsen H.	BMC bioinformatics	NetStart 2.0: prediction of eukaryotic translation initiation sites using a protein language model.	10.1186/s12859-025-06220-2	2025	0.0	0.0	translation initiation sites	1.0	2025-11-03T16:52:21.250Z	2026-02-02T19:46:52.000Z	fb507654-1f75-40da-8b6d-f2a729e0547c	undefined	rvf3nedwl1				DOME_JSON	Match	Match
67d04f4c478cfd7e6983b167	No: The data is not available - likely falling under sensitive patient data restrictions.	"Data source: direct experimentation (collection) & publication (for one of the test datasets)

In total: 128,175 macula-centered images  (training data)
-From 2x sites:  33,894 were from India  (Aravind Eye Hospital, Sankara Nethralaya and Narayana Nethralaya) and the rest from EyePACS sites in the USA.

+ separate testing data:
1. EyePACS-1: 9,963 - Random sample of macula-centered images taken at EyePACS screening sites between May 2015 and October 2015.
2. Messidor-2 data set: 1,748 - images were obtained between January 2005 and December 2010 at 3 hospitals in France. (community recognised benchmark dataset)"	Authors assert independent test and training data splits. No data available to explicitly show this. 	"Training: 128,175

Test set 1 - EyePACS-1: 9,963

Test set 2 -  Messidor-2: 1,748

Data distributions - 'Table.  Baseline Characteristics'
                                                                     training set             EyePACS-1          Messidor-2
                                                                    118,419 (100          8,788 (100%)	1,745 (100%)
No diabetic retinopathy	                     53,759 (45.4%)	7,252 (82.5%)	1,217 (69.7%)
Mild diabetic retinopathy	             30,637 (25.9%)	842 (9.6%)	             264 (15.1%)
Moderate diabetic retinopathy 	     24,366 (20.6%)	545 (6.2%)                211 (12.1%)
Severe diabetic retinopathy	              5,298 (4.5%)	          54 (0.6%)	             28 (1.6%)
Proliferative diabetic retinopathy    4,359 (3.7%)	           95 (1.1%)                25 (1.4%)
Referable diabetic macular edema  18,224 (15.4%)	272 (3.1%)	            125 (7.2%)
Referable diabetic retinopathy	     33,246 (28.1%)	683 (7.8%)	            254 (14.6%)"	4.0	0.0		"Comparison was performed against ophthalmology experts classification grading as a baseline:
-Test set graders: Graders who were US board-certified ophthalmologists with the highest rate of self-consistency were invited to grade the clinical validation sets, EyePACS-1 (n‚Äâ=‚Äâ8) and Messidor-2 (n‚Äâ=‚Äâ7)
-Training set graders: 54 graders for the training set were US-licensed ophthalmologists or ophthalmology trainees in their last year of residency (postgraduate year 4). 

Messidor-2 test data set - has been used by other groups for benchmarking performance of automated detection algorithms for diabetic retinopathy. Can be considered a benchmark dataset as a result."	Yes - confidence intervals noted in text in section 'Statistical Analysis and Performance Comparison on Clinical Validation Sets'. Authors note: statistical significance and simultaneous 2-sided confidence intervals were computed.	"-AUC
-Sensitivity
-Specificity

Compared to literature these are standard CNN measures but more performance could be disclosed to support performance claims."	Independent dataset	4.0	1.0	No		Black box - no model code, no CNN network interpretability noted, no dataset files availability, no config files - completely unreproducible and core information missing.	Classification	2.0	2.0	"Convolutional Neural Network (CNN)

Not  a new algorithm: It is Inception-v3 architecture proposed by Szegedy et al."	No	Images are scale normalised to 299 pixels wide using a circular mask of the fundus. Data preprocessing noted in main supplementary data file section: 'Data pre-processing' - images excluded where the circular mask could not be detected.	"f = unknown 

Feature selection not performed - authors relied on CNN learning of features from data provided"		No	p = 22 million	Yes - overfitting prevention techniques used: early stopping, model ensembling using 10 models - final prediction was computed by a linear average over the predictions of the ensemble and hold out set (80:20 split)	5.0	3.0	665a01aa7089c469b4646267	27898976.0		02/02/2026 19:46:52	Gulshan V, Peng L, Coram M, Stumpe MC, Wu D, Narayanaswamy A, Venugopalan S, Widner K, Madams T, Cuadros J, Kim R, Raman R, Nelson PC, Mega JL, Webster DR.	JAMA	Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs.	10.1001/jama.2016.17216	2016	0.0	0.0	Deep learning, Diabetic Retinopathy, Image data	1.0	2025-03-11T14:57:16.452Z	2026-02-02T19:46:52.000Z	001e9b6a-5273-4c53-b93e-465fbbebd3a5	undefined	cvu6djmvfq				DOME_JSON	Match	Match
67f53833822ee03b50b6c3ce	The TR166 and TE82 datasets are available at https://yanglab.nankai.edu.cn/APOD/benchmark/.	DisProt 8.0 is used to extract proteins with Disorder Flexible Linker (DFL). The annotation protocol is the same as in DFLPred. Using the functional annotations DisProt they annotated 5893 residues in 175 DFL regions that are located in 117 IDPs. They collected 131 proteins that have majority of their residues functionally annotated and where this annotation is not DFL.	They cluster the corresponding 117‚Äâ+‚Äâ131‚Äâ=‚Äâ248 protein sequences using BLASTClust (Altschul et al., 1997) at the 25% sequence identity. They divide the resulting 194 clusters into training and test datasets at random, where the training set (TR166) includes 166 sequences with 3661 DFL residues, and the test set (TE82) is composed of 82 chains with 2223 DFL residues. The placement of entire clusters into the two datasets ensures that training and test proteins share below 25% sequence similarity. 	They cluster the corresponding 117‚Äâ+‚Äâ131‚Äâ=‚Äâ248 protein sequences using BLASTClust (Altschul et al., 1997) at the 25% sequence identity. They divide the resulting 194 clusters into training and test datasets at random, where the training set (TR166) includes 166 sequences with 3661 DFL residues, and the test set (TE82) is composed of 82 chains with 2223 DFL residues.	4.0	0.0		The method is compared with DFLPred, IUPred2, DISOPRED3, PredyFlexy, and FUpred.		Precision, Recall, AUC and MCC are reported.	The method was tested on an independent dataset, TE82.	3.0	2.0	a webserver is available at https://yanglab.nankai.edu.cn/APOD.	Not reported.	Since the model uses a SVM, it could be interpretable.	The predictions take form of numeric propensity scores, which quantify likelihood that a given residue is in the DFL region, and the corresponding binary score, which categorizes a given residue as either DFL or non-DFL.	4.0	0.0	A support vector machine (SVM) is used.	They are reported in the paper.	They use sequence profiles as inputs. 	AAC = 40, CONS=56, SS+RSA_based=38, SWdis23, total number of features is 169.		APOD takes protein sequence as the only input and processes it in two steps to produce prediction for every residue. The first step involves use of several third-party programs to provide a rich sequence profile that characterizes relevant (putative) structural properties of the input chain. More precisely, these properties include the amino acid composition (AAC) extracted directly from the input sequence, CONS generated with PSI-BLAST (Altschul et al., 1997), putative SS and the relative solvent accessibility (RSA) predicted with SPARKS-X (Yang et al., 2011) and putative intrinsic disorder derived from the DISOPRED3 prediction (Meszaros et al., 2018), which includes the sliding window-based representation of disorder (SWdis) and protein-level DisCon value.	They use the popular RBF kernel and they optimize values of two parameters: complexity constant C and width of the kernel Œ≥. They set their values to 2i where i = ‚àí5, ‚àí4, ‚Ä¶, 5. They also optimize the sliding window size ws by considering the values ranging between 9 and 25; the two values correspond to the 10 percentile and the median length of DFL in the training dataset, respectively. They select the optimal values for ws, C, Œ≥  by maximizing the average AUC values calculated over the five test folds in the 5-fold cross validation on the training set TR166. The optimized APOD model applies ws = 13, C‚Äâ=‚Äâ1 and Œ≥ ‚Äâ=‚Äâ 0.0625. 	5 fold cross validation was used in the training. 	7.0	1.0	66c495857089c469b477b4ce	33381830.0	PMC7773485	02/02/2026 19:46:52	Peng Z, Xing Q, Kurgan L.	Bioinformatics (Oxford, England)	APOD: accurate sequence-based predictor of disordered flexible linkers.	10.1093/bioinformatics/btaa808	2020	0.0	0.0	Critical Assessment of Protein Intrinsic Disorder, CAID	1.0	2025-04-08T14:52:35.623Z	2026-02-02T19:46:52.000Z	21cd97b9-dad0-4f1c-9334-615667b5f5db	undefined	lk0hxbd7im				DOME_JSON	Match	Match
67f5537a822ee03b50b6c3e7	The training and test datasets have 144 sequences and 60 sequences, respectively, and they are available at http://biomine.ece.ualberta.ca/DFLpred/ .	The functionally annotated data were collected from the newest release 6.0.2 of DisProt that includes 694 sequences. They excluded DP00688 sequence that was too long (>18 000 residues) to predict with the PSIPRED ( Buchan et al. , 2013 ) to generate secondary structure. They selected 204 sequences, which include 82 proteins that have annotations of DFLs and 122 proteins that do not have DFL annotations but for which all residues are annotated.	They clustered the 204 sequences with sequence identity threshold at 25% and coverage of at least 10% of the sequence length. Second, the resulting 160 clusters that include similar sequences (>25% similarity) were divided at random between the five sub sets to ensure that each subset has similar number of sequences and similar ratio of DFL to NDFL residues.	They divided the set of 204 proteins into five subsets and reduced sequence similarity between these subsets with BLASTClust ( Altschul et al. , 1990 ). First, They clustered the 204 sequences with sequence identity threshold at 25% and coverage of at least 10% of the sequence length. Second, the resulting 160 clusters that include similar sequences (>25% similarity) were divided at random between the five sub sets to ensure that each subset has similar number of sequences and similar ratio of DFL to NDFL residues. Four of these subsets were used in 4-fold cross-validation protocol to empirically design the predictor, i.e. to conceptualize and select inputs for the predictive model, and to select and parameterize this model. These data constitute the training dataset. The remaining fifth subset was used as an independent (never used in the design) test dataset.	4.0	0.0	It is available in the supplementary materials.	on the same test set, it was compared with UMA, EspritzNMR, IUPred, PredBF, PredyFflexy, FlexPred, PROFbval, DynaMine, MFDp.		AUC is reported.	an independent test set was used.	4.0	1.0	the webserver is available at http://biomine.ece.ualberta.ca/DFLpred/ 	On human proteome (20 193 sequences with an average length of 561), DFLpred takes 40min .	The model is a logistic regression, so it is interpretable.	The prediction of DFLs results in a numeric score in the range between 0 and 1 representing propensity of each residue in the input sequence being in a DFL.	4.0	0.0	Logistic regression is used. 	they are reported in the paper. 		In the first layer, they represented every residue by its AA type, its physicochemical properties estimated based on the AA indices from the AAindex database ( Kawashima et al. , 2008 ), secondary structure predicted with PSIPRED ( Buchan et al. , 2013 ), intrinsically disordered and structured regions predicted with IUPred ( Doszt√°nyi et al. , 2005 ) and sequence complexity computed with SEG. In the second layer, they generated numerical features that quantify the considered structural and sequence-based properties for each residue of the input AA sequence. they represented every residue by a feature vector calculated from a sliding window centered on that residue. The sliding window aggregates structural and sequence-based information by considering characteristics of AAs adjacent in the sequence. Consequently, they normalize values of features computed over the residues in the window by the size of the window. In total, they considered 2236 features including 40 features derived directly from the sequence, 2124 features derived from physicochemical properties of AAs quantified based on the AAindex database, 22 features generated from the putative secondary structure, 40 features from putative intrinsic disorder and structured regions and 10 features from the sequence complexity.		The model uses output from IUPred. 	Parameters of logistic regression were selected with 4 fold cross-validation.	4 fold cross-validation was used. 	6.0	2.0	66c495857089c469b477b4ce	27307636.0	PMC4908364	02/02/2026 19:46:52	Meng F, Kurgan L.	Bioinformatics (Oxford, England)	DFLpred: High-throughput prediction of disordered flexible linker regions in protein sequences.	10.1093/bioinformatics/btw280	2016	0.0	0.0	Critical Assessment of Protein Intrinsic Disorder, CAID	1.0	2025-04-08T16:48:58.347Z	2026-02-02T19:46:52.000Z	34108257-17f7-4d3b-a08f-b68bf38e1970	undefined	u5wbqo3nr4				DOME_JSON	Match	Match
67f63fc4822ee03b50b6c442	The training data is available in DisProt. The Disorder-NOX test set was used from CAID2 that is available at  https://caid.idpcentral.org/challenge/results	The dataset collection for flDPnn2 follows the process that they used for flDPnn (see Supplement for details) but using a much larger and publicly available collection of the disorder-annotated proteins from the version 9.1 of DisProt that they downloaded in March 2022.	Not reported.	They randomly divided these 2,227 sequences into the training dataset (1,527 sequences), validation dataset (200 sequences), and test dataset (500 sequences). They designed the new predictive model using the training and validation datasets.	4.0	0.0	Evaluations are available in the paper and in the supplementary materials. 	The method was compared to all the methods in CAID1 and CAID2. 	Moreover, we assess statistical significance of differences between predictors, focusing on comparing other methods against flDPnn2They evaluate whether differences are consistent over a range of different test sets by performing 20 random selections of 50% of proteins from the Disorder-NOX dataset. They evaluate significance of differences over these 20 paired results using the student‚Äôs t-test if the measurements are normal, as evaluated with the Anderson-Darling test at 0.05 significance; otherwise they use the Wilcoxon test. 	Inspired by the CAID2 assessment, they evaluate the binary predictions with F1 and MCC (Matthews correlation coefficient) and they use AUC (area under the receiver-operating characteristic (ROC) curve) and AUPRC (area under the precision-recall curve) to assess the propensities. Since the disordered residues are in minority (19.5% of amino acids), they follow related studies and compute lowAUCratio and lowPRCratio.	The method is tested on the CAID2 Disorder-NOX reference set. 	5.0	0.0	flDPnn2 is freely available as a convenient web server at http://biomine.cs.vcu.edu/servers/flDPnn2/	The runtime is about 27s per proteins. A comparison between the runtimes of all CAID2 methods including flDPnn2 is reported in CAID2 paper: doi/10.1002/prot.26582	The model contains a feed forward neural network, so it could be treated as not interpretable. 	The model outputs continuous values, but also optimizes a threshold to assign binary states to each residue. 	4.0	0.0	flDPnn2 model produces disorder predictions in three steps : 1) compute a sequence profile from the input sequence; 2) encode features from the profile; and 3) process the features via a deep neural network and an alignment module to generate putative propensities of disorder.	They are reported in the paper. 	 They  use the profile to compute three feature sets: residue-level, window-level and protein-level			The method uses the outputs of IUPred2, PSIPRED, DisoRDPbind (DNA-/RNA-/protein- binding), DFLPred, ANCHOR2. Since the method was tested on CAID2 Disorder-NOX dataset, the training data of the methods trained before CAID2's timeline do not have overlap with the test set. 		Dropout is used. 	5.0	3.0	66c495857089c469b477b4ce	39237195.0		02/02/2026 19:46:52	Wang K, Hu G, Basu S, Kurgan L.	Journal of molecular biology	flDPnn2: Accurate and Fast Predictor of Intrinsic Disorder in Proteins.	10.1016/j.jmb.2024.168605	2024	0.0	0.0	Critical Assessment of Protein Intrinsic Disorder, CAID	1.0	2025-04-09T09:37:08.251Z	2026-02-02T19:46:52.000Z	2fcf5fb0-9896-49a3-8b83-dd8b0040853f	undefined	1cvlnr4o3g				DOME_JSON	Match	Match
68039755ea60466a7ca5a83b		They used DM4229, which is a training dataset for the IDR prediction program, SPINE-D. DM4229 contains 4,229 sequences selected from PDB and DisProt. In the procedure to create DM4229, PDB structures, with resolution < 2 ‚Ñ´ and length > 60 residues, were clustered at 25% sequence identity to select representative proteins with long contiguous IDRs from each cluster. These representative sequences were combined with fully disordered proteins from the IDP database, DisProt, and sequence redundancy was reduced again. The sequences identified in the test dataset (IDEAL) were excluded. The procedure produced 4,189 sequences consisting of 925,412 ordered and 100,284 disordered residues. Among these, 842 sequences were used to validate the hyperparameters of the neural networks, and the remaining 3,347 sequences were used to optimize the biases and weights. 	Data was redundancy reduced at 25%	The procedure produced 4,189 sequences consisting of 925,412 ordered and 100,284 disordered residues. Among these, 842 sequences were used to validate the hyperparameters of the neural networks, and the remaining 3,347 sequences were used to optimize the biases and weights. 	3.0	1.0	Evaluation results are available in the paper and in the supplementary materials.	The method was compared with ANCHOR2, MoRFchibi-web, DISOPRED3 and IUPred2		Sensitivity, precision, fscore, MCC is provided which are the common metrics used in CAID.	The model was tested on an independent dataset (IDEAL and CheZOD and CASP10)	4.0	1.0	Web server is available at http:// flab.neproc.org/neproc/index.html and also in the IDEAL database https://www.ideal-db.org/	Not reported	The model could be interpretable since it combines neural networks and SVM. 	The models output a three states labels of disorder, order or binding region,  so it is classification.	4.0	0.0	NeProc combined neural networks and support vector machines (SVM)	Details are provided in the supplementary materials. 	The query sequence is converted to position specific scoring matrix (PSSM) using PSI-BLAST search against UniReg90.  . From a PSSM, they extracted the 21-dimensional vector for a site, including scores for each of the amino acid residues and one for the information per position, as with DISOPRED3.	Input features come from PSSM, and they are 21. 		It doesn't use meta predictors.	The hyperparameters were determined using the subset of the training dataset of 842 sequences. The parameters, weights and biases of the first and the second networks, were optimized using the subset of the remaining 3,347 sequences.	Adam optimizer with weight decay was used.	7.0	1.0	66c495857089c469b477b4ce	33304713.0	PMC7692026	02/02/2026 19:46:52	Anbo H, Amagai H, Fukuchi S.	Biophysics and physicobiology	NeProc predicts binding segments in intrinsically disordered regions without learning binding region sequences.	10.2142/biophysico.bsj-2020026	2020	0.0	0.0	Critical Assessment of Protein Intrinsic Disorder, CAID	1.0	2025-04-19T12:30:13.770Z	2026-02-02T19:46:52.000Z	d6dfa62f-ffcc-4fe2-8218-c511d3ba9275	undefined	4z11aw60r6				DOME_JSON	Match	Match
6809f90dea60466a7ca5aac2	Details are provided in the paper. 	"They obtained 4229 non-redundant, high-resolution protein sequences from the Protein Data Bank (PDB) and Database of Protein Disorder (DisProt). These include 4157 X-ray crystallography structures (deposited to the PDB prior to August 05, 2003) and 72 fully-disordered proteins from DisProt v5.0. 
For testing, they obtained three independent test datasets (SL250, Mobi9414, and DisProt228) . They removed long proteins (>700 residues) in these sets, and homologous proteins from the training set. "	Data is non redundant. The sequence similarity in the entire dataset is less than 25%. 	"The data is randomly split into a training set (Training) of 2700 chains, a validation set (Validation) of 300 chains, and a testing set (Test) of 1229 chains. Sequence similarity among these proteins is <25% according to BLASTClust . They remove all proteins of length >700 from all datasets. This reduces our training, validation, and test sets to 2615, 293, and 1185 proteins, respectively. For convenience, they will label this test set as Test1185.
For test set, they obtained three independent test datasets (SL250, Mobi9414, and DisProt228) . They removed long proteins (>700 residues) in these sets, and homologous proteins from the training set. "	4.0	0.0	Details are available in the supplementary materials. 	They compare SPOT-Disorder2 to several high-performing protein disorder predictors. These include the local versions of DISOPRED2 and DISOPRED3, MobiDB-lite, AUCpreD, s2D, GlobPlot, DisEMBLE, IUPred, AUCPred, JRONN, MFDp2, PONDR-VSL, SPOT-Disorder,  SPOT-Disorder-Single, ESpritz-D, ESpritz-N, ESpritz-X, and SPINE-D. They also used the webserver of NetSurfP-2.0.	Statistical significance test is done and the results are statistically significant. 	Sensitivity, precision, specifity, AUC ROC, AUC PR, and MCC are used, which are common metrics used in CAID.	They obtained three independent test datasets (SL250, Mobi9414, and DisProt228) . They removed long proteins (>700 residues) in these sets, and homologous proteins from the training set. 	5.0	0.0	SPOT-Disorder2 is available as a web server and as a standalone program at https://sparks-lab.org/server/spot-disorder2/.		The model is a combination of several neural networks, so it could be treated as a black-box. 	The model is regression. It outputs the probability of an amino acid being disordered, so it's a value between 0 and 1.	3.0	1.0	The neural network topology employed in SPOT-Disorder2 consists of various models sequentially combining IncReSeNet, LSTM, and fully-connected (FC) topographical segments.	All the details are available in the supplementary materials. 		"SPOT-Disorder2 employed a similar set of features to SPOT-Disorder. Besides the same evolutionary content consisting of the position-specific substitution matrix (PSSM) profile from PSI-BLAST , SPOT-Disorder2 also includes the hidden Markov model (HMM) profile from HHblits [38]. The PSSM profile is generated by 3 iterations of PSI-BLAST against the UniRef90 sequence database (UniProt release 2018_03), and consists of 20 substitution values of each position for each AA residue type. The HMM profile consists of 30 values generated by using HHblits v3.0.3 with the UniProt sequence profile database from Oct 2017. These 30 values themselves consist of 20 AA substitution probabilities, 10 transition frequencies, and the number of effective homologous sequences of a given protein (Neff). In addition, they utilized the predicted structural properties from SPOT-1D. The features from SPOT-1D consist of 11 secondary structure probabilities (both three- and eight-state predicted secondary structure elements), 4 sine and 4 cosine Œ∏, œÑ, œÜ, and œà backbone angles, 1 relative solvent-accessible surface area (ASA), 1 contact number (CN), and 2 half-sphere exposure (HSE) values based on the carbon-Œ± atoms.

In total there are 73 features in the input. "		They utilized the predicted structural properties from SPOT-1D. The features from SPOT-1D consist of 11 secondary structure probabilities (both three- and eight-state predicted secondary structure elements), 4 sine and 4 cosine Œ∏, œÑ, œÜ, and œà backbone angles, 1 relative solvent-accessible surface area (ASA), 1 contact number (CN), and 2 half-sphere exposure (HSE) values based on the carbon-Œ± atoms.	A large corpus of models with varying hyperparameters are trained and their performance is analyzed on a validation set. These hyperparameters are swept through in a grid search and include the layout of the network, the number of nodes in each layer (one parameter each for LSTM, IncReSeNet, and FC layers), and the number of layers for each layer type. The five top-performing models with hyperparameters are chosen from this validation period and used in the final ensemble for SPOT-Disorder2.	Dropout is used in different layers. 	6.0	2.0	66c495857089c469b477b4ce	32173600.0	PMC7212484	02/02/2026 19:46:52	Hanson J, Paliwal KK, Litfin T, Zhou Y.	Genomics, proteomics & bioinformatics	SPOT-Disorder2: Improved Protein Intrinsic Disorder Prediction by Ensembled Deep Learning.	10.1016/j.gpb.2019.01.004	2019	0.0	0.0	Critical Assessment of Protein Intrinsic Disorder, CAID	1.0	2025-04-24T08:40:45.204Z	2026-02-02T19:46:52.000Z	a7fe5bb8-985a-4fd8-ae1f-9f2ecbf1c06d	undefined	w55oz05fks				DOME_JSON	Match	Match
67cf5b261f0965481b0c7b53	The details about dataset split is provided in the supplementary materials.	Protein sequence of 12 proteomes (~300000) were used to train the model, predicting the disorder consensus score of eight predictors:  IUPred short, IUPred long, ESpiritz (DisProt, NMR, and xray), DisEMBL 465, DisEMBL hot loops, and GlobPlot.	For training, validation, and testing, 70% of the data was used for training, 15% of the data was used for validation, and 15% of the data was used for testing. To independently test the model, CAID1 test set (646 proteins) and CheZOD (116 proteins) were used.	300000 proteins were used to train. For training, validation, and testing, 70% of the data was used for training, 15% of the data was used for validation, and 15% of the data was used for testing. To independently test the model, CAID1 test set (646 proteins) and CheZOD (116 proteins) were used.	4.0	0.0	All the comparison results are available in the supplementary materials.	The method is compared to all the disorder predictors available at the time. Comprehensive assessment could be found in the supplementary materials.		All the common performance metrics aligned with CAID challenge are provided: MCC, AUC, APS, BAC, F1-max, execution time, etc.	The model was tested on CAID1 dataset and CheZOD dataset.	4.0	1.0	The software is available in the repo: https://github.com/idptools/metapredict/tree/master. It is available as a webserver: https://metapredict.net/ . Documentation is available at : https://metapredict.readthedocs.io/en/latest/index.html.	On all hardware tested (which included a laptop from 2012), metapredict obtained prediction rates of ~7000‚Äì12,000 residues per second (see Supporting materials and methods for further details). A single 300-residue protein takes ~25 ms, and the human proteome (20,396 sequences) takes ~21 min. Importantly and unlike some other predictors, the computational cost scales linearly with sequence length.	The model is BiRNN, so it could be considered a black-box.	The model output is regression, values between 0 and 1, but they are converted to binary class using an optimized threshold.	4.0	0.0	BiRNN has been used to predict a disorder consensus score from sequence.	They are available in the supplementar materials. Additionally they are available in the github: https://github.com/idptools/metapredict/tree/master 	Residues are converted into One-hot encoded vectors.	Residues are converted to one-hot vectors of length 20. 		No			4.0	4.0	66c495857089c469b477b4ce	34480923.0	PMC8553642	02/02/2026 19:46:52	Emenecker RJ, Griffith D, Holehouse AS.	Biophysical journal	Metapredict: a fast, accurate, and easy-to-use predictor of consensus disorder and structure.	10.1016/j.bpj.2021.08.039	2021	0.0	0.0	CAID, CAID2, Critical Assessment of Protein Intrinsic Disorder	1.0	2025-03-10T21:35:34.475Z	2026-02-02T19:46:52.000Z	a492dbd3-cfc4-4a8d-be15-7bb4490e224f	undefined	rdo1uf4g11				DOME_JSON	Match	Match
67ea8a51822ee03b50b6b96f	Details are provided in supplementary materials.	"Sequences from DisProt 2023_12 are used in training, augmented by annotations from IDEAL and DIBS. Four datasets are made labeled as ""linker"", ""protein(binding)"", ""nucleic acid(binding)"", and ""all(protein and binding)"". 4-fold cross validation is used, and sequences are clustered by 90% identity. Finally, for each of the four groups, they doubled (for 'linker', 'protein', and 'all') and tripled (for nucleic) its sequences by adding random sequences from the remaining DisProt sequences."	Training set was redundancy reduced by 90% identity, and test set was redundancy reduced against Training set by 30% identity using CD-HIT. HAM was applied too.	They included all of the 185 new DisProt release 2024_06 sequences that are not in the 2023_12 release. To eliminate potential data contamination related to test sequences homologous to those used in training, they used CD-HIT to filter out test sequences with more than 30% identity to those used in training. Then, they extend the annotations of these sequences from the MobiDB-LIP database.	4.0	0.0	Details are provided in the supplementary materials.	"the method was compared to DisoFLAG ,NSHROUD, AlphaFold_binding, DeepDISOBind, APOD, MoRFchibi_Light, MoRFchibi_web, ANCHOR2, DisoRDPbind,
OPAL, ProBiPred ."		AUC and other common performance metrics established by CAID are reported.	Independent test sets were developed. 	4.0	1.0	software is available at orca.msl.ubc.ca/nmshare/ipa.tar.gz	using an Intel Core i9, 8/16 cores/threads CPU with 40G DDR4 RAM, and an RTX 3080 GPU machine, it took IPA 4 hours, 9 minutes, and 1 second to process all of the SwissProt (2024_06) 572,619 sequences and generate nucleotide, protein, and all binding predictions. On average, 2,300 sequences per minute or 38.3 sequences per second. When using the CPU to run IPA on the same machine, processing these SwissProt sequences took 11 hours, 45 minutes, and 39 seconds. That is 812 sequences per minute or 13.5 sequences per second.	since the method uses CNN, it could be treated as a blakc-box.	the model outputs a probability for each residue for being protein-binding, nucleotide-binding, all-binding and linker, so it is regression.	4.0	0.0	Convolutional Neural Network is used. 	Details are provided in the paper and supplementary materials.		"Input features are divided into three categories: two affinity features, five DisProt features (F5 in short), and two alphaFold-2 features. IPA nucleic only utilizes the F5 features, IPA (protein and all) utilizes the affinity and the F5 features, and IPA-AF2 utilizes all nine. Affinity features are derived from P, a 20 x 20 energy predictor matrix. For each position p between 1 and the last in the sequence, one of the affinity features is the sum of the energy in P between the residue at p and the w1=10 residues on each side of p. The second is the sum of this energy between w1 and w2=w1+25, from p + w1 to p + w2 on each p side. The F5 features are the frequency of each of the 20 standard amino acids in each of the five classes in the DisProt/CAID datasets: IDR, nucleic binding (N_bind), protein binding (P_bind), and 'Linker' in the DisProt data and 'PDB' annotation in the combined CAID1 and CAID2 sequences, divided by the frequency of that amino acid in the entire dataset, Supplementary Table S3. Finally, the two AlphaFold-2 features are those
used by the AlphaFold-binding predictor [48] and collected from the AlphaFold-2 generated structures; the alphaFold-2 relative solvent accessibility, RSA, and the AlphaFold-2 prediction accuracy score, pLDDT."		pLDDT and RSA from AlphaFold2 are used in the features.		4-fold Cross validation and grid-search are used. 	5.0	3.0	66c495857089c469b477b4ce			02/02/2026 19:46:52	Nawar Malhis	bioRxiv	Probabilistic Annotations of Protein Sequences for Intrinsically Disordered Features	https://doi.org/10.1101/2024.12.18.629275	2025	0.0	0.0	Critical Assessment of Protein Intrinsic Disorder, CAID	1.0	2025-03-31T12:28:01.797Z	2026-02-02T19:46:52.000Z	e2153f36-3804-4b1b-aff7-6c0b6a47de22	undefined	2335p2x49r				DOME_JSON	Match	Match
67c42e8f1f0965481b0c743b	The data is not available, but the statistics of data and splits are available in the supplementary materials.	For training and validation, two data sets are used: DM4229 and IDEAL database. Validation set is extracted from the training set after redundancy reduction. Furthermore another dataset, SL dataset is used as validation. For the test set, DisProt646 and DisProt-PDB646 (from CAID1), Disorder-PDB and Disorder-NOX (from CAID2) are used.	The training and validation datasets were redundancy reduced with BlastClust with 25% identity. They were again redundancy reduced against CAID evaluation sets with 25% identity. 	The final training set contained 4286 proteins and the validation set was 476 proteins. The data was redundancy reduced using BlastClust from the beginning with 25% identity. The training and validation data again were redundancy reduced against CAID evaluation sets with 25% identity. 	4.0	0.0	Evaluations are available in the paper and in the supplementary materials.	Yes, comparisons with IUPred3, SPOT-disorder, fIDPnn are reported.		MCC, AUROC, Precision, Sensitivity, and  Execution times were reported. They align with the performance metrics established in the CAID community challenge. 	The method was evaluated on CAID1 and CAID2 datasets. Validation was performed during training. 	4.0	1.0	The software is available as a webserver, a python package and a docker image. The source code is available in Github. They could be found at https://antepontem.org/daruma/index.html	 The execution times for processing the human proteome were 23.2, 29.6, and 43.6 minutes on the Xeon E5 128G, Xeon Silver 92G, and Core i3 4G 2nd Gen, respectively, resulting in per protein execution times of 0.068, 0.084, and 0.13 seconds.	Since the model is composed of convolutional nerual networks and feed foward neural networks, it could be treated as a blackbox. While CNNs provide some interpretability through learned filters, the feedforward layers still introduce non-linearity, making full transparency challenging.	Classification	4.0	0.0	The architecture is composed of two parts: a feature extraction unit which is a convolutional neural network and a prediction unit which is a feed forward neural network.	Everything is reported in the papern and additionally in the supplementary materials.	The proteins datasets were redundancy reduced iwth 25% identity. To represent the features of amino acid sequences, AAindex1 from AAindex database was used which contains 566 indexes for physiochemical properties of each amino acid. They utilized 553 indexes as feature values, excluding any with N/A values. The values were normalized to a scale from 0 to 1 across the 20 numerical properties. Consequently, each amino acid residue was represented by a 553-dimensional vector of physicochemical properties, and an amino acid sequence with L residues was embedded into an L √ó 553-dimensional vector.	"They selected AAindex1,which contains 566 indexes for the physicochemical properties of each amino acid, including factors such as side chain size and hydrophobicity. They utilized 553 indexes as feature values, excluding any with N/A values. The values were normalized to a scale from 0 to 1 across the 20 numerical properties.
Consequently, each amino acid residue was represented by a 553-dimensional vector of physicochemical properties, and an amino acid sequence with L residues was embedded into an L √ó 553-dimensional vector."					5.0	3.0	66c495857089c469b477b4ce			02/02/2026 19:46:52	Itsuki Shimizu, Takuya Ida, Yuhei Ozawa, Satoshi Fukuchi, Hiroto Anbo	BMC Bioinformatics	DARUMA: Your gateway to fast and easy prediction of intrinsically disordered regions	10.21203/rs.3.rs-5414158/v1	2024	0.0	0.0	Critical Assessment of Protein Intrinsic Disorder, CAID	1.0	2025-03-02T10:10:23.303Z	2026-02-02T19:46:52.000Z	779edd6d-cdee-4176-821a-49c30467ca51	undefined	jwdcelhyd3				DOME_JSON	Match	Match
67cf64de1f0965481b0c7b57	Details are provided in the Materials and Methods in the paper. 	455,666 protein sequences were obtained from SwissProt.	The model was tested on independent test sets of CAID1 Disorder-PDB and CAID2 Disorder-PDB. 	455,666 protein sequences were used and the dataset was split into 70:15:15 ration for train, validation and test.	4.0	0.0		The method is compared to the most disorder predictors in CAID.		Matthew‚Äôs correlation coefficient (MCC) was reported. 	The model was test with independent dataset of CAID1 Disorder-PDB and CAID2 Disorder-PDB. 	3.0	2.0	metapredict V3 is distributed in a range of implementations, which include (1) a more streamlined Python interface (https://metapredict.readthedocs.io/en/latest/), (2) an online web server (https://metapredict.net), (3) an easy-to-use command line tool (https://metapredict.readthedocs.io/en/latest/), (4) a Docker container (https://hub.docker.com/r/holehouselab/metapredic) and (5) a Colab notebook (https://colab.research.google.com/github/idptools/metapredict/blob/master/colab/metapredic t_colab.ipynb).	metapredict V3 enables the prediction of entire proteomes in seconds (e.g., yeast proteome in < 1 second, human proteome in ~4 seconds). Details about the hardware used are provided in the paper	Since the model uses BiRNN-LSTM network, it could be interpretted as a black-box.	The model is a regression, but the outputs are converted to binary classification based on an optimized threshold.	4.0	0.0	A BiRNN-LSTM network is used to predict the combination of disorder consensus score and pLDDT values from AlphaFold2.	All the details are provided in the supplementary materials plus the github repo: https://github.com/idptools/metapredict/tree/master	Residues were converted to one-hot vectors.	Residues were converted to one-hot vectors of 20 dimension.		It doesn't use other predictions in the input.			5.0	3.0	66c495857089c469b477b4ce			02/02/2026 19:46:52	Jeffrey M. Lotthammer, Jorge Hern√°ndez-Garc√≠a, Daniel Griffith, Dolf Weijers, Alex S. Holehouse, Ryan J. Emenecker	bioRxiv (Pre-print)	Metapredict enables accurate disorder prediction across the Tree of Life	https://doi.org/10.1101/2024.11.05.622168	2024	0.0	0.0	Critical Assessment of Protein Intrinsic Disorder, CAID	1.0	2025-03-10T22:17:02.204Z	2026-02-02T19:46:52.000Z	d3ad609d-7004-4bf7-958f-7032cdb992f1	undefined	cftpt5nnni				DOME_JSON	Match	Match
67f55ee6822ee03b50b6c3eb	Data set is available at https://biomine.cs.vcu.edu/servers/CLIP/#Materials. 	They use MobiDB 3.0 to collect the 2303 proteins that include manually curated LIPs. Following the annotation protocol from, they first cluster these 2303 sequences using CD-HIT with 100% sequence identity. They set the longest chain in a given cluster as a representative sequence and transfer annotations of LIPs in a given cluster into this chain. Altogether, this procedure introduces 69 additional LIP residues when compared with the annotation without the transfer, i.e. 0.19% increase in the amount of LIP annotations; Supplementary Materials provide further details. Second, they group the resulting 2285 sequences into 1440 clusters by applying CD-HIT with 25% sequence identity threshold, and select the longest sequence to represent each cluster. They divide these sequences into a training dataset TR1000 and a test dataset TE440 at random. The TR1000 dataset is composed of 1000 proteins that have 1380 LIPs (average of 1.38 LIP per protein) and 24 821 amino acids in the LIP regions. The remaining 440 proteins, which cover 612 LIPs (average of 1.39 LIP per protein) and 11 994 residues in the LIP regions, constitute the TE440 dataset.	CD-HIT with 25% redundancy was used. 	The TR1000 dataset is composed of 1000 proteins that have 1380 LIPs (average of 1.38 LIP per protein) and 24 821 amino acids in the LIP regions. The remaining 440 proteins, which cover 612 LIPs (average of 1.39 LIP per protein) and 11 994 residues in the LIP regions, constitute the TE440 dataset.	4.0	0.0		It was compared to SPOT-MorF, MoRFCHiBi,  DisoRDPbind, and ANCHOR2 on EXP25 and TE440 datasets. 		AUC, precision, recall and MCC are reported. 	the method used an independent dataset TE440 to evaluate the model on a LIP dataset. They used  another experimental MoRF dataset, EXP25, to compare the performance with MoRF predictors. 	3.0	2.0	 The webserver of CLIP is freely available at http://biomine.cs.vcu.edu/servers/CLIP/ and the standalone code can be downloaded from http://yanglab.qd.sdu.edu.cn/download/CLIP/		The model uses linear regression in the end so it is interpretable, but to calculate the features it uses SVM and Random Forest.  	The model outputs both a binary prediction and a propensity score for the residue to be disordered.	3.0	1.0	SVM, Random Forest and Logistic regression are used. 	Details are provided in the paper. 		The three types of inputs include co-evolutionary information (COEV), physiochemical characteristics of amino acids that are relevant to binding (PHYS) and disorder prediction generated with the popular DISOPRED3 method [81]. We produce these inputs at the residue level, combine them together into a 3-dimensional vector and process the resulting vector using sliding windows to produce inputs for the logistic regression.		The model uses outputs from DISOPRED3. 	They use logistic regression to combine the COEV, PHYS and disorder prediction inputs. They parametrize this model by identifying a favorable value of the ridge parameter based on the 5-fold cross-validation on the TR1000 dataset. More specifically, we consider ridge‚Äâ=‚Äâ10^n where n‚Äâ=‚Äâ‚àí5, ‚àí4, ‚Ä¶‚Ä¶4, 5 and select the ridge value‚Äâ=‚Äâ0.1 that produces the highest average AUC that we calculate over the five AUC scores from the five test folds in the cross-validation experiment.	5 fold cross validation was used. 	6.0	2.0	66c495857089c469b477b4ce	36458437.0		02/02/2026 19:46:52	Peng Z, Li Z, Meng Q, Zhao B, Kurgan L.	Briefings in bioinformatics	CLIP: accurate prediction of disordered linear interacting peptides from protein sequences using co-evolutionary information.	10.1093/bib/bbac502	2022	0.0	0.0	Critical Assessment of Protein Intrinsic Disorder	1.0	2025-04-08T17:37:42.490Z	2026-02-02T19:46:52.000Z	d18229c7-9411-4f5c-b962-74d0bc45f3cf	undefined	joud1a8rk2				DOME_JSON	Match	Match
68036cb4ea60466a7ca5a837	All dataset files are available at https://huggingface.co/datasets/deeeeeeeeee/PUNCH2_data.	PDB entries with missing residues and DisProt entries that are fully disordered are used as training dataset. The test sets include CAID1, CAID2, and CAID3. The details about the number of instances are provided in the paper. 	The test sets were independent and included the official test sets from CAID1, CAID2 and CAID3. The training sets were redundancy reduced at multiple levels. 	The details about all dataset splits are reported in the paper. The validation set was extracted from the training set. 	4.0	0.0		The comparison with other methods is available in the paper. 		AUCROC, APS, F1, MCC, and AUC-PR were reported, which are used in CAID. 	The method used independent datasets for testing. 	3.0	2.0	the sever is available at https://alienlabs.ucd.ie/punch2/. The source code is available at https://github.com/deemeng/punch2 and https://github.com/deemeng/punch2_light. Docker images are available at https://hub.docker.com/r/dimeng851/		The model could be treated as a black box.	The model outputs a score for each amino acid to be disordered. they are converted to binary states using a threshold. 	3.0	1.0	Convolutional Neural Networks (CNN) is used. 	All the details are provided in the paper, at https://huggingface.co/datasets/deeeeeeeeee/PUNCH2_data.,  https://github.com/deemeng/punch2 and https://github.com/deemeng/punch2_light	Each amino acid was encoded to a an embedding using the protein Language Model. Additionally, an amino acid is converted to One-Hot encoding of 21 dimension (1 dimension for rare AAs ).	"One-Hot = 21
MSA-prob = 22 
MSA-probAA = 22 
MSA-prob-numTemp = 23
MSA-probAAnumTemp = 23 
ESM-2 = 1280 
MSA = Transformer 768 
ProtTrans = 1024 "		The model used embeddings from ProtTrans(ProtT5) protein Language Models. 	The training process was structured into two phases to develop robust predictors for intrinsically disordered regions (IDRs). Phase 1 focused on identifying the best combinations of embedding methods and network architectures, referred to as ‚Äúbest singles‚Äù. Phase 2 involved incremental improvements and the creation of ensemble predictors. The outcomes of Phase 1 include several optimal model solutions (single predictors), while Phase 2 yielded the final predictors, PUNCH2 and PUNCH2-light. These predictors were trained on larger datasets and benchmarked on CAID2, CAID3, and CAID1 datasets for validation.		6.0	2.0	66c495857089c469b477b4ce	40138319.0	PMC11940444	02/02/2026 19:46:52	Meng D, Pollastri G.	PloS one	PUNCH2: Explore the strategy for intrinsically disordered protein predictor.	10.1371/journal.pone.0319208	2025	0.0	0.0	Critical Assessment of Protein Intrinsic Disorder, CAID	1.0	2025-04-19T09:28:20.459Z	2026-02-02T19:46:52.000Z	265aab6b-3e48-4b35-ac2d-21aea306667d	undefined	zeetwgvare				DOME_JSON	Match	Match
6803a35cea60466a7ca5a83f	All details are provided in the supplementary materials.	"For disorder prediction, the first database used was DisProt v5.0 (228 proteins). The other dataset was a 90% redundancy reduced subset of high resolution X-ray structures chains from PDB derived from PISCES. Chains shorter than 25 amino acids were discarded. Missing residues, including those with occupancy equal to zero, were treated as disordered.

For binding prediction in the disordered regions, A set of 840 peptides, with lengths between 5 and 25 amino acids and solved in complex with globular protein domains, was initially obtained from a previous study (Disfani et al., 2012). They discarded 196 peptides that couldn‚Äôt be mapped onto UniProtKB (UniProt,2014) sequences with SIFTS (Velankar et al., 2013)‚Äîbecause they were synthetic constructs, or the PDB files had been superseded, or the ATOM records mapped onto discontinuous fragments. They also removed 247 chains sharing 30% or more sequence identity to other regions, based on the BLASTP alignments generated with the recommended settings for short peptides (-seg no -matrix PAM30 -gapopen 10 -gapextend 1 -word_size 3). The positive training set was therefore made up of 5501 amino acids from 397 regions occurring in as many PDB chains (372 UniprotKB sequences). Just 104 of these UniProtKB chains were also used as positive examples for the new disordered residue prediction modules. Negative training examples were obtained from unbound protein domain linker regions in known protein structures. A total of 1164 linkers annotated in CATH v.3.5 . The negative training set consisted of 4930 amino acidsfrom 373 protein domain linkers occurring in 322 PDB chains (297UniprotKB sequences)."	data was redundancy reduced at 90%.	"They built an independent benchmark set. They mined database annotations and scientific reports to collate 29 protein chains, which have been investigated using biophysical techniques and have been shown to be disordered in isolation and to fold upon protein binding. These sequences include 4077 disordered residues forming 36 regions, within which 37 protein-binding sites occur
spanning between 5 and 47 positions and comprising a total of 708 amino acids."	4.0	0.0	The evaluation results are available in the paper and in the supplementary materials.	The method was compared to DISOPRED2, MoRFpred, MFSPSSMpred, and ANCHOR		Sensitivity, specifity, precision, MCC and f1 score is used, which are the common metrics used in CAID. 	"They built an independent benchmark set. They mined database annotations and scientific reports to collate 29 protein chains, which have been investigated using biophysical techniques and have been shown to be disordered in isolation and to fold upon protein binding. These sequences include 4077 disordered residues forming 36 regions, within which 37 protein-binding sites occur
spanning between 5 and 47 positions and comprising a total of 708 amino acids."	4.0	1.0	software is available at https://github.com/psipred/disopred		The model is a combination of SVM and neural networks, with explainability approaches like SHAP analysis etc. could be interpretable.	The model outputs 3 states for disorder, binding and order, so it is a classification task. 	3.0	1.0	The model uses neural networks and support vector machines. 		Position-specific scoring matrix (PSSM) scores were calculated for each residue using three iterations of PSI-BLAST  running on the UniRef90 data bank (Suzek et al., 2007) with an inclusion E-value threshold of h = 0.001.	Features are obtained from PSSM matrix.		No the model doesn't use meta predictions. 		Cross validation was used. 	5.0	3.0	66c495857089c469b477b4ce	25391399.0	PMC4380029	02/02/2026 19:46:52	Jones DT, Cozzetto D.	Bioinformatics (Oxford, England)	DISOPRED3: precise disordered region predictions with annotated protein-binding activity.	10.1093/bioinformatics/btu744	2014	0.0	0.0	Critical Assessment of Protein Intrinsic Disorder, CAID	1.0	2025-04-19T13:21:32.356Z	2026-02-02T19:46:52.000Z	e98cb220-1191-4250-8c34-c320506c74e4	undefined	mg14nkwvpa				DOME_JSON	Match	Match
67fe0e11822ee03b50b6f19a	All the data are available at http://bliulab.net/IDP-Fusion/benchmark/ . statistical information about data is availble in the additional file 1.	The training dataset included 614 LDR proteins, 3024 SDR proteins, and 616 fully ordered proteins, which is constructed based on a training set derived from DeepIDP-2L and RFPR-IDP (They are obtained from DisProt and MobiDB). 	In this study, five commonly used datasets with different ratios between SDRs and LDRs were used to evaluate the performance of different methods., including MXD494, SL329, DISORDER723, CASP, and Disprot504(https://disprot.org/). To further test the generalization of various methods, the MSDCD independent test dataset was constructed by combining these five datasets. IDP-Fusion was also evaluated on the CAID1.	They constructed 5 validation sets with different ratios of SDR and LDR proteins. 	4.0	0.0		It was compared to most of the methods that participated in CAID. Details are provided in the additional file1.		All the common metrics that are established in CAID are reported. 	The method was tested on various independent datasets. 	3.0	2.0	A webserver is available at http://bliulab.net/IDP-Fusion/benchmark/	Runtime of a single protein is not available, but they compare the runtime with SPOT-Disorder2 on a dataset.	The final model combines 6 neural network models, so it could be treated as a blackbox.	The individual models output continuous values that are then turned into binary labels.  	4.0	0.0	IDP-Fusion fused six base methods to stably predict both the SDRs and LDRs . Among the six base methods, five models were derived from the field of natural language processing, including CAN , HAN , IDP-Seq2Seq , CNN-LSTM , and LSTM-CNN . CAN used the convolutional attention network to obtain the discrete distribution patterns of SDRs in protein sequences. HAN employed the hierarchical attention model to capture the sequence location of LDRs mainly located in the N‚Äô and C‚Äô of the sequences. IDP-Seq2Seq combined Seq2Seq and attention mechanism to capture the global and non-local correlation features of residues in IDRs. Convolutional neural networks (CNN) was used to extract local features of IDRs, and long short-term memory (LSTM) was used to extract global features of IDRs. The CNN and LSTM were combined to obtain both the local features and global features of IDRs. Two models CNN-LSTM and LSTM-CNN were constructed inspired by SPOT-Disorder2. They employed the neural architecture search (NAS) model DARTS to capture the hidden features of protein sequences		Three types of features were combined into IDP-Fusion, including residue-profile features, evolutionary features, and structural features. Residue-profile features included seven commonly used amino acid physic-chemical properties. Evolutionary-level features included position-specific frequency matrix (PSFM) and position-specific scoring matrix (PSSM).  The PSSM, PSFM, and HMM features are 20-dimensional features. Structural-level features included 8-dimensional secondary structure (SS), 2-dimensional CN, and 4-dimensional HSE predicted by using SPIDER2 software tool , 21-dimensonal predicted residue-residue contacts (CCMs) predicted by using CCMpred software tool, and 1-dimensional solvent accessibility (SA) predicted by using the Sable Version 2 software tool.	Three types of features were combined into IDP-Fusion, including residue-profile features, evolutionary features, and structural features. Details of the number of gathered features are available in the paper. 		They don't use meta predictions.			4.0	4.0	66c495857089c469b477b4ce	37674132.0	PMC10483879	02/02/2026 19:46:52	Tang YJ, Yan K, Zhang X, Tian Y, Liu B.	BMC biology	Protein intrinsically disordered region prediction by combining neural architecture search and multi-objective genetic algorithm.	10.1186/s12915-023-01672-5	2023	0.0	0.0	Critical Assessment of Protein Intrinsic Disorder, CAID	1.0	2025-04-15T07:43:13.774Z	2026-02-02T19:46:52.000Z	63cedb35-e85b-406f-b0e5-ef87c0305aa0	undefined	6ht0zcp9ox				DOME_JSON	Match	Match
692eb780def4ff8a77671701	All training images and corresponding ground truths (binary masks) have been uploaded to the Gigadb private dropbox.	Dataset comprised of root images taken by myself. The total training dataset consisted of 83 images of varying dimensions, containing images from multiple plant species: Rice, Wheat, Arabidopsis, Maize, Medicago, Brachypodium. 	Training sets were independent from the test sets. The model was trained multiple times, where upon discovering images that the model poorly segments, I then manually  curated binary masks for those images, and re-trained the model. Those test images would subsequently become part of the training set.	83 images in the training split, and all images were used in validation. The test set comprised of > 200 images, from species including wheat, oat, tomato. For all the datasets described in the paper, the predicted output of the segmentation model was visually inspected for all images. An example of the model performance relative to manually annotated binary masks can be seen in Supp Figure 3 in the manuscript.	4.0	0.0				nnUNet uses mean dice score during training to compute accuracy. I manually calculated IOU scores for a select group of testing images with manually annotated binary masks.	Evaluated based on visual inspection of output binary masks.	5.0	0.0	Model was trained according to instructions on nnUNet's github. Model is available on huggingface: https://huggingface.co/iantsang779/pyroothair_v1	~ 7 seconds per input image of ~ 2600 x 1500 x 3 pixels	Model is interpretable. Model outputs segmentations of input images, which can be easily viewed and compared with the original image to determine segmentation performance.	Classification (pixel wise)	4.0	0.0	Convolutional Neural Network	Model plans available on huggingface: https://huggingface.co/iantsang779/pyroothair_v1/blob/main/plans.json	Images were z-score normalized across all 3 image channels and augmentation was automatically carried out. Full details are available in the nnUNet paper describing the image preprocessing steps: https://doi.org/10.1038/s41592-020-01008-z	"7 convolutional layers with the follwing number of feature maps: 32,64,128,256,512,512,512
                   "		No			7.0	1.0	687908e2e5963d3a2411916b	41231584.0	PMC12824728	02/02/2026 19:46:52	Tsang I, Percival-Alwyn L, Rawsthorne S, Cockram J, Leigh F, Atkinson JA.	GigaScience	pyRootHair: Machine learning accelerated software for high-throughput phenotyping of plant root hair traits.	10.1093/gigascience/giaf141	2025	0.0	0.0	Software, Plants, Phenotyping, Computer Vision	1.0	2025-12-02T09:55:12.881Z	2026-02-02T19:46:52.000Z	f0d0cab8-5bda-4c7c-93ab-dcc8add5ec79	undefined	cc6r385psy				DOME_JSON	Match	Match
682dd55bec6931461ae8b33f	The names of the datasets are shared, including the citations. There is no information on the data splits.	"~4.8 million chemical compounds from PubChem, containing molecular images for image captioning tasks
the standard ChEBI-20 dataset, consisting of 33,010 molecule-description pairs, for fine-tuning and evaluation"	Using `random_scaffold_split`.	frac_train=0.8, frac_valid=0.1, frac_test=0.1	4.0	0.0			no			0.0	5.0	The source code is released at https://github.com/AI-HPC-Research-Team/GIT-Mol/				1.0	3.0	multi-modal large language model with graph, image, and text		graph, text, image encoders with MolT5 and the Swin Transformer						2.0	6.0	682dd55bb99adbaaf1147f5d	38359660.0		02/02/2026 19:46:52	Liu P, Ren Y, Tao J, Ren Z.	Computers in biology and medicine	GIT-Mol: A multi-modal large language model for molecular science with graph, image, and text.	10.1016/j.compbiomed.2024.108073	2024	0.0	0.0	AI-summer-school	1.0	2025-05-21T13:30:03.205Z	2026-02-02T19:46:52.000Z	07d9f7da-e0e1-4695-8151-12c1573f4aa0	undefined	lgfn7xigc7				DOME_JSON	Match	Match
63516fedb9c880af1f305b3b	Data available on request from the corresponding author.	Specimens collected at the Royal London Hospital, University College London Hospital, Liverpool University and the CNIO Madrid, Spain, plus further samples obtained from Pancreas Tissue Bank (https://www. bartspancreastissuebank.org.uk).  N_pos = 199 ( pancreatic ductal adenocarcinoma (PDAC) patients), N_neg = 180 (healthy patients). No previously used.	Not applicable.	Random division in a 1:1 ratio for train and test.   N_pos_train = 96, N_neg_train = 95,  N_pos_test = 103, N_neg_test = 85.    50.3% positives on training set.   54.8% positives on test set.			No.	Logistic regression was compared to neural network (NN), neuro-fuzzy technology (NF), random forest (RF) and support vector machine (SVM).  None of those additional approaches significantly outperformed logistic regression. 	Inference for the ROC curves was based on cluster-robust standard errors that accounted for the serially correlated nature of the samples. It was not possible to create ROC curves and therefore AUC for RF and SVM since the outcome was not continuous. McNemar‚Äôs exact test was used to assess the significance of difference in SN at fixed SP and DeLong‚Äôs test was used to assess the significance of differences in AUC between approaches. Confidence intervals (CI 95%) for AUCs were derived based on the DeLong‚Äôs method to evaluate the uncertainty of an AUC; SN and SP 95% CI were derived using bootstrap replicates. To allow for multiple testing, both types of tests were adjusted using the Bonferroni correction. 	AUROC (except for RF and SVM) and sensitivity at clinically relevant specificity.	Independent dataset.			Logistic regression: The ‚Äúglmnet‚Äù package from R was used with elastic net regularisation.    NN: Python packages tensorflow, keras, and scikit-learn.  RF: The ‚Äúparty‚Äù package from R.   SVM:  The ‚ÄúsvmLinear‚Äù method from the ‚Äúcaret‚Äù package in R was used.  NF: The r-algorithm developed by Shor was used with a precision Œµ = 0.001. Software implementation of this approach was developed within the Visual Studio 2013 environment.	Not reported.	Logistic Regression is generally considered transparent.  In a previous work (PMID: 26240291) the authors examined the good predicting power of each of the 3 urine biomarkers (LYVE1, REG1A, and TFF1) taken separately.   However, an explaination of the joint effect of the variables has not been attempted.	Binary classification (high PDAC risk or not).			Logistic regression.	No.	Not reported.	Only 5 predictors available, no selection applied in this work.  The 5 predictors were the result of a selection applied in a previous work (PMID: 21136905), by quantifying differentially expressed proteins in urine proteomes.	Bootstrap cross-validation was used for the internal validation to ensure that overfitting is avoided. Following that, elastic net was used for the regularisation of the coefficients to obtain the final model.  	No.	The authors do not mention parameters numbers different from standard. 	Yes, use of elastic net.			65faa4f792c76639b82bab29	31857725.0	PMC7054390	02/02/2026 19:46:52	Blyuss O, Zaikin A, Cherepanova V, Munblit D, Kiseleva EM, Prytomanova OM, Duffy SW, Crnogorac-Jurcevic T.	British journal of cancer	Development of PancRISK, a urine biomarker-based risk score for stratified screening of pancreatic cancer patients.	10.1038/s41416-019-0694-0	2020	0.0	0.0		1.0	2022-05-20T16:23:02.000Z	2026-02-02T19:46:52.000Z	b879f61f-b41d-4e82-ad6e-fd33b4e669f1	undefined	ku9smf88dv				Starting_TSV	Match	No Match
63516fedb9c880af1f305bad	Yes.    Raw and processed microarray data used in this study is available via Gene Expression Omnibus at: http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?token=bviftkociimgsnk&acc=GSE20098	CD4 T-cell transcriptomes from 173 UK patients, of which N_pos = 72  (RA, i.e. outcome Rheumatoid Arthritis), N_neg =101 (outcome Non-RA).  Not used in previous papers.	Not applicable.	111 patients in training set, of which N_pos_train = 47  (RA), N_neg_train = 64 (Non-RA).    62 patients in testing set, of which N_pos_test = 25  (RA), N_neg_test = 37 (Non-RA).   42.3% positives on training set.   40.3% positives on test set.			No.	A transcriptional ‚Äòrisk metrics‚Äô for ACPA-negative patients was bild, and the AUROC curve of the 12-gene signature was compared to the existing ‚ÄòLeiden prediction rule‚Äô as a predictor of RA in the test set.   No difference in the performance was seen in this case.  However, by combining all features of the Leiden prediction rule with the 12-gene risk metric, and applying it to the ACPA-negative UA cohort (test set), the AUROC curve value improved from 0.74, SEM=0.08 (original Leiden prediction rule) to 0.84; SEM=0.06 (modified metric incorporating gene signature).                   	AUROC curve (original Leiden prediction rule)=0.74; SEM=0.08 versus area under ROC curve (modified metric incorporating gene signature)=0.84; SEM=0.06; p<0.001 in both cases.	Sensitivity, specificity, positive and negative likelihood ratio.	Test set (independent dataset).			Standard algorithms are used.	Not reported.	No statement about ante-hoc interpretability of the model is made by the authors.   SVM is generally considered black box.   Post-hoc analysis indicates that PB CD4 T cells in early RA are characterised by a predominant upregulation of biological pathways involved in cell cycle progression (ACPA-positive) and survival, death and apoptosis (ACPA-negative).  ( ACPA = anti-citrullinated peptide antibodies)	Binary prediction:  RA or non-RA.			SVM	No.	Not reported.	f = 12.   Starting from 16,205 genes, 12 genes were at the end selected as differentially expressed in RA versus non-RA patients.  Differential expression was defined as a fold-change cut-off of 1.2, combined with a significance level cut-off of p<0.05 (Welch‚Äôs t-test), corrected for multiple testing using the false-discovery-rate method of Benjamini.	Not applicable.	No.	 The authors do not mention parameters numbers different from standard.  	Not applicable.			65faa4f792c76639b82bab29	22532634.0	PMC3396452	02/02/2026 19:46:52	Pratt AG, Swan DC, Richardson S, Wilson G, Hilkens CM, Young DA, Isaacs JD.	Annals of the rheumatic diseases	A CD4 T cell gene signature for early rheumatoid arthritis implicates interleukin 6-mediated STAT3 signalling, particularly in anti-citrullinated peptide antibody-negative disease.	10.1136/annrheumdis-2011-200968	2012	0.0	0.0		1.0	2022-05-20T15:57:36.000Z	2026-02-02T19:46:52.000Z	c3ffe65b-e1f7-4326-a75c-44891ea42eb2	undefined	y6rde1aew5				Starting_TSV	Match	No Match
63516fedb9c880af1f305b3c	According to the author's statement, the data are not available because of patients‚Äô privacy.	Clinical data from 4 different hospitals in China (4 different cities) about piatients with chronic B virus hepatitis. N = 1289 (patients from 4 different cities).  N_pos (fibrosis) = 815, N_neg (fibrosis) = 474 ;    N_pos (cirrhosis) = 290, N_neg (cirrhosis) = 999.   Data not used by previous authors.	Not applicable.	Training set:   N = 549  (patients from 2 different cities, joined together).  N_pos (fibrosis) = 382 / N_neg (fibrosis) = 167 ;    N_pos (cirrhosis) = 157 // N_neg (cirrhosis) = 392.   Test set Anhui:   N = 408 (independent patients dataset from Anhui city).  N_pos (fibrosis) = 254 // N_neg (fibrosis) = 154 ;    N_pos (cirrhosis) = 59 // N_neg (cirrhosis) = 359.     Test set Beijing:   N = 332 (independent patients dataset from Beijing city).  N_pos (fibrosis) = 179 // N_neg (fibrosis) = 153 ;    N_pos (cirrhosis) = 74 // N_neg (cirrhosis) = 258.        69.6% positives (fibrosis), 28.6% positives (cirrhosis) on training set.   62.3% positives (fibrosis), 14.5% positives (cirrhosis) on Anhui test set.  53.9% positives (fibrosis), 22.3% positives (cirrhosis) on Beijing test set.  			No.	FibroBox was compared to 3 other pre-existing predicting methods: TE (Transient Elastography), APRI (Aspartate transaminase-to-platelet ratio index), FIB-4 (fibrosis-4 index), the letter two being serum biomarkers. The LightGBM algorithm was coompared to Logistic regression and XGBoost. 	Confidence intervals at 95% for AUROC, statistical significance confirmed for the difference between FibroBox and each of TE, APRI, FIB-4 methods, both for fibrosis and for cyrrhosis predictions.	Precision, recall, F1-score, accuracy,  AUROC.  The latter was used for feature selection, and for comparison with other models. 	5-fold cross-validation during training-testing.  Two independent datasets were used as evaluation sets:  the Anhui cohort (n = 408), and the Beijing cohort (n=332).			The ML algorithms were implemented using Python 3.7.	The run time of FibroBox is mentioned in the Discussion to be only a few seconds.	No statement about ante-hoc interpretability of the model is made by the authors.   This LightGBM model looks to me complex enough to result rather black box.   Post-hoc analysis showed e.g. that the contribute of the Transient Elastography (TE) feature to the good predictability was very relevant, which is meaningful.  An explaination for an eventual joint effect of the 9 features was not adressed.	Classification.  Case A: positive and negative samples are non-significant fibrosis vs. significant fibrosis. Case B: positive and negative samples are non-cirrhosis vs. cirrhosis.			LightGBM (a gradient boosting framework using tree based learning algorithms) is stated in the Supplementary to have been the algorithm of choice.	No.	For character data field (like sex, pathogenesis), the LabelEncoder method in Python was used (categorical features encoded as a one-hot numeric array).	Starting from 24 features (clinical measurements commons to the 4 hospitals), 9 features were selected for training, by means of LASSO regression and filter methods (the latter being variance threshold, Pearson Correlation Coefficient, chi-square test and mutual information).	The LightGBM algorithm is stated in its documentation likely to be over-fitting if not used with the appropriate parameters. 	No.	The number of parameters was the standard for Python implementation of LightGBM.  Parameters were tuned to get good results by means of grid search, random search and the Python library Hyperopt.	"Yes.  The authors state that, ""to prevent overfitting, the index of colsample_bytree was set to 0.9"".    Other parameters tuning for over-fitting prevention is not mentioned. "			65faa4f792c76639b82bab29	33005419.0	PMC7520974	02/02/2026 19:46:52	Lu XJ, Yang XJ, Sun JY, Zhang X, Yuan ZX, Li XH.	Biomarker research	FibroBox: a novel noninvasive tool for predicting significant liver fibrosis and cirrhosis in HBV infected patients.	10.1186/s40364-020-00215-2	2020	0.0	0.0		1.0	2022-05-20T15:26:30.000Z	2026-02-02T19:46:52.000Z	74f23aa2-985c-49d2-9a0d-0d7b3e0f4bda	undefined	p00ybazkxy				Starting_TSV	Match	No Match
664b2a4db30933003cc21847	Yes. Supporting Information.	Proteomes of urine samples from 54 T2D (Type 2 Diabetes) patients from Pusan National University Hospital, South Korea. N_pos = 19 (poor prognosis group (PPG) due to DKD (Diabetic Kidney Disease), N_neg = 35 (good prognosis group (GPG), i.e. no DKD).   Not used in previous papers.	Not applicable	SVM model with linear kernel was generated by a 10 fold repeated three-fold cross validation.  The RF model was generated by a 3-fold cross validation method repeated 100 times.			No.	The performances of the SVM and RF models were compared to the predicting performance of the albumin-to-creatinine ratio, a simple biomarker for DKD which has been widely used so far.	The authors state that the AUROC of the two classifiers (SVM and RF) differed significantly from albumin-to-creatinine ratio (likelihood ratio test: p-value < 0.05). However, for the RF AUROC (value = 1.0) no confidence intervals are reported, and the confidence intervals for SVM AUROC are not further specified.	AUROC.  Comparison of disease prediction scores.	Cross-validation.   Since the authors were unable to find a benchmarking study in the discovery of urine protein biomarkers, they evaluated the models with mRNA expression in the kidney.   The SVM and RF models consisting of 5 urine proteins were applied to 4 publicly available GEO datasets:  GSE99339, GSE47185, GSE30122, and GSE96804.  However, the predictions on such datasets were not statistically significant.			Data were analyzed using the publicly available RStudio (version 1.1.456) including R (version 3.6.0). 	Not reported.	No statement about ante-hoc interpretability of the models is made by the authors.   The RF and SVM are generally considered black box.   Post-hoc analysis resulted somewhat interpretable, since the 5 selected features correspond to 5 urinary proteins that are considered likely to be related to DKD.  However, their eventual joint effect remains not interpretable.	Binary classification (PPG or GPG).  The binary results of RF and SVM models were also transformed in disease prediction scores, which ranged from 0 to 1.			SVM and RF.	No.	Not reported.	The study focused on 412 proteins (of the 1296 identified proteins in each proteome) quantified in more than 80% of urine samples, with missing values filled by local least squares imputation.  From those 412 proteins, 5 proteins (ACP2, CTSA, GM2A, MUC1, and SPARCL1) were selected as significant by an AUC-based random forest method. 	The authors do not mention overfitting issues, although their multivariate AUROCs showed very high values.	No.	The authors do not mention parameters numbers different from standard.  They mention the value of some parameters, without mentioning how they were chosen. 	Not reported.			65faa4f792c76639b82bab29	32545899.0	PMC7352871	02/02/2026 19:46:52	Ahn HS, Kim JH, Jeong H, Yu J, Yeom J, Song SH, Kim SS, Kim IJ, Kim K.	International journal of molecular sciences	Differential Urinary Proteome Analysis for Predicting Prognosis in Type 2 Diabetes Patients with and without Renal Dysfunction.	10.3390/ijms21124236	2020	0.0	0.0		1.0	2022-05-20T15:07:04.000Z	2026-02-02T19:46:52.000Z	85fd4f8b-595f-4be3-8d57-2d351b44e1ce	undefined	9hqbg4dzys				Starting_TSV	Match	No Match
63a25db2e8edf6ce46f6e84b	Not explicitly stated	Provenance stated via references	Not applicable (it is a cluster approach)	Not applicable (it is a cluster approach)			Not explicitly stated	Across benchmarking datasets	p and q values	Variance	Benchmarking over 3 datasets with scATAC-seq data			https://github.com/zji90/SCATE and release used at https://doi.org/10.5281/zenodo.3711558	1-2 days. running SCATE to reconstruct regulome approximately takes 5 minutes per cell cluster on a computer with 10 computing cores (2.5 GHz CPU/core) and a total of 20GB RAM.	Not explicitly stated	Clustering			Statistical analysis and clustering (K-means)	Not explicitly stated	Trimming and segmentation over sequences	Not applicable	Not applicable	Not applicable	Clusters number treated as a tuning parameter. Cross-validation approach to choose optimal K (looks like it was 5000 clusters)	Not applicable			6312169df3794236aa987a13	32620137.0	PMC7333383	02/02/2026 19:46:52	Ji Z, Zhou W, Hou W, Ji H.	Genome biology	Single-cell ATAC-seq signal extraction and enhancement with SCATE.	10.1186/s13059-020-02075-3	2020	0.0	0.0		1.0	2022-04-18T16:44:10.000Z	2026-02-02T19:46:52.000Z	8c0c94cd-3172-4f65-92b0-b997742324e0	undefined	lrlwou3dt7				Starting_TSV	Match	No Match
63516fedb9c880af1f305bb4	PSMs at http://noble.gs.washington.edu/proj/intense (also stated that availability is upon request but the URL indeed has links to the data)	1208 peptide spectrum matches (PSMs) and 18149 mass spectra for validation	Not explicitly stated	Positive and negative points for the Bayesian network are mentioned but no further info provided			Not explicitly stated	SEQUEST, Percolator	q value, which is defined as the minimal false discovery rate threshold at which the PSM is deemed significant	Kind of discussed but did not see values	Comparison against SEQUEST (Riptide with the static SVM outperforms SEQUEST by 10.8% at a 1% false discovery rate) and Percolator			Upon request (but did not try to get it, there is a link to a tar file with, it says, C++ and Python for the Riptide part, I did not examine the files)	Not explicitly stated	Not explicitly stated	Classification			Hybrid dynamic Bayesian network (DBN) / support vector machine (SVM)	Not explicitly stated	Pre-processing: Input data obtained from MS/MS data, an aqueous soluble protein sample from E.coli lysate was reduced, carbamidomethylated and digested with trypsin.	99-dimensional feature vectors generated by Riptide (Bayesian part)	Not explicitly stated	Not explicitly stated	For the SVM, we use a Gaussian kernel, and hyperparameters C (soft margin penalty) and sigma (low case, width of the Gaussian). Hyperparameters are selected using five-fold nested cross-validation, where the parameter with the largest area under the ROC curve is selected.	Not explicitly stated			6312169df3794236aa987a13	18586734.0	PMC2665034	02/02/2026 19:46:52	Klammer AA, Reynolds SM, Bilmes JA, MacCoss MJ, Noble WS.	Bioinformatics (Oxford, England)	Modeling peptide fragmentation with dynamic Bayesian networks for peptide identification.	10.1093/bioinformatics/btn189	2008	0.0	0.0		1.0	2022-04-18T16:37:38.000Z	2026-02-02T19:46:52.000Z	8067befa-adce-4b5a-9da3-d16a3566a794	undefined	36d464edpz				Starting_TSV	Match	No Match
63516fedb9c880af1f305b3f	Not explicitly stated	311 patients chosen out of 445 available records. 	Not explicitly stated	Split 80:20. Random splitting of the data resulted in 249 cases (150 pCR, 99 no pCR) in the training set and 62 cases (38 pCR, 24 no pCR) in the test set.			Not explicitly stated	Comparison of different configurations. Spearman correlation	p-value	Percentile, specificity, sensitivity, positive predict value, negative predict value, accuracy	AUROC			Not explicitly stated	Not explicitly stated	Not explicitly stated	Classification (pCR or no pCR). HER2 expression levels wrt IHC and FISH			Coarse decision trees and five-fold cross-validation.	Not explicitly stated	3D segmentation on MRI exams, enhancement maps calculated. Data reduced to fixed bin number of 16 greys levels. Combat harmonisation to reduce centre effect	Features computed from 2D directional matrix and average over 2D directions and slices. 	Not explicitly stated	Not explicitly stated	102 texture parameters in 6 categories	Not explicitly stated			6312169df3794236aa987a13	33039708.0	PMC7648120	02/02/2026 19:46:52	Bitencourt AGV, Gibbs P, Rossi Saccarelli C, Daimiel I, Lo Gullo R, Fox MJ, Thakur S, Pinker K, Morris EA, Morrow M, Jochelson MS.	EBioMedicine	MRI-based machine learning radiomics can predict HER2 expression level and pathologic response after neoadjuvant therapy in HER2 overexpressing breast cancer.	10.1016/j.ebiom.2020.103042	2020	0.0	0.0		1.0	2022-04-18T16:33:16.000Z	2026-02-02T19:46:52.000Z	21016d93-6e18-4e47-83ec-458e3a565ffe	undefined	3ggc5le2qz				Starting_TSV	Match	No Match
63516fedb9c880af1f305b5e	The database, hosting the dataset, is accessible via registration request:    http://www.bio.unisi.it/aku-db/	Source:  AKU-related dataset, 203 patients in total.  Not used previously.	Not reported	Training set: 181 patients, Validation set: 22 patients			No	Not applicable	Significance of the model by F statistics. 	Adjusted coefficient of determination, Mean Standard Error, Maximum Absolute Difference Predicted vs Experimental	Independent validation set.			Not available	Not reported	Transparent.  All 5 features are clinical parameters known to be related to the AKU illness.	Regression: prediction of patients PTI (Protein Thiolation Index) values, given the patients clinical information.			Multiple Linear Regression (MLR) 	Coefficients Estimates and Standard Errors of the predictors are reported.	Global features	The 110 numeric fields included in the platform were considered as the possible predictors with a backward elimination method. From the screening of every model, the authors selected the most significant ones (5 predictors) according to the F statistics of the ChiSquare coefficients.  From the text it appears that the feature elimination procedure was applied on the whole dataset.  	Apparently both over-fitting and under-fitting can be excluded (p<N, f =5).  	No	7 MLR parameters:  5 feature parameters, 1 intercept, 1 error term.	Not applicable.			6312169df3794236aa9879e1	31462106.0	PMC6902683	02/02/2026 19:46:52	Cicaloni V, Spiga O, Dimitri GM, Maiocchi R, Millucci L, Giustarini D, Bernardini G, Bernini A, Marzocchi B, Braconi D, Santucci A.	FASEB journal : official publication of the Federation of American Societies for Experimental Biology	Interactive alkaptonuria database: investigating clinical data to improve patient care in a rare disease.	10.1096/fj.201901529r	2019	0.0	0.0		1.0	2022-04-06T18:58:39.000Z	2026-02-02T19:46:52.000Z	10e71e8f-f4e5-4904-9a7b-9428370d1e99	undefined	2t5i3s7g3y				Starting_TSV	Match	No Match
63516fedb9c880af1f305b1e	Yes (GEO: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE152075)	Data source:  Gene expression profiles from NCBI/GEO GSE152075.  Data points: 484 individuals.  N_pos (swabs Covid positive)= 430, N_neg (swabs Covid negative)= 34.  Used by at least one previous paper (PMID: 32898168).   	Not reported	Due to sample imbalance, the python package imblearn was used to amplify the number of small samples to the same as that of large samples.  			No	No	Not assessed	MCC	Leave-One-Out Cross-validation			No	Not reported	Black box. PCA and GO-Term enrichment analysis on 66 selected genes shows an association with ribosomal protein-encoding, viral protein translation, and protein-membrane location.	Classification (Covid swab positive or negative)			Support vector machine (SVM) 	No	Global features. Gene Expression Profile	The optimized method uses expression from 66 selected genes. The expression profile genes (genes nr = 16032) were first ranked by feature importance (gene expression) via the minimum redundancy maximum relevancy (mRMR) method, getting down to 500 genes. Subsequently, a SVM classifier was used to screen the optimal feature genes by the incremental feature selection (IFS) method. Optimal genes were selected maximizing the the MCC obtained with a Leave-One-Out Cross-Validation procedure on training set.	Not reported.   The number of features were reduced to 66 by the feature selection algorithms, which however could have induce over-fitting by themselves.	No	Not reported	No. Not reported how the regularization parameter in SVM was tuned.			6312169df3794236aa9879e1	34603483.0	PMC8485143	02/02/2026 19:46:52	Zhang S, Qu R, Wang P, Wang S.	Computational and mathematical methods in medicine	Identification of Novel COVID-19 Biomarkers by Multiple Feature Selection Strategies.	10.1155/2021/2203636	2021	0.0	0.0		1.0	2022-04-06T17:27:29.000Z	2026-02-02T19:46:52.000Z	79228609-a725-4ad4-a390-21d4cfe00670	undefined	rggiypqqgz				Starting_TSV	Match	No Match
63516fedb9c880af1f305b40	Raw data were deposited into the Gene Expression Omnibus (GSE86474, https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE86474).	miRNA samples were extracted from prostate cancer patients cohorts, and profiling obtained.  The dataset was composed of N_pos = 61, N_neg = 78. Data not used previously.    An additional 'discovery' cohort was composed of 10 patients (all negatives) .  (Positive: high-risk patients, i.e. GS (Gleason grade) > 7. low-risk patients (GS = 6).	Not reported	Training set: N_pos = 50, N_neg = 49.  Testing set: N_pos = 11, N_neg = 29.			No	No	Confidence interval shows overlap between the performance on training and validation sets	AUC	cross-validation on training set and validation on an independent set			Not available	Not reported	Black box. No information about the optimized parameters were reported.	Binary classifier (high- and low-risk groups)			Random forest	No	Global features	Intra-stable miRNAs (fourth quartile of ICCs; Q100) from the discovery cohort were used as a set of input features. Feature selection protocol identified the expression of 7 miRNA which maximize the performance of the method tested in cross-validation on the training set.	Number of features ~ N_pos + N_neg and Number of parameters > N_pos + N_neg	No	Random forest (mtry and ntree) optimized by grid search using 5-fold cross-validation with 10 randomized replicates. Predictive model built with randomForestSRC (v2.4.2) for R.	Yes. miRNA normalization with NanoStringNorm (v1.1.20).			6312169df3794236aa9879e1	31161221.0	PMC7073919	02/02/2026 19:46:52	Jeon J, Olkhov-Mitsel E, Xie H, Yao CQ, Zhao F, Jahangiri S, Cuizon C, Scarcello S, Jeyapala R, Watson JD, Fraser M, Ray J, Commisso K, Loblaw A, Fleshner NE, Bristow RG, Downes M, Vesprini D, Liu S, Bapat B, Boutros PC.	Journal of the National Cancer Institute	Temporal Stability and Prognostic Biomarker Potential of the Prostate Cancer Urine miRNA Transcriptome.	10.1093/jnci/djz112	2020	0.0	0.0		1.0	2022-04-06T16:25:21.000Z	2026-02-02T19:46:52.000Z	d6d6562d-8254-45bc-88b2-6a88827e9cf8	undefined	mewdwg9y4z				Starting_TSV	Match	No Match
63516fedb9c880af1f305bb2	No	Source: Pinus taeda resequencing data, not further specified.  Training set is composed of a total of 300 validated sequences.  Test set is composed of 120 independent sequences, with 563 manually validated SNPs.	Not reported	 Testing: 120 Sequences 563 manually validated SNPs 			No	Polyphred, Polybayes. Used for generating the features.	Not reported	Accuracy, sensitivity, specificity	Independent dataset of 120 unique sequences with 563 manually validated SNPs. Validation = All SNP calls were identified as based on visual inspection of Polyphred and Polybayes predictions in Consed.			Broken link (http://dendrome.ucdavis.edu/adept2/ resequencing.html). The customized pipeline for feature extraction is reported in a new link:   https://nealelab.ucdavis.edu/adept2-overview/pinesap/	Not reported	Input features are transparent (Sequence Depth, Local Average Quality, Alignment Quality) while their combination is not interpretable (Black box).	Binary classifier (SNP predictions accepted or rejected).			J48 Algorithm For Decision Tree (WEKA)   Decision tree J48 is the implementation of algorithm ID3 (Iterative Dichotomiser 3) developed by the WEKA project team. 	No	Global features	Sequence-based statistics were derived through a customized feature extraction program and fed as a vector for each polymorphism to the J48 classification tree available in the WEKA classifier package.  9 features were used to enhance polymorphism prediction accuracy (as reported in https://nealelab.ucdavis.edu/adept2-overview/pinesap/).	Not reported	No	Not reported	No			6312169df3794236aa9879e1	19667082.0	PMC2752621	02/02/2026 19:46:52	Wegrzyn JL, Lee JM, Liechty J, Neale DB.	Bioinformatics (Oxford, England)	PineSAP--sequence alignment and SNP identification pipeline.	10.1093/bioinformatics/btp477	2009	0.0	0.0		1.0	2022-04-06T03:38:12.000Z	2026-02-02T19:46:52.000Z	98d8027e-41f2-4b54-8fd6-20c9b51e7bdf	undefined	w5mge5bmyl				Starting_TSV	Match	No Match
63516fedb9c880af1f305b5f	?	 distinguish 32 patients from 30 healthy control	Independance	 distinguish 32 patients from 30 healthy control			No	No	No	ROC curve	Cross validation			No	No	Black box	Binary classification			SVM	No	Yes, e.g. voxel resolution	Yes	No	No	Yes	No			6312169df3794236aa987a36	31148311.0	PMC6679781	02/02/2026 19:46:52	Jing R, Li P, Ding Z, Lin X, Zhao R, Shi L, Yan H, Liao J, Zhuo C, Lu L, Fan Y.	Human brain mapping	Machine learning identifies unaffected first-degree relatives with functional network patterns and cognitive impairment similar to those of schizophrenia patients.	10.1002/hbm.24678	2019	0.0	0.0		1.0	2022-03-30T16:46:29.000Z	2026-02-02T19:46:52.000Z	55301cf1-e2b8-4e59-86d5-6502872104d6	undefined	ftglxzast4				Starting_TSV	Match	No Match
63516fedb9c880af1f305b64	https://github.com/b2slab/genedise	Open Targets platform,  at least 1,000 Open Targets associations. 22 diseases were considered	none	genetic association with disease. Based on given score some  associations were considered positive.			no	network topology, basic GBA approach	The rankings produced by the different algorithms were qualitatively compared using Spearman‚Äôs footrule	20 hits, AUPRC, AUROC 	Cross-validation			https://github.com/b2slab/genedise	not mentioned	black box	regression: ranking of genes in terms of their association scores to the disease			Diffusion (propagation) methods, Purer ML-based methods and naive baseline methods	no	Associations were binarised	22	none	no	not mentioned	none			6312169df3794236aa987a1b	31479437.0	PMC6743778	02/02/2026 19:46:52	Picart-Armada S, Barrett SJ, Will√© DR, Perera-Lluna A, Gutteridge A, Dessailly BH.	PLoS computational biology	Benchmarking network propagation methods for disease gene identification.	10.1371/journal.pcbi.1007276	2019	0.0	0.0		1.0	2022-03-28T12:30:56.000Z	2026-02-02T19:46:52.000Z	02cbd7a6-f8b7-425b-8d94-ec6d79a8aebc	undefined	5a9ger6xi3				Starting_TSV	Match	No Match
63516fedb9c880af1f305b67	Yes, online under the following accession numbers: for metabolomics data, MSV000081482 (Ldlr knockout animal) at ftp://massive.ucsd.edu/MSV000081482, MSV000082813 (ApoE knockout animal) at ftp://massive.ucsd.edu/MSV000082813, and MSV000081853 (commercial standards) at ftp://massive.ucsd.edu/MSV000081853, and for microbiome data, ERP106495 (Ldlr knockout animals; EBI database) and ERP110592 (ApoE knockout animals).	16S rRNA sequencing and untargeted liquid chromatography-tandem mass spectrometry (LC-MS/MS) data from fecal samples of atherosclerosis-prone, 10-week-old, male mice on a C57BL/6J background. 24 knockout mice for ApoE (ApoE-/-) and 16 knockout mice for Ldlr (Ldlr-/-). Fecal samples were collected at baseline and twice each week. 6 weeks, 12 time points and 192 samples, for Ldlr-/- mice. 10 weeks, 20 time points and 480 samples, for ApoE-/- mice. 	Analytical standards for bile acids of interest were used with the same LC-MS/MS to ensure feature annotation. Data set stratification by genotypes and effect size calculation of each of the covariates within each mouse model to untangle the effect of genotype. 	1) Npos=96 samples for 8 Ldlr-/- mice exposed to intermittent hypoxia and hypercapnia (IHH). Nneg=96 samples for 8 Ldlr-/- mice exposed to air. Ntrain=192 for Ldlr-/- mice. Ntest=480 for ApoE-/- mice. 2) Npos=12 ApoE-/- mice exposed to intermittent hypoxia and hypercapnia (IHH). Nneg=12 ApoE-/- mice exposed to air. Ntrain=480 for ApoE-/- mice. Ntest=192 for Ldlr-/- mice. 			Yes. Supplementary information.	Not specified	Not specified	ROC curves, AUC score.	Cross-validation. Details not specified.			Yes, in Jupyter notebooks available on GitHub (https://github.com/knightlab-analyses/crossmodel_prediction).	Not specified	Transparent. The abundance of each feature, 16S sequence and metabolite, was used as the score to plot the ROC curve and compute the AUC score. Features that can single-handedly distinguish IHH-exposed samples on ROC plots were highlighted.	1) Binary predictions of IHH-exposed samples (positive) or air-exposed samples (negative) using total Ldlr-/- and ApoE-/- data as training and testing dataset, and vice versa. 2) For longitudinal binary predictions of IHH-exposed samples (positive) or air-exposed samples (negative), using Ldlr-/- or ApoE-/- data per time point as training and testing dataset, and vice versa. 			Random forest	Yes, in Jupyter notebooks available on GitHub (https://github.com/knightlab-analyses/crossmodel_prediction).	1) 16S rRNA raw amplicon sequencing data were processed and converted to sub-operational taxonomic unit (sOTU) abundance per sample (BIOM format) using the Deblur workflow. Taxonomies for sOTUs were assigned using the sklearn-based taxonomy classifier trained on the Greengenes 13_8 99% OTUs in QIIME 2. The sOTU table was rarefied to a depth of 2,000 sequences/sample to control for sequencing effort. A phylogeny was inferred using SAT√©-enabled phylogenetic placement, which was used to insert 16S Deblur sOTUs into Greengenes 13_8 at a 99% phylogeny. 2) Raw LC-MS/MS data sets were converted to m/z extensible markup language (mzXML) in centroid mode using MSConvert. All mzXML files were cropped with an m/z range of 75.00 to 1,000.00 Da. Feature extraction was performed in MZmine2 with a signal intensity threshold of 2.0e5 and minimum peak width of 0.3 s. The maximum allowed mass and retention time tolerances were 10 ppm and 10 s, respectively. A local minimum search algorithm with a minimum relative peak height of 1% was used for chromatographic deconvolution; the maximum peak width was set to 1 min. The detected peaks were aligned across all samples using the above-mentioned retention time and mass tolerances, producing the final feature table used in these analyses.  Molecular networking in GNPS was performed to putatively identify molecular features using MS/MS-based spectral library matches. 	1) First data layer.16S rRNA amplicon sequencing-derived abundances of microbial taxonomies. 2) Second data layer. Quantified LC-MS/MS peaks of detected metabolites following preprocessing. 	Cross-validation with all the samples from the same mouse appearing only in either training or validation data but not both, to avoid overoptimistic cross-validation accuracy scores because of the classifier learning idiosyncrasies of the individual itself rather than the treatment.	No	Not specified	Not specified			6312169df3794236aa9879ed	31058230.0	PMC6495231	02/02/2026 19:46:52	Tripathi A, Xu ZZ, Xue J, Poulsen O, Gonzalez A, Humphrey G, Meehan MJ, Melnik AV, Ackermann G, Zhou D, Malhotra A, Haddad GG, Dorrestein PC, Knight R.	mSystems	Intermittent Hypoxia and Hypercapnia Reproducibly Change the Gut Microbiome and Metabolome across Rodent Model Systems.	10.1128/msystems.00058-19	2019	0.0	0.0		1.0	2022-03-27T14:51:24.000Z	2026-02-02T19:46:52.000Z	e104aa68-ff85-4fed-bc9f-06f1cdc39835	undefined	acyudepk6j				Starting_TSV	Match	No Match
63516fedb9c880af1f305b24	Data is available from authors upon request.	Profiles of 150 metabolites based on gas chromatography mass spectrometry (GC/MS) analysis of plasma samples from 70 postmenopausal women.	During GC/MS analysis tentative substances were not reported. All known artificial peaks were identified and removed before data mining. The metabolic feature columns, which had more than 40% of data missing, were eliminated.	Npos=23 patients with coronary microvascular disease (CMD). Nneg=47 participants, including 21 patients with coronary artery disease (CAD) and 26 healthy participants as control group.			Data is available from authors upon request.	1) One-way-ANOVA model and chi-squared analysis were fitted to test the statistical significance of clinical differences between different participant groups, followed by Tukey‚Äôs post hoc test. p < 0.05 was considered significant. 2) Z-scores were calculated for each metabolite. An unpaired t-test was performed to identify metabolites that are different between each participant group. 	Not specified	Receiver operating curves (ROC), precision‚Äìrecall (PR) curves, AUC score, F1 score	5-fold cross-validation			Data is available from authors upon request.	Not specified	Transparent. 15 selected metabolites following feature elimination, including stearic acid and ornithine, leading to best model performance. Unpaired t-test to identify metabolites with differences between CMD and non-CMD.	The binary predictions of ‚ÄúCMD‚Äù and ‚Äúnon-CMD‚Äù, with the latter combining both CAD and control group samples.			Recursive feature elimination with cross-validation (RFECV) from sklearn. Random forest was used as the estimator.	Data is available from authors upon request.	1) To allow comparison between samples, all data were normalized to the internal standard in each chromatogram. The raw data consisting of 175 metabolites measured across 70 participants was normalized using min/max scaling.  2) Data imputation with IterativeImputer of sklearn was utilized to handle missing data. The trained imputer was tested by removing the data of one metabolite feature column and imputing the data afterward. The performance of the imputer was measured for each of the metabolites using R^2 metric. Metabolite columns with R^2 < 0.3 and more than 5% of missing values across all patients were removed. 3) Data standardization to zero mean and unit variance for each metabolite feature column.  Data pre-processing step reduced the feature columns to 75 metabolites.  	Metabolic profiles of 45 metabolites following data preprocessing. Recursive feature elimination with cross-validation (RFECV), using random forest, reduced the number of features to 15 metabolites.	Not specified	No	Not specified	Not specified			6312169df3794236aa9879ed	34070374.0	PMC8230313	02/02/2026 19:46:52	Arredondo Eve A, Tunc E, Liu YJ, Agrawal S, Erbak Yilmaz H, Emren SV, Akyƒ±ldƒ±z Ak√ßay F, Mainzer L, ≈Ωurauskienƒó J, Madak Erdogan Z.	Metabolites	Identification of Circulating Diagnostic Biomarkers for Coronary Microvascular Disease in Postmenopausal Women Using Machine-Learning Techniques.	10.3390/metabo11060339	2021	0.0	0.0		1.0	2022-03-26T18:39:06.000Z	2026-02-02T19:46:52.000Z	f0936cfb-e3a1-44a1-b760-3897fce5b861	undefined	nhcfgr8oi1				Starting_TSV	Match	No Match
63516fedb9c880af1f305bb5	Not available	Manually re-annotated 313 sentences, containing 2304 predicate-argument structures (PAS) annotated for 49 biomedical verbs.	Not specified	Separation of dataset into 3 subsets. Ntrain = 2 subsets. Ntest = 1 subset. The process is repeated three times, with each of the test sets being used exactly once (3-fold cross-validation).			Not available	Comparison with ML-based SRL systems of other specific domains based on F-score performance	Not specified	Precision, recall, F-score	3-fold cross-validation			Not available	Not specified	Not specified	Multi-label predictions			The semantic role labeling (SRL) system BIOSMILE, which is based on Maximum entropy.   Novel rule-based converter based on verb-by-verb conversion rules which describe under which conditions each mapping is valid.The algorithm used by the rule-generator compares corresponding framesets for a given verb sense, checks each argument in its PASBio frameset, and tries to find an argument in its BioProp frameset that has the same semantic role under a set of conditions. When a match is found, the algorithm maps a link between the two frameset arguments, which includes a description of required conditions, named entities (NEs) and keywords. 	Not available	1) Tagging of 5 names entities (NEs), protein, DNA, RNA, cell line, and cell type, with NERBio recognition software. A dictionary was used to find other NE types, such as exon and intron. 2) Identification of the PAS objects of each sentence followed by classification of the semantic roles of the arguments according to BioProp format, using BIOSMILE SRL system. 3) Rule-based conversion from BioProp to PASBio annotation, using the novel rule-based converter. 	Semantic roles of PAS objects following PASBio annotation	Not specified	Yes. Combination of BIOSMILE SRL system output with novel rule-based converter. Independency not specified.	Not specified	Not specified			6312169df3794236aa9879ed	19091017.0	PMC2638158	02/02/2026 19:46:52	Tsai RT, Dai HJ, Huang CH, Hsu WL.	BMC bioinformatics	Semi-automatic conversion of BioProp semantic annotation to PASBio annotation.	10.1186/1471-2105-9-s12-s18	2008	0.0	0.0		1.0	2022-03-25T13:35:02.000Z	2026-02-02T19:46:52.000Z	37b6eb38-2c74-40e9-9f75-95d27907ae41	undefined	qto6tkwcli				Starting_TSV	Match	No Match
63516fedb9c880af1f305bb6	DOI: 10.1093/nar/gkh894, DOI: 10.1093/nar/gkj109, 	FunCat dataset used by DOI: 10.1093/nar/30.1.31, BioGRID database, SMD, MIPS	not specified	1) 13 general functional classes were selected, and 4049 genes have been annotated in total. 2) 82,633 pairs of interactions among 5,299 yeast genes, of which 4049 genes are annotated by the 13 functional classes.  3) 5,132 genes with 278 real value features for gene expression data.			no	The AGPS algorithm is different from existing methods, which have inappropriate assumptions about those genes that have no target annotation.	not mentioned	precision, recall and F1	cross-validation and independent dataset			no	not mentioned	Black box	classification			SVM	no	the protein interaction data, gene expression profiles and protein complex data for yeast genes are integrated into one functional linkage graph	SVD technique was employed to reduce the dimensionality and remove noise. 13 features.	to evaluate the functional similarity between a pair of genes, the Czekanowski-Dice distance was employed. After that, the functional similarity between any pair of genes was represented as a real value between 0 and 1	no	not mentioned	no			6312169df3794236aa987a1b	18221567.0	PMC2275242	02/02/2026 19:46:52	Zhao XM, Wang Y, Chen L, Aihara K.	BMC bioinformatics	Gene function prediction using labeled and unlabeled data.	10.1186/1471-2105-9-57	2008	0.0	0.0		1.0	2022-03-24T13:09:43.000Z	2026-02-02T19:46:52.000Z	8bb488cc-4a32-481c-8c5c-82e9d7990c0e	undefined	3p7aj2vzii				Starting_TSV	Match	No Match
63a25db2e8edf6ce46f6e84b	 http://www.cogsys.cs.uni-tuebingen.de/software/dna-methylation/.	NAME21 consortium, ENCODE consortium,  whole-genome catalogue of DNA methylation in human, DOI: 10.1371/journal.pgen.1000438	not mentioned	56 methylated (112 unmethylated) instances for leukocytes, 73 methylated (117 unmethylated) instances for HEK293, 44 methylated (142 unmethylated) instances for HEPG2, 43 methylated (142 unmethylated) instances for fibroblasts, and 32 methylated (137 unmethylated) instances for trisomic fibroblasts			no	direct comparison is challenging since there are a lot of factors contributing in the final results. Some comparison is indeed presented from other publications.	To ensure a fair comparison, all analyses have been repeated ten times with a ten-fold cross-validation so the mean and standard deviation for each experiment are presented.	accuracy, Matthews correlation coefficient (MCC) and the area under the receiver operating characteristics curve (AUC), average absolute error (AAE)	Cross-validation and  independent dataset			 http://www.cogsys. cs.uni-tuebingen.de/software/dna-methylation/.	not mentioned	the algorithms were used in order to show the predictive performance of each feature and thus outline the correlation between features and classification	binary classification			decision trees, naive Bayes,  k-nearest neighbor, K*, random decision forest and support vector machines with Gaussian radial basis function and linear kernel	no	not mentioned	948 features from 15 categories but each training included a subset of them	not applicable	no	not mentioned	not applicable			6312169df3794236aa987a1b	22558141.0	PMC3340366	02/02/2026 19:46:52	Wrzodek C, Wrzodek C, B√ºchel F, Hinselmann G, Eichner J, Mittag F, Zell A.	PloS one	Linking the epigenome to the genome: correlation of different features to DNA methylation of CpG islands.	10.1371/journal.pone.0035327	2012	0.0	0.0		1.0	2022-03-24T11:24:39.000Z	2026-02-02T19:46:52.000Z	8c0c94cd-3172-4f65-92b0-b997742324e0	undefined	f4zymru581				Starting_TSV	Match	No Match
63516fedb9c880af1f305b86	https://github.com/ KlugerLab/deepcytof.git.	 three CyTOF datasets consisting of 56, 136 and 16 PBMC samples	none	none			no	no	comparison with manually performed task	 F-measure statistic (the harmonic mean of precision and recall)	 independent dataset			https://github.com/ KlugerLab/deepcytof.git.	not mentioned 	black box	 cell classification			NN	"yes, in ""savedmodels"" folder in https://github.com/KlugerLab/deepcytof.git."	sample denoising, calibration between target samples and a single reference source sample and finally cell classification. We implement each of these tasks using the following three neural nets: (i) a denoising autoencoder (DAE) for handling missing data; (ii) an MMD-ResNet for calibrating between the target samples and a reference source sample; (iii) a depth-4 feed-forward neural net for classifying/gating cell types trained on a reference source sample.	4	none	no	depth-4 feed-forward neural nets	logarithmic transform, followed by rescaling			6312169df3794236aa987a1b	29036374.0	PMC5860171	02/02/2026 19:46:52	Li H, Shaham U, Stanton KP, Yao Y, Montgomery RR, Kluger Y.	Bioinformatics (Oxford, England)	Gating mass cytometry data by deep learning.	10.1093/bioinformatics/btx448	2017	0.0	0.0		1.0	2022-03-23T13:41:44.000Z	2026-02-02T19:46:52.000Z	5de83145-b739-43f7-a0cf-8c223ddf45f9	undefined	8ockqu6jl9				Starting_TSV	Match	No Match
63a25db2e8edf6ce46f6e84b	yes in Supporting information section	made their own dataset	not applicable	Among 222 patients, 126 developed postinduction hypotension			no	not mentioned	ML algorithms metrics	precision, recall, accuracy	cross-validation			no	not mentioned	Black box	classification 			Na√Øve Bayes, logistic regression, random forest, and artificial neural network models	no	not applicable	performed feature selection using the caret R package. from 89 initial features, redundant features were removed, attributes with an absolute correlation coefficient of 0.5 or greater were also removed. Last, specific features were selected using the recursive feature elimination (RFE) method. 20 features were finally selected. 	not applicable	no	default	not applicable			6312169df3794236aa987a1b	32298292.0	PMC7162491	02/02/2026 19:46:52	Kang AR, Lee J, Jung W, Lee M, Park SY, Woo J, Kim SH.	PloS one	Development of a prediction model for hypotension after induction of anesthesia using machine learning.	10.1371/journal.pone.0231172	2020	0.0	0.0		1.0	2022-03-23T11:05:15.000Z	2026-02-02T19:46:52.000Z	8c0c94cd-3172-4f65-92b0-b997742324e0	undefined	qxfdrs4tuj				Starting_TSV	Match	No Match
63516fedb9c880af1f305b68	yes	https://doi.org/10.7554/eLife.01256.001, https://doi.org/10.1534/g3.115.018853	In order to avoid excessive numbers of false positive calls due to this imbalance, we trained the models to optimize the metric Kappa rather than accuracy, as Kappa accounts for imbalanced number of genes belonging to each class in training data	253 MAE genes and 1127 BAE genes			no	not mentioned	qualitative description of the advantages of the presented pipeline	 precision and recall	Cross-validation and independent dataset			https://github.com/gimelbrantlab/magic	not mentioned	various methods to directly determine class of genes	classification 			various	no	processing of signals and the resulting values are converted to quantile rank.	defaults classification labels lists or can be provided by the user	no	authors recommend to choose the model with the highest F1-score	not clearly  stated	no			6312169df3794236aa987a1b	30819107.0	PMC6394031	02/02/2026 19:46:52	Vinogradova S, Saksena SD, Ward HN, Vigneau S, Gimelbrant AA.	BMC bioinformatics	MaGIC: a machine learning tool set and web application for monoallelic gene inference from chromatin.	10.1186/s12859-019-2679-7	2019	0.0	0.0		1.0	2022-03-23T10:25:57.000Z	2026-02-02T19:46:52.000Z	f04fcb26-53d5-4ac0-8458-c45183b92b0c	undefined	8ltvqe2m93				Starting_TSV	Match	No Match
63516fedb9c880af1f305b9f	No	Data taken from Wade 2002 (101 data points), Ufkes 1982 (31 data points),  its quantitative continuous data - peptide sequence and anti-microbial activity (binding affinity, IC50), hence no positive negative, 	Synthetic data used	Data is peptide sequence and anti-microbial activity (binding affinity, IC50),  hence no pos and negative control,  first built a model to generate synthetic data and used 1000 of such data for training.			No, URL isn't working.	Pearson correlation of prediction with values in databases)	Correlation coefficient of 0.90 and 0.93 were reported for two different datasets used.	Pearson correlation coefficient ( correlation of prediction with values in databases)	kernel ridge regression with training used for validation. Also performed lab experiments.			link given https://graal.ift.ulaval.ca/peptide-design/, but page not found	Not reported	Transparent, graph model	Predict a string of amino acids with antimicrobial properties.			Novel, graph theory based approach to learn Generic String kernel (G	No	Sequence to binding affinities and IC50	Sequence (GS kernel), Synthetic data used.	not a binary classification problem, hence positive negative training data not used.	No	position-specific weight matrix (PSWM)	No			6312169df3794236aa987a77	25849257.0	PMC4388847	02/02/2026 19:46:52	Gigu√®re S, Laviolette F, Marchand M, Tremblay D, Moineau S, Liang X, Biron √â, Corbeil J.	PLoS computational biology	Machine learning assisted design of highly active peptides for drug discovery.	10.1371/journal.pcbi.1004074	2015	0.0	0.0		1.0	2022-03-21T11:49:08.000Z	2026-02-02T19:46:52.000Z	432f82cf-b140-411c-917b-b6c655b11e1f	undefined	r3o9d6vk1p				Starting_TSV	Match	No Match
63516fedb9c880af1f305b69	No	Patient data from the  Rheumatoid Arthritis Medication Study (RAMS). N_pos=42, N_neg=43. Not previuolsy used. 	Not applicable	Nested cross-validation split. N_pos, N_neg not available			No	No	No confidence reported	Balanced accuracy and ROC-AUC	10-fold nested cross-validation. No independent validation data			No	Not availbale	Black box	Binary classification			regularized logistic regression and random forest	No	Gene expression data, clinical variables.	f=10 ?	Not available	No	Not available	No			6603042f92c76639b849e69f	30615300.0	PMC9328381	02/02/2026 19:46:52	Plant D, Maciejewski M, Smith S, Nair N, Maximising Therapeutic Utility in Rheumatoid Arthritis Consortium, the RAMS Study Group, Hyrich K, Ziemek D, Barton A, Verstappen S.	Arthritis & rheumatology (Hoboken, N.J.)	Profiling of Gene Expression Biomarkers as a Classifier of Methotrexate Nonresponse in Patients With Rheumatoid Arthritis.	10.1002/art.40810	2019	0.0	0.0		1.0	2022-03-21T10:33:58.000Z	2026-02-02T19:46:52.000Z	d4ddd51f-3d4c-4c5d-8269-38a65992f2da	undefined	0yzdxjimwz				Starting_TSV	Match	No Match
63516fedb9c880af1f305b26	Yes, https://github.com/ gkanfer/AI-PS/tree/master/facs	Parking and GFP-TFEB images generated in the same study	Not applicable	Training 80%, validation 15%, 5% Independent test data. N_pos and N_neg unknown for training data. For independent data: N_pos=5401, N_neg=4948.			Not available	No	No confidence reported	Accuracy, AUPRC	Independent testing			Yes https://github.com/gkanfer/AI-PS	Not availbale	Black box	Binary classification			SVM and Convolutional Neural Network (ImageNet architecture)	Some hyperparameters specified in the main text	150x150 pixel images (CNN). Image descrptors for SVM	f=3 (pixel intensities) for CNN. f=3 for SVM (descriptors selected after PCA-based feature selection)	Early stopping used to prevent overfitting (CNN)	No	Unknown	No			6603042f92c76639b849e69f	33464298.0	PMC7816647	02/02/2026 19:46:52	Kanfer G, Sarraf SA, Maman Y, Baldwin H, Dominguez-Martin E, Johnson KR, Ward ME, Kampmann M, Lippincott-Schwartz J, Youle RJ.	The Journal of cell biology	Image-based pooled whole-genome CRISPRi screening for subcellular phenotypes.	10.1083/jcb.202006180	2021	0.0	0.0		1.0	2022-03-21T08:46:51.000Z	2026-02-02T19:46:52.000Z	42bd8a6e-bec6-4f31-b0b6-e19be5f8c86f	undefined	x35e8g9f2z				Starting_TSV	Match	No Match
63516fedb9c880af1f305b47	from other papers, and requests can be made to the senior author. Data also available via GitHub link	Comes from two of other publications [13] and [8]. Note that the purpose of this article is to show a new data representation for classification. First data set consist of 365 NGS samples, 108 are recently infected hosts and 257 are chronically infected hosts. The second set consists of 335 infected persons, 142 correspond to outbreaks with more than one person and 193 are isolated cases. the latter is used for clustering, so not further considered here	No stratification explicitly discussed, yet leave-one-outbreak out aims to resolve overlaps between the folds used for training and testing.	"no separate validation set. Cross-validation is used, both classic 10-fold and ""leave-one-outbreak-out"", also under sampling of the larger set (chronically affected) is tested."			might be part of the githuv rep 	method in ref [32], which has significantly worse performance.  	not provided	Accuracy, AUC, presicion and recall	Cross-validation 			yes via GitHub	not mentioned	no obvious interpretation	binary classification			A series of classification methods, all from scimitar learn ; including SVM (best performance), RF and NB. 	not	The paper proposed sequence image normalisation for encoding.  yet very poorly explained and thus difficult to understand	RGB matrices encoding sequences, yet unclear how this works exactly.	many more features, if one considers that every RGB value as a feature.  	not features coming from other predictors, all raw data 	ML parameters are optmized, e.g.  regularization parameter c for SVM and number of trees for RF.  Not all parameters are systematically mentioned 	Cross-validation approaches			6312169df3794236aa9879f2	33349236.0	PMC7751093	02/02/2026 19:46:52	Basodi S, Baykal PI, Zelikovsky A, Skums P, Pan Y.	BMC genomics	Analysis of heterogeneous genomic samples using image normalization and machine learning.	10.1186/s12864-020-6661-6	2020	0.0	0.0		1.0	2022-03-19T20:19:10.000Z	2026-02-02T19:46:52.000Z	39fb8263-4ce3-4283-87ed-5ceba5717b62	undefined	9m31lscgpm				Starting_TSV	Match	No Match
63516fedb9c880af1f305b27	patient clinical information available in SI, as well as the protein biomarker data cfDNA mutation and cfDNA methylation features. Link to raw data also provided (or via author)	Data is experimentally derived from 125 patients. Training data divided over 84 positive and 41 negative instances. An independent validation set 	not discussed.	Data is split into 96 for training and 29 for validation. The training set consisted of 69 positive and 27 negative, whereas the validation set is divided into 14 negative and 15 positive cases.  			not available	no other methods are compared	not provided	AUC. sensitivity and specificity	cross validation and independent set			not provided	not discussed	partially interpretable but no apart from feature analysis on the baseline predictors, nothing else reported	classification in benign or malignant			SVM	not available 	data normalised, missing data inputed using median value, data standardised using z-scores. 	10 clinical features, eight cancer biomarkers 29-gene NGS panel for mutations, 30 methylation-correlated blocks (feature-selected from 697 MCBs). 	parameter smaller than samples, features larger but still smaller than samples pin the training set, but univariate analysis reduced then number of features.	Features are coming from experimental data.  Four different predictors using different data are grouped in a stacked ensemble classifier, using Naive Bayes. 	linear kernel used in SVMs for each independent predictor.  Naive bayes sued to determine the optimal combination of these predictors.	Cross-validations procedures and feature selection are used to reduce chances for over-fitting.  Evaluation based on independent validation set.			6312169df3794236aa9879f2	34258160.0	PMC8261512	02/02/2026 19:46:52	Liu QX, Zhou D, Han TC, Lu X, Hou B, Li MY, Yang GX, Li QY, Pei ZH, Hong YY, Zhang YX, Chen WZ, Zheng H, He J, Dai JG.	Advanced science (Weinheim, Baden-Wurttemberg, Germany)	A Noninvasive Multianalytical Approach for Lung Cancer Diagnosis of Patients with Pulmonary Nodules.	10.1002/advs.202100104	2021	0.0	0.0		1.0	2022-03-19T17:28:38.000Z	2026-02-02T19:46:52.000Z	db022eae-23a7-444b-b7ad-0e845a5ec029	undefined	sfncop89n7				Starting_TSV	Match	No Match
63516fedb9c880af1f305b28	No	Yes: dataset generated by own experiment	No	~Yes but not very clear: 12 pos and 12 neg images, duplicated (image divided into four quadrant)->48 images pos and 48 images neg. divided: 80% cross-validation training set and 20% testing set.			No	No	No	Yes: accuracy = R-square and RMSE	independent dataset			No	No	~Yes: comparison between different features	Regression			Yes: SVM	No	Yes: images features (fractal dimension, 2D wavelet coefficients, percolation score)	No	No	No	No	No			6312169df3794236aa9879f4	33953534.0	PMC8042551	02/02/2026 19:46:52	Uthamacumaran A, Suarez NG, Banir√© Diallo A, Annabi B.	Cancer informatics	Computational Methods for Structure-to-Function Analysis of Diet-Derived Catechins-Mediated Targeting of In Vitro Vasculogenic Mimicry.	10.1177/11769351211009229	2021	0.0	0.0		1.0	2022-03-18T15:31:10.000Z	2026-02-02T19:46:52.000Z	c9c5b90f-7cb6-4c2e-9fd0-85cb16728ff1	undefined	6ed6jt5sbi				Starting_TSV	Match	No Match
63516fedb9c880af1f305b4a	Raw APEX-seq data available at Gene Expression Omnibus (GEO) under accession GSE116008	Training data: RNA localization data from  APEX-seq results available in literature (Fazal et al., 2019). Eight localization classes: N_c1=1223,N_c2=768,N_c3=301,N_c4=208,N_c5=1361,N_c6=823,N_c7=159,N_c8=739.  Independent test data: ENCODE Project Consortium 2012, cell line HeLa-S3: N=7641, cell line K562: N=6359. Individual class abundance in independent data is unknown	Not handled	N_train=80%,N_test=10%,N_val=10%. ENCODE data used for testing			Not available	Basset (Kelley et al., 2016), RNATracker (Yan et al.,. 2019). Baselines implemented with other tree-based approaches (e.g XGBoost), neural networks, convolutional networks, recurrent networks (long-short term memory, gated recurrent units). 	No confidence reported	Accuracy, AUROC, AUPRC	10-fold cross-validation. Independent dataset			Yes, GitHub https://github.com/wukevin/rnagps	Not availbale	Interpretable. Interpretation performed via RF feature importance	Multi-class classification			Random forest	No	K-mer featurization (k=3,4,5) scheme for 5'-UTR, CDS and 3'-UTR.	f=4032.	Risk of underfitting, since p=64 << N_total=2928.	No	p=sqrt(f)=64, where f is the number of features. 	No			6603042f92c76639b849e69f	32220894.0	PMC7297119	02/02/2026 19:46:52	Wu KE, Parker KR, Fazal FM, Chang HY, Zou J.	RNA (New York, N.Y.)	RNA-GPS predicts high-resolution RNA subcellular localization and highlights the role of splicing.	10.1261/rna.074161.119	2020	0.0	0.0		1.0	2022-03-15T15:36:27.000Z	2026-02-02T19:46:52.000Z	80e739c4-91d8-49a9-8620-1f876215ff63	undefined	cqy67zmg5o				Starting_TSV	Match	No Match
63516fedb9c880af1f305baa	all data is made available via Supplementary information	four datasets are used, two for training and two for testing. The first set consist of 146 positive and 250 negative cases.  They were obtained from two earlier publications on the same topic.  The second set is an expansion of the first, adding more negative instances. Te negative set is increases to 2125 instances. The third set is obtained from another publication, consisting of 92 positive and 100 negative instances.  The last set contains 823 positive and 823 negative instances, also extracted from another publication. 	With the data sets, sequences with a pairwise identity larger or equal to 25% were removed.  All sequences in the test sets that had a pairwise sequence identity larger or equal to 40% were removed from the test sets (using CD-HIT).  	Data set 1 and.2 are used for training, dataset 3 and 4 for testing. 			no 	Compared to other methods : DNAbinder, DNA-prot, iDNA-prot They were all reimplemented in house on the same data sets.	value comparison, no statistical tests 	ACC, MCC, SE, SP and F1	independent data sets only			platform  : http://bliulab.net/Ensemble-DNA-Prot/index.jsp	not mentioned	no interpretation provided, only performance assessment.  Remains black box as it is based on 20 different learners. 	binary (I assume based on the pseudocode provided)			Heterogeneous ensemble classifier baed on adaboost (but for unbalanced data), with 20 different based classifiers  including rule-based, SVM, tree-based and KNN-based classifiers.	Not available, but there is a platform for the tool 	188 feature vector for each protein sequence with information on composition, distribution and physiochemical properties	188 features per sequence. No feature selection appears to be performed.	Nothing reported except for the claim that their method of selecting negative cases in function of their difficulty leads to less over-fitting.  No feature selection was performed, no cross-validation	There is not sufficient information.  They calculate a 188 feature vector with properties about the sequence composition, distribution and physiochemical properties, but they not provide details on whether these are predictions or actual calculations.	Details on the parameters of each independent learner  in the ensemble are not provided. Their ensemble method uses a weight for each negative sample in the training set to tune the sampling of the negative instances	not really			6312169df3794236aa9879f2	24977146.0	PMC4058174	02/02/2026 19:46:52	Xu R, Zhou J, Liu B, Yao L, He Y, Zou Q, Wang X.	BioMed research international	enDNA-Prot: identification of DNA-binding proteins by applying ensemble learning.	10.1155/2014/294279	2014	0.0	0.0		1.0	2022-03-15T11:52:26.000Z	2026-02-02T19:46:52.000Z	6fbdbce3-56b4-4ceb-853f-9b5c4604cd2c	undefined	7hxvmu9cij				Starting_TSV	Match	No Match
63516fedb9c880af1f305b98	they do not provide themselves the data they used in their regressor and classifier.	1) regression data : 355 compounds from ChEMBL bioactivity database for training the regressor, 2) classification data : DUD-E dataset (102 target proteins, 20000 active molecules and more than a million decoy molecules); classes active/decoy. The validation/testing of the predictive methods is done on the D3R data form HSP90 and MAP4K4. They generate 29K poses for the former to test their predictors and 5329 poses for the latter. No mention of which ones are active fits and which ones are decoys. 	The DUD-E set consists of a HSP90 target.  the authors make an independent set with this information, I assume to checkt the influence of the presence of this information own the classification.	The training and test set are completely separate (see explanations above).  Note imbalance in dataset for the classifier, which they overcome by also creating a balanced dataset for training. In the HSP90 test set there were 136 active and 44 inactive compounds (threshold set by authors on affinity).			no data	they compare only their own approaches	confidence bars given in bart plots. Significance is mentioned but no p-values are provided	only AUC	Cross-validation when training the classifiers and the test set is independent as it is provided by the D3R challenge			no	not mentioned but linear regression is fast 	the weights of the regressor explain the importance of the features.  The NN is black box 	both, 			1) Elastic net linear model for affinity prediction, 2) linear regression models (linear and logistic) and neural network for pose prediction (active versus decoy).  The linear model was submitted for D3R	no	1) training data is transformed into Boolean fingerprints, 2) training data for classifier is translated into numerical vectors	no feature selection was performed. 2) classifier has 61 features and 1) size of bit pattern defines number of features in the regressor (2048 or more)	classifier has less parameters than samples.  	They use a number of other methods to infer the feature that are used by the regressor and classification algorithm.  Not clerk what the overlap is.	1) two parameters alpha and rho. 2) 1 for regressor and 20 hidden node/2 output node NN.decay and momentum parameters for training the network.	Cross-validation performed to determine the generalisation of the predictions.			6312169df3794236aa9879f2	27592011.0	PMC5079830	02/02/2026 19:46:52	Sunseri J, Ragoza M, Collins J, Koes DR.	Journal of computer-aided molecular design	A D3R prospective evaluation of machine learning for protein-ligand scoring.	10.1007/s10822-016-9960-x	2016	0.0	0.0		1.0	2022-03-13T19:09:08.000Z	2026-02-02T19:46:52.000Z	4fa6c779-0e82-43a8-8a08-65a2feaa0bad	undefined	9t9rwycfgl				Starting_TSV	Match	No Match
63516fedb9c880af1f305b6b	Bitbucket available (https://bitbucket.org/alexeyg-com/irespredictor/src). Datasets non clearly labelled	Training dataset (dataset 2): high throughput experimental data (doi: 10.1126/science.aad4939), filtered and annotated as in (doi:10.1371/journal.pcbi.1005734) describing an available predictor.  20872 examples: 2129 positive, 18743 negative. Testing dataset (dataset 1): low throughput experimental data extracted from a public database (https://doi. org/10.1093/nar/gkp981). 167 examples: 116 positive, 51 negative.	Random split. Overall similarity in the dataset 2 was checked: 7.56% sequences have more than 80% identity, 15.3% sequences have more than 50% identity, and 17.02% sequences have more than 30% identity. There are no sequences with 100% identity- Similarity between dataset 1 and dataset 2 is not reported.	Random split of dataset 2 in 90% training and 10% testing.			No	Compared with IRESpred	Not provided	Accuracy, sensitivity, specificity, precision, Matthews correlation	Independent set			Model available on bitbucket (https://bitbucket.org/alexeyg-com/irespredictor/src/v2/)	Not provided 	Feature interpreted by XGBoost feature importance	Classification			XGBoost	Hyperparameter values reported in the paper. Model available in bitbucket (https://bitbucket.org/alexeyg-com/irespredictor/src/v2/)	340 global kmer features + 5440 local kmer features	The final model includes 1281 individual trees and each tree incorporates 340 features. Features selected by XGBoost feature importance	not provided	No	The final model includes 1281 individual trees and each tree incorporates 340 features. The maximum depth of each tree is set to be 6	No			6312169df3794236aa9879fe	31362694.0	PMC6664791	02/02/2026 19:46:52	Wang J, Gribskov M.	BMC bioinformatics	IRESpy: an XGBoost model for prediction of internal ribosome entry sites.	10.1186/s12859-019-2999-7	2019	0.0	0.0		1.0	2022-03-13T11:00:59.000Z	2026-02-02T19:46:52.000Z	62a0f5bf-1440-4bce-855f-1beedf6af6f5	undefined	dnrghw65xy				Starting_TSV	Match	No Match
63516fedb9c880af1f305b2c	previous publication gives some data but not the raw data	"Data comes from a previous paper : Koureas, Michalis, et al. ""Target analysis of volatile organic compounds in exhaled breath for lung cancer discrimination from other pulmonary diseases and healthy persons."" Metabolites 10.8 (2020): 317. 85 patients (49Ca+ and 36 Ca-) and 52 control group individuals."	No mentioned how stratification is done in the cross-validation procedure.  Feature elimination is performed	No split in training/test /validation.  The splitting is done within a 10-fold cross-validation procedure. Three classes are possible, and predictors are designed for pairs of classes. 			not available	focus on RF as performance of others is low (obvious for NB and regression)	not provided 	AUC and accuracry	cross-validation			not available	not mentioned	No analysis provided 	classification (Ca+ vs HC, CA- vs HC and Ca+ vs Ca-)			Weka platform used,  naive bases, logistic regression and random forests.  Only the latter is shown given its better performance. 	not available	Not discussed explicitly.  metabolic data per patient was used,	mass spectrum data is used.   Either focussing on a specific subset fo know compounds or the full VOC data (requiring extensive feature selection , see Fig2 of paper)	not specified	no 	Not detailed what the parameters were for the random forest. 	Thorough feature selection procedure (wrapper based) and two step cross-validation (10-fol)			6312169df3794236aa9879f2	33946997.0	PMC8125376	02/02/2026 19:46:52	Koureas M, Kalompatsios D, Amoutzias GD, Hadjichristodoulou C, Gourgoulianis K, Tsakalof A.	Molecules (Basel, Switzerland)	Comparison of Targeted and Untargeted Approaches in Breath Analysis for the Discrimination of Lung Cancer from Benign Pulmonary Diseases and Healthy Persons.	10.3390/molecules26092609	2021	0.0	0.0		1.0	2022-03-12T20:11:43.000Z	2026-02-02T19:46:52.000Z	14a845e7-557b-486b-8b10-d636b8a1c8f2	undefined	0zyckt3df0				Starting_TSV	Match	No Match
63516fedb9c880af1f305b4c	data is not provided.	In house produce experimental and clinical data (neurophysiological tests, age, education, ethnicity.  20 normal  and 40 neurologically impaired patients. New data not used in other studies.	not specified, although they claim that data leakage (which I assume is bad stratification) between folds was precluded.  No details on how.  I assume random fold separation, hence the 10 times repetition.	They employed 6-fold cross-validation (repeated 10 times), not needing split between training and test set.  they did not provide a validation set.  Pos=40, Neg=20 patients.			not available 	4 different methods were compared. No comparison with other method on their data.	paired t-test with p<0.05	area under the curve	cross validation			not available 	not mentioned	They performed a feature importance analysis for the best models (M3 and M4) and showed partial dependency plots between classification and a series ofd features. No details on how this was produced.	classification in two classes (impaired versus normal)			SVM, Adaboost with decision trees, Ensemble KNN, Random forest	no available	some data were transformed either by binning or one-hot encoding .  	It's is unclear which features are exactly used. There is no clear summary table.  They used clinical test and patient data next to protein concentration information (HMGB1, NFL, p-181-tau).  Missing values were filled up with K-NN imputation.  They examine always 7 models (in terms of the feature set used), M1 is only clinical data the others use M1 plus a subset of the 3 proteins.	number if features is smaller than number of samples. Cross-validation (I assume with random splits) to avoid over-fitting.	No, all raw data	They are not specified (number of KNN in the ensemble, RF settings, adaboost)	no validation set used.			6312169df3794236aa9879f2	32681213.0	PMC7718328	02/02/2026 19:46:52	Pulliam L, Liston M, Sun B, Narvid J.	Journal of neurovirology	Using neuronal extracellular vesicles and machine learning to predict cognitive deficits in HIV.	10.1007/s13365-020-00877-6	2020	0.0	0.0		1.0	2022-03-12T19:15:18.000Z	2026-02-02T19:46:52.000Z	3cb70f9b-d503-4330-9b57-4a893f11f3b0	undefined	p75y9vrkgn				Starting_TSV	Match	No Match
63516fedb9c880af1f305b88	Yes, http://molsync.com/ebola/ (link not working)	Dataset from literature (Madrid et al., 2013; Madrid et al., 2015). N_pos=41, N_neg=653	Not applicable	5-Fold Cross-validation split: N_pos_train ~= 33, N_neg_train ~= 522, N_pos_test ~= 8, N_neg_test ~= 131. Leave out 50% √ó 100 fold cross validation: N_pos_train ~= 20, N_neg_train ~= 327, N_pos_test ~= 20, N_neg_test ~= 327			Partially available in supplementary material	No	No confidence reported	ROC-AUC, Confusion matrix, Sensitivity, Specificty	5-fold Cross-validation and Leave out 50% √ó 100 fold cross validation. No independent test.			No	Not available	Black box	Binary classification			Bayesian, Support Vector Machine and Recursive Partitioning Forest	Configuration avalailble in the supplementary material. models available http://molsync.com/ebola/ (link not working)	Molecular descriptors: molecular function class fingerprints of maximum diameter 6 (FCFP_6), AlogP, molecular weight, number of rotatable bonds, number of rings, number of aromatic rings, number of hydrogen bond acceptors, number of hydrogen bond donors, and molecular fractional polar surface area	f=9	Number of training examples is at least twice p_svm. Reduced risk of over- and under-fitting	No	p_svm=222	No			6603042f92c76639b849e69f	26834994.0	PMC4706063	02/02/2026 19:46:52	Ekins S, Freundlich JS, Clark AM, Anantpadma M, Davey RA, Madrid P.	F1000Research	Machine learning models identify molecules active against the Ebola virus <i>in vitro</i>.	10.12688/f1000research.7217.3	2015	0.0	0.0		1.0	2022-03-09T11:07:20.000Z	2026-02-02T19:46:52.000Z	96990d27-f206-43f3-ac18-2c14125e545c	undefined	kbejtqvi3h				Starting_TSV	Match	No Match
63516fedb9c880af1f305bb7	No	Los Alamos National Laboratory Bioscience Division STD Sequence Databases. N_pos and N_neg for each functional class are unknown.	Redundancy between traiing/testing reduced with PSI-BLAST (e-value threshold set to 0.001)	Five independent training/testing splits with ratio 4:1. N_pos, N_neg for each training/testing is unknown. No separate validation set.			No	No	No	ROC-AUC	Repeated training/testing split (five times). No indipendent datasets			No	Not available	Black box	Multi-class classification			Support Vector Machines	Configuration and hyper-parameter configuration available in the main text	Global features encoding frequency and total number of each amino acid, as well as of certain sets of amino acids (e.g. hydrophobic, charged, polar). Protein subdivided into four equally sized fragments and calculated the same feature values for each fragment and combination of fragments. Predicted the secondary structure using Prof, position of putative transmembrane helices using TMHMM and of disordered regions using DisEMBL (predicted features are processed using the above fragmentation strategy). 	f=2579. Feature selection performed using Wilcoxon signed-rank test	Not available	Yes, for computing some feature. No handling of potential dataset redundancy.	Not available	No			6603042f92c76639b849e69f	17374164.0	PMC1847686	02/02/2026 19:46:52	Al-Shahib A, Breitling R, Gilbert DR.	BMC genomics	Predicting protein function by machine learning on amino acid sequences--a critical evaluation.	10.1186/1471-2164-8-78	2007	0.0	0.0		1.0	2022-03-09T10:14:51.000Z	2026-02-02T19:46:52.000Z	a6fa00a2-c4cf-4004-b63f-2abb8ffd46bf	undefined	vs7yv4y6i5				Starting_TSV	Match	No Match
63516fedb9c880af1f305b89	No, only ClinicalTrials.gov identifier: NCT01717573	clinical trials, 11 pos and 27 neg	not reported	LOOCV for testing and internal LOOCV or 5-fold CV for validation			no	only among the models in the paper	Not given	sensitivity, specificity, accuracy, AUROC	LOOCV			not provided	not provided	transparent: identification of the main biomarkers	both, depending on the model used			Logistic regression, KNN, LDA, DT, RF, Gaussian processes (GP-ARD)	partially: supplement 	global features	6	a range of different model types was tested	no	various but equal to f in most cases	for some predictors, e.g. Lasso for logistic regression			6312169df3794236aa9879fc	28747397.0	PMC5550971	02/02/2026 19:46:52	Gao H, Aderhold A, Mangion K, Luo X, Husmeier D, Berry C.	Journal of the Royal Society, Interface	Changes and classification in myocardial contractile function in the left ventricle following acute myocardial infarction.	10.1098/rsif.2017.0203	2017	0.0	0.0		1.0	2022-03-08T18:05:03.000Z	2026-02-02T19:46:52.000Z	0f6385d3-bd3b-4257-b8e8-3289261b719f	undefined	b9ln75l2jr				Starting_TSV	Match	No Match
63516fedb9c880af1f305b6d	yes: https://github.com/jertubiana/ProteinMotifRBM	supervised (contact prediction): 18 sets of multiple sequence alignments from pfam database + contact maps based on pdbs; 	Reweighting procedure is applied: each sequence is assigned a weight equal to the inverse of the number of sequences with more than 90% identity	train/test: 80%/20% at random 			No	performance was compared to direct coupling-based methods, namely the Pseudo-Likelihood Method (plmDCA) and Boltzmann Machine (BM)	not provided	PPV, accuracy (contact prediction task)	independent test (20% of initial data)			yes: https://github.com/jertubiana/ProteinMotifRBM	in the order of 1‚Äì2 days on an Intel Xeon Phi processor with 2¬†√ó¬†28 cores	yes: weights are interrelated for various case studies	probability score			Restricted Boltzmann Machines	yes: https://github.com/jertubiana/ProteinMotifRBM	aligned sequences	MxNx21 where M is length and N is width of an MSA	regularisation and model selection used	No	not given explicitly, a different number of hidden units (1-400) were tested	L2 and L2/L1			6312169df3794236aa9879fc	30857591.0	PMC6436896	02/02/2026 19:46:52	Tubiana J, Cocco S, Monasson R.	eLife	Learning protein constitutive motifs from sequence data.	10.7554/elife.39397	2019	0.0	0.0		1.0	2022-03-08T17:16:35.000Z	2026-02-02T19:46:52.000Z	0e3da6d0-101b-4136-926f-f122310fb98c	undefined	v9brv84km1				Starting_TSV	Match	Match
63516fedb9c880af1f305b6e	NCBI Gene Expression Om-nibus (https://www.ncbi.nlm.nih.gov/geo/): GSE86354 and GSE62944	Three classes are three gene expression datasets collected before: 373 normal, 59 normal adjacent to tumour, and 541 tumour patients	None checked	10-fold CV			yes: supporting information	comparison with conventional t-test and fold change methods	None provided	Recall, F1, Accuracy, MCC	10-fold CV			Barely: https://github.com/Zhixun-Zhao/GeneMarker	not given	transparent: identification of marker genes for lung cancer	scores			Kernel maximum mean discrepancy to select features + RF	No	global features	 expression data of 23368 genes	Not provided	No	non-parametric method (it seems)	None mentioned			6312169df3794236aa9879fc	31856830.0	PMC6923882	02/02/2026 19:46:52	Zhao Z, Peng H, Zhang X, Zheng Y, Chen F, Fang L, Li J.	BMC medical genomics	Identification of lung cancer gene markers through kernel maximum mean discrepancy and information entropy.	10.1186/s12920-019-0630-4	2019	0.0	0.0		1.0	2022-03-08T14:15:46.000Z	2026-02-02T19:46:52.000Z	73502b02-ed6c-48e0-9ab6-148d33d45944	undefined	imwu5xtkc9				Starting_TSV	Match	No Match
63516fedb9c880af1f305b4e	yes, https://github.com/taigangliu/HMMPred	Two published datasets from literature: PDB1075 (525 pos and 550 neg) and PDB186 (93 pos and 93 neg)	sequences with more than 25% sequence similarity were removed	training: 525 pos / 550 neg; validation: 10-fold CV and jackknife CV; testing: 93 pos / 93 neg			no raw evaluation files 	comparisons with some existing predictors on the same datasets: DNAbinder, DNA-Prot, iDNA-Prot, iDNA-Prot|dis, Kmer1+ACC, iDNAPro-PseAAC, PseDNA-Pro, Local-DPP, HMMBinder  	not given	Accuracy, sensitivity, specificity, MCC, AUC	Cross-validation, independent dataset			https://github.com/taigangliu/HMMPred	not provided	Black box	binary classification 			 SVM, RF, XGBoost for feature ranking	yes: https://github.com/taigangliu/HMMPred	global features were derived from sequece-based features by averaging over protein length	a range from 420 to 4020 was tested; 2000 in the final model	a range of feature numbers was tested based on CVs; XGBoost feature ranking	no	p = f+3 for SVM,  	included in SVM (margin maximisation)			6312169df3794236aa9879fc	32300371.0	PMC7142336	02/02/2026 19:46:52	Sang X, Xiao W, Zheng H, Yang Y, Liu T.	Computational and mathematical methods in medicine	HMMPred: Accurate Prediction of DNA-Binding Proteins Based on HMM Profiles and XGBoost Feature Selection.	10.1155/2020/1384749	2020	0.0	0.0		1.0	2022-03-08T12:38:13.000Z	2026-02-02T19:46:52.000Z	f2fcb740-5dca-4b4c-b5e1-8ea073ecc3e0	undefined	lohsb5kzme				Starting_TSV	Match	No Match
63516fedb9c880af1f305b2d	Data is provided as supplementary information (supplementary data 1) and via GEO	RNA-seq data produced by the authors. The set includes 934 exon triplets responding to a BPN15477b treatment. 245 with increased exon inclusion and 680 with increased exclusion. They added 382 exon triplets which did not trigger a response (negative set). Data is used for a three-class predictor. 	No stratification wa reported	Data divided in training (178 inclusion responded, 478 exclusion responded and 268 unchanged), validation (51inclusion responded, 136 exclusion responded and 76 unchanged) and test set (25 inclusion responded, 68 exclusion responded and 38 unchanged). the data is split randomly over these 3 sets. 			didn't find it	not relevant for this work,	Provided in detail (see statistical analysis section)	AUC and precision recall	Data split in training, validation and testing. Experimental verification of predictions.			via GitHub	not mentioned	Method is black box but they provide an exhaustive evaluation of the predictions (compared to 1000 other models) and performed an analysis of the data, through other methods and experiments.	classification (values between 0 and 1 for each class)			CNN, explained in Methods and figure in SI. Predicts a value between 0 and 1 for each of the three classes: inclusion, exclusion and unchanged. Aim is to predict splicing size changes (including no change) after BPN15477 treatment.	Code is available on GitHub (haven't checked it)	Onehot encoding of concatenated triplets of 3 exons (details see paper)	400x4 matrix for each exon triplet. No feature selection performed	used L-1 regularisation to avoid overfitting (coefficient =0.6) in the convolutional layer and dropout strategy im the hidden layer	No data from other predictors is used	Huge number of network parameters (2.5 million trainable parameters as mentioned by the authors). 	Overfitting limited through limitation on number of training epochs (12th epoch).  Comparison to 1000 other models with same structure using different random initialisations.  Evaluations done on separate validation and test sets			6312169df3794236aa9879f2	34099697.0	PMC8185002	02/02/2026 19:46:52	Gao D, Morini E, Salani M, Krauson AJ, Chekuri A, Sharma N, Ragavendran A, Erdin S, Logan EM, Li W, Dakka A, Narasimhan J, Zhao X, Naryshkin N, Trotta CR, Effenberger KA, Woll MG, Gabbeta V, Karp G, Yu Y, Johnson G, Paquette WD, Cutting GR, Talkowski ME, Slaugenhaupt SA.	Nature communications	A deep learning approach to identify gene targets of a therapeutic for human splicing disorders.	10.1038/s41467-021-23663-2	2021	0.0	0.0		1.0	2022-03-07T21:14:17.000Z	2026-02-02T19:46:52.000Z	488264e6-f952-472e-add0-e6d8fd6f5d1d	undefined	2ee7tbw9bc				Starting_TSV	Match	No Match
63516fedb9c880af1f305b4f	yes, https://github.com/taozhy/identifying-vesicle-transport-proteins	based on UniProt search; dataset published in 2019; 2533 pos and 9086 neg	The original data was processed by BLAST to get the sequence similarity to less than 30%; the test set was used before.	Train: 2214 (pos) and 2214 (neg) -> 5-fold CV; test 319 (pos) 1513 (neg);			no raw evaluation files; confusion matrix is in the text	none	none	Recall, Precision, Accuracy, MCC	5-fold CV; independent test set			not provided	not given but should be instant	black box	binary classification			SVM with RBF kernel	partially - only some hyper-parameters are given in the text	global features	"originally over 433; 39 after ""parameter optimisation""; further reduced to 21 by max-relevance-max-distance algorithm"	none	No	39 or 21 (after dimension reduction using max-relevance-max-distance)	yes, L2 regularisation			6312169df3794236aa9879fc	33133228.0	PMC7591939	02/02/2026 19:46:52	Tao Z, Li Y, Teng Z, Zhao Y.	Computational and mathematical methods in medicine	A Method for Identifying Vesicle Transport Proteins Based on LibSVM and MRMD.	10.1155/2020/8926750	2020	0.0	0.0		1.0	2022-03-07T10:25:36.000Z	2026-02-02T19:46:52.000Z	e9544652-63df-4961-abc7-af965754adc8	undefined	25pxfm641i				Starting_TSV	Match	No Match
63516fedb9c880af1f305b7f	Yes. Supplementary Data.	ChIP-seq data, including uniformly processed peak calling results and peak width of promoter histone modifications from Roadmap and ENCODE projects for active (H3K4me3, H3K9ac, and H2A.Z) and repressive (H3K27me3) promoter modifications, and marks associated with enhancers (H3K4me1, H3K27ac, DNase I hypersensitivity sites).   Curated haploinsufficient (HIS) genes, positive (Npos) training observations, were collected from haploinsufficient training genes used in previous studies and genes with haploinsufficient score of 3 in ClinGen Dosage Sensitivity Map.  Curated haplosufficient (HS) genes, negative (Nneg) training observations, included genes deleted in two or more healthy people, based on CNVs detected in 2026 normal individuals.  All data were used in previous studies. 	Only genes with half or more of its length covered by any deletion were considered ‚Äúdeleted‚Äù (HS genes) in an individual.  The initial raw training may have included some false positive and false negative genes, as it contained results from automated literature mining that is known to give noisy output. To optimize the performance, the following pruning of the raw training set was performed:   1) Only protein-coding genes in autosomes were kept, as non-protein coding genes or genes on sex chromosomes may be under different mechanism of epigenomic regulation. 2) From the positive training set, genes with sufficient contradictory evidence were removed (ExAC pLI ‚â§ 0.1 and expected loss-of-function variants >1011). 3) From the negative training set, genes with sufficient contradictory evidence (pLI ‚â• 0.2 and expected loss-of-function variants>10) were removed. 	Following pruning for the training set, Npos=287 curated haploinsufficient genes and Nneg=574 curated haplosufficient genes were considered for further analysis. 1) Ntrain=90%. Ntest=10%. 10-fold cross validation for training. 100 randomized runs. 2) Final training with all training data and estimation of probabilities. 			Yes. Supplementary Data.	Random Forest model performed better than SVM and SVM with Lasso, and it was chosen for training the final model (Episcore). Comparison of Episcore with pLI scores from ExAC, Shet values, and ranks of mouse heart expression level, using de novo likely-gene-disrupting (LGD) variants identified in: 1)  A previously published whole exome sequencing study DDD (Deciphering Developmental Disorders consortium) of 1365 trio families with congenital heart disease (CHD). 2) A second CHD WES cohort of 2645 parent‚àíoffspring trios from the Pediatric Cardiac Genomics Consortium (PCGC). LGD variants include frameshift, nonsense and canonical splice site mutations. Episcore achieved better performance in prioritizing LGD de novo variants than the other methods. 	Permutation testing.	ROC curve, AUC score, sensitivity, specificity	1) 100 randomized runs with Ntrain=90% and Ntest=10% for each run. 10-fold cross validation was applied. 2) For the best performing model (Random Forest) all training data were used to train the final model. 			Yes. GitHub website. https://github.com/ShenLab/episcore	Not specified	Epigenomic features critical for the prediction were determined by calculating a Spearman correlation coefficient between each epigenomic feature and Episcore (random forest model) prediction.   One epigenomic feature corresponds to a data type per certain tissue/cell type. To examine which data types are more important, these Spearman correlation coefficients were plotted by data type.  To examine what tissue/cell types are more important, the averaged z-score for each tissue/cell type was calculated by: 1) Converting every Spearman correlation coefficient to a Z-score using mean and standard deviation specific to each data type and 2) Average Z-scores from various data types for each tissue/cell type.  This analysis indicated that epigenomic features in stem cells, brain tissues, and fetal tissues contribute more to Episcore prediction than other features. 	Binary predictions. All training genes used to train the best performing Random Forest model, and then estimate the probabilities of being positive (HIS) for all genes. The whole process was repeated 30 times and the arithmetic mean of the 30 sets of probabilities was used as result.			Random forest, Support Vector Machine (SVM), and SVM with LASSO feature selection.	Yes. GitHub website. https://github.com/ShenLab/episcore	For promoter features (H2A.Z, H3K27me3, H3K4me3, and H3K9ac), GappedPeaks were used to allow for broad domains of ChIP-seq signal. The assignment of a GapppedPeak to a gene followed these steps in order:  1) For each gene, only TSS of Ensembl canonical transcripts were used. 2) A GappedPeak was assigned to a TSS if the GappedPeak overlaps with the upstream 5 kb to downstream 1 kb region around the TSS. This definition of basal cis-regulatory region around promoter was according to GREAT tool. Assigning one GappedPeak to multiple TSS was allowed. 3) For TSS having more than 1 GappedPeak assigned, only the closest one was kept. 4) For genes with multiple TSS and hence multiple assigned GappedPeaks, only the longest GappedPeak was kept. After these four steps, if one gene had been associated with a GappedPeak, then the width of the peak was used as an epigenomic feature in the following machine learning models. If a gene had no associated GappedPeak, then the peak width is 0.  Coverage of H3K27ac, H3K27me3, H3K36me3, H3K4me1, H3K4me3, H3K9me3, DNase I, and RNA-seq data was used as input for EpiTensor to infer interacting enhancers across distant genomic regions. This was done as balance between more input data types and more cell types included, as not every cell type has all these histone modifications characterized. 	The widths of called ChIP-seq peaks was used as promoter features. The counts of the interacting number of promoters and enhancers within pre-defined topologically associated domains (TADs) as enhancer features. Specifically, the results of peak width and number of interacting enhancers were consolidated into a matrix, with each row being a gene and each column representing a combination of a tissue and a data type, e.g. ‚ÄúH3K4me3 peak width in fetal heart‚Äù. One combination of a tissue and a data type was referred to as one epigenomic feature. This matrix was used as input for the machine learning models. 	Not specified	No	Alpha parameter equal to 1 for Lasso regularization. No further parameters were specified.	Yes. LASSO regularization was used with SVM for overfitting prevention.			6312169df3794236aa9879ed	29849042.0	PMC5976622	02/02/2026 19:46:52	Han X, Chen S, Flynn E, Wu S, Wintner D, Shen Y.	Nature communications	Distinct epigenomic patterns are associated with haploinsufficiency and predict risk genes of developmental disorders.	10.1038/s41467-018-04552-7	2018	0.0	0.0		1.0	2022-03-04T12:54:32.000Z	2026-02-02T19:46:52.000Z	82cf0bf2-90df-48f5-9e3f-8671010c3fc3	undefined	58820ywq9x				Starting_TSV	Match	No Match
63516fedb9c880af1f305ba4	Yes. Supporting information.	Training dataset of 40 pairs of Pdu protein sequences whose presence or absence of physical, protein-protein interactions (PPIs) could be experimentally validated via binding assays, complementation and expression studies or crystallographic data.  Testing dataset of protein orthologs collected from 34 bacterial genomes in the KEGG database and collapsed among 22 orthologous protein groups, which represent types of bacterial microcompartment (MCP) proteins known to be associated with the propanediol utilizing (Pdu) system. 	Incomplete or erroneous annotations of the Pdu gene products were corrected after sequence comparison with the Pdu operon from Salmonella enterica LT2, the best-characterized strain in terms of Pdu MCP.	For the training set, Npos=16 interacting and Nneg=24 non-interacting protein pairs. For the testing set, pairwise combinations of the 22 orthologous protein groups resulted in 231 unique protein pairs that needed to be classified. 			Yes. Supporting information.	Not applicable.	High training accuracy for experimentally characterized PPIs, 15 of 16 correctly predicted as pos. Claiming support for the high specificity criterion.	ROC curve, AUC score	10-fold cross-validation. Experimental confirmation of a predicted positive PPI from the testing dataset.			Not available	Not specified	Not explicitly interpreted. Comparison of classification performance for fewer features and evaluation of their discriminatory power individually by ranking their accuracies in the context of an unsupervised analysis. Results support the importance of all features for optimal model performance.	Binary predictions of pos for an interacting protein group pair and neg for those not interacting. For prediction probability is less than 0.7 then the result is neg, and for equal or greater than 0.7 the result is pos (increased specificity).			Random Forest classifier	Not available	For each ortholog group, its corresponding protein sequences were aligned with MUSCLE. The multiple sequence alignments were subsequently input in PhyML for the construction of phylogenetic trees using the Maximum Likelihood method. For amino acid and nucleotide-based tree construction in PhyML, the LG and HKY85 substitution matrices were used, respectively. Additionally, distance matrices were calculated for each tree, where the distance between two leaves corresponds to the sum of the branch lengths separating them.   Comparing two trees can be subject to artefacts and lead in some cases to spurious correlations if speciation events are not taken in account. For this reason, some of the co-evolution features also involve the Tree of Life (ToL) of the 34 genomes studied, which originated from submitting sequences of their respective 16S ribosomal RNA to similar treatment. Since distances in the ToL are computed from a nucleotide-based substitution matrix, the distances in the ToL matrix were rescaled for proper comparison with the protein-based distance matrices. 	For each protein pair, 7 coevolution features measuring the pairwise tree similarities have been defined.  Of these, 4 features are based on pairwise comparison of the distance matrices, as defined in the mirrortree approach, and whose metrics correspond to the linear correlation coefficient between the two matrices in consideration. Let A and B be the two MCP ortholog groups, mA and mB their respective matrices, tA and tB their trees. The parameter mirrorAB is the correlation between mA and mB, mirrorA is between mA and ToL, and mirrorB is between mB and ToL. The fourth descriptor, mirrorAB-ToL, involves an adaptation of the mirror tree, also known as tol-mirror, which measures the correlation between mA and mB after removing the background similarity inherent to speciation events in the ToL. The remaining 3 topological features are derived from the Icong index, defined as the probability that the Maximum Agreement Subtree (MAST) between two trees is arising by chance. Along the same idea, topological similarities were computed between tree A and ToL, tree B and ToL, and finally A and B (topA, topB, topAB). 	Not specified	No	Not specified	Not specified			6312169df3794236aa9879ed	25646976.0	PMC4315436	02/02/2026 19:46:52	Jorda J, Liu Y, Bobik TA, Yeates TO.	PLoS computational biology	Exploring bacterial organelle interactomes: a model of the protein-protein interaction network in the Pdu microcompartment.	10.1371/journal.pcbi.1004067	2015	0.0	0.0		1.0	2022-03-03T14:47:08.000Z	2026-02-02T19:46:52.000Z	961066cf-8ef7-40b4-9b4f-2659655f5c2f	undefined	82s7pnn2t8				Starting_TSV	Match	No Match
63516fedb9c880af1f305b30	"No download information provided, but article contains statement that ""the data will  be made available without undue reservation"""	New cohort of 524 enrolled patients, 988 mammography images divided in 494 malignant and 494 benign masses.Data was preprocessed to be useful for DL work.  An additional validation set from another hospital is also used (58 patients). Not used before	No stratification effort	Data split 744 (training) and 244 (test) in a random manner. No details in Pos/Neg split but assumed to be equal.  			not available	No comparison made with other approaches.	yes, Dejong's test	Confusion matrix, calculating AUC, accuracy, sensitivity, precision, and F-score. Statistical significance with Delong's test (P<0.05 was considered significant).	test set and independent validations set.  Cross-validation when determining the optimal configuration of the SVM. 			not available 	not mentioned	black box due to DL, but relatively transparent in the final classification step with SVM.  The question is how interpretable the DL features are. 	Benign or malignant mass.			Transfer learning with DL to extract features trained with stochastic gradient descent, SVM (linear kernel) for final classification into benign and malignant.	Not available.	Five preprocessing steps to prepare mammography images (identification of ROI, image and size normalisation, and data augmentation (flipping and rotations). Image size was  224x224x3 	Handcrafted (455), clinical (5) and DL-based features (1024-dimensional vector) are used in the final classifier. MRMR is used for feature selection, reducing the number of features to 30 Hcr and 27 DL features, next to the clinical ones.	There are 7x744 datapoints for training, yet this seems still limited given the complexity of the DL network to extract features. Not considering the DL, and only the SVM, things look better as the feature set is reduced to 62 features in total, so f > p.	Yes, for the DL feature extraction the VGG16 image-net trained network was used in combination with the Inception-V3 network into a fusion network.	The DL has its weights and layers next to the epoch, learning rate, momentum and weight decay parameters.  All parameters for the fusion network were transferred rom VGG16 and Inception V3 into a DL fusion network.  They added three additional FC layers.	SVM hyperparameters were tuned with grid-search and 10-fold cross-validation.  Tests were performed in an independent set and verified in an independent validation set.  Yet no stratification seems to be done.			6312169df3794236aa9879f2	33828982.0	PMC8019900	02/02/2026 19:46:52	Cui Y, Li Y, Xing D, Bai T, Dong J, Zhu J.	Frontiers in oncology	Improving the Prediction of Benign or Malignant Breast Masses Using a Combination of Image Biomarkers and Clinical Parameters.	10.3389/fonc.2021.629321	2021	0.0	0.0		1.0	2022-03-02T13:51:09.000Z	2026-02-02T19:46:52.000Z	bd99b02c-4c88-4e85-b466-4a97ef729643	undefined	ij8nvrxx7j				Starting_TSV	Match	No Match
63516fedb9c880af1f305b53	The datasets generated and/or analyzed during the present study are available in the TCGA database.	RNA-Seq expression values of immune regulatory molecules, including CTLA‚Äë4, IDO1, LAG3, PDCD1, PDL1 and TIM3 across four types of breast cancer (HER2, luminal A, luminal B and TNBC) from The Cancer Genome Atlas (TCGA) database. Data for tumor mutation burden (TMB) from TCGA database. 	Not specified	Not specified			Not available	Factors that were associated with immune response in this analysis were consistent with established knowledge on antitumor immunity.	OOB samples providing estimates of model error rate for the decision trees.	Out of bag (OOB) scores	Not specified			Not available	Not specified	Feature importance score, relative contribution of each factor to the resulting immune response, for determining the most important features for CYT.	Multi-label predictions			Random Forests	Not available	RNA-Seq expression values in transcripts per million (TPM) were log2-transformed and normalized.  Estimation of relative immune cell relative infiltration and abundance  by using the method single‚Äësample gene set enrichment analysis (ssGSEA) which identified gene sets from the Molecular Signatures Database that were enriched in TNBC RNA-Seq data. 28 heterogeneous immune cells were classified according to gene sets. The degree of immune cell infiltration was determined by the ssGSEA scores. The immune signature was clustered into 3 populations, namely high‚Äë, medium‚Äë and low‚Äëinfiltration.	Hundreds of variants, TMB data, were employed as input parameters, including the relative infiltration of the 28 types of heterogenous immune cells, somatic mutation counts, 78 immune‚Äërelated molecules, and 50 signaling pathways from the HALLMARK collection. In total 782 features were used to assess cytolytic activity (CYT).	Not specified	No	Not specified.	Not specified			6312169df3794236aa9879ed	32218835.0	PMC7068237	02/02/2026 19:46:52	Cheng J, Ding X, Xu S, Zhu B, Jia Q.	Oncology letters	Gene expression profiling identified TP53<sup>Mut</sup>PIK3CA<sup>Wild</sup> as a potential biomarker for patients with triple-negative breast cancer treated with immune checkpoint inhibitors.	10.3892/ol.2020.11381	2020	0.0	0.0		1.0	2022-03-02T11:14:34.000Z	2026-02-02T19:46:52.000Z	35561cc1-0576-4757-b30f-5bcd7b65ce52	undefined	yuk8w6yxtm				Starting_TSV	Match	No Match
63516fedb9c880af1f305b54	Declared availability upon request.	Functional connectivity (FC) patterns obtained from resting-state functional magnetic resonance imaging (RS-fMRI). 1) Npos=38 Schizophrenia (SCZ) patients and Nneg=38 healthy controls (HC) 2) The produced classifier was applied to Ntest=38 high-risk first-degree relatives (FDRs) to predict their cognitive performance. 	Not specified	Leave one- out cross-validation, with Ntrain=37 and Ntest=1 for each iteration.			Not available	Not applicable	1) Permutation statistical testing. 2) Semantic fluency test (animal version) was administered to evaluate the executive function and the semantic memory, which are severely affected in SCZ. The performance was analyzed using the number of correct words within 1 min. Statistical testing of the correlation between classification score and semantic fluency scores of FDR participants. 	Accuracy, sensitivity, specificity, ROC curve, AUC score.	Leave one-out cross-validation (LOOCV). Independent dataset of 142 features obtained by masking DMN, FP, auditory, and sensorimotor brain networks. Similar training of a Linear SVM classifier and evaluation of the resulting metrics. 			Not available	Not specified	Feature weight extraction from the Linear SVM model and feature selection for data dimension reduction, using F-score for feature ranking, were performed to interpret feature importance. This allowed for the identification of the 18 ROIs having weights that were at least 1 standard deviation greater than the average of the weights of all regions, thus making the greatest contribution to the model.	1) Binary predictions. 2) Classification score, which is the average of the 76 prediction labels. From a range of -1 to 1, a positive score indicates a SCZ pattern, and a negative score indicates a HC pattern. Binary predictions. 			Support Vector Machine with Linear kernel (Linear SVM)	No	The subjects‚Äô brains were parcellated into 200 regions of interest (ROIs) using the Craddock atlas. Œ§he RS-fMRI data were preprocessed to calculate FC measures between each pair of ROIs.  Friston-24 parameters were used to regress out the effects of head motion. To further reduce the effects of nuisance factors, signals from cerebrospinal fluid and white matter were also regressed out.  DARTEL toolbox was used to normalize the data and the resulting images were finally smoothed with a 6-mm full width at half maximum (FWHM) Gaussian kernel. The time series within each ROI were first band-pass filtered (0.01‚Äì0.08 Hz) and then averaged. For each participant, FC was calculated between each ROI using Pearson‚Äôs correlation coefficients, resulting in 19900-dimensional FC feature vectors for each subject. Patients with SCZ were labeled as 1, and HCs were labeled as -1. 	FC measurements between each pair of the 200 brain ROIs were used as classification features. Feature selection was performed for data dimension reduction using F-score for feature ranking. The 644 highest-ranked FC features were used to build the classifier.	Not specified	No	Linear SVM was implemented using the LIBSVM toolbox with the parameter C set to the default value of 1.	Yes. Linear SVM with C=1 and feature selection for data dimension reduction were selected to avoid overfitting.			6312169df3794236aa9879ed	33324147.0	PMC7725002	02/02/2026 19:46:52	Liu W, Zhang X, Qiao Y, Cai Y, Yin H, Zheng M, Zhu Y, Wang H.	Frontiers in neuroscience	Functional Connectivity Combined With a Machine Learning Algorithm Can Classify High-Risk First-Degree Relatives of Patients With Schizophrenia and Identify Correlates of Cognitive Impairments.	10.3389/fnins.2020.577568	2020	0.0	0.0		1.0	2022-03-01T14:21:27.000Z	2026-02-02T19:46:52.000Z	5a5db1d1-8e51-4dca-aee1-ec8f06e46300	undefined	5iyltalim1				Starting_TSV	Match	No Match
63516fedb9c880af1f305ba5	Yes. Stage-1 CNV calls are available in dbGAP as phs000267.v3.p2. Stage-2 CNV calls are available in dbGAP as phs000267.v4.p2. Supplementary files for rare variants of ASD subjects and controls.	Rare Copy Number Variation (CNV) data and comprehensive gene annotations from Npos=1892 Autism Spectrum Disorder ASD subjects (1623 males and 270 females), and Nneg=2342 platform-matched controls (1093 males and 1250 females) with at least one rare CNV (frequency 1% or less). All subjects (Npos) are of European ancestry and Caucasian ethnicity. Data were collected from previous studies: Autism Genome Project (AGP), SAGE (Study of Addiction Genetics and Environment), Ontario Colorectal Cancer study, HABC (Health Aging and Body Composition). 	Subjects with karyotypic abnormalities, Fragile X syndrome or other genetic syndromes causing congenital malformations were excluded from the analysis. Only samples meeting quality thresholds were used for CNV analysis. CNVs (of size 30 kb or greater) were detected using an analytical pipeline optimized for Illumina 1M arrays. All de novo CNVs were experimentally validated. Samples with copy number variation greater than 7.5 MB were excluded. 	Only subjects harboring at least one rare genic CNV were used for classification, as features would be constantly zero for the other subjects, but all subjects were considered when reporting percentage ‚Äúexplained‚Äù statistics. This resulted in a subset of Npos=1570 ASD subjects (80.8%) and Nneg=1916 controls (81.8%). Random division into 3 equal and stratified subsets. Ntrain = 2 subsets. Ntest=1 subset. This process was repeated 3 times without re-dividing the dataset. 			Not available.	Comparison and evaluation of algorithms‚Äô performance by splitting data into:  1) all subjects,  2) subjects with de novo CNVs, and  3) subjects with pathogenic CNVs Moreover, evaluating algorithms‚Äô performance by splitting observations (CNVs) into: 1) Total CNVs,  2) gain CNVs, and  3) loss CNVs Additionally, evaluating algorithms‚Äô performance by separating features, following feature selection, into: 1) top 20 ranking features 2) top 15% ranking features 1) top 40% ranking features CF reported as an optimal classification approach based on comparisons with different algorithms and the respective AUC scores. 	Not reported.	AUC score. Percentage of correctly classified ASD subjects, which was calculated as the number of ASD subjects correctly predicted in at least 15 out of 20 iterations divided by the study total (Npos=1892). 	Stratified 3-fold cross-validation.			Not available.	Not reported.	Feature selection for RF based on Mean Decrease Accuracy (MDA) and Mean Decrease Gini. Feature selection for CF based on MDA was performed with and without step-wise decorrelation, and on MRMR (Minimum Redundancy Maximum Relevance Feature Selection). Reported in Additional information. Black box for SVM with Linear kernel and Neural network. 	Prediction probabilities. Binary predictions.			Random Forest (RF), Conditional Inference Forest (CF), SVM with Linear kernel, Neural network.	Not available.	Clinical categorization of de novo and inherited CNVs as pathogenic, uncertain or benign following clinical annotation guidelines. Large and very rare CNVs were also classified as pathogenic.  Gene annotations based on CNVs. Gene-set construction, based on CNV gene annotations.	Clinically categorized CNVs. CNV annotated genes. 20 curated gene-sets of neurobiological relevance. Total gene count.	Stratified 3-fold cross-validation was used to avoid overfitting.  The absence of overfitting was further assessed by replacing real classification features with randomized features based on gene identity permutation.  Feature selection was based on the feature relevance metrics calculated on the data subset used for training, and performed independently for every training set, to avoid any overfitting issues. Additionally, all subsets presented a gender composition similar to the full dataset. 	No	For RF and CF default settings were used unless otherwise specified. For Linear SVM the cost parameter was kept at default as 1 and class weights were kept even. Each feature was independently normalized and rescaled to a 0-1 interval prior to being input into the classifier. The Neural Network was built with two middle layers of 100 and 50 nodes each, a learning rate of 0.005 with a 0.9 momentum. The network was trained through back-propagation, and without feature normalization or scaling. 	Not specified.			6312169df3794236aa9879ed	25783485.0	PMC4315323	02/02/2026 19:46:52	Engchuan W, Dhindsa K, Lionel AC, Scherer SW, Chan JH, Merico D.	BMC medical genomics	Performance of case-control rare copy number variation annotation in classification of autism.	10.1186/1755-8794-8-s1-s7	2015	0.0	0.0		1.0	2022-02-28T13:11:53.000Z	2026-02-02T19:46:52.000Z	d692ec44-aeae-4a41-9383-a899977f51f3	undefined	mhyzrdvtd1				Starting_TSV	Match	No Match
63516fedb9c880af1f305bb3	Yes. Additional information.	1) First dataset of protein sequences for identification experiments of DNA repair proteins. Gene Ontology (GO) annotated proteins from PDB. Npos=557 and Nneg=1443 total protein sequences. Npos=114 and Nneg=353 protein sequences with 90% sequence similarity. Npos=76 and Nneg=215 protein sequences with 50% sequence similarity. 2) Second dataset of protein sequences for identification experiments of DNA repair proteins. Gene Ontology (GO) annotated proteins from Uniprot: - Protein sequences of ‚ÄúBase Excision Repair‚Äù, Npos=2624 and Nneg=4723 for total protein sequences, Npos=1721 and Nneg=2924 protein sequences with 90% sequence similarity, and Npos=630 and Nneg=1200 protein sequences with 50% sequence similarity. - Protein sequences of ‚ÄúDNA Dealyklation‚Äù, Npos=25 and Nneg=7322 for total protein sequences. - Protein sequences of ‚ÄúDNA synthesis during DNA repair‚Äù, Npos=28 and Nneg=7319 for total protein sequences. - Protein sequences of ‚ÄúDouble Strand Break repair‚Äù, Npos=364 and Nneg=6983 for total protein sequences, Npos=266 and Nneg=4379 protein sequences with 90% sequence similarity and Npos=174 and Nneg=1656 protein sequences with 50% sequence similarity. - Protein sequences of ‚ÄúError-prone DNA repair‚Äù, Npos=46 and Nneg=7301 for total protein sequences, Npos=36 and Nneg=4609 protein sequences with 90% sequence similarity. - Protein sequences of ‚ÄúMismatch repair‚Äù, Npos=1777 and Nneg=5570 for total protein sequences, Npos=1020 and Nneg=3625 protein sequences with 90% sequence similarity and Npos=468 and Nneg=1362 protein sequences with 50% sequence similarity. - Protein sequences of ‚ÄúNucleotide Excision Repair‚Äù, Npos=2106 and Nneg=5241 for total protein sequences, Npos=1325 and Nneg=3320 protein sequences with 90% sequence similarity and Npos=363 and Nneg=1467 protein sequences with 50% sequence similarity. - Protein sequences of ‚ÄúPostreplication repair‚Äù, Npos=28 and Nneg=7319 for total protein sequences. - Protein sequences of GO ‚ÄúRegulation of DNA repair‚Äù, Npos=264 and Nneg=7083 for total protein sequences, Npos=174 and Nneg=4471 protein sequences with 90% sequence similarity, and Npos=114 and Nneg=1716 protein sequences with 50% sequence similarity. - Protein sequences of GO ‚ÄúSingle Strand Break repair‚Äù, Npos=40 and Nneg=4605 for total protein sequences, and Npos=25 and Nneg=1805 protein sequences with 90% sequence similarity. 3) 31 vertebrate genomes from ENSEMBL for identification of novel, DNA repair-related proteins. 	Overlaps between protein sequence datasets of 0%(unfiltered), 50% and 90% sequence similarity. Only DNA repair pathways which were consisted of at least 25 proteins, were considered. Removed redundancy of protein sequences which belong to more than one pathway. Removed proteins which contain the following keywords in their GO descriptions: putative, similar, possible/possibly, probable/probably, theoretical, and hypothetical.	5-fold and one-versus-one-versus-rest cross-validation. Not specified further.			Yes. Additional information.	The transformation based SVM experimental results were compared with independent BLAST trials. A continuum of thresholds was used in order to obtain ROC curves and compare different techniques. 	Pairwise comparisons of classifiers using both the parametric t-test and non-parametric Wilcoxon Signed-Rank Test.	ROC curves, AUC scores	1) 5-fold cross-validation 2) One-versus-one-versus-rest cross-validation. This technique differs in that for k-fold validation, one portion is still set aside for evaluation, but instead of k - 1 portions of data for training, only a single portion is used for training, and the k - 2 remaining portions are used as a reference homology database for querying training and test data. The end goal this technique is to combine homology and sequence data in an unbiased way and obtain a realistic estimate of method performance. 			Yes. Web server: https://sunflower.kuicr.kyoto-u.ac.jp/~jbbrown/dnaRepairPrediction/v2/index.py	Yes. Available report in Additional information.	Comparing model performance following training on datasets with different features.	Binary predictions. SVM scores or BLAST e-value as outputs, and comparisons with thresholds to determine the prediction (DNA repair protein or Not)			Support Vector Machine (SVM)	Not available	Different data processing methods for transforming input protein sequences: 1) Tripeptide (k=3) frequency in protein sequence: Considering all possible subsequences of length k in the query protein sequence and dividing the number of occurrences of each k-mer by the total number of possible k-mers. 2) Secondary structure: For each protein in each dataset SSPro was used to obtain a secondary structure role for each amino acid in the protein in question. 3) Frequency Priors: Scanning database of known DNA repair proteins for the frequencies of each type of amino acid, and using this information to determine if a query protein not belonging to the known database has a similar percentage of each type of amino acid (vf+) or not (vf-). 4) BLAST homology: Homology positive or negative assessment and return of value, vb+ or vb-, if BLAST e-value is lower or higher than a 0.001 threshold, respectively. 	1) Primary Sequence 2) Primary Sequence and Secondary Structure 3) Primary Sequence and Frequency Priors 4) Primary Sequence and Homology 5) Primary Structure, Secondary Structure, Homology 6) BLAST Homology 	Not available	Yes. Utilization of BLAST in protein sequences data encoding for homology features.	Not available	Yes, manually setting the gamma value of the radial basis function (RBF) similarity-metric.			6312169df3794236aa9879ed	19154573.0	PMC2660303	02/02/2026 19:46:52	Brown JB, Akutsu T.	BMC bioinformatics	Identification of novel DNA repair proteins via primary sequence, secondary structure, and homology.	10.1186/1471-2105-10-25	2009	0.0	0.0		1.0	2022-02-23T23:30:01.000Z	2026-02-02T19:46:52.000Z	10fa32b7-b2d7-4aeb-9de4-351d2c086bf9	undefined	y6wfiqd2mt				Starting_TSV	Match	No Match
63516fedb9c880af1f305b32	Alzheimer ‚Äôs Disease Neuroimaging Initiative (ADNI) (http://www.adni-info.org)	Alzheimer ‚Äôs Disease Neuroimaging Initiative (ADNI), online database. Dataset size: 994. Split into 2 dataset: CTH, n= 650 and HCV, n= 299.	Not stated.	CTH test set, n=167, training set n=483. HCV test set n=120, training set n=179.			no.	Direct, Direct VOI, STAND-score, Atlas, COMPARE, CTH-SVM, CTH-J48,  CTH-NB, NTI, ROI, Feature Vector, HCV-SVM, HCV-J48, HCV-NB, Volume- SPM5, Volume-FreeSurfer, Shape. These are machine learning techniques used for mild cognitive impairment patients classification. Accuracy is compared in Table 1 of the paper.	Not stated.	Accuracy.	Not stated.			no.	Not stated.	Black box.	Classification.			support vector machine (SVM) for which a linear C-SVM algorithm was applied, ii) decision trees, for which a Java open source implementation of the C4.5 algorithm (the J48 algorithm) was used and iii) the Naive Bayes (NB) classifier.	no.	Not stated.	Features based on cortical thickness (CTH) and hippocampal volumes (HCV) extracted from brain scans were used to train the learning algorithms.	Not stated.	no.	Not stated.	Not stated.			6312169df3794236aa987a03	33510068.0	PMC8328792	02/02/2026 19:46:52	Skolariki K, Terrera GM, Danso SO.	Neural regeneration research	Predictive models for mild cognitive impairment to Alzheimer's disease conversion.	10.4103/1673-5374.306071	2021	0.0	0.0		1.0	2022-02-23T14:26:24.000Z	2026-02-02T19:46:52.000Z	31990540-50aa-4378-b78d-4ee10a62bdda	undefined	7kuh4ta7dl				Starting_TSV	Match	No Match
63516fedb9c880af1f305b9b	No	27 electrical penetration graph (EPG) waveform recordings  totaling 470 hours on nine different citrus varieties. Not previously used.	1) A classification model was trained and used for predictions on each EPG recording independently.  2) Random selection of 5% of 26 EPG recordings used as training and 1 EPG recording as testing. 27 repeats (Leave-one-out cross-validation). 	1) Data split for each EPG recording. Ntrain = 5%. Ntest = 95%. 3 splits. 2) Ntrain = 5% from each of 26 EPG recordings. Ntest = 100% from 1 EPG recording (27th). 			No	Not applicable	95% confidence intervals. Confusion matrices.	Average of: Accuracy, Sensitivity, Specificity, Positive Predictive value, Negative Predictive value, Prevalence, Detection Rate, Detection Prevalence, Balanced Accuracy.	1) 3 repeats of 10-fold cross validation for each EPG recording. 2) Leave-one-out cross-validation. 3 repeats of 10-fold cross validation for 5% of 26 EPG recordings (train). 1 EPG recording for testing. 			Not available	Not available	Not reported	Multi-label predictions on 6 different feeding states. Binary predictions between phloem (E1 and E2) and non-phloem (C, D, NP, and G) feeding states.			Random Forests Classification	No	EPG waveform recordings were manually classified into 6 feeding states: C, D, E1, E2, G, NP. Fast Fourier transform of EPG recordings. The 6 frequencies with the highest magnitudes, often harmonics, were extracted and used for further analysis.	Main periodic components of the time series	No	No	Not applicable	No			6312169df3794236aa9879ed	27832081.0	PMC5104375	02/02/2026 19:46:52	Willett DS, George J, Willett NS, Stelinski LL, Lapointe SL.	PLoS computational biology	Machine Learning for Characterization of Insect Vector Feeding.	10.1371/journal.pcbi.1005158	2016	0.0	0.0		1.0	2022-02-21T23:04:51.000Z	2026-02-02T19:46:52.000Z	4df4e637-ba5a-4be5-83d3-aed3e3c4742c	undefined	du3gc2b5fz				Starting_TSV	Match	No Match
63516fedb9c880af1f305b35	Yes. URL for waltz database: http://waltzdb.switchlab.org/	Waltz database of hexapeptides, classified in amyloidogenic (pos) and nonamyloidogenic (neg). N_pos = 541; N_neg = 901. Dataset used in other papers. Dataset enriched with physicochemical proprieties of amino-acids derived from AAindex. 	The two dataset have no overlaps.	Dataset splitted in test set (N_pos = 158; N_neg = 309) and training set (N_pos = 383; N_neg = 592)			No.	Compared with another algorithm, but the comparison is only based on the accuracy.	No confidence intervals or statistical significance stated.	Accuracy, ROC curve, Area Under Curve (AUC)	Prediction on the test set.			The classiffier is available at: https://pitgroup.org/bap/; the source code is not available.	Not stated	Transparent. The support vector machine linearly separates samples based on the physicochemical features of the aminoacids.	Classification (amyloidogenic or nonamyloidogenic)			SVM	No.	Aminoacid sequences enriched with physicochemical proprieties from AAindex (for each amino-acid)	3318. No feature selection strategy.	The number of features is huge, but no strategy for overfitting prevention was adopted.	No	3319	No.			6312169df3794236aa987a01	33810341.0	PMC8067080	02/02/2026 19:46:52	Keresztes L, Sz√∂gi E, Varga B, Farkas V, Perczel A, Grolmusz V.	Biomolecules	The Budapest Amyloid Predictor and Its Applications.	10.3390/biom11040500	2021	0.0	0.0		1.0	2022-02-18T08:57:44.000Z	2026-02-02T19:46:52.000Z	b3b4561c-e904-4def-89cd-783ff01a3b12	undefined	dmb4u51v5o				Starting_TSV	Match	No Match
63516fedb9c880af1f305b36	Yes. They used a public dataset published in a previous paper. Paper doi: 10.1109/ACCESS.2020.3010287	"Public dataset of Chest X Rays scans labeled for ""normal"", ""viral pneumonia"", and ""COVID-19"". N_normal = 1341; N_viral_pneumonia = 1345; N_covid19 = 219. Used in previous papers."	Only one dataset is used. They used 5-fold cross-validation on the dataset for evaluation.	No test or validation set used. They used 5-fold cross-validation on the dataset for evaluation. Distribution of the classes in the splits not stated.			No	Compared with other methods presented in literature. 	No confidence intervals or statistical significance provided.	Accuracy, F1 score, Recall, and Precision	5-fold cross-validation 			No	Not stated	Black box.	Classification			Combination of three DenseNet-121, the output of which was combined in a final fully connected layer. 	No.	The classifier input data are generated by an unsupervised ML algorithm (MS-AdaNet). No transformation is stated for the input data of MS-AdaNet.	The inputs of the algorithm are three images. The definition of the images is not stated.	Not stated	"Yes. The classifier input data are generated using an unsupervised ML algorithm extracting three image features from Chest X Rays scans. Such unsupervised ML algorithm is a convolutional adversarial network (they call that ""MS-AdaNet"").  They train and test MS-AdaNet on a independent dataset to prove its ability in extrating features. However they trained this algorithm also on the dataset use for classification when the MS-AdaNet is used as input for the classifier."	Not stated.	No overfitting prevention strategy is mentioned.			6312169df3794236aa987a01	34388102.0	PMC8843059	02/02/2026 19:46:52	An J, Cai Q, Qu Z, Gao Z.	IEEE journal of biomedical and health informatics	COVID-19 Screening in Chest X-Ray Images Using Lung Region Priors.	10.1109/jbhi.2021.3104629	2021	0.0	0.0		1.0	2022-02-17T09:36:07.000Z	2026-02-02T19:46:52.000Z	621ff42a-1daa-4b6d-b190-e35478b9af68	undefined	kr8wmng69g				Starting_TSV	Match	No Match
63516fedb9c880af1f305b56	Dataset 1: Yes. URL: https://portal.gdc.cancer.gov/projects/TCGA-HNSC  Dataset 2: Yes. URL: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE41613 	Dataset 1: RNA-Seq data from The Cancer Genome Atlas for Head and Neck Squamous Cell Carcinoma (TGCA-HNSCC). N = 544; N_pos (survived patients) = 211, N_neg (not survived patients) = 280. Used in the community.  Dataset 2: GSE41613 from the Gene Expression Omnibus (GEO). N = 97; N_pos (survived patients) = 46, N_neg (not survived patients) = 50. Used in the community.	Dataset 1 and Dataset 2 are independent	Dataset 1: splitted in 50% for training and 50% for test. Split was conducted randomly. N_pos and N_neg for each subset not stated.  Dataset 2: entirely used for test.			No.	The method was not compared to others.	Not provided.	area under curve (AUC)	Independent dataset (using Dataset 2)			No	Not stated.	Transparent. The model generates a risk score according to the expression of the 8 selected genes.	Regression			One class logistic regression. 	No.	Cox regression analysis, least absolute shrinkage and selection operator (lasso), and Akaike information criterion (AIC) were used to identify the relevant genes to train the logistic regression model.	8 input features selected using Cox regression analysis, least absolute shrinkage and selection operator (lasso), and Akaike information criterion (AIC)	Not clearly stated, but could be inferred from the text that p is smaller than N. 	No.	Not clearly stated, could be inferred from the text.	Not stated.			6312169df3794236aa987a01	33329703.0	PMC7721480	02/02/2026 19:46:52	Tian Y, Wang J, Qin C, Zhu G, Chen X, Chen Z, Qin Y, Wei M, Li Z, Zhang X, Lv Y, Cai G.	Frontiers in genetics	Identifying 8-mRNAsi Based Signature for Predicting Survival in Patients With Head and Neck Squamous Cell Carcinoma via Machine Learning.	10.3389/fgene.2020.566159	2020	0.0	0.0		1.0	2022-02-16T09:43:09.000Z	2026-02-02T19:46:52.000Z	17ff2958-4461-4d8f-9c75-f7e3992a9bde	undefined	i4p484v6d2				Starting_TSV	Match	No Match
63516fedb9c880af1f305b8d	no.	The data comes from a direct experiment. They consider firing rates of ~200 ganglion cells.	Not stated.	Training set: ~80% of the data. Test set: ~20% of the data.			no.	Comparison between models used in the paper.	Median and variance of cells across models.	Not stated.	Not stated.			no.	Not stated.	black box.	The model gives as output the neuron's spike rate (time course of the firing rate).			Single linear-nonlinear cascade models (LN), linear-nonlinear-sum-nonlinear (LNSN), linear-nonlinear-sum-nonlinear-feedback (LNSNF), linear-nonlinear-feedback-sum- nonlinear-feedback (LNFSNF) and LNFDSNF models. (to predict firing rates of ganglion cells)	no.	Not stated.	Not stated.	Fitting is performed but the fitting algorithm is not explained.	no.	~100-150 parameters	"yes. A ""fitting algorithm"" is mentioned but the technique is not explained."			6312169df3794236aa987a03	28065610.0	PMC5821114	02/02/2026 19:46:52	Real E, Asari H, Gollisch T, Meister M.	Current biology : CB	Neural Circuit Inference from Function to Structure.	10.1016/j.cub.2016.11.040	2017	0.0	0.0		1.0	2022-02-15T17:17:01.000Z	2026-02-02T19:46:52.000Z	e735e3a3-1291-4134-b91b-e7eed358f5e6	undefined	ebmz3cuuuq				Starting_TSV	Match	No Match
63516fedb9c880af1f305baf	The American Seashell book and a list of PubMed Central ids used for evaluation of NetiNeti can be found at http://ubio.org/netinetifiles	Data source are databases.	Not stated.	A total of about 40,000 positive examples together with another set of about 43,000 negative examples were used to generate a training set of 83,000 examples for the two class labels.			no.	TaxonFinder and FAT tool.	The recall for TaxonFinder is significantly lower compared to NetiNeti, while the precisions are comparable. . The FAT approach has lower precision and recall values compared to NetiNeti and TaxonFinder.	Precision and recall values.	PubMed Central ids used for evaluation of NetiNeti.			The software system implementing NetiNeti can be accessed at http://namefinding.ubio.org.	Not stated.	black box.	Classification.			Probabilistic machine learning algorithms like Na√Øve Bayes and Maximum Entropy.	no.	Strings of text are tokenized and pre-filtered to select candidates.	4-5 features are indicated.	Not stated.	no.	The parameters are estimated via hill climbing approaches (Improved Iterative Scaling (IIS), Generalized Iterative Scaling (GIS)) and Limited-Memory Variable Metric optimization (L-BFGS). The number of parameters is not stated.	no.			6312169df3794236aa987a03	22913485.0	PMC3542245	02/02/2026 19:46:52	Akella LM, Norton CN, Miller H.	BMC bioinformatics	NetiNeti: discovery of scientific names from text using machine learning methods.	10.1186/1471-2105-13-211	2012	0.0	0.0		1.0	2022-02-11T16:40:03.000Z	2026-02-02T19:46:52.000Z	7207b60b-e1a1-41bd-bc26-c3720ecbca51	undefined	fv16isykca				Starting_TSV	Match	No Match
63a25db2e8edf6ce46f6e84b	in supplementary files	they created the dataset	not applicable	training set: 40 (23 active and 17 inactive) patients & 24 healthy, validation set: 26 (18 active and 8 inactive) patients & 16 healthy			no	novel approach	svms accuracy	ROC, AUC, sensitivity, specificity	indipendent dataset			no	not mentioned	Black box	classification: binary predictions			SVM	No	global features invlude various clinical characteristis of sampes.	4(Plasma samples were used to measure cell-free DNA, NE-DNA, MPO-DNA, and citH3-DNA complexes from training and validation sets.)	Not applicable	No	Grid search and Gaussian radial basis function kernels were implemented for tuning parameters.	Not applicable			6312169df3794236aa987a1b	33240258.0	PMC7680913	02/02/2026 19:46:52	Jia J, Wang M, Ma Y, Teng J, Shi H, Liu H, Sun Y, Su Y, Meng J, Chi H, Chen X, Cheng X, Ye J, Liu T, Wang Z, Wan L, Zhou Z, Wang F, Yang C, Hu Q.	Frontiers in immunology	Circulating Neutrophil Extracellular Traps Signature for Identifying Organ Involvement and Response to Glucocorticoid in Adult-Onset Still's Disease: A Machine Learning Study.	10.3389/fimmu.2020.563335	2020	0.0	0.0		1.0	2022-02-09T15:02:29.000Z	2026-02-02T19:46:52.000Z	8c0c94cd-3172-4f65-92b0-b997742324e0	undefined	bd02oq0dn5				Starting_TSV	Match	No Match
63516fedb9c880af1f305b58	Yes, supposedly. Datasets are supposed to be available through URL (https://zhulab.ahu.edu.cn/m5CPred-SVM/) but link does not respond	Yes: source of data. Used by previous papers.	Yes: procedure to remove sequences with similarity >70% is applied in negative and positive datasets	Yes: size of training, validation and test sets as well as distribution of N_pos and N_neg are given.			No	Comparison of SVM with other classifiers (KNN, Adaboost, random forests, decision tree, logistic regression and XGBoost) on the same cross-validation dataset. Comparison of their best method (SVM) with five other existing methods with webserver availability (RNAm5Cfinder, iRNA-m5C, iRNAm5C-PseDNC and RNAm5Cpred, PEA-m5C). Table presenting algorithm class and features used by these methods is presented.	List of so-called ‚Äúsignificantly higher‚Äú performance metric values in favor of their method, but without confidence interval or explicit p-values or even name of test performed. 	Accuracy, sensitivity, specificity, precision, Matthews correlation coefficient and F1-score. Area under ROC and PRC	Independent test set			Website URL (but link is broken)	No	Transparent at the level of the feature selection (evaluation results for different combinations of features), put in connection with mean sequence difference between positive and negative sets	Classification			SVM (FITCSVM)	No. URL given for model availability but link is broken (not sure what exact information was available in the URL)	Yes: specific sequence features (k-nucleotide frequency (KNF), k-spaced nucleotide pair frequency (KSNPF), position-specific nucleotide propensity (PSNP), k-spaced position-specific dinucleotide propensity (KSPSDP), pseudo dinucleotide composition (PseDNC), Chemical property with density (CPD)) 	Yes: number of features and feature selection on the ten-fold cross-validation dataset using sequential forward feature selection (SFS)	No exclusion mentioned	No	Yes: 2 (box constraint and kernel scale) Selected through grid search on ten-fold cross-validation dataset	Yes (sequential forward feature selection (SFS) to reduce redundant features)			6312169df3794236aa9879f4	33126851.0	PMC7602301	02/02/2026 19:46:52	Chen X, Xiong Y, Liu Y, Chen Y, Bi S, Zhu X.	BMC bioinformatics	m5CPred-SVM: a novel method for predicting m5C sites of RNA.	10.1186/s12859-020-03828-4	2020	0.0	0.0		1.0	2022-02-09T14:43:24.000Z	2026-02-02T19:46:52.000Z	9d6ac10d-1ae3-42d0-bc30-3a9363a41b6f	undefined	lmmde595pw				Starting_TSV	Match	No Match
63516fedb9c880af1f305b8e	No	Exome sequencing data for 1000 samples. Npos = 500 (control group). Nneg = 500 (disease group). Ntrain = 500. Ntest = 500. Data were provided from the Regents of the University of California under the challenge ‚ÄúBipolar Exomes‚Äù.	Random sampling from the pool of 1000 samples and balanced datasets between train, validation, and test datasets.	Npos,train = 200. Nneg,train = 200. Validation set present. Npos,validation = 50. Nneg,validation = 50. Npos,test = 249. Nneg,test = 251.			No	Decision trees and random forests.	Higher performance metrics of the convolutional neural network (e.g., accuracy = 0.65) than those of the comparing algorithms (accuracy = 0.55). Confidence intervals were not calculated, and statistical testing was not applied.	Accuracy, Precision, Recall, F1-Score, AUC score, ROC curve.	Cross-validation			Not available	Training the model end-to-end takes close to 6 hours on Nvidia Tesla M40 servers.	Black box. Reduced performance due to repeated feature subsampling supports the importance of all features in model performance and the absence of sequencing method artifacts.	Classification with binary predictions.			Deep convolutional neural network trained using gradient descent by standard backpropagation algorithm.	No.	Moving window length across the input sequences by the convolutional kernels and formation of feature maps. One-hot-encoding was used to create vectors for all the types of genotypes available, to ensure that all the categories are equidistant from each other.	Featured maps formed from filtered exonic variants, one-hot encoded per chromosome.	Application of a batch normalization layer after the max-pooling layer contributes to avoiding overfitting the data. Additionally, balanced training, validation and testing datasets were used.	No	The deep convolutional network takes in a 23-channel input, with each input being the one-hot encoded variants of a chromosome. Initializing weights to reduce the loss function at each epoch.	Yes. Reduction of feature dimension using L1 regularization with penalty parameter C = 0.85. Less than 1% of the total features remain.			6312169df3794236aa9879ed	28600868.0	PMC5656045	02/02/2026 19:46:52	Sundaram L, Bhat RR, Viswanath V, Li X.	Human mutation	DeepBipolar: Identifying genomic mutations for bipolar disorder via deep learning.	10.1002/humu.23272	2017	0.0	0.0		1.0	2022-02-08T23:45:07.000Z	2026-02-02T19:46:52.000Z	a17d89ff-989f-4293-97a5-94abd04679d9	undefined	b365zwv48h				Starting_TSV	Match	No Match
63516fedb9c880af1f305b81	Yes (supporting information and website URL)	Yes (The Cancer Genome Atlas)	No dataset split  (unsupervised machine learning)	No dataset split  (unsupervised machine learning) Labels used to assess performance are given in raw datasets			Yes (supporting information)	Comparison of logranks‚Äô test p-values and number of enriched clinical parameters	No. But no claiming of performance difference	Logrank test (differential survival between clusters) Chi-squared test and Kruskal-Wallis (clinical labels (six) enrichement in clusters)	Extrinsic measures (clinical labels)			Yes (GitHub)	Yes (runtime in seconds on Windows desktop for eight methods and Linux cluster for one methods)	Black box	Produces clusters			Multi-view clustering methods (early and late integration, similarity based, dimension reduction and statistical methods)	No	No, not in detail. Class of feature used are described (gene sequence, expression and methylation) but no detail on encoding (only reference to source of data)	Yes, indirectly: processed raw data and algorithm for feature selection are given	No exclusion mentioned	No	Number of clusters: algorithm for selection is presented (elbow method)  Other parameters are not directly given but protocol used to select them are given (= if available: adherence to the guidelines given by the packages developers) 	Yes, regularization may be used by some methods (but not clear in the publication what was really implemented in the used packages) e.g. Least Absolute Shrinkage and Selection Operator regularization is used by iCluster Nuclear norm regularization is used by LRACluster A regularization term is used by rMKL-LPP Sparsity regularization is used by Canonical Correlation Analysis (CCA)			6312169df3794236aa9879f4	30295871.0	PMC6237755	02/02/2026 19:46:52	Rappoport N, Shamir R.	Nucleic acids research	Multi-omic and multi-view clustering algorithms: review and cancer benchmark.	10.1093/nar/gky889	2018	0.0	0.0		1.0	2022-02-08T16:25:04.000Z	2026-02-02T19:46:52.000Z	10dbaffe-4a11-44af-a1ac-7efd84988759	undefined	7cpzfr4v7k				Starting_TSV	Match	No Match
63516fedb9c880af1f305ba6	No	Yes (Mexican Reference Genomic DNA Collection (MGDC-REF) ). Source of data and number of positives and negatives given, negative dataset was used by a previous paper.	No, only training set	Yes: number of N_pos and N_neg No data splits. Only training set			Yes: confusion matrix in publication	Univariate statistical analysis (Fishers‚Äô exact test) and decision tree classifier (J48 ID3)	Chi-squared test p-value comparison	chi-squared test	Evaluation on the training dataset			Yes (URL http://www.cs.waikato.ac.nz/~ml/weka/index.html)	No	Transparent: 24 most frequent rules generated by the apriori algorithm are detailed	Binary prediction (statistical classifier)			‚ÄúA priori‚Äù algorithm	No	Yes: Presence/absence of genes	Yes (13 features)	No exclusion	No	2 parameters (minimum support threshold and confidence levels)	No			6312169df3794236aa9879f4	26495028.0	PMC4606520	02/02/2026 19:46:52	Rodr√≠guez-Escobedo JG, Garc√≠a-Sep√∫lveda CA, Cuevas-Tello JC.	Computational and mathematical methods in medicine	KIR Genes and Patterns Given by the A Priori Algorithm: Immunity for Haematological Malignancies.	10.1155/2015/141363	2015	0.0	0.0		1.0	2022-02-08T15:10:18.000Z	2026-02-02T19:46:52.000Z	cf9dcd08-f887-4f92-837d-6a795fe980ec	undefined	wa4n4o4gna				Starting_TSV	Match	No Match
63516fedb9c880af1f305b37	Yes, https://chemicalchecker.org/	Public database	Not reported	Size not reported. 80:20 train-test split			No	Not reported	Not applicable	Accuracy	Cross-validation			https://gitlabsbnb.irbbarcelona.org/packages/signaturizer	Not reported	Black box	Classification			Neural network	No	n-dimensional vectors	3200	Not reported	No	Not reported	Yes, Global orthogonal regularization (alpha = 1)			6312169df3794236aa9879f6	34168145.0	PMC8225676	02/02/2026 19:46:52	Bertoni M, Duran-Frigola M, Badia-I-Mompel P, Pauls E, Orozco-Ruiz M, Guitart-Pla O, Alcalde V, Diaz VM, Berenguer-Llergo A, Brun-Heath I, Villegas N, de Herreros AG, Aloy P.	Nature communications	Bioactivity descriptors for uncharacterized chemical compounds.	10.1038/s41467-021-24150-4	2021	0.0	0.0		1.0	2022-02-08T00:18:31.000Z	2026-02-02T19:46:52.000Z	4d57e85d-7eba-478f-9400-c8f4acb9fdcc	undefined	1bn52lke7u				Starting_TSV	Match	No Match
63516fedb9c880af1f305bb1	Method and data are available to the public upon request.	Redundant dataset of type III effectors. Positive cases were identified type III effector (T3SE) proteins from Pseudomonas syringae (P. syringae) pv. tomato strain DC3000, P. syringae pv. syringae strain B728a and P. syringae pv. phaseolicola strain 1448A. Negative cases were all the proteins extracted from the genome of P. syringae pv. tomato strain DC3000, excluding proteins related to type III secretion system (T3SS) and hypothetical proteins. 4,062 proteins total. Npos = 283 protein sequences. Nneg = 3,779 protein sequences.  Non-redundant dataset of type III effectors. Removal of all homologous proteins (redundant), with sequence similarity greater than 60% from redundant dataset of type III effectors. 3,532 proteins total. Npos = 108 protein sequences. Nneg = 3,424 protein sequences. Only the first 100 N-terminal residues were used in both datasets. They were not previously used.	Homology present within positive cases. Possibility of overestimating negative cases due to the presence of uncharacterized T3SE proteins within them.	5-fold cross validation was performed. No information about the sizes of training and testing data sets.			Method and data are available to the public upon request.	EffectiveT3, T3SS prediction. Known classifiers for T3SS effector prediction. SVC with cross-validation but with different data encoding. 	Only the non-redundant data were used for the comparisons. Very low precision from EffectiveT3 (recall of 72.2% and precision of 17.9%) and T3SS prediction (recall of 83.3% and precision of 24%) due to being developed in less imbalanced and non-realistic training sets. For different data encoding methods, the recall and precision of effectors were 55.6% and 84.5%, respectively, which were over 5% lower than those of the proposed SSE-ACC method. 	Accuracy, Recall, Precision.	5-fold cross-validation. Independent dataset. A single predicted positive case of the independent dataset was validated through wet-lab experiments.			Method and data are available to the public upon request.	The whole computation process took several ten hours. All computation tasks were conducted on a Pentium IV desktop PC with dual CPU (2.8 GHz) and 2 GB RAM.	Black box. Not investigating feature importance.	Classification, i.e. binary predictions based on the probability of a predicted positive case being p > 0.01.			Support Vector Machine (SVM) classification with RBF kernel	Method and data are available to the public upon request.	Calculated amino acid composition in terms of different secondary structures strand (E), helix (H) and coil (C), and solvent accessibility states (buried (B) and exposed (E)), with a method called SSE-ACC. The value of each dimension is calculated by  f_i^j=  (N_i^j)/L  where j = {H, E, C}, N_i^j is the frequency of amino acid i in secondary structure element j, and L is the length of the sequence. The value is calculated similarly for solvent accessibility states. 	100 features. The first 60 features are used to describe the frequency of each amino acid in each of the three possible secondary structure elements. The last 40 dimensions represent the frequency of each amino acid having each of the two possible solvent accessibility states.	Single input for SVM method. Optimization through grid search.	No	The parameters used for the redundant data set are g = 0.25, C = 4. The parameters used for the non-redundant data set are g =0.5, C = 4. Grid search was used for parameter optimization.	Yes, by setting the regularization parameter C=4. L2 regularization adds an L2 penalty equal to the square of the magnitude of coefficients.			6312169df3794236aa9879ed	20122221.0	PMC3009519	02/02/2026 19:46:52	Yang Y, Zhao J, Morgan RL, Ma W, Jiang T.	BMC bioinformatics	Computational prediction of type III secreted proteins from gram-negative bacteria.	10.1186/1471-2105-11-s1-s47	2010	0.0	0.0		1.0	2022-02-07T18:57:16.000Z	2026-02-02T19:46:52.000Z	dcfa96ac-4881-4efa-baf5-2907ca1a48a8	undefined	3rafrbsxow				Starting_TSV	Match	No Match
63516fedb9c880af1f305b38	No. However, raw data can be downloaded  as data sets can be of GSE53625 from GEO (https://www.ncbi.nlm.nih.gov/geo/) and 37 ESCC cases with Asian ancestry from TCGA (UCSC Xena, https://xena.ucsc.edu/).	Downloaded from public sources	No similarity check	Train set of 134 samples, test of 45 samples, and a validation sets consisting of 86 samples. No mention to N_pos or N_neg.			No	No comparison	Unpaired or paired Student t-test and log-rank tests for Kaplan-Meier	ROC-AUC	one training set, one test set and one validation set.			No	No description	Linear models and ensemble methods	both regression and classification			Generalized Linear models, SVM, NN, Random Forests and XGBoost,	No	Normalization according to an external reference Gut. 2014;63(11):1700‚Äì10. https://doi.org/10.1136/gutjnl-2013-305806.	No description	No description	No	No description	No			6312169df3794236aa987a0a	34372798.0	PMC8351329	02/02/2026 19:46:52	Li MX, Sun XM, Cheng WG, Ruan HJ, Liu K, Chen P, Xu HJ, Gao SG, Feng XS, Qi YJ.	BMC cancer	Using a machine learning approach to identify key prognostic molecules for esophageal squamous cell carcinoma.	10.1186/s12885-021-08647-1	2021	0.0	0.0		1.0	2022-02-02T10:53:38.000Z	2026-02-02T19:46:52.000Z	9bbabc44-4041-4c29-aacc-649db0ba034b	undefined	ix9p2rdt6k				Starting_TSV	Match	No Match
63516fedb9c880af1f305b8f	Yes, in supporting information	Yes (ChEMBL-NTD Novartis dataset and dataset from previous publication)	Yes, independent training, test and external test sets generated using sphere exclusion algorithms	Yes: N_pos and N_neg for training, test and external test sets			No	No	No	Accuracy (Acc), Sensitivity (Se), Specificity (Sp) and Balanced Classification Rate (BCR) Area Under the Accumulation Curve (AUAC); Area under the Receiver Operating Characteristic Curve (ROC); Enrichment factor (EF) and Boltzmann-enhanced discrimination of ROC (BEDROC)	Independent dataset			No	No	Black box	Classification and score based on LSSVM scores			LSSVM	No	Fragments of atoms and bonds (ISIDA Fragmentor software) scaled to the interval [0-1]	Yes: 5-25 features per model, randomly selected among 250 most informative on training dataset (removal of features returning nearly constant values (about for 99%) and selection of top-250 according to Mutual Information Quotient score (Minimal Redundancy Maximal Relevance mRMR algorithm))	No	Yes: model aggregation (ensemble modeling) and multi-criteria decision making Trained on the same dataset	2 LSSVM parameters (RBF kernel (œÉ2) and regularization (Œ≥))  Minimization of the misclassification rate of the 10-fold cross-validated training dataset	Yes: Œ≥ parameter (LSSVM) 			6312169df3794236aa9879f4	28624633.0	PMC5650527	02/02/2026 19:46:52	S√°nchez-Rodr√≠guez A, P√©rez-Castillo Y, Sch√ºrer SC, Nicolotti O, Mangiatordi GF, Borges F, Cordeiro MNDS, Tejera E, Medina-Franco JL, Cruz-Monteagudo M.	Drug discovery today	From flamingo dance to (desirable) drug discovery: a nature-inspired approach.	10.1016/j.drudis.2017.05.008	2017	0.0	0.0		1.0	2022-02-02T10:08:47.000Z	2026-02-02T19:46:52.000Z	35bd5800-8ba0-4243-bf5c-09f8cfdd9734	undefined	47cb9iahma				Starting_TSV	Match	No Match
63516fedb9c880af1f305b90	Yes	Yes, CheMBL	Yes	Yes			Yes	No	No	precision, recall, F1-score	Cross-validation			Soft available separately.	No	Black box	Multiclass Multilabel Classification			Word2Vec + Random Forest	No	Doc2Vec	Word2Vec	No	No	No	No			6312169df3794236aa987a36	28678787.0	PMC5517062	02/02/2026 19:46:52	Zwierzyna M, Overington JP.	PLoS computational biology	Classification and analysis of a large collection of in vivo bioassay descriptions.	10.1371/journal.pcbi.1005641	2017	0.0	0.0		1.0	2022-01-31T08:45:48.000Z	2026-02-02T19:46:52.000Z	92182157-6ae9-46f7-873c-ac1450fee9f5	undefined	fnmlhciok7				Starting_TSV	Match	No Match
63516fedb9c880af1f305b91	Availability of data is not stated. There is availability of the code (mentioned as data availability). If the data is part of the code repo, I did not see it. Only *.m files for MatLab	Multiple ML pipelines are discussed. Data source is mentioned only for 1 of them (zebra finch singing) via an article reference (so it seems to have been used by a previous paper). I did not find any mention of data points.	I did not find it in the text	I did not find any of this in the text			I did not see it in the text but it could be part of the available code	Izhikevich, Theta and LIF models are used	I did not see it in the text	Accuracy is mentioned in the text.	For the songs, it looks like they compared the waves but I did not see a clear mention on how the method was evaluated			Yes, see https://senselab.med.yale.edu/ModelDB/ShowModel?model=190565 (also in GitHub at https://github.com/ModelDBRepository/190565) I did not see a license	I did not see it in the text	It did not look very transparent to me	It produces a reproduction, e.g. given a song, the neural network is trained to reproduce it.			FORCE method using Izhikevich, Theta and LIF neuron models. It does not seem to be a novel approach as they mention building upon others.	Not sure. MatLab code is available so I guess it is possible to change the hyper-parameter configuration there. See https://senselab.med.yale.edu/ModelDB/ShowModel?model=190565	I did not see it in the text	I did not see it in the text	I did not see it in the text	I did not see it in the text	There is a box listing parameters and their values. They mention previous works for tuning the parameters (not sure if they cover all of the parameters though)	I did not see it in the text			6312169df3794236aa987a13	29263361.0	PMC5738356	02/02/2026 19:46:52	Nicola W, Clopath C.	Nature communications	Supervised learning in spiking neural networks with FORCE training.	10.1038/s41467-017-01827-3	2017	0.0	0.0		1.0	2022-01-28T12:34:45.000Z	2026-02-02T19:46:52.000Z	b3a17544-6140-497c-a0d5-c3ee482dce5e	undefined	mnu68fbrdn				Starting_TSV	Match	No Match
63516fedb9c880af1f305b82	http://archive.ics.uci.edu/ml/index.php	1) Ecoli, Pima Indians Diabetes (Diabetes), Epileptic Seizure, Iris, Heart Disease, Glass Identification (Glass), Image Segmentation (Image), and Statlog (Satellite) 2) Breast Cancer, Parkinson, SinC, Servo, and Yacht Hydro (Yacht)	all datasets are without overlap, kept coincident for each trial of the algorithms.	1) Ecoli train: 180 val:78 test:78, Diabetes train: 384 val:22 test:192, Epileptic Seizure train: 6000 val:2750 test:2750, Heart Disease train: 150 val:76 test:76, Iris train: 70 val:40  test:40, Glass train: 100 val:57 test:57, Image train: 1200 val:555 test:555, Satellite train: 3435 val:1500 test:1500 2)Breast Cancer train:98 val:50 test:50, Parkinson train:500 val:270 test:270,  SinC train:5000 val:2500 test:2500, Servo train:384 val:192 test:192,  Yacht Hydro train:150 val:79 test:79			No	-	better timing and accuracy	Means and Standard Deviation and training time	20-fold cross validation			No	1) 1-37 seconds 2) 13-33 seconds	Black Box	1) classification 2) regression 			AIS-ELM is compared with DS-ELM, PSOELM, SaE-ELM, traditional ELM,SVM, and Back Propagation.	No	All the inputs have been normalized into the range [-1, 1] for fairness.	1) Ecoli 7, Diabetes 8, Epileptic Seizure 179, Heart Disease 75, Iris 4, Glass 9, Image 19, Satellite 36 2)Breast Cancer 32, Parkinson 26,  SinC 1, Servo 4,  Yacht Hydro 13	Not applicable	No	p=5, the parameters for AIS-ELM are set as follows: ùëé(antibody population) =10,ùëè = 50(bit position of the last ‚Äúon‚Äù bit starting from the most significant bit),ùúÄ = 0.1(stimulus region),ùëò = 5(number of bits that must be flipped to mutate),ùëü = 0.2(mutation probability)	No			6312169df3794236aa987a1b	30046299.0	PMC6036855	02/02/2026 19:46:52	Tian HY, Li SJ, Wu TQ, Yao M.	Computational intelligence and neuroscience	An Extreme Learning Machine Based on Artificial Immune System.	10.1155/2018/3635845	2018	0.0	0.0		1.0	2022-01-26T14:34:16.000Z	2026-02-02T19:46:52.000Z	98d62f4d-4577-4892-9130-684c6464d69c	undefined	conhqw242m				Starting_TSV	Match	No Match
63516fedb9c880af1f305b59	no.	SNP data for a total of 9966 subjects: 4974 training cohort subjects living in Miyagi prefecture recruited by Tohoku University and 4992 validation cohort subjects living in Iwate prefecture recruited by Iwate Medical University. 	Training and test sets are independent. Subjects with a low call rate (<0.98, n = 2 in the training cohort and n = 3 in the validation cohort) were excluded. They detected 2156 close-relationship pairs (620 in the training cohort and 1536 in the validation cohort) using the identity-by-descent method in PLINK software among the training cohort, the validation cohort, or between these cohorts. Then, in each of these pairs, a subject with lower call rates was excluded. Variants with low call rates, low Hardy‚ÄìWeinberg equilibrium exact-test P values, or low minor-allele frequencies were filtered out. Subjects without outcome or covariate information (n = 669 in the training cohort and n = 408 in the validation cohort) were excluded. Finally, 3685 subjects in the training cohort and 3048 subjects in the validation cohort with 615,386 variants were subjected to prediction analyses.	After filtering, 3685 subjects in the training cohort and 3048 subjects in the validation cohort with 615,386 variants were subjected to prediction analyses.			no.	The performance of STMGP was evaluated in terms of prediction accuracy and the degree of overfitting, and compared with that of other state-of-the-art methods, which included, in addition to PRS and GBLUP, summary-data-based best linear-unbiased prediction (SBLUP) , BayesR (a Bayesian hierarchical model for complex trait analysis) , and ridge regression (penalized regression model). 	STMGP showed the highest prediction accuracy with the lowest degree of overfitting, although there was no significant difference in prediction accuracy.	Accuracy.	The performance of STMGP was evaluated in terms of prediction accuracy and the degree of overfitting.			The program code for STMGP (STMGP v1.0), including the function used in the study, is available via CRAN, the official R package archive.	Not stated.	Black box.	Regression.			Novel prediction algorithm: Smooth-Threshold Multivariate Genetic Prediction (STMGP)	no.	Not stated.	They prepared SNP data, phenotype data, and covariate data for both the training and test datasets for the input of the function (3 features).	Not stated.	no.	2 tuning parameters.	Strategy to reduce overfitting: screening and building penalized regression models.			6312169df3794236aa987a03	32826857.0	PMC7442807	02/02/2026 19:46:52	Takahashi Y, Ueki M, Tamiya G, Ogishima S, Kinoshita K, Hozawa A, Minegishi N, Nagami F, Fukumoto K, Otsuka K, Tanno K, Sakata K, Shimizu A, Sasaki M, Sobue K, Kure S, Yamamoto M, Tomita H.	Translational psychiatry	Machine learning for effectively avoiding overfitting is a crucial strategy for the genetic prediction of polygenic psychiatric phenotypes.	10.1038/s41398-020-00957-5	2020	0.0	0.0		1.0	2022-01-26T11:41:26.000Z	2026-02-02T19:46:52.000Z	26486d2e-e909-4b8a-a883-ec4878c2a8e0	undefined	65taxw9gs8				Starting_TSV	Match	No Match
63516fedb9c880af1f305b92	1) No 2) No	1) We simulate random datasets by extending a casecontrol model adopted in https://doi.org/10.1186/1752-0509-2-10, for evaluation of all methods. 2)USA dataset for breast cancer metastasis	1) Simulation of three different types of interaction patterns between two interacting variables xi, xj and the outcome y: ‚Äúsimple‚Äù, ‚Äúcomplex‚Äù, and ‚Äúvery complex‚Äù. Among six pairs of interacting variables, simulation of the data by including two pairs of each pattern. Depending on the outcome then they assigned a value drawn from an equally weighted Mixture-of-Gaussian with various Gaussian components for each pattern case. 2) not applicable	1) Randomly assignment of outcome variable y uniformly distributed in {0, 1}. Generation of input variables: random generation of 50 input variables, randomly select six of them to simulate the individual effects and another six distinct pairs of other variables from all 1, 225 possible variable pairs to simulate significant interactive effects on disease outcome y. Creation of 1,000 datasets for each one of the following sample sizes: 20, 40, 60, 80, 100, 120, and 140. 2) USA(train): n=286, 107 metastasis cases Netherlands(test): n=295, 79  metastasis cases			1) No 2) No	1) LASSO. The performance of LASSO is worse than all the other non-linear methods when we have relatively small numbers of samples. 2) Not applicable	1)All the measurements are based on the statistics estimated from data. 2) p-values	1) AUC (area under ROC curve) values 2) AUC and p-values	1) Not applicable 2) 10-fold cross-validation and independent Dataset			1)No 2) No	1) a few seconds 2) not applicable	1) transparent since results are based on statistics. 2) transparent since results are based on statistics.	1) regression 2) classification 			1) various methods for classification and clustering. Both supervised and unsupervised. 2) first network-based feature ranking method for selection of top genes, then SVM for classification	1) No 2) No	1) Not applicable 2) Not applicable	1)50 and 200 input variables. 2) 6,168 genes as input features for network-based feature ranking method, and then top T (1,5,10...,50)	1) average over a wide range of K to avoid overfitting for KNN-based methods 2) Not applicable	1) yes. Some in similar ways. Some other vary from unsupervised to supervised learning. 2) No	1) based on the complexity of each pattern 2) p=2, with complexity parameter C = 100 and RBF kernel with œÉ = 1	1) No 2) No			6312169df3794236aa987a1b	27362985.0	PMC5775817	02/02/2026 19:46:52	Adl AA, Lee HS, Qian X.	IEEE/ACM transactions on computational biology and bioinformatics	Detecting Pairwise Interactive Effects of Continuous Random Variables for Biomarker Identification with Small Sample Size.	10.1109/tcbb.2016.2586042	2017	0.0	0.0		1.0	2022-01-26T11:27:05.000Z	2026-02-02T19:46:52.000Z	1a0158e9-2f41-4c14-9b7e-4d7369584811	undefined	3p5oxxtlts				Starting_TSV	Match	No Match
63516fedb9c880af1f305b78	Yes. Supplementary material of the paper.	307 cervical tumor samples from The Cancer Genome Atlas, 113 normal.	Not stated.	The data were randomly divided into ten different sets. Nine sets were used for training, and the remaining set was used for validation.			no	no.	no.	96.2% sensitivity and 95.2% specificity	tenfold cross-validation			no.	Not stated.	Black box.	Classification.			Logistic regression model.	no.	Feature selection.	Hybrid feature selection schema based on information gain and sequential backward feature selection (SBFS).	Not stated.	no.	Not stated.	no.			6312169df3794236aa987a03	31871774.0	PMC6908647	02/02/2026 19:46:52	Xu W, Xu M, Wang L, Zhou W, Xiang R, Shi Y, Zhang Y, Piao Y.	Signal transduction and targeted therapy	Integrative analysis of DNA methylation and gene expression identified cervical cancer-specific diagnostic biomarkers.	10.1038/s41392-019-0081-6	2019	0.0	0.0		1.0	2022-01-25T14:56:54.000Z	2026-02-02T19:46:52.000Z	bfdbba26-c83c-43c0-8067-6e78c75facb7	undefined	m2bccoqnyy				Starting_TSV	Match	No Match
63516fedb9c880af1f305ba7	no.	669 paired recordings were made across 3 subjects. Data are divided in 4 classes of (276, 149, 157, 87 points each). Data source: experiment.	Not stated.	90% of the data was used for training and the remaining 10% was used for testing.			no.	no.	Not stated.	Sensitivity/specificity measures for each subject/class.	10-fold cross-validation			no.	Not stated.	black box.	Classification.			support vector machine (SVM)	no.	Each 30- to 120-s paired recording was converted into a single bipolar LFP. The multitaper method was used to calculate a spectrogram of the LFP. using a 2-s window with 10% window steps, resulting in a 1-Hz frequency resolution. Time windows containing movement artifacts were visually identified and removed. The spectrogram was averaged over the remaining windows to produce a single power spectral density (PSD) for each paired recording. An additional feature set was created with phase-amplitude coupling (PAC).	The entire feature set including both spectral and PAC components contained 206 features.  Feature selection was performed. The lasso regularization technique with 10-fold cross-vali- dation was used to perform feature selection through least-squares regression with a penalty on the size of the estimated coefficients. This procedure finds the combination of features that produces the minimum mean-square error (MSE). Lasso regularization was performed separately for the spectral and PAC feature sets.	Large number of features. Feature selection was performed to reduce the number of features used for the classificationto avoid overfitting.	no.	Not stated.	yes: The lasso regularization technique with 10-fold cross-validation was used to perform feature selection through least-squares regression with a penalty on the size of the estimated coefficients			6312169df3794236aa987a03	25878156.0	PMC4507953	02/02/2026 19:46:52	Connolly AT, Jensen AL, Baker KB, Vitek JL, Johnson MD.	Journal of neurophysiology	Classification of pallidal oscillations with increasing parkinsonian severity.	10.1152/jn.00840.2014	2015	0.0	0.0		1.0	2022-01-21T17:00:14.000Z	2026-02-02T19:46:52.000Z	001278bf-79ae-42c1-b064-88bbc3e18183	undefined	e1fgv8jwbe				Starting_TSV	Match	No Match
63516fedb9c880af1f305b5b	Yes. https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE80672	The data set is publicly available and described in detail in https://doi.org/10.1016/j.cmet.2017.03.016  141 data points, problem of regression for predicting chronological age of mice based on their methylation profiles.	Œëll age classes appeared almost equally in both sets.	training (n = 75) and validation(n = 66)			No	comparison of the outcome of seagull to that of the established R package SGL 1.3	the results of seagull and SGL were very similar (R^2 > 0.99)	MSE of predicted age based on methylation data, squared correlation coefficient R^2 between predicted and chronological age, the number of features with an estimated effect different from zero, the execution time needed to compute the entire regularization path.	independent dataset			yes in Additional files and https://github.com/jklosa/seagull	20'-5h compared to 45h of previous methods	transparent. 	regression of methylation age of mice. Eventually, seagull provides a sequence of penalty parameters and calculates the corresponding path of solutions.			accelerated generalized gradient descent. 	no	SVD for groups' separation	Not applicable	No	No	p=6. penalty parameter Œª,  Œ± ‚àà [0, 1] is the mixing parameter which convexly links the penalties, Œµ_rel relative accuracy,  groups from SVD, weights for each explanatory variable, proportion Œæ Œªmin = ŒæŒªmax.	Yes. Regularization paths for the lasso, group lasso, sparse-group lasso, and IPF-lasso for linear regression models			6312169df3794236aa987a1b	32933477.0	PMC7493359	02/02/2026 19:46:52	Klosa J, Simon N, Westermark PO, Liebscher V, Wittenburg D.	BMC bioinformatics	Seagull: lasso, group lasso and sparse-group lasso regularization for linear regression models via proximal gradient descent.	10.1186/s12859-020-03725-w	2020	0.0	0.0		1.0	2022-01-18T17:50:21.000Z	2026-02-02T19:46:52.000Z	d8f4a424-12a1-40cc-8bde-3f756df2316b	undefined	v4efeulceq				Starting_TSV	Match	No Match
63516fedb9c880af1f305b9d	Yes.  https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE29315, vhttps://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE33630, vhttps://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE27155, vhttps://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE3678   	Source:   Four published datasets from the gene expression omnibus (GEO)  series  GSE29315, GSE33630,  GSE27155 and GSE3678.      N_pos=176 patients, N_neg=113 patients.   GSE29315 already used in (Finley et al, Ann Surg, 2004)  GSE27155 already used in (Giordano et al, Oncogene, 2005, and Giordano et al, Clin Cancer Res 2006).	Not applicable.	Training set, GSE29315: N_pos_train=31, N_neg_train=40.         Testing sets, GSE33630: N_pos_test=60, N_neg_test=45.    GSE27155: N_pos_test=78, N_neg_test=21.  GSE3678:  N_pos_test=7, N_neg_test=7.    43.7% positives on training set,   57.1% positives on testing set GSE33630 ,  78.8% positives on testing set GSE27155,  50.0% positives on testing set GSE3678.			No.	Despite the small number of genes in the panel, the ability to predict different categories of thyroid nodules was similar to that of published data from over 100 genes.	Confidence Intervals are reported, by which a performance similarity between the 3 genes panel and the 100 genes panel is claimed.	Sensitivity, specificity, accuracy.	Independent datasets and novel experiment.     Independent dataset: GSE33630,  GSE27155 and GSE3678.   Novel experiment:   Thyroid tissue specimens excised intraoperatively from 70 patients undergoing primary thyroidectomies in Renji Hospital			Publicly available R packages.	Not reported.	No statement about ante-hoc interpretability of the model is made by the authors.   The model looks to me black box.   Post-hoc interpretability for the ipeptidylprolyl peptidase 4 (DPP4) gene is supported e.g. by literature reports that this gene significant increases in differentiated carcinomas vs. normal or benign thyroid nodules at average 46 times.   However, the low univariate rankings of 2 of the 3 genes indicated that it was the genes combination, that resulted in good predictive power, and no attempt was made to explain such joint effect.	Binary classification between benign and malignant thyroid tumors.			Iterative Bayesian Model Averaging (BMA) algorithm.	No.	Not reported.	f=3.   The feature selection method to distinguish benign from malignant samples consisted of two steps: a LIMMA linear model, and an iterative BMA algorithm.   The R package LIMMA was used  to  select  significantly  differentially  expressed  genes (fold-change cutoff of >=2 and a p-value<0.01). To minimize the number of signature genes, the iterative BMA R package was applied, which accounts for model uncertainty and the dependency between signature genes.    The BMA step was applied to further reduce the number of signature genes from 43 to 3, and thus the costs associated with microarrays and time-consuming data analysis.	 Over- or under- fit could probably be excluded a posteriori by the good predictive performance in independent test sets and novel experiments. 	No.	The authors do not mention parameters numbers different from standard. 	No.			65faa4f792c76639b82bab29	25175491.0		02/02/2026 19:46:52	Zheng B, Liu J, Gu J, Lu Y, Zhang W, Li M, Lu H.	International journal of cancer	A three-gene panel that distinguishes benign from malignant thyroid nodules.	10.1002/ijc.29172	2015	0.0	0.0		1.0	2022-05-20T17:45:46.000Z	2026-02-02T19:46:52.000Z	ed449019-062c-4769-99ec-83a325549f96	undefined	hl7w8279ra				Starting_TSV	Match	No Match
63516fedb9c880af1f305b84	No.	Total dataset composed of 535 in-field observations, N_pos = 84  (presence of spore-realising apothecia) and N_neg = 352 (absence of spore-realising apothecia).  Dataset not previously used.	Not applicable.	The dataset was randomly split into training (70%) and testing (30%) using the ‚ÄúcreateDataPartition‚Äù function part of the caret package in R (version 3.2.4 for iOS).  Each model was trained with 10-fold cross-validation.    The random splitting of the data, model training, and assessment on the test set were performed 100 times to ascertain the variance of each model due to the random data splitting.			No.	Four approaches were compared: LR, MARS, ANN, RF.  Model performance was compared based on the AUROC, kappa-metrics, and Brier score.  The best measures were those of RF.	Model performance was compared based on the AUROC, kappa metrics, and Brier score. The AUROC of the RF model was highest at 0.74, significantly higher than LR (with a value of 0.57) but not significantly different from the ANN or MARS models.   LR had a very low kappa value (0.16), while RF had kappa values between 0.43 and 0.57, higher than ANN and MARS values of between 0.35 and 0.5 and, therefore, has the best prediction potential among the four models tested in this study based on the kappa metric.	Accuracy, sensitivity, specificity, AUROC.	Cross-validation.			 Standard algorithms were used. 	Not reported.	No statement about ante-hoc interpretability of the models is made by the authors.   LR and MARS are generally considered interpretable models, while ANN and RF are generally considered not interpretable.    Post-hoc feature importance analysis indicated as the most important ones soil moisture, precipitation, and air temperature (LR model), or soil temperature, soil moisture, and solar radiation (MARS, ANN, RF models) -- all of them in line with conclusions found in experimental literature.	Classification (binary prediction).  The continuous output of the LR, MARS, ANN was assigned as 'close to 1' or 'close to 0'.			Four approaches were compared: logistic regression (LR), multivariate adaptive regression splines (MARS), artificial neural networks (ANN), random forest (RF). 	No	Not reported.	Exploratory data analyses resulted in removal of correlated predictors (¬±0.90).   In the end, 7 predictors were used (i.e. 7 types of environmental data -- air temperature, relative humidity, etc).  The ‚ÄúvarImp‚Äù function contained in the caret package was used to determine the relative predictor importance for each model. Each model was run using 3, 4, 5, 6, and 7 predictors. The most important variables were selected for the development of the final model.	Over-fitting in MARS and ANN was discussed and prevented.   In LR and RF, over-fitting should not have been an issue.  Possible underfitting was probabiy there for LR, which, with a kappa of 0.16, indicated a low degree of similarity, between observed and predicted, beyond random chance.	No.	For LR, MARS, RF, the authors do not mention parameters numbers different from standard.   For ANN, weights can be deduced to have been in the range of 100. Their number depended on the number of nodes in the hidden layer, which was optimized using 10-fold cross-validation and the AUROC curve for model assessment.	For MARS, the forward stepwise algorithm leads to an overfitted model which is then run through a backward stepwise algorithm where basis functions that contribute the least are removed (Friedman1991).   For RF,  to reduce overfitting, the tree was often pruned, resulting in a smaller tree with fewer splits. This was accomplished using the Gini index, which is a measure of variance across all classes‚Äîsmaller values mean a more accurate prediction at that node.			65faa4f792c76639b82bab29	28696170.0		02/02/2026 19:46:52	Harteveld DOC, Grant MR, Pscheidt JW, Peever TL.	Phytopathology	Predicting Ascospore Release of Monilinia vaccinii-corymbosi of Blueberry with Machine Learning.	10.1094/phyto-04-17-0162-r	2017	0.0	0.0		1.0	2022-05-20T17:19:51.000Z	2026-02-02T19:46:52.000Z	052c537c-d355-4d83-9a5b-2587a9451a4d	undefined	0zaakstjvr				Starting_TSV	Match	No Match
63516fedb9c880af1f305b20	Yes, Data are available from the Clinical Research Committee of KOBIO under the Korean College of Rheumatology for researchers who meet the criteria for access to confidential data	This study used data from the KOBIO registry, which is a nationwide multicenter cohort in Korea that was established to evaluate the effectiveness and side effects of biologic therapies in patients with RA [13]. Patients in the registry were recruited from 38 hospitals since 2012, and their demographics, medications, comorbidities, extra-articular manifestations, disease activities, radiographic findings, and laboratory findings performed within 4 weeks prior to the patient‚Äôs visit were recorded with the date. The data from patients who were followed up annually were recorded on the KOBIO website (http://www.kobio.or.kr/kobio/), and these patients provided informed consent prior to registration. Ethical approval of the KOBIO-RA was obtained from the institutional review boards of all 38 participating institutions, including the Institutional Review Board of Inje University Seoul Paik Hospital (PAIK 2018-11-005).		 the training and test sets were divided in a 7:3 ratio, and the models were trained with the training set			no	The no information rate, which is the largest proportion of the observed classes, was used as a baseline to determine the overall distribution of the classification and to compare with those of the machine learning models	na	Bootstrapping (random sampling with replacement) was also performed to obtain a median value for the AUROC curve and to determine the accuracy for reducing measurement variances caused by small samples when dividing between the training and test sets.	CV			na	na	transparent	classification			the models included lasso and ridge based on linear relationships [14], support vector machine using kernel methods [15], tree-based random forest [16], and Xgboost	yes, SI	Dimension reduction was performed to avoid the ‚Äúcurse of dimensionality‚Äù caused by a large number of variables compared with the size of the data. 	Among the 64 variables, we selected variables that are frequently encountered in clinical practice for prescription of biologics and excluded variables that are not referenced when prescribing biologics. As a result, 15 variables known to be of clinical importance were preselected (i.e., sex, age, baseline DAS28-ESR, methotrexate dose, steroid dose, erythrocyte sedimentation rate [ESR], C-reactive protein [CRP], rheumatoid factor [RF], anti-cyclic citrullinated peptide antibody [ACPA], anti-nuclear antibody [ANA], and five comorbidities). Subsequently, 20 variables that were highly correlated with the drug response (remission) of each bDMARD were selected. After selecting variables based on data, we created a prediction model by training with a fixed set of 35 variables. Missing data for variables (Additional file: Table S3) were replaced with the median value for each variable. With a similar logic, binary variables such as comorbidities were coded as 1 if ‚Äúyes‚Äù and 0 if ‚Äúno‚Äù or ‚Äúno test‚Äù because ‚Äúno‚Äù was the most common value.	To avoid overfitting problems, the training and test sets were divided in a 7:3 ratio, and the models were trained with the training set; then, the prediction results were verified using the test set. For the training dataset, a 5-fold cross validation was performed to tune the hyperparameters determined as outside models (Additional file: Table S1 and Table S2). In this procedure, a grid search was conducted to evaluate all possible combinations of the hyperparameters. The grid search found optimal hyperparameters with the objective function of determining the area under the receiver operating characteristics (AUROC) in each model. Bootstrapping (random sampling with replacement) was also performed to obtain a median value for the AUROC curve and to determine the accuracy for reducing measurement variances caused by small samples when dividing between the training and test sets.	no	a grid search was conducted to evaluate all possible combinations of the hyperparameters. The grid search found optimal hyperparameters with the objective function of determining the area under the receiver operating characteristics (AUROC) in each model	no			6312169df3794236aa9879e7	34229736.0	PMC8259419	02/02/2026 19:46:52	Koo BS, Eun S, Shin K, Yoon H, Hong C, Kim DH, Hong S, Kim YG, Lee CK, Yoo B, Oh JS.	Arthritis research & therapy	Machine learning model for identifying important clinical features for predicting remission in patients with rheumatoid arthritis treated with biologics.	10.1186/s13075-021-02567-y	2021	0.0	0.0		1.0	2022-03-29T22:22:24.000Z	2026-02-02T19:46:52.000Z	d70f673b-b310-4054-9672-a93679f1d817	undefined	htsvoi90ft				Starting_TSV	Match	No Match
63516fedb9c880af1f305b60	Yes, Supporting information.	Neonatal Intensive Care Unit (NICU) at the Children‚Äôs Hospital of Philadelphia, hospizalized infants with sepsis evaluation (culture positive, clinically positive)	NA (there were pre-selection criteria for inclusion)	618 infants with 1188 sepsis evaluations, 110 culture positive, 265 clinically positive, 492 negative			Yes, Supporting information, https://github.com/chop-dbhi/sepsis_01, MIT license (code)	Ada boost, Gradient Boosting, kNN, Logistic Regression, Random Forest, SVM	NA (The null hypothesis of equal inter-model distributions was rejected by the Friedman rank sum test with p-values of <0.001 for both the CPOnly and CP+Clinical datasets.)	AUC, Specificity, PPV, NPV	k-fold cross validation, negative control dataset			https://github.com/chop-dbhi/sepsis_01, Data is in S3, unclear if project can be fully bootstrapped.		Transparent, names of models and parameters are provided	probability score, classification into positive case if p > 0.5			AdaBoost, Gradient boosting, Gaussian process, K-nearest neighbors, logistic regression, naive Bayes, Random forest, SVM (RBF)	Yes, supporting information	Mean imputation for missing values, normalization of continuous valued features to zero mean and unit variance.	36 clinical attributes per patient per time-point	Second feature selection process as part of the hyper-parameter tuning process, referred to as ‚Äúautomated feature selection‚Äù in Fig 1. The automated method is based on the mutual information between each individual feature and sepsis class (case or control) to select top n features, 11 features selected for CPOnly DS, 35 (all) for CP+Clinical dataset.	No	Ada boost: 3, Gradient Boosting: 2, kNN: 2, Logistic Regression: 1, Random Forest: 3, SVM: 2	Nested k-fold cross-validation			6312169df3794236aa9879e9	30794638.0	PMC6386402	02/02/2026 19:46:52	Masino AJ, Harris MC, Forsyth D, Ostapenko S, Srinivasan L, Bonafide CP, Balamuth F, Schmatz M, Grundmeier RW.	PloS one	Machine learning models for early sepsis recognition in the neonatal intensive care unit using readily available electronic health record data.	10.1371/journal.pone.0212665	2019	0.0	0.0		1.0	2022-03-29T09:39:03.000Z	2026-02-02T19:46:52.000Z	92789330-e8c7-4761-a47b-6758bb151af7	undefined	m8bml54z62				Starting_TSV	Match	No Match
63516fedb9c880af1f305b96	no	data were primarily collected from the online Allosteric Database (ASD)	selection of negative and positive for balanced datasets	allosteric sites (59), regular sites (99), orthosteric sited (159)			no	none	Statistical confidence. The times assigned to each class are given so as to express the approximate level of confidence with which a class has been assigned from 100 repeats.	Gini importance measure, RF-score and CavSeek	out-of-bag set and independent test set			no	not mentioned	transparent since there is a methodological feature selection.	classification			Random Forest	no	random sample of (number of descriptors) ^(1/2) until the tree can no longer grow	existing protein-ligand scoring function RF-Score and a new accessibility-like algorithm called CavSeek to compute structurally-based binding descriptors and descriptors pertaining to the composition and flexibility of the clefts.	no	no	default	no			6312169df3794236aa987a1b	27491922.0		02/02/2026 19:46:52	Chen AS, Westwood NJ, Brear P, Rogers GW, Mavridis L, Mitchell JB.	Molecular informatics	A Random Forest Model for Predicting Allosteric and Functional Sites on Proteins.	10.1002/minf.201500108	2016	0.0	0.0		1.0	2022-03-23T12:35:43.000Z	2026-02-02T19:46:52.000Z	1212cc08-554d-44ca-a4b8-1544c008b3ce	undefined	nlsxj0478u				Starting_TSV	Match	No Match
63a25db2e8edf6ce46f6e84b	(http://lncrnadb.org), (http://www.cuilab.cn/lncrnadisease), (http://www.ensembl.org), (https://araport-dev.tacc.utexas.edu)	positive: lncRNAdb v2.0, lncRNAdisease , total of 436 unique, validated lncRNA sequences // negative: Ensembl, Araportv11 	variety of training datasets was used to maximize model diversity and samples were equally and randomly selected to get a balanced training	 8 different combinations of negative data from multiple species			no	compared to GreeNC (uses a transcript filtering method, rather than a machine learning approaches).	(qualitative explanation) An important consideration of this tool is that it is not constrained by preconceived rules that may or may not appropriately classify lncRNA properties and the stacking generalizer model based on gradient boosting models will facilitate lncRNA identification without imposing arbitrary rules for lncRNA detection.	 accuracy, sensitivity, specificity and AUC values	10-fold cross-validation			https://github.com/gbgolding/crema	measured in minutes		classification  binary if a transcript was or was not predicted as a lncRNA and stacking with logistic regression for ensemble method			 random forest and gradient boosting. Also ensemble method from different classifiers, Two separate values were used for the creation of each ensemble model ‚Äì scores sij and predictions pij where i represents model number and j transcript.  The four ensemble approaches included both algebraic combiners and voting methods as non-trainable methods, and a stacking generalizer as a meta-learner.	no	Diamond alignment in SwissProt database	9 features were extracted using a combination of custom Python scripts and known software CPAT, Diamond, RepeatMasker.	not applicable	no	 gradient boosting parameters :  learning_rate, max_depth, subsample, n_estimators.  Random forest parameters: only change from default parameters being n_estimators and min_samples_leaf.	not applicable			6312169df3794236aa987a1b	29720103.0	PMC5930664	02/02/2026 19:46:52	Simopoulos CMA, Weretilnyk EA, Golding GB.	BMC genomics	Prediction of plant lncRNA by ensemble machine learning classifiers.	10.1186/s12864-018-4665-2	2018	0.0	0.0		1.0	2022-03-22T13:13:16.000Z	2026-02-02T19:46:52.000Z	8c0c94cd-3172-4f65-92b0-b997742324e0	undefined	wd9oesbckf				Starting_TSV	Match	No Match
63516fedb9c880af1f305b46	https://github.com/chemosim-lab/SweetenersDB	316 sweet compounds from SweetenersDB dataset. https://doi.org/10.1016/j.foodchem.2016.10.145	The updated SweetenersDB was split in training and test sets using a Sphere Exclusion clustering algorithm. 	64 diverse compounds (20.3%) were selected for the test set, leaving 252 compounds in the training set			no	e-Sweet platform (Zheng et al., 2019) is based on a consensus model of various machine learning protocols.  The performance of BitterSweet is comparable to e-Sweet and Predisweet (R2 of 0.72 on our test set) but the protocol is still unpublished, and seven molecules of the test set has not been considered as sweet.	statistical metrics	predictive performance was evaluated based on criteria previously defined by: https://doi.org/10.1016/S1093-3263(01)00123-1. correlation coefficient,  coefficient of determination,  slopes of the regression lines through the origin for the observed vs. predicted and predicted vs. observed values respectively, corresponding coefficients of determination,  root mean squared error,  mean absolute error	independent dataset 			http://chemosimserver.unice.fr/predisweet/	not mentioned	direct correlation between compound and sweetness	regression to predict the logSw (relative sweetness)			Random Forest, Support Vector Machine (SVM), Adaptative Boosting with a Decision Tree base estimator (AdaBoost Tree), and k-Nearest Neighbors. AdaBoost Tree was selected	no	Every compound in the datasets were collected as SMILES strings and sanitized with RDKit. The resulting datasets consisted of 635 descriptors for the Dragon dataset, and 506 features for the ‚Äúopen-source‚Äù dataset. 	Five-fold cross validation was performed with hyperparameter tuning using a grid search.  The optimal percentile of features was tuned as a parameter of the Grid Search. 32 descriptors for the ‚ÄúDragon‚Äù model, and 51descriptors for the ‚Äúopen source‚Äù model	To avoid any model bias due to overfitting, the number of features used by the model is a hyperparameter that has been optimized.	no	default	Structures were standardized using the ‚Äústandardizer‚Äù from Python.			6312169df3794236aa987a1b	32344344.0		02/02/2026 19:46:52	Bouysset C, Belloir C, Antonczak S, Briand L, Fiorucci S.	Food chemistry	Novel scaffold of natural compound eliciting sweet taste revealed by machine learning.	10.1016/j.foodchem.2020.126864	2020	0.0	0.0		1.0	2022-03-22T11:54:11.000Z	2026-02-02T19:46:52.000Z	2ab96eb2-c809-4f94-9729-9573ca70ad20	undefined	od7rdaz7w7				Starting_TSV	Match	No Match
63516fedb9c880af1f305b6a	Yes: website url (http://lin-group.cn/server/HBPred2.0/download.html)	Yes: previous paper	Yes: exclusion of identical sequences between testing and training datasets. Reduced redundancy within testing dataset (sequence identity >60% CD-HIT).	Yes, size of training and test set, including distribution N_pos N_neg			No	Yes: Comparison with other classifiers: J48, Bagging, Random Forest, Naive Bayes Comparison with other published methods: HBPred, iGHBP, HBPred2.0	No: only comparing difference between performance measures 	Yes: Sensitivity, Specificity, Accuracy, Matthew correlation coefficient, AUC	Yes: on 5-fold training dataset and also on independent dataset			No, only a prediction web service	No	Black box	Classification			Yes: SVM	No	Yes: sequence features (NVM, g-gap, TPC) and composition properties (CTD, pseAAC) 	Yes: 5 types of features (NVM, g-gap, TPC, CTD, pseAAC). Feature selection using ANOVA for g-gap and pseAAC, binomial distribution for TPC and Incremental feature selection. and size of vectors (21, 60, 81, 400, 8000 depending on the feature extraction method...)	No	No	Yes: parameters C and g, optimised by grid search space	Yes: Parameter C, optimised by grid search space			6312169df3794236aa9879f4	31137222.0		02/02/2026 19:46:52	Tan JX, Li SH, Zhang ZM, Chen CX, Chen W, Tang H, Lin H.	Mathematical biosciences and engineering : MBE	Identification of hormone binding proteins based on machine learning methods.	10.3934/mbe.2019123	2019	0.0	0.0		1.0	2022-03-18T14:48:58.000Z	2026-02-02T19:46:52.000Z	4427eadc-dc28-4afb-8ae5-dcaf51935306	undefined	oi6cru1897				Starting_TSV	Match	No Match
63516fedb9c880af1f305b7d	Yes.   -- Datasets for prostate cancer -- Dataset 1 URL: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE74013 Dataset 2 URL: https://portal.gdc.cancer.gov/projects/TCGA-PRAD Dataset 3 URL: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE55479 Dataset 4 URL: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE38240 Dataset 5 URL: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE73549  -- Datasets for bladder cancer -- Dataset 6 URL: https://portal.gdc.cancer.gov/projects/TCGA-BLCA  -- Datasets for colorectal cancer -- Dataset 7 URL: https://portal.gdc.cancer.gov/projects/TCGA-COAD Dataset 8 URL: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE74013  -- Datasets for renal clear cells cancer -- Dataset 9 URL: https://wiki.cancerimagingarchive.net/display/Public/TCGA-KIRC  -- Datasets for renal papillary cell carcinoma -- Dataset 10 URL: https://www.google.com/search?channel=trow5&client=firefox-b-d&q=TCGA+KIRP	All the datasets are public. For all datasetes, pos refer to tumor sample, neg to non-tumor sample.  -- Datasets for prostate cancer -- Dataset 1: GEO dataset GSE74013 (N_pos = 21; N_neg = 27) Dataset 2: TCGA-PRAD (N_pos = 293; N_neg = 23) Dataset 3: GEO dataset GSE55479 (N_pos = 143; N_neg = 0) Dataset 4: GEO dataset GSE38240 (N_pos = 8; N_neg = 4) Dataset 5: GEO dataset GSE73549 (N_pos = 77; N_neg = 15)  -- Datasets for bladder cancer -- Dataset 6: TCGA-BLCA (N_pos = 335; N_neg = 23)  -- Datasets for colorectal cancer -- Dataset 7: TCGA-COAD (N_pos = 333; N_neg = 37) Dataset 8: GEO dataset GSE74013 (N_pos = 14; N_neg = 20)  -- Datasets for renal clear cells cancer -- Dataset 9: TGCA-KIRC (N_pos = 290; N_neg = 130)  -- Datasets for renal papillary cell carcinoma -- Dataset 10: TGCA-KIRP (N_pos = 252; N_neg = 167)	No redundancy stated for dataset splits. All the datasets are independent from each other	In all cases, the training dataset is used also for validation.  -- Datasets for prostate cancer -- Dataset 1: Splitted in Training (N_pos = 8; N_neg = 11) and Test set (N_pos = 13; N_neg = 16) Dataset 2: Splitted in Training (N_pos = 117; N_neg = 15) and Test set (N_pos = 176; N_neg = 23) Dataset 3: Used as test set (N_pos = 143; N_neg = 0) Dataset 4: Used as test set (N_pos = 8; N_neg = 4) Dataset 5: Used as test set (N_pos = 77; N_neg = 15)  -- Datasets for bladder cancer -- Dataset 6: Splitted in training (N_pos = 134; N_neg = 9) and test set (N_pos = 201; N_neg = 14)  -- Datasets for colorectal cancer -- Dataset 7: Splitted in training (N_pos = 133; N_neg = 15) and test set (N_pos = 200; N_neg = 22) Dataset 8: Splitted in training (N_pos = 6; N_neg = 9) and test set (N_pos = 8; N_neg = 11)  -- Datasets for renal clear cells cancer -- Dataset 9: Splitted in training (N_pos = 116; N_neg = 52) and test set (N_pos = 174; N_neg = 78)  -- Datasets for renal papillary cell carcinoma -- Dataset 10: Splitted in training (N_pos = 101; N_neg = 63) and test set (N_pos = 151; N_neg = 104)			No.	Compared with other classification models, all based on logistic regression. 	No confidence intervals. No static significance claimed	Precision, Recall, F1-score, Area Under Curve (AUC)	Independent Datasets (test sets)			Yes, the code is available at URL: https://github.com/bioinformatics-IBCH/logloss-beraf	Not stated	Black box	Classification			Random Forest 	No.		Number of features not stated.  The feature selection was actually the main topic of the paper. The feature selection method is carefully described. In brief, features were selected passing through different steps: 1. initial filtering 2. Random Logistic Regression 3. Feature importance of Random Forest 4. evaluating the LogLoss of the prediction performed using different combination of the selected features.	No	No.	Not stated.	No.			6312169df3794236aa987a01	30388122.0	PMC6214495	02/02/2026 19:46:52	Babalyan K, Sultanov R, Generozov E, Sharova E, Kostryukova E, Larin A, Kanygina A, Govorun V, Arapidi G.	PloS one	LogLoss-BERAF: An ensemble-based machine learning model for constructing highly accurate diagnostic sets of methylation sites accounting for heterogeneity in prostate cancer.	10.1371/journal.pone.0204371	2018	0.0	0.0		1.0	2022-03-08T14:37:33.000Z	2026-02-02T19:46:52.000Z	c0279a1e-2bf3-4fe2-8d40-9c9cc040608d	undefined	o94lxlja8t				Starting_TSV	Match	No Match
63516fedb9c880af1f305bab	No	55 mouse ES ChIP-seq and DNaseI-seq experiments from a variety of sources, each with up to 4000 pos and 10000 negs.	mot checked	20 repetitions of random selection of 100 data points for testing			No		Not explicitly given but 20 replicates are shown in the ROC graphs	ROC-AUC	20 random replicates of train/test splits			Not available for SVM, but the focus of the study was a non ML-based tool: https://github.com/seqcode/multigps	not given	transparent: discriminative motif discovery touched upon	binary classification			SVM	No	global features (k-mers)	not clear: in one place they mention 55-dimensional vectors, but also 4-mer frequencies (256) and 3 additional features.	None provided	No	p = f	Not mentioned			6312169df3794236aa9879fc	24675637.0	PMC3967921	02/02/2026 19:46:52	Mahony S, Edwards MD, Mazzoni EO, Sherwood RI, Kakumanu A, Morrison CA, Wichterle H, Gifford DK.	PLoS computational biology	An integrated model of multiple-condition ChIP-Seq data reveals predeterminants of Cdx2 binding.	10.1371/journal.pcbi.1003501	2014	0.0	0.0		1.0	2022-03-08T13:05:23.000Z	2026-02-02T19:46:52.000Z	783e858f-5f8f-4b82-afb8-d61eaf60be03	undefined	ek1c4fvz6e				Starting_TSV	Match	No Match
63516fedb9c880af1f305b2e	yes: https://www.olink.com/application/mgh-covid-19-study/ upon request	clinical and normalized protein expression profile data for 306 COVID-19 patients and 78 other patients (control) from the Olink website; 42 pos and 264 neg	Not commented	only LOOCV was used			Confusion matrices in the supplement: http://14.139.62.220/covidprognosis/supple.php	None, only the 44 different models used in the study were compared		MCC, Accuracy, Sensitivity, Specificity, area under ROC	LOOCV			only on the web: http://14.139.62.220/covidprognosis/	not given	transparent: identification of proteins associated with labels based on feature selection methods in WEKA	possibly both depending on the model			44 different types of ML classification algorithms available in WEKA (v3.8.2)	Partially in the supplement: http://14.139.62.220/covidprognosis/supple.php	global features	1463	none	No	no details provided, but p varies based on the models	no details provided			6312169df3794236aa9879fc	34093642.0	PMC8175075	02/02/2026 19:46:52	Sardar R, Sharma A, Gupta D.	Frontiers in genetics	Machine Learning Assisted Prediction of Prognostic Biomarkers Associated With COVID-19, Using Clinical and Proteomics Data.	10.3389/fgene.2021.636441	2021	0.0	0.0		1.0	2022-03-07T11:16:44.000Z	2026-02-02T19:46:52.000Z	9f2653e6-d8fb-455a-ac07-31ffd86fbf52	undefined	1de6o2bxxv				Starting_TSV	Match	No Match
63516fedb9c880af1f305b7e	All processed data are available on GitHub: https://github.com/umutcaglar/ecoli_multiple_growth_conditions (permanent archived version available via zenodo: 10.5281/zenodo.1294110)	Previous publication: 155 E. coli samples (102 have both mRNA and protein abundance data; 50 have only mRNA abundance data; 3 have only protein abundance data).  External validation performed on 5 samples from a different publication.	No measure of independence provided	Diffent conditions are considered: carbon sources (glucose, glycerol, gluconate, and lactate),  sodium concentrations (base and high), and magnesium concentrations (low, base, and high). Different splits are therefore adopted. Splitting: training/validation set:test set=80:20. Training:validation=75:25 (x 10 independent runs). Semi-random split preserving the ratios of different conditions between the training/validation and the test subsets. 			no	Different kernels are used. Random forests are used.	Distributions over the samples are reported, but no statistical evaluation is computed.	F1 score	Independent test set + externa test set			Allanalysis scripts are available on GitHub: https://github.com/umutcaglar/ecoli_multiple_growth_conditions (permanent archived version available via zenodo: 10.5281/zenodo.1294110)		Black box	classification			SVM	All analysis scripts are available on GitHub: https://github.com/umutcaglar/ecoli_multiple_growth_conditions (permanent archived version available via zenodo: 10.5281/zenodo.1294110)	level of transcripts/proteins expression	Number of transcripts/proteins in E.coli unspecified. Feature selection performed on training data with PCA: 10 features retained	f = 10, N_train ‚âà 90 - Low dimensional SVM	No	Kernel selected with a grid search and performance evaluation on the validation set	No			6312169df3794236aa9879fe	30388153.0	PMC6214550	02/02/2026 19:46:52	Caglar MU, Hockenberry AJ, Wilke CO.	PloS one	Predicting bacterial growth conditions from mRNA and protein abundances.	10.1371/journal.pone.0206634	2018	0.0	0.0		1.0	2022-03-06T19:15:35.000Z	2026-02-02T19:46:52.000Z	c3dde50f-83db-4289-96ec-dbfdca27594b	undefined	duas5qkjag				Starting_TSV	Match	No Match
63516fedb9c880af1f305b2f	Yes, but the server is not working: http://protein.ict.ac.cn/ProFOLD	PDB, CATH, CASP13.  31,247 non-redundant domains + 104 domains from CASP13 (test). The number of contacts (positive) and non contacts (negative) is not reported	Training/Validation: 35% sequence similarity cluster representatives of CATH (Mar 16, 2018). Test set released after 2018. Similarity with Training/Validation unclear	Training: 29,247 domains; Validation: 1,820 domains; Test: 104 domains. Number of positive and negative not reported.			No	RaptorX, AlphaFold, trRosetta	No confidence intervals nor statistical significance reported	Precision of contact predictions at different thresholds	Independent test set 			https://github.com/fusong-ju/ProFOLD		Black box	classification (interresidue contacts) and regression (interresidue distance)			Deep neural network, end-to-end learning	The overall architecture is available in supporting information and released in github (https://github.com/fusong-ju/ProFOLD). Webserver is indicated but not functional (http://protein.ict.ac.cn/ProFOLD)	Multiple sequence alignment	Given the MSA, for each pair target-homologous sequences, each position is encoded with a 41-valued vector. Total encoding = Sequence length x Number of homologous sequences in MSA (Max 1000) x 41. No feature selection applied.	MSA sampling and distance matrix cropping in training process is adopted to avoid potential overfitting as well	No	6.46 M to 16.46 M parameters. Performance evaluated on the validation set.	Not mentioned			6312169df3794236aa9879fe	33953201.0	PMC8100175	02/02/2026 19:46:52	Ju F, Zhu J, Shao B, Kong L, Liu TY, Zheng WM, Bu D.	Nature communications	CopulaNet: Learning residue co-evolution directly from multiple sequence alignment for protein structure prediction.	10.1038/s41467-021-22869-8	2021	0.0	0.0		1.0	2022-03-06T18:09:45.000Z	2026-02-02T19:46:52.000Z	0e4a9a17-a5bf-41d9-84a8-1eb49a8f35e5	undefined	3qzygb13ir				Starting_TSV	Match	No Match
63516fedb9c880af1f305b9e	Partially available: primary data available from external sources. Processed input data are unavailable (after filtering, missing data imputation and consistency check). External dataset: Arabidopsis: http://publiclines.versailles.inra.fr/page/33 Barley: http://triticeaetoolbox.org Maize and Wheat (P√©rez-Rodr√¨guez) : https://www.uni-giessen.de/fbz/fb09/institute/pflbz2/population-genetics/downloads (No longer available) Rice: http://www.ricediversity.org/data/index.cfm Wheat (CIMMYT): part of the R package BLR (https://cran.r-project.org/web/packages/BLR/index.html)	Arabidopsis: 417 lines, 3 traits, 69 Simple Sequence Repeats, External dataset Barley: 381 lines, 3945 SNPs, External dataset Maize: 264 lines, 1076 SNPs, External dataset Rice: 383 lines, 34 traits, 1311 SNPs, External dataset Wheat (CIMMYT): 599 lines, 4 evironments, 1279 markers, External dataset Wheat (P√©rez-Rodr√¨guez): 306 lines, 2 traits, 1695 markers, External dataset	Random split among cross-validation sets. No similarity threshold applied.	Regression problem (no positive/negative examples). 10 fold cross-validation			No	Different methods are compared: reproducing kernel Hilbert space regression, Stochastic search variable selection, Bayesian mixture regression model, Random forests, LambdaMART, Bayesian lasso, Ordinal McRank, BayesC, RankSVM, Gradient boosted regression trees, Extended Bayesian lasso, Weighted Bayesian shrinkage regression, Ridge	No confidence interval provided	Pearson's correlation, Kendall's correlation, normalized discounted cumulative gain (new index introduced in the paper)	Cross-validation			External softwares are adopted		scarcely interpretable	regression-ranking			Regression methods: Ridge, RKHS, Bayesian methods, Random Forests. Ranking based regression methods: McRank (poinwise, multiclass, ordinal), RankSVM, LambdaMART.	No	Genotype: 0/1 for each marker, SNP or repeat.	69 to 3945 depending on the set. Random feature selection (60%) for random forests	in this problem n_data << p	No	Unclear	Small learning rate adopted			6312169df3794236aa9879fe	26068103.0	PMC4466774	02/02/2026 19:46:52	Blondel M, Onogi A, Iwata H, Ueda N.	PloS one	A Ranking Approach to Genomic Selection.	10.1371/journal.pone.0128570	2015	0.0	0.0		1.0	2022-03-06T17:08:06.000Z	2026-02-02T19:46:52.000Z	e92f4f40-8f6b-4688-9bf5-24409e3eb9bb	undefined	4ecj3a4lnj				Starting_TSV	Match	Match
63516fedb9c880af1f305b70	no 	106 patients with non small cell lung cancer, 22 positive (>=2) long radiation pneumotis and 84 (<2) negative patients. Each patient is annotated with 230 features. Patient data comes from another publication 28237401. Data does not seem to be downloadable.	not discussed	System of inner and outer cross validation (5-fold) with multiple repeats of both loops. Distributions across sets or stratifications were not reported.			not available	no other methods compared	DeLong method to compare AUC's 	average AUC	double loop of cross-validations and multiple repeats to overcome the imbalance in the data set.			not available	not mentioned	dependent on the classification approach.  Most relevant features are explained. An expert should be able to get some understanding from the remaining features,  yet interpreting the latent variables produced by the VAE will be complicated.	binary prediction 			Several methods are tested to find the optimal feature subset for classification (see fig1 in the paper). Includes multi-layer perceptrons, random forest, SVM and a combination of a VAE with a MLP.	some parts yes, other parts no	not specified in this article (ref to earlier article)	230 features, wrapper method with cross-validation to find the optimal subset.  VAE extraction applied also to data to identify other features	double loop of cross validation (5-fold) and ,multiple runs to obtain statistically robust results. 	not meta-predictions, everything is performed on the raw data. 	Not all details provided about the parameters for each model. RF optimised by verifying different depths (yet number of trees was not mentioned). No details about SVM. VAE and MLP are shown in figures				6312169df3794236aa9879f2	30891794.0	PMC6510637	02/02/2026 19:46:52	Cui S, Luo Y, Tseng HH, Ten Haken RK, El Naqa I.	Medical physics	Combining handcrafted features with latent variables in machine learning for prediction of radiation-induced lung damage.	10.1002/mp.13497	2019	0.0	0.0		1.0	2022-03-03T13:08:10.000Z	2026-02-02T19:46:52.000Z	94f52be3-39fa-4f72-9fc2-9ab4ae112839	undefined	st1jp9fgiv				Starting_TSV	Match	No Match
63516fedb9c880af1f305b31	Dataset 1: publicy available with the publication at doi: https://doi.org/10.1016/j.imu.2019.100275. Dataset splits not provided.  Dataset 2: publicy available with the publication at doi: https://doi.org/10.1101/2020.11.02.365536	"Dataset 1: 387 patients labeled as ""survived"" (pos) (N_pos = 335) and ""dead"" (neg) (N_neg = 49).  Dataset 2: 375 patients labeled as ""survived"" (pos) (N_pos = 201) and ""dead"" (neg) (N_neg = 174).  Both datasets used in previous publications."	No redundancy between the subsets.	Dataset 1 was splitted in two subsets. 80% of the data used for training and validation; 20% of the data used for testing.  Dataset 2 was entirely used for testing			Yes, it is available at: https://github.com/tawsifur/Mortality-severity-prediction-using-blood-biomarkers	Compared with Random Forest, Support Vector machine, K-nearest neighbor, XGBoost, and Extra-tree	The performance metrics have confidence intervals. No statistical significance is claimed.	Accuracy, Precision, Sensitivity, F1-Score, Specificity, AUC	Predictor evaluated on independent datasets			Yes, the code is available at: https://github.com/tawsifur/Mortality-severity-prediction-using-blood-biomarkers	Not stated	Black box	Regression (survival probability)			Logistic Regression	Yes, the 6 parameters resulted from training are reported on the paper	Feature selection (operated on the training set finding the top 5 significant features) Imputation of missing data points through MICE Balancing on the dataset (regarding pos and neg cases) through SMOTE	"5 selected out of 18. The 5 more relevant feature were selected using chi-square test to identify the feature which significantly differ between the ""pos"" cases and the ""neg"" cases.  It is not clear whether the feature selection was performed just on training data. "		No	6 (inferred from the context)	No			6312169df3794236aa987a01	34573923.0	PMC8469072	02/02/2026 19:46:52	Rahman T, Al-Ishaq FA, Al-Mohannadi FS, Mubarak RS, Al-Hitmi MH, Islam KR, Khandakar A, Hssain AA, Al-Madeed S, Zughaier SM, Chowdhury MEH.	Diagnostics (Basel, Switzerland)	Mortality Prediction Utilizing Blood Biomarkers to Predict the Severity of COVID-19 Using Machine Learning Technique.	10.3390/diagnostics11091582	2021	0.0	0.0		1.0	2022-02-28T14:25:46.000Z	2026-02-02T19:46:52.000Z	628354fa-6ad5-4a13-9ba2-0c5e5b0fd8dc	undefined	k4k0b6uu1l				Starting_TSV	Match	No Match
63516fedb9c880af1f305b1b	no.	265 candidate proteins containing hydroxylated prolines and 34 candidate proteins containing hydroxylated lysines were collected from the UniProtKB/Swiss-Prot database (version 2014_1, www.uniprot.org). Sequence segments around the hydroxylation sites and non-hydroxylation sites were extracted as positive and negative training datasets, respectively. 	Not stated.	After removing the identical sequence, the original datasets contain 659 positive sites and 3855 negative sites for hydroxyproline from 112 proteins, and 97 positive sites and 855 negative sites for hydroxylysine from 25 proteins. The size of the negative datasets is much larger (approximate ratio of 1:6) than that of the positive training datasets. After addressing this problem, ratios of 1:1 and 1:3 of the number of positive samples and the number of negative samples were used to construct the negative training set. (Reduction of the negative set).  Test set is not described. Validation set:  randomly split 10% of the samples from the dataset as an independent test dataset, and the remaining 90% of the samples as a training dataset.			no.	iHyd-PseAAC (Xu, 2014) and PredHydroxy (Shi, 2015). Performance measures are compared.	Not stated.	Sensitivity, specificity, accuracy.	Independent test dataset.			The MATLAB package of OH-PRED is available as Supplementary files.	Not stated.	black box.	Classification.			SVM classification method.	no.	The feature vectors are encoded in a bi-profile manner. This approach is explained in the paper (2.2.2. Adapted normal distribution bi-profile Bayes (ANBPB)).	For feature extraction a modified version of classical bi-profile Bayes was used. Thirteen physicochemical features were selected. They used the jackknife test to select important features and optimize all parameters.	Not stated.	no.	2 parameters ( c = 4, Œ≥ = 0.25 for the hydroxyproline prediction and c = 4, Œ≥ = 0.125 for the hydroxylysine prediction). Parameters were downloaded from http://www.matlabsky.com and optimized by the SVMcgForClass program. 	no.			6312169df3794236aa9879df	26957000.0		02/02/2026 19:46:52	Jia CZ, He WY, Yao YH.	Journal of biomolecular structure & dynamics	OH-PRED: prediction of protein hydroxylation sites by incorporating adapted normal distribution bi-profile Bayes feature extraction and physicochemical properties of amino acids.	10.1080/07391102.2016.1163294	2017	0.0	0.0		1.0	2022-02-23T16:01:54.000Z	2026-02-02T19:46:52.000Z	8becb7c4-2b09-4c8a-b48f-eb340481376d	undefined	fozqqtqf2d				Starting_TSV	Match	No Match
63516fedb9c880af1f305b55	Only unfiltered data: NCBI dbGaP repository phs001273.v3.p2, https://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi?study_id=phs001273.v3.p2.	Originally, there were 533,631 SNPs from 57,775 individuals in the OncoArray-TRICL Consortium population-based study. The filtered data had 27722 individuals (14935 positive and 12787 negative cases). Two simulated datasets were also used to evaluate the type I error rate and power. 	None between the splits, but participants who were close relatives (second degree relatives or closer) and duplicate individuals were filtered out from the beginning.	random sampling: 2/3 into a training set and 1/3 into the testing set; 2-fold cross-validation			No	Only comparison against previous state-of-the-art method, Survival MDR (Surv-MDR)	None for the model for the real data; two simulation studies were created to estimate the 5% type I error and power; the significance of the selected models (intermediate step) was evaluated by a 10000-fold permutation test.	time-dependent ROC and its AUC	2-fold cross validation + independent test set.			https://github.com/jluyapan/ESMDR	unknown	transparent: identification of genes 	binary			a version of Lasso-penalized Cox regression	No	filtering of data points and features (SNPs)	f = 533 000 SNPs; SNPs in linkage disequilibrium were removed, using a correlation threshold of 0.1 and resulting in 108 254 features (not clear if this was done only based on the training data). 		No	Not stated explicitly; should be at least n+n^2 for n=108 254	Yes: lasso + selecting only the SNPs involved in the top 1000 one-way and all the SNPs from the top 1000 two-way interactions models based on 2-fold CV.			6312169df3794236aa9879fc	33126877.0	PMC7596958	02/02/2026 19:46:52	Luyapan J, Ji X, Li S, Xiao X, Zhu D, Duell EJ, Christiani DC, Schabath MB, Arnold SM, Zienolddiny S, Brunnstr√∂m H, Melander O, Thornquist MD, MacKenzie TA, Amos CI, Gui J.	BMC medical genomics	A new efficient method to detect genetic interactions for lung cancer GWAS.	10.1186/s12920-020-00807-9	2020	0.0	0.0		1.0	2022-02-23T08:54:37.000Z	2026-02-02T19:46:52.000Z	2598df6d-6714-4983-8db6-20a2fe1309b4	undefined	opx6a1juph				Starting_TSV	Match	No Match
63516fedb9c880af1f305b73	Dataset 1: No.  Dataset 2: No.  Dataset 3. Yes. URL: https://portal.gdc.cancer.gov/projects/TCGA-THCA	They use different datasets in the paper.  Dataset 1: obtained composing data from different databases. miRNA-disease association data were retrived from the database HMDD v2.0. They added miRNA information using data from the miRBase database and added disease information using MeSH descriptors. The final dataset was used to compose a matrix where miRNA associated to a disease was labeled 1 and 0 otherwise. N, N_pos, and N_neg not stated, but could be inferred from the text.   Dataset 2: miRNA-disease association data from HMDD v1.0  Dataset3: miRNA expression data associated with Tyroid cancer, retrived from The Cancer Genome Atlas (Project TCGA-THCA).  Dataset 1 not used in previous papers. Datasets 2 and 3 used in the community.	Not stated.	Not stated, but could be inferred from the text.			No.	Compared with other 4 methods for predicting miRNA-disease association. Statistical significance is given as proof for performance.	The performarce difference is statistically significant compared to other 4 algorithms	Area Under Curve (AUC)	Leave-one-out validation and 5-fold cross-validation on Dataset 1  5-fold cross validation on Dataset 3 			Yes for one specific case: https://github.com/alcs417/AMVML	Not stated	Trasparent. Prediction based on similarity between different miRNAs and different diseases. If a given miRNA is associated to a given disease D, it might be associated only with diseases similar to D	Regression (association probability with a disease)			Novel semi-supervised algorthm called AMVML (Adaptive Multi-View Multi-Label). No reason stated for not having published it before.	No	Not transformed	Not stated, but could be inferred from the text.		No.	Not stated, maybe could be inferred from the paper.	Not stated			6312169df3794236aa987a01	30933970.0	PMC6459551	02/02/2026 19:46:52	Liang C, Yu S, Luo J.	PLoS computational biology	Adaptive multi-view multi-label learning for identifying disease-associated candidate miRNAs.	10.1371/journal.pcbi.1006931	2019	0.0	0.0		1.0	2022-02-11T10:17:27.000Z	2026-02-02T19:46:52.000Z	874bbd17-26ef-4992-9473-d3a4a22d9280	undefined	p9igs00nw2				Starting_TSV	Match	No Match
63516fedb9c880af1f305b9c	No	"Dataset generated by the authors. Total number of instances not clearly stated (they state ""around 250""), but could be inferred from Supplementary Informations. N_pos or N_neg not stated. Dataset not used in previous papers."	Not stated	Not stated			No	Comparison was made among the methods used, based on the ROC Area	Not stated	ROC Area	Experimental Validation			Not available.	Not stated.	Both transparent (e.g. Decision Trees) and black box algorithms (e.g. Neural Networks) were used.	Classification			Many algorithms compared: Decision Trees, Support Vector Machine, Neural Network (multilayer perceptron), Naive Bayes, Decision Rule	No.	Not clearly stated. They just mention the elimination of non valid attributes (e.g. missing or NaN value for some features) and the selection of attributes through Chi Square Evaluation.	"Not clearly stated (they say ""around 100"")"		No	Not stated 	No.			6312169df3794236aa987a01	27488918.0	PMC5034704	02/02/2026 19:46:52	Liu HC, Goldenberg A, Chen Y, Lun C, Wu W, Bush KT, Balac N, Rodriguez P, Abagyan R, Nigam SK.	The Journal of pharmacology and experimental therapeutics	Molecular Properties of Drugs Interacting with SLC22 Transporters OAT1, OAT3, OCT1, and OCT2: A Machine-Learning Approach.	10.1124/jpet.116.232660	2016	0.0	0.0		1.0	2022-02-09T10:22:18.000Z	2026-02-02T19:46:52.000Z	b96ff1c7-58ce-47a8-8d72-140992f72b1c	undefined	0mlbkqclbr				Starting_TSV	Match	No Match
63516fedb9c880af1f305bb0	http://www3.ntu.edu.sg/home/EPNSugan/index_files/AFP_Pred.htm	source: 10.1016/j.jtbi.2010.10.037 481 antifreeze proteins and 9493 non-antifreeze proteins	To get rid of redundancy and homology bias, the sequences with ‚â•40% sequence similarity have been removed using program CD-HIT	training dataset contains 300 antifreeze proteins randomly selected from the 481 antifreeze proteins and 300 non-antifreeze proteins randomly selected from the 9493 non-antifreeze proteins. The test dataset contains the remaining 181 antifreeze proteins and 9193 non-antifreeze proteins.			No	comparison with work from 10.1016/j.jtbi.2010.10.037. this study obtains accuracy	Model's accuracy	sensitivity (S_n), specificity (S_p), and accuracy (Acc), ROC	Ten-fold cross validation & independent testing dataset			No	20 seconds for 500 amino acids sequences		classficiation			SVM	http: //59.73.198.144/AFP_PSSM/	Evolutionary Information, Amino Acid and Dipeptide Composition, Chou‚Äôs Pseudo Amino acid Composition	400, PSSM-400 (composition of occurrences of each type of amino acid corresponding to each type of amino acids in protein sequence)	No	4 SVM models based on amino acids composition, dipeptides composition, Chou‚Äôs PseAAC and PSSM-400	p=2, regularization parameter C and the kernel width parameter Œ≥.	No			6312169df3794236aa987a1b	22408447.0	PMC3292016	02/02/2026 19:46:52	Zhao X, Ma Z, Yin M.	International journal of molecular sciences	Using support vector machine and evolutionary profiles to predict antifreeze protein sequences.	10.3390/ijms13022196	2012	0.0	0.0		1.0	2022-01-31T11:13:16.000Z	2026-02-02T19:46:52.000Z	87a282cd-4534-4ee9-a07e-455260a89a6a	undefined	kottgt7uak				Starting_TSV	Match	No Match
63516fedb9c880af1f305b75	Dataset 1: yes. Dataset ID: EGAD00001000200; URL: https://ega-archive.org/datasets/EGAD00001000200; license: stated with specific Data Use Ontology (DUO) codes: DUO:0000005, DUO:0000026, DUO:0000027, DUO:0000029, DUO:0000019, DUO:0000028.  Dataset 2: no (available upon request at: https://thl.fi/en/web/thl-biobank/for-researchers/sample-collections/twin-study)  Dataset 3: yes. Dataset id: GSE42861; URL: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=gse42861; license: not stated.  Dataset 4: yes. Dataset id: GSE50660; URL: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE50660; license: not stated.	For each dataset 3 categories are considered: Smokers (S), Former Smokers (FS), and Never Smokers (NS). For each dataset N_S, N_FS, and N_NS will be used for the number of Smokers, Former Smokers, and Never Smokers respectively. For each dataset only data related to DNA methilation.  Dataset 1: data from 474 individuals from the Dietary, Lifestyle and Genetic determinants of Obesity and Metabolic syndrome (DILGOM). N_S = 113, N_FS = 118, N_NS = 243. Used in other papers.  Dstaset 2: data from 408 individuals from the Finnish Twin Cohort (FTC). N_S = 67; N_FS = 141; N_NS = 200. Used in other papers.  Dataset 3: data from 687 individuals from the GSE42861 in Gene Expression Omnibus (GEO). N_S = 266; N_FS = 228; N_NS = 193. Used in other papers.  Dataset 4: data from 464 individuals from the GSE50660 in Gene Expression Omnibus (GEO). N_S = 22; N_FS = 263, N_NS = 179.	Dataset 1, 2, 3 and 4 are independent from each other.	Dataset 1: Used as training test. Used for cross-validation randomly subdividing data in 90% training and 10% test data. Distribution of S, FS, and NS after random subdivisions is not stated.  Dataset 2: used as independent set to verify generalization capability of the classifier. Distribution of S, FS, and NS stated above.  Dataset 3: used as independent set to verify generalization capability of the classifier. Distribution of S, FS, and NS stated above.  Dataset 4: used as independent set to verify generalization capability of the classifier. Distribution of S, FS, and NS stated above.			No.	Comparison with non-ML methods provided. An extensive comparison was not performed due to differences in the methods.	Not stated.	Sensitivity and Specificity	Using independent dataset.			Code is available at: https://github.com/sailalithabollepalli/EpiSmokEr. No License provided.	Not stated.	Black box.	Regression (probability scores of being Smoker, Former Smoker or Never Smoker)			Least absolute shrinkage and selection operator (LASSO).	The code and the parameters used are available at: https://github.com/sailalithabollepalli/EpiSmokEr. No license specified.	Training dataset are quantile-normalized before training.	Number of f = 122.	N/A.	No.	Number of p not stated, but could be inferred from the context.	"Introduction of a parameter ""lambda"" in the penalized log likelihood procedure for selecting the ""trainable"" parameter. Selection of the ""lambda"" parameter through cross-validation."			6312169df3794236aa987a01	31466478.0		02/02/2026 19:46:52	Bollepalli S, Korhonen T, Kaprio J, Anders S, Ollikainen M.	Epigenomics	EpiSmokEr: a robust classifier to determine smoking status from DNA methylation data.	10.2217/epi-2019-0206	2019	0.0	0.0		1.0	2022-01-28T21:38:37.000Z	2026-02-02T19:46:52.000Z	54b0bca4-f5c2-48bb-81d3-2aa6f88cbafc	undefined	c80vqcn356				Starting_TSV	Match	No Match
63516fedb9c880af1f305b76	Yes. Made available by the authors at: https://github.com/HanJingJiang/GIPAE. No License stated.	Dataset 1: Cdataset (N_pos = 2532; N_neg = 2532); Used in previous papers.  Dataset 2: Fdataset (N_pos: 1933; N_neg = 1933); Used in previous papers.  Both databases contain established drug-disease interactions, that they used as positive cases. Negative cases generated randomly pairing diseases and drugs available in the databases. The number of negative cases generated is always equal to the number of positive cases. Both the dataset are integrated with information coming from other databases (PubChem, MeSH).	Dataset 1 and 2 are independent. No redundancies stated in 10-fold cross validation.	Both databases used as training and validation set using 10-fold cross-validation.			No.	Compared with other predictors of drug-disease association: DrugNet, HGBI, KBMF, MBiRW, DRRS. 	No confidence intervals or statistical significance stated.	Sensitivity, Specificity, F1-score, and accuracy.	10-fold cross vaildation			The repository of the code exists at https://github.com/HanJingJiang/GIPAE but the actual code has been removed.	Not stated	Algorithm 1: Transparent. Input features regarding diseases and drugs are transform to provide a sort of similarity measure between different drugs and different diseases. These information are then used to train Algorithm 2.  Algorithm 2: black box.	Classification			Algorithm 1: they use a novel machine learning method for feature extraction, called Gaussian interaction profile kernel and autoencoder (GIPAE).   Algorithm 2: they use Random Forest for classification.	No.	Input information is transformed by GIPAE, in order to extract new features from data. The transformed data are then used for training a Random Forest algorithm.	Not stated. 		No.	Not stated.	Not stated			6312169df3794236aa987a01	31534955.0	PMC6732622	02/02/2026 19:46:52	Jiang HJ, Huang YA, You ZH, You ZH.	BioMed research international	Predicting Drug-Disease Associations via Using Gaussian Interaction Profile and Kernel-Based Autoencoder.	10.1155/2019/2426958	2019	0.0	0.0		1.0	2022-01-28T14:49:16.000Z	2026-02-02T19:46:52.000Z	d7de0b56-fc2d-4ad5-a3d8-d33f721d5f36	undefined	nzflw09djs				Starting_TSV	Match	No Match
63516fedb9c880af1f305bb8	Yes but need to contact the authors apparently	Yes,  Prof. Connie Cepko at Harvard Medical School, claimed publicly available	Yes, cross-validation	Yes			Yes.	KStar, comparison vs. MLP, C4.5	Yes	Rappel/Precision, AUR	Cross validation			Weka	No	Black box	Classification			KStar (K-NN ?), Not novel, Multilayer perceptron, C4.5	No	Gene expression data	No feature selection.		No	Depends on algorithms tested.	None.			6312169df3794236aa987a36	16524483.0	PMC1421439	02/02/2026 19:46:52	Wang H, Zheng H, Simpson D, Azuaje F.	BMC bioinformatics	Machine learning approaches to supporting the identification of photoreceptor-enriched genes based on expression data.	10.1186/1471-2105-7-116	2006	0.0	0.0		1.0	2022-01-28T00:13:56.000Z	2026-02-02T19:46:52.000Z	5ad4302c-53ed-4207-b229-6528206e2f0b	undefined	4ord5sdl9n				Starting_TSV	Match	No Match
63516fedb9c880af1f305b79	Available upon request to the authors.	Data from previous publication and direct experiments. N= 26.		training set n = 26, divided in 4 classes. Class 1 = 10 elements, class 2 = 12, class 3 = 3, class 4 = 1.			no	No comparison.	25 of 26 cases correctly assigned. There is a table with the probability of each case to be correctly classified.	Not stated.	Cross-validation, using a leave-one-out approach.			Model developed based on an artificial neural network (ANN; JMP software, v12.0.1). There is no executable file available.	Not stated.	Transparent: classifies chemicals based on AUC parameters. Small dataset, few features and parameters considered.	Classification (4 classes).			ML prediction algorithm was developed based on an artificial neural network (ANN; JMP software, v12.0.1).	no.	Data were comprised of AUC values from each of the 26 cases.	2 factors.	Not stated.	no.	3 hidden nodes.	Not stated.			6312169df3794236aa987a03	31132080.0	PMC6657583	02/02/2026 19:46:52	Bernacki DT, Bryce SM, Bemis JC, Dertinger SD.	Toxicological sciences : an official journal of the Society of Toxicology	Aneugen Molecular Mechanism Assay: Proof-of-Concept With 27 Reference Chemicals.	10.1093/toxsci/kfz123	2019	0.0	0.0		1.0	2022-01-20T15:22:27.000Z	2026-02-02T19:46:52.000Z	1788b70c-e3a7-4a8f-96a4-95a8ea1f3f40	undefined	9ydtb6du8j				Starting_TSV	Match	No Match
63516fedb9c880af1f305b95	Yes	PDB	Independence	100 pos, 2000 neg			Yes			ROC curve	Cross validation			No	Yes	Black box	Binary classification but evaluated a ranker: ROC			Ensemble, incl. Adaboost Random Forrest, and SVM	No	Compounds	No	Class imbalance is compensated	No	Protocol is clear	No but kernel: RBF			6312169df3794236aa987a36	27127534.0	PMC4834164	02/02/2026 19:46:52	Wang MY, Li P, Qiao PL.	Computational and mathematical methods in medicine	The Virtual Screening of the Drug Protein with a Few Crystal Structures Based on the Adaboost-SVM.	10.1155/2016/4809831	2016	0.0	0.0		1.0	2022-03-30T16:22:15.000Z	2026-02-02T19:46:52.000Z	2882dbba-6819-442d-b027-50598cd5a0a6	undefined	yirxngqbuc				Starting_TSV	Match	No Match
63516fedb9c880af1f305b1f	All data included in this study is available upon request at ELIXIR Luxemburg ( https://doi.org/10.17881/th9v-xt85 )	Clinical data collected by the authors N_neg 330, N_pos 304		N_pos: Discovery 227 and Validation 77			no	no	no	Accuracy (94.81% for the C4 prediction model, and 96.72% for the multi-classification model.)	independent dataset 			https://lbai-infolab.github.io/SjTree/	no	Black box	classification			composite model  with several stepp with different classification algorithms (hclust, K-means, mclust, Random Forest)	no	global features (RNA-Seq, Flow cytometry)	features selection by Boruta algorithm ( 10.18637/jss.v036.i11 )	n/a 	no	Molecular subgroups discovery:  - Step 1: Unsupervised gene selection,  - Step 2: Robust consensus clustering (hclust, K-means, mclust)  - Step 3: Identification of molecular signature (one-way ANOVA and Random Forest),  - Step 4: Robustness classification  - Step 5: Classification of discordant patients -> definition of 4 cluster  Composite model for cluster prediction: #1 xgboost-tree: predict C4 vs all #2 multi-classification:  C1, C2, or C3				6312169df3794236aa9879e5	34112769.0	PMC8192578	02/02/2026 19:46:52	Soret P, Le Dantec C, Desvaux E, Foulquier N, Chassagnol B, Hubert S, Jamin C, Barturen G, Desachy G, Devauchelle-Pensec V, Boudjeniba C, Cornec D, Saraux A, Jousse-Joulin S, Barbarroja N, Rodr√≠guez-Pint√≥ I, De Langhe E, Beretta L, Chizzolini C, Kov√°cs L, Witte T, PRECISESADS Clinical Consortium, PRECISESADS Flow Cytometry Consortium, Bettacchioli E, Buttgereit A, Makowska Z, Lesche R, Borghi MO, Martin J, Courtade-Gaiani S, Xuereb L, Guedj M, Moingeon P, Alarc√≥n-Riquelme ME, Laigle L, Pers JO.	Nature communications	A new molecular classification to drive precision treatment strategies in primary Sj√∂gren's syndrome.	10.1038/s41467-021-23472-7	2021	0.0	0.0		1.0	2022-03-29T23:01:05.000Z	2026-02-02T19:46:52.000Z	1b20e970-8ffe-4825-aebd-9e1d924a6d06	undefined	r4ntb0iqha				Starting_TSV	Match	No Match
63516fedb9c880af1f305b61	no	 6132 pECG beats was used for training in three class: Control, Mild, Severe	no	10-fold cross-validation			no	no	n/a 	sensitivity,  PPV, F1-score	10-fold cross-validation			no	no	Black box	classification			neural network	no	pECG signal features: (A) QRS duration,  (B) QT interval,  (C) ST deviation,  (D) T wave duration,  (E) QRS amplitude  (F) T wave amplitude			no	 multiple setup was tested, selected with : 5 hidden layers and 7 hidden neurons per layer, 	no			6312169df3794236aa9879e5	31404081.0	PMC6690680	02/02/2026 19:46:52	Ledezma CA, Zhou X, Rodr√≠guez B, Tan PJ, D√≠az-Zuccarini V.	PloS one	A modeling and machine learning approach to ECG feature engineering for the detection of ischemia using pseudo-ECG.	10.1371/journal.pone.0220294	2019	0.0	0.0		1.0	2022-03-29T01:42:53.000Z	2026-02-02T19:46:52.000Z	006ff363-6c34-4593-86af-0aa3b8af83d1	undefined	2so9xiaxer				Starting_TSV	Match	No Match
63516fedb9c880af1f305b43	preprocessed data: https://drive.google.com/file/d/1JBp9RH9-yBEdtkNYDi6wWL79o62JD5Td/view     	two public COVID-19 CT datasets:   -   https://www.medrxiv.org/content/10.1101/2020.04.24.20078584v3 2482 CT images from 120 patients,  N_pos 1252, N_neg 1230   -   http://arxiv.org/abs/2003.13865 N_pos 349 CT images from 216 patients,  N_neg 397 CT images from 171 patients	no	no			no	firstly comparision to COVID-Net method https://doi.org/10.1038/s41598-020-76550-z (Scientific Reports)  by ROC  AUC  further comparisions by p-value:  - Series-Adapter https://scholar.google.com/scholar?as_q=Learning+multiple+visual+domains+with+residual+adapters&as_occt=title&hl=en&as_sdt=0%2C31   - Parallel-Adapter https://ieeexplore.ieee.org/document/8578945  - MS-Net https://doi.org/10.1038/s41598-020-76550-z	outperforming the original COVID-Net trained on each dataset by 12.16% and 14.23% in AUC respectively,	Accuracy, F1 score, Sensitivity, Precision, AUC	four-fold cross-validation			https://github.com/med-air/Contrastive-COVIDNet		color maps using Grad-CAM	regression			deep learning, novel approach	implemented with PyTorch [38] using an Nvidia TITAN Xp GPU.	datasets, all images are first resized to 224 √ó 224 in axial plane,		no	no	Batch Normalization   	no			6312169df3794236aa9879e5	32915751.0	PMC8545175	02/02/2026 19:46:52	Wang Z, Liu Q, Dou Q.	IEEE journal of biomedical and health informatics	Contrastive Cross-Site Learning With Redesigned Net for COVID-19 CT Classification.	10.1109/jbhi.2020.3023246	2020	0.0	0.0		1.0	2022-03-28T12:23:43.000Z	2026-02-02T19:46:52.000Z	c61651cd-193f-4b5e-aa19-77eed7eb2649	undefined	217brrych3				Starting_TSV	Match	No Match
63516fedb9c880af1f305b65	no	datasets are based on mouse strains generated by the lab itself The aggerated dataset has complete time-series (7 timepoints) data for 3479 proteins and 513 metabolites.	no	no			no	Comparison only betweeen models created by the authors	no	pathway enrichment analysis using Reactome knowledgebase	no			no		black box	classification			K-means clustering, hierarchal clustering (HC), partitioning around medoids (PAM), LSTM based on variational autoencoder (VAE) deep convolutional embedded clustering (DCEC) 	platform: Intel Xeon Processor E5-2643 v4, 128 GB RAM and NVIDIA Quadro M4000 GPU.  K-means and HC: Python scikit-learn  PAM: R ‚Äòcluster‚Äô package LSTM-VAE: https://github.com/bilalmirza8519/LSTM-VAE DCEC: https://github.com/XifengGuo/DCEC		no	no	no	K-means, HC, PAM: K = 6 based on prior biological knowledge  LSTM-VAE:  the input layer of dimension 7 √ó 1 first layer generated 7 √ó n second layer 7 √ó 1  DCEC: used as DOI: 10.1007/978-3-319-70096-0_39	no			6312169df3794236aa9879e5	30853547.0	PMC6708480	02/02/2026 19:46:52	Chung NC, Mirza B, Choi H, Wang J, Wang D, Ping P, Wang W.	Methods (San Diego, Calif.)	Unsupervised classification of multi-omics data during cardiac remodeling using deep learning.	10.1016/j.ymeth.2019.03.004	2019	0.0	0.0		1.0	2022-03-28T11:30:10.000Z	2026-02-02T19:46:52.000Z	30e2b822-a203-454f-9cb8-41623505bcda	undefined	ek6yatpcos				Starting_TSV	Match	No Match
63516fedb9c880af1f305b4b	No	MRI scans of Traumatic Brain Injury (TBI) subjects recruited in EpiBioS4Rx. Npos=16, Nneg=37. Not previously used in literature.	Not applicable	N_pos,train=13, N_neg_train=30, N_pos,test=3, N_neg_test=7. Not separate validation set.			No	Comparison with a standard baseline method based on Region-Of-Interest (ROI) segmentation	No confidence reported	Sensitivity, Specificity, Accuracy, ROC-AUC	Repeated 80%-20% cross-validation. No indipendent datasets			No		Black box. Random forest used for feature seleciton	Binary classification			Random forest	No	MRI scans parceled into non-overlapping patches of V voxels. Patches represent nodes of a brain network, and the absolute values of Pearson‚Äôs correlation between patch pairs were considered the links between the node. Given N, the number of network nodes, 8N features for each subject	f=?. Feature selection by Random Forest		No	Not known	No			6603042f92c76639b849e69f	33328863.0	PMC7734183	02/02/2026 19:46:52	La Rocca M, Garner R, Amoroso N, Lutkenhoff ES, Monti MM, Vespa P, Toga AW, Duncan D.	Frontiers in neuroscience	Multiplex Networks to Characterize Seizure Development in Traumatic Brain Injury Patients.	10.3389/fnins.2020.591662	2020	0.0	0.0		1.0	2022-03-15T11:03:41.000Z	2026-02-02T19:46:52.000Z	7ff7eafe-6d4e-4b8e-b924-ee81c0e447ff	undefined	7rqpjvvf0w				Starting_TSV	Match	No Match
63516fedb9c880af1f305ba1	yes	a dataset of 515 sequences annotated with experimentally verified subtypes; 11 classes	the dataset was also clustered at similarity thresholds of 30%, 50%, 75%, and 90%	20 runs of non-stratified 5-fold CVs and 2-fold CVs			No	one other method based on ANN is mentioned in the text 	standard deviation is reported in a figure based on 20*5 values	accuracy	5-fold and 2-fold CV			https://services.birc.au.dk/patbox/ - not available as of 09/03/2022	not provided	transparent: k=1 was selected, which is basically the sequence similarity method	binary predictions			KNN	Yes: text	global features: distances are calculated by BLAST	variable sequence lengths of around 1000 AAs	a range of K values tested (1-50), 2 different CV protocols, weighted KNN	No					6312169df3794236aa9879fc	26422234.0	PMC4589233	02/02/2026 19:46:52	S√∏ndergaard D, Pedersen CN.	PloS one	PATBox: A Toolbox for Classification and Analysis of P-Type ATPases.	10.1371/journal.pone.0139571	2015	0.0	0.0		1.0	2022-03-09T10:11:09.000Z	2026-02-02T19:46:52.000Z	01f23578-67f2-4b8c-ab9e-121a1b840b39	undefined	zwmdye1tg8				Starting_TSV	Match	No Match
63516fedb9c880af1f305b8a	No	Recorded FFRs to 4 Mandarin tones from 28 people	To avoid stimulus artifact bias, training and testing subsets alternated the same number of trials with opposite stimulus polarity	K-fold CV of varying K for each tone separately (a version of unsupervised learning)			No		Yes: SDs are given for each accuracy estimates	Accuracy	Cross-validation			partially in the supplement	not given	Black box	probability scores			hidden Markov models were trained for each class	No	moving average window	22 discrete intervals	testing various sizes of the moving average window	No	not clear				6312169df3794236aa9879fc	28807860.0	PMC5610945	02/02/2026 19:46:52	Llanos F, Xie Z, Chandrasekaran B.	Journal of neuroscience methods	Hidden Markov modeling of frequency-following responses to Mandarin lexical tones.	10.1016/j.jneumeth.2017.08.010	2017	0.0	0.0		1.0	2022-03-08T13:39:24.000Z	2026-02-02T19:46:52.000Z	2c4e1331-5d62-4abc-a990-d4cea1bea700	undefined	1r4k25lx15				Starting_TSV	Match	No Match
63516fedb9c880af1f305b99	not provided	62419 anonymized blood biochemistry records from Invitro Laboratory, Ltd.; continuous labels; dataset not used before	none	random 90/10 train-test split; 10-fold CV mentioned only in a figure				only a set of baseline models, based on R2 and accuracy		epsilon-prediction accuracy (epsilon=10); R2; Pearson correlation (not reported); MAE (not reported)	seemingly independent test set			http://www.aging.ai/ - looks proprietary 	not provided	transparent: marker importance based on Permutation Feature Importance; discussion of top 10 features	continuous labels			21 DNNs + Elastic net as stacking model; GBoost; RF; DT; LR; kNN; Elastic net; SVM	Only the summary for the DNNs architectures 	global features	6	DNNs outperform baseline ML approaches	No	around 5M	Yes: dropout (p=0.2) after each layer, and L2 weight decay			6312169df3794236aa9879fc	27191382.0	PMC4931851	02/02/2026 19:46:52	Putin E, Mamoshina P, Aliper A, Korzinkin M, Moskalev A, Kolosov A, Ostrovskiy A, Cantor C, Vijg J, Zhavoronkov A.	Aging	Deep biomarkers of human aging: Application of deep neural networks to biomarker development.	10.18632/aging.100968	2016	0.0	0.0		1.0	2022-03-07T10:49:51.000Z	2026-02-02T19:46:52.000Z	2d650333-5e1b-4cc3-a7bb-512a00abfce0	undefined	m6t40hhxhx				Starting_TSV	Match	No Match
63516fedb9c880af1f305b34	Dataset 1: URL: https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga  Dataset 2: URL: https://ocg.cancer.gov/programs/target  Dataset 3: URL: https://proteomics.cancer.gov/programs/cptac	"Dataset 1: RNA-seq data from The Cancer Genome Atlas for different patients affected by different types of cancer. To the end of ML classification, patients where classified as ""High risk"" or ""Low risk"" based on the Overall Survival time. N = 2210; distribution of High risk and Low risk patients not stated. Used in other papers.  Dataset 2: RNA-seq data from Therapeutically Applicable Research to Generate Effective Treatments (TARGET) dataset (N = 1122). Used in other papers.  Dataset 3: RNA-seq data from Clinical Proteomic Tumor Analysis Consortium (CPTAC) dataset (N=391). Used in other papers."	Dataset splits for Dataset 1 do not overlap.   Dataset 1, 2 and 3 are independent from each other.	Dataset 1 divided in training set (N = 1878) and test set (N = 332). Distribution of High and Low risk patients not stated.  Datasets 2 and 3 used as independent test set. Distribution of High and Low risk patients not stated.			Yes, the evaluation procedure and code is available at: https://github.com/guoqingbao/PanCancerLncRNA	The model is not compared with others.	Not stated	Area Under Curve (AUC)	Test set derived from Dataset 1; Independent Datasets 2 and 3.			Yes, code available at: https://github.com/guoqingbao/PanCancerLncRNA	Not stated.	Black box.	Classification			Random Forest	No, but full code is available.	No tranformation.	5 lncRNA sequences selected from 731. A ML algorithm based on random forest was used for feature selection. Feature selection was operated on training data only.		Yes. Feature selection operated using ML. The set used for feature selection is the same used for the training of the classifier, but the validation for the classifier is operated on independent dataset (Dataset 2 and 3).	Not stated.	No			6312169df3794236aa987a01	32991297.0		02/02/2026 19:46:52	Bao G, Xu R, Wang X, Ji J, Wang L, Li W, Zhang Q, Huang B, Chen A, Zhang D, Kong B, Yang Q, Yuan C, Wang X, Wang J, Li X.	IEEE journal of biomedical and health informatics	Identification of lncRNA Signature Associated With Pan-Cancer Prognosis.	10.1109/jbhi.2020.3027680	2021	0.0	0.0		1.0	2022-02-18T09:58:28.000Z	2026-02-02T19:46:52.000Z	0ade3d57-3ebf-4c42-af9c-48b4b4fd2e54	undefined	agwmu9cbwt				Starting_TSV	Match	No Match
63516fedb9c880af1f305b74	Yes.   Dataset 1: URL: http://scop.berkeley.edu/  Dataset 2: URL: https://predictioncenter.org/casp13/	Dataset 1: Protein sequences retrived from SCOPe 2.07 database. N = 7,671. Used in other papers. Dataset used for training.  Dataset 2: CASP13. Used in other papers. Dataset used for test. N = 122.  The paper is about the prediction of contact maps in the protein sequence, so data are not in classes and N_pos and N_neg do not apply (N/A from now on)	Dataset 1 (used for training) and Dataset 2 (used for test) are independent.				No.	Not clear. They compare Algorithm 1 and Algorithm 2. 	They claim statistical significance in the different performances of Algorithm 1 and Algorithm 2.	Prediction accuracy (performed by CASP13 assesors)	Indipendent dataset (Dataset 2, i.e. CASP13)			Not available.	Not stated.	Black box.	The model produces a prediction of the contact map for each residue pair in the sequence alignment.			Algorithm 1 (TripletRes): neural network  Algorithm 2 (ResTriplet): neural network	No.	For each sequence a multiple sequence alignment (MSA) was performed.	For each sequence three matrix features are generated: Covariance matrix, Precision Matrix, coupling parameters of the Potts model.		Algorithm 1: No.  Algorithm 2: Yes. The network is actually composed of two subnetworks (called Stage 1 and Stage 2 in the paper), the first giving the inputs to the second. 	Not stated	No.			6312169df3794236aa987a01	31407406.0	PMC6851483	02/02/2026 19:46:52	Li Y, Zhang C, Bell EW, Yu DJ, Zhang Y.	Proteins	Ensembling multiple raw coevolutionary features with deep residual neural networks for contact-map prediction in CASP13.	10.1002/prot.25798	2019	0.0	0.0		1.0	2022-02-10T16:35:16.000Z	2026-02-02T19:46:52.000Z	23c26bad-c7ec-455d-aec5-edc6cea9e204	undefined	28yy20yr51				Starting_TSV	Match	No Match
63516fedb9c880af1f305b7a	at the suplementary files  https://www.frontiersin.org/articles/10.3389/fpls.2018.01559/full#supplementary-material  	Yeast data set:  from the PRIDE repository  C. reinhardtii data set:  previous proteome-wide studies each dataset filtered for single occurrence of all proteins  	no	training datasets consisting of 2,652 yeast and 2,732 C. reinhardtii proteins two validation datasets consisting of 664 yeast and 685 C. reinhardtii proteins			no	ChemScore (Parker, 2002)  PeptideSieve (Mallick et al., 2007), PeptideRank (Qeli et al., 2014), CONSequence (Eyers et al., 2011),  ESPPredictor (Fusaro et al., 2009).	no statistical significance declared.	normalized discounted cumulative gain (nDCG) metric	randomly selected 20% of the proteins in each assembly to use them as validation-data-sets,   32 peptides from a QconCAT protein was experimentally validated			webservice: (http://csbweb.bio.uni-kl.de/)		Black box	classification			neural network		BioFSharp framework (available at: https://github.com/CSBiology/BioFSharp) and converted into a feature vector with 45 entries.	The networks were trained using a minibatch size of 3 for 10 epoch.		no	five dense layers with 128 nodes each	Yes, ‚Äúdropout‚Äù technique 			6312169df3794236aa9879e5	30483279.0	PMC6242780	02/02/2026 19:46:52	Zimmer D, Schneider K, Sommer F, Schroda M, M√ºhlhaus T.	Frontiers in plant science	Artificial Intelligence Understands Peptide Observability and Assists With Absolute Protein Quantification.	10.3389/fpls.2018.01559	2018	0.0	0.0		1.0	2022-03-28T23:57:02.000Z	2026-02-02T19:46:52.000Z	fd5713cd-748e-4a55-889a-9ed418021731	undefined	0qudu672ba				Starting_TSV	Match	No Match
63516fedb9c880af1f305bac	No	1859 data points from another publication, continuous lables	data were split by the year of birth (before 2005 -> train, after -> test)	train/test: 1601/258 or 1574/235 (depending on the label type); 10-fold CV			No			"Pearson correlation, Estimated prediction bias (=""average difference between predicted and observed responses in standard deviation units"")"	independent test set			no	not given	transparent: gene identification	regression			gradient boosting algorithm using 2 different weak learner models: OLS and RKHS regression; modification -> random boosting	No	global features	only 39714 SNPs mentioned	Not provided	No	not given	No			6312169df3794236aa9879fc	23102953.0		02/02/2026 19:46:52	Gonz√°lez-Recio O, Jim√©nez-Montero JA, Alenda R.	Journal of dairy science	The gradient boosting algorithm and random boosting for genome-assisted evaluation in large data sets.	10.3168/jds.2012-5630	2013	0.0	0.0		1.0	2022-03-08T14:59:55.000Z	2026-02-02T19:46:52.000Z	f0944313-d0c6-4160-94cb-b073fe88e4bd	undefined	v1rge4dd87				Starting_TSV	Match	No Match
63516fedb9c880af1f305b9a	No	6 experimental datasets from the literature (803 = 171+296+46+27+35+228) and 4 classes. The raw data is not provided to evaluate the imbalances	Not studied	575 data points in the training and validation sets; 228 in the test set; the raw data is not provided to evaluate the imbalances			Only final accuracy values for training, test, and combined datasets in the supporting information.	Baselines: VarSelRF and simple RFE as an alternative to their feature selection; benchmarking: ClaNC by Verhaak et al. (claimed to be the standard in the field).	Only standard deviations for the accuracy values are provided	accuracy	Cross-validation, independent dataset			https://www.semel.ucla.edu/coppola-lab/simple-glioblastoma-subclassifier	Not provided	Partially transparent: the features (genes) selected in the final model were scrutinized for other associations.	4-class classification			random forest	Partially yes: in the text	global features	All datasets were combined and normalized using the R package limma, and batch effects were adjusted using ComBat. Then the optimal number of features (48 out of presumably 753) was selected using all datasets, potentially leading to contamination. Then subsets of 48 features were used to train models (based on dataset 1), and the best model was chosen (based on datasets 2-5 + genetic algorithm) and evaluated (based on dataset 6).		No					6312169df3794236aa9879fc	27855170.0	PMC5113897	02/02/2026 19:46:52	Crisman TJ, Zelaya I, Laks DR, Zhao Y, Kawaguchi R, Gao F, Kornblum HI, Coppola G.	PloS one	Identification of an Efficient Gene Expression Panel for Glioblastoma Classification.	10.1371/journal.pone.0164649	2016	0.0	0.0		1.0	2022-02-23T17:15:55.000Z	2026-02-02T19:46:52.000Z	4f314982-05d0-429f-aa3f-d163dc7c0834	undefined	l4cnu26i2o				Starting_TSV	Match	No Match
63516fedb9c880af1f305b77	no.	2 large sleep EEG data sets: the Massachusetts General Hospital (MGH) sleep lab data set (N = 2532; ages 18-80); and the Sleep Heart Health Study (SHHS, N =1974; ages 40-80)	They maintain strict separation of training and testing participants.	The MGH data set (N =2532) was partitioned into a healthy training set (N = 1343) used to train the model, and a testing set (N =1189) to evaluate model performance.  As validation, they used a subset of the SHHS data set where each participant underwent 2 study visits, referred as SHHS1 and SHHS2. The data set contains 987 adults for a total of 1974 nights of EEG recorded. They trained the model in 2 ways. First, they trained the model on 752 participants with paired EEGs (1504 EEGs) from both visits and tested on the held out 235 participants with paired EEGs (470 EEGs) in both visits. They trained the model on the 2365 EEGs from healthy participants in MGH data set. We then predicted BA on the 1974 paired EEGs in SHHS as the testing set.			no.	no.	The method reports statistical significant results.		The model is validated on a longitudinal cohort from a subset of the SHHS data set without neurological or cardiovascular disease.			no.		Transparent. The use of a parametric model, improves model interpretation by inspecting each EEG feature and comparing to the age norm for each feature explicitly. An example is provided in Figure 5 of the paper.	Regression.			Novel approach: They develop a machine learning model to predict BA based on 2 large sleep EEG data sets. The model is described in the paper.	yes, included in the paper.	EEG signals are notch-filtered at 60 Hz to reduce line noise, and bandpass filtered from 0.5 Hz to 20 Hz to reduce myogenic artifacts. For 30s-epochs, those with absolute amplitude larger than 500 m V are removed to minimize movement artifacts. Epochs containing flat EEG for more than 2 seconds are also removed. We also exclude EEGs contaminated by electrocardiogram, indicated by 1 Hz harmonic in the EEG spectrogram. To reduce interparticipant variance, the amplitude of each EEG channel is normalized to have zero median and unit interquartile range across the whole night. The total amount of data removed by these preprocessing procedures is 7% in the MGH data set and 9% in the SHHS data set.	They extract 102 features from each 30-second epoch covering both time and frequency domains. For each EEG recording, they average the features in each of the 5 sleep stages overtime, yielding 102 x 5 = 510 features per EEG.	One strength of the study is the use of large data sets. The number of EEGs involved in this study is large among relevant ‚ÄúBA‚Äù studies. The large size of the data sets helps to ensure the statistical power as well and to minimize selection bias. A large training set helps ensure that the trained model does not overfit to a particular data set, improving the ability togeneralize when applied beyond the training set. A large testing set allows accurate statistical measurement of how accurately the model performs.	no.	one. To determine the optimal hyperparameter, they randomly select 300 EEGs from the training set to serve as internal validation data.				6312169df3794236aa987a03	30448611.0	PMC6478501	02/02/2026 19:46:52	Sun H, Paixao L, Oliva JT, Goparaju B, Carvalho DZ, van Leeuwen KG, Akeju O, Thomas RJ, Cash SS, Bianchi MT, Westover MB.	Neurobiology of aging	Brain age from the electroencephalogram of sleep.	10.1016/j.neurobiolaging.2018.10.016	2019	0.0	0.0		1.0	2022-01-26T17:25:36.000Z	2026-02-02T19:46:52.000Z	ea4a498a-ad09-4fe4-9fba-a01b495ea341	undefined	6ykikkh959				Starting_TSV	Match	No Match
63516fedb9c880af1f305b7b	yes, GEO GSE39582	GEO GSE39582		Within the 585 colon patients, there were 369 CIN+ and 112 CIN-, 93 CIMP+ and 420 CIMP-, 77 dMMR and 459 pMMR			yes, Figure 1	na	na	Accuracy, sensitivity, specificity, mcc	na			na	na	transparent, listed in the The CIN-associated gene selection section in Methods	classification			SVM	no	The probes corresponding to the same gene were averaged. The gene expression data was preprocessed with quantile normalization			no		no			6312169df3794236aa9879e7	30008861.0	PMC6036478	02/02/2026 19:46:52	Zhang TM, Huang T, Wang RF.	Oncology letters	Cross talk of chromosome instability, CpG island methylator phenotype and mismatch repair in colorectal cancer.	10.3892/ol.2018.8860	2018	0.0	0.0		1.0	2022-03-28T23:21:09.000Z	2026-02-02T19:46:52.000Z	4a3c8c15-7e55-4033-a570-4dd9364e7be4	undefined	2imglukj27				Starting_TSV	Match	No Match
63516fedb9c880af1f305b63	Yes, supporting information, https://ega-archive.org/datasets/EGAD00001004775, https://ega-archive.org/datasets/EGAD00001004776, esophageal adenocarcinoma samples from OCCAM consortium / part of ICGC	Data are EAC (esophageal adenocarcinoma) genes from 1) ICGC (261), TCGA (86), Other Study (21)	Training and validation are completely separate. Parameter optimization was based on training data with grid search using 10,000 iterations and three fold cross-validation on four different SVM classifiers using a linear, radial, sigmoid and polynomial kernel.	Training and validation data were completely independent (ICGC/Occam data for training, TCGA and additionaly study data for testing of robustness and validation)			Supporting information and GitHub / R Project https://github.com/ciccalab/sysSVM	No comparison to other methods. Identified helper genes show similar properties to known  cancer genes. Estimation based on pathway enrichment of identified novel genes (helpers) vs known cancer genes (drivers) based on functional association		Sensitivity	Cross validation & independent datasets			https://github.com/ciccalab/sysSVM, License not defined		Transparent	Classification			SVM	Yes, Supporting information 		34 featuress mapped to genes altered in sample cohort (one of the three datasets). See Supplementary Material 1for details.		No	nu (all kernels), gamma (radial, sigmoid, polynomial kernels), degree (polynomial kernel). 10000 iterations of three-fold cross validation, random split of training set into 2/3 train, 1/3 test set. Sensitivity is used to select the least varying model from top 5 models for each kernel. (more details in Supplementary Material 1)	best model selection takes into account all previous cross-validation iterations every n (100) cross validations, random reordering of cross validation iterations (5x)			6312169df3794236aa9879e9	31308377.0	PMC6629660	02/02/2026 19:46:52	Mourikis TP, Benedetti L, Foxall E, Temelkovski D, Nulsen J, Perner J, Cereda M, Lagergren J, Howell M, Yau C, Fitzgerald RC, Scaffidi P, Oesophageal Cancer Clinical and Molecular Stratification (OCCAMS) Consortium, Ciccarelli FD.	Nature communications	Patient-specific cancer genes contribute to recurrently perturbed pathways and establish therapeutic vulnerabilities in esophageal adenocarcinoma.	10.1038/s41467-019-10898-3	2019	0.0	0.0		1.0	2022-03-28T20:10:36.000Z	2026-02-02T19:46:52.000Z	f11b2731-c298-4706-b5fb-70d1068217da	undefined	b171clx1jy				Starting_TSV	Match	No Match
63516fedb9c880af1f305b6c	yes: supporting information	new database search in Influenza Research Database; 10 protein datasets with 4240-5373 data points (around 2/3 are pos and 1/3 are neg in each dataset) 	none reported	"There is a mysterious ""test data"" set mentioned only once without any additional information."			No		None for evaluation	accuracy	"mysterious ""test set"" mentioned only once"			No	Not provided	transparent: biological interpretation of identified rules (mutations)	probability score			DT, CBA, Ripper	No	global features (multiple sequence alignment)	f = protein sequence length (200-900)	None provided	No					6312169df3794236aa9879fc	30769139.0		02/02/2026 19:46:52	Kargarfard F, Sami A, Hemmatzadeh F, Ebrahimie E.	Gene	Identifying mutation positions in all segments of influenza genome enables better differentiation between pandemic and seasonal strains.	10.1016/j.gene.2019.01.014	2019	0.0	0.0		1.0	2022-03-09T09:50:58.000Z	2026-02-02T19:46:52.000Z	036db907-b055-40cb-b38e-3b1ef226ba31	undefined	nfh7iw73hz				Starting_TSV	Match	No Match
63516fedb9c880af1f305b41	MTBLS1695, study still private	20 Alzheimer's Disease patients (AD), 10 Mild cognitive impairment (MCI) and 29 control patients. Metabolomics samples measured with HNMR and DI LC-MS/MS, identification of 142 metabolites with HNMR and 51 with DI LC-MS/MS, demographic information (age and gender)	Average of concentrations of overlapping metabolites between HNMR and DI LC-MS/MS. PCA to screen for and remove subjects outside of 95% percentile. Student‚Äôs t-test was performed to determine if there were any significantly different metabolites between AD, MCI, and age-matched controls (p < 0.05) when compared pairwise. Non-normally distributed data were analyzed using a Mann‚àíWhitney U test and a Bonferroni correction was applied to account for multiple comparisons. To determine if sample demographics were statistically significantly different, a one-way analysis of variance analysis (ANOVA) was conducted using the IBM SPSS Statistics toolbox (v. 24.0).	20, 10, 29 (see provenance) * 142 metabolites, 			Supporting information, potentially in MetaboLights repository under private accession MTBLS1695	SVM, logistic regression	p-values, CI (95%) for AUCs	AUC, sensitivity, specificity	10-fold Cross-validation			NA (MetaboLights accession is private)		NA (missing or inaccessible model data and source code)	Classification			SVM (RBF kernel), Logistic regression		Log-transform, auto-scaling	Correlation-based feature selection, LASSO, step-wise variable selection		No					6312169df3794236aa9879e9	32878308.0	PMC7569858	02/02/2026 19:46:52	Yilmaz A, Ugur Z, Bisgin H, Akyol S, Bahado-Singh R, Wilson G, Imam K, Maddens ME, Graham SF.	Metabolites	Targeted Metabolic Profiling of Urine Highlights a Potential Biomarker Panel for the Diagnosis of Alzheimer's Disease and Mild Cognitive Impairment: A Pilot Study.	10.3390/metabo10090357	2020	0.0	0.0		1.0	2022-03-29T09:10:15.000Z	2026-02-02T19:46:52.000Z	805d6ac8-7e63-41b2-a36b-427fbcf4b64c	undefined	q3f9qsuitm				Starting_TSV	Match	No Match
63516fedb9c880af1f305b21	Additional file 1, https://www.ncbi.nlm.nih.gov/sra/?term=PRJNA666033	Custom sampling of 22 populations of honey bee in Europe and adjacent regions, total 2145 samples leading to 1.6 billion paired-end fragments. 1998 remained after removal of 62 outliers.	Selection of SNPs (Single Nucleotide Polymorphisms), originally 4400 selected, 4165 after QC check. 4094 SNPs were genotyped (71 failed base calling).	Training: 1391 samples (70% of 1998),  597 (30% of 1998) and 2505 independent samples (out-of-sample with known bee subspecies classification) for validation.			Supporting information	Multiple supervised methods were compared, best method selection based on average accuracy.		Accuracy	CV			https://github.com/jlanga/smsk_popoolation, MIT License			Classification			Linear Support Vector Classification	Supplementary Information	One-hot encoding as vectors	Not defined, 1396 * number of genotypes		No	NA, multiple supervised ML methods were used with scikit-learn				6312169df3794236aa9879e9	33535965.0	PMC7860026	02/02/2026 19:46:52	Momeni J, Parejo M, Nielsen RO, Langa J, Montes I, Papoutsis L, Farajzadeh L, Bendixen C, CƒÉuia E, Charri√®re JD, Coffey MF, Costa C, Dall'Olio R, De la R√∫a P, Drazic MM, Filipi J, Galea T, Golubovski M, Gregorc A, Grigoryan K, Hatjina F, Ilyasov R, Ivanova E, Janashia I, Kandemir I, Karatasou A, Kekecoglu M, Kezic N, Matray ES, Mifsud D, Moosbeckhofer R, Nikolenko AG, Papachristoforou A, Petrov P, Pinto MA, Poskryakov AV, Sharipov AY, Siceanu A, Soysal MI, Uzunov A, Zammit-Mangion M, Vingborg R, Bouga M, Kryger P, Meixner MD, Estonba A.	BMC genomics	Authoritative subspecies diagnosis tool for European honey bees based on ancestry informative SNPs.	10.1186/s12864-021-07379-7	2021	0.0	0.0		1.0	2022-03-28T20:30:14.000Z	2026-02-02T19:46:52.000Z	dd72ec96-8461-42d7-887c-700c90446755	undefined	hk4hhg2svs				Starting_TSV	Match	No Match
63516fedb9c880af1f305b22	"Conatact (hoganca@stanford.edu) is provided right the beggining of the ""Methods"" section of the papaer for further information and requests.  The data and code generated during this study were made available at https://github.com/stanfordmlgroup/influenza-qtof."	samples from Stanford Health Care and Stanford Children‚Äôs Health, 1:1 ratio of positive to age and sex-matched negative controls.  Data collection was performed after the reference test (RT-PCR) and before the index test (metabolomics). Discovery cohort (from April 23 2015 to October 13 2019):  118 samples, N_neg 118 samples validation cohort (December 21 2019 to February 18 2020): N_pos 48 samples, N_neg 48 samples		The final analysis included for discovery cohort: training set:  94 positive, 92 negative test set: 24 positive, 26 negative			yes, https://github.com/stanfordmlgroup/influenza-qtof/tree/master/data	costeffective comare to PCR tests and could be performed at the point-of-care comapre to RT-PCR test, no other methods for direct omparisonc		AUC, sensitivity, specificity	 novel experiments.			https://github.com/stanfordmlgroup/influenza-qtof	no	Black box	classification			gradient boosted decision trees and random forests	no				no	Python version 3.6.8, gradient boosted decision trees: LightGBM v2.2.3 RF: scikit-learn v0.20.2	The models were not retrained using SRM data to avoid overfitting and overestimating test performance. In addition, within the training set, cross-validation was used to develop the models to avoid overfitting to the training set.			6312169df3794236aa9879e5	34419924.0	PMC8385175	02/02/2026 19:46:52	Hogan CA, Rajpurkar P, Sowrirajan H, Phillips NA, Le AT, Wu M, Garamani N, Sahoo MK, Wood ML, Huang C, Ng AY, Mak J, Cowan TM, Pinsky BA.	EBioMedicine	Nasopharyngeal metabolomics and machine learning approach for the diagnosis of influenza.	10.1016/j.ebiom.2021.103546	2021	0.0	0.0		1.0	2022-03-28T16:40:10.000Z	2026-02-02T19:46:52.000Z	84fcc9b6-a284-4d43-8940-fee068877b2f	undefined	3hsc5woplt				Starting_TSV	Match	No Match
63516fedb9c880af1f305b23	"no, they claim: ""Publicly available datasets were analyzed in this study. This data can be found here: The datasets collected in the current study are available in the TCGA3 and GEO repository."" "	public databases: 901 samples from The Cancer Genome Atlas cohort (TCGA-LUAD) and gene expression omnibus (GEO) database		10-fold cross validation			no	no	no 	AUCs for 2-, 3-, and 5-year OS were 0.527, 0.596 and 0.671, respectively	10-fold cross validation			no			regression			Spathial, Random forest, LASSO	no			no	no	"R package randomForestSRC: ""The training was running with ‚Äúimportance = TRUE, block size = 1‚Äù and set with all other parameters set to defaul"""	no			6312169df3794236aa9879e5	33679869.0	PMC7933593	02/02/2026 19:46:52	Sun S, Fei K, Zhang G, Wang J, Yang Y, Guo W, Yang Z, Wang J, Xue Q, Gao Y, He J.	Frontiers in genetics	Construction and Comprehensive Analyses of a METTL5-Associated Prognostic Signature With Immune Implication in Lung Adenocarcinomas.	10.3389/fgene.2020.617174	2020	0.0	0.0		1.0	2022-03-28T00:44:11.000Z	2026-02-02T19:46:52.000Z	282ed2ca-7c65-4eb6-9c7d-8c69881d722c	undefined	039id1a4pi				Starting_TSV	Match	No Match
63516fedb9c880af1f305b85	no	source of data:  large pharmacogenomics studies: Genomics of Drug Sensitivity in Cancer Project (GDSC), and the Cancer Cell Line Encyclopedia (CCLE), after filtering, they created a single array of data containing information on 3577 features in 624 cell lines		for training a deep autoencoder: randomly split into training and testing datasets of 520 and 104 samples, respectively  for drug-sensitivity prediction: 25-fold cross-validation			no	they mentioned two similar studies:  10.1126/science.1127647 and 10.1038/nature12831 		lastic net models: average sensitivity 0.75, average specificity 0.78, AUROC 0.81 SVM models: average sensitivity 0.59,  average specificity 0.56, AUROC 0.55 	25-fold cross-validation			no		black box	regression			Elastic net regression, SVM  and deep autoencoder	no	selected features from the deep learning autoencoder	"selected features from the deep learning autoencoder: ""Matlab code for training a deep autoencoder as described by Hinton and Salakhutdinov was obtained from Hinton's website (http://www.cs.toronto.edu/$hinton/MatlabForSciencePaper.html)"""		no		Yes, variance-based mixture-fitting feature selection scheme			6312169df3794236aa9879e5	29133589.0	PMC5821274	02/02/2026 19:46:52	Ding MQ, Chen L, Cooper GF, Young JD, Lu X.	Molecular cancer research : MCR	Precision Oncology beyond Targeted Therapy: Combining Omics Data with Machine Learning Matches the Majority of Cancer Cells to Effective Therapeutics.	10.1158/1541-7786.mcr-17-0378	2018	0.0	0.0		1.0	2022-03-28T00:12:23.000Z	2026-02-02T19:46:52.000Z	e9716212-a945-4dc9-b615-7fc1704b6dbf	undefined	3htxdzl6gy				Starting_TSV	Match	No Match
63516fedb9c880af1f305b66	Stated to use previously reported data, but no explicit reference provided. 	Previously reported data, 210 lines, 1619 bins (synthetic markers), 24994 genes transcripts, 1000 metabolites and four agronomic traits	Random choice from Cross-Validation	10-fold Cross-Validation			"Yes (not all parameters, evaluation based on ""Predictability""), Supporting information"	Lasso regression		Predictability	Cross-validation					Model is not available for evaluation	"Regression, outputs ""predictability"""			Lasso regression	Only values for alpha are provided		24994 genomic transcripts, 1000 metabolites, four agronomic traits, CV on transcripts > 0.2 and PD > 1 (90th% - 10th%), 5467 genomic transcripts after CV filter		No	Modified lasso regression, alpha (first layer), beta (second layer), gamma (third layer), 10-fold CV for all parameters, successively	10-fold CV			6312169df3794236aa9879e9	30950198.0	PMC6737184	02/02/2026 19:46:52	Hu X, Xie W, Wu C, Xu S.	Plant biotechnology journal	A directed learning strategy integrating multiple omic data improves genomic prediction.	10.1111/pbi.13117	2019	0.0	0.0		1.0	2022-03-27T20:48:59.000Z	2026-02-02T19:46:52.000Z	8fbaaea0-bcec-4aaf-a34b-86f7471a84cc	undefined	909naoy87e				Starting_TSV	Match	No Match
63516fedb9c880af1f305b6f	Support information files.	Database CATNAP http://hiv.lanl.gov/catnap		3864 exact IC50 values are split randomly in half into a training set and a validation set.				Neural Network with two hidden layers, k-nearest neighbors, Random Forest, SVM	Confidence intervals	Confusion matrix	Cross-validation			Supplementary information	about 13 minutes	Black Box	Both			Multi-Layer Perceptron 			6179		No	200	No, high ratio between the number of experimental values in the training and number of parameters in the model 			6312169df3794236aa9879f6	30970017.0	PMC6457539	02/02/2026 19:46:52	Conti S, Karplus M.	PLoS computational biology	Estimation of the breadth of CD4bs targeting HIV antibodies by molecular modeling and machine learning.	10.1371/journal.pcbi.1006954	2019	0.0	0.0		1.0	2022-03-04T17:32:12.000Z	2026-02-02T19:46:52.000Z	59ef281d-0bee-4188-a2b4-10dd495bb666	undefined	f66mtky2wk				Starting_TSV	Match	No Match
63516fedb9c880af1f305b72	yes for real-world data: https://www.ebi.ac.uk/arrayexpress/help/GEO_data.html (ids E-MTAB-3929, GSE87795, GSE60361, GSE82187)	Simulated and real-world experimental scRNA-seq datasets: the simulated sets had 100 points for each of 3, 5, 7, or 9 classes. The real-world sets were 4 from literature (1059 points with 3 classes, 367 points with 6 classes, 3005 points with 7 classes and 705 points for 10 classes). Some of the labels of all the datasets were artificially corrupted.		multiple AdaSampling (self-supervision) 			No			mean classification accuracy; adjusted Rand index	Training data only			https://github.com/SydneyBioX/scReClassify; https://bioconductor.org/packages/release/bioc/html/scReClassify.html; GPL-3 license	Not provided	Black box	multi-class reclassification (error correction)			SVM, random forest, ensemble learning	Some hyperparameters are given in the text.	Global features	PCA to capture at least 70%, projected to the range of 10-20 features 		No	Forced to be in the range of 10-20 + 4 (for SVM) by PCA to capture at least 70%				6312169df3794236aa9879fc	31874628.0	PMC6929456	02/02/2026 19:46:52	Kim T, Lo K, Geddes TA, Kim HJ, Yang JYH, Yang P.	BMC genomics	scReClassify: post hoc cell type classification of single-cell rNA-seq data.	10.1186/s12864-019-6305-x	2019	0.0	0.0		1.0	2022-02-23T14:14:06.000Z	2026-02-02T19:46:52.000Z	d6512a82-bc58-4d38-a7de-a80eecdc7a16	undefined	4zvfuyfezh				Starting_TSV	Match	No Match
63516fedb9c880af1f305b62	Yes,  upon reasonable request from the corresponding author	six available blood parameters (C-reactive protein, white cell count, bilirubin, creatinine, ALT and alkaline phosphatase) from 160203 individuals.					no	no direct comparision	no	ROC AUC 0.84 92% sensitivity, 94%specificity	10-fold cross-validation			no		Black box 	classification			SVM	no	six blood parameters (C-reactive protein, white cell count, bilirubin, creatinine, ALT and alkaline phosphatase)		Removal of outliers, Synthetic Minority Over-Sampling Technique was used	no		no			6312169df3794236aa9879e5	30590545.0		02/02/2026 19:46:52	Rawson TM, Hernandez B, Moore LSP, Blandy O, Herrero P, Gilchrist M, Gordon A, Toumazou C, Sriskandan S, Georgiou P, Holmes AH.	The Journal of antimicrobial chemotherapy	Supervised machine learning for the prediction of infection on admission to hospital: a prospective observational cohort study.	10.1093/jac/dky514	2019	0.0	0.0		1.0	2022-03-29T00:49:10.000Z	2026-02-02T19:46:52.000Z	964f49d7-ae30-4db1-8d54-ca667d7e64d5	undefined	84zp8aovt4				Starting_TSV	Match	No Match
63516fedb9c880af1f305b4d	Yes. Dataset available at URL: https://github.com/ncbi/BioConceptVec	Only one dataset employed, created using all PubMed abastracts 					No	"Compared with other methods (BioAgvWord and ""Yu et al."")"	No confidence interval	Precision, Recall, F1-score, Area Under Curve (AUC)	Intrinsic and Extrinsic evaluation from 9 independent datasets			Yes, available at URL: https://github.com/ncbi/BioConceptVec	Not stated	Black box	Embeddings			Novel approach called BioConceptVec. The paper explains why they used a novel approach	Yes, available at URL: https://github.com/ncbi/BioConceptVec				No.	Not stated				6312169df3794236aa987a01	32324731.0	PMC7237030	02/02/2026 19:46:52	Chen Q, Lee K, Yan S, Kim S, Wei CH, Lu Z.	PLoS computational biology	BioConceptVec: Creating and evaluating literature-based biomedical concept embeddings on a large scale.	10.1371/journal.pcbi.1007617	2020	0.0	0.0		1.0	2022-03-08T15:42:05.000Z	2026-02-02T19:46:52.000Z	5d549370-43e5-4163-977c-59e9d5cadf79	undefined	iugmhjifoe				Starting_TSV	Match	No Match
63516fedb9c880af1f305b39	The features table dataset has not been made available. Authors made available the imaging data at https://www.ebi.ac.uk/biostudies/ with accession number S-BIAD161. Authors also report that features were extracted from images with CellProfiler 3.1.8 and provide the list of morphological and contextual measurements extracted to generate the features table (Table B of http://journals.plos.org/ploscompbiol/article/asset?unique&id=info:doi/10.1371/journal.pcbi.1009193.s002http://journals.plos.org/ploscompbiol/article/asset?unique&id=info:doi/10.1371/journal.pcbi.1009193.s002 )	Imaging data produced by the same authors. 826 cells classified in 3 classes (no negative/positive binary classification).		90% training set 10% test set			No			F1 score of 0.98	cross-validation					Black box	Classification			Random forest	No	global features	100. Iterative feature selection removing features with Pearson‚Äôs correlation coefficient > 0.8. Performed on all data.		No	Number of parameters not specificed. Hyperparameter tuning was performed using randomised search with K-fold cross-validation to optimise the model parameters 				6312169df3794236aa987a0c	34297718.0	PMC8336795	02/02/2026 19:46:52	Rozova VS, Anwer AG, Guller AE, Es HA, Khabir Z, Sokolova AI, Gavrilov MU, Goldys EM, Warkiani ME, Thiery JP, Zvyagin AV.	PLoS computational biology	Machine learning reveals mesenchymal breast carcinoma cell adaptation in response to matrix stiffness.	10.1371/journal.pcbi.1009193	2021	0.0	0.0		1.0	2022-01-18T16:56:16.000Z	2026-02-02T19:46:52.000Z	36e9870f-9db6-4673-90a4-ad00c967c0bd	undefined	ie0f3buyr4				Starting_TSV	Match	No Match
63516fedb9c880af1f305b42	yes, processed, not raw	The quantitative image features extracted from H&E stained whole-slide images are available from GitHub. No access to raw data provided								AUC and confidence intervals were computed with the R package pROC.	In dataset 1, five-fold cross-validation was used. To further validate our method using an external validation set, classification models were trained using dataset 1 and evaluated using dataset 2			https://github.com/chengjun583/tRCC-ccRCC-classification		transparent	classification			Logistic regression, SVM with linear or Gaussian kernels, and random forest were used to conduct supervised machine learning	no		Due to the high dimensionality of the image features and relatively small sample size, overfitting of the data is likely; therefore, before building classification models, we performed feature selection to avoid the overfitting problem. Feature dimensionality was reduced by the mRMR algorithm42 using R package mRMRe. mRMR has been shown to be a robust feature selection algorithm in various tasks43,44,45. The mRMR algorithm was applied to all image features with regard to the class label of sample (i.e., TFE3-RCC or ccRCC) to select an informative and non-redundant set of features.	See Features	no		Yes, see Features			6312169df3794236aa9879e7	32286325.0	PMC7156652	02/02/2026 19:46:52	Cheng J, Han Z, Mehra R, Shao W, Cheng M, Feng Q, Ni D, Huang K, Cheng L, Zhang J.	Nature communications	Computational analysis of pathological images enables a better diagnosis of TFE3 Xp11.2 translocation renal cell carcinoma.	10.1038/s41467-020-15671-5	2020	0.0	0.0		1.0	2022-03-29T07:28:13.000Z	2026-02-02T19:46:52.000Z	051b93a6-4e37-4b9c-8cc9-05f531317aea	undefined	ixx5zlqxpf				Starting_TSV	Match	No Match
63516fedb9c880af1f305b97	https://bbbc.broadinstitute.org/ accession BBBC021	Public		148649 cells. 1/2 source and 1/2 target						Accuracy, confusion matrices	cross-validation			http://www.deepnets.ineb.up.pt/files/software/DTL_frontend.html	500 min	Black box	Classification			Deep Transfer Learning (type of Deep Neural Network)		Autoencoder	453		No		leave-one-compound-out cross-validation (LOOCV)			6312169df3794236aa9879f6	26746583.0		02/02/2026 19:46:52	Kandaswamy C, Silva LM, Alexandre LA, Santos JM.	Journal of biomolecular screening	High-Content Analysis of Breast Cancer Using Single-Cell Deep Transfer Learning.	10.1177/1087057115623451	2016	0.0	0.0		1.0	2022-03-17T23:10:43.000Z	2026-02-02T19:46:52.000Z	cfb3eddf-4a2d-4640-9d9e-8650193f3286	undefined	r7jsdtsoxh				Starting_TSV	Match	No Match
63516fedb9c880af1f305b8c	http://dlab.org.cn/PredRBR/ (not working now)	Used by previous papers	 eliminating sequences more similar than 40%					BindN, PPRint, BindN+, RNABindR2.0, RNABindRPlus, SNBRFinder		sensitivity, specificity, accuracy, precision, F-measure, MCC score	Independent dataset			http://dlab.org.cn/PredRBR/ (not working)		Black box	Binary predictions			Gradient tree boosting		global features	63 + 63, Incremental Feature Selection		No					6312169df3794236aa9879f6	29219069.0	PMC5773889	02/02/2026 19:46:52	Tang Y, Liu D, Wang Z, Wen T, Deng L.	BMC bioinformatics	A boosting approach for prediction of protein-RNA binding residues.	10.1186/s12859-017-1879-2	2017	0.0	0.0		1.0	2022-03-02T17:27:58.000Z	2026-02-02T19:46:52.000Z	4f8760d4-406b-4faf-bfce-2e05bee0335c	undefined	akj12z17yv				Starting_TSV	Match	No Match
63516fedb9c880af1f305ba8	Yes, website URL	Dataset links provided in Table 1		Some details provided in Additional information			no							https://www.targetscan.org/vert_80/		black box	probability score			TargetScan - non-ML algorithm	no				No	params provided in additional info	No			6312169df3794236aa9879e7	25404408.0	PMC4289375	02/02/2026 19:46:52	Ultsch A, L√∂tsch J.	BMC genomics	What do all the (human) micro-RNAs do?	10.1186/1471-2164-15-976	2014	0.0	0.0		1.0	2022-03-28T23:06:53.000Z	2026-02-02T19:46:52.000Z	0f4944df-a1cc-4bf4-a6da-bb1ce13b71df	undefined	74z8knxsmc				Starting_TSV	Match	No Match
63516fedb9c880af1f305b25	yes, described at S1 table: https://deposition.proteinensemble.org/job/25e45232-2c53-409f-b734-bbc1710609a2  all models and dataset at https://gitlab.com/mgarciat/genome-wide-prediction-of-topoisomerase-iibeta-binding	yes, N_pos: 13128  N_neg: 32766	Correlation Feature Selection is used.	5-fold cross-validation						ROC  curves and AUC values	5-fold cross validation			yes, GitLab https://gitlab.com/mgarciat/genome-wide-prediction-of-topoisomerase-iibeta-binding						Naive Bayes, SVM, Random Forests	no	sliding window on sequence	Fast Correlation Based Filter  and  Scatter Search  were used		no					6312169df3794236aa9879e5	33465072.0	PMC7845959	02/02/2026 19:46:52	Mart√≠nez-Garc√≠a PM, Garc√≠a-Torres M, Divina F, Terr√≥n-Bautista J, Delgado-Sainz I, G√≥mez-Vela F, Cort√©s-Ledesma F.	PLoS computational biology	Genome-wide prediction of topoisomerase IIŒ≤ binding by architectural factors and chromatin accessibility.	10.1371/journal.pcbi.1007814	2021	0.0	0.0		1.0	2022-03-23T16:36:18.000Z	2026-02-02T19:46:52.000Z	1ac93280-f1b7-4064-8b5e-5e1553dbd65f	undefined	9regsp7l7u				Starting_TSV	Match	No Match
63516fedb9c880af1f305b2a	Yes, https://data.broadinstitute.org/bbbc/image_sets.html https://doi.org/10.6084/m9.figshare.c.5067638.v1 http://www.mitocheck.org/mitotic_cell_atlas/downloads/v1.0.1/mitotic_cell_atlas_v1.0.1_fulldata.zip https://doi.org/10.6084/m9.figshare.c.5075093.v1	Synthetic dataset + 3 original datasets						Multiple			cross validation and novel experiments			http://www.cellclassifier.org			Regression			Novel approach (Regression Plane)	heuristic hyperparameter initialization methods	position and features of each cell to be analyzed		Na	No		Na			6312169df3794236aa9879f6	33953203.0	PMC8100172	02/02/2026 19:46:52	Szkalisity A, Piccinini F, Beleon A, Balassa T, Varga IG, Migh E, Molnar C, Paavolainen L, Timonen S, Banerjee I, Ikonen E, Yamauchi Y, Ando I, Peltonen J, Pieti√§inen V, Honti V, Horvath P.	Nature communications	Regression plane concept for analysing continuous cellular processes with machine learning.	10.1038/s41467-021-22866-x	2021	0.0	0.0		1.0	2022-03-16T16:16:20.000Z	2026-02-02T19:46:52.000Z	b11547a3-b7fc-4667-b0c8-24f256717893	undefined	1nvj60arng				Starting_TSV	Match	No Match
63516fedb9c880af1f305b51	http://www.cuilab.cn/hmdd	Database HMDD3.0 (http://www.cuilab.cn/hmdde)						DeepWalk, Line, Node2Vec, GraRep, GF, Lap, lle (all Network Embedding  methods)		ROC-AUC, PR_AUC, Precision, Accuracy, F1, Recall	Cross validation				Minutes	Black box	Binary			Random Forest + Network Embedding  + Network Similarity			128		No	3, Random Forest				6312169df3794236aa9879f6	33354569.0	PMC7735824	02/02/2026 19:46:52	Li J, Liu Y, Zhang Z, Liu B, Wang Y.	BioMed research international	PmDNE: Prediction of miRNA-Disease Association Based on Network Embedding and Network Similarity Analysis.	10.1155/2020/6248686	2020	0.0	0.0		1.0	2022-03-03T17:42:56.000Z	2026-02-02T19:46:52.000Z	0f17c012-ed0e-4766-801b-39bb206b5767	undefined	caz69j3bqb				Starting_TSV	Match	No Match
63516fedb9c880af1f305b2b	Yes. Original dataset: https://www.ncbi.nlm.nih.gov/bioproject/?term=PRJNA417767	4 datasets, 3 form previous papers and 1 original. 							NAA	custom	cross-validation					Black box	Classification			Random Forest		Amplicon Sequence Variants (ASV) used as features in a relative abundance matrix	Depending on the dataset, from 50 to 5000		No					6312169df3794236aa9879f6	33995917.0	PMC8093828	02/02/2026 19:46:52	Dully V, Wilding TA, M√ºhlhaus T, Stoeck T.	Computational and structural biotechnology journal	Identifying the minimum amplicon sequence depth to adequately predict classes in eDNA-based marine biomonitoring using supervised machine learning.	10.1016/j.csbj.2021.04.005	2021	0.0	0.0		1.0	2022-03-16T12:48:49.000Z	2026-02-02T19:46:52.000Z	91f62631-e353-48e7-b960-4aef0562c730	undefined	r23jj6bm7u				Starting_TSV	Match	No Match
63516fedb9c880af1f305b50	Yes, https://www.ncbi.nlm.nih.gov/sra/SRP130967 (Sequences) https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE109174 (transcripts half-life)	Experimental								R2	Cross-validation					Transparent, T rich UTRs provide stability	Regression			Variant of random forest regression	Hyperparameters reported in the paper	Counts of all possible 3- to 8-letter-long sequence motifs present in the 5= and 3= UTRs	200,000 features for UTR sequence motif analysis / using a random subset of features to identify the best candidate feature for splitting at each node		No					6312169df3794236aa9879f6	32753502.0	PMC7406221	02/02/2026 19:46:52	Gordon GC, Cameron JC, Gupta STP, Engstrom MD, Reed JL, Pfleger BF.	mSystems	Genome-Wide Analysis of RNA Decay in the Cyanobacterium <i>Synechococcus</i> sp. Strain PCC 7002.	10.1128/msystems.00224-20	2020	0.0	0.0		1.0	2022-03-04T17:00:53.000Z	2026-02-02T19:46:52.000Z	3785a62f-b82d-40db-9c45-88dbe0b8582f	undefined	hlpvh7cgmc				Starting_TSV	Match	No Match
63516fedb9c880af1f305b52	No	CT images from a total of 739 patients with gastric cancer. 		286 training - 453 validation No info on N_pos and N_neg immediately available 						sensitivity, specificity, AUC-ROC curve	Independent dataset					Black box	Binary predictions			SVM			584, SVM-recursive feature elimination used for feature selection on training data only.		No					6312169df3794236aa9879f6	33425719.0	PMC7794018	02/02/2026 19:46:52	Li J, Zhang C, Wei J, Zheng P, Zhang H, Xie Y, Bai J, Zhu Z, Zhou K, Liang X, Xie Y, Qin T.	Frontiers in oncology	Intratumoral and Peritumoral Radiomics of Contrast-Enhanced CT for Prediction of Disease-Free Survival and Chemotherapy Response in Stage II/III Gastric Cancer.	10.3389/fonc.2020.552270	2020	0.0	0.0		1.0	2022-03-02T16:10:05.000Z	2026-02-02T19:46:52.000Z	827777a1-d886-4bc3-b9c3-91083dffda4f	undefined	43nakaafp9				Starting_TSV	Match	No Match
63516fedb9c880af1f305b29	Yes, Upon reasonable request from the corresponding author	Two monkeys!						neural dynamic mod- eling (NDM) e representational modeling  (RM)		cross-validated CC between the true and predicted behavior	cross-validation			https://github.com/ShanechiLab/PSID		Black box	regression			Novel approach (Preferential Subspace IDentification)					No					6312169df3794236aa9879f6	33169030.0		02/02/2026 19:46:52	Sani OG, Abbaspourazad H, Wong YT, Pesaran B, Shanechi MM.	Nature neuroscience	Modeling behaviorally relevant neural dynamics enabled by preferential subspace identification.	10.1038/s41593-020-00733-0	2021	0.0	0.0		1.0	2022-03-16T16:40:33.000Z	2026-02-02T19:46:52.000Z	4fd5e711-86f0-4064-a357-31d2f610128b	undefined	l9ylxpirie				Starting_TSV	Match	No Match
63516fedb9c880af1f305b87	http://c3.pcons.net/	PFAM								ROC Curve	Independent dataset			http://c3.pcons.net/		Black box	Regression								Yes. Protein structures/Protein sequences					6312169df3794236aa9879f6	28881974.0	PMC5870574	02/02/2026 19:46:52	Michel M, Men√©ndez Hurtado D, Uziela K, Elofsson A.	Bioinformatics (Oxford, England)	Large-scale structure prediction by improved contact predictions and model quality assessment.	10.1093/bioinformatics/btx239	2017	0.0	0.0		1.0	2022-03-17T23:25:21.000Z	2026-02-02T19:46:52.000Z	ae4b3d28-aac1-4f6c-87b5-cd0f72a69674	undefined	xyv3h983ib				Starting_TSV	Match	No Match
63516fedb9c880af1f305ba0	Yes, the Drosophila Bicoid data used in this study is available from the FlyEx database, http://urchin.spbcas.ru/flyex/ 	syntetic data and one public dataset						stochastic simulations						Supplementary data of the article		Black box	Regression			Novel approach					No					6312169df3794236aa9879f6	27222432.0	PMC4894951	02/02/2026 19:46:52	Schnoerr D, Grima R, Sanguinetti G.	Nature communications	Cox process representation and inference for stochastic reaction-diffusion processes.	10.1038/ncomms11729	2016	0.0	0.0		1.0	2022-03-16T17:02:02.000Z	2026-02-02T19:46:52.000Z	d6ffe3b1-8da7-40dd-ae1e-a31ebb7c757c	undefined	5i5yt6gjy3				Starting_TSV	Match	No Match
63516fedb9c880af1f305b49		109 patients		60% training 40% validation							Cross-validation					Transparent - Lipid peroxidation was the most important predictor of the changes in LDL levels	Regression			Classification and Regression Tree			12 markers		No					6312169df3794236aa9879f6	32858131.0	PMC7446623	02/02/2026 19:46:52	Grundler F, Mesnage R, Goutzourelas N, Tekos F, Makri S, Brack M, Kouretas D, Wilhelmi de Toledo F.	Food and chemical toxicology : an international journal published for the British Industrial Biological Research Association	Interplay between oxidative damage, the redox status, and metabolic biomarkers during long-term fasting.	10.1016/j.fct.2020.111701	2020	0.0	0.0		1.0	2022-03-16T15:35:06.000Z	2026-02-02T19:46:52.000Z	f0595a6e-043d-4fb6-8569-dbdf82e84558	undefined	l0wznwpm1x				Starting_TSV	Match	No Match
63516fedb9c880af1f305b71		Untargeted two-dimensional gas chromatography and time-of-flight mass spectrometry. 12_pos and 13_neg								AUC	Other experiments and literature.					Transparent. 12 metabolites were determined to be informative for MS status	Binary			Random forest			325		No					6312169df3794236aa9879f6	30877925.0	PMC6548586	02/02/2026 19:46:52	Andersen SL, Briggs FBS, Winnike JH, Natanzon Y, Maichle S, Knagge KJ, Newby LK, Gregory SG.	Multiple sclerosis and related disorders	Metabolome-based signature of disease pathology in MS.	10.1016/j.msard.2019.03.006	2019	0.0	0.0		1.0	2022-03-02T16:41:47.000Z	2026-02-02T19:46:52.000Z	52d91d41-3b36-4240-bf74-f433787be438	undefined	21vzta5ft2				Starting_TSV	Match	No Match
