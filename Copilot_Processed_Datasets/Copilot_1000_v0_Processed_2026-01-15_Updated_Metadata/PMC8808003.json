{
  "publication/title": "Using Machine Learning to Predict Remission in Patients With Major Depressive Disorder Treated With Desvenlafaxine.",
  "publication/authors": "Benoit JRA, Dursun SM, Greiner R, Cao B, Brown MRG, Lam RW, Greenshaw AJ",
  "publication/journal": "Canadian journal of psychiatry. Revue canadienne de psychiatrie",
  "publication/year": "2022",
  "publication/pmid": "34379019",
  "publication/pmcid": "PMC8808003",
  "publication/doi": "10.1177/07067437211037141",
  "publication/tags": "- antidepressants\n- major depressive disorder\n- randomized controlled trial\n- diagnosis\n- machine learning\n- artificial intelligence\n- symptom remission\n- desvenlafaxine\n- clinical trials\n- predictive modeling",
  "dataset/provenance": "The dataset used in this study was obtained through a data access agreement between Pfizer Inc. and the University of Alberta. The data were collected from 11 phase-III/IV clinical trials of desvenlafaxine succinate (DVS), a serotonin and norepinephrine reuptake inhibitor (SNRI). These trials were conducted between 2003 and 2011 and spanned 23 countries across five continents. The total enrollment across these trials was 7,051 patients. However, after applying inclusion criteria, the dataset was reduced to 3,776 patients. From this reduced dataset, 3,399 patients were randomly selected for the training set, and 377 patients were set aside as a holdout set. The inclusion criteria specified that only patients with a primary diagnosis of Major Depressive Disorder (MDD), who were treated with DVS monotherapy, and who completed a 17-item Hamilton Depression Rating Scale (HAM-D) assessment at both baseline and 8 weeks were included. The dataset included a variety of features such as psychiatric scale items, demographic data, lab tests, and a measure of polypharmacy. This dataset has not been used in previous papers by the community, as it was specifically compiled for this study.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a holdout set. The training set consisted of 3,399 data points, while the holdout set contained 377 data points. The training set was further divided into five folds for cross-validation purposes, with each fold containing approximately 80% of the training data for training and 20% for validation. This process was repeated five times to ensure robustness in the model's performance estimation. The holdout set was used to evaluate the generalizability of the trained classifier to novel patients, providing a single accuracy value. The distribution of data points in the training and holdout sets was balanced to maintain the proportion of remitters versus nonremitters, ensuring that the model's performance could be accurately assessed.",
  "dataset/redundancy": "The dataset used in this study was derived from 11 phase-III/IV clinical trials of desvenlafaxine succinate, conducted between 2003 and 2011, involving a total of 7,051 patients. To ensure a robust and independent evaluation of the predictive model, the dataset was split into a training set and a holdout set.\n\nThe training set consisted of 3,399 patients, which accounted for 90% of the total dataset. The remaining 10%, comprising 377 patients, formed the holdout set. This split was done randomly to ensure that the holdout set was entirely independent of the training set, mimicking a real-world scenario where the model would be applied to new, unseen patients.\n\nThe independence of the training and holdout sets was enforced by ensuring that the holdout set was not used in any part of the model training process. This included feature selection and model evaluation, which were performed exclusively on the training data. The holdout set was only used to assess the generalizability of the trained model to novel patients.\n\nThe distribution of the dataset in terms of demographic information and HAM-D scores was carefully considered. The training and holdout sets were balanced to have similar mean demographic information and HAM-D scores, ensuring that the model's performance could be reliably evaluated on a representative sample of the patient population.\n\nCompared to previously published machine learning datasets in the field of psychiatry, this dataset is notable for its large size and global scope, spanning 23 countries across five continents. This diversity enhances the model's potential to generalize well to different populations, addressing a limitation often seen in single-country trials. The rigorous splitting and independent evaluation process used in this study align with best practices in machine learning to ensure the reliability and validity of the predictive model.",
  "dataset/availability": "The data used in this study were obtained through a data access agreement between Pfizer Inc. and the University of Alberta. This study was approved by the University of Alberta Research Ethics Board, study Pro00064974, and all patients involved gave written consent for their anonymized data to be used. The data are not publicly available due to the terms of the data access agreement and the need to protect patient privacy. The data access agreement ensures that the data are used solely for the purposes outlined in the approved study protocol and that the data are not shared with third parties without prior consent. The data access agreement also includes provisions for data security and confidentiality, ensuring that the data are stored and transmitted securely. The data access agreement was enforced through regular audits and monitoring by both Pfizer Inc. and the University of Alberta to ensure compliance with the terms of the agreement.",
  "optimization/algorithm": "The machine-learning algorithm class used is a linear support vector machine (SVM). This algorithm was selected after evaluating 11 different algorithms, each of which produced a classifier from the training dataset using five-fold internal cross-validation. The linear SVM classifier demonstrated the highest accuracy among the tested algorithms.\n\nThe machine-learning algorithm is not new. It is a well-established method in the field of machine learning. The choice to use this algorithm was based on its performance in predicting remission versus nonremission in patients with major depressive disorder (MDD) using the selected subset of features.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of the study was on its application in clinical practice rather than the development of a new machine-learning technique. The study aimed to demonstrate the feasibility of using easily collected baseline data to improve the prediction of antidepressant efficacy. The linear SVM was chosen for its effectiveness in this specific clinical context, and the results were published in a psychiatric journal to highlight its practical implications for clinical practice.",
  "optimization/meta": "The model described does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it directly utilizes a set of 26 selected features from the training data to predict symptom remission in patients diagnosed with Major Depressive Disorder (MDD) after 8 weeks of treatment with desvenlafaxine (DVS) monotherapy.\n\nThe learning algorithm involves three main steps:\n\n1. **Feature Selection**: The algorithm identifies the most informative features for predicting remission versus nonremission. This is done by partitioning the input dataset into five disjoint subsets, or \"folds,\" and selecting features from each training fold using Lasso, a regularization method that removes less useful features. The common features between these folds form a feature subset.\n\n2. **Base Learner Selection**: The algorithm then applies 11 different algorithms to the feature subset, each producing a classifier from the training dataset using five-fold internal cross-validation. The linear support vector machine (SVM) classifier was found to have the highest accuracy.\n\n3. **Classifier Training**: The linear SVM learner is run on all the labeled training data using the selected subset of features to produce a final trained classifier.\n\nThe training data used for the classifier is independent and consists of historical data from clinical trials. The model's performance is evaluated using external cross-validation over the training data and by applying the trained classifier to a held-out patient dataset to determine its generalizability to novel patients. The held-out dataset is entirely separate from the dataset used to train the classifier, ensuring that the training data is independent.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the data was suitable for training and evaluation. Initially, the dataset included 92 features, which were composed of psychiatric scale items, demographic data, lab tests, and a measure of polypharmacy. Missing data points were handled using mean imputation, where each missing value was replaced with the mean of that feature's nonmissing values. This approach was necessary for a small percentage of the data, with a maximum imputation of 8.87% for any single feature. Features that were missing from some trial datasets were excluded during preprocessing, such as body mass index (BMI).\n\nThe dataset was partitioned into five disjoint subsets, or \"folds,\" balanced to the proportion of remitters versus nonremitters. This partitioning was crucial for the feature selection process, which used Lasso, a regularization method that effectively removes less useful features from the model. The common features selected across these folds formed a subset of 26 features, which were then used for further analysis.\n\nThe machine learning algorithm was designed in Python 3, utilizing the pandas and sklearn libraries for preprocessing and modeling, respectively. The algorithm involved supervised machine learning, as the data explicitly provided labels indicating whether patients remitted at 8 weeks. This approach allowed for the creation of a classifier that could predict symptom remission based on personal features. The final trained classifier was evaluated using cross-validation and a held-out patient dataset to ensure its generalizability to novel patients.",
  "optimization/parameters": "The model utilized 92 features initially, which were known for each patient at the start of the trial. These features included psychiatric scale items, demographic data, lab tests, and a measure of polypharmacy. The learning algorithm employed a consistency-based feature selection method to reduce the initial set of features, ensuring that the selected features were robust and consistent across different subsets of the data. This process involved partitioning the input dataset into five disjoint subsets, or \"folds,\" and using Lasso, a regularization method, to select features from each training fold. The common features between these folds were then used to form a subset. This method helped to avoid overfitting and ensured that the model was not overly complex. Ultimately, 26 features were selected by all five folds and used in the final model. These features included items from psychiatric scales, countries of origin, ethnicity, lab tests, and a measure of polypharmacy. The selection of these 26 features was consistent and robust, providing a reliable basis for the model's predictions.",
  "optimization/features": "The input features for the predictive model were initially composed of 92 features, which included psychiatric scale items from the Clinical Global Impressions Scale (CGI), Montgomery\u2013\u00c5sberg Depression Rating Scale (MADRS), and Hamilton Depression Rating Scale (HAM-D), along with demographic data, lab tests, and a measure of polypharmacy.\n\nFeature selection was performed to identify the most informative features for predicting remission versus nonremission. This process involved partitioning the input dataset into five disjoint subsets, or \"folds,\" balanced to the proportion of remitters versus nonremitters. Features were selected from each training fold using Lasso, a regularization method that effectively removes less useful features from the model. The common features between these folds were then used to form a feature subset. This resulted in a set of 26 features that were consistently selected across all five folds.\n\nThe feature selection process was conducted using only the training set, ensuring that the evaluation of the model's performance on the held-out dataset remained unbiased. This approach helped to reduce overfitting and ensured that the selected features were robust and generalizable to new data.",
  "optimization/fitting": "The fitting method employed in this study involved a machine learning algorithm designed to predict treatment remission using baseline clinical information. The algorithm initially considered 92 features, which included psychiatric scale items, demographic data, lab tests, and a measure of polypharmacy. To address the potential issue of overfitting, given the large number of features relative to the training points, a feature selection process was implemented. This process involved partitioning the input dataset into five disjoint subsets, or \"folds,\" and using Lasso regularization to select features from each training fold. The common features across these folds were then used to form a subset, resulting in 26 features that were consistently informative. This approach helped to mitigate overfitting by ensuring that the selected features were robust and not overly specific to any single subset of the data.\n\nTo further validate the model and rule out underfitting, the algorithm was evaluated using five-fold cross-validation over the training data. This involved running the entire learning process, including feature selection, five times, each time on 80% of the training data and evaluating the classifier on the remaining 20%. The average accuracy of these five evaluations provided an estimate of the model's performance on the overall dataset. Additionally, the trained classifier was applied to a held-out patient dataset to assess its generalizability to novel patients. The accuracy on this holdout set was further validated using bootstrapping, ensuring that the model's performance was significantly above chance.\n\nThe use of cross-validation and bootstrapping techniques helped to ensure that the model was neither overfitting nor underfitting the data. The consistent selection of features across different folds and the robust performance on both training and holdout datasets indicate that the model is well-generalized and capable of making accurate predictions on new, unseen data.",
  "optimization/regularization": "In our study, we employed regularization techniques to prevent overfitting. Specifically, we used Lasso, a regularization method that effectively removes less useful features from the model. This was part of our feature selection process, where the input dataset was initially partitioned into five disjoint subsets, or \"folds,\" balanced to the proportion of remitters versus nonremitters. Lasso was applied to each training fold to select features, and the common features between these folds were used to form a feature subset. This approach helped in identifying the most informative features for predicting remission versus nonremission, thereby reducing the risk of overfitting. Additionally, we used five-fold cross-validation over the training data to further ensure that our model's performance was not optimistically biased. This involved running the entire learning process, including feature selection, five times, each time on 80% of the training data, and evaluating the classifier on the remaining 20%. The average accuracy of these five performance evaluations was then reported as an estimate of the accuracy on the overall learned model.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed in the supplemental material accompanying the publication. This material provides a formal description of the process, including the specific parameters and schedules employed during the training and evaluation of our machine learning models.\n\nThe model files themselves are not directly available, as they are proprietary and tied to the specific data and agreements under which the study was conducted. However, the methods and algorithms used are standard and can be replicated using publicly available libraries such as pandas and sklearn in Python.\n\nRegarding the optimization parameters, these are also outlined in the supplemental material. The process involved using cross-validation to ensure that the models were not optimistically biased, and the results of these evaluations are reported in the main text.\n\nThe data used in this study was obtained through a data access agreement between Pfizer Inc. and the University of Alberta, and the study was approved by the University of Alberta Research Ethics Board. All patients involved gave written consent for their anonymized data to be used. Therefore, the data itself is not publicly available due to these agreements and ethical considerations.\n\nIn summary, while the specific model files are not available, the hyper-parameter configurations, optimization schedule, and parameters are documented in the supplemental material. The methods and algorithms used are standard and can be replicated using publicly available tools.",
  "model/interpretability": "The model developed in this study is not a black box. It is based on a Support Vector Machine (SVM) classifier, which is a type of machine learning model known for its interpretability, especially when used with linear kernels. The SVM classifier operates by finding a hyperplane that best separates the two classes in a 26-dimensional feature space. This hyperplane is defined by the selected features, making it possible to understand which features contribute most to the classification decision.\n\nThe features selected by the model are clear and interpretable. They include nine countries of origin, one ethnicity, eight items from the Hamilton Depression Rating Scale (HAM-D), three items from the Montgomery-\u00c5sberg Depression Rating Scale (MADRS), one measure of polypharmacy, and four lab tests. These features were consistently selected across all five folds of the cross-validation process, indicating their robustness and importance in predicting symptom remission.\n\nFor example, the HAM-D items selected include anxiety/somatic symptoms, feelings of guilt, genital symptoms, loss of insight, insomnia/early, somatic symptoms/gastrointestinal, somatic symptoms/general, and work and activities. These items are well-validated psychiatric measures that are easily understood and interpreted by clinicians. Similarly, the MADRS items selected include apparent sadness, pessimistic thoughts, and reported sadness, which are core features of major depressive disorder.\n\nThe inclusion of demographic features like countries of origin and ethnicity, along with lab tests such as albumin, creatinine, potassium, and urine pH, adds another layer of interpretability. These features provide insights into how biological and demographic factors influence treatment outcomes. The measure of polypharmacy, which counts the number of pills including supplements and nonprescription drugs, also offers a practical perspective on how medication complexity might affect remission.\n\nOverall, the model's transparency is enhanced by the clear selection of features and the straightforward mechanism of the SVM classifier. This makes it easier for clinicians to understand the basis of the predictions and to integrate the model's insights into their clinical practice.",
  "model/output": "The model developed is a classification model. It is designed to predict whether a patient will experience symptom remission after 8 weeks of desvenlafaxine (DVS) monotherapy. The model uses a linear support vector machine (SVM) as the base learner, which generates a hyperplane in a 26-dimensional space to separate the two classes: remitters and nonremitters. The output of the model is a binary classification, indicating whether a novel patient is likely to achieve symptom remission or not.\n\nThe model's performance was evaluated using accuracy as the primary metric. In the internal cross-validation folds, the model achieved an accuracy of 67.0% \u00b11.8%, which is significantly better than the chance accuracy of 62.1%. When tested on a holdout dataset, the model demonstrated a mean accuracy of 69.0%. This indicates that the model can classify patients into remitters and nonremitters at above-chance levels, even when applied to data that was not used during training.\n\nThe model's output is based on 26 selected features, which include items from psychiatric scales, countries of origin, ethnicity, lab tests, and a measure of polypharmacy. These features were chosen through a feature selection process that involved Lasso regularization and five-fold cross-validation. The model's ability to generalize to novel patients suggests that it can be a useful tool in clinical practice for predicting treatment outcomes in patients with major depressive disorder.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The software used to develop the predictive model for treatment remission was designed in Python 3, primarily utilizing the pandas and sklearn libraries for preprocessing and modeling, respectively. However, the source code for the machine learning algorithm is not publicly released. There is no mention of an executable, web server, virtual machine, or container instance being made available for running the algorithm. Therefore, no specific details about the availability or licensing of the software can be provided.",
  "evaluation/method": "The evaluation method employed for the predictive model involved two primary approaches to ensure its robustness and generalizability. Firstly, external cross-validation was utilized over the training data. This process involved running the entire learning algorithm, including feature selection, five times. Each iteration used 80% of the training data to train the classifier, while the remaining 20% was used for evaluation. This method ensured that the performance estimate was not optimistically biased, as each model was evaluated on instances it had not been trained on. The average accuracy of these five evaluations was reported as an estimate of the overall model's accuracy.\n\nSecondly, the trained classifier was applied to a held-out patient dataset, which consisted of data entirely separate from the training dataset. This step was crucial for determining whether the model could generalize to novel patients. The accuracy value obtained from this evaluation was assessed for statistical significance using bootstrapping. This involved 10,000 draws with replacement from the holdout set, considering the number of correctly classified instances. The confidence level that the classifier performed significantly above chance was determined by checking if at least 95% of these accuracy values were above the chance probability of correctly classifying a patient by assuming all patients were nonremitters, which corresponded to the majority class at 62.1%.",
  "evaluation/measure": "In our evaluation, we primarily focused on accuracy as our performance measure. This metric was chosen because it equally weighs the importance of true positives and true negatives, which is crucial for our binary classification problem of predicting remission versus nonremission in patients with Major Depressive Disorder (MDD).\n\nWe reported the average accuracy from five-fold cross-validation on the training data, which was 67.0% \u00b11.8%. This indicates the model's performance on data it was trained on. To assess the statistical significance of this accuracy, we performed a two-tailed t-test, showing that our model's accuracy is significantly different from the chance accuracy of 62.1%, with a P-value of 0.0065.\n\nAdditionally, we evaluated the model's generalizability by testing it on a held-out dataset, which was not used during the training process. The model achieved a mean accuracy of 69.0% on this holdout set. To ensure that this accuracy was above chance, we used bootstrapping with 10,000 draws with replacement. The results showed that the bootstrap sampling fell below chance accuracy only 0.25% of the time, providing strong evidence that our model performs significantly better than random guessing.\n\nWhile accuracy is a straightforward and commonly used metric, it is important to note that it does not provide information about the types of errors the model makes. Future work could include reporting additional metrics such as precision, recall, and the F1-score to give a more comprehensive view of the model's performance, especially in the context of imbalanced datasets. However, given the balanced nature of our dataset and the focus on overall predictive performance, accuracy serves as a representative and informative metric for our study.",
  "evaluation/comparison": "Not applicable. The publication does not provide information about comparisons to publicly available methods or simpler baselines on benchmark datasets. The focus is on the development and evaluation of a specific machine learning approach for predicting symptom remission in patients with major depressive disorder. The evaluation involves internal cross-validation and testing on a held-out dataset, but there is no mention of comparing the proposed method to other publicly available methods or simpler baselines.",
  "evaluation/confidence": "To assess the confidence in our evaluation, we employed bootstrapping to determine if our model's accuracy was significantly above chance. We performed 10,000 draws with replacement from the holdout set, each of size 377, and checked how many instances were correctly labeled by our model. We found that at least 95% of these accuracy values were above the chance probability, which was calculated assuming all patients were nonremitters (the majority class at 62.1%). This approach provided a robust measure of confidence that our trained classifier performed significantly better than random chance.\n\nAdditionally, we used external cross-validation over the training data. This involved running the entire learning process, including feature selection, five times, each time on 80% of the training data, and evaluating the classifier on the remaining 20%. This method ensured that our accuracy estimates were not optimistically biased, as each model was evaluated on instances it was not trained on. The average accuracy of these five evaluations gave us a reliable estimate of the overall model's performance.\n\nWhile we did not explicitly report confidence intervals for our performance metrics, the use of bootstrapping and cross-validation provides a strong indication of the statistical significance of our results. These methods help to ensure that our claims of the model's superiority are well-founded and not due to random chance.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The data were obtained through a data access agreement between Pfizer Inc. and the University of Alberta. This agreement ensures that the data are used solely for the purposes outlined in our approved study protocol. The data include sensitive patient information, and their release is restricted to protect patient privacy and comply with ethical guidelines. Therefore, while the results and methodologies are openly shared, the raw datasets themselves are not publicly accessible."
}