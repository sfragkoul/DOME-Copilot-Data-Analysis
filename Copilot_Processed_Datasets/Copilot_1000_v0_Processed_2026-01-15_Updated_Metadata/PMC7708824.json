{
  "publication/title": "Development and validation of a real-time artificial intelligence-assisted system for detecting early gastric cancer: A multicentre retrospective diagnostic study.",
  "publication/authors": "Tang D, Wang L, Ling T, Lv Y, Ni M, Zhan Q, Fu Y, Zhuang D, Guo H, Dou X, Zhang W, Xu G, Zou X",
  "publication/journal": "EBioMedicine",
  "publication/year": "2020",
  "publication/pmid": "33254026",
  "publication/pmcid": "PMC7708824",
  "publication/doi": "10.1016/j.ebiom.2020.103146",
  "publication/tags": "- Artificial Intelligence\n- Deep Convolutional Neural Networks\n- Early Gastric Cancer\n- Diagnostic Accuracy\n- Sensitivity\n- Specificity\n- Endoscopy\n- Medical Imaging\n- Retrospective Study\n- Diagnostic Performance",
  "dataset/provenance": "The dataset used in this study consisted of endoscopic images and videos collected from multiple hospitals. The primary training dataset included 35,823 images from 1085 patients, recorded between January 2016 and October 2018. Among these images, 26,172 contained cancerous lesions.\n\nA temporal validation dataset, independent of the training dataset, comprised 9,417 images from 279 patients, recorded between November 2018 and January 2019. Of these, 4,153 images contained cancerous lesions.\n\nFor external validation, images were selected from three different hospitals: 406 images from 20 patients from WXPH, 556 images from 20 patients from TZPH, and 552 images from 20 patients from GCPH. These images were obtained between June 2019 and October 2019. Additionally, a video dataset included 26 videos from 26 patients, recorded between November 2019 and December 2019.\n\nA testing dataset was created by randomly selecting 300 cancerous images and 300 control images (without malignant lesions) from the temporal validation dataset. The control images included various types of non-cancerous conditions such as chronic non-atrophic gastritis, chronic atrophic gastritis, and erosion.\n\nAll endoscopic images and videos were recorded using Olympus endoscopes and video processors. The images were anonymized to protect patient privacy. The dataset was used to develop and validate a deep convolutional neural network (DCNN) system for detecting early gastric cancer (EGC) lesions. The system's performance was evaluated using multiple validation datasets, demonstrating good diagnostic ability and stability.",
  "dataset/splits": "There were five distinct data splits used in this study.\n\nThe first split was the training dataset, which consisted of 35,823 images from 1,085 patients. Among these images, 26,172 contained cancerous lesions.\n\nThe second split was the temporal validation dataset, which included 9,417 images from 279 patients. Of these images, 4,153 contained cancerous lesions.\n\nThe third split involved external validation datasets from three different hospitals. From Wuxi People's Hospital, 406 images of 20 patients were selected, with 203 images containing malignant lesions. From Taizhou People's Hospital, 556 images of 20 patients were included, with 228 images containing malignant lesions. From Gaochun People's Hospital, 552 images of 20 patients were used, with 226 images including cancerous lesions. All these images were obtained and filed between June 2019 and October 2019.\n\nThe fourth split was a video dataset, which included 26 videos from 26 patients. These videos were recorded between November 2019 and December 2019.\n\nThe fifth split was the testing dataset, which consisted of 300 cancerous images and 300 control images (no malignant lesions). These images were randomly selected from the temporal validation dataset to compare the performance of the DCNN system and endoscopists. The control images included several types of non-cancerous conditions, such as chronic non-atrophic gastritis, chronic atrophic gastritis, and erosion.",
  "dataset/redundancy": "The datasets used in this study were carefully split to ensure independence and to avoid redundancy. The training dataset consisted of 35,823 images from 1,085 patients, collected between January 2016 and October 2018. The temporal validation dataset included 9,417 images from 279 patients, collected between November 2018 and January 2019. This temporal separation ensured that the validation dataset was independent of the training dataset, simulating a prospective clinical trial setting and yielding more objective results.\n\nFor external validation, images were selected from three different hospitals: 406 images from 20 patients, 556 images from 20 patients, and 552 images from 20 patients. These images were collected between June 2019 and October 2019, further ensuring independence from the training and temporal validation datasets. Additionally, a video dataset consisting of 26 videos from 26 patients was used to assess the system's performance in real-time, collected between November 2019 and December 2019.\n\nThe testing dataset included 300 cancerous images and 300 control images, randomly selected from the temporal validation dataset. This dataset was used to compare the performance of the deep convolutional neural network (DCNN) system with that of endoscopists. The control images included various types of non-cancerous conditions, such as chronic non-atrophic gastritis, chronic atrophic gastritis, and erosion.\n\nThe distribution of the datasets in this study is notable for its careful separation in time and source, which is a significant improvement over some previously published machine learning datasets. For instance, in a study by Hong et al., the validation dataset was a small subset randomly selected from the entire collected images, potentially leading to overfitting due to the presence of the same patient's images in both training and validation datasets. In contrast, the datasets used in this study were collected at different time intervals and from different hospitals, ensuring a more robust and generalizable validation process. This approach helps to mitigate the risk of overfitting and provides a more reliable assessment of the DCNN system's performance.",
  "dataset/availability": "The data used in this study is not fully released in a public forum. However, an open-access image database containing 300 cancerous lesions and 300 non-cancerous controls is available on a website developed to provide free access to our DCNN system. This database might be a useful resource for researchers in the field of AI-assisted medical imaging. The website can be accessed at http://112.74.182.39. The availability of this dataset is subject to reasonable request, and the specific terms and conditions, including the license, are not detailed here. The dataset includes images from various validation datasets, ensuring a comprehensive resource for further research.",
  "optimization/algorithm": "The optimization algorithm employed in our study is based on deep convolutional neural networks (DCNN). This class of machine-learning algorithms is well-established and widely used for various image recognition tasks, including medical image analysis.\n\nThe specific DCNN architecture used in our system is not entirely novel, as it leverages the Darknet-53 model, which is a pre-existing backbone structure known for its effectiveness in feature extraction. However, the application of this architecture to the detection of early gastric cancer (EGC) lesions is a significant contribution of our work. The decision layers of our DCNN system are designed to detect lesions at multiple scales, enhancing the system's ability to identify both large and small objects within the images.\n\nThe reason this work was published in a medical journal rather than a machine-learning journal is that the primary focus of our study is on the clinical application and performance of the DCNN system in detecting EGC lesions. The innovation lies in the medical context and the specific adaptations made to the DCNN architecture for this particular diagnostic task. The study evaluates the system's diagnostic accuracy, sensitivity, specificity, and other performance metrics in comparison to human endoscopists, providing valuable insights for the medical community.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding process involved categorizing images into cancer and non-cancer groups. Experienced endoscopists annotated the cancer images by outlining the lesion boundaries, which were then converted into annotation boxes by a computer. These boxes were precisely controlled in size to ensure consistency. To minimize individual bias, annotations were finalized only when a consensus was reached among more than four endoscopists.\n\nThe training process for the deep convolutional neural network (DCNN) system involved initializing the parameters of the network's neurons to random values. For each annotated image, the system compared the predicted lesion location with the annotated areas and slightly modified the parameters to reduce the error. This process was repeated multiple times for every image in the training set.\n\nThe DCNN architecture comprised two main parts: the backbone structure for feature extraction and the detection and decision layers for identifying lesion locations. The backbone structure used was the Darknet-53 model, which includes 53 layers of neurons. This model employs residual networks to increase the depth of the feature-extracting network. The decision layers predict the coordinates of bounding boxes based on the extracted features and output the confidence of the content within these boxes. Three different scales of predictors were used to detect objects of varying sizes.\n\nThe confidence of the detection task was defined as the product of the intersection-over-union (IoU) and the probability of classification. The IoU evaluates the overlap between the predicted and ground truth bounding boxes, while the probability of classification assesses the likelihood of the object within the predicted bounding box. This approach ensured that the system could accurately detect and classify lesions in the images.",
  "optimization/parameters": "In our study, the deep convolutional neural network (DCNN) system employed a backbone structure based on the Darknet-53 model, which comprises 53 layers of neurons. This architecture includes a sequence of non-linear processing modules, each consisting of one or more convolutional layers with batch normalization and Leaky ReLU non-linearity activation functions. The number of parameters in the model is determined by the architecture of the Darknet-53 and the additional layers used for detection and decision-making.\n\nThe parameters of the neurons in the network were initially set to random values. During the training process, these parameters were adjusted to minimize the error in detecting lesion locations within the annotated images. This iterative process was repeated multiple times for every image in the training set.\n\nThe selection of the Darknet-53 model was based on its proven effectiveness in feature extraction and its ability to handle complex image data. The model's depth and the use of residual networks allowed for increased feature extraction capabilities, which are crucial for accurately detecting lesions in endoscopic images.\n\nThe decision layers of the model predict four coordinates for each bounding box based on the features extracted from the convolutional layers. Three different scales of predictors were employed to detect large, medium, and small objects, ensuring that the model could handle a variety of lesion sizes. The confidence of the results is calculated using the Intersection-over-Union (IoU) to evaluate the overlap of predicted bounding boxes and the ground truth bounding boxes, along with the probability of classification. This comprehensive approach ensures that the model's predictions are both accurate and reliable.",
  "optimization/features": "The input features for the deep convolutional neural network (DCNN) system consist of endoscopic images. Specifically, the training dataset included 35,823 images, while the temporal validation dataset comprised 9,417 images. Additionally, external validation datasets included 406 images from WXPH, 556 images from TZPH, and 552 images from GCPH. The testing dataset consisted of 300 cancerous images and 300 control images.\n\nFeature selection in the traditional sense was not explicitly performed, as the DCNN system relies on the raw pixel data of the images. The backbone structure of the DCNN, specifically the Darknet-53 model, automatically extracts relevant features from the images through its convolutional layers. This model contains 53 layers of neurons, which process the images to identify and locate lesions.\n\nThe annotations for the images were created by experienced endoscopists who outlined the boundaries of lesion areas. These annotations were used to generate annotation boxes, which were then used to train the DCNN system. The consensus of more than four endoscopists was required to finalize the annotations, ensuring the accuracy and reliability of the input features.",
  "optimization/fitting": "The deep convolutional neural network (DCNN) system employed in this study utilized a substantial number of parameters, which could potentially lead to overfitting, especially given the complexity of the network architecture. To mitigate this risk, several strategies were implemented.\n\nFirstly, the training and validation datasets were collected at different time intervals, simulating the conditions of prospective clinical trials. This approach helped to ensure that the model's performance was more objective and less prone to overfitting compared to using a small, randomly selected subset of images from the same dataset.\n\nSecondly, the architecture of the DCNN system included residual networks within the backbone structure, which are designed to increase the depth of the feature-extracting network without compromising performance. This design helps in maintaining a balance between the complexity of the model and its ability to generalize from the training data.\n\nAdditionally, the decision layers of the DCNN system employed three different scales of predictors to detect objects of varying sizes, enhancing the model's robustness and reducing the likelihood of overfitting to specific image features.\n\nTo further ensure that the model did not underfit, the training process involved multiple iterations where the parameters of the network were adjusted to minimize errors in the training set. This iterative process helped in fine-tuning the model to capture the essential features of the images accurately.\n\nThe performance of the DCNN system was evaluated using multiple independent validation datasets, including both internal and external datasets. The consistent high accuracy, sensitivity, and specificity across these datasets indicated that the model was well-generalized and not overfitted to the training data.\n\nIn summary, the DCNN system was designed with careful consideration to avoid both overfitting and underfitting. The use of temporally separated datasets, residual networks, multi-scale predictors, and rigorous validation processes ensured that the model achieved a balanced and robust performance in detecting early gastric cancer lesions.",
  "optimization/regularization": "In our study, we implemented several techniques to prevent overfitting and ensure the robustness of our deep convolutional neural network (DCNN) system. One key approach was the use of separate datasets for training and validation, collected at different time intervals. This strategy simulates the conditions of prospective clinical trials, providing a more objective evaluation of our model's performance.\n\nAdditionally, we employed a backbone structure based on the Darknet-53 model, which includes residual networks to increase the depth of the feature-extracting network without compromising performance. This architecture helps in mitigating overfitting by allowing the network to learn more complex features while maintaining stability.\n\nDuring the training process, we utilized techniques such as batch normalization and Leaky ReLU activation functions, which help in regularizing the network and preventing overfitting. Batch normalization standardizes the inputs to a layer for each mini-batch, which stabilizes and accelerates the training process. Leaky ReLU activation functions allow a small, non-zero gradient when the unit is not active, which helps in preventing the neurons from dying and ensures that the network continues to learn.\n\nFurthermore, we used multiple scales of predictors to detect lesions of different sizes, enhancing the model's ability to generalize to various types of early gastric cancer lesions. This multi-scale approach ensures that the model is not overly specialized to the training data and can perform well on unseen data.\n\nOverall, these regularization methods contributed to the development of a DCNN system with high diagnostic accuracy and robustness, capable of detecting early gastric cancer lesions effectively.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model employed in our study is a Deep Convolutional Neural Network (DCNN), which is inherently a complex system with multiple layers of neurons. This complexity makes the model somewhat of a black box, as the internal workings and decision-making processes are not easily interpretable by humans.\n\nThe DCNN architecture primarily consists of two main parts: the backbone structure and the detection and decision layers. The backbone structure, based on the Darknet-53 model, is responsible for extracting features from the input images. This structure includes 53 layers of neurons organized into non-linear processing modules, each containing convolutional layers with batch normalization and Leaky ReLU activation functions. The use of residual networks within these modules increases the depth of the feature-extracting network, allowing it to capture intricate patterns in the images.\n\nThe detection and decision layers utilize the features extracted by the backbone to predict the location of lesions within the images. These layers output four coordinates for each bounding box, representing the predicted lesion areas. Additionally, they provide a confidence score for the content within these bounding boxes, which is composed of two parts: the Intersection-over-Union (IoU) and the probability of classification. The IoU evaluates the overlap between the predicted bounding boxes and the ground truth bounding boxes, while the probability of classification indicates the likelihood of the object within the bounding box belonging to a specific class.\n\nWhile the DCNN model is powerful in detecting lesions, its interpretability is limited due to its complexity. The model's decisions are based on intricate patterns and features that are not easily discernible to human observers. However, the use of bounding boxes and confidence scores provides some level of transparency, as these outputs can be visually inspected and compared against ground truth annotations. This allows for a basic understanding of where and how confidently the model is making its predictions.",
  "model/output": "The model is a classification system. It is designed to categorize images into cancer and non-cancer groups. Specifically, it detects early gastric cancer (EGC) lesions in endoscopic images. The system uses a deep convolutional neural network (DCNN) to analyze images and output predictions regarding the presence of cancerous lesions. The output includes the location of lesions, confidence in the detection, and classification into cancer or non-cancer categories. The model's performance is evaluated using metrics such as accuracy, sensitivity, specificity, positive predictive value, and negative predictive value, which are typical for classification tasks. The system's outputs are compared against ground truth annotations made by experienced endoscopists, further confirming its role as a classification model.",
  "model/duration": "The model, a deep convolutional neural network (DCNN) system, demonstrated an exceptionally fast diagnostic speed. It required only 15 milliseconds to diagnose a single image, which translates to the capability of diagnosing over 60 images per second in real-time. This rapid processing speed was a key factor in testing the system's performance on a video dataset, where it successfully detected 23 lesions in 26 OGD videos. The sensitivity of the DCNN system for detecting lesions in these videos was 88.5%, with a 95% confidence interval ranging from 71.0% to 96.0%. This efficiency underscores the potential of the DCNN system to assist in real-time detection during routine OGD examinations.",
  "model/availability": "We have developed a website to provide free access to our Deep Convolutional Neural Network (DCNN) system. The website can be accessed at http://112.74.78.39. This platform allows users to upload gastroscope images for analysis. The website supports multiple image selections, with a maximum of nine images at a time. The results of the analysis are displayed on the webpage, with the original images shown in the left column and the AI-analyzed results demonstrated in the right column. Users can also download the results.\n\nAdditionally, we have made an open-access image database available on the website. This database contains 300 cancerous lesions and 300 non-cancerous controls, which can be a valuable resource for researchers in the field of AI-assisted medical imaging. The dataset is available upon reasonable request through the website.",
  "evaluation/method": "The evaluation of the DCNN system involved several rigorous steps to ensure its diagnostic performance and stability. The system was assessed using four independent validation datasets, including a temporal validation dataset and three external validation datasets from different hospitals. The temporal validation dataset consisted of images collected at a different time interval from the training dataset, simulating prospective clinical trials. The external validation datasets included images from Wuxi People\u2019s Hospital, Taizhou People\u2019s Hospital, and Gaochun People\u2019s Hospital, ensuring the system's generalizability across different settings.\n\nThe primary outcomes measured were accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). These metrics were calculated to evaluate the system's ability to detect early gastric cancer (EGC) lesions accurately. Additionally, the area under the receiver operating characteristic (ROC) curve (AUC) was calculated to assess the diagnostic ability of the DCNN system.\n\nTo compare the performance of the DCNN system with that of endoscopists, a testing dataset was used. This dataset included images assessed by both expert and trainee endoscopists, who were blinded to the clinical characteristics and pathological results of the patients. The endoscopists were divided into two groups based on their level of expertise, and their diagnostic performance was evaluated and compared with the DCNN system.\n\nFurthermore, the stability of the DCNN system and endoscopists was assessed by re-testing the same group of images three days later. This step ensured that the diagnostic performance was consistent over time. The system's performance was also evaluated using a video dataset, which included real-time endoscopic videos, to assess its applicability in clinical practice.\n\nStatistical analysis was performed using SPSS and R software. A two-sided McNemar test was used to compare the differences in diagnostic metrics between endoscopists before and after consulting the DCNN results. Cohen\u2019s kappa coefficient was used to calculate inter-observer and intra-observer agreement. The study design was reviewed and approved by the Medical Ethics Committee, and the study was registered in the WHO Registry Network.",
  "evaluation/measure": "In the evaluation of our deep convolutional neural network (DCNN) system, several key performance metrics were reported to assess its diagnostic ability in detecting early gastric cancer (EGC) lesions. These metrics include accuracy, sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and the area under the receiver operating characteristic curve (AUC).\n\nAccuracy measures the proportion of true predictions (both true positives and true negatives) out of the total number of cases. Sensitivity, also known as recall, indicates the proportion of true positive cases correctly identified by the system. Specificity represents the proportion of true negative cases correctly identified. PPV is the proportion of positive predictions that are true positives, while NPV is the proportion of negative predictions that are true negatives. The AUC provides a single scalar value that summarizes the performance of the classifier across all classification thresholds.\n\nThe reported metrics are comprehensive and widely used in the literature for evaluating diagnostic systems, ensuring that our evaluation is representative and comparable to other studies in the field. The inclusion of sensitivity and specificity allows for an assessment of the system's ability to correctly identify both positive and negative cases. PPV and NPV provide additional insights into the practical utility of the system in clinical settings, where the consequences of false positives and false negatives can be significant. The AUC offers a summary measure of the system's overall diagnostic performance.\n\nThese metrics were evaluated across multiple validation datasets, including internal and external datasets, to ensure the robustness and generalizability of our findings. The results demonstrate that our DCNN system achieves high performance across all reported metrics, indicating its potential for effective clinical application in the detection of EGC lesions.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the performance of our Deep Convolutional Neural Network (DCNN) system against human endoscopists. The comparison involved two groups of endoscopists: experts with a minimum of 10 years of experience and 10,000 OGD examinations, and trainees with 2 years of experience and 2,000 OGD examinations. These endoscopists were blinded to the clinical characteristics, endoscopic manifestations, and pathological results of the patients.\n\nThe diagnostic performance was assessed using a testing dataset, where the DCNN system demonstrated superior accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) compared to both expert and trainee endoscopists. Specifically, the DCNN system achieved an accuracy of 95.3%, which was higher than the 87.3% accuracy of expert endoscopists and the 73.6% accuracy of trainee endoscopists. The sensitivity, specificity, PPV, and NPV of the DCNN system were also superior to those of the endoscopists.\n\nFurthermore, we combined the diagnostic ability of the DCNN system with that of the endoscopists. In the expert group, the diagnostic accuracy, sensitivity, and NPV significantly increased after combining the results of the DCNN system. The specificity and PPV remained comparable regardless of the assistance of the DCNN system. In the trainee group, the accuracy, sensitivity, specificity, and PPV all improved significantly with the assistance of the DCNN system.\n\nThis comparison highlights the potential of the DCNN system to enhance the diagnostic performance of endoscopists, particularly for those with less experience. The superior performance of the DCNN system in detecting early gastric cancer lesions suggests its potential for clinical application in improving diagnostic accuracy and reducing the burden on endoscopists and pathologists.",
  "evaluation/confidence": "The evaluation of the deep convolutional neural network (DCNN) system's performance included several key metrics, each accompanied by 95% confidence intervals (CIs). These metrics were used to assess the diagnostic ability of the DCNN system across various datasets.\n\nThe accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) were all reported with their respective CIs. For instance, in the NJDTH validation dataset, the accuracy was 87.8% with a CI of 87.1% to 88.5%. Similar detailed CIs were provided for other datasets, ensuring that the performance metrics were robust and reliable.\n\nStatistical significance was evaluated using the McNemar test to compare the differences in accuracy, sensitivity, specificity, PPV, and NPV of endoscopists before and after consulting the DCNN results. The results indicated significant improvements in diagnostic performance when the DCNN system was used in conjunction with endoscopists. For example, the accuracy of expert endoscopists increased from 87.3% to 94.3% when aided by the DCNN system, with a P-value of 0.002, indicating statistical significance.\n\nThe area under the receiver operating characteristic (ROC) curve (AUC) was also calculated to evaluate the diagnostic ability of the DCNN system. High AUC values, ranging from 0.887 to 0.940 across different datasets, demonstrated the excellent diagnostic performance of the DCNN system. Sensitivity analyses further confirmed the system's consistent performance across different subtypes of early gastric cancer lesions.\n\nIn summary, the performance metrics were thoroughly evaluated with confidence intervals, and statistical tests confirmed the superiority of the DCNN system over baseline methods and individual endoscopists. This comprehensive evaluation underscores the reliability and effectiveness of the DCNN system in diagnosing early gastric cancer.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, an open-access image database containing 300 cancerous lesions and 300 non-cancerous controls is available on the website developed for the DCNN system. This database might be a useful resource for researchers in the field of AI-assisted medical imaging. The website provides free access to the DCNN system and the open-access database, which can be accessed upon reasonable request. The specific details regarding the license for accessing these resources are not provided."
}