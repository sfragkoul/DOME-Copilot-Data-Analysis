{
  "publication/title": "Predicting community acquired bloodstream infection in infants using full blood count parameters and C-reactive protein; a machine learning study.",
  "publication/authors": "Brouwer L, Cunney R, Drew RJ",
  "publication/journal": "European journal of pediatrics",
  "publication/year": "2024",
  "publication/pmid": "38634890",
  "publication/pmcid": "PMC11192659",
  "publication/doi": "10.1007/s00431-024-05441-6",
  "publication/tags": "- Machine Learning\n- Pediatrics\n- Blood Stream Infection\n- Infants\n- Diagnostic Algorithms\n- Predictive Modeling\n- Logistic Regression\n- Decision Trees\n- Random Forest\n- Clinical Decision Making\n- Full Blood Count\n- C-Reactive Protein\n- Neonatal Sepsis\n- Emergency Department\n- Microbiological Analysis",
  "dataset/provenance": "The dataset used in this study was sourced from the electronic laboratory information system at Children's Health Ireland (CHI) Temple Street, Dublin. This dataset included infants aged 7 to 60 days who presented at the emergency department between January 1st, 2005, and December 17th, 2022, and received a work-up for suspected bloodstream infection (BSI). The total number of infants in the dataset was 2876. However, after excluding infants with likely contaminants in their blood cultures, outliers in their full blood count (FBC) results, and those with unknown sex, the final dataset consisted of 2692 infants.\n\nThe data collected included various parameters such as age in days, sex, year of sampling, FBC parameters (including white cell count, neutrophils, lymphocytes, monocytes, eosinophils, basophils, platelets, red cell count, red cell distribution width, hemoglobin, hematocrit, mean platelet volume, mean corpuscular hemoglobin, mean corpuscular hemoglobin concentration, and mean corpuscular volume), C-reactive protein (CRP), and blood culture results. This dataset has not been previously used in other publications by the community.",
  "dataset/splits": "The dataset was initially split to balance the case-control ratio, resulting in a subsampled dataset with a 1:3 ratio. This subsampled dataset was then divided into a training set and a test set. The training set contained 70% of the data, while the test set contained the remaining 30%. The subsampled dataset consisted of 304 data points, with the training set having approximately 213 data points and the test set having around 91 data points.\n\nAdditionally, the models were applied to the full dataset, which included 2692 data points. Furthermore, the models were tested on a separate 2023 cohort consisting of 206 infants, with 197 data points after excluding those with likely contaminants. This 2023 cohort was used to evaluate the models' performance on new, unseen data.",
  "dataset/redundancy": "The dataset used in this study consisted of infants aged 7 to 60 days who presented at the emergency department with suspected bloodstream infections (BSI). To ensure balanced data for model training, the controls were randomly subsampled to achieve a case-control ratio of 1:3. This subsampled dataset was then randomly split into a training set containing 70% of the data and a test set containing the remaining 30%. This split was done to ensure that the training and test sets were independent, minimizing the risk of data leakage and ensuring that the models could be evaluated on unseen data.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the medical field, which often struggle with imbalanced classes, particularly in rare disease or condition predictions. By subsampling the controls, we aimed to create a more balanced dataset that would allow our models to learn more effectively. This approach is consistent with best practices in machine learning, where balanced datasets are crucial for training robust and generalizable models.\n\nTo enforce the independence of the training and test sets, we used random splitting without any overlap between the two sets. This ensures that the performance metrics calculated on the test set are a true reflection of the model's ability to generalize to new, unseen data. The random splitting process was repeated multiple times to ensure that the results were consistent and not dependent on a particular split of the data. This rigorous approach helps to validate the reliability and robustness of the models developed in this study.",
  "dataset/availability": "The clinical data used for this study were extracted from the electronic laboratory information system at CHI Temple Street, Dublin, Ireland. This data is not publicly available to maintain patients\u2019 privacy in line with the European General Data Protection Regulation. However, anonymized data is available on request. This approach ensures that individual patient information remains confidential while still allowing for potential replication or further analysis by other researchers. The decision to keep the data private but offer anonymized versions upon request balances the need for data sharing in scientific research with the ethical responsibility to protect patient privacy.",
  "optimization/algorithm": "The study employed several machine-learning algorithms to predict positive blood cultures in infants. The algorithms used included Multivariable Logistic Regression, Linear Discriminant Analysis, K-Nearest Neighbors, Support Vector Machine, Random Forest, and Decision Tree. These are well-established machine-learning techniques commonly used in predictive modeling.\n\nNone of the algorithms used in this study are new. They are standard methods that have been extensively studied and applied in various fields, including medical research. The choice of these algorithms was driven by their proven effectiveness in handling similar predictive tasks.\n\nThe decision to use these established algorithms rather than developing a new one was influenced by the need for reliability and interpretability in a clinical setting. The Decision Tree model, in particular, was selected for its ease of interpretation at the bedside, making it practical for clinical use. The Random Forest model was chosen for comparison due to its robustness and ability to handle complex datasets.\n\nThe focus of this study was on applying these algorithms to a specific medical problem\u2014predicting bloodstream infections in infants\u2014rather than on developing new machine-learning techniques. Therefore, the results and insights gained from using these algorithms are more relevant to the medical community than to the machine-learning research community, which is why the study was published in a pediatric journal rather than a machine-learning journal.",
  "optimization/meta": "The models developed in this study do not utilize data from other machine-learning algorithms as input. Instead, they are trained on clinical and laboratory parameters derived from the electronic laboratory information system. The models include various machine-learning techniques such as multivariable logistic regression, linear discriminant analysis, K-nearest neighbors, support vector machine, random forest, and decision tree. Each of these models was trained independently on the same dataset, which was split into training and test sets to ensure that the training data is independent.\n\nThe training process involved several steps. First, the controls in the dataset were randomly subsampled to achieve a case-control ratio of 1:3. This subsampled dataset was then randomly split into a training set containing 70% of the data and a test set containing 30% of the data. Each model was built based on the training set, and their performance was evaluated on the test set. This approach ensures that the training data for each model is independent, minimizing the risk of overfitting and providing a robust evaluation of the models' predictive capabilities.\n\nThe models were subsequently applied to the full dataset and a 2023 cohort to assess their generalizability and performance in different scenarios. The decision tree and random forest models, in particular, were used to make predictions on the full dataset and the 2023 cohort, demonstrating their ability to handle varying data distributions and maintain predictive accuracy.",
  "optimization/encoding": "The data used for the machine-learning algorithms consisted of various parameters extracted from the electronic laboratory information system. These parameters included age in days, sex, year of sampling, full blood count (FBC) parameters, C-reactive protein (CRP) levels, and blood culture results. The FBC parameters encompassed a wide range of measurements such as white cell count, neutrophil count, lymphocyte count, monocyte count, eosinophil count, basophil count, platelet count, red cell count, red cell distribution width, hemoglobin levels, hematocrit, mean platelet volume, mean corpuscular hemoglobin, mean corpuscular hemoglobin concentration, and mean corpuscular volume.\n\nTo ensure the data was suitable for training the models, several preprocessing steps were undertaken. Initially, the dataset was balanced by randomly subsampling the controls to achieve a case-control ratio of 1:3. This balanced dataset was then randomly split into a training set containing 70% of the data and a test set containing the remaining 30%. For models that required normalization, the training set was normalized to standardize the features. This normalization process was crucial for algorithms like linear discriminant analysis (LDA), K-nearest neighbors (KNN), and support vector machines (SVM) to ensure that all features contributed equally to the model's predictions.\n\nThe decision tree and random forest models, however, were built using the non-normalized training set. These models are capable of handling raw data without the need for normalization, making them robust for various types of input features. Additionally, univariable logistic regression was performed to identify variables with a p-value of 0.2 or less, which were then included in the subsequent multivariate analyses. This approach helped in minimizing the exclusion of potentially valuable variables while avoiding the inclusion of noise that could degrade model performance.\n\nIn summary, the data encoding and preprocessing involved balancing the dataset, splitting it into training and test sets, normalizing the data for specific models, and selecting relevant variables based on statistical significance. These steps ensured that the machine-learning algorithms were trained on high-quality, relevant data, enhancing their predictive accuracy and reliability.",
  "optimization/parameters": "In our study, the number of parameters used in the models varied depending on the specific model and the stage of analysis. Initially, we performed univariable logistic regression on the full dataset, which included a wide range of variables such as sex, age, C-reactive protein (CRP), various blood parameters (e.g., haemoglobin, haematocrit, mean corpuscular volume), and ratios like the neutrophil-to-lymphocyte ratio (NLR) and platelet-to-lymphocyte ratio (PLR). This initial analysis helped us identify variables with a p-value of 0.2 or less, which were then considered for inclusion in the multivariable models.\n\nFor the multivariable logistic regression model, we started with all variables that had a p-value of 0.2 or less in the univariable analysis. We then employed a top-down strategy to remove variables, ensuring that all remaining independent variables were significant (p-value < 0.05). This process resulted in a more refined set of parameters that were used in the final multivariable model.\n\nIn addition to the multivariable logistic regression, we built other models such as linear discriminant analysis (LDA), K-nearest neighbors (KNN), support vector machine (SVM) with a linear kernel, decision tree, and random forest. Each of these models was constructed using the same initial set of variables identified in the univariable analysis, but the final number of parameters could differ based on the model's specific requirements and the feature selection process.\n\nThe decision tree model, for instance, uses a set of concrete cut-off values, making it interpretable at the bedside. This model was selected for further testing on the full dataset and a 2023 cohort due to its ease of implementation in clinical practice. The random forest model, on the other hand, can handle a larger number of parameters and interactions between them, providing a more robust prediction but at the cost of interpretability.\n\nOverall, the selection of parameters was a careful process that involved initial screening through univariable analysis, followed by refinement through multivariable modeling and model-specific feature selection techniques. This approach ensured that the models were built on a solid foundation of statistically significant variables, minimizing noise and maximizing predictive power.",
  "optimization/features": "The study utilized a comprehensive set of features derived from full blood count (FBC) parameters and C-reactive protein (CRP) levels. Initially, a wide range of variables was considered, including age, sex, and various hematological parameters such as hemoglobin, hematocrit, mean corpuscular volume, and white cell count, among others.\n\nFeature selection was performed using univariable logistic regression. Variables with a p-value of 0.2 or less were included in the subsequent modeling steps. This approach helped in minimizing the exclusion of potentially valuable variables while avoiding the inclusion of noise. The selected features were then used to build multivariable logistic regression models, as well as other machine learning models like linear discriminant analysis, K-nearest neighbors, support vector machines, decision trees, and random forests.\n\nThe feature selection process was conducted using the training set only, ensuring that the test set remained independent and unbiased. This method helped in maintaining the integrity of the model evaluation and preventing data leakage, which could otherwise lead to overoptimistic performance estimates.",
  "optimization/fitting": "The study involved fitting several models to a dataset of infants aged 7 to 60 days who presented with suspected bloodstream infections. The dataset was initially balanced by randomly subsampling the controls to achieve a case-control ratio of 1:3. This subsampled dataset was then split into a training set (70% of the data) and a test set (30% of the data).\n\nSeveral models were built, including multivariable logistic regression, linear discriminant analysis (LDA), K-nearest neighbors (KNN), support vector machine (SVM) with a linear kernel, decision tree, and random forest. The number of parameters in these models varied, but generally, the number of parameters was not excessively large compared to the number of training points, which helped mitigate the risk of overfitting.\n\nTo further ensure that overfitting was not an issue, variables were selected based on their significance in univariable logistic regression (p-value of 0.2 or less). This approach helped in including only the most relevant variables, reducing the complexity of the models. Additionally, the models were validated on a separate test set, which provided an unbiased estimate of their performance. The area under the receiver operating characteristic curve (AUROC) and other performance metrics were calculated for each model, ensuring that they generalized well to unseen data.\n\nUnderfitting was addressed by including all variables with a p-value of 0.2 or less in the univariable regression model. This ensured that potentially important variables were not excluded prematurely. Furthermore, the use of multiple modeling techniques, including both linear and non-linear methods, helped capture different aspects of the data, reducing the risk of underfitting.\n\nThe decision tree and random forest models were particularly robust, as they are less prone to overfitting due to their ability to handle interactions between variables and capture non-linear relationships. The decision tree model, in particular, was chosen for its interpretability and ease of use in clinical settings. The random forest model, which aggregates multiple decision trees, further reduced the risk of overfitting by averaging the predictions of individual trees.\n\nIn summary, the study employed a combination of variable selection, model validation, and the use of diverse modeling techniques to balance the risk of overfitting and underfitting. The models were rigorously evaluated on a separate test set, ensuring their reliability and generalizability.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key approach was the use of a top-down strategy for variable selection in our multivariable logistic regression model. This involved initially including all variables with a p-value of 0.2 or less from the univariable analysis and then systematically removing variables to retain only those that were significant (p-value < 0.05). This method helped in building a parsimonious model that minimized the inclusion of noise while retaining important predictors.\n\nAdditionally, we utilized random subsampling to balance the case-control ratio to 1:3. This step was crucial in creating a more representative training dataset, which helped in reducing the risk of overfitting to the majority class (controls). The dataset was further split into training and test sets, with 70% of the data used for training and 30% reserved for testing. This split ensured that the model's performance could be evaluated on unseen data, providing a more accurate assessment of its generalizability.\n\nFor the linear discriminant analysis (LDA) model, we normalized the training set. Normalization is an essential preprocessing step that standardizes the features, making the model less sensitive to the scale of the input variables and thus reducing the risk of overfitting.\n\nIn the case of the K-nearest neighbors (KNN) and support vector machine (SVM) models, we also normalized the training set. Normalization is particularly important for these models as they are distance-based, and differences in feature scales can significantly affect their performance.\n\nFor the decision tree and random forest models, we did not normalize the data. Instead, these models inherently handle feature scaling differently. The decision tree model, in particular, is less prone to overfitting due to its ability to capture non-linear relationships and interactions between variables. The random forest model, which is an ensemble of decision trees, further reduces overfitting by averaging the predictions of multiple trees, thereby improving the model's stability and generalization.\n\nOverall, these techniques collectively helped in mitigating overfitting and ensuring that our models were robust and generalizable to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the publication. Specifically, the models were built using various machine learning techniques, including multivariable logistic regression, linear discriminant analysis, k-nearest neighbors, support vector machines, random forests, and decision trees. The training process involved subsampling the dataset to achieve a case-control ratio of 1:3, followed by splitting the data into training and test sets. The training set was used to build and optimize the models, with variables selected based on their significance in univariable logistic regression.\n\nThe optimization schedule involved normalizing the training set for certain models and using specific packages in R for model building. For instance, the linear discriminant analysis model was constructed using the MASS package, while the k-nearest neighbors and support vector machine models utilized the class and e1071 packages, respectively. The decision tree and random forest models were built using the rpart and randomForest packages.\n\nModel files and detailed optimization parameters are not explicitly provided in the publication, as the focus was on the methodology and results rather than the raw model files. However, the steps and parameters used for model optimization are thoroughly described, allowing for reproducibility. The data used in this study was extracted from the electronic laboratory information system at CHI Temple Street, Dublin, Ireland, and is available upon request, adhering to privacy regulations.\n\nThe publication is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction, provided appropriate credit is given to the original authors and the source. This license ensures that the methodology and findings can be accessed and utilized by other researchers for further studies or applications.",
  "model/interpretability": "The models developed in this study vary in their interpretability. The decision tree model stands out as the most transparent and interpretable among them. This model uses a set of concrete cut-off values for various parameters, making it feasible to interpret at the bedside. This transparency allows clinicians to understand the decision-making process of the model, which is crucial for integrating it into clinical practice.\n\nIn contrast, models like the random forest and support vector machine are more complex and can be considered black-box models. These models do not provide clear, interpretable rules but rather rely on complex interactions between multiple variables. While they can offer high predictive performance, their lack of transparency makes it difficult for clinicians to understand how predictions are made.\n\nThe decision tree model's interpretability is particularly beneficial in clinical settings where understanding the rationale behind a prediction is essential. This model can help clinicians make informed decisions by providing clear criteria that can be easily understood and applied. However, it is important to note that even the most interpretable models should be used as supportive tools rather than the sole basis for clinical decision-making. Clinical expertise and judgment remain paramount in patient care.",
  "model/output": "The models developed in this study are classification models. They are designed to predict the presence of positive blood cultures in infants, which is a binary outcome (positive or negative). Several different types of classification models were employed, including multivariable logistic regression, linear discriminant analysis, k-nearest neighbors, support vector machine, random forest, and decision tree. These models were trained and tested on datasets to evaluate their performance in classifying infants as either having a positive blood culture or not. The performance metrics provided, such as sensitivity, specificity, accuracy, and the area under the receiver operating characteristic curve (AUROC), further confirm that the models are used for classification purposes.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method involved several steps to ensure the robustness and generalizability of the models. Initially, the dataset was subsampled to achieve a case-control ratio of 1:3, and then split into a training set (70%) and a test set (30%). Various models, including multivariable logistic regression, linear discriminant analysis, K-nearest neighbors, support vector machine, random forest, and decision tree, were trained on the training set. These models were then evaluated on the test set to calculate key performance metrics such as sensitivity, specificity, negative predictive value (NPV), positive predictive value (PPV), and accuracy. The area under the receiver operating characteristic curve (AUROC) was also computed for each model.\n\nAdditionally, the decision tree and random forest models, which were trained on the subsampled dataset, were applied to the full dataset consisting of 2692 infants. This allowed for the assessment of their performance on a larger and more diverse population. The models were further evaluated on a separate 2023 cohort of 197 infants to test their predictive capabilities on new, unseen data. This cohort included infants who had a work-up for suspected bloodstream infection between January 1st, 2023, and September 27th, 2023. The performance metrics for this cohort were also calculated to provide insights into the models' real-world applicability.",
  "evaluation/measure": "In the evaluation of our models, several key performance metrics were reported to provide a comprehensive assessment of their predictive capabilities. These metrics include the Area Under the Receiver Operating Characteristic Curve (AUROC), accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). Each of these metrics offers unique insights into the models' performance.\n\nThe AUROC is a critical metric that evaluates the model's ability to distinguish between positive and negative cases across all possible classification thresholds. It provides a single scalar value that summarizes the trade-off between sensitivity and specificity.\n\nAccuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. It gives an overall sense of how often the model is correct.\n\nSensitivity, also known as the true positive rate, indicates the proportion of actual positives that are correctly identified by the model. It is particularly important in medical diagnostics where missing a positive case can have serious consequences.\n\nSpecificity, or the true negative rate, measures the proportion of actual negatives that are correctly identified. High specificity is crucial for minimizing false positives, which can lead to unnecessary treatments or interventions.\n\nThe PPV represents the probability that subjects with a positive screening test truly have the condition. A high PPV is desirable to ensure that positive predictions are reliable.\n\nThe NPV, on the other hand, indicates the probability that subjects with a negative screening test truly do not have the condition. A high NPV is essential for ruling out the condition with confidence.\n\nThese metrics were calculated for different datasets, including a subsampled dataset, the full dataset, and a 2023 cohort. The subsampled dataset had a case-control ratio of 1:3, ensuring a balanced representation for training and testing the models. The full dataset included all available cases, providing a real-world evaluation of the models' performance. The 2023 cohort offered a temporal validation, assessing how well the models generalize to new, unseen data.\n\nThe reported metrics are representative of standard practices in the literature, ensuring that our evaluation is comparable to other studies in the field. This comprehensive set of metrics allows for a thorough assessment of the models' strengths and weaknesses, providing valuable insights for their potential clinical application.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, we focused on developing and evaluating several machine learning models tailored to our specific dataset of infants aged 7 to 60 days with suspected bloodstream infections (BSI). The models we compared included multivariable logistic regression, linear discriminant analysis (LDA), K-nearest neighbors (KNN), support vector machine (SVM), random forest, and decision tree.\n\nTo ensure a comprehensive evaluation, we compared these models using a subsampled dataset with a case-control ratio of 1:3, the full dataset, and a 2023 cohort. We assessed their performance using metrics such as area under the receiver operating characteristic curve (AUROC), accuracy, sensitivity, specificity, negative predictive value (NPV), and positive predictive value (PPV). This approach allowed us to evaluate the models' predictive capabilities in different scenarios and datasets.\n\nRegarding simpler baselines, we did not explicitly compare our models to simpler baselines such as constant predictors or rule-based systems. However, the decision tree model, which uses concrete cut-off values, can be considered a simpler baseline compared to more complex models like random forest and SVM. The decision tree model was chosen for its interpretability and ease of use in clinical settings, making it a practical baseline for comparison.\n\nIn summary, while we did not compare our models to publicly available methods on benchmark datasets, we conducted a thorough evaluation using multiple datasets and performance metrics. The decision tree model served as a simpler baseline, providing a practical comparison within our study.",
  "evaluation/confidence": "The performance metrics presented in this study include confidence intervals for all parameters, providing a clear indication of the variability and reliability of the results. These intervals are crucial for understanding the precision of the estimates and for assessing the statistical significance of the findings.\n\nThe confidence intervals for the area under the receiver operating characteristic curve (AUROC), accuracy, sensitivity, specificity, negative predictive value (NPV), and positive predictive value (PPV) are provided for each model across different datasets. This comprehensive reporting allows for a thorough evaluation of the models' performance and their potential applicability in clinical settings.\n\nStatistical significance is a key consideration in determining whether the observed differences in performance metrics are due to actual differences in the models or due to random variation. The inclusion of confidence intervals helps in this assessment by providing a range within which the true value of the metric is likely to fall. If the confidence intervals of different models do not overlap, it suggests that the differences in performance are statistically significant.\n\nIn the subsampled dataset, the models generally showed similar performance metrics with overlapping confidence intervals, indicating that there may not be a statistically significant difference between them. However, in the full dataset and the 2023 cohort, the decision tree and random forest models demonstrated high NPVs, which are critical for clinical decision-making. The low PPVs observed in these models are consistent with findings from other studies, suggesting that while these models may not be perfect, they can still provide valuable support in clinical settings.\n\nOverall, the inclusion of confidence intervals in the performance metrics allows for a robust evaluation of the models' effectiveness and their potential to inform clinical decisions. The statistical significance of the results is supported by the provided intervals, which help in understanding the reliability and applicability of the models in different clinical scenarios.",
  "evaluation/availability": "The raw evaluation files used in this study are not publicly available. This decision was made to maintain patient privacy and comply with the European General Data Protection Regulation. However, anonymized data is available upon request. This approach ensures that the data can be used for further research while protecting the identities of the individuals involved. The study was conducted in accordance with ethical guidelines, including approval from the relevant research ethics committee and adherence to the Declaration of Helsinki."
}