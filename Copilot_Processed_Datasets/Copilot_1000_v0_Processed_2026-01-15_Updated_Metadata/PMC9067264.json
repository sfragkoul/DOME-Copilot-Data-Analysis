{
  "publication/title": "Prediction of large vessel occlusion for ischaemic stroke by using the machine learning model random forests.",
  "publication/authors": "Wang J, Zhang J, Gong X, Zhang W, Zhou Y, Lou M",
  "publication/journal": "Stroke and vascular neurology",
  "publication/year": "2022",
  "publication/pmid": "34702747",
  "publication/pmcid": "PMC9067264",
  "publication/doi": "10.1136/svn-2021-001096",
  "publication/tags": "- Acute ischaemic stroke\n- Large vessel occlusion\n- Machine learning\n- Random forests\n- Prehospital prediction\n- Stroke management\n- Predictive modeling\n- Medical data analysis\n- Stroke scales\n- Gradient Boosting Machine\n- Extreme Gradient Boosting\n- National Institutes of Health Stroke Scale\n- Reperfusion therapy\n- Stroke outcomes\n- Mechanical thrombectomy",
  "dataset/provenance": "The dataset used in this study originates from a multicentre prospective registry known as the Computer-based Online Database of Acute Stroke Patients for Stroke Management Quality Evaluation (CASE-II), registered under NCT04487340. This registry was initiated in 2016 with the aim of examining the current status of stroke care in China and developing strategies to improve it.\n\nThe dataset includes in-hospital medical documents of consecutive stroke patients collected through a specialized electronic data capture system. For this study, we retrospectively reviewed the CASE-II dataset from January 2016 to August 2021. The inclusion criteria involved consecutive acute ischaemic stroke (AIS) patients who underwent CT angiography (CTA) or time-of-flight MR angiography (TOF-MRA) and received reperfusion therapy within 8 hours from symptom onset. Additionally, patients had to have complete information of emergency for analysis. Those with poor image quality due to motion artefacts were excluded.\n\nThe dataset was split into training and test sets by time period. Patients from January 2016 to January 2021 were included in the training set, totaling 15,365 patients. The test set included patients from January 2021 to August 2021, comprising 4,215 patients. The dataset encompasses demographic, clinical, laboratory, and imaging data at admission, including age, gender, prior antiplatelet therapy, prior anticoagulant therapy, risk factors such as smoking, hypertension, atrial fibrillation, diabetes mellitus, hyperlipidaemia, hyperhomocysteinaemia, coronary heart disease, congestive heart failure, history of stroke/transient ischaemic attack (TIA), and family history of cardiovascular disease. Blood pressure at admission was also recorded.\n\nThis dataset has not been used in previous papers by the community.",
  "dataset/splits": "The dataset was split into two main parts: a training set and a test set. The training set consisted of 15,365 patients, while the test set included 4,215 patients. The training set encompassed data from January 2016 to January 2021, whereas the test set covered data from January 2021 to August 2021. Additionally, during the model derivation process, the training set was further divided into ten overlapping training datasets and ten unique validation datasets through a ten-fold cross-validation approach. This method involved splitting the training dataset into ten mutually exclusive parts, using nine parts for training and one part for validation in each iteration.",
  "dataset/redundancy": "The dataset used in this study was derived from a multicentre prospective registry known as the Computer-based Online Database of Acute Stroke Patients for Stroke Management Quality Evaluation (CASE-II). The dataset included consecutive acute ischaemic stroke (AIS) patients who underwent CT angiography (CTA) or time-of-flight MR angiography (TOF-MRA) and received reperfusion therapy within 8 hours from symptom onset. The dataset was split into training and test sets based on time periods. Specifically, patients from January 2016 to January 2021 were included in the training set, while patients from January 2021 to August 2021 were included in the test set. This temporal split ensured that the training and test sets were independent, reducing the risk of data leakage and ensuring that the model's performance could be evaluated on unseen data.\n\nThe training set consisted of 15,365 patients, while the test set included 4,215 patients. The distribution of the data in the test set differed from the training set in several ways. Patients in the test set were less likely to be female, have atrial fibrillation (AF), coronary heart disease, a family history of cardiovascular disease, hyperlipidaemia, hyperhomocysteinaemia, or be smokers. Additionally, the test set had a higher proportion of patients with large vessel occlusion (LVO) and presented with a lower baseline National Institutes of Health Stroke Scale (NIHSS) score at admission.\n\nThis approach to dataset splitting and the resulting differences in distribution provide a robust framework for evaluating the model's generalizability and performance in real-world scenarios. The temporal split ensures that the model is tested on data that was not used during training, mimicking the conditions under which the model would be deployed in clinical practice. The differences in distribution between the training and test sets also highlight the model's ability to handle variations in patient characteristics, which is crucial for its practical application.",
  "dataset/availability": "The data used in this study is not publicly available. However, it can be obtained upon reasonable request. The data was collected from a multicentre prospective registry known as the Computer-based Online Database of Acute Stroke Patients for Stroke Management Quality Evaluation (CASE-II). This registry was initiated in 2016 and is designed to examine the current status of stroke care in China. The data includes in-hospital medical documents of consecutive stroke patients collected through a special electronic data capture system.\n\nThe study population consisted of acute ischemic stroke (AIS) patients who underwent CT angiography (CTA) or time-of-flight MR angiography (TOF-MRA) and received reperfusion therapy within 8 hours from symptom onset. Patients with poor image quality due to motion artifacts were excluded. The dataset was split into training and test sets by time period, with patients from January 2016 to January 2021 included in the training set, and patients from January 2021 to August 2021 included in the test set.\n\nThe data availability is enforced through a request process, ensuring that the data is shared responsibly and in accordance with ethical guidelines. The study was approved by the human ethics committee of the Second Affiliated Hospital of Zhejiang University, School of Medicine, and the clinical investigation was conducted according to the principles expressed in the Declaration of Helsinki. This ensures that the data is handled with the necessary ethical considerations and that the privacy and rights of the patients are protected.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is ensemble learning, specifically focusing on tree-based methods. We employed eight common machine learning models: random forests (RFs), logistic regression, Extreme Gradient Boosting (XGBoost), K-Nearest Neighbour, Ada Boosting, Gradient Boosting Machine (GBM), LightGBM, and artificial neural network (ANN). These models were developed using the Scikit-Learn package in Python software.\n\nThe algorithms used are not new; they are well-established in the field of machine learning. The choice to use these algorithms in a medical context, rather than a machine-learning journal, is driven by the specific application and the need to address a critical healthcare challenge. Our study aims to predict large vessel occlusion (LVO) in acute ischemic stroke patients using prehospital accessible data. The focus is on the practical application of these models to improve stroke management and outcomes, rather than the development of new machine-learning algorithms.\n\nThe optimization of model hyperparameters was performed using a grid search algorithm. This process involved setting the area under the curve (AUC) of the receiver operating characteristic (ROC) as the scoring metric. The model with the highest F1 score was selected as the optimal model. This approach ensures that the models are fine-tuned to provide the best possible predictive performance for identifying LVO in stroke patients.",
  "optimization/meta": "The model developed in this study does not use data from other machine-learning algorithms as input. Instead, it directly utilizes prehospital accessible data, including demographics, NIHSS items, medical history, and vascular risk factors.\n\nThe study involved the development and comparison of eight different machine learning models: random forests (RFs), logistic regression, Extreme Gradient Boosting (XGBoost), K-Nearest Neighbour, Ada Boosting, Gradient Boosting Machine (GBM), LightGBM, and artificial neural network (ANN). These models were developed using the Scikit-Learn package in Python software.\n\nThe process of model derivation and internal evaluation was conducted using ten-fold cross-validation. This involved dividing the training dataset into ten mutually exclusive parts, with nine parts used for training and one part used for validation. This process was repeated ten times to generate ten different but overlapping training datasets and ten unique validation datasets. This approach ensures that the training data is independent for each fold, providing a robust evaluation of the model's performance.\n\nThe final prediction model chosen was the random forests (RF) model, which presented higher AUC and specificity compared to the other models. The performance of the RF model was also compared with previously established prehospital prediction scales, demonstrating superior diagnostic parameters.",
  "optimization/encoding": "For the machine-learning algorithm, the data encoding and preprocessing involved several steps. Initially, we selected features based on published literature, pathophysiological considerations, and the availability of items in the prehospital setting. This included 15 NIHSS items, age, gender, prior antiplatelet therapy, prior anticoagulant therapy, various risk factors, and blood pressure at admission.\n\nStatistical analysis was then performed to identify features with statistical significance, defined as p<0.05. These significant features were retained for model development. The final selection of parameters was performed using the Scikit-Learn package in Python software.\n\nThe dataset was split into training and test sets by time period. Patients from January 2016 to January 2021 were included in the training set, while patients from January 2021 to August 2021 were included in the test set. Ten-fold cross-validation was employed for model derivation and internal evaluation. This involved dividing the training dataset into ten mutually exclusive parts, using nine parts for training and one part for validation, repeating this process ten times to generate ten different but overlapping training datasets and ten unique validation datasets.\n\nDuring the training step, model hyperparameters were optimized using a grid search algorithm. The area under the curve (AUC) of the receiver operating characteristic (ROC) was set as the score for this optimization process. The model with the highest F1 score was selected. The training of non-linear models incorporated linear correlations between input and output parameters, as well as intricate associations among the input parameters. The models were developed using the Scikit-Learn package in Python software.",
  "optimization/parameters": "In our study, we initially considered a comprehensive set of parameters for our model. These included 15 NIHSS items, age, gender, prior antiplatelet therapy, prior anticoagulant therapy, various risk factors such as smoking, hypertension, atrial fibrillation, diabetes mellitus, hyperlipidemia, hyperhomocysteinemia, coronary heart disease, congestive heart failure, history of stroke/transient ischemic attack, and family history of cardiovascular disease, as well as blood pressure at admission.\n\nThe selection of these parameters was based on published literature, pathophysiological considerations, and the availability and convenience of these items in the prehospital setting. To refine our model, we performed statistical analysis to identify features with statistical significance, defined as p<0.05. These significant features were then chosen as the final model variables. The selection of final parameters was performed using the Scikit-Learn package in Python software. This rigorous process ensured that only the most relevant and significant parameters were included in our model, enhancing its predictive accuracy and reliability.",
  "optimization/features": "The study utilized a comprehensive set of features as input for the predictive models. Initially, 15 NIHSS items were considered, along with demographic information such as age and gender. Additionally, prior medical therapies like antiplatelet and anticoagulant treatments were included. Risk factors such as smoking, hypertension, atrial fibrillation, diabetes mellitus, hyperlipidaemia, hyperhomocysteinaemia, coronary heart disease, congestive heart failure, history of stroke/transient ischemic attack, and family history of cardiovascular disease were also taken into account. Blood pressure at admission was another crucial feature.\n\nFeature selection was performed to identify statistically significant variables. This process was conducted using the training set only, ensuring that the model's performance on the test set remained unbiased. The final parameters were selected using the Scikit-Learn package in Python software, which helped in identifying the most relevant features for predicting large vessel occlusion (LVO). The number of features used as input is not explicitly stated, but it involved a thorough selection process to retain only the most significant variables.",
  "optimization/fitting": "In our study, we employed a robust approach to ensure that our models were neither overfitting nor underfitting the data. We utilized eight common machine learning models, including random forests (RFs), logistic regression, Extreme Gradient Boosting (XGBoost), K-Nearest Neighbour, Ada Boosting, Gradient Boosting Machine (GBM), LightGBM, and artificial neural networks (ANN). The number of parameters in these models was indeed larger than the number of training points, which could potentially lead to overfitting. To mitigate this risk, we implemented a ten-fold cross-validation process. This involved dividing the training dataset into ten mutually exclusive parts, using nine parts for training and one part for validation in each fold. This process was repeated ten times, ensuring that each part of the data was used for validation exactly once. By doing so, we were able to assess the model's performance on different subsets of the data, reducing the likelihood of overfitting.\n\nAdditionally, we optimized model hyperparameters using a grid search algorithm. The hyperparameters were selected based on the model's performance on the validation set, specifically focusing on achieving the highest F1 score. During the hyperparameter tuning process, we set the area under the curve (AUC) of the receiver operating characteristic (ROC) as the scoring metric. This approach helped in finding the best hyperparameters that generalized well to unseen data, further reducing the risk of overfitting.\n\nTo address the potential issue of underfitting, we ensured that our models were complex enough to capture the intricate associations among the input parameters. The models, particularly the non-linear ones like RFs, GBM, and XGBoost, were capable of incorporating both linear and non-linear relationships between the input and output parameters. This allowed the models to learn from the data more effectively, reducing the risk of underfitting. Furthermore, the models were evaluated on a separate test set that was not used during the training or validation process. This test set included patients from a different time period, ensuring that the models' performance was assessed on truly unseen data. The results showed that the models, particularly the RF model, performed well on the test set, indicating that they were neither overfitting nor underfitting the training data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the primary methods used was ten-fold cross-validation. This process involved dividing the training dataset into ten mutually exclusive parts, using nine parts for training and one part for validation. This cycle was repeated ten times, each time with a different part used as the validation set. This approach helped to ensure that our models were not overfitting to any specific subset of the data.\n\nAdditionally, we optimized model hyperparameters using a grid search algorithm. This method systematically worked through multiple combinations of hyperparameter values to determine the best configuration. The selection criterion for the optimal hyperparameters was the model with the highest F1 score, which balances both precision and recall. During the grid search, we also monitored the area under the curve (AUC) of the receiver operating characteristic (ROC) to further evaluate model performance.\n\nWe also chose models that are inherently robust to overfitting, such as Random Forests (RF), Gradient Boosting Machine (GBM), and Extreme Gradient Boosting (XGBoost). These models aggregate the predictions of multiple decision trees, which helps to reduce the variance and improve generalization to unseen data. Among these, the RF model was ultimately selected as the final prediction model due to its superior performance in terms of AUC, sensitivity, specificity, and accuracy.\n\nFurthermore, feature selection was performed to retain only the most relevant variables for model training. This step helped to reduce the complexity of the models and prevent them from learning noise in the data. The final set of features was chosen based on statistical significance and their importance in predicting the outcome, as measured by Gini importance.\n\nIn summary, our approach to preventing overfitting included cross-validation, hyperparameter optimization, the use of ensemble models, and careful feature selection. These techniques collectively contributed to the development of a robust and generalizable predictive model for identifying large vessel occlusion in acute ischemic stroke patients.",
  "optimization/config": "The code used to generate the results presented in this study is available from the corresponding author upon request. This includes the hyper-parameter configurations, optimization schedule, and model files used during the development process. The specific optimization parameters and their values are not explicitly detailed in the publication but are encapsulated within the codebase. The availability of the code ensures that other researchers can replicate the study's findings and build upon the work. However, the licensing terms for the code are not specified, so interested parties should contact the corresponding author for details on usage permissions.",
  "model/interpretability": "The model developed in our study is not entirely a black box. While machine learning models, particularly complex ones like random forests, can be seen as black boxes due to their intricate structures, we employed techniques to enhance interpretability.\n\nOne key method used was the calculation of Gini importance for each feature. Gini importance measures the contribution of each feature to the model's predictions. Features with higher Gini importance values are more influential in the model's decision-making process. For instance, the total NIHSS score, gaze deviation, level of consciousness, and motor left leg were identified as significant contributors to the identification of large vessel occlusion (LVO). This provides insight into which factors are most important for the model's predictions.\n\nAdditionally, the model's performance was compared with previously established prehospital prediction scales, such as NIHSS, mNIHSS, and sNIHSS. This comparison helps in understanding how the model's predictions align with or differ from existing clinical scales, thereby adding a layer of transparency.\n\nFurthermore, the model's development process, including feature selection and hyperparameter optimization, was documented. This transparency allows other researchers to replicate the study and understand the rationale behind the model's design.\n\nIn summary, while the random forest model itself is complex, the use of feature importance metrics and comparisons with established scales provide a degree of interpretability, making it more transparent than a typical black-box model.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the presence of large vessel occlusion (LVO) in patients with acute ischemic stroke (AIS). The model uses prehospital accessible data, including demographics, NIHSS items, medical history, and vascular risk factors, to classify patients into LVO and non-LVO groups. The performance of the model was evaluated using metrics such as area under the curve (AUC), sensitivity, specificity, and accuracy, which are typical for classification tasks. The random forest (RF) model, which was selected as the final prediction model, demonstrated higher AUC and accuracy compared to previously established prehospital prediction scales, indicating its effectiveness in classifying patients with LVO.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code used to generate the results presented in this study is available from the corresponding author upon request. This code includes the implementation of the machine learning models, data preprocessing steps, and the evaluation metrics used. However, there is no mention of an executable, web server, virtual machine, or container instance being released for public use. The code is not openly available on a public repository or platform. The corresponding author can be contacted for access to the code, and the specifics of the licensing terms are not provided.",
  "evaluation/method": "The evaluation method employed in this study was rigorous and multifaceted. We utilized a ten-fold cross-validation process to derive and internally validate the model. This involved dividing the training dataset into ten mutually exclusive parts, using nine parts for training and one part for validation, and repeating this process ten times to generate ten different but overlapping training datasets and ten unique validation datasets. This approach ensured that the model was robust and generalizable.\n\nDuring the training step, we optimized model hyperparameters using a grid search algorithm. The area under the curve (AUC) of the receiver operating characteristic (ROC) was set as the score during this process, and the model with the highest F1 score was selected.\n\nThe final evaluation was conducted on an independent test set, which included patients from a different time period than the training set. This test set comprised 4,215 patients, providing a real-world assessment of the model's performance. We evaluated each model in the test set and compared its predictive power with previously established scales. The performance metrics included AUC, sensitivity, specificity, and accuracy.\n\nThe random forests (RF) model was chosen as the final prediction model due to its superior performance in terms of AUC and specificity compared to other models like Gradient Boosting Machine (GBM) and Extreme Gradient Boosting (XGBoost). The RF model's performance was also compared with various prehospital prediction scales, demonstrating its superiority in both AUC and accuracy.",
  "evaluation/measure": "In our study, we evaluated the performance of various models and clinical scales using several key metrics to predict large vessel occlusion (LVO) in acute ischemic stroke (AIS) patients. The primary metrics reported include the area under the curve (AUC) of the receiver operating characteristic (ROC), sensitivity (SEN), specificity (SPE), and accuracy.\n\nThe AUC provides a measure of the model's ability to distinguish between patients with and without LVO, with higher values indicating better performance. Sensitivity refers to the proportion of true positive cases correctly identified by the model, while specificity measures the proportion of true negative cases correctly identified. Accuracy represents the overall proportion of correctly predicted cases, combining both true positives and true negatives.\n\nThese metrics are widely used in the literature for evaluating predictive models in medical research, ensuring that our results are comparable with other studies. By reporting AUC, sensitivity, specificity, and accuracy, we provide a comprehensive assessment of the models' performance, highlighting their strengths and potential areas for improvement.\n\nIn addition to these metrics, we also determined the optimal cut-off points for each scale using the maximal Youden Index, which balances sensitivity and specificity. This approach ensures that the reported metrics are based on the most effective thresholds for predicting LVO.\n\nOverall, the set of metrics reported in our study is representative of the standards in the field, allowing for meaningful comparisons with other predictive models and clinical scales.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of our machine learning model with various established prehospital prediction scales to evaluate its performance in identifying large vessel occlusion (LVO) in acute ischemic stroke (AIS) patients. We chose eight common machine learning models, including random forests (RF), logistic regression, Extreme Gradient Boosting (XGBoost), K-Nearest Neighbour, Ada Boosting, Gradient Boosting Machine (GBM), LightGBM, and artificial neural network (ANN). These models were developed using the Scikit-Learn package in Python software.\n\nDuring the model evaluation phase, we specifically compared the performance of RF, GBM, and XGBoost, as these models presented higher area under the curve (AUC) values than the other five models. Among them, the RF model demonstrated higher specificity compared to GBM and XGBoost, leading us to select the RF model as the final prediction model.\n\nWe then compared the RF model's performance with several previously established prehospital prediction scales, including 3I-SS, G-FAST, CPSSS, FAST, FAST-ED, FPSS, LAMS, PASS, RACE, ROSIER, VAN, NIHSS, mNIHSS, sNIHSS, and sNIHSS-EMS. The comparison metrics included AUC, sensitivity, specificity, and accuracy. The results, as shown in Table 3, indicated that the RF model outperformed all the other scales in terms of AUC and accuracy. Specifically, the RF model achieved an AUC of 0.831 and an accuracy of 0.772, which were superior to the next best performing scales, mNIHSS and sNIHSS-EMS, both with an AUC of 0.809.\n\nAdditionally, we found that the accuracy of our RF model was improved by 6.4% compared to the NIHSS scale. This comparison highlights the effectiveness of our machine learning approach in predicting LVO, demonstrating its potential to enhance prehospital stroke management and improve patient outcomes.",
  "evaluation/confidence": "The evaluation of our model's performance included several key metrics, each accompanied by confidence intervals to provide a measure of uncertainty. The area under the curve (AUC) for the receiver operating characteristic (ROC) is reported with 95% confidence intervals, offering insight into the reliability of the AUC estimates. For instance, the AUC for our Random Forest (RF) model is 0.831 with a 95% confidence interval of 0.819 to 0.843. This interval indicates a high level of confidence in the model's discriminative ability.\n\nStatistical significance was assessed using appropriate tests. For continuous variables, differences between groups were estimated using the t-test or Mann-Whitney U test, while categorical data were analyzed using the Pearson \u03c72 test. The significance level was set at p<0.05, ensuring that the observed differences were unlikely to have occurred by chance.\n\nComparisons between our model and previously established scales, such as NIHSS and others, were made using these statistical methods. The results showed that our RF model had a significantly higher AUC and accuracy compared to these scales. For example, the accuracy of our model was improved by 6.4% compared to NIHSS, demonstrating its superior performance.\n\nThe ROC-derived optimal cut-off was determined at the maximal Youden Index, which balances sensitivity and specificity. This approach ensures that the model's performance is optimized for practical use. Sensitivity, specificity, and accuracy were calculated for the prediction of large vessel occlusion (LVO), providing a comprehensive evaluation of the model's effectiveness.\n\nIn summary, the performance metrics of our model are robust, with confidence intervals providing a clear understanding of the results' reliability. The statistical significance of our findings supports the claim that our method is superior to other prehospital prediction scales.",
  "evaluation/availability": "The raw evaluation files used in this study are not publicly available. The code used to generate the results shown in this study is available from the corresponding author upon request. This approach ensures that other researchers can replicate the findings, but it does not provide direct access to the raw evaluation data. The decision to make the code available rather than the raw data may be due to privacy concerns or the need to maintain control over the data's use."
}