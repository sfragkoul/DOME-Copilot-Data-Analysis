{
  "publication/title": "Multimodal Metaverse Healthcare: A Collaborative Representation and Adaptive Fusion Approach for Generative Artificial-Intelligence-Driven Diagnosis.",
  "publication/authors": "Lv J, Slowik A, Rani S, Kim BG, Chen CM, Kumari S, Li K, Lyu X, Jiang H",
  "publication/journal": "Research (Washington, D.C.)",
  "publication/year": "2025",
  "publication/pmid": "40078668",
  "publication/pmcid": "PMC11899152",
  "publication/doi": "10.34133/research.0616",
  "publication/tags": "- Metaverse\n- Multimodal Health Evaluation\n- Virtual Healthcare\n- AI-Powered Diagnostics\n- Adaptive Fusion\n- Cross-Modal Information Sharing\n- Generative AI\n- Health Status Monitoring\n- Machine Learning\n- Neural Networks\n- Multimodal Data\n- Metaverse Healthcare\n- Health Prediction\n- AI Interpretability\n- Clinical Data Analysis\n- Multimodal Fusion\n- Metaverse Environment\n- Virtual Patient Data\n- Health Assessment\n- AI in Healthcare",
  "dataset/provenance": "The datasets utilized in our study were carefully selected to evaluate the effectiveness of the MMLMH framework in multimodal metaverse healthcare provision. The first dataset is the Multimodal Corpus of Sentiment Intensity (CMU-MOSI). This dataset comprises 2,199 video segments sourced from 93 YouTube movie review videos. It is designed for multimodal sentiment analysis, focusing on the integration of text, audio, and visual modalities. The dataset has been previously used in the community for sentiment analysis tasks, but our study augmented it with synthetic virtual environment data to simulate virtual world healthcare consultations.\n\nThe second dataset is the Medical Information Mart for Intensive Care III (MIMIC-III). This is an extensive, open-access database containing health-related information of over 40,000 subjects who spent time in critical care units. The data spans from 2001 to 2012 and includes anonymized patient information. The MIMIC-III dataset has been widely used in the medical research community for various studies, particularly those involving intensive care unit data. Both datasets were modified to fit the requirements of the MMLMH framework, incorporating synthetic metaverse environment features to enhance their applicability in virtual healthcare settings.",
  "dataset/splits": "Not applicable.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The datasets generated and analyzed during the current study are not publicly released in a forum. They are available from the corresponding authors upon reasonable request. This approach ensures that the data is shared responsibly and ethically, maintaining the privacy and security of the information. There is no specific license mentioned for the data sharing, but it is implied that the data will be shared under reasonable terms agreed upon between the requester and the corresponding authors. This method of data availability ensures that the data is used appropriately and that the researchers maintain control over how the data is accessed and utilized.",
  "optimization/algorithm": "The optimization algorithm employed in our framework is based on well-established machine-learning techniques, specifically utilizing deep learning methods implemented with PyTorch. The core of our approach involves a multimodal learning architecture designed to handle and integrate various types of data, including text, audio, and visual inputs.\n\nThe machine-learning algorithm class used falls under the category of multimodal deep learning frameworks. This class of algorithms is particularly suited for tasks that require the fusion of information from multiple modalities, which is crucial in virtual healthcare environments where data can come from diverse sources.\n\nThe algorithm itself is not entirely new but represents an innovative application and extension of existing multimodal learning techniques. The novelty lies in its adaptation to the specific challenges and requirements of the metaverse healthcare setting. This includes the integration of synthetic data, the handling of real-time data streams, and the need for adaptive fusion methods that can dynamically adjust to the most relevant data streams during health evaluations.\n\nThe decision to publish this work in a healthcare-focused journal rather than a machine-learning journal is driven by the application domain. The primary focus of our research is on improving healthcare outcomes in virtual environments, leveraging advanced machine-learning techniques as a means to achieve this goal. The metaverse healthcare context provides unique challenges and opportunities that are best addressed within the healthcare research community, where the impact on patient care and clinical practice can be more directly assessed and appreciated.",
  "optimization/meta": "The MMLMH framework does not operate as a traditional meta-predictor that solely relies on the outputs of other machine-learning algorithms as its input. Instead, it integrates multimodal data directly from various sources such as text, audio, and visual inputs. These modalities are processed and fused adaptively to capture complex clinical relationships and ensure diagnostic accuracy.\n\nThe framework employs a multilayer perceptron (MLP) for health prediction, which models the intricate mapping of combined features toward respective health targets in a metaverse. For classification tasks, such as predicting specific health conditions through virtual consultations, a softmax activation function is applied in the final layer. For regression tasks, like estimating healthy minutes or risk scores, a linear activation function is used.\n\nThe training process involves task-specific loss functions. For classification, cross-entropy loss is utilized, which includes an L2 regularization term to prevent overfitting. For regression tasks, mean squared error loss is employed. These loss functions ensure that the model learns effectively from the multimodal data.\n\nAdditionally, the framework incorporates a virtual health avatar and generative AI models to enhance interpretability. The avatar illustrates expected health conditions based on predicted outcomes and virtual environment context. Generative AI provides natural language explanations, improving transparency and trust in AI-powered diagnostics within the metaverse healthcare delivery ecosystem.\n\nThe training data for the MMLMH framework is designed to be independent, ensuring that the model learns from diverse and uncorrelated samples. This independence is crucial for maintaining the model's generalization capabilities and preventing overfitting. The framework's ability to handle varying levels of performance consistency and adapt to complex virtual environments is rigorously evaluated through a comprehensive 12-month prospective evaluation protocol. This protocol includes regular assessments of diagnostic accuracy across demographically diverse patient populations and measurements of the system's resilience to variations in data quality and environmental complexity.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure effective learning from multimodal health data in the metaverse. Initially, L2 normalization was applied to the features of text, audio, and visual modalities. This step helped in standardizing the data, making it easier for the subsequent encoding processes.\n\nEach modality was then processed through individual transformer networks. These networks were designed to capture contextual information specific to each data stream. The outputs from these transformers were averaged, resulting in compact encodings for text, audio, and visual modalities. These encodings were dimensionally consistent, with the dimensions determined by the last feed-forward layer of the transformer network.\n\nTo handle the complex relationships between different modalities, we developed two types of encoders: modality-shared and modality-specific. The modality-shared encoder was designed to capture common health-related information across text, audio, and visual modalities. This encoder incorporated explanatory variables in a latent construct, allowing the model to benefit from the relationships between different types of data.\n\nIn contrast, the modality-specific encoders were built to maintain the distinct features of each modality. These encoders projected the features into different latent spaces, ensuring that language-specific and modality-specific information was preserved. This approach allowed for a thorough analysis of numerous health data streams from a virtual environment in a multimodal perspective.\n\nThe feature dimensions for text, audio, and visual modalities were established through ablation studies. These studies helped in determining the optimal dimensions that balanced diagnostic accuracy and computational overhead. The sequence lengths for text, audio, and visual data were also carefully selected to cover a significant portion of patient-provider interactions while maintaining real-time processing capabilities.\n\nOverall, our encoding and preprocessing steps were designed to capture both shared and unique information across modalities, ensuring that the machine-learning algorithm could effectively learn from the complex multimodal health data in the metaverse.",
  "optimization/parameters": "The model employs several key parameters that were meticulously selected through a systematic optimization process. The feature dimensions for text, audio, and visual data were established at 768, 128, and 2,048, respectively. These dimensions were determined through ablation studies, which revealed that increasing beyond these values yielded diminishing returns in diagnostic accuracy while also monitoring computational overhead.\n\nThe collaboration loss weights for intrasample, intersample, and reconstruction were set at 0.7, 0.8, and 0.5, respectively. These weights were derived from a grid search optimization process evaluated on a validation set comprising diverse virtual healthcare scenarios. This process ensured an optimal balance between maintaining modality-specific clinical features and promoting cross-modal information sharing.\n\nThe central moment discrepancy (CMD) order was set to 5 after examining performance across orders 3 to 7. Order 5 provided the best trade-off between computational complexity and feature distribution matching accuracy.\n\nArchitecture-specific parameters, such as the hidden layer size, were iteratively refined through cross-validation experiments. A hidden layer size of 256 neurons proved optimal for capturing complex clinical relationships while preventing overfitting. The learning rate was set to 1 \u00d7 10\u22124, and L2 regularization was set to 1 \u00d7 10\u22125, both determined through learning curve analysis to ensure stable convergence and model generalization.\n\nTraining episodes were set to 100 after observing convergence patterns across multiple initialization seeds, with early stopping implemented based on validation performance. A dropout rate of 0.2 was established through systematic testing across rates from 0.1 to 0.5, optimizing for model robustness in varying virtual environment conditions. The AdamW optimizer was selected for its superior performance in handling the varying scales of multimodal medical data, particularly in synthetic data integration scenarios.",
  "optimization/features": "The input features for our framework are designed to capture the essential characteristics of multimodal health data in virtual healthcare environments. We utilize three primary modalities: text, audio, and visual, each with specific feature dimensions. The text modality has a feature dimension of 768, the audio modality has a feature dimension of 128, and the visual modality has a feature dimension of 2,048. These dimensions were established through ablation studies, which helped us identify the optimal balance between diagnostic accuracy and computational overhead.\n\nFeature selection was performed to ensure that the most relevant features were included in our model. This process involved iterative refinement through cross-validation experiments across different patient cohorts. The selected features were chosen based on their ability to capture complex clinical relationships while preventing overfitting. The final set of features includes a hidden layer size of 256 neurons, which proved optimal for maintaining the model's performance and generalization.\n\nThe feature selection process was conducted using the training set only, ensuring that the model's performance on the validation and test sets was not biased. This approach helped us maintain the integrity of our evaluation metrics and ensure that our model's performance was generalizable to new, unseen data. The sequence lengths for each modality were also determined through analysis of typical virtual consultation patterns, ensuring coverage of 98.5% of patient-provider interactions.",
  "optimization/fitting": "The MMLMH framework was designed with a focus on balancing model complexity and performance to avoid both overfitting and underfitting. The initial model size was significantly reduced from 380 GB to 125 GB through quantization and pruning techniques, ensuring that the model remained efficient while maintaining diagnostic accuracy. This optimization process was crucial in managing the computational overhead associated with processing complex multimodal health data streams in real-time.\n\nTo address overfitting, several strategies were employed. A dropout rate of 0.2 was established through systematic testing across rates from 0.1 to 0.5, which helped in optimizing model robustness. Additionally, L2 regularization with a value of 1 \u00d7 10\u22125 was implemented to prevent the model from becoming too complex and fitting the noise in the training data. The learning rate was set to 1 \u00d7 10\u22124, which was determined through learning curve analysis to ensure stable convergence and generalization.\n\nEarly stopping based on validation performance was also implemented, which helped in preventing the model from overfitting to the training data by halting the training process when performance on the validation set started to degrade. The use of a hidden layer size of 256 neurons, determined through cross-validation experiments, provided an optimal balance between capturing complex clinical relationships and preventing overfitting.\n\nTo rule out underfitting, the model's architecture and hyperparameters were iteratively refined through cross-validation experiments. The sequence length parameters (text: 512; audio: 1,000; visual: 100) were determined by analyzing typical virtual consultation patterns, ensuring coverage of 98.5% of patient-provider interactions. The collaboration loss weights (intrasample: 0.7; intersample: 0.8; reconstruction: 0.5) were optimized through a grid search process, balancing modality-specific clinical features and promoting cross-modal information sharing.\n\nThe central moment discrepancy (CMD) order of 5 was selected after examining performance across orders 3 to 7, providing the best trade-off between computational complexity and feature distribution matching accuracy. The AdamW optimizer was chosen for its superior performance in handling the varying scales of multimodal medical data, particularly in synthetic data integration scenarios.\n\nOverall, these strategies ensured that the MMLMH framework achieved a robust balance between model complexity and performance, effectively avoiding both overfitting and underfitting.",
  "optimization/regularization": "In our study, we implemented several regularization techniques to prevent overfitting and ensure the robustness of our model. One key technique employed was dropout, with a rate of 0.2. This involved randomly setting a fraction of the input units to zero at each update during training time, which helped to prevent the model from becoming too reliant on any single feature.\n\nAdditionally, we used L2 regularization with a value of 1 \u00d7 10\u22125. This technique adds a penalty equal to the square of the magnitude of coefficients to the loss function, encouraging the model to keep the coefficients small and thus reducing the complexity of the model.\n\nWe also utilized a reconstruction loss with a regularization parameter to ensure that the decoder could reconstruct the original features from the learned representations. This helped in preserving the original health information while training for more complex representations.\n\nFurthermore, the use of the AdamW optimizer, which includes weight decay, helped in regularizing the model by decaying the weights during the optimization process. This approach was particularly effective in handling the varying scales of multimodal medical data, especially in synthetic data integration scenarios.\n\nThese regularization methods collectively contributed to maintaining the model's generalization capabilities and preventing overfitting, ensuring reliable and accurate health assessments in virtual healthcare applications.",
  "optimization/config": "The hyperparameter configurations and optimization schedule for the MMLMH framework are thoroughly detailed in the publication. Key hyperparameters, such as sequence lengths for text, audio, and visual data, feature dimensions, loss weights, and other architectural parameters, are presented in a dedicated table. This table provides a clear overview of the settings used, ensuring reproducibility.\n\nThe optimization process involved a grid search approach to tune hyperparameters for optimal performance on validation sets. This methodical approach ensures that the reported configurations are well-optimized for the tasks at hand.\n\nRegarding model files and optimization parameters, while the specific files are not directly available in the publication, the detailed descriptions and tables provide all necessary information to replicate the model and its optimization. The datasets generated and analyzed during the study are available from the corresponding authors upon reasonable request, which includes the data needed to reproduce the optimization process.\n\nThe publication adheres to standard practices for sharing scientific work, ensuring that the community can build upon the findings. The authors declare no competing interests, further reinforcing the transparency and reliability of the reported configurations and optimization parameters.",
  "model/interpretability": "The MMLMH framework is designed with a strong emphasis on interpretability, making it transparent rather than a black box. This transparency is crucial, especially in virtual healthcare settings where building trust between AI systems and users is paramount.\n\nTo enhance interpretability, the framework incorporates explanatory models powered by generative AI. These models provide natural language explanations of the health predictions, making it easier for users to understand how the AI arrives at its conclusions. This aspect is particularly important for instilling trust in AI-powered diagnostics within the metaverse healthcare delivery ecosystem.\n\nFor instance, the framework generates a virtual health avatar that illustrates the expected health conditions based on the predicted health outcome and the virtual environment context. Additionally, a generative AI model provides natural language explanations of these health predictions, further enhancing interpretability. This combination of visual and textual explanations ensures that users can comprehend the AI's decision-making process, making the system more transparent and trustworthy.\n\nThe final output of the health prediction module includes the health outcome prediction, a visual avatar representation, and a textual explanation. This multimodal representation provides a rich and accessible way for users to understand their health status in the metaverse environment. By offering clear and detailed explanations, the MMLMH framework ensures that users can trust and effectively utilize the AI-powered diagnostics.",
  "model/output": "The model we developed is versatile and can handle both classification and regression tasks. For classification problems, such as predicting specific health conditions through virtual consultations, we use a softmax activation function in the final layer of our multilayer perceptron (MLP). This allows the model to output probabilities for each class, enabling effective categorization of health states.\n\nFor regression tasks, like estimating the number of healthy minutes an individual can exercise or calculating risk scores, we apply a linear activation function in the final layer. This setup allows the model to predict continuous health metrics accurately.\n\nThe choice of activation function depends on the specific health prediction task at hand. For instance, when dealing with survival prediction, we might use regression techniques, while for diagnosing diseases, classification methods are more appropriate. This flexibility ensures that our model can adapt to a wide range of healthcare applications within the metaverse.\n\nIn addition to the core prediction tasks, our framework includes supplementary components to enhance the benefits of the metaverse and generative AI in health prediction. One such component is a virtual health avatar that visualizes expected health conditions. This avatar is generated based on the predicted health outcome and the virtual environment context, providing a tangible representation of the health data.\n\nFurthermore, we utilize a generative AI model to provide natural language explanations of the health predictions. This feature improves the interpretability of the predictions, making it easier for healthcare providers and patients to understand the outcomes in the metaverse healthcare context. The generated explanations are created using a pre-trained language model fine-tuned for medical explanations, ensuring accuracy and relevance.\n\nOverall, our model's output is designed to be comprehensive and interpretable, supporting a variety of healthcare tasks and providing valuable insights in virtual healthcare settings.",
  "model/duration": "The execution time of the MMLMH framework was optimized for real-time processing capabilities in virtual healthcare environments. The system demonstrated processing latencies of 150 milliseconds for standard virtual consultations, which involve the simultaneous transmission of text, audio, and visual data streams. This performance was achieved using a distributed computing architecture equipped with high-performance graphics processing units (GPUs) and sufficient system memory.\n\nTo maintain fluid virtual interactions, the network bandwidth required was at least 100 Mbps with sub-20-millisecond latency. Under a load of 50 concurrent virtual consultations, the system showed stable performance, utilizing approximately 85% of the available GPU computing capacity and 70% of the available memory resources. In distributed deployments, load balancing was implemented across multiple processing nodes, with each node handling up to 20 concurrent sessions while maintaining response times below 200 milliseconds.\n\nThe framework's ability to process complex multimodal health data streams in real-time was crucial for its effectiveness in virtual healthcare scenarios. The optimization process, including quantization and pruning techniques, reduced the initial model size significantly while maintaining diagnostic accuracy. This ensured that the framework could consistently handle the demands of real-world virtual healthcare applications.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the MMLMH framework was conducted using a comprehensive set of metrics tailored to the specific healthcare tasks performed within the metaverse. We adopted standardized evaluation metrics aligned with the nature of each dataset to ensure a thorough assessment.\n\nFor regression performance and health classification accuracy, we utilized metrics such as mean absolute error (MAE) to evaluate continuous health scores, which describes the average size of prediction errors. The Pearson correlation coefficient (PCC) was used to measure the degree of relationship between predicted and observed outcomes, particularly in survival predictions.\n\nIn binary classification tasks, we employed accuracy (Acc-2) to categorize health states as either positive or negative sickness. The F1 score was calculated to combine precision and recall measurements, providing a balanced view of the model's performance. Additionally, we used a baseline 7-class rascal classifier (Acc-7) to demonstrate the health state within the metaverse.\n\nThe framework was implemented using PyTorch, and hyperparameters were tuned using a grid search approach to optimize performance on validation sets. This method ensured that the model was fine-tuned for the specific requirements of the datasets and the tasks at hand.\n\nWe compared the MMLMH framework against six baseline methods, each representing significant milestones in multimodal fusion frameworks. These methods included Uni2Mul, CSID, DAMUN, HAMF, MFNet, and MD-RCNN, each selected for their unique strengths in addressing various aspects of multimodal integration problems in healthcare.\n\nThe evaluation was conducted on two datasets: CMU-MOSI and MIMIC-III-MV. The results, presented in Tables 2 and 3, demonstrated that the MMLMH framework outperformed all baseline methods across all measures. This superior performance was attributed to the framework's ability to adaptively fuse modality-specific features, understand subtle patterns within patient data, and incorporate contextual information from virtual interactions.\n\nAblation studies were also conducted to analyze the contribution of different components within the MMLMH framework. These studies indicated that every feature was essential for the framework's performance, with the intrasample collaboration mechanism playing a crucial role in understanding the relationships within individual health records. The collaboration activity across different samples and reconstruction loss also significantly contributed to the framework's effectiveness.\n\nIn summary, the evaluation of the MMLMH framework involved a rigorous assessment using standardized metrics, comparison with baseline methods, and ablation studies to validate the importance of each component. The results confirmed the framework's superiority in handling complex multimodal health data streams within virtual healthcare scenarios.",
  "evaluation/measure": "In our evaluation of the MMLMH framework, we employed a comprehensive set of performance metrics to ensure a thorough assessment of its capabilities in virtual healthcare scenarios. These metrics were carefully selected to align with the nature of each dataset and the specific healthcare tasks performed within the metaverse.\n\nFor regression performance and health classification accuracy, we utilized standardized metrics. Continuous health scores were evaluated using the mean absolute error, which provides an average measure of prediction errors. This metric is crucial for understanding the precision of health score predictions.\n\nWhen predicting survival outcomes, we used the Pearson correlation coefficient. This statistical measure assesses the degree of relationship between predicted and observed outcomes, offering insights into the framework's predictive accuracy.\n\nIn quantifying health residues, we employed binary classification accuracy (Acc-2) to categorize health states as either positive or negative sickness. Additionally, we calculated the F1 score, which combines precision and recall measurements, providing a balanced view of the model's performance. For more granular health state classification, we used a baseline 7-class classifier (Acc-7), which demonstrated the health state within the metaverse context.\n\nThese metrics are representative of those commonly used in the literature for evaluating healthcare models, ensuring that our results are comparable and meaningful within the field. The selection of these metrics allows for a robust evaluation of the MMLMH framework's performance, highlighting its strengths and areas for potential improvement in virtual healthcare applications.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of the MMLMH framework with six baseline methods, each representing significant milestones in multimodal fusion for healthcare applications. These baselines were selected for their ability to integrate various data types relevant to healthcare settings. The methods included Uni2Mul, a conformer-based model adapted for health status prediction in virtual environments; CSID, which fuses virtual and real medical imaging data; DAMUN, a domain-adaptive network for patient behavior analysis; HAMF, a hierarchical attention-based framework for early disease detection; MFNet, a network for intensive care unit patient outcome prediction; and MD-RCNN, a recurrent convolutional neural network for disease risk prediction using synthetic data.\n\nThe selection criteria focused on methods that effectively integrate different data types, such as text, audio, and visual information, which are crucial for virtual healthcare scenarios. Each baseline method brought unique strengths to the table, addressing various aspects of multimodal integration. For instance, Uni2Mul's conformer-based architecture excelled in temporal alignment of modalities, while CSID's image fusion capabilities were vital for combining visual features with other modalities. DAMUN's domain-adaptive approach handled the heterogeneous nature of healthcare data, and HAMF's hierarchical attention mechanism was effective in selecting important features across modalities. MFNet demonstrated real-time processing of clinical data streams, and MD-RCNN utilized temporal information from different healthcare data sources.\n\nWe evaluated the performance of MMLMH against these baselines on benchmark datasets, including CMU-MOSI and MIMIC-III-MV. The results consistently showed that MMLMH outperformed all baseline methods across various evaluation metrics. This superior performance can be attributed to MMLMH's adaptive fusion method, which focuses on the most relevant data streams for each health evaluation, and its ability to learn from both individual patient cases and broader datasets. The framework's capacity to handle blended patient data from real-world and synthetic sources further enhances its effectiveness in metaverse healthcare environments.",
  "evaluation/confidence": "The evaluation of our MMLMH framework included a comprehensive analysis of performance metrics across two datasets: CMU-MOSI and MIMIC-III-MV. The results, presented in Tables 2 and 3, demonstrate that MMLMH outperforms all baseline methods across various measures, including mean absolute error (MAE), Pearson correlation coefficient (PCC), binary classification accuracy (Acc-2), F1 score, and 7-class classification accuracy (Acc-7).\n\nTo ensure the robustness of our findings, we conducted ablation studies to analyze the contribution of different components within the MMLMH framework. These studies confirmed that each feature is essential for the framework's performance, particularly highlighting the importance of the intrasample collaboration mechanism.\n\nWhile the specific mention of confidence intervals is not detailed, the systematic approach to evaluation, including ablation studies and longitudinal performance monitoring, suggests a high level of confidence in the results. The framework's ability to adapt and improve over time, as shown in the longitudinal study, further supports the reliability of the performance metrics.\n\nThe statistical significance of the results is implied by the consistent superiority of MMLMH over baseline methods across multiple evaluation metrics and datasets. This consistency indicates that the improvements are not due to random variation but reflect the genuine advantages of the MMLMH framework.\n\nIn summary, the evaluation process was rigorous and comprehensive, providing strong evidence of the framework's superior performance and reliability in virtual healthcare settings.",
  "evaluation/availability": "The datasets generated and analyzed during the current study are available from the corresponding authors upon reasonable request. This approach ensures that the data can be accessed by other researchers for verification or further study, promoting transparency and reproducibility in scientific research. However, the datasets are not publicly released, and specific details about the licensing terms are not provided. Interested parties should contact the corresponding authors to discuss the terms of access and any potential restrictions that may apply."
}