{
  "publication/title": "A Deep Learning Approach to Antibiotic Discovery.",
  "publication/authors": "Stokes JM, Yang K, Swanson K, Jin W, Cubillos-Ruiz A, Donghia NM, MacNair CR, French S, Carfrae LA, Bloom-Ackermann Z, Tran VM, Chiappino-Pepe A, Badran AH, Andrews IW, Chory EJ, Church GM, Brown ED, Jaakkola TS, Barzilay R, Collins JJ",
  "publication/journal": "Cell",
  "publication/year": "2020",
  "publication/pmid": "32084340",
  "publication/pmcid": "PMC8349178",
  "publication/doi": "10.1016/j.cell.2020.01.021",
  "publication/tags": "- Deep Learning\n- Antibiotic Discovery\n- Machine Learning\n- Drug Repurposing\n- Molecular Prediction\n- Neural Networks\n- Antimicrobial Resistance\n- Computational Biology\n- Bioinformatics\n- Drug Development",
  "dataset/provenance": "The dataset used in our study was initially sourced from a widely available FDA-approved drug library, which consisted of 1,760 molecules with diverse structures and functions. To enhance chemical diversity, we supplemented this library with an additional 800 natural products derived from plant, animal, and microbial sources. This resulted in a primary training set comprising 2,560 molecules. After deduplication, this set contained 2,335 unique compounds.\n\nThis dataset was specifically curated for our study to be inexpensive, chemically diverse, and to not require sophisticated laboratory resources. It was used to train a binary classification model that predicts the probability of a new compound inhibiting the growth of E. coli based on its structure. The dataset was split into training, validation, and test sets to evaluate model performance rigorously.\n\nThe dataset was not used in any previous papers by our group or the community. It was created de novo for this study to address the need for a robust model in antibiotic discovery efforts. The primary screening data from this dataset was used to identify molecules with growth inhibitory activity against E. coli, which were then used to train and validate our model.",
  "dataset/splits": "We began by evaluating our model on a training set of 2,335 molecules. The dataset was randomly split into 80% training data, 10% validation data, and 10% test data. This procedure was repeated with 20 different random splits of the data, and the results were averaged. After initial training, we conducted predictions on new datasets. To maximize the amount of training data, we trained new models on the training data from each of the 20 random splits, with 90% training data and 10% validation data, and no test data. This resulted in an ensemble of 20 models. The ensemble was then applied to the Drug Repurposing Hub and the WuXi anti-tuberculosis library. After empirical testing, the data were included in the original training sets to create a new training set containing 2,911 unique molecules.",
  "dataset/redundancy": "The datasets were initially split into 80% training data, 10% validation data, and 10% test data. This split was done randomly and repeated 20 times to ensure robustness and to average the results, making the training and test sets independent in each iteration. To enforce independence, different random splits were used for each of the 20 iterations.\n\nAfter initial model development, the training process was adjusted to maximize the amount of training data. For this, new models were trained on 90% of the data as training data and 10% as validation data, with no separate test set. This approach was taken because the focus shifted from measuring performance on a test set to making predictions on new datasets.\n\nThe distribution of the datasets aimed to include a diverse range of molecules. The primary training set consisted of 2,335 unique compounds, derived from a screen of 2,560 molecules, which included an FDA-approved drug library and natural products. This set was designed to be chemically diverse and did not require sophisticated laboratory resources, making it cost-effective and practical for initial model training.\n\nThe updated training set, after including new data from predictions and empirical testing, contained 2,911 unique molecules. This set included molecules from various libraries, such as the Drug Repurposing Hub and the WuXi anti-tuberculosis library, ensuring a broad chemical space was covered. The goal was to have a training set that was both diverse and representative of potential antibacterial compounds, which is crucial for the model's ability to generalize to new chemical spaces.\n\nThe datasets used in this study were designed to be more diverse and representative compared to some previously published machine learning datasets in the field of antibiotic discovery. The inclusion of natural products and a wide range of FDA-approved drugs helped to capture a broad spectrum of chemical structures and properties, which is essential for developing a robust model capable of identifying novel antibacterial compounds.",
  "dataset/availability": "The data used in our study, including the specific data splits, are not publicly released in a forum. The dataset consists of 2,335 molecules initially, which was later augmented to include 2,911 unique molecules after incorporating new data from empirical testing. The dataset was split into training, validation, and test sets multiple times for model training and evaluation. However, the exact splits and the dataset itself are not made publicly available.\n\nThe models and the methodology used for predictions are described in detail in the publication, allowing others to replicate the study. The web-based version of the antibiotic prediction model is available at http://chemprop.csail.mit.edu/. This model can be used to make predictions on new datasets, but the original dataset used for training is not provided.\n\nThe decision to not release the dataset publicly was enforced by not including it in any public repositories or forums. The focus was on providing the methodology and the model for others to use, rather than sharing the specific dataset. This approach ensures that the model's performance can be evaluated and replicated without compromising the integrity of the original dataset.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is a directed-message passing deep neural network (D-MPNN). This model translates the graph representation of a molecule into a continuous vector via a directed bond-based message passing approach. It builds a molecular representation by iteratively aggregating the features of individual atoms and bonds, passing \"messages\" along bonds that encode information about neighboring atoms and bonds. This process constructs higher-level bond messages containing information about larger chemical substructures.\n\nThe D-MPNN model is not entirely new; it was previously described by K. Yang et al. in 2019. However, our work builds upon this existing architecture by incorporating several optimizations tailored to our specific problem of antibiotic discovery. These optimizations include the addition of molecule-level features computed with RDKit, hyperparameter optimization using a Bayesian scheme, and ensembling techniques to improve model performance.\n\nThe reason this work was not published in a machine-learning journal is that the primary focus of our study is on antibiotic discovery rather than the development of new machine-learning algorithms. Our contributions lie in the application of existing machine-learning techniques to a novel and impactful problem in the field of biology and medicine. The optimizations we employed are standard practices in machine learning and were adapted to enhance the performance of the D-MPNN model for our specific use case. The publication highlights the significant impact that machine learning can have on early antibiotic discovery efforts by increasing the accuracy rate of lead compound identification and decreasing the cost of screening efforts.",
  "optimization/meta": "The model employed in our study does not function as a meta-predictor. Instead, it utilizes an ensemble of models to improve performance. This ensemble consists of multiple instances of the same model architecture, each trained with different random initial weights and on different random splits of the data. The predictions from these individual models are then averaged to produce a final prediction.\n\nThe core model used is a directed-message passing neural network, specifically Chemprop. This model learns to predict molecular properties directly from the graph structure of the molecule, where atoms are represented as nodes and bonds as edges. The model aggregates information from neighboring atoms and bonds through a series of message-passing steps to build an understanding of local chemistry. After a fixed number of these steps, the learned featurizations across the molecule are summed to produce a single featurization for the whole molecule, which is then fed through a feed-forward neural network to output a prediction.\n\nThe ensemble approach does not involve using data from other machine-learning algorithms as input. Instead, it leverages the variability introduced by different random initializations and data splits to create a more robust and accurate predictive model. The training data for each model in the ensemble is independent, as each model is trained on a different random split of the dataset. This independence ensures that the ensemble captures a broader range of possible patterns in the data, leading to improved generalization and performance.",
  "optimization/encoding": "For our machine-learning algorithm, we employed a directed-message passing neural network, specifically Chemprop, to predict molecular properties directly from the graph structure of molecules. Each molecule was reconstructed into a molecular graph using its SMILES string, with atoms represented as nodes and bonds as edges. We utilized the open-source package RDKit to determine the set of atoms and bonds for each molecule.\n\nWe initialized feature vectors for each atom and bond based on computable features. Atom features included atomic number, number of bonds, formal charge, chirality, number of bonded hydrogens, hybridization, aromaticity, and atomic mass. Bond features encompassed bond type, conjugation, ring membership, and stereochemistry.\n\nThe model then applied a series of message-passing steps, aggregating information from neighboring atoms and bonds to build an understanding of local chemistry. During each message-passing step, each bond's featurization was updated by summing the featurization of neighboring bonds, concatenating the current bond's featurization with the sum, and applying a neural network layer with non-linear activation.\n\nAfter a fixed number of message-passing steps, the learned featurizations across the molecule were summed to produce a single featurization for the whole molecule. This featurization was then fed through a feed-forward neural network to output a prediction of the property of interest. In our case, the property of interest was the binary classification of whether a molecule inhibits the growth of E. coli, so the model was trained to output a number between 0 and 1, representing the prediction of growth inhibition.\n\nTo enhance the model's performance, we concatenated the molecular representation learned via message passing with 200 additional molecule-level features computed with RDKit. This hybrid molecular representation helped the model capture both local and global molecular features, improving its ability to generalize and make accurate predictions.",
  "optimization/parameters": "In our study, we employed a directed-message passing deep neural network model, which translates the graph representation of a molecule into a continuous vector. The model's architecture includes several key parameters that were optimized to enhance performance.\n\nThe number of message-passing steps was set to 5, chosen from a range of 2 to 6. This parameter determines how many times the model iteratively aggregates features of individual atoms and bonds to construct higher-level bond messages.\n\nThe hidden size of the neural network layers was optimized to 1600, selected from a range of 300 to 2400. This parameter controls the capacity of the neural network to learn complex representations of molecular structures.\n\nThe number of feed-forward layers was set to 1, chosen from a range of 1 to 3. This parameter influences the depth of the neural network, affecting its ability to capture intricate patterns in the data.\n\nAdditionally, a dropout probability of 0.35 was used, selected from a range of 0 to 0.4. Dropout is a regularization technique that helps prevent overfitting by randomly setting a fraction of input units to 0 at each update during training time.\n\nThese parameters were optimized using a Bayesian hyperparameter optimization scheme with 20 iterations. This method learns to select optimal hyperparameters based on performance using prior hyperparameter settings, allowing for rapid identification of the best set of hyperparameters for the model.",
  "optimization/features": "The input features for our model consist of both atom and bond features, which are used to represent the molecular graph of each compound. Specifically, the atom features include atomic number, number of bonds, formal charge, chirality, number of bonded hydrogens, hybridization, aromaticity, and atomic mass. The bond features encompass bond type, conjugation, ring membership, and stereochemistry.\n\nIn addition to these features, we employed 200 molecule-level features computed with RDKit. These features help capture global molecular properties that the message-passing paradigm might struggle to extract, especially for large molecules. This augmentation was done to enhance the model's ability to generalize and improve its performance.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, we concatenated the molecular representation learned via message passing with the additional molecule-level features from RDKit. This approach allowed us to leverage both local and global molecular information without the need for a separate feature selection step.\n\nThe feature engineering process, including the computation of the 200 molecule-level features, was done using the entire dataset. However, the model training and validation were performed using separate splits of the data to ensure that the model's performance was evaluated on unseen data. This approach helps to prevent overfitting and ensures that the model generalizes well to new, unseen molecules.",
  "optimization/fitting": "The fitting method employed in our study involved several strategies to address potential overfitting and underfitting issues. Given the limited size of our initial training dataset, consisting of 2,335 unique molecules, we augmented the learned molecular representations with additional features computed using RDKit. This hybrid approach helped to enrich the molecular representations, providing more information to the model and aiding in generalization.\n\nTo further mitigate overfitting, we utilized an ensemble of classifiers. By training multiple models with different random initial weights and averaging their predictions, we improved the robustness and generalization capability of our model. Additionally, we employed Bayesian hyperparameter optimization with 20 iterations to fine-tune the model's parameters, ensuring that the model could learn effectively from the data without overfitting.\n\nTo rule out underfitting, we carefully selected hyperparameters within specified ranges, such as the number of message-passing steps, neural network hidden size, number of feed-forward layers, and dropout probability. These ranges were chosen based on empirical evidence and domain knowledge to ensure that the model had sufficient capacity to learn from the data.\n\nDuring training, we used a validation set to monitor the model's performance and prevent underfitting. The model was trained for 30 epochs, and the parameters that performed best on the validation data were selected for evaluation on the test data. This procedure was repeated with 20 different random splits of the data, and the results were averaged to ensure the stability and generalizability of the model.\n\nMoreover, we compared our augmented directed-message passing neural network (D-MPNN) model with other models, including a D-MPNN without RDKit features, a feedforward deep neural network (DNN) with RDKit features, and models using Morgan fingerprints. This comparison helped to validate the effectiveness of our chosen approach and rule out potential underfitting issues.\n\nIn summary, by using a hybrid molecular representation, ensembling, Bayesian hyperparameter optimization, and thorough validation, we addressed both overfitting and underfitting concerns, ensuring that our model could generalize well to new data.",
  "optimization/regularization": "To prevent overfitting, several techniques were employed. Firstly, we augmented the learned molecular representation with additional molecule-level features computed using RDKit. This hybrid approach helped the model to generalize better by incorporating global molecular properties that the message-passing paradigm might struggle to capture, especially for large molecules.\n\nAdditionally, we utilized an ensemble of classifiers. This method involves training multiple models with different random initial weights and averaging their predictions. Ensembling is a well-known technique in machine learning that improves performance and reduces the risk of overfitting by leveraging the strengths of multiple models.\n\nFurthermore, we employed Bayesian hyperparameter optimization. This technique helps in selecting the optimal hyperparameters by learning from prior settings, which allows for rapid identification of the best set of hyperparameters. Properly tuned hyperparameters are crucial for model performance and generalization.\n\nLastly, during the training phase, we used a validation set to monitor the model's performance and selected the model parameters that performed best on this validation data. This approach ensures that the model generalizes well to unseen data and helps in preventing overfitting to the training data.",
  "optimization/config": "The hyperparameter configurations and optimization schedule used in our study are reported in detail. Specifically, we employed a Bayesian hyperparameter optimization scheme with 20 iterations to fine-tune parameters such as the number of message-passing steps, neural network hidden size, number of feed-forward layers, and dropout probability. The ranges and selected values for these hyperparameters are documented, providing a clear guide for replication.\n\nThe model files and optimization parameters are not explicitly mentioned as being available for download. However, a web-based version of the antibiotic prediction model is accessible at http://chemprop.csail.mit.edu/. This online resource allows users to interact with the model and potentially infer the underlying configurations and parameters used during training.\n\nRegarding the license, there is no specific mention of the licensing terms for the model or the data used. Typically, academic publications do not include detailed licensing information for the models or code within the paper itself, but such information might be available through the associated repository or contact with the authors. For precise licensing details, it would be best to refer to the repository or contact the corresponding authors.",
  "model/interpretability": "The model employed in our study is primarily a deep learning approach, specifically a Directed Message Passing Neural Network (D-MPNN), which is inherently a black-box model. This means that the internal workings of the model are not easily interpretable, and it is challenging to directly understand how specific features of the input molecules contribute to the model's predictions.\n\nHowever, we have incorporated several techniques to enhance the interpretability of our model. One key aspect is the use of additional molecule-level features computed with RDKit. These features provide global molecular information that complements the local chemical features extracted through message passing. By including these features, we aim to make the model's decisions more transparent, as these features can be more easily understood and related to known chemical properties.\n\nAdditionally, we utilized Tanimoto similarity to quantify the chemical relationships between molecules. This method allows us to compare the similarity of molecules based on their shared chemical substructures, providing insights into why certain molecules might be predicted as growth inhibitory. For example, we identified that halicin, a top prediction from our model, has a high Tanimoto similarity to the antiprotozoal drug nithiamide. This similarity suggests that halicin shares chemical substructures that are likely responsible for its antibacterial activity.\n\nFurthermore, we employed t-SNE (t-Distributed Stochastic Neighbor Embedding) to visualize the chemical relationships between molecules in the training dataset and the Drug Repurposing Hub. This visualization helps in understanding the chemical space that our model is exploring and can highlight clusters of molecules with similar properties. By examining these clusters, we can gain insights into the types of molecules that our model is likely to predict as antibacterial.\n\nIn summary, while the core D-MPNN model is a black-box, the incorporation of additional molecule-level features, Tanimoto similarity analysis, and t-SNE visualizations provide avenues for interpreting the model's predictions. These techniques help in understanding the chemical basis of the model's decisions and in identifying potential antibacterial compounds.",
  "model/output": "The model is a binary classification model. It predicts the probability that a new compound will inhibit the growth of E. coli based on its structure. The output is a number between 0 and 1, representing the model's prediction about whether the input molecule is growth inhibitory.\n\nThe model was trained to output this binary classification using a directed-message passing neural network, which translates the graph representation of a molecule into a continuous vector. This vector is then fed through a feed-forward neural network to produce the final prediction.\n\nThe performance of the model was evaluated using the ROC-AUC metric, which measures the ability of the model to distinguish between the two classes (growth inhibitory and non-growth inhibitory). The model achieved a ROC-AUC of 0.896 on the test data, indicating strong performance in this binary classification task.\n\nThe model's output can be used to rank molecules according to their predicted likelihood of being antibacterial. This ranking can guide experimental efforts to identify new antibiotics. The model's predictions have been applied to various molecular libraries, including the Drug Repurposing Hub, the WuXi anti-tuberculosis library, and subsets of the ZINC15 database, to identify potential antibacterial compounds.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the deep learning model used in this study is not publicly released. However, a web-based version of the antibiotic prediction model is available. This online model allows users to input molecular data and receive predictions based on the trained model. The web-based model can be accessed at http://chemprop.csail.mit.edu/. This provides a convenient way for researchers and practitioners to utilize the model without needing to implement it locally. The specific licensing details for the use of this web-based model are not provided.",
  "evaluation/method": "The evaluation of our method involved several phases, each designed to rigorously assess the model's performance and generalizability. Initially, we evaluated our model on a training set of 2,335 molecules, which was randomly split into 80% training data, 10% validation data, and 10% test data. This process was repeated with 20 different random splits to ensure robustness. The model was trained for 30 epochs, with performance evaluated on the validation data at the end of each epoch. The best-performing model parameters, as determined by validation performance, were then tested on the held-out test data. This procedure was crucial for determining the best performance of a single model without ensembling.\n\nAfter establishing a satisfactory model performance, we moved to a prediction phase. We applied the ensemble of models, trained on twenty folds, to identify potential antibacterial molecules from the Drug Repurposing Hub and the WuXi anti-tuberculosis library. The highest and lowest predicted molecules from these libraries were empirically tested for growth inhibition against E. coli. This step validated the model's predictions in a real-world setting.\n\nFollowing this, we conducted a retraining phase. The data from the empirical tests were incorporated into the original training set, creating an updated training set of 2,911 unique molecules. This augmented dataset was used to retrain the model, which was then applied to make predictions on a subset of the ZINC15 database. Molecules with a prediction score greater than 0.7 were selected for further curation, ensuring they were not clinical antibiotics.\n\nTo compare the performance of our augmented directed-message passing neural network (D-MPNN) model, we evaluated it against several other models. These included a D-MPNN without RDKit features, a feedforward deep neural network (DNN) using RDKit features, a DNN using Morgan fingerprints, and random forest (RF) and support vector machine (SVM) models using Morgan fingerprints. The scikit-learn implementations of RF and SVM were used with default parameters, except for the number of trees in the RF, which was set to 500. The predictions from these models were compared to assess the effectiveness of our approach.\n\nAdditionally, we trained a Chemprop model on the ClinTox dataset to predict the toxicity of molecules for potential in vivo applications. This dataset consisted of 1,478 molecules with binary properties indicating clinical trial toxicity and FDA-approval status. The Chemprop model was used to prioritize compounds based on predicted toxicity, ensuring that the identified molecules were safe for further investigation.",
  "evaluation/measure": "In our evaluation, we primarily focused on the performance of our model in predicting growth inhibition against E. coli. To assess this, we used the Receiver Operating Characteristic - Area Under the Curve (ROC-AUC) metric. This metric provides a single scalar value that represents the ability of the model to distinguish between molecules that inhibit growth and those that do not, across all possible classification thresholds. We reported the mean ROC-AUC score from multiple trials to provide a robust estimate of our model's performance.\n\nAdditionally, we evaluated the model's predictions by comparing them to empirical testing results. We curated the top predictions from our model and tested them for growth inhibition. The correlation between the prediction scores and the actual growth inhibition observed in the experiments was analyzed. This approach allowed us to validate the model's predictions in a real-world setting and demonstrate its practical utility.\n\nTo ensure the comprehensiveness of our evaluation, we compared our model's performance with several other machine learning models. These included a Directed Message Passing Neural Network (D-MPNN) without RDKit features, a feedforward Deep Neural Network (DNN) using RDKit features and Morgan fingerprints, and traditional machine learning models like Random Forest (RF) and Support Vector Machine (SVM) using Morgan fingerprints. This comparison provided a benchmark for our model's performance and highlighted the advantages of our approach.\n\nFurthermore, we considered the structural uniqueness and predicted toxicity of the molecules. We prioritized molecules that were structurally unique relative to known clinical antibiotics and had low predicted toxicity. This multifaceted evaluation ensured that our model not only performed well in terms of predictive accuracy but also identified molecules that were practically useful for further investigation.\n\nWhile we did not report metrics such as precision, recall, or F1-score explicitly, the ROC-AUC metric and the empirical validation provided a comprehensive assessment of our model's performance. The ROC-AUC metric is widely used in the literature for evaluating binary classification models, making our reported metric representative and comparable to other studies in the field.",
  "evaluation/comparison": "A comparison was performed between the augmented D-MPNN model and several other models to evaluate its performance. The models compared included a D-MPNN without RDKit features, a feedforward deep neural network (DNN) using RDKit features, a DNN using Morgan fingerprints, and traditional machine learning models such as random forest (RF) and support vector machine (SVM) using Morgan fingerprints.\n\nThe random forest classifier was implemented using scikit-learn with 500 trees, and it outputted the growth inhibition probability for each molecule based on the proportion of trees predicting a positive outcome. The support vector machine also used scikit-learn's implementation, outputting the signed distance between the molecule's Morgan fingerprint and the separating hyperplane, which indicated the likelihood of the molecule being antibacterial.\n\nFor the Drug Repurposing Hub, the prediction ranks from the augmented D-MPNN model were compared to those from the other models. The augmented D-MPNN model identified 51 out of 99 predicted molecules that displayed growth inhibition against E. coli, demonstrating a higher accuracy compared to the other models. Notably, the compound halicin, which was identified as a broad-spectrum bactericidal antibiotic, had a higher prediction rank in the augmented D-MPNN model than in most of the other models.\n\nSimilarly, for the ZINC15 database, the top predictions from the augmented D-MPNN model were compared to those from the other models. The augmented D-MPNN model identified 6,820 molecules with prediction scores greater than 0.7, and further comparisons were made with the top-ranked molecules from the other models.\n\nThese comparisons highlighted the effectiveness of the augmented D-MPNN model in identifying antibacterial compounds, particularly in its ability to generalize to new chemistries and outperform simpler baselines and publicly available methods.",
  "evaluation/confidence": "The evaluation of our model involved multiple phases, including initial training, prediction, retraining, and final prediction. During the initial phase, we split the dataset into training, validation, and test sets, and trained the model for 30 epochs. The performance was evaluated on the validation set at the end of each epoch, and the best-performing model parameters were selected based on validation performance. This process was repeated with 20 different random splits of the data to ensure robustness, and the results were averaged.\n\nTo assess the statistical significance and confidence in our model's performance, we compared it against several baselines and alternative models. These included a directed-message passing neural network (D-MPNN) without RDKit features, a feedforward deep neural network (DNN) using RDKit features and Morgan fingerprints, and traditional machine learning models like random forest (RF) and support vector machine (SVM) using Morgan fingerprints. The comparison involved evaluating prediction scores and ranks for molecules from various libraries, such as the Drug Repurposing Hub and the WuXi anti-tuberculosis library.\n\nFor the Drug Repurposing Hub, we curated and empirically tested the top 99 predicted molecules, finding that 51 displayed growth inhibition against E. coli. This empirical validation provided strong evidence of our model's predictive power. Additionally, we observed that higher prediction scores correlated with a greater probability of growth inhibition, further boosting our confidence in the model's performance.\n\nThe model's predictions were also compared to those of other models. For instance, halicin, a compound identified by our model, was ranked higher in our model than in most of the alternative models. This comparison highlighted the importance of using a directed-message passing approach in antibiotic discovery.\n\nIn summary, the performance metrics were evaluated with multiple random splits and empirical testing, providing confidence intervals and statistical significance. The model's superior performance was demonstrated through comparisons with various baselines and alternative models, as well as empirical validation of predicted molecules.",
  "evaluation/availability": "Not enough information is available."
}