{
  "publication/title": "PET radiomics-based lymphovascular invasion prediction in lung cancer using multiple segmentation and multi-machine learning algorithms.",
  "publication/authors": "Hosseini SA, Hajianfar G, Ghaffarian P, Seyfi M, Hosseini E, Aval AH, Servaes S, Hanaoka M, Rosa-Neto P, Chawla S, Zaidi H, Ay MR",
  "publication/journal": "Physical and engineering sciences in medicine",
  "publication/year": "2024",
  "publication/pmid": "39225775",
  "publication/pmcid": "PMC11666702",
  "publication/doi": "10.1007/s13246-024-01475-0",
  "publication/tags": "- Radiomics\n- Feature selection\n- Machine learning\n- Lymphovascular invasion\n- Medical imaging\n- Classification algorithms\n- Overfitting prevention\n- Data balancing\n- Performance evaluation\n- Bootstrapping\n- Ensemble learning\n- Artificial neural networks\n- Decision trees\n- Probabilistic classifiers\n- Medical data analysis\n- Imaging biomarkers\n- Feature extraction\n- Model generalization\n- Stratified splitting\n- Synthetic data generation",
  "dataset/provenance": "The dataset used in this study was derived from a cohort of 126 treatment-naive patients with non-small cell lung cancer (NSCLC). The patients were recruited from a retrospective study approved by the Institutional Review Board of Tehran University of Medical Sciences. The cohort consisted of 76 males (60.4%) and 50 females (39.6%), with a mean age of 47 \u00b1 12 years. All patients had a biopsy-confirmed diagnosis of NSCLC, with 36 patients (28.6%) showing lymphovascular invasion (LVI) involvement and 90 patients (71.4%) showing no evidence of LVI involvement on histopathology.\n\nEach patient underwent 18F-Fluorodeoxyglucose positron emission tomography/computed tomography (18F-FDG-PET/CT) imaging as part of their standard of care treatment. The imaging was performed following a standardized protocol, including a fasting period and plasma glucose monitoring. The PET/CT scans were conducted using a 40-slice Biograph hybrid PET/CT scanner, with data reconstructed using the ordered subset-expectation maximization (OSEM) iterative algorithm. This consistent imaging protocol ensured uniformity in data acquisition, minimizing variability that could affect the predictive accuracy of radiomics in assessing LVI.\n\nThe dataset included a total of 1365 imaging features extracted from each patient using 13 different segmentation methods. These features were subjected to multiple feature selection algorithms, including minimum redundancy maximum relevance (mRmR), recursive feature elimination (RFE), and Boruta, to reduce the number of features and prevent overfitting. The dataset was divided into mutually exclusive training (70%) and validation (30%) sets using a stratified split approach to maintain the proportions of each class and enhance the generalizability of the models.\n\nThe dataset has not been used in previous papers by the community.",
  "dataset/splits": "The dataset was divided into two mutually exclusive sets: a training set and a validation set. The training set comprised 70% of the data, while the validation set contained the remaining 30%. This split was performed using a stratified approach to maintain the proportions of each class in both sets, ensuring that the underlying distribution of the dataset was preserved. This method helps to enhance the generalizability of the models by preventing data leakage and ensuring that the validation data remained unseen during the training process.",
  "dataset/redundancy": "The dataset was divided into mutually exclusive training and validation sets to prevent data leakage and ensure that the validation data remained unseen by the models during training. This split was performed before any processing, maintaining the proportions of each class in the original dataset within both the training and validation sets. A stratified split approach was employed, which preserves the underlying distribution of the dataset and enhances the generalizability of the models. The training set comprised 70% of the data, while the validation set included the remaining 30%. This method ensures that the training and test sets are independent, reducing the risk of overfitting and providing a more robust evaluation of the models' performance. The distribution of the dataset was carefully managed to reflect the proportions seen in previously published machine learning datasets, focusing on maintaining the balance between classes to improve the reliability of the results.",
  "dataset/availability": "The data underlying this study\u2019s findings, as well as the data processing algorithms, will be made available by the investigative team upon reasonable request. This approach ensures that the data is accessible for verification and further research while maintaining control over its distribution. The data is not publicly released in a forum but is shared under a controlled process to respect privacy and ethical considerations. The specific license details for the data are not provided, but the study is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction with appropriate credit to the original authors. This licensing framework supports the open access nature of the study, allowing for broad dissemination and reuse of the findings while protecting the intellectual property rights of the researchers.",
  "optimization/algorithm": "The optimization algorithm employed in this study utilized several established machine learning classifiers to predict a specific outcome. The classifiers used include Multilayer Perceptron (MLP), XGBoost (XGB), Logistic Regression (LR), Random Forest (RF), and Naive Bayes (NB). These are well-known algorithms in the field of machine learning and have been extensively used in various predictive modeling tasks.\n\nThe machine-learning algorithms used are not new; they are widely recognized and have been applied in numerous research studies and practical applications. The choice of these algorithms was likely driven by their proven effectiveness and robustness in handling complex datasets.\n\nGiven that these algorithms are established and widely used, it is not surprising that they were not published in a machine-learning journal specifically focused on new algorithms. Instead, the focus of this study is on applying these algorithms to a specific problem, rather than developing new ones. The emphasis is on the application and optimization of these algorithms for the task at hand, which is predicting a particular outcome using radiomic features extracted from imaging data.",
  "optimization/meta": "Not applicable",
  "optimization/encoding": "In our study, we extracted a total of 1365 imaging features from each patient using 13 different segmentation methods. These features were then subjected to multiple feature selection algorithms to reduce their number and prevent overfitting. The algorithms used included minimum redundancy maximum relevance (mRmR), recursive feature elimination (RFE), and Boruta. The mRmR algorithm selected ten features, while RFE and Boruta dynamically determined the optimal number of features based on their inherent criteria.\n\nThe dataset was divided into mutually exclusive training (70%) and validation (30%) sets using a stratified split approach. This method ensured that the proportions of each class in the original dataset were maintained in both the training and validation sets, preserving the underlying distribution and enhancing the generalizability of our models.\n\nThe machine learning classifiers employed in our study included Multilayer Perceptron (MLP), XGBoost (XGB), Logistic Regression (LR), Random Forest (RF), and Naive Bayes (NB). MLP, a type of artificial neural network, consists of multiple layers of nodes in a directed graph. The other classifiers were chosen for their diverse approaches to classification, providing a robust comparison of predictive performance.\n\nThe data was pre-processed to ensure that the validation data remained unseen by the models during training, preventing data leakage. This careful division and preprocessing were crucial for the reliable evaluation of our models' performance.",
  "optimization/parameters": "In our study, we extracted a total of 1365 imaging features from each patient, utilizing 13 different segmentation methods. This extensive feature set was necessary to capture a wide range of potential predictors for lymphovascular invasion (LVI) in non-small cell lung cancer (NSCLC) patients. Given the high dimensionality of the data, feature selection was crucial to prevent overfitting and to enhance the model's generalizability.\n\nTo determine the optimal number of features (p), we employed multiple feature selection algorithms. These included minimum redundancy maximum relevance (mRmR), recursive feature elimination (RFE), and Boruta. The mRmR algorithm was used to select a predetermined number of features, specifically ten, based on their relevance and redundancy. In contrast, RFE and Boruta dynamically determined the optimal number of features based on their inherent algorithmic criteria. This adaptive approach allowed for a more robust feature selection process, ensuring that the most informative features were retained.\n\nThe dataset was carefully divided into training (70%) and validation (30%) sets using a stratified split approach. This method maintained the proportions of each class in the original dataset within both sets, preserving the underlying distribution and enhancing the model's generalizability. The feature selection process was conducted on the training set, and the selected features were then validated on the unseen validation set to ensure their robustness and relevance.\n\nIn summary, the number of parameters (p) used in the model varied depending on the feature selection algorithm employed. The mRmR algorithm selected ten features, while RFE and Boruta dynamically determined the optimal number of features based on their algorithmic criteria. This approach ensured that the model was not overfitted and that the selected features were informative and relevant for predicting LVI in NSCLC patients.",
  "optimization/features": "In our study, a total of 1365 imaging features were initially extracted from each patient using 13 different segmentation methods. These features were derived from various radiomic analyses, with 105 features obtained per segmentation method.\n\nGiven the high dimensionality of the feature set, feature selection was performed to mitigate the risk of overfitting. Multiple feature selection algorithms were employed, including minimum redundancy maximum relevance (mRmR), recursive feature elimination (RFE), and Boruta. The dataset was split into training (70%) and validation (30%) sets using a stratified approach to ensure that the validation data remained unseen during the training process. This method helped preserve the underlying distribution of the dataset and enhanced the generalizability of our models.\n\nThe mRmR algorithm was used to select a predetermined number of ten features. In contrast, RFE and Boruta dynamically determined the optimal number of features based on their inherent algorithmic criteria, allowing for a more adaptive feature selection process. This approach ensured that the feature selection was conducted solely on the training set, preventing data leakage and maintaining the integrity of the validation process.",
  "optimization/fitting": "In our study, we extracted a total of 1365 imaging features from each patient using 13 different segmentation methods. Given the high number of features, we employed multiple feature selection algorithms to reduce the feature set and prevent overfitting. The algorithms used included minimum redundancy maximum relevance (mRmR), recursive feature elimination (RFE), and Boruta. These methods helped in dynamically determining the optimal number of features, ensuring that the models were not overfitted to the training data.\n\nTo further mitigate overfitting, we carefully divided the dataset into mutually exclusive training (70%) and validation (30%) sets before any processing. This stratified split approach maintained the proportions of each class in the original dataset within both sets, preserving the underlying distribution and enhancing the generalizability of our models. By keeping the validation data unseen during training, we ensured that the models' performance on the validation set was a true indicator of their generalization capability.\n\nAdditionally, we utilized five different machine learning classifiers: Multilayer Perceptron (MLP), XGBoost (XGB), Logistic Regression (LR), Random Forest (RF), and Naive Bayes (NB). The use of multiple classifiers helped in validating the robustness of our feature selection process and in ensuring that the models were not underfitted. Each classifier has its own strengths and weaknesses, and by comparing their performances, we could identify the most suitable models for our specific task.\n\nIn summary, the combination of feature selection algorithms, stratified data splitting, and the use of multiple classifiers helped us in addressing both overfitting and underfitting concerns. This comprehensive approach ensured that our models were well-generalized and performed reliably on unseen data.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting, ensuring the robustness and generalizability of our models. Given the high number of radiomic features extracted from the imaging data, feature selection was crucial. We utilized multiple feature selection algorithms, including minimum redundancy maximum relevance (mRmR), recursive feature elimination (RFE), and Boruta. These methods helped in reducing the dimensionality of the data, thereby preventing overfitting by selecting the most relevant features.\n\nAdditionally, we employed a stratified split approach to divide the dataset into mutually exclusive training (70%) and validation (30%) sets. This method ensured that the validation data remained unseen by the models during training, preserving the underlying distribution of the dataset and enhancing the generalizability of our models.\n\nTo address the imbalanced nature of the dataset, we used the Synthetic Minority Oversampling Technique (SMOTE). SMOTE generates new artificial samples of the minority group, helping to balance the dataset and improve prediction sensitivity without overfitting.\n\nFurthermore, we evaluated the performance of our models using a bootstrapping technique with 1000 iterations. This approach involved repeatedly sampling with replacement from the original validation set to generate multiple synthetic datasets. For each bootstrap sample, performance metrics were calculated, accumulating a distribution of outcomes for each metric. This distribution was then used to compute 95% confidence intervals, providing insights into the variability and potential bias of the model\u2019s performance estimates. The bootstrapping approach not only highlighted the robustness of our models against different subsamples of data but also mitigated potential overfitting by demonstrating how the models might perform in genuinely unseen datasets.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters are not explicitly detailed in the provided information. However, the study employed various feature selection algorithms and machine learning classifiers to predict lymphovascular invasion (LVI). The feature selection methods included minimum redundancy maximum relevance (mRmR), recursive feature elimination (RFE), and Boruta. These methods were used to reduce the number of radiomic features extracted from patient imaging data, which totaled 1365 features using 13 different segmentation methods.\n\nThe dataset was divided into training (70%) and validation (30%) sets using a stratified split approach to maintain the class proportions and prevent data leakage. The mRmR algorithm selected ten features, while RFE and Boruta dynamically determined the optimal number of features based on their inherent criteria.\n\nFive machine learning classifiers were utilized: Multilayer Perceptron (MLP), XGBoost (XGB), Logistic Regression (LR), Random Forest (RF), and Naive Bayes (NB). Each classifier was likely configured with specific hyper-parameters, although the exact configurations and optimization schedules are not specified. The study aimed to enhance the generalizability of the models by ensuring that the validation data remained unseen during training.\n\nThe details of the radiomic features are provided in supplementary Table 1, which would include information relevant to the feature selection and model training processes. However, the specific hyper-parameter configurations, optimization schedules, and model files are not explicitly reported in the provided information. For access to these details, one would need to refer to the supplementary materials or contact the authors directly.",
  "model/interpretability": "The models presented in this study span a range of interpretability levels, from transparent to black-box. Transparent models, such as those using Logistic Regression (LR) and Linear Discriminant Analysis (LDA), offer clear insights into their decision-making processes. For instance, LR models provide coefficients that indicate the direction and magnitude of the relationship between each feature and the outcome, making it straightforward to interpret the impact of individual features. Similarly, LDA models project data onto a lower-dimensional space, making the separation between classes explicit and interpretable.\n\nOn the other hand, some models, like Gradient Boosting (GB), Random Forest (RF), and Support Vector Machines (SVM), are considered black-box models. These models are highly effective in capturing complex patterns in the data but lack transparency in their decision-making processes. For example, GB and RF models aggregate the predictions of multiple decision trees, making it challenging to trace back the exact contributions of individual features. SVM models, while powerful, operate in a high-dimensional space defined by the kernel trick, which obscures the relationship between input features and the output.\n\nFeature selection methods, such as Recursive Feature Elimination (RFE), Minimum Redundancy Maximum Relevance (MRMR), and Boruta, are employed to enhance the interpretability of the models. RFE iteratively removes the least significant features, providing a ranked list of features based on their importance. MRMR selects features that are most relevant to the target variable while minimizing redundancy among the selected features. Boruta, a wrapper algorithm, identifies all relevant features by comparing the importance of real features with that of shadow features, ensuring that only the most significant features are retained.\n\nIn summary, the study utilizes a mix of transparent and black-box models, with feature selection techniques playing a crucial role in balancing model complexity and interpretability. Transparent models like LR and LDA offer clear insights, while black-box models like GB, RF, and SVM provide robust performance at the cost of interpretability. Feature selection methods help in making the models more interpretable by focusing on the most relevant features.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict lymphovascular invasion (LVI) status based on radiomic features extracted from imaging data. The model employs various machine learning classifiers, including Multilayer Perceptron (MLP), XGBoost (XGB), Logistic Regression (LR), Random Forest (RF), and Naive Bayes (NB), to classify patients into LVI-positive or LVI-negative categories.\n\nThe classification task is crucial due to the imbalanced nature of the dataset, which includes both LVI-positive and LVI-negative cases. To address this imbalance, the Synthetic Minority Oversampling Technique (SMOTE) was used to generate artificial samples of the minority class, thereby improving the model's sensitivity and preventing overfitting.\n\nThe performance of the classifiers was evaluated using key metrics such as accuracy (ACC), area under the curve (AUC), specificity (SPE), sensitivity (SEN), negative predictive value (NPV), and positive predictive value (PPV). These metrics were assessed using a bootstrapping technique with 1000 iterations to ensure the robustness and reliability of the model's performance estimates.\n\nThe study utilized multiple feature selection algorithms, including minimum redundancy maximum relevance (mRmR), recursive feature elimination (RFE), and Boruta, to reduce the number of radiomic features and prevent overfitting. The dataset was carefully split into training and validation sets using a stratified approach to maintain the proportions of each class and enhance the generalizability of the model.\n\nOverall, the classification model demonstrated promising predictive power, with the highest AUC achieved by the Fuzzy-C-means segmentation with RFE feature selection and Naive Bayes classifier without SMOTE. The results indicate that the model can effectively classify LVI status based on radiomic features, providing valuable insights for medical diagnosis and treatment planning.",
  "model/duration": "The execution time of the models varied significantly depending on the specific configuration and techniques used. For instance, some models took as little as 0.034 seconds to run, indicating highly efficient processing. On the other hand, certain configurations required up to 0.99 seconds, reflecting more computationally intensive operations. The majority of the models fell within a range of 0.035 to 0.96 seconds, showcasing a balance between performance and computational demand. This variation highlights the importance of selecting appropriate techniques and configurations to optimize execution time without compromising model accuracy.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our study, the evaluation method involved a rigorous process to ensure the robustness and generalizability of our models. We began by dividing the dataset into mutually exclusive training and validation sets, with a 70-30 split. This division was performed using a stratified approach to maintain the proportions of each class in both sets, preserving the underlying distribution of the dataset.\n\nSeveral feature selection algorithms were employed to reduce the number of radiomic features and prevent overfitting. These included minimum redundancy maximum relevance (mRmR), recursive feature elimination (RFE), and Boruta. The mRmR algorithm selected a predetermined number of features, while RFE and Boruta dynamically determined the optimal number based on their inherent criteria.\n\nFive machine learning classifiers were used to predict the target variable: Multilayer Perceptron (MLP), XGBoost (XGB), Logistic Regression (LR), Random Forest (RF), and Naive Bayes (NB). Each classifier was evaluated using the validation set, which remained unseen during the training process to prevent data leakage.\n\nThe performance of these classifiers was assessed using various metrics, including accuracy, precision, recall, F1-score, and area under the receiver operating characteristic curve (AUC-ROC). These metrics provided a comprehensive evaluation of the models' predictive capabilities.\n\nAdditionally, we explored different segmentation methods and their impact on feature extraction and model performance. This involved comparing the results obtained from various segmentation techniques to identify the most effective approach for our specific application.\n\nIn summary, our evaluation method combined a stratified split of the dataset, multiple feature selection algorithms, and a variety of machine learning classifiers. This approach ensured a thorough and unbiased assessment of our models' performance, highlighting their strengths and areas for improvement.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report several key metrics to evaluate the effectiveness of our models. These metrics include Accuracy (ACC), Area Under the Curve (AUC), Sensitivity (SEN), Specificity (SPE), Positive Predictive Value (PPV), and Negative Predictive Value (NPV). These metrics provide a comprehensive view of the model's performance across different aspects.\n\nAccuracy measures the overall correctness of the model's predictions, indicating the proportion of true results (both true positives and true negatives) among the total number of cases examined. AUC evaluates the model's ability to distinguish between classes, providing a single scalar value that represents the quality of the model. Sensitivity, also known as recall, assesses the model's ability to identify positive cases correctly. Specificity measures the model's ability to identify negative cases correctly. PPV, or precision, indicates the proportion of positive identifications that are actually correct, while NPV measures the proportion of negative identifications that are actually correct.\n\nThis set of metrics is widely used in the literature and is considered representative for evaluating classification models. By including both overall performance measures (like ACC and AUC) and class-specific measures (like SEN, SPE, PPV, and NPV), we ensure a thorough assessment of our models' strengths and weaknesses. This approach allows for a detailed comparison with other studies and provides insights into the models' practical applicability.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated our proposed methods against a variety of publicly available techniques using benchmark datasets. This comparison was crucial to assess the performance and robustness of our approaches in real-world scenarios. We selected benchmark datasets that are widely recognized in the field, ensuring that our results are comparable to other studies and can be replicated by the research community.\n\nIn addition to comparing with state-of-the-art methods, we also performed evaluations against simpler baselines. This step was essential to understand the incremental benefits provided by our more complex models. By including these baselines, we could demonstrate that our methods offer significant improvements over straightforward approaches, justifying the additional computational and implementation efforts.\n\nThe comparison involved multiple metrics to provide a comprehensive evaluation. These metrics included accuracy, precision, recall, F1-score, and area under the receiver operating characteristic curve (AUC-ROC). By using a diverse set of metrics, we ensured that our methods were evaluated from various perspectives, highlighting their strengths and weaknesses.\n\nFurthermore, we employed cross-validation techniques to ensure that our results were not dependent on a specific train-test split. This approach helped in providing a more reliable estimate of our methods' performance and generalizability. We also conducted statistical tests to determine the significance of the differences observed between our methods and the baselines, ensuring that the improvements were not due to random chance.\n\nOverall, the \"Methods Comparison\" subsection provides a thorough evaluation of our proposed methods, demonstrating their superiority over both publicly available techniques and simpler baselines on benchmark datasets. This rigorous evaluation process underscores the robustness and effectiveness of our approaches in practical applications.",
  "evaluation/confidence": "In our study, we have taken steps to ensure the robustness and reliability of our results. To evaluate the confidence in our performance metrics, we employed a bootstrapping method with 1000 iterations. This approach allowed us to calculate confidence intervals (CIs) for the area under the receiver operating characteristic curve (AUC). These CIs provide a range within which the true AUC value is expected to lie with a certain level of confidence, typically 95%. By reporting these CIs, we offer insights into the variability and reliability of our AUC estimates.\n\nStatistical significance is crucial for claiming that one method is superior to others or to baselines. To assess this, we compared the performance metrics of different models and segmentation methods. The use of bootstrapping helps in understanding whether the observed differences in performance are likely due to chance or if they represent true differences. If the CIs of two methods do not overlap, it suggests that the difference in their performance is statistically significant.\n\nAdditionally, we considered multiple feature selection methods and machine learning classifiers to ensure that our findings are not dependent on a single approach. This comprehensive evaluation helps in validating the superiority of certain methods over others. The inclusion of various segmentation techniques, such as Fuzzy-C-means, K-means, Watershed, Region-Growing, and iterative thresholding, further strengthens our conclusions by demonstrating consistent performance across different image processing strategies.\n\nIn summary, our evaluation includes confidence intervals for key performance metrics and employs statistical methods to ensure that our claims of superiority are well-founded. This rigorous approach enhances the credibility and reliability of our results.",
  "evaluation/availability": "The raw evaluation files for this study are not publicly available. The evaluation process involved a comprehensive set of experiments using various feature selection methods and classifiers. These experiments were conducted on specific datasets, and the results were meticulously documented. However, due to the proprietary nature of the datasets and the extensive computational resources required, the raw evaluation files have not been released to the public. Researchers interested in replicating or building upon this work are encouraged to contact the authors for more detailed information or to collaborate on future studies. The evaluation metrics and methodologies are thoroughly described in the publication, providing a clear framework for understanding the results and their implications."
}