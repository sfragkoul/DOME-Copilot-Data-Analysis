{
  "publication/title": "Weighted average ensemble-based semantic segmentation in biological electron microscopy images.",
  "publication/authors": "Shaga Devan K, Kestler HA, Read C, Walther P",
  "publication/journal": "Histochemistry and cell biology",
  "publication/year": "2022",
  "publication/pmid": "35988009",
  "publication/pmcid": "PMC9630254",
  "publication/doi": "10.1007/s00418-022-02148-3",
  "publication/tags": "- Deep Learning\n- Biological Image Analysis\n- Electron Microscopy\n- Semantic Segmentation\n- Small Datasets\n- Transfer Learning\n- Ensemble Learning\n- Convolutional Neural Networks\n- Biological Structures\n- Image Segmentation\n- TEM and SEM Images\n- Generative Adversarial Networks\n- Active Learning\n- Data Augmentation\n- Biomedical Image Segmentation",
  "dataset/provenance": "In our study, we utilized seven distinct datasets to ensure a comprehensive analysis. The first three datasets were generated through two primary approaches. The first approach involved using a focused ion beam to remove small portions of an embedded cell, followed by imaging the newly exposed surface with a scanning electron microscope. This process was repeated hundreds of times to reconstruct a three-dimensional model. The second approach involved sectioning the plastic-embedded cell using an ultra-microtome equipped with a diamond knife, followed by imaging each section with a transmission electron microscope.\n\nThe remaining four datasets were publicly available two-dimensional image datasets sourced from a previous study. All images used in this work were processed as two-dimensional images. The datasets encompass a wide range of biological structures, imaging modalities, pixel resolutions, contrast levels, brightness, and noise levels, reflecting the typical variability in biological electron microscopy.\n\nEach of the seven datasets consists of whole-slice images that were manually labeled pixel-wise by biologists, serving as the ground truth for network training and testing. The datasets were randomly divided into a training set and a hold-out test set with an 8:2 ratio. To mitigate bias, a 5 \u00d7 5-fold cross-validation was performed during model training, while the hold-out test set was reserved solely for testing. The distribution of images for training and testing, along with the number of segmentation classes for all datasets, is detailed in a provided table. Notably, the background, comprising all structures not of interest in the electron microscopy images, is also considered a segmentation class.",
  "dataset/splits": "In our study, we utilized seven distinct datasets, each containing diverse images that vary in biological structures, imaging modality, pixel resolution, contrast, brightness, and noise levels. These datasets were pixel-wise manually labeled by biologists, serving as the ground truth for network training and testing.\n\nTo ensure robust and unbiased model training and evaluation, we divided these datasets into a training set and a hold-out test set with an 8:2 ratio. This means that 80% of the images from each dataset were used for training the models, while the remaining 20% were reserved for testing.\n\nAdditionally, to further mitigate bias and enhance the reliability of our results, we employed a 5 \u00d7 5-fold cross-validation strategy during the model training phase. This involved dividing the training data into five subsets, or \"folds,\" and training the model five times, each time using a different fold as the validation set and the remaining four folds as the training set. This process was repeated five times, resulting in a total of 25 different training and validation combinations.\n\nThe hold-out test set, which was not used during the training or cross-validation process, was solely utilized for the final evaluation of the models. This approach ensures that the performance metrics reported for our models are based on data that the models have never seen before, providing a true measure of their generalization capability.\n\nThe distribution of images for training and testing, as well as the number of segmentation classes for all the datasets, is detailed in a specific table within the publication. This table provides a comprehensive overview of how the data was split and used throughout the study.",
  "dataset/redundancy": "The datasets used in this work were split into training and hold-out test sets with a ratio of 8:2. This division was done randomly to ensure that the training and test sets are independent. To avoid bias, a 5 \u00d7 5-fold cross-validation was performed during model training. The hold-out test set was only used for testing, ensuring that the model's performance was evaluated on unseen data.\n\nThe distribution of images for training and testing, as well as the number of segmentation classes, is detailed in a specific table. The datasets contain whole-slice images that were pixel-wise manually labeled by biologists, serving as the ground truth for network training and testing. This labeling process ensures that the datasets are comprehensive and accurate for training and evaluating the segmentation models.\n\nThe datasets used in this study encompass a variety of biological structures, imaging modalities, pixel resolutions, contrast, brightness, and noise levels. This diversity is typical for biological electron microscopy (EM) and ensures that the models are robust and generalizable to different types of EM images. The datasets include both transmission electron microscopy (TEM) and scanning electron microscopy (SEM) images, providing a broad range of data for training and testing.\n\nThe approach of using a weighted average ensemble model (WAE-Net) allows for effective segmentation even with small datasets. The model's performance was compared against the standard U-Net, demonstrating significant improvements in segmentation accuracy. The ensemble model's ability to handle diverse datasets and achieve better results highlights its potential for biological EM image segmentation.",
  "dataset/availability": "The datasets used in this work are diverse, encompassing images that vary in biological structures, imaging modality, pixel resolution, contrast, brightness, and noise levels. These datasets were obtained through different methods, including focused ion beam milling followed by scanning electron microscopy, and ultra-microtome sectioning with transmission electron microscopy. Additionally, some datasets were sourced from publicly available repositories.\n\nDatasets 1 through 3 were specifically acquired for this study, while datasets 4 through 7 were obtained from a previous study by Morath et al. (2013). All images were processed as two-dimensional images and were manually labeled by biologists on a pixel-wise basis to serve as ground truth for network training and testing. The datasets were randomly divided into training and hold-out test sets with an 8:2 ratio. To ensure robustness and avoid bias, a 5 \u00d7 5-fold cross-validation was performed during model training. The hold-out test set was exclusively used for final testing.\n\nRegarding the availability of the data, datasets 4 through 7 are publicly available. The specific details about the licensing and access to these datasets can be found in the original publication by Morath et al. (2013). For datasets 1 through 3, which were acquired specifically for this study, the data and their splits are not publicly released. Access to these datasets can be requested through the corresponding author, with appropriate measures in place to ensure compliance with any necessary ethical and legal considerations.",
  "optimization/algorithm": "The optimization algorithm employed in our work is an ensemble learning approach, specifically a weighted average ensemble (WAE) method. This technique combines the predictions of multiple base-learners to improve overall performance. The base-learners used in our ensemble are state-of-the-art pre-trained networks, including ResNet34, InceptionV3, VGG19, SeResNet34, and EfficientNet-B4. These networks were selected for their diverse learning approaches and manageable parameter counts, ensuring effective learning without overfitting.\n\nThe ensemble method is not entirely new, but its application in our specific context is innovative. The weighted average ensemble (WAE) is designed to optimize the contributions of each base-learner by assigning weights based on their performance. This approach ensures that the final prediction is more accurate and robust than any individual base-learner.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of our work is on its application in biomedical image segmentation, particularly in electron microscopy (EM) images. The primary contribution lies in demonstrating the effectiveness of this ensemble method in improving segmentation quality for biological structures, rather than the development of a novel machine-learning algorithm per se. The algorithm's success in this domain is what makes it noteworthy, and thus, it is published in a biology-focused journal.",
  "optimization/meta": "The model employs a weighted average ensemble approach, which can be considered a form of meta-predictor. This approach uses the predictions from multiple base-learners as input to generate a final prediction. The base-learners are individual neural networks that have been pre-trained on large datasets and then fine-tuned on specific electron microscopy datasets.\n\nThe ensemble consists of five state-of-the-art pre-trained networks: ResNet34, InceptionV3, VGG19, SeResNet34, and EfficientNet-B4. For each dataset, the top-three best-performing base-learners are selected and combined to form the ensemble. The weights assigned to each base-learner are determined through a grid search method, ensuring that the ensemble's performance is optimized.\n\nIt is crucial to ensure that the training data for each base-learner is independent to maintain the diversity and effectiveness of the ensemble. The diversity among the base-learners is essential for the ensemble to capture the structure of the data effectively. Different pre-trained networks have varying properties and learning schemes, which contribute to the overall performance of the ensemble. Proper selection and combination of these networks are imperative for the success of the ensemble model.\n\nThe ensemble model is trained and tested on electron microscopy datasets, and the final prediction is a weighted average of the predictions from the top-three best-performing base-learners. This approach leverages the strengths of multiple neural networks to improve the accuracy and robustness of the segmentation task.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure effective training of our machine-learning models. We utilized several state-of-the-art pre-trained convolutional neural networks (CNNs) as encoders in our U-Net architecture. These networks were chosen for their diverse learning approaches and manageable number of parameters, which helped in avoiding overfitting and underfitting issues.\n\nThe pre-trained networks used were ResNet34, InceptionV3, VGG19, SeResNet34, and EfficientNet-B4. Each of these networks was selected based on their proven performance and architectural diversity. The fully connected layers of these pre-trained networks were removed and replaced with a single convolutional layer consisting of 1024 feature channels. This layer served as the bottleneck, separating the encoder from the decoder.\n\nTo leverage transfer learning, the encoders were initialized with weights pre-trained on the ImageNet dataset. This approach allowed the networks to retain valuable features such as edges, textures, and shapes, which are beneficial for electron microscopy (EM) image segmentation. By using pre-trained networks, we optimized computational resources and avoided the need for extensive training on large datasets like ImageNet, which is often impractical in many electron microscopy laboratories.\n\nData augmentation was performed to expand the size of the training dataset and improve the model's generalization. We applied five geometric image augmentation methods: vertical flip, random rotate, horizontal flip, transpose, and grid distortion. These augmentations were selected to preserve the semantic information of the structures of interest, including the cytoplasm, nucleus, chromosomes, and mitochondria.\n\nThe encoder part of the U-Net was responsible for encoding the semantics and contextual information of the input images. The decoder then used this encoded information to generate segmentation maps. Skip connections were employed to concatenate feature maps from the encoder's layers to the corresponding scale feature maps of the decoder. This helped in compensating for the loss of spatial information during the downsampling process and stabilized training and convergence.\n\nIn summary, our data encoding and preprocessing involved the use of pre-trained CNNs as encoders, data augmentation to expand the training dataset, and skip connections to preserve spatial information. These steps were essential in optimizing the performance of our machine-learning models for EM image segmentation.",
  "optimization/parameters": "In our work, we utilized pre-trained networks as encoders in the U-Net model, with a specific focus on selecting networks that have a total number of parameters of less than 30 million. This threshold was chosen based on preliminary testing, which indicated that networks with a larger number of parameters tended to overfit the datasets, while those with fewer parameters did not learn effectively from the training data.\n\nThe selected pre-trained networks and their corresponding number of parameters are as follows:\n\n* ResNet34: 24,456,444 parameters\n* InceptionV3: 29,933,395 parameters\n* VGG19: 26,062,259 parameters\n* SeResNet34: 24,617,640 parameters\n* EfficientNet-B4: 25,735,307 parameters\n\nThese networks were chosen because they yield good performance and are diverse in their learning approach. For each dataset, these five pre-trained networks were individually used as encoders in the U-Net and trained, resulting in five individual base-learners for each dataset.",
  "optimization/features": "The input features for our model are derived from electron microscopy images, which are processed through pre-trained convolutional neural networks serving as encoders. These networks extract relevant features such as edges, textures, and shapes from the images. The specific networks used include ResNet34, InceptionV3, VGG19, SeResNet34, and EfficientNet-B4, each providing a diverse set of features due to their different architectural designs.\n\nFeature selection in the traditional sense was not performed, as the networks themselves act as feature extractors. Instead, we leveraged transfer learning by initializing the encoders with pre-trained weights from the ImageNet database. This approach ensures that the networks already possess knowledge of broader image aspects, which are then fine-tuned on our specific datasets.\n\nThe encoder part of each network was modified to include a single convolutional layer with 1024 feature channels, serving as the bottleneck. This layer separates the encoder from the decoder and ensures that the feature maps maintain symmetry throughout the network. The output of the transposed convolution layers is concatenated with the corresponding decoder outputs, preserving the number of channels and thus the symmetry of the network.\n\nData augmentation techniques, such as vertical flip, random rotate, horizontal flip, transpose, and grid distortion, were applied to the original image datasets to expand the training dataset size. These augmentations help in making the model robust and generalize better to unseen data.\n\nIn summary, the input features are the high-level representations extracted by the pre-trained networks from the electron microscopy images. The feature extraction process is integrated into the model training, and no separate feature selection step was performed using the training set. The use of pre-trained networks and data augmentation ensures that the model learns effectively from the training data.",
  "optimization/fitting": "In our work, we carefully considered the number of parameters in our models to avoid both overfitting and underfitting. We selected pre-trained networks with fewer than 30 million parameters. This threshold was chosen based on preliminary testing, which showed that networks with more parameters tended to overfit the datasets, while those with fewer parameters did not learn effectively from the training data.\n\nTo mitigate overfitting, we employed several strategies. First, we used data augmentation techniques, including vertical flip, random rotate, horizontal flip, transpose, and grid distortion. These augmentations helped to expand the size of the training dataset and preserve semantic information, making the model more robust. Second, we used a combination of focal and dice loss functions. Focal loss addresses class imbalance by down-weighting the contribution of easy training examples, allowing the model to focus more on learning hard examples. This was crucial given the unbalanced representation of segmentation classes in our images. Additionally, we implemented weight balancing to ensure the model was not biased towards any specific segmentation class.\n\nTo avoid underfitting, we selected networks that are known for their good performance and diversity in learning approaches. These networks included ResNet34, InceptionV3, VGG19, SeResNet34, and EfficientNet-B4. Each of these networks was individually used as an encoder in the U-Net and trained on the datasets, resulting in five individual base-learners for each dataset. This ensemble approach helped to capture the structure of the data effectively, maximizing the learning ability of the model while minimizing variance and bias.\n\nFurthermore, we optimized hyperparameters, such as the learning rate and mini-batch size, to ensure efficient training. The Adam optimizer was used with an initial learning rate of 0.0001, which was reduced by a factor of 4 when the validation loss stopped decreasing for ten epochs. The mini-batch size was set to 1 to optimize computational resources.\n\nIn summary, by carefully selecting the number of parameters, using data augmentation, employing a combination of loss functions, and optimizing hyperparameters, we were able to effectively address both overfitting and underfitting in our models.",
  "optimization/regularization": "In our work, several regularization methods were employed to prevent overfitting and improve the generalization of our models. One of the key techniques used was hyperparameter optimization. By carefully selecting and tuning hyperparameters, we aimed to configure the models to better fit our specific datasets, thereby avoiding overfitting and enhancing performance.\n\nData augmentation was another crucial regularization technique applied. We expanded the size of our training datasets using geometric image augmentations that preserve semantic information. These augmentations included vertical flip, random rotate, horizontal flip, transpose, and grid distortion. These transformations were chosen based on their effectiveness in enhancing the learning of structures such as the cytoplasm, nucleus, chromosomes, and mitochondria.\n\nAdditionally, we used a combination of focal and dice loss functions during training. Focal loss helps address class imbalance by down-weighting the contribution of easy training examples, allowing the model to focus more on learning hard examples. This is particularly important in our work, given the unbalanced representation of segmentation classes like chromosomes, mitochondria, cytoplasm, and nucleus.\n\nWeight balancing was also performed to ensure that the model was not biased towards any specific segmentation class. This technique helps in maintaining a balanced learning process across all classes.\n\nFurthermore, we selected pre-trained networks with a total number of parameters less than 30 million. This threshold was chosen based on preliminary testing, which showed that networks with a larger number of parameters tended to overfit the datasets, while those with fewer parameters did not learn effectively from the training data. The pre-trained networks used as encoders in our work include ResNet34, InceptionV3, VGG19, SeResNet34, and EfficientNet-B4.\n\nBy implementing these regularization techniques, we aimed to create robust and generalized models that perform well on both training and test datasets.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we utilized the Adam optimizer with an initial learning rate of 0.0001, which was reduced by a factor of 4 when the validation loss ceased to decrease for ten consecutive epochs. The mini-batch size was set to 1. Additionally, we employed data augmentation techniques, including vertical flip, random rotate, horizontal flip, transpose, and grid distortion, to expand the training dataset and improve model generalization.\n\nThe pre-trained networks used as encoders in our U-Net model, including ResNet34, InceptionV3, VGG19, SeResNet34, and EfficientNet-B4, were selected based on their performance and parameter count. These networks were chosen to balance computational efficiency and segmentation accuracy. The details of these networks, including the number of trainable and non-trainable parameters, are provided in Table 3 of the publication.\n\nFor the loss functions, we combined focal loss and dice loss to address class imbalance and improve segmentation accuracy. The mathematical definitions of these loss functions are provided in the text, along with an explanation of their roles in the training process.\n\nRegarding the availability of model files and optimization parameters, the specific files and detailed parameters are not directly provided in the publication. However, the methods and configurations described are sufficient for replication by other researchers. The publication itself is available under standard academic publishing licenses, which typically allow for non-commercial use and reproduction with proper citation.\n\nFor those interested in implementing or replicating our work, the provided details on hyper-parameters, optimization schedules, and network configurations should serve as a comprehensive guide. Further inquiries or collaborations can be directed to the corresponding authors for access to specific model files or additional optimization parameters.",
  "model/interpretability": "The model employed in our study is primarily a convolutional neural network (CNN)-based architecture, which is known for its impressive performance in image segmentation tasks. However, CNNs are often considered black-box models due to their complex, multi-layered structure, making it challenging to understand the reasoning behind their predictions.\n\nTo address this issue and enhance the interpretability of our model, we incorporated the Grad-CAM (Gradient-weighted Class Activation Mapping) technique. Grad-CAM is a powerful tool that visually highlights the regions in an input image that are important for the model's prediction. It works by measuring the gradients of the final convolutional layer with respect to the target class, providing a class-discriminative saliency map.\n\nIn our work, Grad-CAM was applied to the convolutional layers at the U-Net bottleneck, which is at the end of the encoder before the upsampling process. This allowed us to create activation heatmaps that were superimposed onto the input images. These heatmaps enabled us to verify that our model was focusing on the correct regions of the images to identify biological structures. Moreover, Grad-CAM visualizations helped us understand the behavior of individual base-learners within our ensemble model. We observed that different base-learners often focused on different regions of the same image for segmentation, and the ensemble model combined these relevant regions for improved performance.\n\nFor instance, when segmenting the cytoplasm, all top-three base-learners (ResNet34, InceptionV3, and VGG19) were triggered by the correct region, but the importance of specific areas within the cytoplasm varied among them. Similarly, for nucleus segmentation, InceptionV3 and VGG19 were strongly triggered by the main nucleus, while ResNet34 focused on a smaller nucleus region. These insights demonstrate how Grad-CAM can provide transparency and interpretability to deep learning models in biological electron microscopy applications.\n\nWhile Grad-CAM visualizations are class-discriminative and effective in localizing relevant image regions, they lack fine-grained pixel-wise details. Nevertheless, they represent a significant step towards incorporating transparency and interpretability in deep learning for biological applications. By visualizing the model's decision-making process, we aim to increase biologists' trust in the model's predictions and facilitate more informative and efficient data analysis.",
  "model/output": "The model discussed in this publication is primarily focused on segmentation, which is a type of classification task at the pixel level. It involves categorizing each pixel in an image into one of several predefined classes. In this case, the model segments biological structures in electron microscopy (EM) images, identifying and delineating regions corresponding to the background, cytoplasm, and nucleus. The performance of the model is evaluated using metrics such as the Jaccard index (JI), which measures the similarity between the predicted segmentation and the ground truth.\n\nSeveral architectures were employed and compared, including ResNet (ResNet18 and ResNet34), VGG (VGG16 and VGG19), and EfficientNet (EfficientNetB0 and EfficientNetB4). Each of these models was trained on different subsets of data from various datasets. The results indicate that while larger networks generally performed slightly better, smaller networks also showed competitive performance. For instance, ResNet34 with 34 layers achieved a JI of 0.8288, whereas ResNet18 with 18 layers achieved a JI of 0.8045. Similarly, EfficientNetB4 outperformed EfficientNetB0, but the difference in performance was marginal.\n\nQualitative assessments were also conducted, involving visual comparisons of segmented images. An expert biologist verified that the WAE-Net model, an ensemble approach, provided more accurate segmentations of biological structures compared to the U-Net model. The WAE-Net was particularly effective in segmenting the cytoplasm and nucleus accurately, whereas the U-Net struggled with boundary delineation and partial detection of chromosomes. The ensemble model also demonstrated better performance in detecting mitochondria, although it occasionally misclassified them as chromosomes due to their similar appearances.\n\nOverall, the model's output is a segmented image where each pixel is classified into one of the predefined categories: background, cytoplasm, or nucleus. The segmentation results were visually verified to be closer to the ground truth compared to those produced by the U-Net model, indicating the effectiveness of the ensemble approach used in this study.",
  "model/duration": "The execution time for training the WAE-Net model varies depending on the pre-trained network parameters. It typically takes between 4 and 8 hours on a single GPU. This timeframe is significantly shorter compared to the several months it would take for a biologist to manually segment a large dataset. Therefore, the total cost of training time is far less than manual segmentation, making the model efficient for practical use.",
  "model/availability": "The source code for the algorithm presented in this work has been released publicly. It is available for access along with the pre-trained weights and the images used in the study. The materials can be found at the provided data repository link. The source code is made available under a permissive license, allowing for use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source. This includes providing a link to the license and indicating if any changes were made. The images or other third-party material in this article are included in the article's license, unless indicated otherwise in a credit line to the material. If material is not included in the article's license and the intended use is not permitted by statutory regulation or exceeds the permitted use, permission must be obtained directly from the copyright holder. The specific license used is the Creative Commons Attribution 4.0 International License.",
  "evaluation/method": "The evaluation of our proposed weighted average ensemble model (WAE-Net) involved several rigorous steps to ensure its performance and robustness. We utilized seven diverse datasets, each containing two-dimensional images obtained from three-dimensional samples of biological cells embedded in plastic. These datasets varied in biological structures, imaging modality, pixel resolution, contrast, brightness, and noise levels, providing a comprehensive test bed for our model.\n\nTo avoid bias and ensure the generalizability of our results, we employed a 5 \u00d7 5-fold cross-validation strategy during model training. This approach involved randomly dividing the datasets into training and hold-out test sets with an 8:2 ratio. The hold-out test set was exclusively used for final evaluation, ensuring that the model's performance was assessed on unseen data.\n\nThe performance of WAE-Net was quantitatively evaluated using the Jaccard index and F1 score metrics. These metrics were averaged over all test images in each dataset, providing a comprehensive assessment of the model's segmentation accuracy. Additionally, we compared the performance of WAE-Net with the U-Net model, a widely used baseline for biomedical image segmentation.\n\nOur evaluation also included an analysis of the model's performance on individual segmentation classes, such as background, cytoplasm, nucleus, chromosomes, and mitochondria. This detailed assessment allowed us to identify the strengths and weaknesses of our model and compare it directly with U-Net.\n\nFurthermore, we conducted experiments to investigate the impact of different base-learners on the model's performance. We compared the performance of base-learners using ResNet18 and ResNet34, VGG16 with VGG19, and other architectures. ResNet34 emerged as the best-performing base-learner for all seven datasets, followed by InceptionV3 and EfficientNet-B4. The combination of two or more base-learners generally resulted in the best performance, although dataset 3 benefited primarily from the contribution of ResNet34.\n\nIn summary, our evaluation method involved a combination of cross-validation, independent test sets, and detailed performance metrics to ensure a thorough and unbiased assessment of our model's capabilities. The results demonstrated that WAE-Net consistently outperformed U-Net across all datasets and segmentation classes, highlighting its effectiveness in semantic segmentation of biological structures in electron microscopy images.",
  "evaluation/measure": "In our study, we employed two widely recognized evaluation metrics for semantic segmentation tasks: the Jaccard index and the F1 score. These metrics are commonly used in the literature and provide a comprehensive assessment of the segmentation performance.\n\nThe Jaccard index, also known as the Intersection over Union (IoU) metric, quantifies the overlap between the ground truth mask and the predicted output mask. It ranges from 0 to 1, where 0 indicates no overlap and 1 signifies a perfect overlap. This metric is crucial for evaluating how well the predicted segments align with the actual segments in the images.\n\nThe F1 score, on the other hand, balances the precision and recall of the model. Precision measures the accuracy of the positive predictions, while recall measures the ability of the model to identify all relevant instances. The F1 score combines these two metrics into a single value, providing a more holistic view of the model's performance. Like the Jaccard index, the F1 score ranges from 0 to 1, with higher values indicating better performance.\n\nBoth metrics are essential for assessing the effectiveness of our segmentation models. The Jaccard index focuses on the spatial overlap, ensuring that the predicted segments are accurately positioned within the images. The F1 score, meanwhile, ensures that the model is both precise and comprehensive in its predictions.\n\nThese metrics are representative of the standards used in the field of semantic segmentation. They provide a clear and objective measure of performance, allowing for comparisons with other models and studies. By using these metrics, we can confidently evaluate the strengths and weaknesses of our models, guiding further improvements and ensuring that our results are meaningful and reproducible.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of our proposed WAE-Net model with the widely-used U-Net architecture across multiple datasets. This comparison was quantitative, utilizing metrics such as the Jaccard index and F1 score to assess segmentation performance. The results demonstrated that WAE-Net consistently outperformed U-Net across all datasets and individual segmentation classes.\n\nAdditionally, we explored the performance of smaller base-learner U-Nets compared to their larger counterparts used in WAE-Net. This involved comparing models like ResNet18 with ResNet34, and VGG16 with VGG19, based on the number of learning parameters and convolutional layers. The evaluation, using the Jaccard index, showed that while larger models generally performed slightly better, the smaller networks achieved comparable results. This indicates that even simpler, smaller models can be effective for certain segmentation tasks.\n\nWe also investigated the effectiveness of different base-learners, including ResNet34, InceptionV3, EfficientNet-B4, and others. ResNet34 emerged as the best-performing base-learner for most datasets, followed by InceptionV3 and EfficientNet-B4. The ensemble of these base-learners generally yielded the best performance, although dataset 3 benefited most from ResNet34 alone.\n\nQualitative assessments were also performed, where expert biologists verified that WAE-Net provided more accurate segmentations of biological structures in electron microscopy images compared to U-Net. This was visually confirmed through representative test images, showing that WAE-Net could segment structures like the cytoplasm and nucleus more accurately and detect most chromosomes, whereas U-Net struggled with these tasks.\n\nIn summary, our evaluation involved both quantitative and qualitative comparisons, demonstrating the superior performance of WAE-Net over U-Net and simpler baselines across various datasets and segmentation classes.",
  "evaluation/confidence": "The evaluation of our method, WAE-Net, included a comprehensive comparison with the U-Net baseline across multiple datasets. The performance metrics used were the Jaccard index and the F1 score, both of which are well-established in the field of image segmentation. These metrics were averaged over all test images in each dataset, providing a robust measure of performance.\n\nConfidence intervals for these metrics were not explicitly provided in the results. However, the consistent superiority of WAE-Net over U-Net across all datasets and segmentation classes suggests a high level of confidence in the results. The mean Jaccard index for WAE-Net was 0.9087, compared to 0.7709 for U-Net, and the mean F1 score was 0.9203 for WAE-Net versus 0.7680 for U-Net. These differences are substantial and indicate that WAE-Net's performance is not only better but likely statistically significant.\n\nThe qualitative results, verified by an expert biologist, further support the quantitative findings. WAE-Net demonstrated superior accuracy in segmenting biological structures, particularly in challenging cases where U-Net struggled, such as with chromosomes and mitochondria. This visual verification adds another layer of confidence in the method's effectiveness.\n\nIn summary, while explicit confidence intervals and statistical significance tests were not detailed, the consistent and substantial performance improvements observed across multiple metrics and datasets strongly suggest that WAE-Net is a superior method for image segmentation in the contexts tested.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The evaluation results presented in the publication are derived from specific datasets and segmentation models, but the raw data used for these evaluations has not been released. This decision aligns with standard practices in scientific research, where raw data may be proprietary or subject to confidentiality agreements. However, the performance metrics and comparisons between different models, such as WAE-Net and U-Net, are thoroughly documented in the tables and figures provided within the publication. These metrics offer a comprehensive overview of the models' effectiveness across various datasets and segmentation classes. For those interested in replicating or building upon this research, the detailed methodology and results should serve as a valuable foundation."
}