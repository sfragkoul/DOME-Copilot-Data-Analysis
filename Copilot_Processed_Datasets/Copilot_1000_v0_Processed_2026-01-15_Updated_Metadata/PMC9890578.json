{
  "publication/title": "Deep learning for prediction of fractional flow reserve from resting coronary pressure curves.",
  "publication/authors": "Zimmermann FM, Mast TP, Johnson NP, Everts I, Hennigan B, Berry C, Johnson DT, De Bruyne B, Fearon WF, Oldroyd K, Pijls NHJ, Tonino PAL, van 't Veer M",
  "publication/journal": "EuroIntervention : journal of EuroPCR in collaboration with the Working Group on Interventional Cardiology of the European Society of Cardiology",
  "publication/year": "2021",
  "publication/pmid": "32863244",
  "publication/pmcid": "PMC9890578",
  "publication/doi": "10.4244/eij-d-20-00648",
  "publication/tags": "- fractional flow reserve\n- innovation\n- stable angina\n- deep learning\n- convolutional neural network\n- recurrent neural network\n- coronary pressure curves\n- non-hyperaemic pressure ratios\n- artificial intelligence\n- diagnostic accuracy",
  "dataset/provenance": "The dataset used in this study originates from the ARTIST study, which is a post hoc analysis of three previously published studies: CONTRAST, VERIFY, and VERIFY 2. These studies have been approved by the institutional review boards of the individual sites and their detailed descriptions and primary results have been published previously.\n\nThe dataset includes a total of 1,666 patients with 1,718 coronary lesions and 2,928 coronary pressure tracings. The data consists of raw tracings of simultaneous aortic (Pa) and distal coronary pressure (Pd) recorded during both resting (non-hyperaemic) conditions and maximal hyperaemia induced by either intravenous or intracoronary adenosine.\n\nThe derivation cohort, which is a random 80% sample of the total cohort, was used to train the deep neural network with paired examples of resting coronary pressure curves and their corresponding FFR values. The validation cohort, comprising a random 20% sample of the total cohort, was used to validate the resulting algorithm against unseen resting pressure curves.\n\nThe dataset includes various non-hyperaemic pressure ratios (NHPR) such as diastolic pressure ratio (dPR), resting Pd/Pa, instantaneous wave-free ratio (iFR), and relative flow reserve (RFR). These ratios were calculated using specific definitions and binary cut-offs as referenced in the literature. The median values for resting Pd/Pa, iFR, and FFR in the pooled cohort are 0.92, 0.89, and 0.80, respectively. Out of 1,718 coronary lesions, 923 (54%) had FFR \u22640.80.",
  "dataset/splits": "The dataset was divided into two main splits: a derivation cohort and a validation cohort. The derivation cohort consisted of a random 80% sample of the total dataset, while the validation cohort comprised the remaining 20%.\n\nThe total dataset included 1,666 patients with 1,718 coronary lesions and 2,928 coronary pressure tracings. Therefore, the derivation cohort contained approximately 1,333 patients with around 1,374 coronary lesions and 2,342 coronary pressure tracings. The validation cohort included approximately 333 patients with around 344 coronary lesions and 586 coronary pressure tracings.\n\nIn the derivation cohort, a fivefold cross-validation procedure was employed to reduce the variance in precision. This means the derivation cohort was further divided into five subsets, with each subset used as a validation set once while the remaining four subsets were used as the training set. This process was repeated five times, ensuring that each subset was used as the validation set exactly once.",
  "dataset/redundancy": "The dataset used in this study was derived from a post hoc analysis of three previously published studies: CONTRAST, VERIFY, and VERIFY 2. The total cohort consisted of 1,666 patients with 1,718 coronary lesions and 2,928 coronary pressure tracings. To ensure the independence of the training and test sets, the dataset was split into a derivation cohort and a validation cohort. The derivation cohort, which was used for training the deep neural network, consisted of a random 80% sample of the total cohort. The validation cohort, used to validate the resulting algorithm, consisted of the remaining 20% of the total cohort. This split was designed to ensure that the validation cohort contained unseen resting pressure curves, thereby providing an unbiased evaluation of the model's performance.\n\nTo reduce the variance in the precision of the model, a fivefold cross-validation procedure was employed. This technique involves partitioning the derivation cohort into five subsets, training the model on four of these subsets, and validating it on the remaining subset. This process is repeated five times, with each subset serving as the validation set once. The results are then averaged to produce a more robust estimate of the model's performance.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the field of cardiovascular research. The cohort size is substantial, and the inclusion of multiple studies ensures a diverse range of patient characteristics and coronary lesions. This diversity is crucial for training a robust model that can generalize well to new, unseen data. The use of a validation cohort and cross-validation further ensures that the model's performance is reliable and not due to overfitting to the training data.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the class of deep neural networks. Specifically, two types of architectures were employed: convolutional neural networks (CNNs) and recurrent neural networks (RNNs). The CNNs were designed to automatically learn and identify features present in resting coronary pressure curves. The RNNs, on the other hand, were used to incorporate temporal dependencies among features by adding information from previous intervals to the next.\n\nThese algorithms are not entirely new but have been adapted for a specific medical application. The choice to publish in a cardiovascular journal rather than a machine-learning journal is likely due to the focus on the medical application and the validation of the algorithms in a clinical context. The primary goal was to derive a novel non-hyperaemic algorithm based on deep learning and to validate it against fractional flow reserve (FFR), which is a critical metric in cardiovascular assessments. The study aimed to demonstrate the potential of these deep learning techniques in improving diagnostic accuracy for FFR prediction from resting coronary pressure curves.",
  "optimization/meta": "Not applicable. The publication does not discuss a meta-predictor or the use of data from other machine-learning algorithms as input. The study focuses on using deep learning architectures, specifically convolutional neural networks (CNNs) and recurrent neural networks (RNNs), to predict fractional flow reserve (FFR) from resting coronary pressure curves. The models were trained and validated using a derivation cohort and a validation cohort, respectively, but there is no mention of combining outputs from multiple machine-learning methods to create a meta-predictor.",
  "optimization/encoding": "The data used for the machine-learning algorithm consisted of resting coronary pressure curves paired with their corresponding fractional flow reserve (FFR) values. These pressure curves were recorded under non-hyperaemic conditions, meaning they were taken without the administration of vasodilators.\n\nThe pressure curves were likely digitized and converted into a format suitable for input into neural networks. This process would involve sampling the continuous pressure signals at regular intervals to create a discrete time series. The sampling rate and any filtering or normalization steps applied to the data would be crucial for ensuring consistency and reducing noise.\n\nFor the convolutional neural network (CNN), the pressure curves were likely encoded as one-dimensional sequences. The CNN architecture was designed to automatically learn and identify relevant features within these sequences. The network consisted of multiple layers, each providing feature extraction at different levels of abstraction.\n\nIn addition to the CNN, recurrent neural networks (RNNs) were also employed. RNNs, particularly long short-term memory cells (LSTM) and gated recurrent units (GRU), are designed to incorporate temporal dependencies among features. This makes them well-suited for handling sequential data like pressure curves, where the temporal order of data points is important.\n\nThe deep learning models were implemented using scikit-learn in Python. This framework would have facilitated the preprocessing steps, such as scaling or normalizing the data, and the training of the neural networks. The models were trained using a derivation cohort, which was an 80% random sample of the total cohort, and validated against a separate 20% sample to ensure robustness and generalizability.\n\nTo reduce variance in the precision of the models, a fivefold cross-validation procedure was used. This involved splitting the derivation cohort into five subsets, training the model on four subsets, and validating it on the remaining subset. This process was repeated five times, with each subset serving as the validation set once. The performance metrics, such as diagnostic accuracy, sensitivity, specificity, positive predictive value, and negative predictive value, were then averaged across the five folds.",
  "optimization/parameters": "The study utilized two primary types of deep learning architectures: convolutional neural networks (CNNs) and recurrent neural networks (RNNs). For the CNN, the architecture consisted of five layers designed to extract features at different levels from the resting coronary pressure curves. Several variations of this CNN architecture were tested to optimize performance. The RNN architecture, on the other hand, included two variations: long short-term memory cells (LSTM) and gated recurrent units (GRU). These variations were used mutually exclusively to incorporate temporal dependencies among features.\n\nThe specific number of parameters (p) in each model was not explicitly stated, but the architectures were designed to automatically learn and identify relevant features from the input data. The selection of the number of parameters was likely influenced by the complexity of the task and the need to capture the intricate relationships within the pressure curves. The models were implemented using scikit-learn in Python, which provides a flexible framework for tuning hyperparameters and selecting the optimal architecture.\n\nThe choice of architectures and the number of parameters were guided by the goal of predicting fractional flow reserve (FFR) from resting pressure curves. The CNN was chosen for its ability to automatically learn features from the data, while the RNN was selected for its capacity to handle temporal dependencies. The use of fivefold cross-validation helped to ensure that the models generalized well to unseen data, reducing the variance in precision.",
  "optimization/features": "The input features for the deep learning models consisted of resting coronary pressure curves, specifically the distal coronary pressure (Pd) and aortic pressure (Pa). These pressure curves were used to train both the convolutional neural network (CNN) and the recurrent neural network (RNN) architectures.\n\nFeature selection was not explicitly mentioned as a separate process. Instead, the models were designed to automatically learn and identify relevant features from the pressure curves. The CNN architecture included multiple layers to extract features at different levels, while the RNN architecture was designed to incorporate temporal dependencies among the features.\n\nThe models were trained using a derivation cohort, which was an 80% random sample of the total cohort. This cohort was used to train the deep learning models with paired examples of resting coronary pressure curves and their corresponding fractional flow reserve (FFR) values. The remaining 20% of the cohort was used as a validation set to assess the performance of the trained models.\n\nThe use of a fivefold cross-validation procedure during training helped to reduce variance and ensure that the models generalized well to unseen data. This procedure involved splitting the derivation cohort into five subsets, training the model on four subsets, and validating it on the remaining subset. This process was repeated five times, with each subset serving as the validation set once.",
  "optimization/fitting": "The study employed deep learning models, specifically convolutional neural networks (CNNs) and recurrent neural networks (RNNs), to predict fractional flow reserve (FFR) from resting pressure curves. The number of parameters in these models is indeed larger than the number of training points, which is a common scenario in deep learning.\n\nTo address the risk of overfitting, several strategies were implemented. Firstly, a fivefold cross-validation procedure was used to reduce variance in the precision of the models. This technique ensures that the model's performance is evaluated on multiple subsets of the data, providing a more robust estimate of its generalization ability. Secondly, different variations of the neural network architectures were tested, including long short-term memory cells (LSTM) and gated recurrent units (GRU) for the RNNs. This approach helps to ensure that the results are not dependent on a specific architecture and reduces the likelihood of overfitting to the training data. Additionally, the diagnostic performance was compared against non-hyperaemic pressure ratios (NHPRs) using a McNemar test, which further validates the model's performance.\n\nRegarding underfitting, the use of deep neural networks with multiple layers and different architectures helps to capture complex patterns in the data. The models were trained on a large dataset of 1,666 patients with 1,718 coronary lesions and 2,928 coronary pressure tracings, which provides a substantial amount of data for training. The diagnostic accuracy of the models was found to be comparable to the most accurate NHPR, indicating that the models are capable of learning relevant features from the data without being too simplistic.\n\nIn summary, the study employed cross-validation, multiple neural network architectures, and comparison with established methods to address both overfitting and underfitting. The results suggest that the models are well-fitted to the data and provide a reliable prediction of FFR from resting pressure curves.",
  "optimization/regularization": "To prevent overfitting, a fivefold cross-validation procedure was employed. This technique helps to reduce the variance in the precision of the models by ensuring that the data is split into training and validation sets multiple times, providing a more robust evaluation of the model's performance. Additionally, the use of different neural network architectures, including convolutional neural networks (CNNs) and recurrent neural networks (RNNs), with variations such as long short-term memory cells (LSTM) and gated recurrent units (GRU), helped to explore various model complexities and prevent over-reliance on any single architecture. This approach ensures that the models generalize well to unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are not explicitly detailed in the main text. However, the architectures of the deep learning models, including the convolutional neural network (CNN) and recurrent neural network (RNN) variations, are described. A detailed description of these neural architectures is provided in Supplementary Appendix 1. Additionally, several variations of the CNN architecture were tested, as summarized in Supplementary Table 1.\n\nThe deep learning models were implemented using scikit-learn in Python. The specific model files and optimization schedules are not made available in the publication. The study does not provide information on where these configurations can be accessed or under what license they might be available. Therefore, while the general approach and some specifics of the model architectures are reported, the exact hyper-parameter configurations and optimization parameters are not publicly accessible.",
  "model/interpretability": "The models employed in our study, specifically the convolutional neural network (CNN) and the recurrent neural network (RNN), are generally considered black-box models. This means that while they can provide accurate predictions, the internal workings and the specific features they use to make these predictions are not easily interpretable.\n\nThe CNN, for instance, automatically learns and identifies features present among the resting coronary pressure curves through multiple layers designed for feature extraction at different levels. This process allows the CNN to capture complex patterns and interactions within the data, but it does not provide a clear, human-understandable explanation of how these patterns lead to the final predictions.\n\nSimilarly, the RNN, which includes variations like long short-term memory cells (LSTM) and gated recurrent units (GRU), is designed to incorporate temporal dependencies among features. This architecture is particularly useful for sequential data like pressure curves, as it can add information from previous intervals to the next. However, the exact mechanisms by which the RNN uses this temporal information to make predictions remain opaque.\n\nIn summary, while these deep learning models offer powerful tools for predicting FFR from resting pressure curves, their interpretability is limited. The models' ability to identify and utilize complex, non-pre-specified features contributes to their predictive power but also makes it challenging to understand the specific reasons behind their decisions.",
  "model/output": "The model encompasses both classification and regression tasks. Initially, it classifies resting pressure recordings into binary categories: FFR positive (FFR \u22640.80) or FFR negative (FFR >0.80). This classification is achieved using a convolutional neural network (CNN) and a recurrent neural network (RNN), with variations including long short-term memory cells (LSTM) and gated recurrent units (GRU). Additionally, the model predicts FFR as a continuous outcome, which falls under the regression category. The diagnostic performance of these models was evaluated using metrics such as accuracy, sensitivity, specificity, positive predictive value, and negative predictive value. The area under the receiver operating characteristic (ROC) curve was also analyzed to assess the model's predictive power for FFR as a continuous variable.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the deep learning models used in this study was implemented using scikit-learn in Python. However, the availability of the source code for public use is not specified. Therefore, it is not clear whether the source code is released or how it can be accessed.\n\nRegarding the method to run the algorithm, there is no information provided about the release of an executable, web server, virtual machine, or container instance. Thus, it is not possible to determine if such resources are available for running the algorithm.",
  "evaluation/method": "The evaluation method for our study involved a comprehensive approach to ensure the robustness and reliability of our deep learning models. We employed a fivefold cross-validation procedure to reduce variance in precision. This involved dividing the derivation cohort, which consisted of a random 80% sample of the total cohort, into five subsets. The model was trained on four of these subsets and validated on the remaining one, with this process repeated five times, each time using a different subset as the validation set.\n\nAfter training the neural networks, we validated their resulting algorithms against unseen resting pressure curves from a random 20% sample of the total cohort, referred to as the validation cohort. The primary endpoint of this validation was the diagnostic accuracy of the deep learning-derived algorithms against binary FFR \u22640.8. Additionally, we calculated sensitivity, specificity, positive predictive value, and negative predictive value, using FFR \u22640.80 as the reference standard.\n\nTo further assess the diagnostic performance, we presented the mean and standard deviation of the fivefold cross-validation procedure. We also compared the diagnostic performance of several non-hyperaemic pressure ratios using a McNemar test and calculated the mean and 95% confidence interval for their diagnostic performance.\n\nThe prediction of FFR as a continuous variable was analyzed using the area under the receiver operating characteristic (ROC) curve, compared using the DeLong method. All applicable tests were two-tailed, and a p-value of less than 0.05 was considered statistically significant. The analysis was conducted using R, version 3.4.3.",
  "evaluation/measure": "In the evaluation of our deep learning models, we focused on several key performance metrics to assess their diagnostic accuracy in predicting fractional flow reserve (FFR) from resting coronary pressure curves. The primary metric reported is diagnostic accuracy, which measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. Alongside accuracy, we also reported sensitivity, which indicates the ability of the model to correctly identify positive cases (FFR \u22640.80), and specificity, which measures the ability to correctly identify negative cases.\n\nAdditionally, we calculated the positive predictive value (PPV) and negative predictive value (NPV). PPV represents the probability that subjects with a positive screening test truly have the disease, while NPV represents the probability that subjects with a negative screening test truly do not have the disease.\n\nTo ensure robustness, these metrics were evaluated using a fivefold cross-validation procedure, providing mean values and standard deviations. This approach helps to reduce variance and provides a more reliable estimate of model performance.\n\nThe area under the receiver operating characteristic (ROC) curve (AUC) was also analyzed to evaluate the models' ability to distinguish between positive and negative cases across all possible classification thresholds. The AUC provides a single scalar value that summarizes the performance of the model across all thresholds.\n\nThese performance metrics are widely used in the literature for evaluating diagnostic tests and models, ensuring that our evaluation is representative and comparable to other studies in the field. The use of multiple metrics allows for a comprehensive assessment of model performance, covering different aspects of diagnostic accuracy.",
  "evaluation/comparison": "In our study, we compared the diagnostic performance of our deep learning-based algorithms, specifically a convolutional neural network (CNN) and a recurrent neural network (RNN), against several non-hyperaemic pressure ratios (NHPRs). These NHPRs included the ratio of distal coronary pressure to aortic pressure (Pd/Pa), instantaneous wave-free ratio (iFR), diastolic pressure ratio (dPR), and relative flow reserve (RFR). The comparison was conducted using a validation cohort consisting of a random 20% sample of the total cohort, which included 1,666 patients with 1,718 coronary lesions and 2,928 coronary pressure tracings.\n\nThe diagnostic accuracy of our CNN and RNN was evaluated against the binary endpoint of fractional flow reserve (FFR) \u22640.80. The CNN achieved an accuracy of 79.6\u00b11.9%, while the RNN had an accuracy of 77.6\u00b12.3%. These results were compared to the diagnostic accuracies of the NHPRs, which were 79.7% for Pd/Pa, 76.1% for iFR, 76.4% for dPR, and 76.3% for RFR. Notably, there was no statistically significant difference between the diagnostic accuracy of our neural networks and the most accurate NHPR (Pd/Pa), with p-values greater than 0.40 for both comparisons.\n\nAdditionally, we analyzed the area under the receiver operating characteristic (ROC) curve (AUC) for our deep learning models and the NHPRs. The CNN had an AUC of 0.88, and the RNN had an AUC of 0.84. These values were compared to the AUCs of the NHPRs, which were 0.86 for Pd/Pa, 0.84 for iFR, 0.85 for dPR, and 0.85 for RFR. The AUC of the CNN was significantly larger than those of the NHPRs, with a p-value of less than 0.01 according to the DeLong method. However, it is important to note that these comparisons were not pre-specified and were not adjusted for multiple comparisons.\n\nIn summary, while our deep learning models showed competitive performance compared to established NHPRs, they did not demonstrate a clinically relevant improvement in diagnostic accuracy for predicting FFR from resting coronary pressure curves. This suggests that the additional information captured by deep learning does not significantly enhance the prediction of FFR beyond what is achievable with simpler, non-hyperaemic pressure ratios.",
  "evaluation/confidence": "The evaluation of our deep learning models, including both convolutional neural networks (CNN) and recurrent neural networks (RNN), was conducted using a fivefold cross-validation procedure to ensure robustness and reduce variance in precision. The diagnostic performance metrics, such as accuracy, sensitivity, specificity, positive predictive value, and negative predictive value, were presented with their respective mean values and standard deviations.\n\nFor instance, the CNN achieved an accuracy of 79.6\u00b11.9%, sensitivity of 81.5\u00b13.2%, specificity of 77.1\u00b16.4%, positive predictive value of 80.6\u00b13.6%, and negative predictive value of 78.5\u00b12.4%. Similarly, the RNN had an accuracy of 77.6\u00b12.3%, sensitivity of 73.8\u00b16.1%, specificity of 81.5\u00b16.4%, positive predictive value of 82.6\u00b13.5%, and negative predictive value of 73.4\u00b13.8%.\n\nThe area under the receiver operating characteristic (ROC) curve was also calculated for both models, with the CNN achieving an AUC of 0.88 and the RNN an AUC of 0.84. These values were compared to other non-hyperaemic pressure ratios (NHPRs) using the DeLong method. The CNN's AUC was found to be larger than that of Pd/Pa (0.86), iFR (0.84), dPR (0.85), and RFR (0.85), with statistical significance (DeLong p<0.01 vs other NHPRs). However, it is important to note that these analyses were not pre-specified or adjusted for multiple comparisons.\n\nStatistical significance was assessed using applicable two-tailed tests, with a p-value threshold of less than 0.05 considered significant. For example, there was no statistically significant difference between the diagnostic accuracy of our neural networks and the most accurate NHPR (Pd/Pa), with p-values greater than 0.40 for both comparisons. This indicates that while our deep learning models performed well, they did not show a clinically relevant improvement over existing NHPRs.\n\nIn summary, the performance metrics were accompanied by confidence intervals, and statistical significance was rigorously evaluated. However, the results did not conclusively demonstrate the superiority of our deep learning models over established baselines in a clinically meaningful way.",
  "evaluation/availability": "Not enough information is available."
}