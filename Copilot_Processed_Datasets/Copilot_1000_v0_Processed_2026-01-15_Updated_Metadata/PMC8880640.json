{
  "publication/title": "Prognostic Value of Machine Learning in Patients with Acute Myocardial Infarction.",
  "publication/authors": "Xiao C, Guo Y, Zhao K, Liu S, He N, He Y, Guo S, Chen Z",
  "publication/journal": "Journal of cardiovascular development and disease",
  "publication/year": "2022",
  "publication/pmid": "35200709",
  "publication/pmcid": "PMC8880640",
  "publication/doi": "10.3390/jcdd9020056",
  "publication/tags": "- Acute Myocardial Infarction\n- Major Adverse Cardiovascular Events\n- Machine Learning\n- Logistic Regression\n- Predictive Modeling\n- Cardiovascular Disease\n- Prognostic Value\n- Medical Records\n- Data Imputation\n- Feature Selection\n- Model Development\n- Model Testing\n- Hyperparameter Optimization\n- Cross-Validation\n- Survival Analysis",
  "dataset/provenance": "The dataset used in this study was sourced from the hospital information system of Zhuzhou Central Hospital. Initially, 500 patients were collected who underwent coronary angiography and successful percutaneous coronary intervention (PCI), and were diagnosed with acute myocardial infarction (AMI) between August 2018 and December 2019. Patients who did not receive standard treatment and died during hospitalization were excluded. Follow-up was conducted from November to December 2020. Forty patients were lost to follow-up, and 52 patients lacked necessary inpatient testing items, resulting in a final dataset of 408 patients. The mean follow-up time was 1.42 years.\n\nThe dataset consisted of 41 clinical variables. The structured data was divided into training and testing datasets in a ratio of six to four. Imputation and normalization of data were carried out for these datasets. Missing values were handled differently for categorical and continuous variables. For categorical variables, missing values were imputed with a set of categories that retained the same categorical proportion as the existing values. For continuous variables, missing values were imputed with random numbers generated by a Gaussian distribution, using the mean and standard deviation of the existing values. If the random numbers were out of the range of the existing values, the missing values were imputed with the mean of the existing values. The min\u2013max scaler was used for normalization after data imputation.\n\nThe dataset was used to explore predictors of major adverse cardiac events (MACEs) in patients with AMI and to identify the most appropriate algorithm for prediction by comparing six machine learning algorithms with traditional logistic regression analysis. The dataset was not used in previous papers or by the community before this study.",
  "dataset/splits": "The dataset was divided into two main groups: one for model development and another for testing. Specifically, 60% of the patients' medical records were used for model development, while the remaining 40% were allocated for testing.\n\nDuring the model development phase, the training dataset was further divided using five-fold cross-validation. This process involved splitting the training data into five exclusive subsets. In each iteration of the cross-validation, four of these subsets were used for model development, and the remaining subset was used for model validation. This procedure was repeated five times to ensure comprehensive validation.\n\nThe distribution of data points in each split was designed to address class imbalances, which are common in dichotomous classification tasks. To mitigate this issue, different weights were assigned to each class in the training dataset, with these weights being inversely proportional to the class frequencies. This approach helped to improve the models' sensitivity and overall performance.",
  "dataset/redundancy": "The dataset used in this study consisted of 41 clinical variables. To ensure the robustness and generalizability of the models, the dataset was split into training and testing sets. Specifically, 60% of the patients' medical records were assigned to the training dataset, while the remaining 40% were allocated to the testing dataset. This split was done randomly to ensure that the datasets were independent of each other.\n\nTo enforce the independence of the training and testing sets, a five-fold cross-validation technique was employed during the model development phase. This involved dividing the training dataset into five exclusive subsets, using four subsets for model development and the remaining subset for model validation. This process was repeated five times, ensuring that each subset was used for validation exactly once. This approach helped in assessing the performance of the models more reliably and in reducing the risk of overfitting.\n\nThe distribution of the dataset in this study is comparable to previously published machine learning datasets in the field of cardiovascular research. The use of a structured dataset with a significant number of clinical variables allowed for comprehensive feature selection and model development. Features with missing values greater than 20% were discarded, and discrete variables with low variance were removed. Recursive feature elimination with random decision forest (RFE-RDF) was used to identify the optimal number of features, ensuring that the models were trained on the most relevant predictors.\n\nIn summary, the dataset was split into independent training and testing sets, with a five-fold cross-validation approach used to enhance the reliability of the models. The distribution and preprocessing of the dataset align with established practices in machine learning for cardiovascular research.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the supervised learning class. The specific algorithms employed include decision tree (DT), Naive Bayes (NB), support vector machine (SVM), random decision forest (RDF), gradient boosting (GB), and multilayer perceptron (MLP). These algorithms are well-established and widely used in the field of machine learning for various predictive tasks.\n\nThe algorithms used are not new; they are mainstream supervised machine-learning methods that have been extensively studied and applied in different domains, including healthcare. These algorithms were chosen for their proven effectiveness in handling classification problems and their ability to process large-scale data.\n\nThe decision to use these established algorithms in a cardiovascular study rather than a machine-learning journal is driven by the specific application and goals of the research. The focus of this study is on predicting major adverse cardiac events (MACEs) in patients with acute myocardial infarction (AMI), rather than on developing new machine-learning techniques. The algorithms were selected for their robustness, accuracy, and suitability for the medical data at hand. The study aims to compare these algorithms with traditional logistic regression (LR) to identify the most appropriate model for predicting MACEs, thereby contributing to the advancement of cardiovascular disease management.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for preparing the dataset for the machine-learning algorithms. Initially, the dataset consisted of 41 clinical variables, and feature selection was performed exclusively on the training dataset. Features with missing values greater than 20% were discarded, and discrete variables with variance below a threshold of 0.09 were removed. This threshold was determined using the calculation method for Bernoulli random variables.\n\nFor imputing missing values, different strategies were employed based on the type of variable. For categorical variables, the proportion of each category in the existing values was calculated, and missing values were imputed to retain the same categorical proportion. Continuous variables were imputed with random numbers generated from a Gaussian distribution, using the mean and standard deviation of the existing values. If the generated random numbers fell outside the range of the existing values, the missing values were imputed with the mean of the existing values.\n\nDue to the wide distribution range of the diverse predictors and the requirement for normalization in some prediction models, a min\u2013max scaler was applied after data imputation. This normalization process ensured that all variables were on a comparable scale, which is essential for the effective training of machine-learning models.\n\nThe structured dataset was then divided into training and testing datasets in a 60:40 ratio. The training dataset was further divided into five exclusive subsets using five-fold cross-validation, with four subsets used for model development and one for validation. This process was repeated five times to ensure robust model performance assessment.\n\nTo address class imbalances, which are common in dichotomous classification and can lead to poor sensitivity, different weights were assigned to each class in the training dataset. These weights were inversely proportional to class frequencies, helping to mitigate the impact of imbalanced data on model performance.\n\nThe machine-learning algorithms used included decision tree (DT), Naive Bayes (NB), support vector machine (SVM), random decision forest (RDF), gradient boosting (GB), and multilayer perceptron (MLP). Hyperparameters for these models were optimized using five-fold cross-validation and manual grid search. For the MLP models, parameters such as epoch, batch size, learning rate, momentum, activation function, dropout rate, and the number of hidden layer neurons were tuned to enhance model performance.\n\nIn summary, the data encoding and preprocessing involved careful handling of missing values, normalization, feature selection, and addressing class imbalances. These steps were essential for preparing the data to train and validate the machine-learning models effectively.",
  "optimization/parameters": "In our study, we utilized a structured dataset consisting of 41 clinical variables for model development. Feature selection was performed exclusively on the training dataset to identify the most relevant predictors. Initially, features with missing values exceeding 20% were discarded, and discrete variables with variance below a threshold of 0.09 were removed. Subsequently, recursive feature elimination with random decision forest (RFE-RDF) was employed to determine the optimal number of features.\n\nHyperparameters were optimized using five-fold cross-validation for five machine learning models, including decision tree (DT), naive Bayes (NB), support vector machine (SVM), random decision forest (RDF), and gradient boosting (GB). For the multilayer perceptron (MLP) models, a manual grid search was conducted to tune parameters such as epoch, batch size, learning rate, momentum, activation function, dropout rate, and the number of hidden layer neurons. This process ensured that the selected features and hyperparameters were optimized for predictive performance.",
  "optimization/features": "The study utilized a structured dataset comprising 41 clinical variables. Feature selection was indeed performed, but it was exclusively conducted on the training dataset. This process involved discarding features with missing values exceeding 20% and removing discrete variables whose variance did not meet a specified threshold. Additionally, recursive feature elimination with random decision forest (RFE-RDF) was employed to identify the optimal number of features. This approach ensured that the selected features were robust and relevant for predicting major adverse cardiovascular events (MACEs) in patients with acute myocardial infarction (AMI).",
  "optimization/fitting": "The study employed several machine learning models, including decision trees, naive Bayes, support vector machines, random decision forests, gradient boosting, and multilayer perceptrons, alongside traditional logistic regression. The dataset consisted of 408 patients, with a 60-40 split for training and testing, respectively.\n\nTo address the potential issue of overfitting, especially given the relatively small sample size, several strategies were implemented. First, hyperparameters were optimized using five-fold cross-validation for most models, ensuring that the models generalized well to unseen data. Additionally, a manual grid search was used for parameter optimization in multilayer perceptron models, tuning parameters such as epoch, batch size, learning rate, and dropout rate. This process helped in selecting the best hyperparameters that minimized overfitting.\n\nFurthermore, the study used techniques like recursive feature elimination with random decision forests to identify the optimal number of features, reducing the risk of overfitting by avoiding the inclusion of irrelevant features. The use of a min-max scaler for normalization also helped in standardizing the data, which is crucial for models like support vector machines and neural networks to perform effectively without overfitting.\n\nTo mitigate underfitting, the models were evaluated using multiple performance metrics, including AUC, accuracy, and F1-score. The random decision forest model, in particular, showed the best discrimination with an AUC of 0.749 in the validation dataset and 0.68 in the testing dataset, indicating a good balance between bias and variance. The calibration of models using Platt's scaling and the assessment of the Brier score further ensured that the models were well-calibrated and not underfitting the data.\n\nIn summary, the study carefully managed the risk of overfitting and underfitting through rigorous cross-validation, feature selection, and model calibration techniques, ensuring robust and reliable predictive models for MACEs in patients with AMI.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our models. One key method used was five-fold cross-validation during the hyperparameter optimization process. This technique involves dividing the training dataset into five exclusive subsets, using four for model development and the remaining one for validation. This process is repeated five times, ensuring that each subset is used for validation once. This approach helps to assess the model's performance more reliably and reduces the risk of overfitting.\n\nAdditionally, we addressed class imbalances by assigning different weights to each class in the training dataset. These weights were inversely proportional to class frequencies, which helped the models to better learn from the minority class and improve their sensitivity.\n\nFor the multilayer perceptron (MLP) models, a manual grid search was conducted to tune various parameters, including the number of hidden layer neurons, learning rate, and dropout rate. The dropout technique, in particular, is a regularization method that prevents overfitting by randomly setting a fraction of input units to zero at each update during training time, which helps to prevent units from co-adapting too much.\n\nFurthermore, Platt's scaling was used to calibrate the probabilities of the models, which helps to improve the reliability of the predicted probabilities and reduces the risk of overfitting. The calibration was measured using the Brier score, which provides an indication of how well the predicted probabilities match the actual outcomes.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule are reported in the publication. The hyperparameters were optimized using five-fold cross-validation for machine learning models, including decision trees, naive Bayes, support vector machines, random decision forests, and gradient boosting. For multilayer perceptron models, a manual grid search was employed to tune parameters such as epoch, batch size, learning rate, momentum, activation function, dropout rate, and the number of hidden layer neurons.\n\nThe specific details of the hyper-parameter configurations and optimization schedules are not explicitly listed in the main text but are referenced in supplementary materials, specifically in Table S2. This table likely contains the exact values and settings used for each model during the optimization process.\n\nRegarding model files and optimization parameters, these are not directly available in the publication. The implementation details for the models, including logistic regression, decision trees, naive Bayes, support vector machines, random decision forests, gradient boosting, and multilayer perceptrons, were carried out using specific software versions: scikit-learn v0.19.1 for most models and Keras v2.2.4 for multilayer perceptrons. The analyses were performed using Anaconda3-5.1.0-Windows.\n\nFor access to the supplementary materials, including Table S2, readers would typically need to refer to the journal's website or the publication's repository, where such materials are often made available under the journal's standard licensing agreements. These agreements usually allow for academic use and citation, ensuring that the methods and results can be replicated and verified by other researchers.",
  "model/interpretability": "The models developed in this study include both transparent and black-box approaches. Among the machine learning models, decision trees (DT) and random forest (RDF) are relatively interpretable. Decision trees provide a clear, visual representation of the decision-making process, showing how different features influence the outcome. Each branch of the tree represents a decision based on a feature, making it easy to trace the path to a specific prediction.\n\nRandom forests, while more complex than individual decision trees, still offer some level of interpretability. They consist of multiple decision trees, and the final prediction is an aggregate of the individual tree predictions. Feature importance can be extracted from random forests, indicating which features are most influential in making predictions. This allows for an understanding of the key drivers behind the model's decisions.\n\nIn contrast, models like support vector machines (SVM), gradient boosting (GB), and multilayer perceptrons (MLP) are considered black-box models. These models do not provide a straightforward way to interpret how they arrive at their predictions. For instance, SVMs find a hyperplane that best separates the classes, but the process and the features contributing to this separation are not easily interpretable. Similarly, gradient boosting and multilayer perceptrons involve complex interactions and transformations of the input features, making it difficult to trace the decision-making process.\n\nLogistic regression (LR), on the other hand, is a transparent model. It provides coefficients for each feature, indicating the direction and magnitude of their influence on the outcome. Positive coefficients suggest a positive relationship with the outcome, while negative coefficients indicate a negative relationship. This makes it straightforward to interpret the impact of each feature on the prediction of major adverse cardiovascular events (MACEs).\n\nOverall, while some models offer clear interpretability, others remain black-box, requiring additional techniques or simplifications to understand their decision-making processes.",
  "model/output": "The model is a classification model designed to predict major adverse cardiovascular events (MACEs) in patients with acute myocardial infarction (AMI). Several machine learning (ML) models were developed and validated, including logistic regression (LR), decision tree (DT), naive Bayes (NB), support vector machine (SVM), random forest (RDF), gradient boosting (GB), and multilayer perceptron (MLP). The performance of these models was evaluated using metrics such as the area under the curve (AUC), accuracy, and F1-score.\n\nIn the validation dataset, the random forest (RDF) model demonstrated the best discrimination with an AUC of 0.749, an accuracy of 0.734, and an F1-score of 0.480. This model also showed superior generalization ability in the testing dataset, with an AUC of 0.68, an accuracy of 0.68, and an F1-score of 0.48. The RDF model outperformed the other models in terms of both accuracy and AUC value, making it the most effective for predicting MACEs.\n\nCalibration plots indicated that most models showed modest concordance between the mean predictive probability and the fraction of MACEs. The logistic regression (LR) and gradient boosting (GB) models exhibited good alignment with R2 values of 0.83 and 0.81, respectively. However, the predictive probability of MACEs in LR was limited below 0.75. The naive Bayes (NB), support vector machine (SVM), and random forest (RDF) models showed modest calibration, while the decision tree (DT) and multilayer perceptron (MLP) models had poor calibration.\n\nKaplan\u2013Meier analysis revealed that the naive Bayes (NB), random forest (RDF), and gradient boosting (GB) models showed significant differences in survival distribution across two subgroups. The random forest (RDF) model had the smallest p-value (log-rank p = 0.017) and was the most appropriate for distinguishing MACEs. In contrast, the logistic regression (LR), decision tree (DT), support vector machine (SVM), and multilayer perceptron (MLP) models did not exhibit obvious differences in survival distribution.\n\nOverall, the random forest (RDF) model demonstrated the greatest predictive and discriminative advantages compared to the other models. This model can help accurately predict the prognoses of AMI patients, thereby strengthening individualized treatment and improving patient outcomes.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved several key steps to ensure the robustness and reliability of the models developed for predicting major adverse cardiovascular events (MACEs) in patients with acute myocardial infarction (AMI).\n\nThe dataset was divided into two main groups: 60% of the patients were assigned to model development, and 40% were reserved for testing. This split allowed for a comprehensive evaluation of the models' performance on unseen data.\n\nHyperparameter optimization was conducted using five-fold cross-validation for five machine learning (ML) models, including decision tree (DT), naive Bayes (NB), support vector machine (SVM), random decision forest (RDF), and gradient boosting (GB). For the multilayer perceptron (MLP) models, a manual grid search was used to optimize parameters such as epoch, batch size, learning rate, momentum, activation function, dropout rate, and the number of hidden layer neurons. The training dataset was divided into five exclusive subsets, with four used for model development and one for validation. This process was repeated five times to ensure thorough validation.\n\nTo address class imbalances, which are common in dichotomous classification and can lead to poor sensitivity, different weights were assigned to each class in the training dataset. These weights were inversely proportional to class frequencies, helping to mitigate the impact of imbalanced data.\n\nThe performance of the models was evaluated using several metrics, including the area under the curve (AUC), accuracy, and F1-score. These metrics were calculated in the testing dataset to assess the models' predictive capabilities. Additionally, the probabilities of the models were calibrated using Platt\u2019s scaling, and the calibration was measured by the Brier score.\n\nTo further evaluate the discriminative ability of the models, patients were divided into two subgroups based on predictive probability. The first subgroup included patients predicted to have a good prognosis (predictive probability less than 0.5), while the second subgroup included those predicted to have a poor prognosis (predictive probability greater than 0.5). The cumulative survival of MACEs was compared across these groups using Kaplan\u2013Meier analysis. This analysis was also performed on randomly divided subgroups within the testing dataset to verify existing survival differences.\n\nOverall, the evaluation method involved a rigorous process of cross-validation, hyperparameter optimization, and comprehensive performance metrics to ensure the models' reliability and predictive accuracy.",
  "evaluation/measure": "In our study, we employed several performance metrics to comprehensively evaluate the models used for predicting major adverse cardiovascular events (MACEs). The primary metrics reported include the Area Under the Curve (AUC), accuracy, and F1-score. These metrics were chosen for their ability to provide a well-rounded assessment of model performance.\n\nThe AUC is a critical indicator that evaluates the model's discrimination capability, reflecting its ability to distinguish between positive and negative cases. It is particularly useful in imbalanced datasets, which are common in medical research. We reported the AUC with 95% confidence intervals to provide a measure of uncertainty around the point estimate.\n\nAccuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. While accuracy is a straightforward metric, it can be misleading in imbalanced datasets. Therefore, we complemented it with the F1-score, which is the harmonic mean of precision and recall. The F1-score is particularly useful when the classes are imbalanced, as it provides a balance between precision and recall.\n\nIn addition to these metrics, we also considered the calibration of the models. Calibration plots and the Brier score were used to assess how well the predicted probabilities matched the actual outcomes. Good calibration indicates that the model's predicted probabilities are reliable.\n\nThe set of metrics used in this study is representative of the literature on model evaluation in medical research. AUC, accuracy, and F1-score are commonly reported metrics that provide a comprehensive view of model performance. The inclusion of calibration metrics further ensures that the models are not only accurate but also reliable in their predictions.\n\nOverall, the combination of these metrics allows for a thorough evaluation of the models' performance, ensuring that they are robust and reliable for predicting MACEs in patients with acute myocardial infarction.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of various machine learning algorithms to identify the most appropriate model for predicting major adverse cardiovascular events (MACEs) in patients with acute myocardial infarction (AMI). We evaluated six machine learning approaches\u2014decision tree (DT), naive Bayes (NB), support vector machine (SVM), random forest (RDF), gradient boosting (GB), and multilayer perceptron (MLP)\u2014against traditional logistic regression (LR) analysis.\n\nThe comparison was performed using a dataset of 408 patients who underwent coronary angiography and successful percutaneous coronary intervention (PCI) and were diagnosed with AMI. The dataset was divided into training and testing subsets, with a ratio of six to four, respectively. We employed five-fold cross-validation to optimize hyperparameters and assess model performance metrics such as the area under the curve (AUC), accuracy, and F1-score.\n\nTo address class imbalances, we assigned different weights inversely proportional to class frequencies in the training dataset. This approach helped mitigate the issue of poor sensitivity often encountered in dichotomous classification tasks.\n\nThe performance of each model was evaluated in both the validation and testing datasets. In the validation dataset, the random forest model demonstrated the best discrimination with an AUC of 0.749, accuracy of 0.734, and F1-score of 0.480. Similarly, in the testing dataset, the random forest model showed superior generalization ability with an AUC of 0.68, accuracy of 0.68, and F1-score of 0.48 compared to the other models.\n\nAdditionally, we performed calibration plots to assess the concordance between the mean predictive probability and the fraction of MACEs. The logistic regression and gradient boosting models exhibited good alignment, while the naive Bayes, support vector machine, and random forest models showed modest calibration. The decision tree and multilayer perceptron models had poor calibration, indicating the need for further refinement.\n\nIn summary, our study involved a thorough comparison of multiple machine learning algorithms against traditional logistic regression to identify the most effective model for predicting MACEs in AMI patients. The random forest model emerged as the most robust, demonstrating superior performance in both validation and testing datasets.",
  "evaluation/confidence": "The evaluation of our models included several performance metrics, each accompanied by confidence intervals to provide a measure of reliability. For instance, the area under the curve (AUC), accuracy, and F1-score were reported with 95% confidence intervals (CI) for various models. This approach ensures that the performance metrics are not just point estimates but are accompanied by a range within which the true value is likely to fall.\n\nStatistical significance was assessed to determine if the differences in performance between models were meaningful. For example, the random forest (RDF) model demonstrated the best discrimination with an AUC of 0.749 (95% CI, 0.644\u20130.85), accuracy of 0.734 (0.647\u20130.820), and F1-score of 0.480 (0.358\u20130.603) in the validation dataset. These metrics, along with their confidence intervals, indicate that the RDF model's performance is statistically significant and superior to other models evaluated.\n\nAdditionally, the Kaplan\u2013Meier analysis provided further evidence of the models' discriminative ability. The Naive Bayes (NB), random forest (RDF), and gradient boosting (GB) models showed significant differences in survival distribution across subgroups, with the RDF model exhibiting the smallest p-value (log-rank p = 0.017). This statistical significance supports the claim that the RDF model is the most appropriate for distinguishing major adverse cardiovascular events (MACEs).\n\nOverall, the use of confidence intervals and statistical significance testing enhances the confidence in the evaluation results, ensuring that the claims of model superiority are robust and reliable.",
  "evaluation/availability": "Not enough information is available."
}