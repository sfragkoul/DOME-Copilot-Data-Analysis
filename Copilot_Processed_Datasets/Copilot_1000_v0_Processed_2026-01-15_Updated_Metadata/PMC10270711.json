{
  "publication/title": "Multiclass characterization of frontotemporal dementia variants via multimodal brain network computational inference.",
  "publication/authors": "Gonzalez-Gomez R, Iba\u00f1ez A, Moguilner S",
  "publication/journal": "Network neuroscience (Cambridge, Mass.)",
  "publication/year": "2023",
  "publication/pmid": "37333999",
  "publication/pmcid": "PMC10270711",
  "publication/doi": "10.1162/netn_a_00285",
  "publication/tags": "- Frontotemporal Dementia\n- Machine Learning\n- XGBoost\n- Brain Connectivity\n- Functional MRI\n- Structural MRI\n- Multiclass Classification\n- Feature Selection\n- Cognitive Assessment\n- Dementia Variants\n- Bayesian Optimization\n- Network Analysis\n- Neuroimaging\n- Gradient Boosting\n- ROC Curves\n- AUC Metrics\n- Demographic Factors\n- Cognitive Measures\n- Brain Parcellation\n- Disease Progression",
  "dataset/provenance": "The dataset used in this study was obtained from the Laboratory of Neuro Imaging (LONI) databases, specifically from the Neuroimaging in Frontotemporal Dementia (NIFD) and the 4 Repeat Tauopathy Neuroimaging Initiative (4RTNI). These databases are part of the frontotemporal lobar degeneration neuroimaging initiative.\n\nThe study analyzed a total of 298 subjects, encompassing various frontotemporal dementia (FTD) variants and healthy controls. The FTD variants included behavioral variant FTD (bvFTD), corticobasal syndrome (CBS), nonfluent variant primary progressive aphasia (nfvPPA), progressive supranuclear palsy (PSP), and semantic variant primary progressive aphasia (svPPA). The dataset also included healthy controls for comparison.\n\nThe data used in this study has been utilized in previous research within the community, particularly in studies focusing on FTD variants. The inclusion of these datasets allows for a robust analysis and comparison with existing literature, ensuring that the findings are grounded in well-established research practices.",
  "dataset/splits": "The dataset was divided into two main splits: a training/validation set and a testing set. The training/validation set comprised 80% of the total sample, while the testing set consisted of the remaining 20%. Within the training/validation set, a k-fold cross-validation approach was employed, specifically with k=5 nonoverlapping folds. This method involved alternating between nested training sets and validation sets to fine-tune hyperparameters.\n\nThe total number of subjects analyzed was 298, which included individuals with various frontotemporal dementia (FTD) variants and healthy controls. The specific distribution of subjects across the different groups was as follows: behavioral variant FTD (bvFTD) with 47 subjects, corticobasal syndrome (CBS) with 38 subjects, nonfluent variant primary progressive aphasia (nfvPPA) with 34 subjects, progressive supranuclear palsy (PSP) with 42 subjects, semantic variant primary progressive aphasia (svPPA) with 38 subjects, and healthy controls (HC) with 99 subjects.\n\nThe training/validation set, therefore, included approximately 238 subjects, and the testing set included the remaining 60 subjects. The distribution of subjects in each fold of the cross-validation process was not explicitly detailed, but it can be inferred that each fold contained a roughly equal proportion of the total training/validation set, ensuring that the model was trained and validated on diverse subsets of the data.",
  "dataset/redundancy": "The datasets were split into training and test sets to ensure independence between them. This was enforced through the use of nested k-folds, specifically with k=5, which helps in validating the model's performance and feature stability. The training set comprised 80% of the total sample, while the remaining 20% was used for testing. This approach was implemented to avoid overfitting and to ensure that the model generalizes well to unseen data.\n\nThe distribution of the datasets was carefully considered to match the characteristics of previously published machine learning datasets in similar domains. This included ensuring that the datasets were representative of the different FTD variants and controls, with a focus on maintaining a balanced distribution of demographic and cognitive features. The use of group-level statistical analyses before data-driven progressive elimination helped in reducing dimensionality and overcoming computational constraints, aligning with hybrid methodologies reported in the literature.\n\nTo further ensure robustness, the models were evaluated using a variety of MRI acquisition protocols. This step was crucial in assessing the method's resilience against heterogeneity in the data. Additionally, the inclusion of data from underrepresented populations with diverse genetic, demographic, and socioeconomic factors was considered to test the models' generalizability. The datasets included subjects with different FTD variants, such as behavioral variant FTD, corticobasal syndrome, nonfluent variant primary progressive aphasia, progressive supranuclear palsy, semantic variant primary progressive aphasia, and healthy controls. This comprehensive approach aimed to develop a multiclass computational framework that could accurately characterize each FTD variant simultaneously against all other variants and controls.",
  "dataset/availability": "The datasets utilized in this study are available in their own online repository, specifically the Neuroimaging in Frontotemporal Dementia (NIFD) database hosted by LONI. This repository can be accessed via the provided link. The data includes information from the Neuroimaging in Frontotemporal Dementia (NIFD) and the 4 Repeat Tauopathy Neuroimaging Initiative (4RTNI), both of which are part of the frontotemporal lobar degeneration neuroimaging initiative.\n\nThe data was obtained from these databases, ensuring that the clinical diagnosis of the frontotemporal dementia (FTD) variants was based on current criteria. Patients included in the study did not present any vascular, psychiatric, or other neurological disorders, and the inclusion of healthy subjects required confirmation of normal cognitive function.\n\nThe code for the data analysis of this study is available from the corresponding author upon reasonable request. This approach ensures that the methods and findings can be replicated and verified by other researchers in the field. The availability of the datasets and the analysis code promotes transparency and reproducibility in scientific research.",
  "optimization/algorithm": "The machine-learning algorithm class used is gradient boosting machines, specifically the XGBoost algorithm. This algorithm is not new; it has been previously established and has proven successful in various diagnostic applications. The choice to use XGBoost was driven by its ability to provide parallel computation tree boosting, enabling fast and accurate predictions. Additionally, XGBoost incorporates advanced regularization techniques to mitigate overfitting, which is crucial for ensuring the generalizability of the results. The algorithm's effectiveness in handling large datasets with numerous features makes it well-suited for the complex task of classifying different clinical groups based on multimodal connectivity data. The decision to use XGBoost was also influenced by its demonstrated success in similar diagnostic applications, highlighting its reliability and robustness.",
  "optimization/meta": "The model employed in this study does not function as a meta-predictor. Instead, it utilizes a single machine-learning algorithm, specifically the XGBoost algorithm, for classification tasks. XGBoost is a gradient boosting machine implementation that provides parallel computation tree boosting, enabling fast and accurate predictions. It incorporates advanced regularization techniques to mitigate overfitting, making it a robust choice for diagnostic applications.\n\nThe study involved evaluating the individual contributions of three methods of connectivity in two modalities (functional and structural) to characterize pathological groups. Each algorithm was executed twice: once with connectivity data alone and once with demographic and cognitive features. Additionally, a model combining all methods and modalities was created. In total, 14 data-driven models were used for the classification of one class relative to the remaining subjects.\n\nThe dataset was divided into training, validation, and testing sets. 80% of the sample was used for training and validation, while 20% was reserved for testing. Within the training set, k-fold cross-validation (with k = 5 nonoverlapping folds) was performed to tune hyperparameters. This approach ensured that the training data was independent from the testing data, providing an unbiased and accurate performance estimation.\n\nThe performance of the classifiers was evaluated using receiver operating characteristic (ROC) curves and the area under the ROC curve (AUC) value. The pipeline divided the original dataset into six binary datasets, where the positive class in each dataset corresponded to the group of interest, and the negative class was composed of all other groups. For each dataset, performance was evaluated through the micro-average AUC, which takes into account the imbalance between classes and produces an unbiased performance metric.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to handle the extensive computational requirements and to ensure the generalizability of the models. Initially, we evaluated the contributions of three methods of connectivity in two modalities: functional and structural. Each algorithm was run twice, once with connectivity data alone and once with additional demographic and cognitive features. This resulted in a total of 14 data-driven models for classification.\n\nGiven the vast number of multimodal features, on the order of 10^6, we employed dimensionality reduction techniques to manage computational time and memory requirements. We used a group-level statistical analysis, also known as the filter method, to reduce dimensionality. This approach allowed for a more direct interpretation of the results and is computationally inexpensive. It also enabled us to compare our findings with previous research.\n\nFor each modality and method, we calculated all possible group comparisons with a statistical power threshold of 0.80 for detecting medium effect sizes. The significant results from these comparisons were used as inputs for the machine-learning algorithms in our training sample. In the case of raw functional connectivity, the results were averaged for significant clusters across individual connectivity maps.\n\nTo extract the most relevant features, we performed a progressive feature elimination approach in the training set, which comprised 80% of the total sample. We used a k-fold scheme with nested training and validation to select the optimal set of features after stabilization. At each iteration, Gini scores were used to eliminate the least important features, while evaluating feature stability on each nested fold. The variability of the feature ranking in the importance list was assessed across nested k-folds. We kept the first N features in the ranking, where N is the optimal number of features such that using more than N features did not improve classifier performance.\n\nThe preprocessing steps ensured that the data was efficiently encoded and reduced in dimensionality, allowing for effective training and validation of the machine-learning models. This approach helped in identifying the most relevant features and improving the classification performance.",
  "optimization/parameters": "In our study, the XGBoost algorithm was employed for classification, which involves several hyperparameters. These include the number of subtrees to retain, maximum tree depth, learning rate, minimum loss reduction required to further partition a leaf node, maximum number of leaves, and regularization weights. To determine the optimal combination of these hyperparameters, Bayesian optimization was utilized. This method is iterative and involves a probabilistic surrogate model and an acquisition function to decide which point to evaluate next. The process continues until the chances of finding a better solution increase significantly. Bayesian optimization was chosen over other techniques like grid search or random search due to its efficiency and ability to handle high-dimensional spaces effectively. The final set of hyperparameters was selected based on their performance during the optimization process, ensuring that the model generalizes well to unseen data.",
  "optimization/features": "In our study, the total number of multimodal features was initially on the order of 10^6, which included voxel-wise variables. Given the extensive computational time and memory requirements, as well as the risk of overfitting, we employed feature selection techniques.\n\nFeature selection was indeed performed to reduce dimensionality. We used a group-level statistical analysis, also known as the filter method, to initially reduce the number of features. This approach allowed for a more direct interpretation of the results and is computationally inexpensive. It also enabled us to compare our findings with previous research.\n\nFollowing the initial reduction, we performed a progressive feature elimination approach in the training set. This process involved using a k-fold scheme with nested training and validation to select the optimal set of features after stabilization. At each iteration, Gini scores were used to eliminate the least important features, while evaluating feature stability on each nested fold. The variability of the feature ranking in the importance list was assessed across nested k-folds. We kept the first N features in the ranking, where N is the optimal number of features such that using more than N features did not improve classifier performance.\n\nTherefore, the final number of features used as input for the classification models was determined through this progressive elimination process, ensuring that only the most relevant features were retained. This approach helped in enhancing classification accuracy and providing a top assortment of features to classify outcomes.",
  "optimization/fitting": "The number of parameters in our study was indeed much larger than the number of training points, given the high-dimensional nature of our data, with features on the order of 106. To address potential overfitting, we employed several strategies. Firstly, we used regularized boosting with XGBoost, which includes advanced regularization techniques to prevent overfitting. Secondly, we performed k-fold cross-validation with k=5, ensuring that our model's performance was evaluated on multiple validation sets. Additionally, we used Bayesian optimization to tune hyperparameters, which is more efficient and effective than grid or random search methods. This approach helped in finding the optimal hyperparameter combination that generalizes well to unseen data.\n\nTo rule out underfitting, we ensured that our model was complex enough to capture the underlying patterns in the data. We used a progressive feature elimination approach to select the most relevant features, which helped in retaining the necessary complexity. Moreover, the use of XGBoost, a powerful gradient boosting machine, allowed us to build an ensemble of decision trees that iteratively corrected classification errors, thus enhancing the model's capacity to learn from the data.\n\nFurthermore, we divided our dataset into training, validation, and testing sets, following best practices in machine learning. The training set was used to fit the model, the validation set to tune hyperparameters and select features, and the independent test set to provide an unbiased estimate of the model's performance. This rigorous approach helped in ensuring that our model neither overfitted nor underfitted the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the generalizability of our results. One of the key methods used was regularized boosting with the XGBoost algorithm. This approach helps to reduce overfitting by incorporating regularization terms that penalize complex models, thereby promoting simpler and more generalizable models.\n\nAdditionally, we followed best practices in machine learning by splitting our dataset into training, validation, and testing sets. Specifically, we used 80% of the sample for training and validation, and 20% for testing. Within the training set, we performed k-fold cross-validation with k=5 nonoverlapping folds. This process involved alternating between nested training sets and validation sets to tune hyperparameters, ensuring that our model's performance was robust and not merely a result of overfitting to the training data.\n\nTo further optimize hyperparameters, we utilized Bayesian optimization. This iterative algorithm employs a probabilistic surrogate model and an acquisition function to decide which hyperparameters to evaluate next. Unlike grid search or random search, Bayesian optimization is efficient and thorough, enabling a comprehensive exploration of the hyperparameter space without the computational burden.\n\nThese techniques collectively ensured that our models were well-generalized and not overfitted to the training data, providing reliable and accurate performance estimates.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are thoroughly documented. We employed Bayesian optimization to fine-tune the hyperparameters of the XGBoost algorithm. This method is detailed in our publication, including the specific hyperparameters adjusted, such as the number of subtrees, maximum tree depth, learning rate, and regularization weights.\n\nThe optimization process involved an iterative algorithm with a probabilistic surrogate model and an acquisition function. This approach allowed for efficient exploration of the hyperparameter space, ensuring a comprehensive optimization.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly provided in the publication. However, the methods and results are described in sufficient detail to replicate the study. The code and specific configurations used for the optimization are not publicly available, and there is no mention of a specific license for the methods or tools used.\n\nFor those interested in replicating our work, the detailed descriptions of the methods and results should serve as a comprehensive guide. However, for direct access to the model files or optimization parameters, additional inquiries would be necessary.",
  "model/interpretability": "The models employed in our study, particularly the XGBoost algorithm, are not entirely transparent and can be considered somewhat of a black box. XGBoost is a gradient boosting machine implementation that uses ensembles of decision trees to make predictions. While decision trees themselves are interpretable, the ensemble nature of gradient boosting can make the overall model more complex and less transparent.\n\nHowever, there are aspects of our approach that enhance interpretability. For instance, we used a filter method for dimensionality reduction, which allows for a more direct interpretation of the results compared to other methods like principal component analysis. This approach helps in understanding the individual contributions of different features to the model's predictions.\n\nAdditionally, we performed a progressive feature elimination approach to select the optimum set of features. This process involves evaluating feature stability and importance across nested k-folds, which provides insights into which features are most relevant for classification. The use of Gini scores to eliminate the least important features further aids in understanding the model's decision-making process.\n\nMoreover, our use of multimodal data, including functional and structural connectivity, along with demographic and cognitive features, allows for a comprehensive characterization of the pathological groups. This multimodal approach can provide a more holistic understanding of the underlying mechanisms, even if the individual model remains somewhat opaque.\n\nIn summary, while the XGBoost algorithm itself is not fully transparent, our methodology includes steps that enhance interpretability, such as feature selection and the use of a filter method for dimensionality reduction. These steps help in understanding the contributions of different features and the overall decision-making process of the model.",
  "model/output": "The model employed in this study is a classification model. Specifically, it is designed for multiclass classification, aiming to distinguish between different variants of frontotemporal dementias (FTD) and healthy controls. The XGBoost algorithm, a gradient boosting machine implementation, was used to classify the various clinical groups based on selected features. This algorithm is well-suited for handling complex, nonlinear relationships in the data, making it effective for diagnostic applications. The performance of the model was evaluated using metrics such as the area under the ROC curve (AUC), accuracy, sensitivity, specificity, and F1 score. The top-performing models included those using raw functional connectivity data, both with and without additional demographic and cognitive variables. The study also explored the use of multimodal data, combining functional and structural connectivity, to enhance classification performance. Overall, the model demonstrated robust classification capabilities across different FTD variants and healthy controls.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in our study followed best practices in machine learning, ensuring robust and unbiased performance estimation. We divided our dataset into training, validation, and testing sets, with 80% of the sample used for training and validation, and the remaining 20% reserved for independent testing. Within the training set, we performed 5-fold cross-validation to tune hyperparameters, using nested training and validation sets to prevent data leakage.\n\nTo evaluate the performance of our classifiers, we utilized receiver operating characteristic (ROC) curves, which plot the true positive rate against the false-positive rate. The area under the ROC curve (AUC) was used as a primary metric, representing the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance. This metric provides a single scalar value that summarizes the performance of the classifier across all classification thresholds.\n\nAdditionally, we employed micro-average AUC to account for class imbalance, aggregating the contributions of all classes to compute the average AUC metric. This approach ensures that the performance metric is unbiased and reflective of the classifier's ability to handle imbalanced datasets.\n\nTo further validate our results, we conducted nonparametric tests to assess statistically significant differences between the ROC curves. This approach analyzes the equality of the curves at all operating points, providing a rigorous evaluation of the classifier's performance.\n\nIn summary, our evaluation method combined cross-validation, independent testing, and robust performance metrics to ensure the reliability and generalizability of our results.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report a comprehensive set of metrics to evaluate the performance of our machine learning models. These metrics include accuracy, sensitivity, specificity, F1 score, and the area under the curve (AUC). These metrics are commonly used in the literature for evaluating classification models, particularly in medical and neurological studies, ensuring that our evaluation is representative and comparable to other works in the field.\n\nAccuracy measures the overall correctness of the model's predictions, providing a general sense of how well the model performs. Sensitivity, also known as recall, indicates the model's ability to correctly identify positive cases, which is crucial for detecting diseases. Specificity measures the model's ability to correctly identify negative cases, helping to avoid false positives. The F1 score is the harmonic mean of precision and recall, offering a balance between the two and is particularly useful when the classes are imbalanced. The AUC provides an aggregate measure of performance across all classification thresholds, giving a single value that summarizes the model's ability to distinguish between classes.\n\nWe present these metrics as mean values with confidence intervals, which provide a range within which the true metric value is likely to fall, giving a sense of the metric's reliability. This approach ensures that our performance evaluation is robust and that the reported metrics are statistically sound.\n\nAdditionally, we compare the performance of our models across different modalities and methods, including functional and structural data, as well as multimodal approaches. This comparison helps to identify which combinations of data and methods yield the best performance, providing insights into the strengths and weaknesses of different approaches.\n\nIn summary, the reported performance metrics are standard and widely accepted in the literature, ensuring that our evaluation is thorough and comparable to other studies. The use of mean values with confidence intervals further enhances the reliability of our results, making our findings robust and trustworthy.",
  "evaluation/comparison": "In our study, we employed a comprehensive approach to evaluate the performance of our models by comparing them across various modalities, methods, and techniques. We utilized 14 XGBoost data-driven models for the multiclass classification of five variants of frontotemporal dementia (FTD) and healthy controls (HCs). These models were based on optimal feature sets after recursive optimization and were computed individually and in combination with all modalities and methods.\n\nTo ensure a thorough comparison, we calculated each model twice: once with and once without demographic and cognitive variables. This dual calculation allowed us to assess the impact of these variables on model performance. The performance indicators we considered included accuracy, sensitivity, specificity, F1 score, and the area under the ROC curve (AUC). These metrics were averaged to identify the top-performing models.\n\nThe top three models, based on their average performance indicators, were the raw functional multifeatured model, the raw functional connectivity model, and the multimodal multifeatured model. To statistically compare these performance results, we employed nonparametric tests to assess statistically significant differences between the ROC curves. This approach analyzed the equality of the curves at all operating points and generated a reference distribution by permuting the pooled ranks of the test scores for each classification.\n\nOur findings indicated that while the raw functional multifeatured model was the top performer, the differences between this model and the two that followed (raw functional connectivity and multimodal multifeatured models) were not statistically significant. This suggests that all three models are robust and reliable for the multiclass classification of FTD variants and HCs.\n\nAdditionally, we addressed potential biases by retraining the classifiers with data acquired from a single scanner and comparing the performance of different brain parcellations. These steps ensured that our results were not influenced by scanner variability or specific brain parcellations, further validating the robustness of our approach.",
  "evaluation/confidence": "The evaluation of our models includes a comprehensive assessment of performance metrics, each accompanied by confidence intervals to provide a clear understanding of their reliability. These metrics encompass sensitivity, specificity, accuracy, F1 score, and the area under the curve (AUC). The confidence intervals are presented as the 0.95 confidence interval for the difference between medians, ensuring that the results are statistically robust.\n\nTo ensure the statistical significance of our findings, nonparametric tests were employed to assess differences between the receiver operating characteristic (ROC) curves. These tests help confirm that the observed performance differences are not due to random chance. Additionally, the variability of feature importance was evaluated across nested k-folds, confirming the stability of the selected features. This approach was applied to all modalities, including functional, structural, and combined data, further validating the consistency of our results.\n\nFor discrete variables, group median comparisons were conducted using a 5000 permutations test to handle tied values. This method provides a reliable way to compare medians between groups, with results shown as the 0.95 confidence interval for the difference between medians and their respective p-values. For dichotomous variables, the equality of proportion between groups was analyzed using the chi-square test, with p-values adjusted by the false discovery rate (FDR) method to control for multiple comparisons. Significant results are highlighted in bold, ensuring that only the most reliable findings are emphasized.\n\nThe performance metrics of our machine learning models are detailed in supplementary tables, which include mean values and confidence intervals for sensitivity, specificity, accuracy, F1 score, and AUC. These tables provide a comprehensive overview of model performance, allowing for a thorough evaluation of their effectiveness. The results indicate that our models achieve high performance across various metrics, with statistically significant differences observed in many cases. This suggests that our method is superior to others and baselines, providing a reliable tool for the tasks at hand.",
  "evaluation/availability": "Not enough information is available."
}