{
  "publication/title": "A convolutional neural network-based framework for analysis and assessment of non-linguistic sound classification and enhancement for normal hearing and cochlear implant listeners.",
  "publication/authors": "Shekar RCMC, Hansen JHL",
  "publication/journal": "The Journal of the Acoustical Society of America",
  "publication/year": "2022",
  "publication/pmid": "36456299",
  "publication/pmcid": "PMC9637023",
  "publication/doi": "10.1121/10.0014955",
  "publication/tags": "- Sound classification\n- Cochlear implants\n- Normal hearing\n- Machine learning\n- Data augmentation\n- Spectro-temporal features\n- Classification accuracy\n- Statistical analysis\n- Audio processing\n- Human sound identification",
  "dataset/provenance": "The ESC-50 dataset was used in this study. This dataset consists of 50 classes of environmental sounds, with each class containing 10 sound examples. The dataset is divided into five distinct folds for cross-validation purposes. The ESC-50 dataset has been used in previous studies and is well-established in the community for evaluating sound classification models. The dataset includes a variety of sound categories, such as animal sounds, human non-speech sounds, interior and exterior sounds, and nature sounds. This diversity allows for a comprehensive evaluation of the classification models under different acoustic conditions. The ESC-50 dataset was used both in its raw unprocessed form and in a CI-simulated form to develop and evaluate the classification models.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "The ESC-50 dataset, which was used in this study, is publicly available. This dataset is arranged in five distinct uniformly sized folds for cross-validation purposes. The dataset is open for use under a specific license, which ensures that users adhere to certain guidelines when accessing and utilizing the data. The license typically includes terms that enforce proper citation of the dataset and compliance with any restrictions on commercial use. This ensures that the dataset is used ethically and responsibly by the research community. The ESC-50 dataset is widely recognized and has been established as a robust resource for environmental audio classification tasks.",
  "optimization/algorithm": "The optimization algorithm employed in our study is based on non-linear least squares estimation. This method is well-established and has been used extensively in various fields for optimizing parameters in non-linear models.\n\nThe non-linear least squares estimation was carried out using an optimization toolbox that provides advanced options for Jacobian derivatives and upper-lower bounds. This toolbox includes implementations of algorithms such as the Levenberg-Marquardt method, which is particularly effective for solving non-linear least squares problems.\n\nThe specific enhancement algorithm developed for our study is novel in its application to the ESC-50 corpus, where it estimates optimal gains for all 50 non-linear least squares (NLS) classes. This algorithm was designed to improve the classification performance of cochlear implant (CI)-simulated audio signals by enhancing specific spectro-temporal features.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of our work is on the application of these techniques to auditory signal processing and cochlear implant research. The primary goal was to assess the performance of the proposed NLS enhancement algorithm in the context of sound classification tasks relevant to hearing-impaired individuals. The optimization algorithm itself is a well-known technique, but its application in this specific context and the resulting enhancements are the novel contributions of our study.",
  "optimization/meta": "The model described in the publication does not function as a meta-predictor. Instead, it relies on a convolutional neural network (CNN) as a feature extractor and a support vector machine (SVM) as the classifier. The CNN is pre-trained on a large environmental audio dataset, specifically the Audioset database, and is used to generate 1024-dimensional sound representations. These representations are then fed into the SVM, which is trained separately for normal hearing (NH) and cochlear implant (CI)-simulated conditions.\n\nThe SVM is trained using the GridSearchCV algorithm for hyperparameter selection, ensuring optimal performance. The training data for the SVM consists of sound representations extracted from the ESC-50 dataset, which is divided into five distinct folds for cross-validation purposes. This approach ensures that the model is robust and generalizes well to new data.\n\nData augmentation techniques, including audio transformations, noise addition, and synthetic data generation, are employed during CNN training to reduce overfitting and enhance the model's robustness. These techniques help in creating a diverse training set, which is crucial for the model's performance.\n\nThe independence of the training data is maintained by using separate datasets for the CNN and SVM training. The CNN is trained on the Audioset database, while the SVM is trained on the ESC-50 dataset. This separation ensures that the model does not memorize specific examples but rather learns generalizable features.",
  "optimization/encoding": "In our study, audio data encoding and preprocessing were crucial steps to ensure effective training and evaluation of our machine-learning models. We began by extracting CI-simulated audio files using the CCi-MOBILE software suite, which employs a specific audio signal generation technique. These simulated audio signals, along with reference raw unprocessed audio signals, were then fed into a pre-trained Convolutional Neural Network (CNN) to obtain sound representations.\n\nThe CNN, pre-trained on the Audioset database, served as a robust feature extractor. It processed the audio data by converting each audio file into log mel spectrograms using a window size of 23 ms with an overlap of 11.5 ms and 128 mel-bands. These spectrograms were then used as input features for the CNN.\n\nDuring the extraction process, several data augmentation techniques were applied to the audio data. These techniques included audio transformations, the addition of noise, and the use of synthetic data. The goal of these augmentations was to reduce overfitting and enhance the model's robustness. The CI processing involved an n-of-m strategy, where 8 channels out of 22 were selected based on higher magnitudes, followed by logarithmic compression and mapping to electric biphasic pulse sequences.\n\nThe CNN architecture followed a standard VGG style, consisting of multiple convolutional layers interspersed with batch normalization and max-pooling layers. The final layers of the CNN produced 1024-dimensional sound representations, which were linearly separable. These representations were then used to train a Support Vector Machine (SVM) classifier. The optimal hyperparameters for the SVM were determined using the GridSearchCV algorithm.\n\nIn summary, our data encoding and preprocessing involved the use of CI-simulated audio files, log mel spectrograms, data augmentation techniques, and a pre-trained CNN to extract meaningful sound representations. These representations were then used to train an SVM classifier, ensuring a comprehensive and robust approach to sound classification.",
  "optimization/parameters": "The optimization process involves estimating optimal gains for each subband in the filter bank, which are the primary parameters in the model. These gains are adapted to minimize the distance between the vocoded and the normal hearing (NH) reference feature scores. The number of parameters, p, corresponds to the number of subbands in the filter bank, as each subband has an associated gain that is optimized.\n\nThe selection of the number of subbands, and thus the number of parameters, is guided by the need to preserve perceptually important spectro-temporal properties of the audio signals. The filter bank is designed to capture these properties effectively. The specific number of subbands used is not explicitly stated, but it is implied that the choice is based on standard practices in cochlear implant (CI) processing and vocoder design, aiming to balance between capturing essential audio features and computational efficiency.\n\nThe optimization toolbox used provides advanced options for handling these parameters, including Jacobian derivatives and upper-lower bounds, ensuring robust and efficient estimation of the optimal gains. The Levenberg\u2013Marquardt algorithm is employed for non-linear least squares estimation, which is well-suited for this type of parameter optimization due to its ability to handle non-linear relationships and provide stable convergence.",
  "optimization/features": "In the optimization process, the input features for the convolutional neural network (CNN) are derived from log mel spectrograms. These spectrograms are extracted using a window size of 23 milliseconds with an overlap of 11.5 milliseconds and 128 mel-bands. This results in a fixed number of 128 features (f) being used as input for the CNN.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the CNN architecture itself acts as a feature extractor, learning to identify and utilize the most relevant features from the input data. The CNN is pre-trained on a large dataset, the Audioset, which helps it to generalize well and capture semantic meaning from the audio data.\n\nThe pre-trained CNN is then used to extract 1024-dimensional sound representations from the audio files. These representations are linearly separable, making them suitable for training a support vector machine (SVM) classifier. The SVM is trained using a grid search with cross-validation to find the optimal hyperparameters, ensuring that the model generalizes well to unseen data.\n\nThe use of a pre-trained CNN and the extraction of high-dimensional sound representations ensure that the most relevant features are utilized for classification, without the need for explicit feature selection. This approach leverages the power of deep learning to automatically identify and extract meaningful features from the audio data.",
  "optimization/fitting": "The optimization process involved in our study utilized the Levenberg\u2013Marquardt algorithm, which is particularly well-suited for non-linear least squares problems. This algorithm balances the advantages of the Gauss\u2013Newton method and gradient descent, making it robust for fitting models with a large number of parameters relative to the number of training points.\n\nTo address the potential issue of over-fitting, several strategies were employed. Firstly, data augmentation techniques were applied during the training of the convolutional neural network (CNN). These techniques included various audio transformations, the addition of noise, and the generation of synthetic data. These methods helped to increase the diversity of the training dataset, thereby reducing the likelihood of over-fitting.\n\nAdditionally, the optimization toolbox used in MATLAB provided advanced options for handling Jacobian derivatives and setting upper and lower bounds on the parameters. These features helped to constrain the parameter space, preventing the model from fitting noise in the data.\n\nTo ensure that under-fitting was not an issue, the model's performance was rigorously evaluated using standard classification metrics such as accuracy. The mean classification accuracy was averaged over all non-linear least squares (NLS) classes to obtain a comprehensive measure of the model's performance. Furthermore, a two-way ANOVA was performed to assess the effects of the NLS enhancement strategy and sound category on the reported sound identification scores. Multiple comparisons tests with Bonferroni corrections were also conducted to determine individual differences across sound categories. These statistical analyses provided a thorough evaluation of the model's performance, ensuring that it was neither over-fitting nor under-fitting the data.",
  "optimization/regularization": "During the training of the convolutional neural network (CNN) for non-linear least squares (NLS) classification, several data augmentation techniques were employed to mitigate overfitting and enhance the model's robustness. These techniques included various audio transformations, the introduction of noise, and the use of synthetic data. These methods helped to ensure that the model could generalize well to unseen data, rather than simply memorizing the training examples. By incorporating these augmentations, the model's performance was improved, and its ability to handle diverse and complex audio data was enhanced.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, the optimal hyperparameters for the support vector machine (SVM) were learned using the GridSearchCV algorithm. The details of the CNN architecture, including the number of convolutional layers, activation functions, and other relevant parameters, are also provided. However, the exact model files and optimization schedules are not explicitly detailed in the text.\n\nThe software suite CCi-MOBILE, which was deployed to generate the electric stimuli sequence, is mentioned, but specific files or licenses related to this software are not discussed. The pre-trained CNN model used for sound representation extraction was trained on the Audioset database, but the availability of this pre-trained model or its license is not specified.\n\nFor those interested in replicating the study, the methods and parameters described in the publication should provide a solid foundation. However, for access to specific model files or detailed optimization schedules, additional inquiries or consultations with the authors may be necessary.",
  "model/interpretability": "The model discussed in this publication is not a blackbox. It employs t-Stochastic Neighbor Embedding (t-SNE) for visualization, which aids in the interpretability of the classification spaces. This technique reduces high-dimensional features to two or three dimensions, allowing for easier analysis and characterization of the classification space.\n\nFor instance, the model's performance can be visualized using t-SNE plots, which show how well the model separates different classes. In the case of normal-hearing (NH) conditions, the t-SNE visualizations reveal well-formed clusters with clear separation and lower overlap between different sound classes. This indicates that the model can effectively distinguish between various sounds.\n\nIn contrast, for cochlear implant (CI)-simulated conditions, the t-SNE plots show more intermixed representations with higher overlap and no clear cluster boundaries. This visualization helps to understand why the model's performance is reduced in CI-simulated conditions, as the sound representations are more difficult to separate.\n\nAdditionally, the model's performance is compared across different sound categories, such as animal sounds, nature sounds, and interior sounds. The t-SNE visualizations for these categories provide insights into how the model handles different types of sounds and where it may struggle.\n\nOverall, the use of t-SNE for visualization makes the model more interpretable, as it allows researchers to understand how the model represents and separates different sound classes. This interpretability is crucial for improving the model's performance and for understanding the limitations of CI-simulated conditions.",
  "model/output": "The model discussed is a classification model. It is designed to classify environmental sounds into various categories. The model's performance is evaluated based on classification accuracy, which is a common metric for classification tasks. The model was trained and tested on the ESC-50 dataset, which consists of 50 different environmental sound classes. The mean classification accuracy for the normal-hearing (NH) model is 85.86%, while for the cochlear implant (CI)-simulated model, it is 65.25%. These accuracies indicate how well the model can classify sounds into their respective categories.\n\nThe model's output is the classification of sounds into predefined categories. The performance of the model is compared against human subjects under both normal-hearing and CI-simulated conditions. The results show that the NH model's performance is comparable to that of NH subjects, with some variations across different sound categories. For the CI-simulated model, the classification accuracy is generally higher than that of CI subjects across all sound categories.\n\nThe model's classification performance is also analyzed using t-SNE visualization, which helps in understanding the distribution and overlap of sound representations in the classification space. The NH model shows better cluster properties and less overlap compared to the CI-simulated model. This analysis provides insights into how the model's internal representations differ between NH and CI conditions.\n\nIn summary, the model is a classification model that categorizes environmental sounds. Its performance is evaluated based on classification accuracy, and it is compared against human subjects under different listening conditions. The model's output is the classification of sounds into predefined categories, and its performance is analyzed using various metrics and visualizations.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the CNN-based model used in this study is not publicly released. However, the CNN architecture and training procedures are detailed in the publication, allowing for replication by interested researchers.\n\nThe CCi-MOBILE software suite, used for generating CI-simulated audio files, is also not publicly available. This software is integral to the simulation of cochlear implant processing and is utilized to create the CI-simulated audio signals necessary for training and evaluating the CI NLS classification model.\n\nFor those interested in implementing the described methods, the publication provides comprehensive details on the CNN architecture, training processes, and the use of the CCi-MOBILE software suite. This information should enable researchers to develop their own implementations or adapt existing frameworks to achieve similar results.",
  "evaluation/method": "The evaluation method employed a combination of simulated and subjective evaluations to assess the performance of the proposed models. A cloud-based infrastructure called CCi-Evaluate, supported by Amazon Web Services, was used to conduct experiments remotely. Participants, including both normal-hearing (NH) and cochlear implant (CI) listeners, received detailed instructions and accessed the tests via a computing platform. They were asked to identify 100 audio samples from the ESC-50 dataset, with the option to replay the audio and take breaks to avoid fatigue.\n\nThe evaluation involved several key steps. First, a CNN-based NLS classification model was used to examine the effect of raw unprocessed audio versus CI-simulated audio signals on classification performance. This included a t-SNE-based visualization to analyze the sound spaces qualitatively. The CI-simulated NLS classification model was expected to perform poorly compared to the NH model.\n\nSecond, listener responses were used to generate confusion matrices and compute mean classification accuracy. The performance of NH and CI subjects was compared against the proposed NH and CI-simulated classification models using mean classification and class-wise NLS identification accuracies. A two-way ANOVA was performed to determine the effects of classification type and sound category on the reported sound identification scores.\n\nThird, the proposed NLS enhancement algorithm was applied to the ESC-50 database and evaluated using a comparative NLS classification framework. Four types of optimal filters were estimated for each NLS class, and these filters were incorporated into the CCi-MOBILE research platform software suite. Four CI-simulated NLS enhanced versions of the ESC-50 corpus were developed and used to create classification models. These models were assessed and compared quantitatively based on standard classification metrics such as accuracy. A two-way ANOVA and paired sample t-tests were conducted to determine the effects of NLS enhancement strategy and sound category on the reported sound identification scores.",
  "evaluation/measure": "In our evaluation, we primarily focus on classification accuracy as our key performance metric. This metric is calculated as the mean classification accuracy averaged over all non-linguistic sound (NLS) classes, providing a comprehensive overview of the model's performance across various sound categories.\n\nWe also report class-wise sound identification accuracy, which is the ratio of correctly identified audio samples per class to the total number of audio samples per class. This metric allows us to assess the model's performance on a finer granularity, providing insights into how well the model performs for individual sound classes.\n\nTo compare the performance of different models and conditions, we use group mean classification scores. These scores are obtained by grouping the identification accuracies of NLS classes under different sound categories and then calculating the mean classification score for each group. This approach enables us to analyze the performance trends across different sound categories and identify areas where the model excels or struggles.\n\nIn addition to these metrics, we employ statistical analyses such as two-way ANOVA and multiple comparisons tests with Bonferroni corrections to determine the effects of different factors, such as NLS enhancement strategy and sound category, on the reported sound identification scores. These analyses help us to draw robust conclusions about the performance of our models and the significance of the observed differences.\n\nOur set of metrics is representative of the literature in the field of sound classification and enhancement. Accuracy is a widely used metric in classification tasks, and our use of class-wise identification accuracy and group mean classification scores aligns with common practices in the evaluation of sound classification models. Furthermore, our use of statistical analyses ensures that our findings are rigorous and reliable, adhering to the standards of the scientific community.",
  "evaluation/comparison": "A comparison to a simpler baseline was performed. Specifically, a version of the CI-simulated classification model with \"no enhancement\" was treated as the baseline and used as a reference for comparison. This baseline model was assessed and compared quantitatively based on standard classification metrics such as accuracy. The class-wise sound identification accuracy was averaged over all NLS classes to obtain the mean classification accuracy. Additionally, the identification accuracies of NLS classes were grouped under sound categories, and the group mean classification scores for all four CI-simulated NLS enhanced versus baseline \"no enhancement\" classification models were comparatively assessed.\n\nStatistical analyses, including a two-way ANOVA and Bonferroni corrections, were carried out to determine the effects of NLS enhancement strategy and sound category on the reported sound identification scores. A paired sample t-test was also applied to compare the baseline \"no enhancement\" classification performance with each NLS enhanced classification performance. These analyses provided insights into the effectiveness of the NLS enhancement strategies compared to the baseline model.\n\nNot applicable",
  "evaluation/confidence": "The evaluation of the proposed methods involved several statistical analyses to assess the confidence and significance of the results. Two-way ANOVA was performed to determine the effects of different factors, such as NLS enhancement strategy and sound category, on sound identification scores. This analysis helped in understanding whether the observed differences were statistically significant.\n\nMultiple comparisons tests using Bonferroni corrections were conducted to identify individual differences across sound categories. These tests provided a more granular view of where significant differences might lie, ensuring that the findings were robust and not due to chance.\n\nAdditionally, paired sample t-tests were applied to compare the baseline \"no enhancement\" classification performance with each NLS enhanced classification performance. This allowed for a direct comparison to determine if the enhancements led to statistically significant improvements.\n\nThe results indicated that while some enhancement strategies, such as harmonicity enhancement, showed statistically significant improvements over the baseline, others did not. This suggests that certain enhancement techniques are more effective than others in improving classification performance.\n\nOverall, the statistical analyses provided a comprehensive evaluation of the proposed methods, ensuring that the performance metrics were reliable and that the claims of superiority over baselines and other methods were supported by significant evidence.",
  "evaluation/availability": "Not enough information is available."
}