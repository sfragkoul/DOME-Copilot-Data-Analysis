{
  "publication/title": "Astrocyte regional heterogeneity revealed through machine learning-based glial neuroanatomical assays.",
  "publication/authors": "Blackburn J, Alves MJ, Aslan MT, Cevik L, Zhao J, Czeisler CM, Otero JJ",
  "publication/journal": "The Journal of comparative neurology",
  "publication/year": "2021",
  "publication/pmid": "33410136",
  "publication/pmcid": "PMC8113076",
  "publication/doi": "10.1002/cne.25105",
  "publication/tags": "- Astrocytes\n- Biomarkers\n- Machine Learning\n- Neuroinflammation\n- Image Analysis\n- Glial Cells\n- GFAP\n- ALDH1L1\n- Clustering\n- Feature Importance\n- Random Forests\n- Support Vector Machines\n- Principal Component Analysis\n- Unsupervised Learning\n- Morphological Heterogeneity\n- Neuroanatomy\n- Data Visualization\n- Statistical Modeling\n- Biomarker Reactivity\n- Dimensionality Reduction",
  "dataset/provenance": "The dataset utilized in this study was derived from images of brain regions, specifically the hippocampus, neocortex, and caudoputamen, from both vehicle-treated and LPS-treated mice. The images were analyzed to segment objects based on three biomarkers: DAPI, GFAP, and ALDH1L1. The segmentation process involved converting images to grayscale and using various techniques such as Otsu's method, k-means clustering, and the WEKA plugin for supervised learning. The segmented data was then loaded into R for further analysis.\n\nThe number of data points is substantial, with over twenty thousand objects clustered in some instances. The dataset includes extracted feature information from these segmented objects, such as fluorescence intensity values, shape measurements, and pixel texture features. These features were used to determine the reactivity state of the objects and to perform clustering and classification tasks.\n\nThe dataset has not been explicitly mentioned as used in previous papers or by the community. However, the methods and techniques employed, such as k-means clustering, DBSCAN, OPTICS, and random forests, are well-established and have been used in various studies. The dataset was specifically curated for this study to evaluate biomarkers within different brain regions and to identify object subclasses using unsupervised clustering methods. The analysis involved center-scaling the data, performing dimensionality reduction with PCA, and using machine learning models to predict cluster designations and reactivity states.",
  "dataset/splits": "In our study, we performed data splits to train and test our classification algorithms. Specifically, we split the data from the hippocampus dataset into two main sets: a training set and a test set. The training set comprised 70% of the data, while the test set contained the remaining 30%. This split was done randomly to ensure that the data was evenly distributed and representative of the entire dataset.\n\nThe training set was used to train our random forest model to predict whether an object was labeled with GFAP or ALDH1L1 based on its morphological features. The test set was then used to evaluate the performance of the trained model. The model achieved an accuracy of 94.45% on the test data, indicating that the morphological features outlined by the GFAP and ALDH1L1 biomarkers are distinct in baseline brain sections.\n\nAdditionally, we conducted another set of data splits to determine the extent to which the biomarkers provided insight into regionalization. For this, we used vehicle-treated objects from each region based on segmented object features from either GFAP or ALDH1L1. The data was split similarly, with 70% used for training and 30% for testing. The GFAP model had an overall accuracy of 79.26%, while the ALDH1L1 model had an overall accuracy of 84.07% in predicting the region over the no information rate. This further supports the idea that astrocytes' morphological features change as a function of neuroanatomical location and that GFAP and ALDH1L1 highlight different aspects of astrocytes.",
  "dataset/redundancy": "The datasets were split into training and test sets to evaluate the performance of machine learning models. Specifically, 70% of the data was used for training the models, while the remaining 30% was reserved for testing. The training data consisted solely of region-specific objects, ensuring that the models were trained on data relevant to the regions being studied.\n\nThe training and test sets were independent, meaning that the objects in the test set were not used during the training phase. This independence was enforced by the split ratio, which ensured that the test set remained unseen by the models until the evaluation phase. This approach helps in assessing the generalizability of the models to new, unseen data.\n\nRegarding the distribution of the datasets, it is not directly comparable to previously published machine learning datasets, as the focus was on specific biomarkers and regions within the brain. The datasets were centered-scaled to standardize the features, which is a common preprocessing step in machine learning to ensure that all features contribute equally to the model training.\n\nThe models evaluated included random forests, support vector machines, and multi-layer perceptrons. The performance of these models was reported in supplemental tables, providing detailed metrics on their accuracy and other evaluation criteria. The models were optimized using performance evaluation tools, and their effectiveness was assessed based on the accuracy of predictions over the no information rate.\n\nIn summary, the datasets were carefully split to ensure independence between training and test sets, and standard preprocessing techniques were applied to prepare the data for model training. The focus was on evaluating the models' ability to predict the reactivity state of segmented objects within specific brain regions.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in our study include random forests, support vector machines (SVM), and multi-layer perceptron (MLP). These are well-established classes of machine-learning algorithms that have been extensively used in various scientific and engineering domains.\n\nThe algorithms employed are not new; they are widely recognized and have been applied in numerous research studies. Random forests, for instance, are an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Support vector machines are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. MLP is a class of feedforward artificial neural network and is an extension of the standard linear perceptron to several layers of nodes in a directed graph, with each node (neuron) in one layer connected to every node in the next one.\n\nThe reason these algorithms were not published in a machine-learning journal is that our primary focus was on applying these established methods to a specific biological problem rather than developing new machine-learning techniques. Our study aimed to evaluate the reactivity state of segmented objects in different brain regions using these algorithms, and the biological insights gained from this application were the main contribution of our work. The algorithms themselves are well-documented and have been thoroughly studied in the machine-learning literature.",
  "optimization/meta": "The meta-predictor does not use data from other machine-learning algorithms as input. Instead, it relies on the outputs of individual machine-learning models to make predictions. The machine-learning methods that constitute the whole include random forests and support vector machines. These models were applied to the LPS data for each biomarker and region to determine the accuracy of cluster predictions. The balanced accuracy for each subclass was calculated, and p-values were determined by comparing the predictive accuracy to the no information rate. The support vector machine model was ultimately used for predictions due to its higher balanced accuracy for each subclass and overall accuracy.\n\nRegarding the independence of the training data, it is clear that the models were trained using region-specific objects. For instance, models were created using 70% training data solely of region-specific objects and tested on the remaining 30% of objects. This approach ensures that the training data is independent for each region-specific model. However, when these models were applied to other regions, the independence of the training data for those regions is not explicitly stated. Therefore, while the training data for region-specific models is independent, the independence of the training data when applying models to other regions is not explicitly clear.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithms involved several steps to ensure the data was suitable for analysis. Initially, data from vehicle-treated animals was extracted from specific brain regions, including the hippocampus, neocortex, and caudoputamen. This data was then center-scaled to standardize the features, which is crucial for many machine-learning algorithms to perform optimally.\n\nFeature selection was performed to identify the most relevant variables. This involved computing feature importance using random forests and the Boruta package, which helps in identifying how well each variable performs compared to shuffled, or \"shadow,\" features. Variables that underperformed or were similar to the shadow features were deemed unimportant and removed from further analysis. Additionally, a correlation plot was generated to identify and reduce redundancies among the variables. The selected variables included two fluorescence intensity values, two shape measurements, and eight pixel texture features that described the texture of the whole object and the comparison of neighboring pixel values.\n\nPrincipal Component Analysis (PCA) was utilized to reduce the dimensionality of the data and identify the main drivers of variance. This step helped in visualizing the data and understanding the underlying patterns. The eigenvector graphs from PCA provided insights into which features contributed most to the variance in the data.\n\nFor the unsupervised clustering, data from LPS-treated mice was loaded into R. Various clustering methods were employed, including k-means, k-medoids, DBSCAN, OPTICS, and distribution-based methods. The optimal number of clusters was determined using the NbClust function and the silhouette method. Internal clustering validation was performed to ensure the clusters were meaningful, involving permutation analysis to compute the distance between objects and their respective clusters.\n\nThe data was further processed for machine-learning algorithms, including random forests, support vector machines, and multi-layer perceptron (MLP). The models were trained on 70% of the data and tested on the remaining 30%. Performance evaluation tools were used to optimize these models, and the models were then applied to other regions to determine if they could predict the reactivity state of the segmented objects. The packages used for these analyses included randomForest, caret, RSNNS, corrplot, MASS, factoextra, and others.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the specific analysis and the clustering method employed. Initially, we utilized the NbClust function to determine the optimal number of clusters for each biomarker in each region. However, when the clusters were three or fewer, meaningful objects were often grouped into a single cluster, which limited our analysis. To address this, we employed the silhouette method to determine the optimal number of k-means clusters, which provided clusters with distinct morphologies of biomarkers in most regions.\n\nFor the random forest models, we selected twelve variables based on a correlation plot of all features and other studies performing image analysis with Haralick features. These variables included two fluorescence intensity values, two shape measurements, and eight pixel texture features that described the texture of the whole object and the comparison of neighboring pixel values. The Boruta algorithm was used to compute feature importance in the random forest, identifying how well each variable performed over the generated shadow feature.\n\nIn summary, the selection of parameters was guided by a combination of statistical methods and domain knowledge, ensuring that the models were both robust and interpretable. The specific number of parameters varied depending on the clustering method and the region being analyzed, but the process was systematic and aimed at identifying the most relevant features for each analysis.",
  "optimization/features": "The initial analysis involved a comprehensive set of 89 variables computed for each segmented object. To streamline the statistical workflows, feature selection was performed using several methods. The Boruta algorithm was employed to identify important features by comparing their performance against shuffled shadow features. This process helped in removing unimportant features.\n\nAdditionally, a correlation plot was generated to identify and reduce redundant features. The selected features included two fluorescence intensity values, two shape measurements, and eight pixel texture features. These features were chosen for their ability to describe the texture of the entire segmented object and the comparison of neighboring pixel values.\n\nThe final set of features used for subsequent analyses consisted of twelve variables: two intensity features (mean intensity and first percentile intensity of the object), two shape features (area and perimeter), and eight texture features (con, idm, f12, asm, cor, ent, f13, and var). These features were selected based on their importance in classifying treatment class and their relevance in describing the morphological characteristics of the objects.\n\nFeature selection was conducted using the training set only, ensuring that the models were not biased by the test data. This approach helped in identifying the most relevant features for predicting the reactivity state of the segmented objects.",
  "optimization/fitting": "The fitting method employed in our study involved several clustering techniques, including k-means, k-medoids, DBSCAN, OPTICS, and distribution-based clustering. Initially, the NbClust function was used to determine the optimal number of clusters for each biomarker in each region. However, when the clusters were limited to k = 3 or fewer, meaningful objects were often grouped into a single cluster, which hindered detailed analysis.\n\nTo address this, the silhouette method was utilized to determine the optimal number of k-means clusters, which provided clusters with distinct morphologies of biomarkers in most regions. To ensure the validity of these clusters, internal clustering validation was performed. This involved computing the distance between objects and their respective clusters and conducting a permutation analysis. The cluster.stats() function in R was used to evaluate the average distance between clusters, the average distance within clusters, and the within-cluster sum of squares. This permutation analysis was repeated 1000 times, allowing for the identification of significance scores up to 10\u22122.\n\nTo determine the accuracy of cluster predictions, random forests and support vector machines were applied to the LPS data for each biomarker and region. The balanced accuracy for each subclass was calculated, and p-values were computed by comparing the predictive accuracy to the no information rate. The support vector machine models demonstrated higher balanced accuracy for each subclass and overall accuracy, leading to their use in predicting the vehicle objects' cluster designation.\n\nTo rule out over-fitting, the support vector machine and random forest models were compared, and both showed high accuracy in cluster prediction. The support vector machine model was chosen due to its marginally better performance. Additionally, the permutation analysis helped ensure that the clusters were distinct and not the result of random assignment.\n\nUnder-fitting was addressed by ensuring that the clusters were meaningful and distinct. The silhouette method and permutation analysis were crucial in determining the optimal number of clusters and validating their significance. The use of multiple clustering techniques and the comparison of their results further ensured that the final clusters were robust and not due to under-fitting.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our models. One key method involved feature selection and dimensionality reduction. We utilized the Boruta algorithm to identify and retain only the most important features, removing those that did not contribute significantly to the model's predictive power. This process helped in reducing the complexity of the models and mitigating the risk of overfitting.\n\nAdditionally, we performed correlation analysis to identify and eliminate redundant features. By selecting a subset of features that were less correlated with each other, we ensured that our models were not overly reliant on any single feature, thereby enhancing their generalizability.\n\nWe also employed cross-validation techniques, specifically using 70% of the data for training and 30% for testing. This approach allowed us to evaluate the performance of our models on unseen data, providing a more reliable estimate of their predictive accuracy.\n\nFurthermore, we conducted permutation analyses to assess the significance of our clustering results. By comparing the observed clustering patterns to those obtained from randomly assigned clusters, we could determine the likelihood of our findings occurring by chance. This step was crucial in validating the meaningfulness of our clusters and ensuring that our models were not overfitting to the specific dataset.\n\nIn summary, our study incorporated multiple regularization techniques, including feature selection, dimensionality reduction, cross-validation, and permutation testing, to prevent overfitting and enhance the reliability of our models.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in this study are not entirely black-box, as several techniques were used to enhance their interpretability. To identify the key drivers of variation between biomarkers and regions, feature importance was evaluated using random forests and principal component analysis (PCA). The Boruta algorithm, which works by comparing the importance of original features to shuffled \"shadow\" features, was utilized to determine the significance of each variable. This process helped in identifying and retaining only the most relevant features, thereby reducing redundancy and improving model transparency.\n\nAdditionally, a correlation plot was generated to visualize the relationships between variables, further aiding in the understanding of which features were most influential. The variables chosen for modeling included fluorescence intensity values, shape measurements, and pixel texture features, all of which were selected based on their importance and correlation with the outcomes.\n\nPCA was also employed to reduce the dimensionality of the data and identify the primary drivers of variance. The results of PCA were visualized using eigenvector graphs, providing a clear representation of how different variables contributed to the overall variance in the data.\n\nIn summary, while the models themselves are complex, the use of feature importance analysis, correlation plots, and PCA has made them more interpretable. These techniques allowed for a deeper understanding of which features were most critical in predicting the reactivity state of segmented objects across different brain regions.",
  "model/output": "The models developed in this study are classification models. They were designed to predict the reactivity state of segmented objects, specifically to distinguish between baseline and reactive astrocytes. Various machine learning methods were employed, including random forests, support vector machines, and multi-layer perceptron (MLP) neural networks. These models were trained using region-specific objects and evaluated based on their accuracy in predicting the reactivity state of test data.\n\nThe performance of these models was assessed using accuracy metrics, and the results indicated that the models performed best when applied to the regions from which they were trained. For instance, the hippocampus GFAP model achieved an accuracy of 67.48%, while the ALDH1L1 model had an accuracy of 73.14% on the test data. However, when these models were applied to different regions, their predictive accuracy decreased significantly. This suggests that the models are effective classifiers for the specific regions they were trained on but may not generalize well to other regions.\n\nIn summary, the models are classification models that aim to predict the reactivity state of astrocytes based on morphological and fluorescence features. Their performance varies depending on the region, highlighting the importance of neuroanatomical localization context in astrocyte reactivity classification.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the algorithms and workflows described in the publication is not explicitly mentioned as being publicly released. However, the code for some of the workflows is available in the supplemental materials. This suggests that while the code may not be openly accessible in a traditional repository, it is provided to readers who wish to replicate the study's findings.\n\nThe publication details the use of various R packages and tools, such as randomForest, caret, RSNNS, EBImage, ggplot2, and others, which are publicly available and can be accessed through standard repositories like CRAN. These packages were utilized to perform tasks such as image segmentation, feature extraction, model training, and validation.\n\nFor those interested in replicating the study, the supplemental materials provide a starting point. However, specific details on how to run the algorithms as executable files, web servers, virtual machines, or container instances are not provided. Interested parties would likely need to adapt the provided code to their own environments.\n\nThe publication does not specify the licensing terms under which the supplemental code is provided. Typically, code shared in academic publications is intended for non-commercial use and replication of the study's methods, but explicit licensing details are not given here.",
  "evaluation/method": "The evaluation method employed a combination of internal clustering validation and machine learning techniques to assess the accuracy and significance of the clustering results. Internal clustering validation was performed to compute the distance between objects and their respective clusters, ensuring that the clusters determined by the silhouette method were meaningful. This involved evaluating the average distance between clusters, the average distance within clusters, and the within-cluster sum of squares. A permutation analysis was conducted, repeating the process 1000 times, to identify significance scores up to 10\u22122. This approach allowed for the determination of the statistical significance of the clustering results.\n\nTo determine the accuracy of cluster predictions, random forests and support vector machines were applied using the LPS data for each biomarker and region. The balanced accuracy for each subclass was calculated, and p-values were computed by comparing the predictive accuracy to the no information rate. Support vector machines were ultimately chosen for predicting vehicle object cluster designation due to their higher balanced accuracy for each subclass and overall accuracy.\n\nAdditionally, the clusters were visually validated by the first author, who identified at least five objects per mouse from both treatments and every cluster within regions. This ensured that the predicted object clusters were visually similar. The clusters showed very little overlap in PCA and t-SNE plots, which were colored based on cluster designation. These visualizations allowed for the appreciation of similar data being plotted together and the ratio of objects in each cluster overall.\n\nThe evaluation also included the use of linear discriminant analysis for dimensionality reduction, which helped in visually understanding the overlap between objects from vehicle-treated and LPS-treated mice. Coefficients from the linear discriminant analysis were computed to determine feature importance in classifying objects\u2019 reactivity. Region-specific random forest models were applied to the objects in each cluster to further validate the clustering results.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our models in predicting the reactivity state of segmented objects and the region of the test data. The primary metric used was accuracy, which measures the proportion of correctly predicted instances out of the total instances. We compared the accuracy of our models against the no information rate (NIR) to ensure that our models provided meaningful predictions.\n\nFor the machine learning models, including random forests, support vector machines, and multi-layer perceptron (MLP), we reported the accuracy of each model in predicting the reactivity state of objects within specific regions. These models were trained on 70% of the data and tested on the remaining 30%. The performance of these models is detailed in supplemental tables 3\u20135.\n\nIn addition to accuracy, we also calculated the balanced accuracy for each subclass when applying random forests and support vector machines to the LPS data for each biomarker and region. This metric is particularly useful when dealing with imbalanced datasets, as it takes into account the accuracy of each class separately and then averages them.\n\nWe further validated the significance of our models by calculating p-values, comparing the predictive accuracy to the no information rate. This statistical measure helps to determine whether the observed accuracy is significantly better than what would be expected by chance.\n\nFor the unsupervised clustering analysis, we used internal clustering validation methods to compute the distance between objects and their respective clusters. This involved evaluating the average distance between clusters, the average distance within clusters, and the within-cluster sum of squares. We performed a permutation analysis, repeating the process 1000 times, to identify significance scores.\n\nOverall, the set of metrics used in our study is representative of standard practices in the field. Accuracy is a commonly reported metric in machine learning and image analysis studies, providing a straightforward measure of model performance. The use of balanced accuracy and p-values adds robustness to our findings, ensuring that our models are not only accurate but also statistically significant. The internal clustering validation methods further support the reliability of our clustering results.",
  "evaluation/comparison": "Not applicable. The publication does not discuss comparisons to publicly available methods or simpler baselines. The focus is on the application of various clustering and machine learning techniques to analyze biomarker data from different brain regions. The methods used include k-means, DBSCAN, OPTICS, and distribution-based clustering for unsupervised clustering, as well as random forests, support vector machines, and multi-layer perceptron (MLP) for supervised learning. The evaluation of these methods is primarily based on their performance in predicting cluster designations and reactivity states of segmented objects, rather than comparisons to external benchmarks or simpler baselines.",
  "evaluation/confidence": "To evaluate the confidence in our results, several statistical methods were employed. Initially, we determined the optimal number of clusters using the NbClust function, but we found that clusters with k = 3 or less grouped meaningful objects into one cluster, limiting analysis. Therefore, we used the silhouette method to determine the optimal number of k-means clusters, which provided clusters with distinct morphologies of biomarkers in most regions.\n\nTo ensure the clusters determined by the silhouette method were meaningful, we performed internal clustering validation. This involved computing the distance between objects and their respective clusters. We conducted a permutation analysis, implementing the cluster.stats() function call in R, and evaluated the average distance between clusters, the average distance within clusters, and the within-cluster sum of squares. This permutation was repeated 1000 times, allowing us to identify significance scores of up to 10\u22122. This rigorous approach permitted us to claim that the clustering results were statistically significant and not due to random chance.\n\nFor determining the accuracy of cluster predictions, we applied random forests and support vector machines (SVM) to the LPS data for each biomarker and region. The balanced accuracy for each subclass was calculated, and p-values were computed by comparing the predictive accuracy to the no information rate. The SVM models demonstrated higher balanced accuracy for each subclass and overall accuracy, leading us to use these models to predict the vehicle objects' cluster designation. The SVM accuracies were greater than 99%, with p-values less than 2.2 x 10\u221216 for all models, indicating strong statistical significance.\n\nAdditionally, we compared object clusters using PCA and t-SNE plots, which showed very little overlap, further supporting the distinctiveness of the clusters. The visual similarity of predicted object clusters was confirmed by the first author, who identified at least five objects per mouse from both treatments and every cluster within regions. This manual verification, combined with the statistical analyses, provides high confidence in the robustness and validity of our clustering results.",
  "evaluation/availability": "Not enough information is available."
}