{
  "publication/title": "Interpretable deep learning for the prognosis of long-term functional outcome post-stroke using acute diffusion weighted imaging.",
  "publication/authors": "Moulton E, Valabregue R, Piotin M, Marnat G, Saleme S, Lapergue B, Lehericy S, Clarencon F, Rosso C",
  "publication/journal": "Journal of cerebral blood flow and metabolism : official journal of the International Society of Cerebral Blood Flow and Metabolism",
  "publication/year": "2023",
  "publication/pmid": "36169033",
  "publication/pmcid": "PMC9903217",
  "publication/doi": "10.1177/0271678x221129230",
  "publication/tags": "- Attention maps\n- Classification\n- Convolutional neural network\n- Deep learning\n- Diffusion-weighted imaging\n- Functional outcome\n- Ischemic stroke\n- Long-term prognosis\n- Machine learning\n- Modified Rankin Scale\n- Prediction models\n- Stroke imaging",
  "dataset/provenance": "The dataset used in this study was sourced from multiple hospitals, with a total of eight centers contributing data. The patients included in the study were those who were admitted to the hospital with an ischemic stroke within 6 hours of stroke onset, had diffusion-weighted imaging (DWI) performed 1 day after stroke onset, and had available 3-month modified Rankin Scale (mRS) scores. The modified Rankin Scale is a commonly used scale for measuring the degree of disability or dependence in the daily activities of people who have suffered a stroke.\n\nThe dataset included patients from three specific sources: the INSULINFARCT trial, the ASTER trial, and the Piti\u00e9-Salp\u00eatri\u00e8re registry. The INSULINFARCT and ASTER trials were randomized controlled trials that obtained ethics committee approval and written informed consent from each patient. The Piti\u00e9-Salp\u00eatri\u00e8re registry, in accordance with French legislation, did not require approval by an ethics committee or written informed consent from patients, as it is a retrospective database involving the analysis of anonymized data collected prospectively as part of routine clinical care.\n\nThe dataset was designed to be representative of cohorts in other endovascular therapy and thrombolysis studies, with a marginally unbalanced dataset (41.9% poor outcome). The rich, diverse data used for training likely enabled a meaningful extraction of features related more to stroke outcome and less to the specific centers from which the data originated.\n\nThe dataset included diffusion-weighted images acquired with 11 different MRI machines from various manufacturers and field strengths, ensuring a wide range of imaging protocols. This diversity in imaging protocols helped to balance for outcome and lesion side, making the model more robust and generalizable.\n\nThe dataset was preprocessed to ensure a similar data distribution of images used for training and evaluation of the convolutional neural network (CNN). This included skull-stripping, normalization of brain voxel intensity values, and cropping and resizing of the images to a standard size. The preprocessing steps were designed to prevent any bias related to outcome measures and to ensure that the model could generalize well to new, unseen data.\n\nThe dataset was used to train and evaluate a series of 3D CNN classifiers with a trainable attention mechanism to predict poor outcome defined as mRS>2 at 3 months post-stroke using day 1 DWI data. The model was evaluated using a leave-one-center-out cross-validation paradigm, which involved holding out data from one center at a time to obtain a completely unbiased measure of model performance. This approach ensured that the model could generalize well to new, unseen data from different centers.",
  "dataset/splits": "The dataset was split into an internal training-validation set and an external test set using a leave-one-center-out (LOCO) cross-validation scheme. This means that for each fold of the cross-validation, data from one center was held out as the test set, while data from the remaining centers were used for training and validation. The training-validation images were further separated into actual training and validation sets with a 75:25 split, stratified by site provenance and outcome. This stratification ensures that the distribution of outcomes and sites is similar in both the training and validation sets.\n\nTwo of the participating centers, each containing only one patient, were merged into a single center to accelerate training for cross-validation purposes. This merging was done to ensure that each fold of the cross-validation had a sufficient number of data points for training.\n\nThe total number of patients included in the analysis was 322. These patients were distributed across three main sources: 113 patients from the Piti\u00e9-Salp\u00eatri\u00e8re registry, 94 patients from the INSULINFARCT trial, and 115 patients across six centers from the ASTER trial. The distribution of data points in each split varied depending on the fold of the cross-validation, as each fold held out a different center as the test set. However, the overall distribution of outcomes and sites was maintained across the training and validation sets due to the stratification process.\n\nThe external test set for each fold consisted of data from one held-out center, ensuring an unbiased measure of model performance. This approach allowed for the evaluation of the model's performance on unseen data from multiple, new hospitals, simulating real-world conditions. The rich, diverse data used for training likely enabled a meaningful extraction of features related more to stroke outcome and less to the specific centers from which the data originated.",
  "dataset/redundancy": "The datasets were split using a leave-one-center-out (LOCO) cross-validation scheme. This approach ensures that the training and test sets are independent. For each fold, data from one center was held out as the test set, while data from the remaining centers were used for training and validation. This method was chosen to obtain a completely unbiased measure of model performance and to simulate the model's performance on unseen data from multiple, new hospitals.\n\nThe training-validation images were further separated into actual training and validation sets with a 75:25 split. This split was stratified by site provenance and outcome to maintain a balanced representation of different centers and outcomes in both sets. Two centers containing only one patient each were merged into a single center to accelerate training for cross-validation purposes.\n\nThe distribution of our dataset is more representative of real-world scenarios compared to previously published machine learning datasets. Our approach resulted in an external test accuracy and AUC that are not only higher but also more reliable. Previous studies have either used a random split of all data for training and test sets or used a single set of held-out centers for external validation. By using multiple held-out centers, we reduced the variance in our estimated classification performance and ensured that our model generalizes well to new, unseen data. This method also accounts for the variable number of patients per center, providing a more robust evaluation of the model's performance.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used is convolutional neural networks (CNNs), specifically a series of 3D CNN classifiers. The backbone of each classifier is a VGGNet-like CNN, which is a common deep learning architecture in image analysis. This architecture applies a series of convolution layers to extract features at various spatial scales, progressively moving from low-level edge and texture features to more abstract, high-level features.\n\nThe attention mechanism implemented in the CNN is similar to that proposed by Jetley and colleagues, which is not entirely new but has been adapted for this specific application. The attention mechanism helps the network highlight and attend more to regions of the image containing important semantic features crucial for classifying poor versus good outcomes. This mechanism involves learning a parameter that maps the sum of the activations of the global feature vector and local convolutional layers to an \"attention\" score, ensuring that the network focuses on relevant parts of the image.\n\nThe reason this specific adaptation of the attention mechanism was not published in a machine-learning journal is likely because the focus of this study is on its application in medical imaging, particularly in predicting long-term functional outcomes from diffusion-weighted imaging (DWI) data in stroke patients. The innovation lies in the application and adaptation of existing techniques to a novel and clinically relevant problem, rather than the development of an entirely new machine-learning algorithm. The study aims to demonstrate the practical utility and effectiveness of these techniques in a medical context, which is more aligned with the scope of journals in medical imaging and stroke research.",
  "optimization/meta": "The model employed in this study is a deep learning ensemble, which can be considered a type of meta-predictor. This ensemble model averages the classification probabilities from multiple individual models to improve overall performance. Specifically, eight lightweight variants of a convolutional neural network (CNN) backbone architecture were investigated. These variants differ in the initial number of feature maps, the multiplicity of feature maps per level, and the inclusion of a bottleneck layer between levels.\n\nThe ensemble approach helps to mitigate the risk of overfitting and enhances the robustness of the predictions by leveraging the strengths of multiple models. Each of the eight models was trained independently on the same dataset, ensuring that the training data remains consistent and independent across all models. This independence is crucial for the ensemble method to effectively combine the predictions and improve accuracy.\n\nThe use of a deep learning ensemble model allows for a more reliable and accurate prediction of long-term functional outcomes using day 1 diffusion-weighted imaging (DWI) data. By averaging the probabilities from multiple models, the ensemble can provide a more stable and generalizable prediction, reducing the variability that might be present in any single model. This approach is particularly beneficial in medical imaging, where the heterogeneity of the data and the complexity of the underlying biological processes can make single-model predictions less reliable.",
  "optimization/encoding": "Diffusion-weighted images were pre-processed using a semi-automatic pipeline to prevent bias related to outcome measures. Initially, skull-stripping was performed using FSL\u2019s Brain Extraction Tool (BET), followed by manual corrections to remove uninformative slices containing extra-cerebral tissue. Brain voxel intensity values were then normalized to the average of the non-lesioned parenchyma by subtracting a personalized lesion mask from the BET brain mask. The normalized, skull-stripped DWIs were cropped using a square prism bounding box from the final brain mask and resized to 96 x 96 x 24 voxels.\n\nDuring training, images were passed in batches of 16 with on-the-fly augmentation. This included a power law transform with a gamma randomly chosen from the range [0.9, 1.1] to vary the contrast between the lesion and healthy tissue. A rotation about the z-axis with an angle randomly chosen from the range [-15, +15] was applied to account for variations in head orientation. Additive Gaussian noise with a value randomly chosen from the range [-0.01, +0.01] was added to account for variations in image quality. Augmentation was not applied to the validation or test sets.\n\nThe model parameters were trained using the Adam optimizer with a learning rate of 1e-4 and a binary cross-entropy loss function. To avoid overfitting, an early stopping criterion was implemented to freeze model parameters on the epoch where the loss on the validation set no longer improved for 30 subsequent epochs, without exceeding 150 epochs.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "The input features for our model are derived from diffusion-weighted imaging (DWI) scans. Specifically, the model utilizes the entire DWI volume as input, which means it processes the full 3D image data without extracting specific features manually. This approach allows the convolutional neural network (CNN) to learn and attend to relevant features directly from the imaging data.\n\nFeature selection in the traditional sense was not performed, as the model is designed to automatically identify important regions and features within the images through its attention mechanisms. The attention mechanism highlights areas of the image that contain crucial semantic features for classifying outcomes, such as poor vs. good outcomes. This process involves learning parameters that map the sum of activations from global feature vectors and local convolutional layers to an \"attention\" score, ensuring that the model focuses on the most relevant parts of the image.\n\nThe model was trained using a leave-one-center-out (LOCO) cross-validation scheme, which ensures that the training and validation sets are stratified by site provenance and outcome. This method helps to prevent data leakage and ensures that the model generalizes well to unseen data. The images were augmented during training to account for variations in contrast, head orientation, and image quality, further enhancing the model's robustness.",
  "optimization/fitting": "The model employed a leave-one-center-out (LOCO) cross-validation scheme to ensure robust training and evaluation. This approach helps to mitigate overfitting by evaluating the model on data from centers that were not included in the training set, thus providing an unbiased assessment of its performance.\n\nTo further prevent overfitting, on-the-fly data augmentation techniques were applied during training. These included a power law transform to vary contrast, rotations to account for head orientation variations, and additive Gaussian noise to simulate image quality variations. These augmentations increased the effective size and diversity of the training dataset, helping the model to generalize better.\n\nAdditionally, an early stopping criterion was implemented. Training was halted when the validation loss did not improve for 30 consecutive epochs, with a maximum limit of 150 epochs. This ensured that the model did not overfit to the training data by continuing to train beyond the point of diminishing returns.\n\nThe model was trained using the Adam optimizer with a learning rate of 1e-4 and a binary cross-entropy loss function. The use of a low learning rate and early stopping helped to prevent overfitting by ensuring that the model parameters were updated gradually and training was stopped before the model could memorize the training data.\n\nThe model architecture was designed to be lightweight, with variations in the initial number of feature maps, the multiplicity of feature maps per level, and the introduction of a bottleneck layer between levels. This design choice helped to reduce the risk of overfitting by limiting the model's capacity.\n\nTo address the potential for underfitting, an ensemble of eight lightweight models was constructed. By averaging the classification probabilities over these models, the ensemble approach improved the overall performance and robustness of the predictions.\n\nThe model was evaluated on a held-out test set from centers not included in the training process. This evaluation provided an unbiased measure of the model's performance and ensured that it could generalize to new, unseen data. Metrics such as accuracy, area under the curve (AUC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) were computed on the merged prediction vector from all folds, rather than averaging the results of each fold. This approach accounted for the variable number of patients per center and provided a more accurate assessment of the model's performance.",
  "optimization/regularization": "To prevent overfitting, several techniques were employed during the training process. One key method was the use of on-the-fly data augmentation, which involved applying random transformations to the training images. This included a power law transform to vary contrast, rotations to account for head orientation variations, and additive Gaussian noise to simulate differences in image quality. These augmentations helped the model generalize better by exposing it to a wider variety of image conditions.\n\nAdditionally, an early stopping criterion was implemented. This involved monitoring the model's performance on a validation set and halting the training process if the validation loss did not improve for 30 consecutive epochs. This ensured that the model did not continue to train beyond the point where it started to overfit the training data, thereby preserving its ability to generalize to new, unseen data.\n\nThe use of a leave-one-center-out (LOCO) cross-validation scheme also contributed to regularization. This approach involved training the model on data from multiple centers while validating it on data from a held-out center. This process was repeated for each center, ensuring that the model's performance was evaluated on diverse datasets, further enhancing its generalization capabilities.\n\nMoreover, the model's architecture was designed to be lightweight, with variations in the number of feature maps and the introduction of bottleneck layers. This reduced the model's complexity, making it less prone to overfitting while maintaining performance. An ensemble of eight lightweight models was also constructed, averaging their classification probabilities to improve robustness and generalization.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we employed a power law transform with a gamma value randomly chosen from the range [0.9, 1.1] to adjust contrast, a rotation about the z-axis with angles ranging from [-15, 15] to account for variations in head orientation, and additive Gaussian noise with a standard deviation randomly chosen from the range [-0.01, 0.01] to simulate variations in image quality. These augmentations were applied during training but not to the validation or test sets.\n\nThe model parameters were trained using the Adam optimizer with a learning rate of 1e-4 and a binary cross-entropy loss function. To prevent overfitting, an early stopping criterion was implemented, freezing model parameters when the validation loss did not improve for 30 consecutive epochs, with a maximum of 150 epochs.\n\nThe deep learning models were constructed and trained on a NVIDIA Quadro RTX 8000 GPU using TensorFlow 2.2.0 in Python 3.8.5. The specific model files and optimization parameters are not explicitly provided in the publication, but the methods and configurations are thoroughly described, allowing for reproducibility.\n\nRegarding the availability and licensing of the configurations and parameters, the publication does not specify a particular license for the methods or code used. However, the detailed descriptions provided should enable other researchers to implement similar configurations and optimization schedules in their own work. For access to the specific model files and detailed code, interested parties may need to contact the authors directly or refer to any supplementary materials or repositories associated with the study.",
  "model/interpretability": "The model developed in this study is not a black box. It incorporates an attention mechanism within the convolutional neural network (CNN) architecture, which allows for interpretability. This attention mechanism highlights the areas of the brain that are crucial for the model's predictions, making it possible to visualize and understand which regions drive the classification outcomes.\n\nThe attention maps generated by the model focus on the lesioned areas of the brain, which are critical for predicting long-term functional outcomes post-stroke. These maps provide insights into how the model attends to specific parts of the brain, particularly the affected hemisphere, and how this attention correlates with lesion locations. For instance, the model's attention is often concentrated on the underlying white matter of critical regions like the temporo-parietal junction, which can be indicative of severe functional impairments.\n\nThe attention mechanism operates at multiple scales within the network. At the earliest stage (a1), the attention maps are fine-tuned to sharp changes in contrast and texture, focusing on smaller lesions or the intersection of larger lesions with healthy tissue. As we move deeper into the network (a2 and a3), the attention maps respond to larger receptive fields, coinciding more with the entire spatial extent of the lesions. This multi-scale attention ensures that the model considers both fine details and broader patterns within the lesions, making the predictions more robust and clinically relevant.\n\nMoreover, the model's attention shifts depending on the predicted outcome. For true positives (correctly predicted poor outcomes), the attention is heavily focused on lesioned areas. For true negatives (correctly predicted good outcomes), the attention may shift to non-lesioned areas, suggesting that the model learns to suppress strong attention on lesioned regions to predict a better outcome. This dynamic attention mechanism provides a transparent view of how the model makes its predictions, enhancing its trustworthiness and clinical applicability.",
  "model/output": "The model is a classification model. It is designed to predict the long-term functional outcome of stroke patients, specifically categorizing the outcome as either good or poor. The outcome is assessed using the modified Rankin Scale (mRS), which is dichotomized into good (mRS \u2264 2) versus poor (mRS \u2265 3). The model outputs the probability of a patient having a poor outcome based on diffusion-weighted imaging (DWI) acquired at day 1 post-stroke.\n\nThe model employs an attention mechanism within a convolutional neural network (CNN) architecture. This attention mechanism helps to highlight and focus on regions of the brain that are crucial for predicting the outcome. The attention maps generated by the model can be visualized to understand which areas of the brain the model is attending to for its predictions.\n\nThe model's performance is evaluated using metrics such as accuracy, area under the curve (AUC) from a receiver-operator curve (ROC) analysis, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). These metrics provide a comprehensive assessment of the model's ability to classify patients into good or poor outcome categories.\n\nThe model was trained and evaluated using a leave-one-center-out (LOCO) cross-validation paradigm, ensuring that the model's performance is assessed on external test sets from different centers. This approach helps to validate the model's generalizability and robustness across different patient populations. The model's predictions are merged to form a prediction vector, and confidence intervals are calculated through bootstrapping to provide a reliable estimate of the model's performance.",
  "model/duration": "The execution time for test predictions using the deep learning ensemble model was quite efficient. With GPU acceleration, the average runtime for a patient to go through all eight models was approximately 0.46 seconds. When using a CPU, the runtime was slightly slower, averaging around 0.60 seconds. This efficiency makes the model suitable for integration into clinical workflows, as it adds minimal extra time to the existing processes. The model's ability to generate predictions and attention maps in less than a second per patient is a significant advantage, ensuring that it can be readily adopted in hospital settings to improve patient care and outcomes.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed a leave-one-center-out (LOCO) cross-validation scheme. This approach involved training and evaluating the model on data from multiple centers, with each center's data being held out once for testing. This method ensures that the model's performance is assessed on completely unseen data from different hospitals, providing a robust measure of its generalizability.\n\nDuring training, images were passed in batches of 16 with on-the-fly augmentation techniques applied. These augmentations included a power law transform to vary contrast, rotations to account for head orientation variations, and additive Gaussian noise to simulate image quality variations. These augmentations were not applied to the validation or test sets.\n\nThe model parameters were trained using the Adam optimizer with a specific learning rate and a binary cross-entropy loss function. To prevent overfitting, an early stopping criterion was implemented, freezing model parameters when the validation loss no longer improved for 30 consecutive epochs, with a maximum of 150 epochs.\n\nFor evaluation, independent predictions of the probability of an out-of-center patient having a good or poor outcome were obtained for each fold. These predictions were then merged to form a prediction vector corresponding to the total number of patients in the study. Metrics such as unbiased accuracy, area under the curve (AUC) from a receiver-operator curve (ROC) analysis, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) were computed on this vector.\n\nThe model's performance was compared against reference imaging biomarkers using univariate logistic regression models based on lesion volume and ASPECTS. These models were also evaluated using the same LOCO cross-validation paradigm. The AUC of each logistic regression classifier was compared to the most accurate deep learning model using the DeLong test, and differences in sensitivity were compared with the McNemar test.\n\nThe attention maps generated by the model were analyzed to inspect model interpretability. These maps were visualized in relation to lesion location over the cohort, providing insights into the areas of the brain that the model focused on for classification. The attention maps were projected back onto the original MRI volumes and brought to MNI space for analysis. Average attention maps were calculated using the normalized attention maps of all patients and models, and lesion probability maps were obtained by averaging all normalized lesion masks.\n\nThe evaluation method ensured a comprehensive and unbiased assessment of the model's performance, highlighting its ability to generalize to new, unseen data from multiple centers.",
  "evaluation/measure": "To evaluate the performance of our deep learning models, we computed several key metrics. These include unbiased out-of-center accuracy, area under the curve (AUC) from a receiver-operator curve (ROC) analysis, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). These metrics were calculated on a merged prediction vector formed from the test set predictions across all folds, ensuring that the variable number of patients per center was taken into account.\n\nThe AUC provides a comprehensive measure of the model's ability to distinguish between good and poor outcomes, while accuracy gives an overall sense of correctness. Sensitivity and specificity assess the model's performance in identifying true positives and true negatives, respectively. PPV and NPV offer insights into the likelihood of a positive or negative prediction being correct.\n\nWe also calculated 95% confidence intervals through bootstrapping the merged prediction vector with 10,000 iterations, providing a robust estimate of the model's performance variability. This approach ensures that our reported metrics are reliable and representative of the model's true capabilities.\n\nThe metrics reported are standard in the literature for evaluating classification models, particularly in medical imaging. They provide a clear and comprehensive view of the model's performance, making it comparable to other studies in the field. The use of a leave-one-center-out cross-validation scheme further ensures that the model's performance is generalizable to new, unseen data from different centers.",
  "evaluation/comparison": "To evaluate the superiority of our deep learning approach, we conducted a comparison against two reference imaging biomarkers. These biomarkers are commonly used in clinical trials for predicting functional outcomes. Specifically, we tested our model against univariate logistic regression models that utilized lesion volume and ASPECTS (Alberta Stroke Program Early CT Score). Both lesion volume and ASPECTS were manually segmented on DWI (Diffusion-Weighted Imaging) images by an experienced neurologist who was blinded to patient outcomes. The logistic regression models were trained and evaluated using the same leave-one-center-out (LOCO) cross-validation paradigm as our deep learning model. This approach ensured that the comparison was fair and unbiased.\n\nWe compared the area under the curve (AUC) of each logistic regression classifier to the most accurate deep learning model using the DeLong test. Additionally, we set the probability threshold of the logistic regression classifiers to match the specificity of the most accurate deep learning model. This allowed us to compare differences in sensitivity using the McNemar test. By conducting these comparisons, we aimed to demonstrate that our deep learning model outperforms traditional methods in predicting stroke outcomes.",
  "evaluation/confidence": "The evaluation of our model included the computation of 95% confidence intervals (CI) through bootstrapping the merged prediction vector with 10,000 iterations. This approach provided a robust measure of the variability and reliability of our performance metrics, including accuracy, area under the curve (AUC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV).\n\nOur deep learning ensemble model demonstrated statistically significant superiority over traditional imaging biomarkers. Specifically, the ensemble model had a significantly higher AUC compared to lesion volume (p = 0.04) and ASPECTS (p = 0.008). Additionally, with specificities fixed to 0.87, the ensemble model showed significantly higher sensitivity than both lesion volume (p = 0.002) and ASPECTS (p = 0.002). These results indicate that our model's performance is not only higher but also statistically significant, reinforcing its reliability and potential for clinical application.",
  "evaluation/availability": "Not applicable."
}