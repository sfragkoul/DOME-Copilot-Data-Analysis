{
  "publication/title": "Prognostic Significance and Associations of Neural Network-Derived Electrocardiographic Features.",
  "publication/authors": "Sau A, Ribeiro AH, McGurk KA, Pastika L, Bajaj N, Gurnani M, Sieliwonczyk E, Patlatzoglou K, Ardissino M, Chen JY, Wu H, Shi X, Hnatkova K, Zheng SL, Britton A, Shipley M, Andr\u0161ov\u00e1 I, Novotn\u00fd T, Sabino EC, Giatti L, Barreto SM, Waks JW, Kramer DB, Mandic D, Peters NS, O'Regan DP, Malik M, Ware JS, Ribeiro ALP, Ng FS",
  "publication/journal": "Circulation. Cardiovascular quality and outcomes",
  "publication/year": "2024",
  "publication/pmid": "39540287",
  "publication/pmcid": "PMC7616866",
  "publication/doi": "10.1161/circoutcomes.123.010602",
  "publication/tags": "- Artificial Intelligence\n- Electrocardiography\n- Machine Learning\n- Neural Networks\n- Cardiovascular Disease\n- Phenotyping\n- Genomics\n- Prognostic Significance\n- Transfer Learning\n- Unsupervised Learning",
  "dataset/provenance": "The dataset utilized in this study is derived from multiple cohorts, each with distinct characteristics and sources. The primary cohort is the Clinical Outcomes in Digital Electrocardiography (CODE) study, which comprises 2,322,513 ECG records from 1,676,384 unique patients across 811 counties in the state of Minas Gerais, Brazil. These records were collected through the Telehealth Network of Minas Gerais (TNMG) between 2010 and 2017, focusing on patients over 16 years old with valid ECGs. The CODE cohort is linked to public mortality databases, providing a comprehensive dataset for analysis.\n\nAnother significant cohort is the UK Biobank, a longitudinal study involving over 500,000 volunteers aged 40-69 at the time of enrollment between 2006 and 2010. This cohort includes detailed health and lifestyle information, physical measurements, and biological samples. For this study, 42,386 subjects with digital ECGs taken during the second visit were analyzed.\n\nThe Whitehall II cohort consists of British civil servants who underwent repeated medical investigations, including digital 12-lead ECGs performed between 2007 and 2009. A total of 5,066 participants were included in this analysis.\n\nThe ELSA-Brasil cohort includes 15,105 Brazilian public servants aged 35 to 74 at enrollment, with 13,739 subjects having both ECG and outcome data available for analysis.\n\nThe SaMi-Trop cohort is a prospective study of 1,631 patients with chronic Chagas cardiomyopathy, with digital ECGs performed in 2011-2012. Notably, 83% of this cohort had abnormal ECGs.\n\nThe Beth Israel Deaconess Medical Center (BIDMC) cohort comprises routinely collected data from over 188,972 subjects over 16 years old with valid ECGs performed from 2000 to 2023. This dataset is linked to mortality records, providing valuable information for survival analysis.\n\nThese cohorts have been utilized in various previous studies, contributing to the broader understanding of cardiovascular health and outcomes. The CODE-CNN model, trained on the CODE dataset, has been used to extract 5120 features from ECGs, which were then employed for clustering subjects into phenogroups. This model and its features are available for further research and validation, promoting reproducibility and collaboration within the scientific community.",
  "dataset/splits": "The study utilized six distinct cohorts, each serving as a separate data split. The primary cohort is the CODE dataset, which comprises 2,322,513 ECG records from 1,676,384 unique patients. This dataset is particularly large and diverse, encompassing individuals from 811 counties in the state of Minas Gerais, Brazil.\n\nThe other five cohorts include the Whitehall II study, which consists of 3,370 data points; the UK Biobank, with 25,723 data points; the ELSA-Brasil cohort, containing 13,739 data points; the SaMi-Trop cohort, which includes 1,631 data points; and the BIDMC cohort, with an unspecified number of data points.\n\nThe CODE dataset was further subdivided into phenogroups for detailed analysis. Phenogroup A includes 450,336 data points, Phenogroup B has 481,026 data points, and Phenogroup C comprises 627,059 data points. These phenogroups were derived using K-means clustering based on neural network-derived ECG features, with K=3 selected through the elbow method.\n\nThe distribution of data points across these phenogroups varies significantly in terms of demographic and clinical characteristics. For instance, the mean age differs across phenogroups, with Phenogroup B having the highest mean age of 58.20 years. The prevalence of conditions such as hypertension, diabetes mellitus, and various cardiac abnormalities also varies, with Phenogroup C showing the highest prevalence of hypertension at 42.1%.\n\nThe other cohorts, while smaller in size, provide valuable external validation for the findings derived from the CODE dataset. Each cohort has its unique demographic and clinical profile, contributing to the robustness of the study's conclusions. The Whitehall II cohort, for example, consists predominantly of older individuals with a mean age of around 65 years, while the SaMi-Trop cohort focuses on patients with chronic Chagas cardiomyopathy, a condition with a high prevalence of abnormal ECGs.",
  "dataset/redundancy": "The datasets used in our study were carefully selected to ensure diversity and independence. We utilized five external datasets for validation, each with distinct characteristics and populations. These datasets included the Clinical Outcomes in Digital Electrocardiography (CODE) cohort, the Whitehall II study, the UK Biobank, the Longitudinal Study of Adult Health (ELSA-Brasil), and the S\u00e3o Paulo-Minas Gerais Tropical Medicine Research Center (SaMi-Trop) study. Each of these datasets was chosen to represent different geographical locations, ethnic backgrounds, and health conditions, ensuring a broad and representative sample.\n\nThe CODE cohort, for instance, consists of ECG records from patients in Minas Gerais, Brazil, linked to public mortality databases. This dataset is particularly large, encompassing over 2.3 million ECG records from more than 1.6 million patients. The Whitehall II study, on the other hand, focuses on civil servants in the UK, providing a different demographic profile. The UK Biobank includes over 500,000 volunteers aged 40-69, offering a comprehensive set of health and lifestyle data. ELSA-Brasil involves Brazilian public servants, while SaMi-Trop concentrates on patients with chronic Chagas cardiomyopathy.\n\nTo ensure the independence of the training and test sets, we employed a transfer learning approach. An existing AI-ECG model, trained to classify six common ECG diagnoses, was used to extract neural network-derived ECG features. These features were then analyzed using unsupervised machine learning techniques to identify clinically distinct phenogroups. The validation process involved applying the model to the five external datasets, which were not used in the initial training phase. This method ensured that the training and test sets were independent, reducing the risk of data leakage and overfitting.\n\nThe distribution of our datasets compares favorably to previously published machine learning datasets in the field of electrocardiography. Our approach of using diverse and independent datasets for validation is a strength, as it enhances the generalizability of our findings. The inclusion of datasets from different continents and with varying health conditions ensures that our model's performance is robust and applicable across different populations. This diversity is crucial for developing models that can be reliably used in clinical settings, where patient populations can vary significantly.",
  "dataset/availability": "The data from the various cohorts used in this study have different availability and access restrictions.\n\nThe UK Biobank data are available upon application through their official website. Similarly, the Whitehall II data can be accessed by applying through the designated portal at University College London.\n\nThe Clinical Outcomes in Digital Electrocardiography (CODE) cohort data, including the CODE-15% subset, and the Longitudinal Study of Adult Health (ELSA-Brasil) cohort data have restrictions applied to additional clinical information.\n\nThe Beth Israel Deaconess Medical Center (BIDMC) dataset is restricted due to ethical limitations. Access to this dataset is granted to researchers affiliated with educational or research institutions. Requests for data access should be directed to the corresponding author of this article, who will forward them to the relevant steering committee for review.\n\nThe S\u00e3o Paulo-Minas Gerais Tropical Medicine Research Center (SaMi-Trop) cohort data also has restrictions applied to additional clinical information.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is a hybrid approach that combines both supervised and unsupervised learning techniques. Specifically, we employed a convolutional neural network (CNN) architecture for the supervised learning component, which was initially trained to classify six common ECG diagnoses. This CNN model, referred to as CODE-CNN, was developed using the Keras framework with a TensorFlow backend.\n\nFor the unsupervised learning component, we utilized K-means clustering to identify phenogroups based on the neural network-derived ECG features extracted from the penultimate layer of the CNN. The choice of K-means clustering was informed by the elbow method, which helped determine the optimal number of clusters (k=3).\n\nThe algorithm is not entirely new, as it builds upon established machine learning techniques. The CNN architecture and K-means clustering are well-known methods in the field of machine learning and have been extensively used in various applications. The innovation lies in the application of these techniques to ECG data and the specific transfer learning approach used to repurpose the CNN for identifying clinically meaningful phenogroups.\n\nThe focus of our publication is on the application of these machine learning techniques to ECG data and the clinical implications of the findings, rather than the development of a new machine learning algorithm. Therefore, it was published in a cardiovascular outcomes journal rather than a machine-learning journal. The primary goal was to demonstrate the potential of neural network-derived ECG features for risk prediction and to validate these findings across diverse populations.",
  "optimization/meta": "The model employs a hybrid machine learning approach that integrates both supervised and unsupervised learning techniques. This hybrid pipeline leverages neural network-derived ECG features to identify phenogroups from the 12-lead ECG. The supervised component involves training a convolutional neural network (CNN) to classify common ECG diagnoses related to rhythm and conduction disease. The unsupervised component then uses these learned features to identify clinically distinct phenogroups with prognostic significance.\n\nThe CNN architecture identifies thousands of ECG features during training, which are not limited to conventional ECG parameters and morphology. These features are extracted from the penultimate layer of the CNN and are used as input for the unsupervised learning phase. This approach allows the model to capture complex patterns and relationships within the ECG data that may not be easily discernible by human interpretation.\n\nThe training data for the supervised component is derived from a large dataset of ECGs, ensuring that the features learned are robust and generalizable. The unsupervised learning phase is then applied to external datasets, including diverse populations across different continents, to validate the identified phenogroups. This external validation helps to confirm the applicability and reliability of the model in various clinical settings.\n\nThe independence of the training data is maintained by using separate datasets for the supervised and unsupervised learning phases. The supervised component is trained on a specific dataset, while the unsupervised component is validated on external datasets that were not used in the initial training process. This ensures that the model's performance is not biased by the training data and that the identified phenogroups are truly representative of the underlying patterns in the ECG data.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the electrocardiogram (ECG) signals were suitable for input into the neural network model. Initially, twelve-lead ECGs were preprocessed by removing baseline drift, which is essential for eliminating artifacts that could interfere with the analysis. The signals were then resampled to a frequency of 400 Hz to standardize the data. To achieve a consistent input size for the neural network, zero padding was applied, resulting in a signal with 4096 samples for each lead over a 10-second recording. This preprocessing step ensured that the input data had a uniform length, which is crucial for training the convolutional neural network (CNN). No normalization was performed on the data, allowing the raw signal characteristics to be preserved for the model. The preprocessed ECGs were then used as input to the CNN model, which was trained to detect six common ECG abnormalities. The features extracted from the penultimate layer of the CNN were subsequently used for further analysis, including clustering subjects into clinically meaningful phenogroups.",
  "optimization/parameters": "In our study, we focused on the selection of the parameter K, which represents the number of clusters in our model. We explored various values of K, ranging from 2 to 9. To determine the optimal number of clusters, we employed the elbow method. This involved plotting the within-cluster sum of squares (WCSS) against the number of clusters (k) and identifying the point where the rate of decrease sharply slows, indicating the optimal number of clusters. Based on this analysis, we selected K = 3 as it provided a clear elbow point and was supported by sensitivity analyses. Higher values of K did not significantly improve the differentiation of phenogroups with respect to prognosis. Specifically, K = 2 resulted in a smaller risk difference between high and low-risk groups compared to K = 3, with unadjusted hazard ratios of 2.54 (2.49-2.59) for K = 2 and 3.03 (2.97-3.10) for K = 3. This selection process ensured that our model effectively captured the underlying structure of the data without overfitting.",
  "optimization/features": "In our study, we utilized a total of 5120 neural network-derived ECG features as input for clustering subjects into phenogroups. These features were extracted from the penultimate layer of a convolutional neural network (CNN) model, which was originally trained to classify six common ECG diagnoses. No explicit feature selection was performed on these 5120 features. The features were derived from the CODE data set, and the same set of features was used across all cohorts without any further selection or modification. This approach ensured that the features were consistent and comparable across different data sets, facilitating the identification of clinically meaningful phenogroups.",
  "optimization/fitting": "The fitting method employed in our study involved the use of neural networks (NN) to derive features from 12-lead ECG signals. The number of parameters in our neural network model was indeed larger than the number of training points, which is a common scenario in deep learning applications. To address the potential issue of overfitting, we implemented several strategies.\n\nFirstly, we utilized cross-validation techniques to ensure that our model generalized well to unseen data. This involved splitting our dataset into training and validation sets multiple times and averaging the performance metrics. Additionally, we employed regularization techniques such as dropout layers within our neural network architecture. Dropout randomly sets a fraction of input units to zero at each update during training time, which helps prevent overfitting by ensuring that the model does not become too reliant on any single neuron.\n\nFurthermore, we performed sensitivity analyses to explore different values of K in our clustering algorithm. This exploratory analysis supported the selection of an optimal number of clusters, K = 3, which did not highlight phenogroups with significantly differential prognosis for higher values of K. This step was crucial in ensuring that our model was not overfitting to the training data by identifying an excessive number of clusters.\n\nTo rule out underfitting, we carefully designed our neural network architecture and training process. We used a well-established AI-ECG model trained to identify common diagnoses, ensuring that the model had sufficient capacity to learn relevant features from the ECG data. Additionally, we validated our findings in multiple external datasets across different populations, demonstrating the robustness and generalizability of our model.\n\nIn summary, our approach to fitting the model included cross-validation, regularization through dropout, sensitivity analyses, and extensive validation in diverse datasets. These measures collectively ensured that our model neither overfitted nor underfitted the data, providing reliable and generalizable results.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are available for reference. These details are provided within the supplemental materials of our publication. The specific configurations and parameters can be found in the figures and descriptions included in these materials.\n\nFor instance, Figure S1 in the supplemental materials illustrates the elbow plot used for selecting the optimal number of clusters (k) in our analysis. This plot is crucial for understanding the clustering process and the rationale behind our chosen hyper-parameters.\n\nAdditionally, Figure S2 provides a visualization of the clustering results using Principal Component Analysis (PCA) followed by t-Distributed Stochastic Neighbor Embedding (t-SNE). This figure helps in interpreting the dimensionality reduction and clustering outcomes, which are integral to our optimization process.\n\nThe supplemental materials are accessible to readers and can be used to replicate or build upon our findings. The figures and descriptions are provided under the standard licensing terms associated with the journal publication, ensuring that researchers can utilize this information for further studies or applications.\n\nThe model files and optimization schedules are not explicitly detailed in the provided information. However, the methods and parameters described in the supplemental materials offer a comprehensive guide to the optimization process, enabling others to implement similar approaches in their research.",
  "model/interpretability": "The model employed in our study is not a black box. To enhance interpretability, we utilized a technique inspired by gradient-weighted class activation mapping, which is commonly used in computer vision. This approach allows us to understand which elements of the ECG contribute most significantly to the determination of phenogroups.\n\nSpecifically, we modified a convolutional neural network (CNN) by removing the final layer of the original trained model and adding a new layer with a fixed operation. This new layer takes the 5120 features from the trained CNN and computes a signal importance map for a given input of a 12-lead ECG signal. This process helps in identifying the critical regions of the ECG that the model focuses on when making predictions.\n\nBy visualizing these importance maps, we can gain insights into which parts of the ECG are most influential in classifying different phenogroups. This transparency is crucial for clinical applications, as it allows healthcare providers to understand the basis for the model's predictions and to trust the results more confidently.",
  "model/output": "The model's output involves the identification of prognostically significant phenogroups from the 12-lead ECG using neural network-derived features. This process begins with an established AI-ECG model trained to identify six common diagnoses related to rhythm and conduction disease. The features extracted from the penultimate layer of the convolutional neural network are then used in an unsupervised machine learning approach to cluster individuals into distinct phenogroups.\n\nThe primary output of the model is the classification of individuals into these phenogroups, which are associated with different prognostic outcomes. The model does not directly predict a continuous value but rather categorizes individuals based on their ECG features. This classification is crucial for understanding the phenotypic and genotypic associations of these phenogroups, which were further explored through phenome-wide association studies (PheWAS) and genome-wide association studies (GWAS).\n\nThe model's performance was validated across five external datasets from diverse populations, ensuring its applicability and generalizability. The phenogroups identified have been shown to have important clinical implications, including associations with adverse prognosis and cardiovascular diseases. The use of unsupervised learning allows for the discovery of novel patterns in the ECG data that may not be apparent through traditional methods.\n\nIn summary, the model's output is a classification of individuals into phenogroups based on neural network-derived ECG features, which have significant prognostic and clinical relevance. This approach highlights the potential of AI-ECG models to provide insights beyond their original training tasks, offering a powerful tool for risk prediction and personalized medicine.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the CODE-CNN model, along with its trained weights, is publicly available on GitHub at https://github.com/antonior92/automatic-ecg-diagnosis. This repository provides access to the neural network model used for detecting common ECG abnormalities. The remaining programming code used in the study will be made available upon reasonable request to the corresponding author. This approach ensures that the research is reproducible and that other researchers can build upon the work presented.",
  "evaluation/method": "The evaluation of our method involved a comprehensive approach to ensure its robustness and generalizability. We began by applying unsupervised machine learning techniques to cluster subjects into phenogroups based on neural network-derived ECG features. The elbow method was employed to determine the optimal number of phenogroups, which was set at three. These phenogroups were then analyzed for their prognostic significance through survival analysis.\n\nSurvival analysis was conducted using Kaplan-Meier plots and Cox proportional hazards regression modeling. This allowed us to estimate hazard ratios for mortality while adjusting for known covariates such as age, sex, and cardiovascular comorbidities. The analysis demonstrated that the phenogroups had distinct mortality profiles, with one phenogroup showing a significantly higher risk of mortality compared to the others.\n\nTo validate our findings, we performed external validation across five diverse cohorts: the Whitehall II study, the UK Biobank, the ELSA-Brasil cohort, the SaMi-Trop cohort, and the BIDMC cohort. These cohorts represented different populations, including volunteers, primary care patients, and those with established cardiomyopathy. The predictive ability of the phenogroups was retained in these external datasets, further supporting the generalizability of our method.\n\nAdditionally, we conducted phenome-wide association studies (PheWAS) to explore the biological basis underlying the differences in prognosis between the phenogroups. This involved investigating the associations of the phenogroups with a wide range of phenotypes and genetic variants. The PheWAS analyses provided novel insights into the phenotypic and genotypic associations of the ECG phenogroups.\n\nIn summary, our evaluation method included both internal and external validation, survival analysis, and phenome-wide association studies. These approaches collectively demonstrated the prognostic significance and generalizability of the phenogroups derived from neural network-derived ECG features.",
  "evaluation/measure": "In the evaluation of our model, we focused on several key performance metrics to assess its effectiveness in predicting mortality and identifying prognostically significant phenogroups from 12-lead ECG data. One of the primary metrics reported is the accuracy of the model, which was found to be 71.6% when using age and sex alone to predict 5-year mortality. Additionally, the area under the receiver operating characteristic curve (AUC-ROC) was reported as 0.78, indicating a strong discriminative ability of the model.\n\nWe also utilized Kaplan-Meier plots to display cumulative mortality and employed the log-rank test to compare survival curves between different phenogroups. This allowed us to visually and statistically assess the differences in survival rates among the identified phenogroups.\n\nCox proportional hazards regression modeling was used to estimate hazard ratios (HRs) for mortality, adjusting for other known variables. This method provided a weighted average of the true hazard ratios over the follow-up period, offering insights into the relative risk of mortality associated with each phenogroup.\n\nIn terms of representativeness, the metrics used are consistent with those commonly reported in the literature for similar predictive models in cardiovascular research. The inclusion of accuracy, AUC-ROC, Kaplan-Meier plots, log-rank tests, and hazard ratios ensures a comprehensive evaluation of the model's performance. These metrics are widely accepted and provide a clear picture of the model's predictive power and clinical relevance.",
  "evaluation/comparison": "In our study, we did not directly compare our approach with publicly available methods on benchmark datasets. Our focus was on the novel application of neural network-derived ECG features for risk prediction and the validation of our findings across diverse populations. We used a transfer learning approach with a convolutional neural network (CNN) originally trained to detect common ECG abnormalities. This model, referred to as CODE-CNN, was repurposed to extract features that were then used for clustering subjects into clinically meaningful phenogroups.\n\nWhile we did not perform a direct comparison with supervised machine learning approaches, we acknowledge that such methods might more accurately identify individuals at high risk of death. Our study primarily explored the biological basis underlying the differences in prognosis between the phenogroups and identified phenotypic and genotypic associations through phenome-wide association studies (PheWAS) and genome-wide association studies (GWAS).\n\nWe also did not compare our method to simpler baselines. The complexity of our approach was driven by the need to handle a large dataset with a high number of features per ECG and to perform detailed downstream phenogroup exploration. This complexity made a detailed exploration of other unsupervised machine learning methodologies beyond the scope of our current work. However, we recognize that other unsupervised methods might identify different phenogroups, and this remains an area for future research.",
  "evaluation/confidence": "In our study, we employed several statistical methods to ensure the robustness and significance of our findings. For survival analysis, we used Cox proportional hazards regression modeling to estimate hazard ratios for mortality, correcting for other known variables. We did not evaluate the proportional hazards assumption, in line with recent recommendations, and thus the hazard ratios should be interpreted as a weighted average over the follow-up period.\n\nFor phenome-wide association studies (PheWAS), we used logistic regression to investigate associations between ECG phenogroups and incident diseases, as well as univariate correlation to explore associations with phenotypes. We adjusted for multiple testing using Bonferroni correction to control for false positives. Given the small number of subjects in phenogroup C, this group was excluded from PheWAS analyses.\n\nIn our genome-wide association study (GWAS), we included single nucleotide polymorphisms (SNPs) with a minor allele frequency greater than 0.1% and an imputation INFO score above 0.4. We used a generalized linear mixed model association analysis to adjust for population structure. Top SNPs were identified based on a P-value threshold of less than 5x10-8, ensuring statistical significance.\n\nTo assess the significance of our model's predictions, we computed signal importance maps using a modified gradient-weighted class activation mapping technique. This approach helped us understand which elements of the ECG contributed most significantly to phenogroup determination.\n\nOverall, our statistical analyses were designed to provide confidence in the performance metrics and the significance of our results. We ensured that our findings were robust by correcting for multiple testing, adjusting for known variables, and using appropriate statistical models. The exclusion of phenogroup C from certain analyses was necessary due to its small sample size, but this does not diminish the validity of our conclusions for the other phenogroups.",
  "evaluation/availability": "The raw evaluation files for the studies discussed in our publication are not publicly available. The data comes from several cohorts, including the Clinical Outcomes in Digital Electrocardiography (CODE) study, the Whitehall II study, the UK Biobank, the Longitudinal Study of Adult Health (ELSA-Brasil), the S\u00e3o Paulo-Minas Gerais Tropical Medicine Research Center (SaMi-Trop) study, and the Beth Israel Deaconess Medical Center (BIDMC) cohort. Each of these studies has its own ethical approvals and data usage protocols, which restrict public access to the raw data.\n\nFor researchers interested in accessing the data, specific procedures and approvals are required. These procedures are in place to ensure the ethical use of the data and to protect participant privacy. Interested parties should contact the respective institutions or ethics committees for information on how to apply for data access. The data usage agreements and licenses vary by cohort, and detailed information can be obtained through the respective study coordinators or ethics committees."
}