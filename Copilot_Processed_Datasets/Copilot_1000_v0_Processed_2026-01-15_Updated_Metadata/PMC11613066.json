{
  "publication/title": "Comparative analysis of machine learning models for efficient low back pain prediction using demographic and lifestyle factors.",
  "publication/authors": "Kim JH",
  "publication/journal": "Journal of back and musculoskeletal rehabilitation",
  "publication/year": "2024",
  "publication/pmid": "39031340",
  "publication/pmcid": "PMC11613066",
  "publication/doi": "10.3233/bmr-240059",
  "publication/tags": "- Machine Learning\n- Low Back Pain\n- Feature Selection\n- Model Training\n- Hyperparameter Tuning\n- Model Evaluation\n- Chronic Pain Prediction\n- Data Collection\n- Health Surveys\n- Medical Diagnosis",
  "dataset/provenance": "The dataset used in this study was sourced from the Korea National Health and Nutrition Examination Survey (KNHANES), focusing on adults over 50 years of age. The dataset consisted of 18,215 data points collected over six years. This is a significant increase compared to the previous study, which used data from 6,119 people over two years. The larger dataset is expected to provide a more representative sample of the subjects, reducing the risk of overfitting and improving the model's ability to generalize.\n\nThe dataset included 4,228 individuals who had experienced low back pain (LBP) for more than 30 days within the last three months, assigned to the LBP group, and 13,987 individuals assigned to the painless control group. A total of 22 predictor variables were selected, encompassing demographic variables such as age, gender, height, and weight; physical activity variables like vigorous physical activity, strength training, and walking days; occupational variables including occupation type and average working hours; and lifestyle variables such as smoking, housing type, and health-related quality of life.\n\nThe dataset was pre-processed to address imbalances between the LBP and control groups using techniques like RandomUnderSampler and synthetic minority oversampling technique (SMOTE). Outliers were detected and processed using the Interquartile Range (IQR) method, and numerical variables were scaled with StandardScaler. Categorical variables were converted using the LabelEncoder function. After pre-processing, the data was divided into training and test sets at a 9:1 ratio for model training and performance evaluation.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a test set. The data was split in a 9:1 ratio, meaning 90% of the data was used for training the models, and 10% was reserved for testing their performance.\n\nThe total number of subjects in the study was 18,215, with 4,228 people assigned to the low back pain (LBP) group and 13,987 to the painless control group. Given the 9:1 split, approximately 16,393 data points were used for training, and around 1,822 data points were used for testing.\n\nThe distribution of data points in each split would reflect the overall distribution of the dataset, maintaining the same proportion of LBP and control group subjects in both the training and test sets. This ensures that the model is trained and evaluated on representative samples of the data.",
  "dataset/redundancy": "The dataset used in this study was divided into training and test sets at a 9:1 ratio. This split was performed after the data pre-processing steps, which included handling imbalanced data, detecting and processing outliers, scaling numerical variables, and encoding categorical variables.\n\nThe training and test sets are independent. This independence was enforced by randomly splitting the pre-processed data into these two sets. The random split ensures that the model is trained on one subset of the data and evaluated on a completely separate subset, which helps in assessing the model's generalization performance.\n\nRegarding the distribution of the dataset, it is notable that this study utilized a larger and more representative dataset compared to previous studies. The dataset included 18,215 people over six years, which is significantly larger than the 6,119 people over two years used in a previous study. This larger dataset is more likely to reflect the representativeness of the subjects and reduces the risk of overfitting, which can occur with smaller datasets. Additionally, the use of techniques like SMOTE and Random Under Sampler helped in balancing the dataset, addressing the imbalance between the number of low back pain (LBP) data and non-LBP data. This balancing ensures that the model can learn effectively from both types of data, leading to better performance.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm used in this study is Optuna, a tool for hyperparameter optimization. Optuna is not a new machine-learning algorithm but rather a framework designed to automate the process of finding the best hyperparameters for machine learning models. It is widely used in the machine learning community to enhance model performance by systematically searching through possible hyperparameter combinations.\n\nOptuna was chosen for its efficiency and effectiveness in optimizing hyperparameters, which is crucial for improving the performance of machine learning models. The algorithms used in this study include XGBoost, LGBM, CatBoost, and RandomForest, all of which are popular boosting and bagging algorithms known for their robustness and performance in various predictive tasks.\n\nThe decision to use Optuna in this context is driven by its ability to automatically search for optimal hyperparameters, thereby maximizing model performance. This approach ensures that each model is fine-tuned to its best possible configuration, leading to more accurate and reliable predictions. The use of Optuna is particularly beneficial in studies involving complex datasets and multiple algorithms, as it streamlines the optimization process and reduces the need for manual tuning.\n\nThe reason Optuna was not published in a machine-learning journal is that it is an established tool in the field, and its development and efficacy have already been well-documented in existing literature. Optuna's primary function is to facilitate the optimization of hyperparameters, making it a valuable asset for researchers and practitioners alike. Its integration into this study underscores its utility in enhancing model performance across different algorithms.",
  "optimization/meta": "The models developed in this study do not use data from other machine-learning algorithms as input. Instead, they utilize various boosting and bagging algorithms to construct predictive models for low back pain (LBP). The algorithms employed include XGBoost, LGBM, CatBoost, and RandomForest. Each of these algorithms was used to build models with variables selected through Recursive Feature Elimination with Cross-Validation (RFECV).\n\nThe study involved training each model using an iterative process to find the optimal feature set, followed by hyperparameter tuning with the Optuna library. This process ensured that each model was optimized for performance. The final models were trained using Stratified K-fold cross-validation with the training set data, setting the number of folds to 5.\n\nThe performance of these models was evaluated using test set data that was not used during the training process. This approach helped prevent overfitting and provided an objective evaluation of the models' performance. The evaluation metrics included accuracy, sensitivity, specificity, and F1-score.\n\nIn summary, the models are not meta-predictors but rather individual predictive models built using different machine-learning algorithms. The training data for each model was independent, ensuring that the evaluation was unbiased and reliable.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for preparing the dataset for the machine-learning algorithms. The dataset included a mix of numerical and categorical variables. Numerical variables were standardized using the StandardScaler, which transforms the data to have a mean of zero and a standard deviation of one. This step is essential for algorithms that are sensitive to the scale of the input features.\n\nCategorical variables were encoded using the LabelEncoder function, which converts categorical data into numerical format. This encoding is necessary because machine-learning algorithms require numerical input.\n\nTo address class imbalance, techniques such as RandomUnderSampler and SMOTE were employed. RandomUnderSampler randomly reduces the size of the majority class, while SMOTE synthetically increases the size of the minority class. These methods helped balance the dataset, ensuring that the models could learn effectively from both classes.\n\nOutliers were detected and handled using the Interquartile Range (IQR) method. Values below Q1 \u2212 1.5 * IQR were replaced with Q1 \u2212 1.5 * IQR, and values above Q3 + 1.5 * IQR were replaced with Q3 + 1.5 * IQR. This approach helps in mitigating the impact of extreme values on the model's performance.\n\nAfter preprocessing, the data was split into training and test sets in a 9:1 ratio. This split ensures that the model is trained on a sufficient amount of data while leaving a portion for unbiased evaluation. The training set was used to develop and tune the models, while the test set was used to evaluate their performance.",
  "optimization/parameters": "In the study, the number of input parameters used in the model varied depending on the algorithm. For the XGBoost algorithm, the highest training accuracy was achieved using the least six variables: sex, housing type, functional limitation, health-related quality of life, weight change over one year, and moderate physical activity days per week. For the other algorithms\u2014LGBM, CatBoost, and RandomForest\u2014the highest training accuracy was obtained when using all 22 variables.\n\nThe selection of these variables was performed using Recursive Feature Elimination with Cross-Validation (RFECV). This technique iteratively removes unnecessary variables that degrade the model's performance, ultimately finding the optimal feature set for each model. This process ensures that the selected variables are the most relevant for predicting low back pain, enhancing the model's performance and generalization capabilities.",
  "optimization/features": "In the optimization process of our study, we utilized a range of input features to develop predictive models for low back pain. The number of features used as input varied between models, with a minimum of six and a maximum of twenty-two features. These features were selected to be easily collectible through interviews or surveys, avoiding the need for medical diagnoses or expert measurements.\n\nFeature selection was indeed performed using Recursive Feature Elimination with Cross-Validation (RFECV). This technique was employed to identify and remove unnecessary variables that could degrade model performance. The RFECV process iteratively finds the optimal feature set for each model, ensuring that only the most relevant variables are retained.\n\nIt is crucial to note that feature selection was conducted using only the training set data. This approach helps prevent data leakage and ensures that the model's performance is objectively evaluated on unseen data. By using the training set exclusively for feature selection, we maintain the integrity of the validation and test sets, allowing for a more reliable assessment of the model's generalizability and robustness.",
  "optimization/fitting": "The study employed several strategies to address potential overfitting and underfitting issues. The dataset used was significantly larger than in previous studies, with data collected from 18,215 individuals over six years. This larger dataset helped to ensure that the model could generalize well and avoid underfitting, as it better represented the population and captured more complex patterns and interrelationships.\n\nTo prevent overfitting, Recursive Feature Elimination with Cross-Validation (RFECV) was used. This technique iteratively removes unnecessary variables that could degrade model performance, ensuring that only the most relevant features are included. Additionally, hyperparameter tuning was performed using the Optuna library, which automatically searches for optimal hyperparameters to maximize model performance without overfitting to the training data.\n\nStratified K-fold cross-validation with 5 folds was employed during model training. This method helps to ensure that the model's performance is evaluated on different subsets of the data, reducing the risk of overfitting to any single subset. Furthermore, the models were evaluated using a separate test set that was not used during training, providing an objective measure of performance and helping to prevent overfitting.\n\nThe study also addressed the curse of dimensionality by carefully selecting the number of variables. While some algorithms performed best with all 22 variables, others achieved high accuracy with just six variables. This approach helped to balance model complexity and generalization ability, further mitigating the risk of overfitting.\n\nTo handle data imbalance, techniques such as SMOTE and Random Under Sampler were applied. These methods ensured that the model could learn effectively from both majority and minority classes, improving overall performance and reducing the risk of underfitting to the minority class.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our models. One key method used was Recursive Feature Elimination with Cross-Validation (RFECV). This technique helps in selecting the optimal set of features by iteratively removing the least important features and building models to evaluate performance. By doing so, it helps in removing unnecessary variables that could degrade the model's performance and contribute to overfitting.\n\nAdditionally, we utilized Stratified K-fold cross-validation with 5 folds during the model training process. This method ensures that each fold of the cross-validation has the same proportion of classes as the original dataset, which helps in maintaining the representativeness of the data and reducing the risk of overfitting.\n\nHyperparameter tuning was performed using the Optuna library, which automatically searches for the optimal hyperparameters to maximize model performance. This process helps in finding the best configuration of the model parameters, reducing the likelihood of overfitting to the training data.\n\nFurthermore, we addressed data imbalance using the Synthetic Minority Over-sampling Technique (SMOTE) and Random Under Sampler. By balancing the dataset, we ensured that the model could learn effectively from both majority and minority classes, thereby improving its generalization capability and reducing overfitting.\n\nLastly, we evaluated the models using a separate test set that was not used during the training process. This approach helps in objectively assessing the model's performance and ensuring that it generalizes well to unseen data, further mitigating the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in this study are available through the reported details of the models trained with the Optuna library. The specific hyper-parameters for each algorithm\u2014XGBoost, LGBM, CatBoost, and RandomForest\u2014are listed in Table 2. These configurations include parameters such as 'n_estimators', 'max_depth', 'learning_rate', and others, which were optimized to maximize model performance.\n\nThe model files themselves are not explicitly mentioned as being available, but the process and results of the optimization are thoroughly documented. The evaluation metrics, including accuracy, sensitivity, specificity, and F1-score, are provided in Table 3, offering a clear view of the models' performance.\n\nRegarding the optimization parameters, the study mentions the use of Recursive Feature Elimination with Cross-Validation (RFECV) for feature selection and Strati\ufb01ed K-fold cross-validation for training. These techniques ensure that the models are robust and generalize well to unseen data.\n\nWhile the specific model files may not be directly accessible, the detailed reporting of hyper-parameters, optimization processes, and performance metrics provides a comprehensive understanding of the configurations available. This information can be used to replicate the study or build upon the findings, ensuring transparency and reproducibility.",
  "model/interpretability": "The models developed in this study, including LightGBM (LGBM), CatBoost, RandomForest, and XGBoost, are generally considered to be black-box models. These models, particularly the boosting algorithms like LGBM, CatBoost, and XGBoost, are known for their high predictive performance but often lack interpretability. This means that while they can accurately predict outcomes, it can be challenging to understand the specific reasons behind their predictions.\n\nHowever, feature importance analysis provides some level of transparency. For instance, in all models, the variable EQ-5D was identified as the most important feature. This indicates that health-related quality of life, as measured by EQ-5D, plays a crucial role in predicting low back pain (LBP). Additionally, demographic data such as age, height, weight, BMI, and waist circumference were highly ranked in the RandomForest and LGBM models. In contrast, the CatBoost model showed relatively high importance for lifestyle pattern variables like sleep time, housing type, frequency of drinking, walking days per week, and stretching days per week.\n\nThese insights suggest that while the models themselves are complex and not fully transparent, the feature importance analysis helps in understanding which variables are most influential in the predictions. This partial interpretability is valuable for identifying key risk factors and developing targeted interventions for LBP management.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the presence of low back pain (LBP) based on various demographic and lifestyle variables. The performance of the model was evaluated using metrics such as accuracy, sensitivity, specificity, and F-1 score, which are commonly used to assess the effectiveness of classification models. The model's output indicates whether an individual is likely to experience LBP or not, making it a binary classification task. The confusion matrix and other evaluation metrics provided further validate that the model's primary goal is to classify individuals into two categories: those with LBP and those without.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study was designed to ensure the robustness and generalizability of the models. To prevent overfitting and to objectively evaluate performance, the models were assessed using a test set that was not utilized during the training process. This approach helps in providing an unbiased estimate of the model's performance on unseen data.\n\nSeveral evaluation metrics were used to comprehensively assess the models' performance. These metrics included accuracy, sensitivity, specificity, and the F1-score. Accuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, also known as recall, indicates the proportion of actual positives that are correctly identified by the model. Specificity measures the proportion of actual negatives that are correctly identified. The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both concerns.\n\nAdditionally, the comprehensive performance of each model was assessed and compared using these indicators. Feature importance for each trained model was evaluated using permutation importance, which reflects the relative impact of each variable on the model\u2019s performance. This method involves randomly shuffling the values of each feature and observing the change in the model's performance, thereby identifying which features are most critical to the model's predictions.\n\nThe models were constructed using variables that could be easily collected through interviews or surveys, rather than relying on variables that require medical diagnosis or expert measurement. This approach ensures that the models are practical and accessible for use in various settings, including areas with limited healthcare resources. The performance of the models ranged from 0.818 to 0.830, demonstrating their effectiveness in predicting low back pain (LBP) using a relatively small number of variables.",
  "evaluation/measure": "In the evaluation of our low back pain (LBP) prediction models, we utilized a comprehensive set of performance metrics to ensure a thorough assessment. The primary metrics reported include accuracy, sensitivity, specificity, and the F1-score. These metrics were calculated from the confusion matrix of the test set for each model, ensuring an objective evaluation of performance.\n\nAccuracy measures the overall correctness of the model's predictions, providing a general sense of how well the model performs. Sensitivity, also known as recall, indicates the model's ability to correctly identify positive cases, which is crucial for detecting instances of LBP. Specificity, on the other hand, measures the model's ability to correctly identify negative cases, ensuring that non-LBP cases are accurately classified. The F1-score, which is the harmonic mean of precision and recall, offers a balanced view of the model's performance, especially when dealing with imbalanced datasets.\n\nThese metrics are widely recognized and used in the literature for evaluating machine learning models, particularly in medical and health-related studies. They provide a robust framework for comparing the performance of different models and algorithms. By including accuracy, sensitivity, specificity, and the F1-score, we ensure that our evaluation is representative and aligns with established standards in the field. This approach allows for a comprehensive understanding of each model's strengths and weaknesses, facilitating informed decisions and improvements in future research.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, we focused on developing and optimizing our own models using boosting and bagging algorithms, specifically XGBoost, LGBM, CatBoost, and RandomForest. These models were constructed using variables selected through Recursive Feature Elimination with Cross-Validation (RFECV), ensuring that unnecessary variables were removed to enhance model performance.\n\nWe did, however, compare our models to a previous study by Shim et al., which used a different set of variables and a smaller dataset. Our models demonstrated superior performance, achieving accuracies ranging from 0.818 to 0.830, compared to the previous study's range of 0.656 to 0.716. This improvement can be attributed to several factors, including the use of a larger and more representative dataset, the selection of optimal variables, and the application of techniques to address data imbalance.\n\nRegarding simpler baselines, our approach involved using well-established machine learning algorithms that are widely recognized for their effectiveness in predictive modeling. While we did not explicitly compare our models to simpler baselines like logistic regression or decision trees, the use of boosting and bagging algorithms inherently incorporates elements of these simpler models. For instance, RandomForest is an ensemble of decision trees, and boosting algorithms like XGBoost and LGBM build upon decision trees to improve predictive performance.\n\nIn summary, our study focused on optimizing and comparing advanced machine learning models rather than benchmarking against publicly available methods or simpler baselines. The comparison with a previous study highlighted the advantages of our approach in terms of dataset size, variable selection, and handling data imbalance.",
  "evaluation/confidence": "Evaluation Confidence\n\nThe evaluation of model performance was conducted using a test set that was not involved in the training process. This approach helps to prevent overfitting and ensures an objective assessment of the models' performance. The metrics used for evaluation included accuracy, sensitivity, specificity, and F1-score. These metrics provide a comprehensive view of the models' capabilities in predicting low back pain.\n\nThe performance metrics reported for each model, such as accuracy, sensitivity, specificity, and F1-score, are point estimates derived from the test set. However, confidence intervals for these metrics were not explicitly provided. Confidence intervals would offer a range within which the true performance metrics are likely to fall, providing a more nuanced understanding of the models' reliability.\n\nStatistical significance was not explicitly discussed in the context of comparing the models' performance. To claim that one method is superior to others or to baselines, statistical tests such as t-tests or chi-square tests could be employed to compare the performance metrics. These tests would help determine whether the observed differences in performance are statistically significant or merely due to random variation.\n\nIn summary, while the evaluation metrics provide valuable insights into the models' performance, the absence of confidence intervals and statistical significance tests limits the ability to make definitive claims about the superiority of one model over another. Future work could include the calculation of confidence intervals and the conduct of statistical significance tests to strengthen the evaluation confidence.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study utilized a dataset consisting of 18,215 individuals over six years, with a focus on subjects aged 50 and above. The dataset included 22 predictor variables, encompassing demographic, physical activity, occupational, and lifestyle factors. The evaluation metrics used in this study included accuracy, sensitivity, specificity, and F1-score, which were calculated from the confusion matrix of the test set. These metrics were employed to assess and compare the comprehensive performance of each model. However, the specific evaluation files or datasets used for these calculations have not been made publicly accessible."
}