{
  "publication/title": "Tracking of Systemic Lupus Erythematosus (SLE) Longitudinally Using Biosensor and Patient-Reported Data: A Report on the Fully Decentralized Mobile Study to Measure and Predict Lupus Disease Activity Using Digital Signals-The OASIS Study.",
  "publication/authors": "Jupe ER, Lushington GH, Purushothaman M, Pautasso F, Armstrong G, Sorathia A, Crawley J, Nadipelli VR, Rubin B, Newhardt R, Munroe ME, Adelman B",
  "publication/journal": "Biotech (Basel (Switzerland))",
  "publication/year": "2023",
  "publication/pmid": "37987479",
  "publication/pmcid": "PMC10660535",
  "publication/doi": "10.3390/biotech12040062",
  "publication/tags": "- Systemic Lupus Erythematosus (SLE)\n- Digital Health\n- Biosensors\n- Patient-Reported Outcomes (PROs)\n- Quality of Life (QOL)\n- Machine Learning\n- Disease Prediction\n- Decentralized Study\n- Biometric Monitoring\n- Mobile Health (mHealth)",
  "dataset/provenance": "The dataset used in this study was sourced from a cohort of 550 participants who self-reported having Systemic Lupus Erythematosus (SLE). The majority of these participants were female, which is consistent with the known higher prevalence of SLE in women. The participants were asked to wear a smartwatch continuously, both day and night, to collect various biometric data. Additionally, they were required to complete different Patient-Reported Outcomes (PROs) either weekly or bi-weekly. These PROs included various metrics such as medications, ICD-10 codes, connective tissue disease screening questionnaires, medical imaging, resource utilization, heart rate variability, and SLE quality of life.\n\nThe dataset includes a wide range of variables, spanning over 200 parameters that could potentially influence several dozen pathological metrics. This comprehensive data collection effort aimed to capture diverse types of information, including periodic self-assessments of life quality and disease activity, biometric measurements, and medical records. The medical records of 68 individual participants, who were regular contributors to the OASIS data collection for at least 90 days, were obtained and analyzed. These records were subjected to optical character recognition and keyword extraction to identify terms relevant to SLE.\n\nThe dataset has not been used in previous papers by the community, as this study represents an initial exploration into the relationships between various biometric and life-quality observations and tangible health outcomes in SLE patients. The goal is to lay the groundwork for future, more extensive longitudinal studies that can validate and refine the relationships identified in this preliminary work. The dataset is unique in its integration of disparate data types, including biometric measurements, self-reported quality of life, and medical records, which together provide a holistic view of the participants' health status.",
  "dataset/splits": "In our study, we employed various cross-validation techniques to evaluate the performance of our predictive models. The primary splits involved four-fold, three-fold, and two-fold cross-validation. The four-fold cross-validation demonstrated the best aggregate predictivity, suggesting a model of reasonable generality and extensibility. This method divides the data into four subsets, using three for training and one for testing, and rotates the testing subset through all four parts.\n\nThe three-fold cross-validation retained a fair capacity for distinguishing possible flare-susceptible patients from those with low risk but showed less sensitivity for assessing patients with intermediate risk. This method splits the data into three parts, using two for training and one for testing, cycling through all three parts.\n\nEven the two-fold cross-validation suggested some predictive value, although it indicated a common pitfall in classification of unbalanced outcomes, where the model sorts a disproportionate number of instances into larger classes, depleting minority classes. This method divides the data into two subsets, using one for training and one for testing, and alternates between the two.\n\nAdditionally, we conducted 10-fold and 5-fold cross-validation analyses to assess the robustness of our model. The 10-fold cross-validation divided the data into ten parts, using nine for training and one for testing, rotating through all ten parts. The 5-fold cross-validation split the data into five parts, using four for training and one for testing, cycling through all five parts. The results showed only a slight decline in precision and recall when increasing the stringency from 10-fold to 5-fold, indicating the model's robustness.\n\nIn summary, our study utilized four-fold, three-fold, two-fold, 10-fold, and 5-fold cross-validation techniques, each providing insights into the model's performance and robustness under different data splitting strategies.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used is Na\u00efve Bayes. This is a well-established algorithm in the field of machine learning, known for its simplicity and effectiveness in classification tasks. It is not a new algorithm, having been extensively studied and applied in various domains for many years.\n\nThe choice of Na\u00efve Bayes for this study was driven by its suitability for the specific problem at hand, which involves classifying patients into different risk categories based on various metrics. The algorithm's ability to handle a large number of features and its robustness to noise in the data made it a practical choice for this analysis.\n\nThe decision to use Na\u00efve Bayes was also influenced by the need for a model that could provide interpretable results. Na\u00efve Bayes, with its probabilistic approach, allows for the examination of the contribution of each feature to the final classification, which is crucial for understanding the factors that influence the risk of flares in patients.\n\nWhile Na\u00efve Bayes is a well-known algorithm, its application in this context is novel. The specific metrics and the way they were integrated into the model are unique to this study. This approach allows for a more nuanced understanding of the factors that contribute to the risk of flares, which can inform better clinical decisions and interventions.\n\nThe results demonstrate that the Na\u00efve Bayes model performs well under different cross-validation conditions, indicating its robustness and generalizability. The model's performance was evaluated using various metrics, including precision, recall, and F-measure, which provide a comprehensive view of its effectiveness in classifying patients into different risk categories.",
  "optimization/meta": "The meta-predictor leverages data from various machine-learning algorithms to enhance predictive accuracy. Specifically, it integrates outputs from Bayesian networks, deep neural networks (multi-layer perceptrons), and decision trees (LMT). This ensemble approach allows the meta-predictor to benefit from the strengths of each individual method, leading to more robust and generalizable predictions.\n\nThe meta-predictor is designed to ensure that the training data for each constituent model is independent. This independence is crucial for maintaining the integrity of the cross-validation process and preventing data leakage, which could otherwise inflate the apparent performance of the models. By using independent datasets for training and validation, the meta-predictor aims to provide a more reliable assessment of its predictive capabilities.\n\nThe use of multiple feature selection techniques further enhances the meta-predictor's performance. Techniques such as Correlation Feature Subset (CFS), Classified Attribute (ClA), and Information Gain Attribute (IGA) are employed to identify the most relevant features. These selected features are then used to train the individual models, which in turn feed into the meta-predictor. This multi-faceted approach ensures that the meta-predictor can effectively distinguish between possible flare-susceptible patients and those with low risk, even under stringent cross-validation conditions.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithms. We began by addressing the variability in data density and regularity, which arose from participants' imperfect compliance with information collection protocols. To tackle this, we implemented a paradigm that first eliminated temporally isolated data points, retaining only those with comparable data within two weeks before or after. We also deprioritized data from participants who did not sustain activity in the study for at least three months.\n\nTo handle gaps in the dataset, we employed linear interpolations between temporally proximal measurements. This approach is common in AI treatments of data with non-homogeneous density. The resulting dataset, spanning over 200 parameters, was then used to explore relationships between biometric and life-quality observations and health outcomes.\n\nFor the machine-learning models, we utilized various feature selection techniques to identify the most informative terms. These techniques included Correlation Feature Subset (CFS), Classified Attribute (ClA), Correlation Attribute (CoA), Gain Ratio Attribute (GRA), Information Gain Attribute (IGA), One R Attribute (ORA), Relief F Attribute (RFA), and Symmetry Uncertain Attribute (SUA). Each technique selected the 25 highest weighted features, which were then applied to different classification algorithms such as Bayesian Networks, Multilayer Perceptrons, and Decision Trees (LMT).\n\nThe preprocessing steps ensured that our models could effectively handle the complex and dynamic nature of the data, leading to robust predictive accuracy for symptoms of possible disease flares. The combination of diverse feature selection techniques and classification algorithms allowed us to identify key factors that differentiate patients with varying levels of flare risk.",
  "optimization/parameters": "The model utilized a diverse set of metrics to predict possible flare vulnerability in patients. Initially, a comprehensive 24-metric descriptor set was employed, exhibiting conceptual diversity. Key factors included average biometrically determined nocturnal wakefulness, variance in steroid usage, and average and maximal values of Raynaud\u2019s symptoms like finger color. Several other symptom-related metrics were also found to be important, with variances and ranges in observations generally being more significant than their means or maxima.\n\nTo identify the most relevant features, various feature selection techniques were applied. Through experimentation, it was concluded that a subset of 10 parameters derived using the Gain Ratio Attribute algorithm provided significant discrimination between patients with strong flaring tendencies and those with lower risk. This subset was selected to optimize the model's predictive performance, ensuring that the most informative variables were included while reducing dimensionality and potential overfitting.\n\nThe selection process involved evaluating different feature selection methods, including Correlation Feature Subset, Classified Attribute, Correlation Attribute, Gain Ratio Attribute, Information Gain Attribute, One R Attribute, Relief F Attribute, and Symmetry Uncertain Attribute. The Gain Ratio Attribute method was found to be particularly effective in identifying the most relevant features for predicting flare vulnerability. This approach helped in creating a robust model that could generalize well to new data, ensuring reliable predictions.",
  "optimization/features": "In the optimization process, a diverse set of 24 metrics was initially considered as input features. These metrics exhibited conceptual diversity, encompassing various aspects such as biometrically determined nocturnal wakefulness, variance in steroid usage, and symptoms related to conditions like Raynaud's, including finger color changes. The importance of these features was evaluated, with variances and ranges often proving more significant than their means or maxima.\n\nFeature selection was indeed performed to identify the most relevant metrics. This process involved using a range of feature selection techniques available in Weka. Through experimentation, it was determined that a subset of 10 parameters derived using the Gain Ratio Attribute algorithm provided significant discrimination between patients with strong flaring tendencies and those who were flare-resistant. This subset was selected based on its ability to foster better predictive performance.\n\nThe feature selection process was conducted using the training data only, ensuring that the model's performance on unseen data was not compromised. This approach helped in identifying the most informative features while maintaining the model's generality and extensibility. The selected features were then used to build predictive models, which were evaluated using cross-validation techniques to assess their robustness and accuracy.",
  "optimization/fitting": "The fitting method employed in this study utilized a comprehensive set of 24 metrics, which were derived from two distinct descriptor sets. The first set, consisting of 20 parameters, was obtained through the ReliefF algorithm and was particularly effective in distinguishing patients with marginal vulnerability. The second set, comprising 4 parameters, was instrumental in differentiating high-risk from low-risk patients. These two sets were combined to create a composite descriptor basis that is sensitive to both high and low possible flare vulnerability and those with intermediate risk.\n\nGiven the extensive number of parameters relative to the number of training points, overfitting was a potential concern. To mitigate this risk, several cross-validation techniques were employed. The study evaluated the model's performance under 10-fold, 5-fold, four-fold, three-fold, and two-fold cross-validation constraints. The best aggregate predictivity was observed with four-fold cross-validation, indicating that the model generalizes well even with a reduced training set. This suggests that the model is not overfitting, as its performance does not significantly degrade with fewer training points.\n\nAdditionally, the model's robustness was further validated by the minimal decline in precision and recall when increasing the stringency of cross-validation from 10-fold to 5-fold. This consistency across different cross-validation schemes underscores the model's ability to generalize beyond the training data, thereby ruling out overfitting.\n\nUnderfitting was addressed by ensuring that the model captured a diverse range of factors influencing flare susceptibility. The 24-metric descriptor set included key factors such as nocturnal wakefulness, variance in steroid usage, and symptoms related to Raynaud's phenomenon. The inclusion of these varied metrics ensured that the model could accurately capture the complexity of the data, thereby avoiding underfitting.\n\nMoreover, the study considered interpersonal support networks and responsibilities, such as childcare, which significantly influenced the predictions. This holistic approach to feature selection and model validation ensures that the model is both comprehensive and robust, capable of identifying flare-vulnerable participants and the factors leading to such differentiation.",
  "optimization/regularization": "In our study, we employed cross-validation as a regularization method to prevent overfitting. Specifically, we utilized 10-fold and 5-fold cross-validation techniques to assess the model's performance and robustness.\n\nCross-validation involves partitioning the data into subsets, training the model on some of these subsets, and validating it on the remaining ones. This process is repeated multiple times with different partitions to ensure that the model generalizes well to unseen data.\n\nOur results indicated that the model maintained good predictive performance even when the stringency of cross-validation was increased from 10-fold to 5-fold, demonstrating its robustness. Additionally, we explored even more stringent conditions, such as four-fold, three-fold, and two-fold cross-validation, to further evaluate the model's performance. The four-fold cross-validation yielded the best aggregate predictivity, suggesting that the model has reasonable generality and extensibility.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in the supplementary materials. These materials include details about the metrics applied in the analysis, which are crucial for understanding the model's performance and the optimization process.\n\nThe optimization schedule and model files are not explicitly detailed in the main text, but the performance statistics and cross-validation results provide insights into the model's robustness and predictive accuracy. For instance, the performance statistics of the Na\u00efve Bayes classification are presented in Table 2, which includes metrics like precision, recall, and F-measure for different cross-validation levels (10-fold and 5-fold).\n\nRegarding the availability and licensing of these resources, the supplementary materials are typically made available to readers and reviewers upon request. However, specific details about the licensing terms are not provided in the main text. Interested parties can contact the authors or the journal for more information on accessing the supplementary materials and any associated licensing agreements.",
  "model/interpretability": "The model employed in this study is not a blackbox, but rather a transparent one, as it utilizes a Na\u00efve Bayes classifier. This type of model is inherently interpretable, allowing for clear insights into the decision-making process. The Na\u00efve Bayes classifier operates by calculating the probabilities of different features given a particular class, making it straightforward to understand which factors contribute most to the predictions.\n\nFor instance, the model considers a diverse set of 24 metrics, including key factors such as average nocturnal wakefulness, variance in steroid usage, and symptoms related to Raynaud's phenomenon, like finger color changes. The importance of these metrics is not just in their average or maximum values but also in their variances and ranges, which often play a crucial role in distinguishing between different patient outcomes.\n\nAdditionally, the model highlights the significance of interpersonal support networks and responsibilities. One notable factor is whether participants have childcare responsibilities, as those with such responsibilities are significantly more likely to experience disease flares. This transparency allows healthcare providers to understand not just the predictions but also the underlying reasons, facilitating more informed decision-making and patient care.\n\nThe use of cross-validation further enhances the interpretability by ensuring that the model's performance is robust and generalizable. The confusion matrices and performance statistics provide a clear view of how well the model distinguishes between different patient classes, with minimal instances of predictive disagreement. This level of detail helps in identifying areas where the model might be improved and ensures that the predictions are reliable and actionable.",
  "model/output": "The model employed in this study is a classification model. Specifically, it uses Na\u00efve Bayes classification to predict whether participants are possible flare-vulnerable, ambiguous, or non-flaring patients. The performance of this model is evaluated using metrics such as precision, recall, F-measure, and ROC, which are typical for classification tasks. The model's predictions are assessed through cross-validation techniques, further indicating its classification nature. Additionally, the confusion matrices and the discussion around true positives, false positives, and other classification metrics reinforce that the model is designed for classification rather than regression.\n\nThe model's robustness is demonstrated through various levels of cross-validation, showing only slight declines in precision and recall when increasing the stringency from 10-fold to 5-fold cross-validation. This suggests that the model maintains its classification accuracy even under more rigorous conditions. The confusion matrices provide a detailed view of how well the model classifies participants into the correct categories, highlighting the model's strengths and areas for improvement in distinguishing between different patient groups.\n\nThe model's performance is also evaluated using different feature selection techniques and machine learning methods, including Bayesian networks, deep neural networks, and decision trees. These evaluations further confirm the model's classification capabilities, as they focus on predicting categorical outcomes rather than continuous values. The relative predictive accuracy for self-reported flares and non-flares is assessed, providing insights into the model's effectiveness in classifying patients based on their symptoms.\n\nIn summary, the model is a classification model that uses Na\u00efve Bayes and other machine learning techniques to predict patient categories based on various metrics and symptoms. The model's performance is thoroughly evaluated through cross-validation and confusion matrices, demonstrating its robustness and accuracy in classification tasks.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved a rigorous cross-validation approach to assess its predictive performance. Various levels of cross-validation were employed, including 10-fold, 5-fold, four-fold, three-fold, and two-fold cross-validation. The best aggregate predictivity was observed with four-fold cross-validation, indicating a model of reasonable generality and extensibility. The three-fold experiment demonstrated a fair capacity to distinguish between possible flare-susceptible patients and those with low risk, but it showed less sensitivity for assessing patients with intermediate risk. Even the two-fold experiment suggested some predictive value, although it highlighted a common pitfall in classification of unbalanced outcomes, where the model tended to sort a disproportionate number of instances into larger classes, depleting minority classes.\n\nThe evaluation also considered the performance of different feature selection techniques and machine learning methods, including directed graph (Bayes), deep neural network (multi-layer perceptron), and decision tree (LMT) methods. The relative predictive accuracy for self-reported symptoms of possible disease flares and non-flares was analyzed using these techniques, with all prediction rates derived from 10-fold cross-validation analysis.\n\nAdditionally, the robustness of the model was assessed by comparing the precision and recall metrics across different levels of cross-validation stringency. The results showed only a slight decline in these metrics when increasing the stringency from 10-fold to 5-fold cross-validation, further validating the model's reliability. The confusion matrices provided insights into the predictive performance, showing minimal instances of predictive disagreement and highlighting that most false negative predictions for possible flare-risk patients fell into the \u2018ambiguous\u2019 category, which would be grounds for cautioning a patient of a possible impending risk in practical medicine.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report several key metrics to evaluate the performance of our classification models. These metrics include precision, recall, F-measure, and the area under the receiver operating characteristic curve (ROC). Precision measures the accuracy of the positive predictions made by the model, while recall indicates the model's ability to identify all relevant instances. The F-measure provides a harmonic mean of precision and recall, offering a single metric that balances both concerns. The ROC curve and its associated area under the curve (AUC) provide a comprehensive view of the model's performance across all classification thresholds.\n\nThe reported metrics are derived from cross-validation experiments, specifically 10-fold and 5-fold cross-validation. This approach ensures that the model's performance is assessed on multiple subsets of the data, providing a robust estimate of its generalizability. The use of these metrics is consistent with standard practices in the literature, ensuring that our evaluation is representative and comparable to other studies in the field.\n\nAdditionally, we present confusion matrices that detail the true positive, true negative, false positive, and false negative rates for different classes. These matrices offer a granular view of the model's performance, highlighting areas where the model excels and where it may need improvement. For instance, the matrices show that most false negative predictions for possible flare-risk patients fall into the 'ambiguous' category, which is crucial for practical medical applications as it indicates a need for caution.\n\nThe performance metrics reported are comprehensive and align with established evaluation practices in the literature. They provide a clear and detailed assessment of the model's strengths and weaknesses, ensuring that the results are both reliable and informative.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of different machine learning methods to assess their predictive performance. We employed various feature selection techniques, including Correlation Feature Subset (CFS), Classified Attribute (ClA), Correlation Attribute (CoA), Gain Ratio Attribute (GRA), Information Gain Attribute (IGA), One R Attribute (ORA), Relief F Attribute (RFA), and Symmetry Uncertain Attribute (SUA). These techniques were applied within the context of three different predictive models: Bayesian Network, Multilayer Perceptron, and Logistic Model Trees (LMT).\n\nThe performance of these models was evaluated using 10-fold cross-validation, ensuring a robust assessment of their generalization capabilities. The results, presented in Table 3, show the relative predictive accuracy for self-reported flares (TP-flare) and self-reported non-flares (TP-no) across the different feature selection techniques and models. This comparison allows us to identify which combinations of feature selection and modeling approaches yield the best predictive performance.\n\nAdditionally, we compared the performance of these models under varying levels of cross-validation stringency, including 10-fold, 5-fold, four-fold, three-fold, and two-fold cross-validation. The results, discussed in relation to Table 2 and Figure 4, indicate that the models maintain good predictivity even under more stringent conditions. For instance, the four-fold cross-validation showed the best aggregate predictivity, suggesting a model of reasonable generality and extensibility.\n\nWe also performed a comparison to simpler baselines, such as the Na\u00efve Bayes classifier, to understand the relative improvement offered by more complex models. The Na\u00efve Bayes classifier was evaluated for its ability to distinguish between possible flare-vulnerable, ambiguous, and non-flaring patients. The performance statistics, as shown in Table 2, provide insights into the precision, recall, and other metrics for different classes under 10-fold and 5-fold cross-validation.\n\nOverall, our evaluation involved a thorough comparison of different methods and baselines, ensuring that the chosen models and feature selection techniques are well-suited for predicting flare risks in patients.",
  "evaluation/confidence": "The evaluation of our model's performance metrics does not explicitly mention confidence intervals. However, the robustness of the model is demonstrated through various levels of cross-validation, including 10-fold, 5-fold, four-fold, three-fold, and two-fold. The slight decline in precision and recall when increasing the stringency from 10-fold to 5-fold cross-validation suggests that the model maintains its performance under different validation conditions.\n\nThe statistical significance of the results is implied by the consistent performance across different cross-validation levels. The model's ability to distinguish between possible flare-susceptible patients and those with low risk, even under more stringent conditions, indicates that the results are reliable. The minimal instances of predictive disagreement further support the model's accuracy and reliability.\n\nThe best aggregate predictivity is observed with four-fold cross-validation, which suggests that the model has reasonable generality and extensibility. The three-fold experiment retains a fair capacity for distinguishing high-risk patients, although it shows less sensitivity for intermediate-risk patients. Even the two-fold experiment demonstrates some predictive value, although it begins to show signs of the common pitfall in classifying unbalanced outcomes.\n\nOverall, while confidence intervals are not explicitly provided, the consistent performance across different validation levels and the minimal predictive disagreements suggest that the model's results are statistically significant and reliable.",
  "evaluation/availability": "Not enough information is available."
}