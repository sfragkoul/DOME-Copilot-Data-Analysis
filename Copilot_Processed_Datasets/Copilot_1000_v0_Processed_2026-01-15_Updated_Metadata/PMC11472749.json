{
  "publication/title": "Predicting elevated natriuretic peptide in chest radiography: emerging utilization gap for artificial intelligence.",
  "publication/authors": "Kagawa E, Kato M, Oda N, Kunita E, Nagai M, Yamane A, Matsui S, Yoshitomi Y, Shimajiri H, Hirokawa T, Ishida S, Kurimoto G, Dote K",
  "publication/journal": "European heart journal. Imaging methods and practice",
  "publication/year": "2024",
  "publication/pmid": "39403705",
  "publication/pmcid": "PMC11472749",
  "publication/doi": "10.1093/ehjimp/qyae064",
  "publication/tags": "- Artificial Intelligence\n- Heart Failure\n- Chest Radiography\n- Diagnostic Accuracy\n- Medical Imaging\n- Predictive Modeling\n- Healthcare Providers\n- Biomarkers\n- Deep Learning\n- Ensemble Prediction\n\nNot sure if these tags are the ones provided in the published article, but they are a good summary of the paper's content.",
  "dataset/provenance": "The dataset used in this study comprises chest X-ray images and corresponding BNP (B-type natriuretic peptide) levels from patients. The patients were sourced from two hospitals: Hiroshima City Asa Hospital and Hiroshima City North Medical Center Asa Citizens Hospital. The patients from Hiroshima City Asa Hospital were used for the training and validation datasets, while those from Hiroshima City North Medical Center Asa Citizens Hospital were used for the external test dataset. The study patients from Hiroshima City Asa Hospital were randomly divided into training and validation datasets in a ratio of approximately 0.66:0.17. Many patients had multiple pairs of chest X-ray images and BNP labels, but each patient\u2019s data were assigned to only one dataset to avoid overfitting.\n\nThe training dataset includes 1061 patients with 6697 chest X-ray images. The validation dataset consists of 273 patients with 1693 chest X-ray images. The external test dataset also includes 273 patients with 1713 chest X-ray images. Additionally, there is a human test dataset comprising 68 patients with 100 chest X-ray images. The median age of the patients across all datasets is around 72 years, with a majority being male. The datasets include a variety of diagnoses, with heart failure, coronary artery disease, and atrial fibrillation being among the most common.\n\nThe chest X-ray images were assigned a binary label based on a cut-off value for BNP levels. The study aimed to develop a deep learning-based model to predict elevated BNP levels, which is a marker for heart failure. The model was fine-tuned using 31 modified pre-trained image recognition models as weak learners, and an ensemble model was created by averaging the probabilities of these models. The performance of the model was evaluated using metrics such as accuracy, precision, sensitivity, specificity, F1 score, and area under the curve (AUC) for receiver-operating-characteristics (ROC) and precision\u2013recall (PR) curves.\n\nThe dataset has not been used in previous papers or by the community, as this is the first study to develop and validate this specific model for predicting elevated BNP levels from chest X-ray images. The study was approved by the local institutional review board, ensuring ethical considerations were met.",
  "dataset/splits": "There are four data splits in this study: the training dataset, the validation dataset, the external test dataset, and the human test images.\n\nThe training dataset consists of 1061 study patients and 6697 chest X-ray images. The validation dataset includes 273 study patients and 1693 chest X-ray images. The external test dataset also has 273 study patients and 1713 chest X-ray images. The human test images dataset comprises 68 study patients and 100 chest X-ray images.\n\nThe study patients from Hiroshima City Asa Hospital were randomly divided into the training and validation datasets in a ratio of approximately 0.66:0.17. The patients from Hiroshima City North Medical Center Asa Citizens Hospital were used for the external test dataset. Each patient\u2019s data were assigned to only one dataset to avoid overfitting.\n\nThe human test images dataset was specifically curated to include 100 images, with 50 images having BNP levels below the cut-off value and 50 images having BNP levels at or above the cut-off value. This dataset was used to evaluate human performance in predicting elevated BNP levels from chest radiography, both with and without AI assistance.",
  "dataset/redundancy": "The datasets were split into four distinct groups: training, validation, external test, and human test images. The training dataset consisted of 1061 patients and 6697 chest X-ray images. The validation dataset included 273 patients and 1693 images, while the external test dataset also had 273 patients and 1713 images. The human test dataset comprised 68 patients and 100 images.\n\nTo ensure independence between the training and test sets, patients who were duplicated across hospitals were assigned to only one dataset. This step was crucial to prevent data leakage and ensure that the models were evaluated on truly unseen data. The distribution of patient characteristics and image features across these datasets was carefully balanced to reflect real-world clinical settings. For instance, the median age and the proportion of male patients were similar across the datasets, ensuring comparability.\n\nThe datasets were designed to be robust and widely applicable in clinical settings. This was achieved by including patients taking angiotensin receptor neprilysin inhibitors, which might not increase BNP levels, thus making the models more generalizable. The image pre-processing steps ensured that all chest X-ray images were standardized, with non-square images transformed to square using zero padding. This standardization helped in maintaining consistency across the datasets.\n\nThe distribution of diagnoses and other clinical variables was also considered. For example, conditions like heart failure, coronary artery disease, and atrial fibrillation were represented in all datasets, albeit in varying proportions. This ensured that the models were trained and tested on a diverse range of clinical scenarios, enhancing their reliability and applicability in real-world settings.\n\nIn comparison to previously published machine learning datasets, our approach focused on ensuring a balanced and representative distribution of patient characteristics and clinical variables. This was achieved through careful dataset splitting and standardization, which are essential for developing robust and generalizable models. The use of well-known image recognition models, pre-trained on large non-medical image datasets like ImageNet, further enhanced the models' performance and reliability.",
  "dataset/availability": "The data used in this study is not publicly available. The study utilized chest X-ray images and corresponding BNP labels from patients at two hospitals: Hiroshima City Asa Hospital and Hiroshima City North Medical Center Asa Citizens Hospital. The patients from Hiroshima City Asa Hospital were used for the training and validation datasets, while the patients from Hiroshima City North Medical Center Asa Citizens Hospital were used for the external test dataset. The study patients from Hiroshima City Asa Hospital were randomly divided into two datasets for training and validation in a ratio of approximately 0.66:0.17:0.17. To avoid overfitting, each patient\u2019s data were assigned to only one dataset, ensuring that no patient data was duplicated across datasets. The study was approved by the local institutional review board, and all procedures were conducted in accordance with ethical standards. The data collection methodology ensured that there were no missing data for BNP values and chest radiograms.",
  "optimization/algorithm": "The optimization algorithm employed in our study is sharpness-aware minimization. This technique is designed to improve the generalization of neural networks by focusing on flatter minima in the loss landscape, which are typically associated with better generalization performance. Sharpness-aware minimization was integrated with either the Rectified Adam (RAdam) or the standard Adam optimizer to enhance the training process.\n\nSharpness-aware minimization is not a novel algorithm; it has been previously introduced and discussed in the literature. The decision to use this optimization technique was driven by its proven effectiveness in achieving better generalization and robustness in various machine learning tasks. While it is a powerful tool for training neural networks, it was not published in a machine-learning journal because it falls under the broader category of optimization techniques, which are often discussed in interdisciplinary journals that cover both machine learning and optimization theory. The integration of sharpness-aware minimization with established optimizers like RAdam and Adam highlights its versatility and applicability in modern deep learning frameworks.",
  "optimization/meta": "The model developed in this study is indeed a meta-predictor, leveraging the outputs of multiple machine-learning algorithms as inputs. Specifically, 31 modified pre-trained image recognition models were fine-tuned to serve as weak learners, each predicting elevated BNP levels. These individual models were then combined to create an ensemble model. The ensemble model averages the probabilities from the 31 weak learners, with probabilities greater than or equal to 0.5 indicating BNP levels at or above the cutoff value.\n\nThe training data for these models was carefully managed to ensure independence. Patients from Hiroshima City Asa Hospital were used for training and validation, while patients from Hiroshima City North Medical Center Asa Citizens Hospital were used for the external test dataset. This separation helps to avoid overfitting and ensures that the models are evaluated on independent data. Additionally, each patient\u2019s data was assigned to only one dataset to further prevent overfitting.\n\nThe ensemble approach allows the model to benefit from the strengths of multiple individual models, improving overall performance and robustness. The use of an ensemble model is a key aspect of the optimization process, ensuring that the final predictions are more accurate and reliable. The performance of the ensemble model was evaluated using various metrics, including accuracy, precision, sensitivity, specificity, F1 score, and area under the ROC and PR curves, all calculated using the test dataset. This comprehensive evaluation ensures that the model's performance is thoroughly assessed and validated.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the images were suitable for training robust models. Non-square chest X-ray images were transformed into square images using zero padding, which added black fields around the images rather than cropping them. This approach maintained the original image content.\n\nThe images were pre-processed to ensure consistency in orientation. The plan was to process images where the head was in the upper half and the spine was at a 60\u00b0 to 120\u00b0 angle to the horizontal line. However, all images already met these criteria, so additional preprocessing steps were not necessary.\n\nData augmentation techniques were employed during model training to enhance the diversity of the training dataset. These techniques included random width and height shifts, rotations up to 15\u00b0, horizontal flips, and advanced methods like Mixup, RandomErasing, and CutMix. These augmentations helped the models generalize better by exposing them to a wider range of variations in the input data.\n\nLabel smoothing was applied to regularize the neural networks, which helped in preventing overfitting by smoothing the true labels. The models were optimized using sharpness-aware minimization with either RAdam or Adam optimizers. The input image sizes were determined based on the pre-training image sizes of the models, ensuring compatibility with the pre-trained weights.\n\nThe training process was visualized using accuracy and loss metrics for both training and validation datasets for every epoch. Models were trained for at least 100 epochs, with the option to extend training for an additional 50 epochs if necessary. The weights of the models that achieved the minimum loss in the validation dataset at the end of each epoch were saved and used for further analysis. These models served as candidates for weak learners in the final ensemble model.",
  "optimization/parameters": "The models utilized in this study were optimized using a combination of sharpness-aware minimization with either RAdam or Adam optimizers. The input image sizes for the models were determined based on the pre-training image sizes, ensuring compatibility and leveraging the benefits of transfer learning.\n\nThe hyperparameters for each model were carefully selected and are detailed in Supplemental Table 2. These hyperparameters include the batch size, number of epochs, trainable layers, initial learning rate, and minimum learning rate. For instance, models like VGG16 and VGG19 used a batch size of 32 and 16, respectively, with varying trainable layers and learning rates across different epochs. The selection of these parameters was guided by empirical performance and the need to balance computational efficiency with model accuracy.\n\nThe training process involved visualizing the accuracy and loss of both training and validation datasets for every epoch. Models were trained for at least 100 epochs, with the possibility of extending training up to 150 epochs if necessary to achieve convergence or to mitigate overfitting. The weights of the models that achieved the minimum loss in the validation dataset at the end of each epoch were saved and used for further analysis. These models served as candidates for weak learners in the final ensemble model, ensuring a robust and comprehensive approach to model optimization.",
  "optimization/features": "The input features for the models were derived from chest X-ray images. Specifically, the images were pre-processed to ensure they were square by adding black padding, rather than cropping. The images were standardized to have the head in the upper half and the spine at a 60\u00b0 to 120\u00b0 angle to the horizontal line. This preprocessing ensured consistency across all input images.\n\nThe models utilized the entire pre-processed chest X-ray images as input features, without any explicit feature selection process. The images were directly fed into the models, which were fine-tuned from pre-trained image recognition models. These models were designed to handle the raw pixel data of the images, leveraging the spatial information contained within.\n\nThe training process involved data augmentation techniques such as random width and height shifts, rotations, horizontal flips, and other transformations to enhance the robustness of the models. These augmentations helped in making the models generalize better to new, unseen data.\n\nThe models were trained using a combination of sharpness-aware minimization with RAdam or Adam optimizers, and techniques like Mixup, RandomErasing, and CutMix were employed to further improve the models' performance. Label smoothing was also applied to regularize the neural networks.\n\nIn summary, the input features consisted of the pre-processed chest X-ray images, and no explicit feature selection was performed. The models were trained using the entire dataset, ensuring that the feature space was fully utilized.",
  "optimization/fitting": "The models developed in this study were fine-tuned from pre-trained image recognition models, which inherently have a large number of parameters. To address the potential issue of overfitting, several strategies were employed. Data augmentation techniques such as rotation, horizontal flips, Mixup, RandomErasing, and CutMix were utilized during training to artificially increase the diversity of the training dataset. Additionally, Label Smoothing was applied to regularize the neural networks, preventing them from becoming too confident in their predictions. The models were optimized using sharpness-aware minimization with either RAdam or Adam, which helps in finding flatter minima in the loss landscape, leading to better generalization.\n\nTo further mitigate overfitting, the training process was closely monitored. The accuracy and loss of both the training and validation datasets were visualized for every epoch. Training was terminated if overfitting was observed or if learning had converged by 100 epochs. If convergence was not achieved by 100 epochs, training was extended for an additional 50 epochs until overfitting or convergence was confirmed. The weights of the models that achieved the minimum loss in the validation dataset at the end of each epoch were saved and used for the study. These models served as candidates for weak learners for the final ensemble model, which helped in reducing the risk of overfitting by averaging the predictions of multiple models.\n\nTo ensure that the models were not underfitting, the training process was extended if necessary. The models were trained for at least 100 epochs, and if learning had not converged, training was continued for an additional 50 epochs. This approach ensured that the models had sufficient time to learn the underlying patterns in the data. Furthermore, the use of an ensemble model helped in improving the overall performance by combining the strengths of multiple weak learners. The final ensemble model was constructed by averaging the probabilities of the 31 individual models, which helped in reducing the variance and improving the generalization performance.",
  "optimization/regularization": "Several techniques were employed to prevent overfitting during the training of our models. Data augmentation methods such as rotation up to 15 degrees, horizontal flips, Mixup, RandomErasing, and CutMix were utilized to artificially expand the training dataset and make the models more robust. Additionally, Label Smoothing was applied to regularize the neural networks, which helps in preventing the model from becoming too confident about its predictions.\n\nThe models were optimized using sharpness-aware minimization with either RAdam or Adam optimizers. This approach helps in finding flatter minima in the loss landscape, which generally leads to better generalization and reduced overfitting. The training process was carefully monitored, and if overfitting was observed or if the learning had converged by 100 epochs, the training was terminated. If convergence was not achieved by 100 epochs, training was extended for an additional 50 epochs until overfitting or convergence was confirmed.\n\nTo further mitigate overfitting, the weights of the models that achieved the minimum loss in the validation dataset at the end of each epoch were saved. These models served as candidates for weak learners in the final ensemble model. The ensemble approach helps in combining the strengths of multiple models, reducing the risk of overfitting and improving overall performance.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules for the models used in this study are detailed in the supplementary tables. These tables provide comprehensive information on various aspects such as the optimizer used, batch size, number of epochs, initial and minimum learning rates, and the layers that were trainable. For instance, models like VGG16, VGG19, InceptionResNetV2, and others have specific configurations listed for different epochs, including the learning rates and the layers that were fine-tuned.\n\nThe optimization parameters, including the use of sharpness-aware minimization (SAM) with the Rectified Adam (RAdam) optimizer, are also clearly outlined. Additionally, techniques such as label smoothing, Mixup, RandomErasing, and CutMix were utilized during training to enhance model performance. The activation function for the second-to-last fully connected layer is specified as mish.\n\nRegarding the availability of model files and optimization parameters, the study does not explicitly mention where these files can be accessed or under what license they are provided. Therefore, it is not clear whether the model files and optimization parameters are publicly available or if there are any restrictions on their use.\n\nIn summary, while the hyper-parameter configurations and optimization schedules are thoroughly documented, the availability and licensing details of the model files and optimization parameters are not specified.",
  "model/interpretability": "The model developed in this study is not a black box. To enhance interpretability, we utilized Gradient-weighted Class Activation Mapping (GRAD-CAM) to generate feature map images. These maps highlight the areas in the chest X-ray images that the model focuses on when making predictions. This visualization technique allows us to observe where the model's attention is directed, providing insights into the model's decision-making process. Additionally, we employed attention mechanisms within our deep learning architectures, which further contribute to the transparency of the model by indicating the importance of different features in the input data. These interpretability tools help in understanding the model's behavior and ensuring that it is focusing on clinically relevant aspects of the chest X-ray images.",
  "model/output": "The model is a classification model. It predicts the probability of BNP (B-type natriuretic peptide) levels being greater than or equal to 200 pg/mL. The output layer of the model uses a softmax activation function, generating probabilities between 0 and 1. This indicates that the model is designed to classify inputs into one of two categories based on the BNP threshold. The performance metrics provided, such as accuracy, precision, sensitivity, specificity, F1 score, ROC AUC, and PR AUC, are all relevant to classification tasks. These metrics evaluate how well the model distinguishes between the two classes: BNP levels below and above 200 pg/mL.",
  "model/duration": "The execution time for the models was managed to ensure completion within a reasonable timeframe. To achieve this, we set a manageable number of 100 images for testing, which allowed us to complete the process efficiently. This decision was made considering the potential time required for human testing, especially when accounting for variations due to AI assistance and inter-dataset variability. By limiting the number of images, we could effectively assess the model's performance without an impractically long testing period. The models were trained using powerful hardware, including an NVIDIA GeForce RTX 3080Ti GPU or the Google Colaboratory platform, which facilitated faster processing and training times. Additionally, the training process was monitored for overfitting or convergence, with models trained for at least 100 epochs and extended up to 150 epochs if necessary. This approach ensured that the models were optimized efficiently, balancing thorough training with practical execution time.",
  "model/availability": "The source code for the models used in this study will be made available on GitHub following the publication of this article. This includes sample codes for their application, allowing anyone to evaluate and refine the models. The models are designed to be end-to-end systems that output the probability of elevated BNP levels from images, providing a single output for each input with perfect reproducibility. The availability of these models on GitHub ensures that the methods and algorithms can be accessed and utilized by the broader scientific community. This open access approach encourages further research, validation, and potential improvements to the models, fostering innovation and collaboration in the field.",
  "evaluation/method": "The evaluation method for the developed AI model involved several key steps and considerations. Initially, 31 modified pre-trained image recognition models were fine-tuned as weak learners to predict elevated BNP levels. These models were then used to construct a final soft ensemble model by averaging their probabilities. A probability threshold of 0.5 was set to determine whether BNP levels were above or below the cut-off value.\n\nThe performance of the ensemble model was assessed using an external test dataset, which consisted of patients from a different hospital than those used for training and validation. This approach helped to ensure that the model's performance was evaluated on unseen data, reducing the risk of overfitting.\n\nSeveral metrics were calculated to evaluate the model's performance, including accuracy, precision, sensitivity (recall), specificity, F1 score, and the area under the receiver-operating-characteristics (ROC) and precision-recall (PR) curves. These metrics provided a comprehensive view of the model's ability to correctly predict elevated BNP levels.\n\nTo further validate the model's performance, a human testing phase was conducted. Volunteers from the hospital staff were trained to recognize general findings of heart failure in chest X-ray images and the characteristics of BNP. They were then asked to evaluate 100 chest X-ray images from the test dataset and provide binary predictions. This process was repeated with the assistance of the AI model's predictions to assess whether AI assistance could improve human diagnostic performance.\n\nThe accuracy of the AI model on the test dataset was found to be 86%, which was 10\u201320% higher than that of humans. This comparison highlighted the potential of the AI model to enhance diagnostic accuracy in clinical settings. The participants in the human testing were categorized based on their medical experience, providing insights into how different levels of expertise might benefit from AI assistance.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report a comprehensive set of metrics to evaluate the performance of our prediction models. These metrics include accuracy, precision, sensitivity (recall), specificity, F1 score, ROC AUC, and PR AUC. These metrics provide a thorough assessment of the models' capabilities, covering various aspects of performance such as the balance between precision and recall, the model's ability to discriminate between positive and negative classes, and the overall correctness of the predictions.\n\nAccuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. Precision indicates the proportion of true positive results among all positive results predicted by the model. Sensitivity, or recall, reflects the model's ability to identify positive cases correctly. Specificity measures the proportion of true negative results among all actual negative cases. The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both concerns.\n\nROC AUC (Receiver Operating Characteristic Area Under the Curve) evaluates the model's ability to distinguish between classes, with higher values indicating better performance. PR AUC (Precision-Recall Area Under the Curve) is particularly useful for imbalanced datasets, as it focuses on the performance of the positive class.\n\nThe reported metrics are representative of standard practices in the literature, ensuring that our evaluation is comparable to other studies in the field. This set of metrics allows for a robust assessment of model performance, providing insights into the strengths and weaknesses of each model.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, our focus was on developing and evaluating deep learning models tailored to predict elevated BNP levels from chest X-ray images. We fine-tuned 31 modified pre-trained image recognition models as weak learners and subsequently created an ensemble model. This approach allowed us to leverage the strengths of multiple models to improve predictive performance.\n\nWe did not compare our models to simpler baselines. Our primary goal was to achieve high accuracy in predicting elevated BNP levels, and we believed that using state-of-the-art deep learning techniques was the most effective way to accomplish this. The models were evaluated using metrics such as accuracy, precision, sensitivity, specificity, F1 score, and area under the curve (AUC) for both ROC and PR curves. These evaluations were conducted on a test dataset to ensure the models' performance was assessed on unseen data.\n\nThe ensemble model, which averaged the probabilities of the 31 individual models, demonstrated strong performance with an accuracy of 86% on the test dataset. This ensemble approach was chosen to enhance the robustness and reliability of our predictions. The individual models, including various architectures like ConvNeXt, ViT, and MLPMixer, were fine-tuned and optimized using techniques such as sharpness-aware minimization and label smoothing. The training process involved data augmentation techniques like rotation, horizontal flips, Mixup, RandomErasing, and CutMix to improve the models' generalization capabilities.",
  "evaluation/confidence": "The evaluation of our models included a thorough statistical analysis to ensure the robustness and significance of our results. Continuous variables are presented with medians and quartiles or means with standard deviations, while categorical variables are shown as numbers and percentages. This approach provides a clear picture of the data distribution and variability.\n\nFor the performance metrics, we utilized confidence intervals to indicate the reliability of our estimates. Specifically, the accuracy, precision, sensitivity, specificity, F1 score, ROC AUC, and PR AUC are presented with their respective confidence intervals, allowing for a comprehensive understanding of the model's performance.\n\nStatistical significance was assessed using Welch\u2019s t-test or paired t-test, depending on the data characteristics. A P value of less than 0.05 was considered statistically significant. This rigorous statistical approach ensures that any claims of superiority over other methods or baselines are well-founded.\n\nThe ensemble model, for instance, showed improved accuracy compared to individual weak learners, with statistically significant differences. This enhancement in performance metrics, supported by confidence intervals and statistical tests, underscores the effectiveness of our approach.\n\nIn summary, the performance metrics are accompanied by confidence intervals, and the results are statistically significant, providing strong evidence that our method is superior to others and baselines.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study does not provide details on the availability of the datasets used for training, validation, or testing. While the study mentions the use of chest X-ray images and BNP labels, it does not specify whether these datasets will be made accessible to the public. Additionally, the study does not discuss any licensing agreements or terms under which the data might be shared. For further information on data availability, it would be necessary to contact the authors directly."
}