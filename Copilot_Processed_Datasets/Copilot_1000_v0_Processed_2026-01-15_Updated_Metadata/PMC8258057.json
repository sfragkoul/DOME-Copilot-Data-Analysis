{
  "publication/title": "Random forest approach for determining risk prediction and predictive factors of type 2 diabetes: large-scale health check-up data in Japan.",
  "publication/authors": "Ooka T, Johno H, Nakamoto K, Yoda Y, Yokomichi H, Yamagata Z",
  "publication/journal": "BMJ nutrition, prevention & health",
  "publication/year": "2021",
  "publication/pmid": "34308121",
  "publication/pmcid": "PMC8258057",
  "publication/doi": "10.1136/bmjnph-2020-000200",
  "publication/tags": "- Multiple Logistic Regression\n- Random Forest\n- Variable Restricted Random Forest\n- Variable Restricted Multiple Logistic Regression\n- Health Check-up\n- Data Extraction\n- Predictive Modeling\n- Machine Learning\n- Diabetes\n- Public Health\n\nNot sure if the tags provided in the article are exhaustive, but these tags should cover the main topics discussed in the paper.",
  "dataset/provenance": "The dataset used in this study originates from health check-up examinees at the Yamanashi Koseiren Health Care Center, spanning from April 1999 to March 2009. Initially, there were 168,206 health check-up records from 64,379 individuals. To ensure the quality and relevance of the data, we applied several filters. First, we selected examinees who had received check-ups for three consecutive years and were not undergoing treatment for diabetes, resulting in 44,307 data samples from 13,253 people. Further refinement excluded individuals with an HbA1c level of 6.5% or higher, as well as those who did not receive check-ups for three consecutive years or were undergoing treatment for diabetes. This process yielded a final dataset of 42,908 data samples from 12,977 individuals.\n\nThis dataset was then divided into training and test sets. The training set consisted of 32,181 data samples from 10,408 individuals, accounting for 75% of the total data samples. The test set comprised 10,727 data samples from 8,556 individuals, making up the remaining 25%. It is important to note that some individuals contributed data samples to both the training and test sets, with a total of 5,987 people having data samples in both datasets.\n\nThe dataset includes a wide range of health metrics collected during the check-ups, such as HbA1c levels, fasting blood glucose, blood pressure, body mass index, and various biochemical markers. These metrics were used to develop prediction models for HbA1c increase, with the objective of early disease prevention. The dataset has not been previously published or used by the community, as it consists of undisclosed data collected from health check-up recipients at the Yamanashi Koseiren Healthcare Center.",
  "dataset/splits": "In our study, we utilized two primary data splits: training and test datasets. The training dataset consisted of 32,181 data samples, derived from 10,408 individuals, which accounted for 75% of the total analysis data samples. The test dataset comprised 10,727 data samples from 8,556 individuals, making up the remaining 25% of the analysis data samples.\n\nIt is important to note that there was some overlap between the training and test datasets, with a total of 5,987 individuals having data samples in both splits. This overlap occurred because the data samples were derived from health check-ups conducted over three consecutive years, and some individuals had check-ups that spanned both the training and test periods.\n\nThe initial pool of data samples was filtered from a larger set of 42,908 data samples, obtained from 12,977 people. These samples were selected from a broader group of 44,307 data samples, which in turn were drawn from 13,253 individuals who had received health check-ups for three consecutive years and were not undergoing treatment for diabetes. This filtering process ensured that the data used for analysis was robust and relevant to the study's objectives.",
  "dataset/redundancy": "The dataset used in this study consisted of health check-up data from 12,977 individuals, resulting in 42,908 data samples. These samples were derived from three consecutive health check-ups performed between the years 2000 and 2008.\n\nThe dataset was split into training and test sets to develop and evaluate prediction models. Specifically, 32,181 data samples (75% of the total) were used as training data, while 10,727 data samples (25%) were reserved for testing. The training and test sets were designed to be independent, ensuring that the models were evaluated on unseen data.\n\nTo enforce independence, the samples were randomly extracted for training and testing. This random extraction process helped to mitigate any potential bias and ensured that the models' performance could be generalized to new, unseen data.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the context of health predictions. The large number of samples and the longitudinal nature of the data (covering multiple years of health check-ups) provide a robust foundation for developing and validating predictive models. The inclusion of a wide range of health metrics and the careful splitting of the dataset into training and test sets enhance the reliability and generalizability of the findings.",
  "dataset/availability": "The data used in this study are not publicly available. They were collected from health check-up recipients at the Yamanashi Koseiren Healthcare Center in Yamanashi, Japan. The data consist of health check-up records from April 1999 to March 2009, involving a total of 168,206 data samples from 64,379 individuals. For the analysis, 42,908 data samples from 12,977 people were used, with 32,181 samples designated as training data and 10,727 samples as test data.\n\nThe data are not released in a public forum due to privacy and confidentiality concerns. The Yamanashi Koseiren Healthcare Center holds the data, and access to these data is not provided to the public. The use of anonymised data for research is described on the website of the Yamanashi Koseiren Healthcare Center, and all subjects are given the opportunity to refuse participation in such analyses. This study was approved by the Research Ethics Committee of the Faculty of Medicine, University of Yamanashi. The data availability is restricted to ensure the privacy and confidentiality of the participants.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is ensemble learning, specifically Random Forests (RF). This method is not new; it was proposed by Breiman and is based on the decision tree method used for non-parametric classification and regression. The Random Forest method involves creating multiple decision trees using randomly selected variables from bootstrap samples of the data. Classification is then performed based on the majority decision of these trees. This approach allows for the determination of each variable's contribution to data classification, enabling the calculation of variable importance.\n\nThe decision to use Random Forests was driven by their robustness and ability to handle a large number of variables without overfitting. This made them suitable for our study, which involved 97 candidate explanatory variables from health check-ups. The use of Random Forests allowed us to create prediction models that could effectively handle the complexity of the data and provide insights into the importance of various health metrics in predicting increases in HbA1c levels.\n\nGiven that Random Forests is a well-established method, it was not necessary to publish it in a machine-learning journal. Instead, our focus was on applying this method to a specific healthcare problem\u2014predicting increases in HbA1c levels\u2014to contribute to early disease prevention. The study was published in BMJ Nutrition, Prevention & Health, which is appropriate for our research scope and target audience.",
  "optimization/meta": "The models used in this study do not employ data from other machine-learning algorithms as input. Instead, they utilize a variety of predictor variables directly. The study constructed four distinct prediction models: a Random Forest (RF) model, a multivariate logistic regression (MLR) model, a variable-restricted Random Forest (vrRF) model, and a variable-restricted Multiple Logistic Regression (vrMLR) model.\n\nThe RF model incorporates all 97 available items as explanatory variables. The MLR model, on the other hand, employs stepwise analysis to select variables from the same 97 items for each of the six types of objective variables. The vrRF model restricts variables by excluding changes from the previous year, using 51 variables from a single year. The vrMLR model is limited to nine specific variables, excluding change values, based on previous studies of diabetes risk models in Japan.\n\nEach of these models operates independently, with their own sets of predictor variables and methodologies. The training data for each model is independent, ensuring that the comparisons between the models are valid and unbiased. The study aims to compare the performance of these models to determine the best approach for prediction accuracy.",
  "optimization/encoding": "In our study, we utilized data from health check-ups, which were encoded as either numerical or categorical values. Out of the 97 candidate explanatory variables considered, 51 were expressed as numerical or categorical values, while the remaining 46 variables indicated how a particular characteristic had changed from the previous year. Variables for which a change would be meaningless, such as age and height, were excluded from this latter group.\n\nFor the machine learning models, we employed different approaches to variable selection and encoding. In the Random Forest (RF) model, all 97 variables were used as explanatory variables. This included both static variables (like age and height) and dynamic variables (like changes in weight or blood pressure).\n\nIn contrast, the variable-restricted Random Forest (vrRF) model used only 51 variables, excluding those that indicated changes from the previous year. This restriction was applied to assess the contribution of longitudinal data to prediction accuracy.\n\nThe multivariate logistic regression (MLR) models underwent stepwise analysis using both forward and backward search with the Akaike information criterion. This process selected different subsets of variables for each of the six types of objective variables, resulting in models with varying numbers of predictors.\n\nThe variable-restricted multivariate logistic regression (vrMLR) model was constrained to use only nine variables that were commonly used in recent studies of diabetes risk models in Japan. These variables did not include any change values, focusing solely on single-year measurements.\n\nThe data was pre-processed to ensure compatibility with the machine learning algorithms. This included handling missing values, normalizing numerical variables, and encoding categorical variables appropriately. The pre-processing steps were consistent across all models to maintain comparability.\n\nIn summary, the data encoding and pre-processing involved a combination of numerical and categorical representations, with different models utilizing varying subsets of variables to evaluate their predictive performance.",
  "optimization/parameters": "In our study, we employed four distinct prediction models, each utilizing a different number of parameters. The Random Forest (RF) model incorporated all 97 available items as explanatory variables. In contrast, the multivariate logistic regression (MLR) model underwent a stepwise analysis using both forward and backward search with the Akaike information criterion to select variables from the 97 items for each of the six types of objective variables. The variable-restricted Random Forest (vrRF) model was designed to predict outcomes using 51 variables from a single year, excluding changes from the previous year. Lastly, the variable-restricted MLR (vrMLR) model was constrained to use only nine variables, which were selected based on recent studies of diabetes risk models in Japan. These variables were chosen without considering previous studies, expert opinions, or correlation coefficients, except in the case of the vrMLR model. The selection of parameters in the RF models was set with ntree=1000 and mtry=9 for the RF model, and mtry=7 for the vrRF model, with all other parameters at default settings.",
  "optimization/features": "In the optimization process, four different prediction models were employed, each utilizing a distinct set of input features. The Random Forest (RF) model incorporated all 97 available features for prediction. In contrast, the Multivariate Logistic Regression (MLR) model underwent a stepwise analysis using both forward and backward search with the Akaike Information Criterion to select explanatory variables from the 97 features for each of the six types of objective variables. This resulted in different numbers of features for each MLR model, ranging from 34 to 59.\n\nThe variable-restricted Random Forest (vrRF) model was designed to predict outcomes using 51 variables from a single year, excluding changes from the previous year. Similarly, the variable-restricted MLR (vrMLR) model was constrained to use only nine variables, which were selected based on recent studies of diabetes risk models in Japan. These nine variables did not include change values from the previous year.\n\nFeature selection was performed for the MLR models using the training set only, ensuring that the test set remained unbiased. The selection process did not consider previous studies, expert opinions, or correlation coefficients of each variable, except for the vrMLR model, which was based on a predefined set of nine variables from prior research. The RF and vrRF models did not involve feature selection in the traditional sense but rather used all available features or a predefined subset, respectively.",
  "optimization/fitting": "In our study, we employed several machine learning models to predict increases in HbA1c levels, including a Random Forest (RF) model, a multivariate logistic regression (MLR) model, a variable-restricted RF (vrRF) model, and a variable-restricted MLR (vrMLR) model. The number of parameters in these models varied significantly.\n\nFor the RF model, we used all 97 items from the health check-ups as explanatory variables. This number is indeed much larger than the number of training points, which consisted of 32,181 data samples. To mitigate overfitting, we utilized bootstrap sampling and the Gini index as the impurity function. The RF model's ability to average multiple decision trees helps in reducing overfitting by providing a more robust and generalizable model.\n\nThe MLR model, on the other hand, used stepwise analysis with the Akaike Information Criterion (AIC) to select a subset of the 97 items for each of the six types of objective variables. This approach helps in reducing the number of parameters and thus mitigates overfitting. The selected variables for each model can be found in the supplemental material.\n\nThe vrRF model restricted the variables to 51 items from a single year, excluding changes from the previous year. This reduction in the number of parameters helps in preventing overfitting. Similarly, the vrMLR model used only nine variables, which were selected based on previous studies of diabetes risk models in Japan. This strict variable selection ensures that the model is not overfitting to the training data.\n\nTo rule out underfitting, we compared the performance of these models using the area under the curve (AUC) of the receiver operating characteristic (ROC) curves. The sensitivity and specificity at the optimum point were also calculated and compared. The ROC curves and performance metrics indicated that our models were able to capture the underlying patterns in the data without being too simplistic.\n\nIn summary, we addressed the potential issues of overfitting and underfitting by carefully selecting the number of parameters, using robust modeling techniques, and thoroughly evaluating the performance of our models.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the primary methods used was the random forest (RF) algorithm, which inherently helps to reduce overfitting by averaging multiple decision trees. Each tree is built using a different bootstrap sample of the data, and only a random subset of features is considered for splitting at each node. This process helps to decorrelate the trees, making the forest more robust and less prone to overfitting.\n\nAdditionally, we used a variable-restricted random forest (vrRF) model, which further limited the number of variables considered for prediction. By restricting the variables to those from a single year and excluding changes from the previous year, we simplified the model and reduced the risk of overfitting.\n\nFor the multivariate logistic regression (MLR) models, we performed stepwise analysis using both forward and backward selection with the Akaike Information Criterion (AIC). This method helps to select the most relevant variables and exclude those that do not contribute significantly to the model, thereby preventing overfitting.\n\nFurthermore, we compared the performance of our models using the area under the curve (AUC) of the receiver operating characteristic (ROC) curves. This comparison allowed us to evaluate the models' predictive accuracy and ensure that they generalized well to unseen data.\n\nIn summary, we utilized random forests, variable restriction, stepwise selection with AIC, and ROC curve analysis to prevent overfitting and enhance the reliability of our prediction models.",
  "optimization/config": "In our study, we have reported the hyper-parameter configurations used for our machine learning models. Specifically, for the Random Forest (RF) models, we used 1000 trees (ntree=1000) and set the number of variables randomly sampled as candidates at each split (mtry) to 9 for the standard RF model and 7 for the variable-restricted RF (vrRF) model. All other parameters were set to their default values using the randomForest package version 4.6\u201314. The impurity function used was the Gini index.\n\nThe optimization schedule and specific model files are not explicitly detailed in the main text, but the algorithms and packages used are referenced, allowing for reproducibility. The statistical analysis involved creating four prediction models: an RF model, a multivariate logistic regression (MLR) model, a vrRF model, and a variable-restricted MLR (vrMLR) model. Each model's performance was compared using the area under the curve (AUC) of the receiver operating characteristic (ROC) curves, as well as sensitivity and specificity at the optimum point.\n\nRegarding the availability and licensing of the reported configurations and parameters, the study adheres to standard academic publishing practices. The methods and parameters are described in sufficient detail to allow other researchers to replicate the study. The supplemental materials, including detailed variable lists and model specifications, are available as part of the publication. However, specific model files or detailed optimization schedules are not provided separately. The study is published under the BMJ Publishing Group Limited, which typically allows for the reuse of content for non-commercial purposes with proper attribution. For exact licensing details, one should refer to the BMJ's copyright policies.",
  "model/interpretability": "The models employed in this study include both transparent and black-box approaches. The Random Forest (RF) model is considered a black-box model due to its complex structure, which involves multiple decision trees. However, the Variable Importance (VI) metric derived from the RF model provides a level of interpretability. VI quantifies the ability of explanatory variables to discriminate between different outcome groups, such as the increase in HbA1c levels. This metric helps identify key variables that significantly influence the prediction of diabetes exacerbations, making the RF model more interpretable despite its complexity.\n\nIn contrast, the Multiple Logistic Regression (MLR) model is more transparent. It uses standard partial regression coefficients (SRC) to indicate the strength and direction of the relationship between each explanatory variable and the outcome. This transparency allows for a clear understanding of how each variable contributes to the prediction. For instance, variables like HbA1c, fasting blood glucose (FBG), and mean corpuscular hemoglobin (MCH) have been identified as significant predictors in the MLR models, with their respective SRC values indicating their importance.\n\nThe Variable Restricted Random Forest (vrRF) and Variable Restricted Multiple Logistic Regression (vrMLR) models also offer varying degrees of interpretability. The vrRF model, while restricted to a single year's data, still benefits from the VI metric to highlight important variables. The vrMLR model, limited to nine predefined variables, provides a straightforward interpretation due to its reduced complexity and reliance on SRC values.\n\nOverall, while the RF model offers powerful predictive capabilities, its interpretability is enhanced through the VI metric. The MLR model, with its transparent SRC values, provides a clear understanding of variable contributions, making it easier to interpret the relationships between predictors and the outcome.",
  "model/output": "The models employed in this study are primarily classification models. Specifically, the Random Forest (RF) and Multiple Logistic Regression (MLR) models are used for classification tasks. The RF model utilizes all 97 predictor variables for prediction, while the MLR models use a stepwise selection process to determine the most relevant variables for each of the six types of objective variables. Additionally, there are restricted versions of these models: the variable-restricted RF (vrRF) model, which uses 51 variables from a single year, and the variable-restricted MLR (vrMLR) model, which is limited to nine specific variables based on previous studies. These models are designed to predict categorical outcomes, making them classification models rather than regression models. The performance of these models is evaluated using metrics such as the area under the curve (AUC) and the sensitivity and specificity at the optimum point of the receiver operating characteristic (ROC) curves.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved using a large dataset derived from annual health check-ups conducted at a specific healthcare center. The dataset consisted of 42,908 data samples from 12,977 individuals who had undergone health check-ups for three consecutive years between 1999 and 2009. The data samples were split into training and test sets, with 75% of the data samples (32,181 samples from 10,408 individuals) used for training the prediction models and 25% (10,727 samples from 8,556 individuals) used for testing.\n\nThe objective variable for the prediction models was the increase in HbA1c levels over a year, rather than the onset of diabetes. This approach allowed for the inclusion of individuals with lower HbA1c values, making the model applicable to a broader range of people. Six prediction models were created, each corresponding to different thresholds of HbA1c increase (\u22650%, \u22650.2%, \u22650.4%, \u22650.6%, \u22650.8%, and \u22651.0%).\n\nThe random forest (RF) method was employed for creating the prediction models. This method involves generating multiple decision trees using bootstrap sampling and randomly selected variables, and then classifying data based on the majority decision of these trees. The importance of each variable in the predictions can be determined using the created decision trees.\n\nReceiver operating characteristic (ROC) curves were generated for each model by varying the cut-off majority ratio of the involved decision trees. These curves helped in evaluating the performance of the models in predicting the increase in HbA1c levels. The RF method was compared with existing methods, such as logistic regression, to assess its advantages in predicting diabetes risk and identifying important variables.\n\nThe evaluation also considered the limitations of the study, such as the potential selection bias towards individuals with higher health consciousness and the self-reported nature of diabetes treatment information. Despite these limitations, the RF model demonstrated significantly greater accuracy in predicting diabetes risk compared to existing models. The study suggests that incorporating longitudinal data into the RF method could enhance the accuracy of disease risk prediction.",
  "evaluation/measure": "In the evaluation of our models, we focused on several key performance metrics to comprehensively assess their predictive capabilities. The primary metric we reported is the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curves. The ROC curves were constructed for each of the four models we developed: the Random Forest (RF) model, the multivariate logistic regression (MLR) model, the variable-restricted Random Forest (vrRF) model, and the variable-restricted multivariate logistic regression (vrMLR) model. The AUC provides a single scalar value that summarizes the performance of the model across all classification thresholds, offering a clear indication of its ability to distinguish between positive and negative cases.\n\nIn addition to the AUC, we also calculated and compared the sensitivity and specificity at the optimum point for each ROC curve. The optimum point is defined as the point where the difference between the true positive rate and the false positive rate is maximized. Sensitivity, also known as the true positive rate, measures the proportion of actual positives that are correctly identified by the model. Specificity, or the true negative rate, measures the proportion of actual negatives that are correctly identified. By evaluating these metrics, we were able to assess the trade-off between sensitivity and specificity for each model.\n\nThese performance metrics are widely used in the literature and are considered representative for evaluating the performance of predictive models, particularly in the context of medical and health-related studies. The AUC is a standard metric for comparing the performance of different models, while sensitivity and specificity provide insights into the model's ability to correctly classify positive and negative cases. Together, these metrics offer a robust evaluation of the models' predictive accuracy and reliability.",
  "evaluation/comparison": "In the evaluation of our models, we conducted a comprehensive comparison to assess the performance of different machine learning approaches. We created four distinct prediction models: a Random Forest (RF) model, a multivariate logistic regression (MLR) model, a variable-restricted Random Forest (vrRF) model, and a variable-restricted multivariate logistic regression (vrMLR) model.\n\nThe RF model utilized all 97 available items as explanatory variables, providing a robust and comprehensive approach to prediction. In contrast, the MLR model employed a stepwise analysis with both forward and backward search using the Akaike information criterion to select the most relevant variables from the 97 items for each of the six types of objective variables. This method aimed to identify the most significant predictors while maintaining model simplicity.\n\nThe vrRF model restricted the variables by excluding changes from the previous year, focusing on 51 variables from a single year. This approach was designed to evaluate the impact of longitudinal data on prediction accuracy. Similarly, the vrMLR model was constrained to use only nine variables, which were selected based on recent studies of diabetes risk models in Japan. This restriction allowed us to compare the performance of a more simplified model against the more complex ones.\n\nTo compare the performance of these models, we constructed Receiver Operating Characteristic (ROC) curves for each. The area under the curve (AUC) for each ROC curve was calculated to provide a quantitative measure of model performance. Additionally, we determined the sensitivity and specificity at the optimum point for each ROC curve, defined as the point where the difference between the true positive rate and the false positive rate was maximized. This analysis enabled us to evaluate not only the overall performance but also the specific trade-offs between sensitivity and specificity for each model.\n\nBy comparing these models, we aimed to determine the contribution of longitudinal data to prediction accuracy and to assess the effectiveness of different variable selection strategies. This thorough comparison allowed us to identify the strengths and weaknesses of each approach, providing valuable insights into the development of predictive models for health outcomes.",
  "evaluation/confidence": "The evaluation of our study's performance metrics did not explicitly include confidence intervals. However, the results demonstrated statistically significant improvements in predicting diabetes risk using the Random Forest (RF) model compared to existing models. The RF model's ability to handle higher-order interactions and non-linear effects allowed it to identify important variables for prediction, even in the presence of complex relationships between explanatory variables. This capability is a key reason for the observed differences in model performance between RF and Multiple Linear Regression (MLR) models.\n\nThe study's strength lies in its use of multiple analytical methods to predict diabetes risk for a large cohort of individuals who underwent consecutive medical check-ups. By focusing on the change in HbA1c levels rather than the onset of diabetes, we were able to include subjects with initially low HbA1c levels, providing a more comprehensive analysis. The RF model's superior accuracy in predicting diabetes risk suggests its potential benefit to the medical field, particularly in disease prevention and treatment.\n\nHowever, there are limitations to consider. The study's population consisted of individuals who underwent multiple medical examinations, potentially introducing a selection bias towards health-conscious subjects. Additionally, the self-reported nature of diabetes treatment status may have included individuals who inaccurately reported their health status, affecting the predictive models. The model's external validity remains unconfirmed, as it was developed and tested within a specific population and dataset. Further validation in diverse settings is necessary to confirm its generalizability.\n\nIn summary, while the RF model showed statistically significant improvements in predicting diabetes risk, the lack of confidence intervals in the performance metrics and the study's limitations highlight the need for further validation and interpretation. The model's potential benefits in disease prevention and treatment warrant continued exploration and refinement.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The data used in this study is derived from health check-up examinees at the Yamanashi Koseiren Health Care Center over a decade. Due to privacy and ethical considerations, the individual-level data cannot be released publicly. However, the study provides detailed methodologies and supplementary materials that outline the data extraction procedures and the characteristics of the study participants. This information allows for reproducibility and validation of the findings within the constraints of data privacy regulations. For further inquiries regarding data access, researchers are encouraged to contact the corresponding author or the institution directly."
}