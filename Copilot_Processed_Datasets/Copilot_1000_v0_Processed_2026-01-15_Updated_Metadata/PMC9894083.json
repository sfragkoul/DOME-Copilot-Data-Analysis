{
  "publication/title": "Evaluating the utility of deep learning for predicting therapeutic response in diabetic eye disease.",
  "publication/authors": "Dong V, Sevgi DD, Kar SS, Srivastava SK, Ehlers JP, Madabhushi A",
  "publication/journal": "Frontiers in ophthalmology",
  "publication/year": "2022",
  "publication/pmid": "36744216",
  "publication/pmcid": "PMC9894083",
  "publication/doi": "10.3389/fopht.2022.852107",
  "publication/tags": "- Deep Learning\n- Ophthalmology\n- Treatment Response\n- Ultra-widefield Fluorescein Angiography\n- Optical Coherence Tomography\n- Transfer Learning\n- Data Scarcity\n- Class Activation Maps\n- Anti-VEGF Therapy\n- Diabetic Macular Edema\n- Retinal Vein Occlusion\n- Predictive Modeling\n- Medical Imaging\n- Machine Learning\n- Classi\ufb01cation Performance",
  "dataset/provenance": "The datasets used in this study originate from two primary sources. The first is the PERMEATE dataset, which is derived from an IRB-approved prospective clinical study conducted by the Cole Eye Institute at the Cleveland Clinic Foundation. This study focuses on assessing the outcomes of Intravitreal Aflibercept Injection (IAI) therapy in eyes with Diabetic Macular Edema (DME) or macular edema secondary to Retinal Vein Occlusion (RVO). The dataset includes Ultra-Widefield Fluorescein Angiography (UWFA) and Optical Coherence Tomography (OCT) images obtained at baseline and specific time points throughout the study. The UWFA PERMEATE dataset consists of 29 samples, while the OCT dataset contains 28 samples due to the exclusion of one rebounder patient because of poor image quality.\n\nThe second dataset is the Anti-VEGF dataset, which comprises UWFA imaging from a retrospective image analysis study on eyes with Diabetic Retinopathy (DR). This study, approved by the Cleveland Clinical Institutional Review Board, includes 217 eyes from 189 patients. The eyes are classified as either requiring anti-VEGF treatment or not requiring treatment.\n\nThe PERMEATE dataset has been used to evaluate the performance of deep learning (DL) techniques in predicting therapeutic response, particularly in scenarios with limited data. Previous studies have utilized similar datasets to explore the utility of DL in ophthalmology, often focusing on identifying key features correlated with treatment response. For instance, Alryalat et al. used an OCT dataset to train a DL classification model, achieving notable performance metrics. Additionally, radiomics and hand-crafted feature approaches have been employed to predict treatment response, demonstrating the potential of these methods in data-scarce environments. The Anti-VEGF dataset has been used to further investigate the impact of training set size on DL model performance, providing insights into the minimum dataset size required for effective training.",
  "dataset/splits": "In our study, we utilized two primary datasets: the PERMEATE dataset and the Anti-VEGF dataset. For the PERMEATE dataset, we employed leave-one-out cross-validation (LOOCV) due to its small size, consisting of 29 samples for UWFA and 28 samples for OCT. This method involves setting aside one sample for validation while using the remaining samples for training, repeating this process for each configuration of the dataset.\n\nThe Anti-VEGF dataset, which is larger, was divided into five subsets to evaluate the impact of training set size on deep learning performance. These subsets increased in size by a factor of 28, starting from 28 eyes and going up to 140 eyes. This approach allowed us to assess how the model's performance changes with varying amounts of training data. The subsets were balanced to ensure an equal distribution of classes within each subset.\n\nAdditionally, we explored different preprocessing techniques, including a patch-based approach and data augmentations, to address the challenges posed by the limited dataset size. These methods were applied to both the UWFA and OCT data to enhance the robustness of our models.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is transfer learning, specifically leveraging pre-trained deep learning models. The models employed include ResNet50, ResNet101, Inception-v3, and DenseNet201, all of which were pre-trained on the ImageNet database. These models are well-established in the field of computer vision and are widely used for image classification tasks due to their robust feature extraction capabilities.\n\nThe transfer learning approach was chosen to maximize the utility of the limited dataset available. By freezing the weights of all layers except the final layer, we aimed to leverage the pre-trained models' ability to recognize general features while allowing the final layer to adapt to the specific task of predicting treatment response.\n\nThis method is not new but is highly effective for tasks where data is scarce. The focus of our study was on evaluating the utility of deep learning for predicting treatment response in ophthalmology, rather than developing a novel machine-learning algorithm. Therefore, the algorithm was not published in a machine-learning journal but rather in an ophthalmology-focused publication to highlight its application in the medical field.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were tailored to the specific imaging modalities used: Ultra-Widefield Fluorescein Angiography (UWFA) and Optical Coherence Tomography (OCT). For UWFA images, instead of aggressive down-sampling, we employed center cropping to 2912x2912 pixels, followed by extracting non-overlapping patches, resulting in 196 patches per image. To mitigate artifacts, only the 25 center patches were used for each sample. This patch-based approach allowed us to utilize all patches in training and evaluation, with the final classification determined by majority class voting.\n\nFor OCT images, we segmented fluid compartments and applied these as masks to each B-scan. Slices lacking pixel data were removed, ensuring that each sample consisted of at most 128 masked B-scan slices. This preprocessing step helped in focusing the model on relevant regions of the OCT images.\n\nData augmentations were applied to both UWFA and OCT images to artificially increase the training data. These augmentations included random rotations, horizontal flips, and vertical flips. For UWFA, each image in the training set underwent these transformations with random probabilities. For OCT, augmentations were applied at the sample level rather than individual B-scan slices.\n\nTransfer learning was utilized with four different model architectures: ResNet50, ResNet101, Inception-v3, and DenseNet201. Leave-one-out cross-validation (LOOCV) was employed to maximize training set sizes, ensuring that each sample was used for both training and validation across different iterations.\n\nIn summary, our data encoding and preprocessing involved a patch-based approach for UWFA images, fluid compartment segmentation for OCT images, and extensive data augmentations to address the limited dataset size. These steps were crucial in preparing the data for effective training and evaluation of our deep learning models.",
  "optimization/parameters": "In our study, we utilized transfer learning with four different model architectures: ResNet50, ResNet101, Inception-v3, and DenseNet201. These models were pre-trained on the ImageNet database, and all layer weights were frozen except for the final layer. This approach allowed us to leverage the features learned from a large dataset while focusing on fine-tuning the final layer for our specific task.\n\nThe number of parameters (p) in each model varies due to the differences in their architectures. Specifically, ResNet50 has approximately 23 million parameters, ResNet101 has around 42 million parameters, Inception-v3 has about 23 million parameters, and DenseNet201 has roughly 20 million parameters. The final layer, which was unfrozen and trained on our dataset, added a relatively small number of additional parameters.\n\nThe selection of these architectures was based on their proven performance in various image classification tasks. By using pre-trained models, we aimed to mitigate the issue of limited data in our dataset. The choice of freezing all layers except the final one was made to ensure that the models could adapt to our specific task without overfitting to the small dataset. This approach also allowed us to maintain computational efficiency during the training process.",
  "optimization/features": "Not applicable",
  "optimization/fitting": "The fitting method employed in this study involved transfer learning using pre-trained deep learning models on the ImageNet database. This approach inherently addresses the issue of having a much larger number of parameters than training points, as the pre-trained models have already learned relevant features from a vast dataset.\n\nTo mitigate overfitting, several strategies were implemented. Firstly, leave-one-out cross-validation (LOOCV) was used to maximize the training set size, ensuring that each sample was used for both training and validation. Additionally, data augmentations such as rotations and horizontal/vertical flips were applied to artificially increase the diversity of the training data. These augmentations help the model generalize better by exposing it to varied versions of the same images.\n\nFurthermore, the final layer of the pre-trained models was the only layer with trainable weights, while all other layers were frozen. This constraint helps in preventing the model from overfitting to the small dataset. The models were trained for a fixed number of epochs (100) with a learning rate of 0.001, and the performance was averaged over multiple seeded runs to ensure robustness.\n\nTo rule out underfitting, the models were evaluated on their ability to learn from the data through the generation of class activation maps (CAMs). These maps visually identify the regions of interest that the model focuses on when making predictions. The analysis of CAMs helped verify that the models were learning relevant features from the data, despite the limited sample size.\n\nOverall, the combination of transfer learning, data augmentations, and careful validation techniques ensured that the models neither overfitted nor underfitted the data, providing a balanced approach to classification.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting, particularly given the limited size of our datasets. One of the primary methods used was data augmentation. For both UWFA and OCT imaging data, we applied random augmentations such as rotations and horizontal and vertical flips. These augmentations were introduced to artificially increase the diversity of the training data, helping the models to generalize better and reducing the risk of overfitting.\n\nAdditionally, we utilized transfer learning, which involves using pre-trained models on large datasets like ImageNet. By freezing the initial layers of these models and only training the final layers on our specific datasets, we leveraged the pre-learned features from the large dataset. This approach not only reduced the amount of data required for training but also helped in preventing overfitting by providing a robust feature extraction mechanism.\n\nLeave-one-out cross-validation (LOOCV) was another crucial technique employed. This method ensures that each sample in the dataset is used once as a validation set while the remaining samples form the training set. This process is repeated for all configurations of the dataset, providing a comprehensive evaluation of the model's performance and helping to mitigate overfitting.\n\nFurthermore, we conducted experiments with different model architectures, including ResNet50, ResNet101, Inception-v3, and DenseNet201. By comparing the performance of these architectures, we aimed to identify the most effective model for our specific task, thereby enhancing the robustness of our findings and reducing the likelihood of overfitting to any single architecture.\n\nIn summary, our study incorporated data augmentation, transfer learning, LOOCV, and the use of multiple model architectures to prevent overfitting and ensure the reliability of our results.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, the learning rate, number of epochs, and other relevant hyper-parameters were determined through grid-search experiments. These details are provided in the methods section, where the implementation of transfer learning models using PyTorch is described. The models were pre-trained on the ImageNet database, with all layer weights frozen except for the final layer.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly provided in the publication. The focus was on reporting the performance metrics and the methodological approach rather than distributing the model files directly. However, the code and models used can be made available upon request, subject to institutional policies and potential licensing agreements. The specific details about data augmentations, such as rotations and flips, are also outlined in the supplementary material, ensuring reproducibility of the experiments.\n\nFor those interested in replicating the experiments, the key configurations and steps are thoroughly documented, allowing researchers to implement similar setups using the described frameworks and techniques. The use of PyTorch ensures that the models and training processes are accessible and can be adapted for further research or applications in ophthalmology.",
  "model/interpretability": "The models used in our study are not entirely black-box, as we employed class activation maps (CAMs) to enhance interpretability. CAMs were generated for each sample to visually identify the regions of interest (ROIs) that the trained models focused on when making predictions. These maps highlight highly activated pixels with warmer tones and less activated pixels with cooler tones, providing insights into the model's decision-making process.\n\nHowever, the identified ROIs were found to be sporadic and inconsistent across different samples and model architectures. For instance, the models alternated between focusing on the optic nerve, macula, or peripheral regions, and fluid leakage was not consistently identified as an ROI. This inconsistency suggests that the models struggled to detect consistent visual indicators from the limited training set size, making the interpretability of the models somewhat limited.\n\nDespite these challenges, the use of CAMs allowed us to analyze whether the ROIs identified by the model correlated with clinically shown indicators for anti-VEGF treatment response. This approach helped us to verify if the underlying task was being learned by the model, regardless of sample variance. Overall, while the models are not entirely transparent, the use of CAMs provided some level of interpretability into the decision-making process.",
  "model/output": "The model employed in this study is designed for classification tasks. Specifically, it aims to classify patients as either \"rebounders\" or \"non-rebounders\" in response to anti-VEGF treatment. This classification is based on imaging data from Ultra-widefield Fluorescein Angiography (UWFA) and Optical Coherence Tomography (OCT) scans.\n\nThe classification performance was evaluated using metrics such as the Area Under the Curve (AUC) and accuracy. For instance, in the UWFA experiments, the best-performing model, DenseNet201, achieved a mean AUC of 0.507 \u00b1 0.042 and a mean accuracy of 0.511 \u00b1 0.075 over 30 seeded runs of Leave-One-Out Cross Validation (LOOCV). Similarly, in the OCT experiments, the performance varied across different fluid compartments, with the best-performing model for combined regions being ResNet101, which had a mean AUC of 0.458 \u00b1 0.052.\n\nConfusion matrices were generated to provide a detailed view of the model's performance, highlighting issues such as high false-positive and false-negative rates. These matrices showed that the models often struggled to correctly classify non-responder samples, frequently predicting them as responders.\n\nAdditionally, Class Activation Maps (CAMs) were used to visualize the regions of interest (ROIs) that the model focused on during predictions. These maps helped in understanding whether the model was identifying clinically relevant features. However, the results indicated that the models were inconsistent in identifying ROIs, often focusing on different areas such as the optic nerve, macula, or peripheral regions, rather than fluid leakage, which is a clinically significant indicator.\n\nOverall, the model's performance was sub-optimal, indicating challenges in predicting treatment response due to limited data and the complexity of the task.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to assess the performance of deep learning (DL) models in predicting treatment response using limited datasets. For the PERMEATE dataset, which consists of Ultra-widefield fluorescein angiography (UWFA) and Optical coherence tomography (OCT) imaging data, leave-one-out cross-validation (LOOCV) was utilized. This technique maximizes the training set size by setting aside one sample for validation in each iteration, ensuring that the model is trained on the largest possible dataset for each configuration.\n\nThe evaluation process included multiple experiments. In Experiment 1, the utility of DL on limited UWFA data was assessed. The images were preprocessed by resizing and center cropping to match the input dimensions of the pre-trained DL models. Transfer learning was applied using four model architectures: ResNet50, ResNet101, Inception-v3, and DenseNet201. The models were trained over 30 seeded runs of LOOCV, with each run consisting of 100 epochs and a learning rate of 0.001. The performance was evaluated using metrics such as the area under the curve (AUC) and accuracy, averaged over the 30 runs.\n\nExperiment 2 focused on the utility of DL on limited OCT data. Similar to Experiment 1, data augmentations were applied, and LOOCV was employed. The models were evaluated over 5 randomly seeded runs, with performance metrics including AUC and accuracy.\n\nAdditionally, the effect of sample size on performance was explored in Experiment 3. The best-performing model architecture from the UWFA experiment (DenseNet201) was retrained on the Anti-VEGF dataset with increasing training set sizes. Subsets of the dataset were created, increasing by a factor of 28, and the model performance was reported from 3-fold cross-validation. This experiment aimed to understand how the size of the training dataset impacts the model's ability to predict treatment response accurately.\n\nClass activation maps (CAMs) were generated to visualize the regions of interest (ROIs) that the trained models focused on when making predictions. These visualizations helped in analyzing whether the models were identifying clinically relevant features and if the underlying task was being learned consistently across different samples.\n\nOverall, the evaluation method involved a rigorous approach to assess the performance of DL models on limited datasets, using cross-validation techniques, data augmentations, and visualizations to ensure comprehensive and reliable results.",
  "evaluation/measure": "In our evaluation, we primarily focused on the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve as our main performance metric. The AUC provides a single scalar value that summarizes the performance of the classifier across all classification thresholds, making it a robust metric for evaluating model performance, especially when dealing with imbalanced datasets.\n\nIn addition to AUC, we also reported the mean accuracy across multiple runs. Accuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. However, given the imbalanced nature of our datasets, accuracy alone may not provide a complete picture of model performance. Therefore, we complemented it with the AUC metric.\n\nWe averaged these metrics over multiple seeded runs of Leave-One-Out Cross-Validation (LOOCV) to ensure the robustness and reliability of our results. This approach helps to mitigate the variability that can arise from a single train-test split, providing a more stable estimate of model performance.\n\nFor the experiments involving different fluid compartments in OCT data, we reported AUC results for individual regions (such as Intraretinal Fluid (IRF) and Subretinal Fluid (SRF)) as well as combined regions. This granularity allows us to understand how the model performs on specific types of data and identify areas where it may struggle.\n\nFurthermore, we visualized the areas of interest identified by the models using class activation heatmaps. These heatmaps highlight the regions in the images that most influence the model's classification decisions, providing insights into what features the model is focusing on.\n\nWhile our set of metrics is representative of common practices in the literature, especially for evaluating classification performance in medical imaging, we acknowledge that other metrics such as precision, recall, and F1-score could also provide valuable insights. However, given the focus of our study and the nature of our datasets, AUC and accuracy were deemed the most relevant and informative metrics for our evaluation.",
  "evaluation/comparison": "In our study, we focused on evaluating the utility of deep learning (DL) for predicting treatment response or future treatment need using ultra-widefield fluorescein angiography (UWFA) and optical coherence tomography (OCT) imaging data from the PERMEATE dataset. Our primary goal was to assess the performance of DL models in this specific context rather than comparing them to publicly available methods on benchmark datasets.\n\nWe utilized transfer learning with four different model architectures: ResNet50, ResNet101, Inception-v3, and DenseNet201. These models were pre-trained on the ImageNet database, and all layer weights were frozen except for the final layer. This approach was chosen to leverage the pre-existing knowledge from a large dataset and adapt it to our smaller, more specialized dataset.\n\nTo maximize the training set size, we employed leave-one-out cross-validation (LOOCV) for the UWFA and OCT experiments. This method involves setting aside one sample for validation while using the rest for training, and repeating this process for each sample. This technique is particularly useful for small datasets, as it ensures that each sample is used for both training and validation.\n\nIn addition to LOOCV, we also explored the use of data augmentations and a patch-based approach to further enhance the performance of our models. Data augmentations involved applying various transformations to the images, such as rotations and flips, to increase the diversity of the training data. The patch-based approach involved dividing the UWFA images into smaller patches and training the models on these patches instead of the whole images.\n\nWe did not perform a direct comparison to simpler baselines, as our focus was on evaluating the potential of DL models for this specific task. However, we did analyze the performance of our models in relation to random chance, which served as a basic benchmark. The results indicated that, despite the use of transfer learning and various enhancements, the performance of the DL models was sub-optimal and often at or slightly below random chance.\n\nIn summary, our study did not involve a comparison to publicly available methods or simpler baselines on benchmark datasets. Instead, we concentrated on assessing the feasibility of using DL for predicting treatment response in the context of UWFA and OCT imaging data from the PERMEATE dataset. Our findings suggest that, while DL has the potential to be a valuable tool in this area, further research and larger datasets are needed to improve the performance of these models.",
  "evaluation/confidence": "The evaluation of the models' performance includes confidence intervals for the reported metrics. For instance, the mean AUC and accuracy values for the models are presented with standard deviations, indicating the variability and confidence in these estimates. This is evident in the results for both UWFA and OCT experiments, where metrics such as mean AUC and accuracy are accompanied by \u00b1 values, reflecting the standard deviation over multiple runs.\n\nStatistical significance is considered through the use of leave-one-out cross-validation (LOOCV) and multiple seeded runs. This approach helps to ensure that the results are robust and not dependent on a single split of the data. For example, the OCT experiments report model AUC as an average over five seeded runs of LOOCV, providing a more reliable estimate of performance. Similarly, the UWFA experiments average metrics over 30 seeded runs of LOOCV, further enhancing the confidence in the reported results.\n\nThe inclusion of data augmentations and the consistent application of transfer learning techniques also contribute to the evaluation confidence. Data augmentations help to mitigate the impact of limited dataset sizes, and transfer learning leverages pre-trained models to improve performance on the specific task at hand. These methods collectively enhance the reliability and generalizability of the results.\n\nHowever, it is important to note that while the performance metrics show some improvement with data augmentations, the results are still suboptimal. This suggests that while the methods used increase confidence in the evaluation, the underlying challenge of predicting treatment response with limited data remains significant. The models struggle to achieve optimal classification performance, indicating that further advancements or additional data may be necessary to improve predictive accuracy.",
  "evaluation/availability": "The original contributions presented in the study are included in the article and supplementary material. Further inquiries regarding the availability of the evaluation data can be directed to the corresponding authors. The studies involving human participants were reviewed and approved by The Cleveland Clinic Institutional Review Board. Written informed consent for participation was not required for this study in accordance with national legislation and institutional requirements. The data availability statement indicates that the original contributions are included in the article and supplementary material, suggesting that the raw evaluation files are not publicly released. For access to the data, interested parties should contact the corresponding authors. The studies were conducted in accordance with ethical guidelines, and the data is not freely available to the public due to these considerations."
}