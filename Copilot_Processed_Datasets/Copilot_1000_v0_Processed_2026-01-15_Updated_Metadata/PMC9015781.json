{
  "publication/title": "Predicting Falls in Long-term Care Facilities: Machine Learning Study.",
  "publication/authors": "Thapa R, Garikipati A, Shokouhi S, Hurtado M, Barnes G, Hoffman J, Calvert J, Katzmann L, Mao Q, Das R",
  "publication/journal": "JMIR aging",
  "publication/year": "2022",
  "publication/pmid": "35363146",
  "publication/pmcid": "PMC9015781",
  "publication/doi": "10.2196/35373",
  "publication/tags": "- Vital signs\n- Machine learning\n- Blood pressure\n- Skilled nursing facilities\n- Independent living facilities\n- Assisted living facilities\n- Fall prediction\n- Elderly care\n- Elderly population\n- Older adult\n- Aging\n- Electronic health records\n- Fall risk assessment\n- Predictive modeling\n- Healthcare data analysis\n- Short-term fall prediction\n- Dynamic care practices\n- Senior care facilities\n- Fall risk surveillance\n- Healthcare technology",
  "dataset/provenance": "The dataset used in this study was sourced from a proprietary database containing electronic health records (EHR) from senior living communities managed by Juniper Communities, LLC, across the United States. The data spanned from 2007 to 2021. Initially, the dataset included records from 2785 residents. However, several filtration steps were applied to ensure the quality and relevance of the data. Residents without essential measurement times or EHR data, including diagnostic codes, were excluded. Additionally, individuals under the age of 60 were removed from the dataset. Finally, residents who did not have at least one month of data available before the machine learning algorithm's runtime were also excluded.\n\nThe final dataset used for analysis included a diverse range of senior care facilities, such as skilled nursing facilities, independent living facilities, assisted living facilities, and other non-major facilities without specific designations. The data was deidentified in compliance with the Health Insurance Portability and Accountability Act (HIPAA), ensuring the privacy and security of the individuals involved. This dataset has not been used in previous publications by the community.",
  "dataset/splits": "In our study, we employed multiple data splits to evaluate the performance of our models under different conditions. Initially, we used a standard holdout test set, which consisted of a portion of the data reserved for final evaluation. This split was used to assess the models' performance on unseen data, providing an estimate of their generalizability.\n\nAdditionally, we conducted post hoc analyses and external validation by separating the training and test sets based on facility type. This resulted in two additional splits:\n\n1. **Skilled Nursing Facility Split**: In this split, data from skilled nursing facilities were used as the test set, while data from all other facilities were used for model training. This split helped us evaluate the model's performance specifically for residents in skilled nursing facilities.\n\n2. **Assisted Living Facility Split**: Here, data from assisted living facilities served as the test set, with data from other facilities used for training. This allowed us to assess the model's performance for residents in assisted living facilities.\n\nThe distribution of data points in each split varied. For the holdout test set, the specific distribution is not detailed here, but it was designed to be representative of the overall dataset. In the facility-type splits, the number of data points corresponded to the number of residents in each type of facility. Skilled nursing facilities had the highest number of fall incidents, while independent living facilities had the lowest. This variation in the number of fall incidents across facility types influenced the distribution of data points in the respective splits.\n\nIn summary, we utilized three main data splits: a standard holdout test set and two facility-type-specific splits (skilled nursing and assisted living facilities). The distribution of data points in each split reflected the underlying data characteristics, with skilled nursing facilities having the most fall incidents and independent living facilities the least.",
  "dataset/redundancy": "The dataset used in this study was partitioned into training and test sets with an 80:20 ratio, employing stratified sampling to account for the imbalance between positive and negative cases. Both sets included a random mix of data from all four types of long-term care facilities within Juniper Communities: skilled nursing facilities, assisted living facilities, independent living facilities, and other non-major facilities. This approach ensured that the training and test sets were independent and representative of the overall population, maintaining the distribution of facility types and fall incidents.\n\nThe stratified sampling method was crucial for handling the class imbalance, as falls were relatively rare events. By ensuring that both sets contained a similar proportion of positive cases, the models were trained and evaluated on data that reflected the true distribution of fall incidents. This strategy helped in assessing the models' performance more accurately and in a manner that could be generalized to real-world scenarios.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets in the context of fall prediction. The inclusion of data from various types of facilities and the use of stratified sampling are consistent with best practices in machine learning, ensuring that the models are robust and generalizable. The dataset's diversity and the careful splitting method enhance the reliability of the findings, making them applicable to different senior care settings.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study include XGBoosting, logistic regression, and a multilayered perceptron. These are well-established algorithms in the field of machine learning and are not new.\n\nXGBoosting is a gradient boosting algorithm that combines the results from various decision trees to obtain prediction scores. It is known for its excellent performance in a wide range of classification problems, including those involving acute and chronic conditions. This algorithm was implemented in Python.\n\nLogistic regression is a statistical method for analyzing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible classes).\n\nA multilayered perceptron is a type of feed-forward artificial neural network that consists of at least three layers of nodes: an input layer, a hidden layer, and an output layer. Each node, or artificial neuron, in one layer connects with a certain weight to every node in the following layer.\n\nThese algorithms were chosen for their ability to handle the complexities of the data, including missing values and class imbalances. The optimization of hyperparameters for these models was conducted using a 5-fold cross-validation grid search, with the area under the receiver operating characteristic (AUROC) curve serving as the evaluation metric. The specific parameters optimized for each model were tailored to enhance their performance on the given dataset.",
  "optimization/meta": "The study did not employ a meta-predictor. Instead, it utilized three distinct machine learning models: XGBoosting, logistic regression, and a multilayered perceptron. Each of these models was trained and evaluated independently using the same dataset, which was partitioned into training and testing sets with an 80:20 ratio. The training and test sets included a random mix of data from various types of long-term care facilities, ensuring that the data was independent across different models.\n\nThe XGBoosting model, a gradient boosting algorithm, combined results from multiple decision trees to obtain prediction scores. Logistic regression and the multilayered perceptron model were also trained and tested to compare their performance with the XGBoosting model. The logistic regression model used optimization parameters such as the penalty term, class weight, optimization problem solver, and inverse of regularization strength. The multilayered perceptron model included parameters like the maximum iteration, hidden layer size, and learning rate.\n\nThe performance of each model was assessed using metrics such as the receiver operating characteristic (ROC) curve, sensitivity, and specificity. The study also conducted exploratory analyses to evaluate the impact of different prediction windows and input features on model performance. However, there was no meta-predictor that combined the outputs of these models to make final predictions.",
  "optimization/encoding": "For the machine-learning algorithms employed in this study, data encoding and preprocessing were crucial steps to ensure optimal model performance. The dataset consisted of electronic health records (EHRs) from various long-term care facilities, including skilled nursing, independent living, and assisted living facilities.\n\nMissing values were handled using different imputation approaches. For vital signs, forward and backward filling methods were utilized to estimate missing data. For all other features, the mean measurement across the entire training set was used for imputation. This approach helped maintain the integrity of the data while addressing missing values effectively.\n\nData standardization was applied to the logistic regression and multilayered perceptron models. A standard scaler from scikit-learn was used to scale the inputs, ensuring that all features contributed equally to the model's predictions. This step is particularly important for algorithms sensitive to the scale of input features.\n\nThe XGBoosting model, a gradient boosting algorithm, was implemented in Python. This model combines the results from multiple decision trees to obtain prediction scores. Within each decision tree, the resident population was split into successively smaller groups based on covariate values and predetermined thresholds. The model was trained using a 5-fold cross-validation grid search to optimize hyperparameters, which included maximum tree depth, regularization term, scale positive weight, learning rate, and the number of estimators.\n\nFor logistic regression, the optimization parameters included the penalty term, class weight, optimization problem solver, and inverse of regularization strength. The multilayered perceptron model, a type of feed-forward neural network, had optimization parameters such as the maximum iteration, hidden layer size, and learning rate. The hidden layer size was set to 250, and the convergence of the solver iteration was determined by reaching a maximum of 100 iterations or a tolerance value of 1e-9.\n\nAll models were trained using the same 68 input features, ensuring a consistent basis for comparison. The dataset was partitioned into a train:test ratio of 80:20 with stratified sampling to account for the relatively small positive class. Both training and test sets included a random mix of all facility types within Juniper Communities.\n\nIn summary, the data encoding and preprocessing involved careful handling of missing values, standardization of inputs for specific models, and rigorous hyperparameter optimization through cross-validation. These steps were essential in preparing the data for effective machine-learning model training and evaluation.",
  "optimization/parameters": "In our study, we optimized different sets of parameters for each of the models used. For the XGBoosting model, we optimized five parameters: maximum tree depth, regularization term (lambda), scale positive weight, learning rate, and number of estimators. The specific values chosen for these parameters were 6, 1.0, 13, 0.001, and 75, respectively.\n\nFor the logistic regression model, the optimization focused on the penalty term, class weight, optimization problem solver, and inverse of regularization strength. These parameters were selected to ensure the model could handle the class imbalance and regularization effectively.\n\nThe multilayered perceptron model had three key parameters optimized: maximum iteration, hidden layer size, and learning rate. The hidden layer size was set to 250, and the convergence of the solver iteration was determined by reaching a maximum of 100 iterations or a tolerance value of 1e-9.\n\nThe selection of these parameters was done using a 5-fold cross-validation grid search. This method involved evaluating the area under the receiver operating characteristic (AUROC) curve for different combinations of hyperparameters. The combinations that yielded the best performance metrics were then chosen as the optimized parameters for each model. This approach ensured that the models were robust and generalizable to the test data.",
  "optimization/features": "In our study, we initially extracted 250 features from the electronic health record (EHR) data. To enhance the performance and interpretability of our machine learning models, we performed feature selection. This process involved using a feature importance metric to preselect the most relevant features. As a result, we reduced the number of features from 250 to 68. These 68 selected features were then used to train our final models, including XGBoosting, logistic regression, and multilayered perceptron.\n\nThe feature selection was conducted using the training set only, ensuring that the evaluation on the test set remained unbiased. This approach helped us to identify the most significant variables associated with fall risk, such as age, sex, vital sign measurements, comorbidities, and medications. By focusing on these key features, we aimed to improve the predictive accuracy and generalizability of our models.",
  "optimization/fitting": "The study employed three machine learning models: XGBoosting, logistic regression, and a multilayered perceptron. Each model was optimized using a 5-fold cross-validation grid search to select the best hyperparameters, ensuring that the models were not overfitting or underfitting the data.\n\nFor the XGBoosting model, the optimization parameters included the maximum tree depth, regularization term (lambda), scale positive weight, learning rate, and number of estimators. The optimized values for these parameters were 6, 1.0, 13, 0.001, and 75, respectively. The scale positive weight parameter was particularly important for handling class imbalance in the dataset, which had a lower number of residents who experienced a fall.\n\nLogistic regression optimization parameters included the penalty term, class weight, optimization problem solver, and inverse of regularization strength. The logistic regression model used a standard scaler from scikit-learn to scale the inputs, which helped in preventing underfitting by ensuring that all features contributed equally to the model.\n\nThe multilayered perceptron model's optimization parameters included the maximum iteration, hidden layer size, and learning rate. The model incorporated a hidden layer of size 250, and the convergence of the solver iteration was determined either by reaching a maximum number of 100 iterations or by reaching a value of 1e-9 for the tolerance optimization parameter. This approach helped in ruling out underfitting by ensuring that the model had enough capacity to learn from the data.\n\nTo handle missing values, various imputation approaches were used. Missing measures of vital signs were imputed using forward and backward filling approaches. For all other features, the mean measurement of the features across all the training set data was used for imputation. This method ensured that the models were not overfitting to the training data by providing a reasonable estimate for missing values.\n\nThe performance of each model was assessed against the test dataset with respect to the receiver operating characteristic (ROC) curve, sensitivity, and specificity. The confidence intervals for these metrics were constructed using 1000 bootstrapped samples, providing a robust estimate of the models' performance and ruling out overfitting.\n\nIn summary, the study employed rigorous optimization techniques and imputation methods to ensure that the models were neither overfitting nor underfitting the data. The use of cross-validation, regularization, and careful selection of hyperparameters contributed to the reliability and generalizability of the models.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and improve the generalization of our models. For the XGBoosting model, we used the regularization term (lambda) as one of the hyperparameters to control the complexity of the model and prevent overfitting. The optimized value for lambda was found to be 1.0 through a grid search with cross-validation.\n\nFor the logistic regression model, we utilized L2 regularization, also known as ridge regularization, to penalize large coefficients and reduce the model's complexity. Additionally, we standardized the input features using a standard scaler, which helps in stabilizing the optimization process and improving the model's performance.\n\nThe multilayered perceptron model incorporated a hidden layer of size 250, and we used early stopping as a regularization technique. The convergence of the solver iteration was determined either by reaching a maximum number of 100 iterations or by achieving a tolerance optimization parameter value of 1e-9. This approach helped in preventing overfitting by stopping the training process when the performance on a validation set stopped improving.\n\nIn summary, we implemented regularization techniques such as L2 regularization, lambda regularization, and early stopping to mitigate overfitting and enhance the robustness of our models.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in detail. For the XGBoosting model, the optimized values for the parameters were 6 for maximum tree depth, 1.0 for the regularization term (lambda), 13 for scale positive weight, 0.001 for the learning rate, and 75 for the number of estimators. For logistic regression, the optimization parameters included the penalty term, class weight, optimization problem solver, and inverse of regularization strength. The multilayered perceptron model's optimization parameters included the maximum iteration, hidden layer size, and learning rate. The hidden layer size for the multilayered perceptron was set to 250, and the convergence of the solver iteration was determined by reaching a maximum of 100 iterations or a tolerance value of 1e-9.\n\nThe specific details of the hyper-parameter configurations and optimization parameters are provided within the text of the publication. However, model files and optimization schedules are not explicitly mentioned as being available. The publication does not specify the availability of these files or the license under which they might be shared. Therefore, while the hyper-parameter configurations and optimization parameters are reported, the model files and optimization schedules are not discussed in terms of availability or licensing.",
  "model/interpretability": "The model employed in this study is primarily based on the Extreme Gradient Boosting (XGBoost) algorithm, which is known for its robustness and performance in various classification problems. However, XGBoost is generally considered a black-box model, meaning that its internal workings and decision-making processes are not easily interpretable.\n\nTo address the interpretability of the model, we utilized Shapely Additive Explanations (SHAP) analysis. SHAP values provide a way to attribute the output of the model to the input features, making it possible to understand the impact of each feature on the model's predictions. The SHAP plots presented in the study show the features in order of importance, with the SHAP values quantifying the magnitude and direction of their impact on the model's predictions. This approach helps in understanding which features are most influential in predicting fall risks and how they contribute to the model's decisions.\n\nAdditionally, the study compared the XGBoost model with other models such as logistic regression and multilayered perceptron. Logistic regression, being a linear model, is more interpretable as it provides coefficients that indicate the direction and strength of the relationship between each feature and the outcome. However, it may not capture complex interactions between features as effectively as XGBoost.\n\nIn summary, while the primary model (XGBoost) is a black-box model, the use of SHAP analysis enhances its interpretability by providing insights into feature importance and their contributions to the model's predictions. This allows for a better understanding of the factors influencing fall risk predictions.",
  "model/output": "The model is a classification model designed to predict the likelihood of falls among residents within a specific time frame. It categorizes individuals into two groups: those who are likely to fall and those who are not. The primary output of the model is a prediction score that indicates the probability of a fall occurring within the next three months. This score is derived from various input features, including vital signs, demographic information, medical history, and other relevant data points. The model's performance is evaluated using metrics such as the area under the receiver operating characteristic (AUROC) curve, sensitivity, and specificity. These metrics help assess the model's ability to correctly identify individuals at risk of falling. Additionally, the model's feature importance is analyzed using SHAP (Shapely Additive Explanations) to understand which factors contribute most significantly to the prediction of falls. The model's outputs are compared with a standard fall risk assessment score to evaluate its effectiveness and reliability. The results indicate that the model outperforms the standard assessment in predicting falls, demonstrating its potential for improving fall risk management in care facilities.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the machine learning models involved several key steps and experiments to ensure robust and comprehensive assessment. The performance of each model was primarily evaluated using the receiver operating characteristic (ROC) curve, sensitivity, and specificity. Confidence intervals for these metrics were constructed using 1000 bootstrapped samples to provide a reliable estimate of the models' performance.\n\nSeveral exploratory analyses were conducted to examine the impact of different factors on model performance. First, the prediction window was reduced to two months to assess how this change affected the models' performance. The ROC curves for this shorter prediction window were analyzed, with the Extreme Gradient Boosting (XGBoost) model showing the highest area under the ROC curve (AUROC) of 0.753, compared to logistic regression (0.690) and multilayered perceptron (0.678).\n\nAdditionally, the training and testing datasets were separated based on facility type to evaluate the models' performance in different care settings. The XGBoost model demonstrated higher AUROCs for both skilled nursing facilities (0.716) and assisted living facilities (0.740) compared to other predictors.\n\nAnother experiment involved modifying the input features to understand their impact on model performance. When vital signs were removed, the XGBoost model still maintained the highest performance with an AUROC of 0.772. Similarly, using only demographic information and vital signs as input features, the XGBoost model achieved an AUROC of 0.765, indicating the importance of these features in fall prediction.\n\nSHAP (Shapely Additive Explanations) analysis was performed to evaluate feature importance, providing insights into which variables most significantly impacted the model predictions. This analysis helped in understanding the contribution of each feature to the model's performance.\n\nOverall, the evaluation method involved a combination of standard performance metrics, exploratory analyses, and feature importance assessment to ensure a thorough understanding of the models' strengths and limitations.",
  "evaluation/measure": "In our study, we evaluated the performance of our machine learning models using several key metrics to ensure a comprehensive assessment. The primary metric reported is the Area Under the Receiver Operating Characteristic Curve (AUROC), which provides a single scalar value that represents the ability of the model to distinguish between positive and negative classes across all possible classification thresholds. This metric is widely used in the literature and is particularly useful for imbalanced datasets, such as ours, where the number of fall incidents is relatively low.\n\nIn addition to AUROC, we also reported sensitivity and specificity at a selected operating point. Sensitivity, or recall, measures the proportion of actual positives that are correctly identified by the model, while specificity measures the proportion of actual negatives that are correctly identified. These metrics are crucial for understanding the model's performance in real-world scenarios, where the costs of false positives and false negatives may differ.\n\nWe also calculated the positive and negative likelihood ratios, which indicate how much the odds of the condition increase or decrease given a positive or negative test result, respectively. These ratios are useful for understanding the clinical significance of the model's predictions.\n\nFurthermore, we reported the diagnostic odds ratio, which is the ratio of the odds of the test being positive if the subject has the condition relative to the odds of the test being positive if the subject does not have the condition. This metric provides a single value that summarizes the overall performance of the model.\n\nTo provide a more nuanced view of the model's performance, we also reported the F1 score, which is the harmonic mean of precision and recall. This metric is particularly useful when the class distribution is imbalanced, as it takes into account both the precision and the recall of the model.\n\nThe confidence intervals for these metrics were constructed using 1000 bootstrapped samples, providing a measure of the uncertainty associated with our estimates. This set of metrics is representative of the literature and provides a comprehensive view of the model's performance across different aspects.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of our primary model, XGBoosting, with other machine learning algorithms to evaluate its performance. We included logistic regression and multilayered perceptron models for comparison, as they represent simpler baselines and are commonly used in similar predictive tasks.\n\nThe logistic regression model is a fundamental statistical method for binary classification problems, providing a straightforward approach to understand the relationship between input features and the outcome. The multilayered perceptron, a type of feed-forward neural network, offers a more complex architecture but is still considered a baseline in many machine learning applications due to its widespread use and interpretability.\n\nTo ensure a fair comparison, all models were trained and tested using the same dataset and input features. We used a standard train-test split with stratified sampling to handle the class imbalance in our data. Hyperparameter optimization was performed using a 5-fold cross-validation grid search, focusing on metrics such as the area under the receiver operating characteristic (AUROC) curve.\n\nAdditionally, we compared our models against an internal fall risk assessment conducted by Juniper Communities staff. This assessment served as a comparator, providing a real-world benchmark for evaluating the practical utility of our machine learning models. The comparator scores were based on various clinical and demographic factors, offering a comprehensive baseline for performance evaluation.\n\nIn summary, our evaluation included comparisons with simpler baselines and publicly available methods, ensuring a robust assessment of our XGBoosting model's performance. This approach allowed us to demonstrate the superiority of our model in predicting fall risks within the studied population.",
  "evaluation/confidence": "The performance metrics presented in our study include confidence intervals, which were constructed using 1000 bootstrapped samples. This approach provides a measure of the reliability and variability of our estimates. For instance, the area under the receiver operating characteristic curve (AUROC) for the Extreme Gradient Boosting (XGBoost) model is reported with a 95% confidence interval, indicating the range within which the true AUROC is likely to fall.\n\nTo assess the statistical significance of our results, we employed various statistical tests. Group differences were calculated using an exact binomial test for noncontinuous variables and a 2-tailed Welch t test for continuous variables. These tests help to determine whether the observed differences in performance metrics between models are statistically significant, thereby supporting the claim that one method may be superior to others.\n\nAdditionally, the confidence intervals for sensitivity, specificity, and other performance metrics were also provided. These intervals help to understand the precision of our estimates and to make informed comparisons between different models. The use of bootstrapping ensures that our confidence intervals are robust and account for the variability in the data.\n\nIn summary, the inclusion of confidence intervals and the use of appropriate statistical tests strengthen the confidence in our evaluation results, allowing us to make reliable claims about the superiority of our method compared to other models and baselines.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study does not provide direct access to the specific datasets or evaluation files used in the research. However, supplementary materials, including additional tables and figures, are available in the Multimedia Appendix 1. This appendix contains detailed information about the models' performance metrics, feature importance, and other relevant data that support the findings presented in the main text. For those interested in replicating or building upon the study, the supplementary materials offer valuable insights into the methodologies and results."
}