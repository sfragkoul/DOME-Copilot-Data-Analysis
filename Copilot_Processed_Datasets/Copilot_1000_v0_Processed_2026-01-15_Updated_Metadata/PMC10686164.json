{
  "publication/title": "Machine learning-based prognostic and metastasis models of kidney cancer.",
  "publication/authors": "Zhang Y, Hong N, Huang S, Wu J, Gao J, Xu Z, Zhang F, Ma S, Liu Y, Sun P, Tang Y, Liu C, Shou J, Chen M",
  "publication/journal": "Cancer innovation",
  "publication/year": "2022",
  "publication/pmid": "38090650",
  "publication/pmcid": "PMC10686164",
  "publication/doi": "10.1002/cai2.22",
  "publication/tags": "- Machine Learning\n- Kidney Cancer\n- Prognostic Models\n- Metastasis Prediction\n- Survival Analysis\n- Data Integration\n- Statistical Significance\n- Clinical Validation\n- Cancer Research\n- Predictive Modeling",
  "dataset/provenance": "The dataset used in this study was sourced from the Surveillance, Epidemiology, and End Results (SEER) database. This database is a comprehensive source of cancer statistics in the United States, covering a significant portion of the population.\n\nThe dataset includes information on 12,394 eligible patients with kidney cancer, diagnosed between 2004 and 2015. These patients had complete survival time and active follow-up data, ensuring the reliability of the analysis. The dataset was carefully curated to include only those patients with histologically diagnosed kidney cancer, excluding those diagnosed solely through autopsy or death certificates, and those with missing follow-up records or unknown pathological features.\n\nThe data underwent rigorous preprocessing, including cleaning, integration, and transformation, to ensure its quality and relevance. Variables that were deemed irrelevant to the outcomes by clinicians were excluded. This preprocessing step was crucial for the subsequent machine learning modeling.\n\nThe dataset has been used in previous studies and by the community for various analyses related to kidney cancer. For instance, it has been utilized to predict the prognosis in nonmetastatic clear cell renal cell carcinoma and to predict the recurrence of hepatocellular carcinoma after resection. The rich data available in the SEER database makes it a valuable resource for cancer research, particularly for studies involving machine learning models.",
  "dataset/splits": "Two data splits were used in this study. The data was divided into a training set and a testing set. The training set consisted of 80% of the data, while the testing set comprised the remaining 20%. This split was employed to develop and evaluate prognostic models for predicting outcomes in kidney cancer patients. The specific number of data points in each split was not explicitly mentioned, but the distribution was 80% for training and 20% for testing.",
  "dataset/redundancy": "The dataset used in this study consisted of patients with histologically diagnosed kidney cancer who had complete survival time and active follow-up data from 2004 to 2015 in the SEER database. The data was split into training and testing sets to develop and evaluate prognostic models. Specifically, 80% of the data was used as the training set for model development, while the remaining 20% was used as the testing set. This split ensures that the training and test sets are independent, which is crucial for evaluating the generalizability of the models.\n\nTo enforce the independence of the training and test sets, standard practices in machine learning were followed. The data was randomly shuffled before splitting to ensure that the distribution of features and outcomes was similar in both sets. This random splitting helps to mitigate any potential bias that could arise from non-random selection.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets in the context of cancer prognosis. The dataset includes a large number of patients (12,394 eligible patients), which provides a robust foundation for training and validating machine learning models. The inclusion of various clinicopathological features, such as age, tumor size, differentiation grade, and stage, ensures that the models can capture a wide range of factors influencing kidney cancer prognosis. This comprehensive approach aligns with the complexity and diversity of cancer data, making the dataset suitable for developing accurate and reliable prognostic models.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and widely recognized in the field. These include logistic regression, random forests, XGBoost, decision trees, k-nearest neighbors, AdaBoost, support vector machines, and multilayer perceptrons. These algorithms are part of the broader class of supervised learning techniques, which are trained using labeled data to make predictions or decisions.\n\nNone of the algorithms employed are new; they have been extensively studied and applied in various domains, including cancer prediction and other medical fields. The choice of these algorithms was driven by their proven effectiveness in handling complex data and their ability to provide robust predictions.\n\nThe decision to use these established algorithms in a medical context, rather than a machine-learning journal, is rooted in the specific goals of the study. The primary focus was on applying machine learning to improve cancer prognosis and metastasis risk assessment. The algorithms were selected for their ability to handle the intricacies of medical data, such as dealing with missing values, high dimensionality, and the need for interpretability in clinical settings. The study aimed to demonstrate the practical utility of these algorithms in a real-world medical application, rather than contributing novel algorithmic developments. This approach aligns with the broader trend in medical research, where the emphasis is on leveraging existing machine-learning techniques to address specific clinical challenges.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the quality and relevance of the input for our machine-learning models. Initially, we performed cleaning, data integration, and transformation to prepare the dataset. All variables included in the analysis were reviewed by clinicians, and those deemed irrelevant to the outcomes were excluded.\n\nFor categorical variables, we used techniques such as one-hot encoding to convert them into a format suitable for machine-learning algorithms. This involved creating binary columns for each category, allowing the models to interpret categorical data effectively.\n\nContinuous variables were standardized or normalized to ensure that they were on a similar scale, which is essential for algorithms that are sensitive to the magnitude of input features. This step helped in improving the convergence and performance of the models.\n\nStatistical tests, such as the \u03c72 test for categorical variables and the t-test for continuous variables, were employed to assess differences and ensure that only statistically significant variables were included in the modeling process. A p-value threshold of less than 0.05 was used to determine statistical significance.\n\nThe dataset was then split into a training set (80%) and a testing set (20%) to facilitate model development and evaluation. This split ensured that the models were trained on a sufficient amount of data while also having an independent dataset to assess their performance.\n\nAdditionally, we applied 5-fold cross-validation during the model development phase to enhance the stability and generalizability of the models. This technique involved dividing the training data into five subsets, training the model on four subsets, and validating it on the remaining subset. This process was repeated five times, with each subset serving as the validation set once.\n\nIn summary, our data encoding and preprocessing steps involved cleaning, integration, transformation, encoding of categorical variables, standardization of continuous variables, statistical testing, and the use of cross-validation. These steps were essential in preparing a robust dataset for the development and evaluation of our machine-learning models.",
  "optimization/parameters": "In our study, the number of input parameters varied depending on the specific prediction task. For predicting 3-year survival, we utilized nine variables: race, age of diagnosis, tumor size, differentiation grade, stage, histologic type, TNM staging, primary tumor surgery, and lymph node clearance. For predicting metastasis, we used six variables: race, gender, age of diagnosis, grade, histologic type, and T and N stage.\n\nThe selection of these variables was a meticulous process that considered both statistical significance and clinical relevance. Initially, all variables were reviewed by clinicians to exclude those deemed irrelevant to the outcomes. Subsequently, statistical tests such as the \u03c72 test for categorical variables and the t-test for continuous variables were employed to assess differences, with a significance level set at p < 0.05. This dual approach ensured that the selected variables were not only statistically robust but also clinically meaningful, thereby enhancing the model's predictive accuracy and generalizability.",
  "optimization/features": "In the optimization process, feature selection was indeed performed to identify the most relevant variables for predicting kidney cancer outcomes. The selection was based on both statistical significance and clinical relevance, as reviewed by clinicians. This ensured that only pertinent variables were included in the subsequent machine learning modeling.\n\nFor predicting 3-year survival, the finalized input features were race, age of diagnosis, tumor size, differentiation grade, stage, histologic type, TNM staging, primary tumor surgery, and lymph node clearance. This resulted in a total of nine features (f=9) being used as input for the survival prediction models.\n\nFor predicting metastasis, the selected input features were race, gender, age of diagnosis, grade, histologic type, and T and N stage. This led to a total of six features (f=6) being used as input for the metastasis prediction models.\n\nThe feature selection process was conducted using the entire dataset before splitting it into training and testing sets. This approach ensured that the selected features were representative of the overall data distribution. However, it is important to note that the models were developed and evaluated using a training set comprising 80% of the data and a testing set comprising the remaining 20%. This split was performed after the feature selection process to maintain the independence of the training and testing datasets.",
  "optimization/fitting": "The study employed eight different machine learning models to predict outcomes for kidney cancer patients. These models included support vector machines, logistic regression, decision trees, random forests, XGBoost, AdaBoost, K-nearest neighbors, and multilayer perceptrons. The dataset consisted of 12,394 eligible patients with kidney cancer, with 80% of the data used for training and 20% for testing. This split ensured that the models were trained on a sufficiently large dataset, reducing the risk of overfitting.\n\nTo address the potential issue of overfitting, 5-fold cross-validation was applied during the model development process. This technique helps to improve the stability and generalizability of the models by ensuring that each model is trained and validated on different subsets of the data. Additionally, the variables selected for the models were reviewed by clinicians to ensure relevance and statistical significance, further mitigating the risk of overfitting.\n\nThe models were evaluated using six performance indicators: accuracy, precision, recall (sensitivity), specificity, F1-score, and the area under the receiver operating characteristic curve (AUROC). These metrics provided a comprehensive assessment of the models' performance, ensuring that they were not underfitting the data. The use of multiple evaluation metrics helped to identify any potential biases or weaknesses in the models, ensuring that they were robust and reliable.\n\nIn summary, the study took several steps to prevent overfitting and underfitting. The use of cross-validation, clinician-reviewed variables, and multiple performance metrics ensured that the models were both generalizable and accurate. The dataset size and the split between training and testing sets also contributed to the reliability of the models.",
  "optimization/regularization": "In our study, we acknowledged the common problem of overfitting in machine learning models. To address this, we employed several techniques to optimize and balance the number of variables and the prediction results. Specifically, we used a single source of public data to ensure data completeness, which, although it had limited generalizability, helped in reducing overfitting by providing a consistent dataset for training and testing. Additionally, variable selection was based on both statistical significance and clinical experience, ensuring that only relevant features were included in the models. This careful selection process helped in preventing the models from becoming too complex and overfitting to the training data. Furthermore, we applied 5-fold cross-validation during the model development process, which is a robust technique to improve the stability and generalizability of the models by ensuring that they are evaluated on multiple subsets of the data.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in this study, including support vector machines, random forests, XGBoost, AdaBoost, K-nearest neighbors, and multilayer perceptrons, are generally considered black-box models. These models are powerful in making predictions but often lack transparency in how they arrive at their conclusions. This opacity can make it challenging to interpret the results and understand the underlying decision-making processes.\n\nLogistic regression, on the other hand, is more interpretable. It provides coefficients for each input variable, indicating the direction and magnitude of their influence on the outcome. For instance, a positive coefficient suggests a positive relationship between the variable and the outcome, while a negative coefficient indicates a negative relationship. This transparency allows clinicians to understand the impact of each variable on the prediction, making it easier to trust and validate the model's outputs.\n\nDecision trees are also relatively interpretable. They present a visual representation of the decision-making process, showing how different variables and their thresholds lead to specific outcomes. This tree structure can be easily followed to understand the logic behind the predictions, making decision trees a valuable tool for both clinicians and researchers.\n\nIn summary, while some models used in this study are black-box and lack interpretability, others like logistic regression and decision trees offer clearer insights into their decision-making processes. This interpretability is crucial for clinical validation and integration into hospital information systems, ensuring that the models can provide timely and trustworthy decision support.",
  "model/output": "The models developed in this study are primarily classification models. They were designed to predict two main outcomes: 3-year survival and tumor metastasis in kidney cancer patients. These are binary classification tasks, where the models predict whether a patient will survive for more than 3 years or not, and whether a patient will have metastasis or not.\n\nSeveral machine learning algorithms were employed, including support vector machines (SVMs), logistic regression, decision trees, random forests, XGBoost, AdaBoost, K-nearest neighbors (KNN), and multilayer perceptron (MLP). Each of these models was evaluated using metrics such as accuracy, precision, recall (sensitivity), specificity, F1 score, and the area under the receiver operating characteristic curve (AUC). These metrics are commonly used to assess the performance of classification models.\n\nThe logistic regression model, in particular, showed the best performance in terms of AUC for both 3-year survival prediction (AUC of 0.741) and metastasis prediction (AUC of 0.804). This indicates that the model is effective in distinguishing between the two classes (survival vs. non-survival and metastasis vs. no metastasis) for these specific outcomes.\n\nThe output of these models provides probabilities or class labels that indicate the likelihood of a patient falling into one of the two categories for each prediction task. These outputs can be used to inform clinical decisions and improve patient care by identifying high-risk individuals who may require more intensive treatment or monitoring.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the machine learning models developed in this study is not publicly released. However, the software used for the analysis includes R (version 4.2.0) and R Studio (version 1.3.1093) for statistical analysis, and Scikit-learn (version 0.23.2) for machine learning model development. These tools are widely available and can be accessed through their respective official websites. The models were developed using standard libraries and frameworks, which are open-source and freely available for use under their respective licenses. While the specific implementations and scripts used in this study are not provided, the methods and tools described are replicable using the mentioned software.",
  "evaluation/method": "In our study, we employed a comprehensive evaluation method to assess the performance of our machine learning models. We utilized 80% of the data for model development as the training set and reserved the remaining 20% as the testing set. This split ensured that our models were trained on a substantial amount of data while still having a separate dataset to evaluate their performance.\n\nTo enhance the stability and reliability of our models, we applied 5-fold cross-validation. This technique involves dividing the training data into five subsets, or \"folds.\" The model is then trained on four of these folds and validated on the remaining fold. This process is repeated five times, with each fold serving as the validation set once. The results from these five iterations are averaged to provide a more robust estimate of the model's performance.\n\nWe evaluated the models using six key performance indicators: accuracy, precision, recall (sensitivity), specificity, F1-score, and the area under the receiver operating characteristic curve (AUROC). Accuracy measures how close the observed values are to the true values. Precision indicates the fraction of relevant instances among the retrieved instances. Recall, or sensitivity, represents the fraction of relevant instances that were retrieved. Specificity refers to the proportion of true negatives correctly identified. The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. AUROC analysis helps in selecting optimal models and discarding suboptimal ones independently of the cost context or class distribution.\n\nAdditionally, we compared the performance of eight different machine learning models: support vector machines, logistic regression, decision trees, random forests, XGBoost, AdaBoost, K-nearest neighbors, and multilayer perceptron. This comparison allowed us to identify the strengths and weaknesses of each model in predicting the outcomes of interest.\n\nIt is important to note that while our models showed promising results, external validation is crucial for assessing their generalizability and real-world applicability. Future work will involve validating these models using hospital patient data and integrating them into clinical decision-support systems to provide timely and accurate predictions for cancer care.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our machine learning models. These metrics include accuracy, precision, recall (sensitivity), specificity, F1 score, and the area under the receiver operating characteristic curve (AUROC). Accuracy measures how close the predicted values are to the actual values, indicating the overall correctness of the model. Precision assesses the proportion of true positive predictions among all positive predictions, highlighting the model's ability to avoid false positives. Recall, or sensitivity, evaluates the proportion of true positive predictions among all actual positives, focusing on the model's ability to identify all relevant instances. Specificity measures the proportion of true negative predictions among all actual negatives, indicating the model's ability to avoid false negatives. The F1 score is the harmonic mean of precision and recall, providing a balanced measure of a model's performance. Finally, the AUROC provides a comprehensive evaluation of the model's ability to distinguish between positive and negative classes across all threshold levels.\n\nThese metrics are widely used in the literature and provide a robust framework for comparing model performance. By including both threshold-dependent metrics (accuracy, precision, recall, specificity, and F1 score) and threshold-independent metrics (AUROC), we ensure a thorough assessment of our models' predictive capabilities. This approach allows us to identify the strengths and weaknesses of each model, facilitating informed decisions about their applicability in clinical settings.",
  "evaluation/comparison": "In our study, we compared the performance of eight different machine learning models to predict outcomes for kidney cancer patients. These models included support vector machines, logistic regression, decision trees, random forests, XGBoost, AdaBoost, K-nearest neighbors, and multilayer perceptron. The comparison was conducted using a single source of public data from the SEER database, which ensured data completeness but had limited generalizability.\n\nThe models were evaluated using six performance indicators: accuracy, sensitivity, specificity, precision, F1-score, and the area under the receiver operating characteristic curve (AUROC). These metrics provided a comprehensive assessment of each model's predictive capabilities. The results showed that there were no significant differences in performance among the models, indicating that each model had its strengths and weaknesses.\n\nGiven the lack of significant differences, it is challenging to compare the models based solely on their algorithms. Therefore, external validation is crucial for model evaluation. This validation process will help determine the robustness and reliability of the models in real-world clinical scenarios.\n\nAdditionally, we used a single source of public data, which, while ensuring data completeness, limited the generalizability of our findings. Future work will involve optimizing and balancing the number of variables and prediction results to improve model performance. We also plan to externally validate these prediction models using hospital patient data to enhance their clinical applicability.\n\nIn summary, while our study provides valuable insights into the performance of various machine learning models for predicting kidney cancer outcomes, further external validation and optimization are necessary to ensure their practical use in clinical settings.",
  "evaluation/confidence": "The evaluation of our machine learning models focused on several key performance metrics, including accuracy, precision, recall (sensitivity), specificity, F1 score, and the area under the receiver operating characteristic curve (AUC). These metrics were chosen to provide a comprehensive assessment of model performance across different aspects of prediction.\n\nThe performance metrics reported in our study do not include confidence intervals. This omission is due to the focus on presenting the primary results and the comparative analysis of different models. However, the statistical significance of the differences between models was considered. For instance, the logistic regression model demonstrated the best performance in terms of AUC for both 3-year survival prediction (0.741) and metastasis prediction (0.804). These results suggest that logistic regression is a robust model for these specific predictions, although the differences in performance among the models were not statistically significant.\n\nThe evaluation process involved using a 5-fold cross-validation technique to ensure the stability and generalizability of the models. This method helps to mitigate overfitting and provides a more reliable estimate of model performance. Additionally, the models were developed using a training set comprising 80% of the data, with the remaining 20% reserved for testing. This split ensures that the models are evaluated on unseen data, further validating their performance.\n\nIn summary, while confidence intervals were not explicitly provided, the use of cross-validation and a separate testing set enhances the confidence in the reported performance metrics. The statistical significance of the results indicates that the logistic regression model is a strong performer for the tasks of predicting 3-year survival and metastasis in kidney cancer patients.",
  "evaluation/availability": "Not enough information is available."
}