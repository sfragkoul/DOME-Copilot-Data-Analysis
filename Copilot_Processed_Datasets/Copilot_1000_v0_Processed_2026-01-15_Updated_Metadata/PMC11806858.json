{
  "publication/title": "Predicting cardiovascular outcomes in Chinese patients with type 2 diabetes by combining risk factor trajectories and machine learning algorithm: a cohort study.",
  "publication/authors": "Huang Q, Zou X, Lian Z, Zhou X, Han X, Luo Y, Chen S, Wang Y, Wu S, Ji L",
  "publication/journal": "Cardiovascular diabetology",
  "publication/year": "2025",
  "publication/pmid": "39920715",
  "publication/pmcid": "PMC11806858",
  "publication/doi": "10.1186/s12933-025-02611-0",
  "publication/tags": "- Cardiovascular Disease\n- Type 2 Diabetes Mellitus\n- Machine Learning\n- Predictive Modeling\n- Risk Stratification\n- XGBoost Algorithm\n- Longitudinal Data Analysis\n- Functional Principal Components\n- Cardiovascular Risk Scores\n- Chinese Population",
  "dataset/provenance": "The dataset used in this study was derived from a cohort of participants enrolled in a longitudinal study. The final cohort comprised 16,378 participants. These individuals were followed over a mean period of 8.5 years, during which incident cardiovascular events were recorded. The primary outcome measured was the first occurrence of major adverse cardiovascular events (MACE), including nonfatal myocardial infarction, nonfatal stroke, or cardiovascular death. Outcome data were sourced from biennial questionnaires, municipal social insurance records, the Kailuan group social security system, and Tangshan medical insurance systems, with identification based on ICD-10 codes.\n\nThe dataset included a range of baseline features and longitudinal data on risk factor trajectories collected over a 4-year observational window following cohort enrollment. Participants who experienced cardiovascular events or were censored during the first four years were excluded to avoid immortal bias. Additionally, individuals under 40 or over 79 years of age, or those with missing data on age, sex, or glucose levels, were also excluded from the analysis.\n\nThe dataset has not been used in previous papers by the community.",
  "dataset/splits": "The dataset was divided into two main cohorts: a derivation cohort and a testing cohort. The split ratio was 8:2, meaning 80% of the data was used for derivation (training) and 20% for testing. Specifically, there were 13,102 participants in the training set and 3,276 participants in the testing set. This division was stratified by the occurrence of major adverse cardiovascular events (MACE) to ensure a balanced representation of outcomes in both cohorts.",
  "dataset/redundancy": "The dataset was divided into derivation and testing cohorts in an 8:2 ratio. This split was stratified by the occurrence of major adverse cardiovascular events (MACE) to ensure that both cohorts were representative of the overall population and that the testing set remained independent of the training set.\n\nTo enforce independence between the training and testing sets, the dataset was randomly divided while maintaining the stratification by MACE events. This approach helps to ensure that the model's performance can be generalized to new, unseen data.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in cardiovascular research. The mean age of participants was 55.3 years, with 4.2% having a history of cardiovascular disease at baseline. Over a mean follow-up of 8.5 years, 13.2% of participants experienced the primary outcome, which is consistent with the incidence rates reported in other large-scale cardiovascular studies. This similarity in distribution suggests that the dataset is robust and representative of the broader population, enhancing the reliability and generalizability of the findings.",
  "dataset/availability": "The data used in our study is not publicly available. However, it can be made available upon request to the corresponding authors. This approach ensures that the data is shared responsibly and in accordance with ethical guidelines and participant consent. The data includes detailed information on the participants, their baseline characteristics, and longitudinal data on various risk factors. The dataset was divided into derivation and testing cohorts in an 8:2 ratio, stratified by the occurrence of major adverse cardiovascular events (MACE). This split was enforced to ensure that the model's performance could be rigorously evaluated on an independent dataset. The data collection and management processes adhered to strict protocols to maintain data integrity and confidentiality.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the XGBoost algorithm, which is a type of gradient boosting framework. This algorithm is not new; it has been widely used and recognized in the machine learning community for its efficiency and effectiveness in handling structured/tabular data.\n\nThe choice of XGBoost was driven by its ability to handle complex interactions and non-linear relationships in the data, which is crucial for predicting cardiovascular disease outcomes. The algorithm's implementation of the Cox negative log-likelihood as the loss function allowed it to effectively model the time-to-event data, making it suitable for our survival analysis.\n\nWhile XGBoost is a well-established algorithm, its application in our study is novel in the context of cardiovascular risk prediction for individuals with type 2 diabetes in the Chinese population. The integration of trajectory information and the specific tuning of hyperparameters for our dataset contributed to the unique performance and insights gained from the model.\n\nThe decision to publish this work in a cardiovascular research journal rather than a machine-learning journal was based on the primary focus of our study. Our aim was to advance the field of cardiovascular risk prediction and management, particularly for individuals with type 2 diabetes. The machine-learning aspects of the study were secondary to this primary goal, serving as a means to achieve more accurate and personalized risk assessments.",
  "optimization/meta": "The model developed in this study, named ML-CVD-C, is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it is a standalone machine learning model built using the XGBoost algorithm. The XGBoost algorithm was chosen for its ability to handle complex interactions and non-linear data, which is crucial for predicting cardiovascular outcomes.\n\nThe model was developed using a dataset that was randomly divided into derivation and testing cohorts in an 8:2 ratio, stratified by the occurrence of major adverse cardiovascular events (MACE). This ensures that the training data is independent of the testing data, which is essential for evaluating the model's performance accurately.\n\nPredictors with more than 20% missing data, high collinearity, or low variance were excluded from the model. Missing baseline data were imputed using a random forest-based approach, which outperformed other imputation methods in handling non-linear data with complex interactions. Variables not normally distributed were log-transformed, and all continuous variables were normalized. Outliers beyond six standard deviations were removed to ensure the robustness of the model.\n\nThe model's performance was evaluated using various metrics, including the C-index, time-dependent area under the receiver operating characteristic curve (AUROC), calibration plots, the expected\u2013observed ratio, and Hosmer\u2013Lemeshow \u03c72 tests. Feature importance within the model was quantified using SHapley Additive exPlanations (SHAP) values, which provided insights into the most influential predictors.\n\nIn summary, the ML-CVD-C model is a standalone machine learning model that does not rely on other machine-learning algorithms as input. The training data is independent of the testing data, ensuring an unbiased evaluation of the model's performance.",
  "optimization/encoding": "For the machine-learning algorithm, the data underwent several preprocessing steps to ensure optimal performance. Predictors with more than 20% missing data, high collinearity, or low variance were excluded to maintain data quality. Missing baseline data were imputed using a random forest-based approach, which was found to be superior in handling non-linear data with complex interactions compared to other methods like k-nearest neighbor and chained equations.\n\nVariables that were not normally distributed were log-transformed to achieve a more normal distribution. All continuous variables were then normalized to ensure they were on a similar scale, which is crucial for many machine learning algorithms. Outliers beyond six standard deviations were removed to prevent them from disproportionately influencing the model.\n\nCategorical features were binary-encoded, while continuous features were scaled and centered around the mean. This standardization process helps in improving the convergence speed and performance of the machine learning model. Each data observation was then prepared for analysis, with the x-axis representing the deviation of feature values from the mean and the y-axis depicting the SHAP value for each feature within the training dataset. This encoding and preprocessing pipeline ensured that the data was in an optimal state for the development and evaluation of the ML-CVD-C model.",
  "optimization/parameters": "In the development of the ML-CVD-C model, several input parameters were considered and refined through a systematic process. Initially, a comprehensive set of predictors was evaluated, including both baseline features and trajectory information derived from longitudinal data. Predictors with more than 20% missing data, high collinearity, or low variance were excluded to ensure the robustness of the model.\n\nThe final set of input parameters included both categorical and continuous variables. Categorical features, such as sex, were binary-encoded, while continuous features, like age and various biochemical markers, were scaled and centered around the mean. The continuous variables were also log-transformed if they were not normally distributed, and outliers beyond six standard deviations were removed.\n\nThe selection of input parameters was further refined using functional principal component (FPC) analysis, which captured the long-term average levels and trend changes of variables over time. For instance, the first FPC score (FPC1) indicated the long-term average level of a variable, while the third FPC score (FPC3) represented fluctuations or trend changes. This approach allowed for the incorporation of dynamic changes in risk factors, enhancing the model's predictive performance.\n\nThe final model utilized an XGBoost algorithm, which is known for its ability to handle complex interactions and non-linear relationships. The number of input parameters (p) was determined through a combination of domain knowledge, statistical analysis, and model performance evaluation. The selected parameters were those that demonstrated significant contributions to the predictive accuracy of the model, as quantified by SHapley Additive exPlanations (SHAP) values.\n\nIn summary, the input parameters for the ML-CVD-C model were carefully selected and refined through a multi-step process that included data preprocessing, FPC analysis, and model evaluation. This approach ensured that the model incorporated relevant and informative predictors, leading to improved discrimination and calibration for cardiovascular risk assessment.",
  "optimization/features": "In the optimization process of our model, we utilized a total of 12 input features. These features included both baseline characteristics and trajectory information derived from longitudinal data. The selected features were age, sex, and various functional principal component (FPC) scores representing different aspects of several clinical variables.\n\nFeature selection was indeed performed to ensure that only the most relevant predictors were included in the model. This process involved excluding variables with more than 20% missing data, high collinearity, or low variance. The selection was conducted using the training set only, ensuring that the testing set remained unbiased and could be used for a true evaluation of the model's performance.\n\nThe FPC scores, which capture the long-term average levels and trend changes of variables, were particularly important in our model. For instance, the first FPC score (FPC1) indicates the long-term average level of a variable, while the third FPC score (FPC3) represents fluctuations or trend changes. This approach allowed us to incorporate dynamic information into our predictions, enhancing the model's ability to assess cardiovascular risk over time.",
  "optimization/fitting": "The machine learning model developed, named ML-CVD-C, utilized an XGBoost algorithm with the Cox negative log-likelihood as the loss function. This approach was chosen for its ability to handle complex interactions and non-linear relationships within the data.\n\nTo address the potential issue of overfitting, given the number of parameters in the model, several strategies were employed. Firstly, the dataset was divided into derivation and testing cohorts in an 8:2 ratio, stratified by the occurrence of major adverse cardiovascular events (MACE). This ensured that the model's performance could be evaluated on an independent test set. Secondly, 5-fold cross-validation was performed within the derivation cohort to optimize the hyperparameters. This technique helps in selecting the best model parameters that generalize well to unseen data, thereby reducing the risk of overfitting. Additionally, the model's performance was assessed using metrics such as the C-index and time-dependent area under the receiver operating characteristic curve (AUROC) in both the training and testing datasets, ensuring that the model's predictive accuracy was consistent across different data splits.\n\nTo mitigate underfitting, predictors with more than 20% missing data, high collinearity, or low variance were excluded from the model. Missing baseline data were imputed using a random forest-based approach, which outperformed other imputation methods in handling non-linear data with complex interactions. Variables not normally distributed were log-transformed, and all continuous variables were normalized. Outliers beyond six standard deviations were removed to ensure that the model was not influenced by extreme values. These preprocessing steps helped in creating a robust dataset that could effectively train the model without underfitting.\n\nFurthermore, the model's performance was compared to traditional statistical approaches, such as a Cox regression model using the same variables. This comparison allowed for a direct evaluation of the machine learning techniques' effectiveness in capturing the underlying patterns in the data. The sensitivity analysis, which involved adjusting the trajectory observation window, also provided insights into how the model's predictive performance varied with different observation periods, further validating its robustness.",
  "optimization/regularization": "In the development of our machine learning model, several regularization techniques were employed to prevent overfitting and ensure robust performance. One key method involved the exclusion of predictors with more than 20% missing data, high collinearity, or low variance. This step helped in retaining only the most relevant and reliable features for model training.\n\nAdditionally, missing baseline data were imputed using a random forest-based approach, which is known for handling non-linear data with complex interactions effectively. This method outperformed other imputation techniques like k-nearest neighbor and chained equations, ensuring that the imputed data did not introduce bias or noise into the model.\n\nOutliers beyond six standard deviations were removed to further enhance the model's stability and prevent the influence of extreme values. Continuous variables were log-transformed and normalized to ensure a consistent scale, which is crucial for the performance of machine learning algorithms.\n\nThe XGBoost algorithm, utilized for model development, inherently includes regularization parameters such as L1 and L2 regularization. These parameters help in controlling the complexity of the model and preventing overfitting by penalizing large coefficients.\n\nFurthermore, 5-fold cross-validation was performed within the derivation cohort to optimize hyperparameters. This technique ensures that the model generalizes well to unseen data by evaluating its performance across multiple subsets of the training data.\n\nOverall, these regularization techniques collectively contributed to the development of a robust and reliable machine learning model for predicting cardiovascular disease in the Chinese population.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule for the ML-CVD-C model are reported in supplementary materials. Specifically, the hyperparameters were optimized using 5-fold cross-validation within the derivation cohort, and the details can be found in Table S2. The model was developed using an XGBoost algorithm with the Cox negative log-likelihood as the loss function.\n\nThe model files and optimization parameters are not explicitly mentioned as being available for download or use. However, the statistical software and packages used for the analysis are specified. The \u2018MissForest\u2019 package was used for baseline data imputation, the \u2018fdapace\u2019 and \u2018longitudinalData\u2019 packages were employed for FPC analysis of the longitudinal data, and the \u2018XGBoost\u2019 package was utilized to build the XGBoost model. All statistical analyses were conducted using R software, version 4.2.2.\n\nRegarding the availability and license of the configurations and parameters, there is no explicit mention of a specific license under which these materials are provided. Typically, supplementary materials in scientific publications are intended for reference and replication purposes, but the specific terms of use would need to be clarified with the authors or the publishing journal.",
  "model/interpretability": "The ML-CVD-C model, while leveraging advanced machine learning techniques, is not entirely a black box. To enhance interpretability, we employed SHapley Additive exPlanations (SHAP) values. SHAP values provide a way to attribute the output of the model to the input features, making it possible to understand the contribution of each feature to the final prediction.\n\nFor instance, age and sex were identified as the most influential baseline factors. Additionally, trajectory risk factors such as the first principal component (FPC1) of fasting plasma glucose and systolic blood pressure, as well as the third principal component (FPC3) of alanine transaminase, were significant contributors. These components capture sustained exposure and fluctuations in the variables, respectively.\n\nThe dependency plots generated using SHAP values visually represent how each feature's value deviation from the mean affects the model's output. This allows clinicians and researchers to understand which factors are driving the predictions and to what extent. For example, the plots show that higher deviations in age and certain FPC scores are associated with higher SHAP values, indicating a stronger influence on the predicted cardiovascular risk.\n\nBy using SHAP values, we can transparently communicate the model's decision-making process, which is crucial for gaining trust and acceptance in clinical settings. This approach ensures that the model's predictions are not only accurate but also interpretable, aiding in better clinical decision-making and patient management.",
  "model/output": "The model developed in this study is a regression model. Specifically, it is designed to predict the probability of developing cardiovascular disease within the next 10 years, which is a continuous outcome ranging from 0 to 1. The model, named ML-CVD-C, utilizes an XGBoost algorithm with the Cox negative log-likelihood as the loss function. This approach is suitable for survival analysis, where the goal is to predict the time to an event (in this case, the occurrence of cardiovascular disease) based on various risk factors.\n\nThe model's performance was evaluated using metrics such as the C-index and the time-dependent area under the receiver operating characteristic curve (AUROC), which are commonly used in regression models to assess predictive accuracy. Additionally, the model's calibration was evaluated using calibration plots, the expected\u2013observed ratio, and Hosmer\u2013Lemeshow \u03c72 tests. These evaluations help ensure that the predicted probabilities align well with the observed outcomes.\n\nThe ML-CVD-C score, derived from this model, stratifies participants into different risk categories (low, medium, high, and very high) based on their predicted 10-year cardiovascular risk. This stratification allows for targeted interventions and monitoring of individuals at higher risk. The model's ability to incorporate trajectory information and machine learning techniques enhances its predictive performance compared to traditional models that rely solely on baseline data.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the ML-CVD-C model involved several rigorous methods to ensure its robustness and accuracy. Initially, the dataset was split into derivation and testing cohorts in an 8:2 ratio, stratified by the occurrence of major adverse cardiovascular events (MACE). This stratification helped in maintaining a balanced representation of outcomes in both cohorts.\n\nTo optimize the model's hyperparameters, 5-fold cross-validation was performed within the derivation cohort. This technique involved dividing the data into five subsets, training the model on four subsets, and validating it on the remaining subset. This process was repeated five times, with each subset serving as the validation set once. The performance metrics from these iterations were averaged to provide a reliable estimate of the model's performance.\n\nThe model's discrimination ability was assessed using the C-index and the time-dependent area under the receiver operating characteristic curve (AUROC) in both the training and testing datasets. These metrics provided insights into the model's ability to distinguish between high-risk and low-risk individuals.\n\nCalibration was evaluated using calibration plots, the expected\u2013observed ratio, and Hosmer\u2013Lemeshow \u03c72 (HL-\u03c72) tests. These methods ensured that the predicted probabilities aligned closely with the actual outcomes, indicating the model's reliability.\n\nFeature importance within the model was quantified using SHapley Additive exPlanations (SHAP) values. This approach helped in identifying the most influential predictors, such as age, sex, and specific functional principal components (FPCs) of various risk factors.\n\nAdditionally, the model's performance was compared to traditional risk scores like China-PAR and PREVENT. Net reclassification improvement (NRI) and integrated discrimination improvement (IDI) were calculated to assess the model's ability to correctly reclassify individuals into appropriate risk categories compared to existing scores.\n\nSensitivity analyses were conducted to evaluate the impact of varying the trajectory observation window on the model's predictive performance. This involved adjusting the observation windows from 0 to 6 years and recalibrating the scores for each window to examine changes in prediction accuracy.\n\nOverall, the evaluation methods employed ensured a comprehensive assessment of the ML-CVD-C model's performance, demonstrating its superiority in predicting cardiovascular risk compared to traditional methods.",
  "evaluation/measure": "In the evaluation of our ML-CVD-C model, we employed a comprehensive set of performance metrics to assess its effectiveness in predicting cardiovascular risk. The primary metric used was the C-index, which measures the model's discriminative ability. For the ML-CVD-C model, the C-index was 0.86 in the training set and 0.80 in the testing set, indicating strong predictive performance. In comparison, traditional risk scores like China-PAR and PREVENT had lower C-index values of 0.62 and 0.64, respectively, in the testing cohort.\n\nAdditionally, we utilized the time-dependent area under the receiver operating characteristic curve (AUROC) to evaluate the model's performance over various follow-up periods. This metric provided insights into how well the model discriminated between high-risk and low-risk individuals over time.\n\nCalibration was assessed using calibration plots, the expected\u2013observed ratio, and Hosmer\u2013Lemeshow \u03c72 (HL-\u03c72) tests. These methods ensured that the predicted probabilities aligned closely with the observed outcomes, which is crucial for the clinical applicability of the model.\n\nWe also quantified feature importance using SHapley Additive exPlanations (SHAP) values. This approach helped identify the most influential variables in the model, such as age, sex, and specific trajectory components of risk factors like fasting glucose and systolic blood pressure.\n\nTo further validate the model's performance, we compared it with traditional Cox regression models using the same variables. This comparison allowed us to highlight the advantages of machine learning techniques in handling complex, non-linear relationships within the data.\n\nThe set of metrics used is representative of current standards in the literature, ensuring a thorough and rigorous evaluation of the model's predictive accuracy and reliability. The inclusion of both discrimination and calibration metrics, along with feature importance analysis, provides a comprehensive assessment of the ML-CVD-C model's performance.",
  "evaluation/comparison": "In the evaluation of our ML-CVD-C model, we conducted a thorough comparison with both publicly available methods and simpler baselines to ensure its robustness and superiority. We compared the ML-CVD-C model to established cardiovascular risk scores such as China-PAR and PREVENT, which are widely recommended and used in clinical practice. These comparisons were performed on the same dataset, ensuring a fair and direct evaluation.\n\nAdditionally, we compared our model to a traditional Cox regression model using the same set of variables. This allowed us to assess the advantages of machine learning techniques over traditional statistical approaches within the same context. The results demonstrated that the ML-CVD-C model outperformed these traditional methods, highlighting the benefits of incorporating trajectory information and advanced machine learning algorithms.\n\nFurthermore, we evaluated the model's performance using various metrics, including the C-index, time-dependent area under the receiver operating characteristic curve (AUROC), and calibration plots. These metrics provided a comprehensive assessment of the model's discrimination and calibration capabilities. The ML-CVD-C model consistently showed superior performance across these metrics, indicating its effectiveness in predicting cardiovascular risk.\n\nIn summary, the comparison to publicly available methods and simpler baselines confirmed the superiority of the ML-CVD-C model. This rigorous evaluation underscores the model's potential as a valuable tool for precision medicine in patients with type 2 diabetes mellitus (T2DM) in China.",
  "evaluation/confidence": "The evaluation of the ML-CVD-C model included several performance metrics, each accompanied by confidence intervals to provide a range within which the true value is likely to fall. For instance, the C-index, a measure of discrimination, was reported with 95% confidence intervals. In the training set, the C-index for the ML-CVD-C model was 0.86 (0.85, 0.87), and in the testing set, it was 0.80 (0.78, 0.82). These intervals indicate the precision of the estimates and help in understanding the reliability of the model's performance.\n\nStatistical significance was assessed using various tests. For example, Wald\u2019s test was used to compare the differences between models that incorporated trajectory information and those that did not. The results showed moderate improvements in predictive performance when trajectory information was included, suggesting that these improvements are likely not due to random chance.\n\nAdditionally, the net reclassification improvement (NRI) and integrated discrimination improvement (IDI) were evaluated to quantify the enhancement in risk stratification provided by the ML-CVD-C model compared to other scores. The NRI was approximately 50%, indicating a substantial improvement in classification accuracy.\n\nCalibration plots and the expected\u2013observed ratio were used to assess how well the predicted probabilities matched the actual outcomes. The ML-CVD-C model exhibited an expected-observed ratio of 1.30, which, while indicating some overestimation at higher risk levels, was better than other models like China-PAR and PREVENT.\n\nThe hazard ratios for major adverse cardiovascular events (MACE) in different risk categories were also statistically significant. For example, participants in the very high-risk group had a hazard ratio of 21.3 (14.2, 32.1) compared to those in the low-risk group, demonstrating a clear and significant increase in risk.\n\nOverall, the performance metrics, confidence intervals, and statistical tests provide strong evidence that the ML-CVD-C model is superior to traditional models and baselines in predicting cardiovascular risk. The results are statistically significant and indicate that the model's improvements are reliable and not due to random variation.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. However, the data can be made available upon request to the corresponding authors. This approach ensures that the data is used responsibly and in accordance with ethical guidelines. The study protocol was approved by the Kailuan General Hospital Ethics Committee, and all participants provided informed consent. This process aligns with the Declaration of Helsinki, ensuring the protection of participant privacy and data security. For those interested in accessing the data, detailed requests can be directed to the corresponding authors, who will facilitate the review and approval process."
}