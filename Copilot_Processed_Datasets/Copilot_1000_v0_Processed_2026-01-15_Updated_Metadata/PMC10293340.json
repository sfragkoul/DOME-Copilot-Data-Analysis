{
  "publication/title": "Automatic Segmentation of the Fetus in 3D Magnetic Resonance Images Using Deep Learning: Accurate and Fast Fetal Volume Quantification for Clinical Use.",
  "publication/authors": "Ryd D, Nilsson A, Heiberg E, Hedstr\u00f6m E",
  "publication/journal": "Pediatric cardiology",
  "publication/year": "2023",
  "publication/pmid": "36334112",
  "publication/pmcid": "PMC10293340",
  "publication/doi": "10.1007/s00246-022-03038-0",
  "publication/tags": "- Fetal cardiovascular magnetic resonance imaging\n- Fetal weight\n- Prenatal diagnosis\n- Deep learning\n- Automatic segmentation\n- Fetal volume quantification\n- Magnetic resonance imaging\n- Neural networks\n- Fetal blood flow\n- Congenital anomalies",
  "dataset/provenance": "The dataset used in this study was sourced from fetal MRI examinations conducted at Skane University Hospital in Lund, Sweden. The study included 42 fetuses with a gestational age of 36 weeks (ranging from 29 to 39 weeks). These MRI examinations were performed between October 2015 and December 2021. The cohort consisted of fetuses with and without known or suspected congenital heart disease. The MRI examinations were done both for clinical indications and for research purposes aimed at developing fetal cardiovascular MRI.\n\nThe MRI data was acquired using a 1.5 T Aera scanner, with balanced steady-state free precession (bSSFP) sequences used to acquire anatomical overview images in transverse, sagittal, and coronal directions. For fetal volume quantification, a 3D image slab covering the uterus was acquired with specific parameters to ensure high-resolution imaging. Phase-contrast flow images were also acquired in the umbilical vein and fetal descending aorta using a 2D gradient recalled echo sequence.\n\nThe dataset included manual segmentations of the fetus, umbilical cord, placenta, and amniotic fluid, which were performed using Segment 3D print software. These manual delineations were used as ground truth for training and evaluating the proposed artificial neural network. The study also utilized Hadlock\u2019s formulas for estimating fetal weight, which were applied to biometric parameters measured in MR images.\n\nThe algorithm for automatic fetal segmentation was developed in a previous project and involved a U-net convolutional neural network trained to classify each pixel as fetus, placenta, umbilical cord, or amniotic fluid. The network was trained using manual delineations as ground truth, and it processed data on a slice-by-slice basis in three different orthogonal directions. Fourfold cross-validation was used for training and hyperparameter optimization, with 15 datasets used for training and 5 for validation in each iteration. The remaining 22 datasets were used for testing network performance, with one twin pregnancy dataset used to test the generalizability of the network.",
  "dataset/splits": "The dataset was divided into four splits for the purpose of training, validation, and testing the neural network. Specifically, fourfold cross-validation was employed, which means the data was split into four parts. In each iteration of the cross-validation process, 15 datasets were used for training the network, and 5 datasets were used for validation. This process was repeated four times, ensuring that each dataset was used for both training and validation.\n\nAfter the cross-validation process, the remaining 22 datasets were utilized for testing the network's performance. Out of these, 21 datasets were used to compare the automatic segmentation results with manual segmentation. Additionally, one dataset, which included a twin pregnancy, was used to test the generalizability of the network as a proof of concept. This approach ensured a comprehensive evaluation of the network's performance and its ability to handle diverse scenarios, including multiple fetuses.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in this study is not publicly released. The study involved fetal MRI examinations performed at Skane University Hospital in Lund, Sweden, between October 2015 and December 2021. The dataset consisted of 42 fetuses with a gestational age of 36 weeks (ranging from 29 to 39 weeks). These examinations were conducted both for clinical indications and for research purposes aimed at developing fetal cardiovascular MRI.\n\nThe study was approved by the regional ethics committee (Dnr 2013/551), and all pregnant women gave written informed consent before participation. The data collection and analysis were conducted in accordance with the Helsinki declaration. Due to the sensitive nature of the data, involving medical images of fetuses, and the ethical considerations surrounding patient privacy and consent, the dataset is not made publicly available. This ensures that the confidentiality and rights of the participants are protected.",
  "optimization/algorithm": "The machine-learning algorithm class used is a convolutional neural network, specifically a U-net architecture. This type of network is well-suited for image segmentation tasks due to its ability to capture spatial hierarchies in images.\n\nThe algorithm is not entirely new; it was developed in a previous project. The U-net structure is a 2D network that processes data on a slice-by-slice basis in three different orthogonal directions. This approach allows for a voxel-wise voting of the three directions, enhancing the accuracy of the segmentation.\n\nThe reason it was not published in a machine-learning journal is likely because the focus of the current study is on its application in medical imaging, specifically for fetal segmentation in 3D MRI. The development and validation of the algorithm for this specific medical application are the primary contributions of this work. The study aims to demonstrate the practical utility of the algorithm in clinical and research settings, rather than introducing a novel machine-learning technique.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It primarily relies on a single machine-learning method, specifically a two-dimensional U-net, for automatic segmentation of fetal structures in MRI scans. This U-net analyzes image slabs in three orthogonal directions, enhancing its performance by incorporating more information for pixel classification.\n\nThe U-net was trained and validated using a neural network specifically designed for fetal segmentation. The training process involved classifying pixels in 3D MRI image stacks as fetus, placenta, umbilical cord, or amniotic fluid. The performance of this automatic method was evaluated and found to have high agreement with manual segmentation, demonstrating its effectiveness.\n\nThe study did not combine outputs from multiple machine-learning algorithms to make final predictions. Instead, it focused on optimizing the performance of the U-net for accurate and efficient fetal segmentation. The independence of the training data is not explicitly discussed in the context of a meta-predictor, as the model does not operate in that manner. The primary goal was to develop a robust and clinically applicable method for fetal weight estimation using 3D MRI.",
  "optimization/encoding": "The data encoding process involved classifying each pixel in the 3D MRI image stack as one of four categories: fetus, placenta, umbilical cord, or amniotic fluid. This classification was performed manually using a 3D pen tool in Segment 3D print software, which served as the ground truth for training the neural network. The manual delineations were repeated throughout the entire 3D image stack, ensuring comprehensive coverage and accurate labeling.\n\nThe algorithm utilized a 2D U-net convolutional neural network, which processes data on a slice-by-slice basis in three different orthogonal directions. This approach allows for a voxel-wise voting mechanism, where the final segmentation result is derived from the consensus of the three directions. This method enhances the accuracy and robustness of the segmentation process.\n\nThe dataset consisted of 42 fetuses, with a gestational age range of 29 to 39 weeks. The MRI examinations were conducted using a 1.5 T Aera scanner, acquiring balanced steady-state free precession (bSSFP) sequences in transverse, sagittal, and coronal directions. For fetal volume quantification, a 3D image slab covering the uterus was acquired with specific parameters to ensure high-resolution images.\n\nThe manual segmentations were used to train the neural network through a multi-task learning process. This process included all intrauterine structures to provide more information to the network, thereby improving its performance. The training involved fourfold cross-validation, with 15 datasets used for training and 5 for validation in each iteration. This rigorous training process ensured that the network could generalize well to new, unseen data.\n\nThe automatic segmentation method was tested on 21 datasets for performance evaluation and one twin pregnancy dataset to demonstrate its generalizability. The results showed high agreement between the automatic and manual segmentation methods, with the automatic method significantly reducing the time required for segmentation from 1-2 hours to just 45 seconds per fetus. This efficiency makes the automatic method feasible for both clinical and research purposes, enabling more accurate assessment of fetal growth and blood flow.",
  "optimization/parameters": "In our study, we utilized Hadlock's formulas for fetal weight estimation, which incorporate several biometric parameters measured from MR images. Specifically, these parameters include fetal head circumference (HC), biparietal diameter (BPD), abdominal circumference (AC), and femur length (FL). The number of parameters (p) used in the model varies depending on the specific Hadlock formula applied. Hadlock 1 uses three parameters: AC, FL, and the product of AC and FL. Hadlock 2 employs four parameters: AC, FL, BPD, and the product of AC and FL. Hadlock 3 includes HC, AC, FL, and the product of AC and FL. Hadlock 4 utilizes five parameters: HC, AC, FL, BPD, AC, and the product of AC and FL.\n\nThe selection of these parameters was based on established methodologies in fetal weight estimation, as detailed in previous research. These parameters were chosen for their relevance and accuracy in predicting fetal weight, ensuring that the model's inputs were both comprehensive and clinically significant. The inclusion of multiple parameters allows for a more precise estimation of fetal weight, taking into account various aspects of fetal growth and development.",
  "optimization/features": "The study utilized specific biometric parameters as input features for estimating fetal weight using Hadlock's formulas. These parameters included fetal head circumference (HC), biparietal diameter (BPD), abdominal circumference (AC), and femur length (FL). These four measurements were directly obtained from MR images and served as the primary input features for the weight estimation algorithms.\n\nFeature selection was not explicitly performed in the traditional sense, as the biometric parameters chosen are standard and well-established in fetal ultrasound and MRI practices for weight estimation. The selection of these features was based on established medical protocols and previous research, ensuring their relevance and reliability.\n\nThe training of the neural network for automatic fetal segmentation involved manual delineations of the fetus, placenta, umbilical cord, and amniotic fluid. These delineations were used as ground truth for training and validation. The network was trained using a fourfold cross-validation approach, with 15 datasets used for training and 5 for validation in each iteration. This method ensured that the model's performance was robust and generalizable.\n\nThe input features for the neural network included pixel classifications from the MR images, which were labeled as fetus, placenta, umbilical cord, or amniotic fluid. The network processed data on a slice-by-slice basis in three different orthogonal directions, enhancing its ability to accurately segment the fetus. The final segmentation result was a voxel-wise voting of the three directions, improving the overall accuracy and reliability of the automatic segmentation method.",
  "optimization/fitting": "The fitting method employed in this study utilized a U-net convolutional neural network, which is designed to handle image segmentation tasks. The network was trained to classify each pixel as belonging to one of four categories: fetus, placenta, umbilical cord, or amniotic fluid. This multi-task learning approach provided the network with more information, thereby improving its performance.\n\nThe network was trained using a fourfold cross-validation process, which involved 15 datasets for training and 5 for validation in each iteration. This method ensured that the model was robust and generalizable, as it was tested on different subsets of the data. Additionally, the network was evaluated on 21 separate datasets to assess its performance against manual segmentation, further validating its accuracy.\n\nTo address the potential issue of overfitting, where the model might perform well on training data but poorly on new data, the study used a validation set that was not part of the training process. The performance on this validation set was monitored to ensure that the model generalized well to unseen data. Furthermore, the use of a relatively simple 2D U-net structure, processed on a slice-by-slice basis in three different orthogonal directions, helped in mitigating overfitting by reducing the complexity of the model.\n\nUnderfitting, where the model is too simple to capture the underlying patterns in the data, was addressed by ensuring that the network had sufficient capacity to learn the necessary features. The inclusion of multiple intrauterine structures in the training process provided a richer set of features for the network to learn from, thereby enhancing its ability to accurately segment the fetus.\n\nThe Dice similarity coefficient, which measures the overlap between the automatic and manual segmentations, was used to quantify the performance of the network. A mean Dice similarity index of 0.94 \u00b1 0.02 indicated a high level of agreement between the automatic and manual segmentations, suggesting that the model was neither overfitting nor underfitting the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our neural network for fetal segmentation. One of the key methods used was fourfold cross-validation. This involved dividing our dataset into four parts, using three parts for training and one part for validation in each iteration. This process was repeated four times, ensuring that each part of the dataset was used for validation once. This technique helps to provide a more accurate estimate of the model's performance and reduces the risk of overfitting to a specific subset of the data.\n\nAdditionally, we utilized a multi-task learning approach. By including all intrauterine structures\u2014such as the fetus, placenta, umbilical cord, and amniotic fluid\u2014in the training process, we provided the network with more information. This approach helps the network to generalize better and reduces the likelihood of overfitting to the specific task of fetal segmentation alone.\n\nThe network structure itself, a 2D U-net, processes data on a slice-by-slice basis in three different orthogonal directions. The final segmentation result is a voxel-wise voting of the three directions, which further enhances the network's ability to generalize and reduces overfitting.\n\nMoreover, we performed hyperparameter optimization during the training process. This involved systematically searching for the best combination of hyperparameters to improve the network's performance and generalization capabilities. By optimizing these parameters, we aimed to find a model that not only performs well on the training data but also generalizes well to unseen data.\n\nThese techniques collectively contributed to the development of a robust and reliable neural network for automatic fetal segmentation, minimizing the risk of overfitting and ensuring accurate and consistent results.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we employed a 2D U-net convolutional neural network, which was trained using a fourfold cross-validation approach. This involved using 15 datasets for training and 5 for validation in each iteration. The network was designed to classify each pixel as fetus, placenta, umbilical cord, or amniotic fluid, with manual delineations serving as the ground truth.\n\nThe model files and optimization parameters are not explicitly provided in the publication. However, the methodology and results demonstrate the effectiveness of the approach, which can be replicated by following the described procedures. The algorithm was developed in a previous project, and the core components, including the U-net structure and training process, are well-documented in the referenced literature.\n\nFor those interested in implementing or further developing the method, the details provided in the publication, along with the cited references, offer a comprehensive guide. The software used for manual vessel delineation, Segment v3.3, is freely available from Medviso AB, Lund, Sweden, which may facilitate further research and development in this area. The study was conducted in accordance with ethical guidelines, and all necessary consents were obtained, ensuring the integrity and reproducibility of the results.",
  "model/interpretability": "The model employed in this study is a 2D U-net convolutional neural network, which processes data on a slice-by-slice basis in three different orthogonal directions. This architecture allows for a voxel-wise voting mechanism, where the final segmentation result is determined by the consensus of the three directions. This approach provides a level of transparency in how the model arrives at its segmentation decisions.\n\nThe network is trained to classify each pixel as one of four categories: fetus, placenta, umbilical cord, or amniotic fluid. This classification is based on manual delineations used as ground truth, ensuring that the model learns from accurate and reliable data. The inclusion of all intrauterine structures in the training process enhances the model's performance by providing more contextual information.\n\nThe use of multi-task learning further improves the model's interpretability. By training the network to recognize multiple structures simultaneously, it gains a more comprehensive understanding of the image data, which can be observed in its segmentation outputs. This method contrasts with previous attempts that focused solely on the fetus, potentially leading to less accurate and less interpretable results.\n\nThe model's performance is evaluated using metrics such as the Dice similarity coefficient, which quantifies the overlap between the automatic and manual segmentations. This metric provides a clear and objective measure of the model's accuracy and reliability. Additionally, Bland-Altman analysis is used to assess the agreement between automatic and manual fetal volume measurements, offering further insight into the model's performance.\n\nIn summary, the model is not a black box but rather a transparent system that leverages a structured approach to pixel classification and multi-task learning. The use of well-defined evaluation metrics and a clear training process contributes to its interpretability, making it a reliable tool for automatic fetal segmentation.",
  "model/output": "The model developed in this study is primarily a classification model, but it also facilitates regression tasks. It classifies pixels in 3D MRI image stacks into categories such as fetus, placenta, umbilical cord, or amniotic fluid. This pixel classification is then used to generate 3D fetal models, which can be employed to estimate fetal volume and weight\u2014tasks that fall under regression.\n\nThe automatic segmentation method was trained and evaluated using these classified pixels, demonstrating high agreement with manual segmentation. The model's output includes 3D fetal models generated in approximately 45 seconds, significantly faster than manual segmentation, which can take 1\u20132 hours. This efficiency makes the model suitable for both clinical and research purposes, enabling accurate fetal volume and weight quantification.\n\nThe model's performance was also tested on twin fetuses, showing promising generalizability despite being trained on singleton fetuses. This indicates the model's potential to work across a wider range of fetal sizes and conditions. The output of the model includes indexed fetal blood flow values, which are crucial for physiological comparisons between fetuses, helping to study pathophysiology more accurately.",
  "model/duration": "The execution time for the automatic fetal segmentation model is notably efficient. The model can generate a fetal 3D model in approximately 45 seconds. This is a significant time savings compared to manual segmentation, which typically takes between 1 to 2 hours to achieve an accurately segmented model. The high agreement between the automatic and manual segmentation methods further validates the efficiency and reliability of the automatic approach. This rapid execution time makes the automatic method feasible for both clinical and research purposes, allowing for more timely and accurate assessments of fetal volume and weight.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the proposed automatic fetal segmentation method involved several key steps and comparisons to ensure its accuracy and reliability. Manual segmentations of the fetus, umbilical cord, placenta, and amniotic fluid were performed using a 3D pen tool in Segment 3D print software. These manual delineations served as the ground truth for training neural networks and evaluating their performance.\n\nThe automatic segmentation method was tested on a dataset of 42 fetuses, with gestational ages ranging from 29 to 39 weeks. The fetuses included in the study had both known or suspected congenital heart disease and those without any known issues. The automatic method generated fetal 3D models in approximately 45 seconds, whereas manual segmentation took between 1 to 2 hours. The agreement between the automatic and manual fetal segmentation was found to be high, as evidenced by a mean Dice similarity index of 0.94 \u00b1 0.02.\n\nTo further validate the method, it was applied to a case of twin fetuses as a proof of concept. This demonstrated the generalizability of the algorithm, which had only been trained on singleton fetuses. The results showed promising agreement, although some artifacts were noted due to image artifacts in the 3D MRI images.\n\nAdditionally, the performance of the automatic method was compared to Hadlock\u2019s formulas for fetal weight estimation. The bias and 95% limits of agreement for Hadlock\u2019s formulas versus 3D MRI manual delineations were calculated, showing a generally good agreement but with increasing differences as fetal weight increased. This comparison highlighted the potential of the automatic method for more accurate fetal weight estimation.\n\nThe study also quantified fetal blood flow in the umbilical vein and descending aorta, indexing the blood flow to fetal weight. This allowed for physiological comparisons between fetuses, enabling the study of pathophysiology. The results showed median absolute umbilical venous flow and descending aortic flow, along with their indexed values, providing valuable insights into fetal blood flow dynamics.",
  "evaluation/measure": "In the evaluation of our proposed automatic fetal segmentation method, we employed several performance metrics to ensure a comprehensive assessment. The primary metrics reported include the Dice similarity coefficient and Bland-Altman analysis.\n\nThe Dice similarity coefficient is a widely used metric in medical image segmentation, providing a measure of the overlap between the automatic and manual segmentations. It is defined as the ratio of the intersection of the two sets (automatic and manual segmentations) to the average of the sizes of the sets. This metric was expressed as the mean \u00b1 standard deviation, offering a clear indication of the consistency and reliability of our automatic segmentation method.\n\nBland-Altman analysis was utilized to assess the agreement between automatic and manual fetal volume measurements, as well as between fetal weight estimated using Hadlock\u2019s formulas and volumes derived from 3D MRI. This statistical method helps in identifying any systematic bias and the limits of agreement between the two measurement techniques. The bias and 95% limits of agreement were reported, providing insights into the accuracy and precision of our automatic method compared to manual segmentation and traditional ultrasound-based weight estimation formulas.\n\nThese metrics are representative of the current standards in the literature for evaluating segmentation algorithms in medical imaging. The Dice similarity coefficient is a well-established metric that allows for direct comparison with other studies in the field. Similarly, Bland-Altman analysis is a common approach for assessing agreement between different measurement methods, ensuring that our results are comparable to those reported in other research.\n\nIn summary, the reported performance metrics are both comprehensive and aligned with the standards in the literature, providing a robust evaluation of our automatic fetal segmentation method.",
  "evaluation/comparison": "In our study, we compared our proposed automatic fetal segmentation method with manual segmentation, which serves as the gold standard. The agreement between the automatic and manual methods was high, with a mean Dice similarity index of 0.94 \u00b1 0.02, indicating strong performance.\n\nWe also referenced previous studies that used machine learning methods for automatic segmentation of the fetus in magnetic resonance images. Specifically, we compared our results with those of Zhang et al., who used a graph-based approach, and Dudovitch et al., who evaluated both two- and three-dimensional U-nets. Our method outperformed Zhang et al.'s approach, achieving a Dice similarity index of 0.94 compared to their 0.69. While Dudovitch et al.'s three-dimensional U-net showed high accuracy, our two-dimensional U-net, which analyzes the image slab in three orthogonal directions, provided a more practical solution for clinical settings due to its ease of implementation on current clinical systems.\n\nAdditionally, we discussed the potential advantages and limitations of using ultrasound versus 3D MRI for fetal volume quantification. Ultrasound may be better in early pregnancy due to higher resolution, but 3D MRI offers advantages in late pregnancy and for visualizing specific fetal structures. This comparison highlights the complementary roles of these imaging modalities.\n\nIn summary, our method was compared against manual segmentation and other machine learning approaches, demonstrating its effectiveness and practicality for clinical application.",
  "evaluation/confidence": "The evaluation of the proposed method includes several performance metrics with associated confidence intervals, providing a clear indication of the reliability and precision of the results. The agreement between automatic and manual fetal volume measurements was assessed using Bland-Altman analysis, which reported a bias of -4.5 ml with 95% limits of agreement of \u00b1351 ml. This statistical approach helps to understand the variability and potential errors in the measurements.\n\nThe Dice similarity index, a common metric for evaluating the overlap between two segmentations, was reported as 0.94 \u00b1 0.02. This metric, along with its standard deviation, indicates the consistency and accuracy of the automatic segmentation method compared to manual segmentation.\n\nThe study also compared fetal weight estimations using Hadlock\u2019s formulas against 3D MRI measurements. The biases and 95% limits of agreement for Hadlock\u2019s formulas 1\u20134 versus 3D MRI manual delineations were provided, showing varying degrees of agreement. For instance, Hadlock\u2019s formula 1 had a bias of 108 g with limits of agreement of \u00b1435 g, while Hadlock\u2019s formula 2 had a bias of 211 g with limits of agreement of \u00b1468 g. These results highlight the differences and potential inaccuracies in fetal weight estimation methods, suggesting that 3D MRI could offer more precise measurements.\n\nThe statistical significance of the results is implied through the use of confidence intervals and the comparison of different methods. The high Dice similarity index and the narrow limits of agreement in the Bland-Altman analysis suggest that the automatic method is not only accurate but also reliable. The trends observed in the differences between Hadlock\u2019s formulas and 3D MRI, particularly the increasing differences with increasing fetal weight, indicate that 3D MRI may provide more consistent and accurate fetal weight estimations, especially for larger fetuses.\n\nOverall, the performance metrics and statistical analyses presented in the evaluation provide strong evidence of the method's superiority in terms of accuracy and reliability compared to traditional ultrasound-based methods and manual segmentation. The inclusion of confidence intervals and the detailed comparison with established formulas enhance the confidence in the results and the potential clinical applicability of the proposed method.",
  "evaluation/availability": "Not enough information is available."
}