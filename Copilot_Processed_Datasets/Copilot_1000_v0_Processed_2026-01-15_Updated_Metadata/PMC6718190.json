{
  "publication/title": "Artificial intelligence detection of distal radius fractures: a comparison between the convolutional neural network and professional assessments.",
  "publication/authors": "Gan K, Xu D, Lin Y, Shen Y, Zhang T, Hu K, Zhou K, Bi M, Pan L, Wu W, Liu Y",
  "publication/journal": "Acta orthopaedica",
  "publication/year": "2019",
  "publication/pmid": "30942136",
  "publication/pmcid": "PMC6718190",
  "publication/doi": "10.1080/17453674.2019.1600125",
  "publication/tags": "- Distal Radius Fractures\n- Deep Learning\n- Medical Imaging\n- Convolutional Neural Networks\n- Faster R-CNN\n- Inception-v4\n- Orthopedics\n- Radiology\n- Diagnostic Performance\n- Machine Learning in Medicine\n- Wrist Radiographs\n- Object Detection\n- Medical Image Analysis\n- Computer-Aided Diagnosis\n- Performance Evaluation",
  "dataset/provenance": "The dataset used in this study consisted of wrist radiographs from patients who visited the hospital for radiological examination. The inclusion criteria required that it was the patient's first visit for such an examination and that both standard anterior-posterior and lateral wrist radiographs were taken and available. Exclusion criteria included the presence of casts or splints in the radiographs, as well as fractures of the distal ulna, carpal bones, or any dislocations in the wrist.\n\nThe original training dataset comprised 1,341 images with distal radius fractures (DRFs) and 699 images without DRFs. This dataset was augmented through various techniques, resulting in a final training dataset of 6,120 images, which included 4,023 images with DRFs and 2,097 images without DRFs. The test dataset consisted of 300 images, evenly split between those with and without DRFs.\n\nThe dataset was not explicitly mentioned as being used in previous papers or by the community. The images were annotated by orthopedists with more than 5 years of professional experience, ensuring the accuracy and reliability of the data used in training and validating the models. The dataset was specifically curated for this study to evaluate the performance of convolutional neural networks (CNNs) in detecting distal radius fractures.",
  "dataset/splits": "The dataset used in this study was split into three main parts: the training dataset, the validation dataset, and the test dataset. The original training dataset consisted of 1,341 images with distal radius fractures (DRFs) and 699 images without DRFs. This dataset underwent augmentation, resulting in a total of 6,120 images. Of these, 15% were randomly selected to form the validation dataset, leaving the remaining 85% for the training dataset. Specifically, the training dataset comprised 4,023 images with DRFs and 2,097 images without DRFs, while the validation dataset included a proportional split of these augmented images.\n\nThe test dataset, used to evaluate the performance of the trained models, consisted of 300 images. This test dataset was balanced, containing 150 images with fractures and 150 images without fractures. This balanced approach ensured that the models could be accurately assessed on their ability to detect DRFs.\n\nIn summary, the dataset was divided into a training set with 6,120 images, a validation set with a 15% split of the augmented images, and a test set with 300 images, evenly distributed between fractured and non-fractured cases.",
  "dataset/redundancy": "The datasets used in this study were carefully curated and split to ensure independence between training, validation, and test sets. The original training dataset consisted of 1,341 images with distal radius fractures (DRFs) and 699 images without DRFs. This dataset underwent augmentation techniques, including random horizontal inversion, offset, rotation, scaling, and shearing, resulting in a final training dataset of 6,120 images. The dataset was then split, with 15% randomly selected for the validation set.\n\nThe test dataset comprised 300 images, which were reviewed by medical professionals under controlled conditions. This ensured that the test set was independent of the training and validation sets, preventing data leakage and ensuring an unbiased evaluation of the models' performance.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets in the field of orthopedic imaging. The inclusion criteria required that the radiographs were from the first visit and included both standard anterior-posterior and lateral views. Exclusion criteria removed images with casts, splints, or other fractures and dislocations, ensuring a focused and relevant dataset for training and evaluating the models.\n\nTo enforce the independence of the datasets, rigorous protocols were followed. The original images were augmented to create a diverse training set, and a separate validation set was used to tune the models' hyperparameters. The test set was kept entirely separate, with images reviewed by orthopedists to ensure accuracy and relevance. This approach minimized the risk of overfitting and ensured that the models' performance could be generalized to new, unseen data.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study is based on stochastic gradient descent, a class of machine-learning algorithms widely used for training deep learning models. This algorithm is not new; it has been extensively used and validated in numerous studies and applications.\n\nThe choice of stochastic gradient descent was driven by its efficiency and effectiveness in handling large datasets, which is crucial for training complex models like Faster R-CNN and Inception-v4. These models require substantial computational resources and optimization techniques that can converge quickly and accurately.\n\nThe focus of our publication is on the application of these models in the medical field, specifically for detecting distal radius fractures in wrist radiographs. The optimization algorithm is a foundational component that enables the successful training and validation of our models. While the algorithm itself is well-established, its application in this specific medical context is what makes our work novel and significant.\n\nThe decision to publish in a medical journal rather than a machine-learning journal is due to the primary focus on the medical implications and the diagnostic performance of the models. The optimization algorithm is a means to an end, facilitating the development of a tool that can assist medical professionals in diagnosing fractures more accurately and efficiently.",
  "optimization/meta": "Not applicable. The provided information does not discuss a meta-predictor or the use of data from other machine-learning algorithms as input. The text focuses on the performance and training of specific models, such as Faster R-CNN and Inception-v4, but does not mention the integration of multiple machine-learning methods into a single meta-predictor. Therefore, it is not possible to provide details about the constituent machine-learning methods or the independence of training data in the context of a meta-predictor.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the images were suitable for training and testing the models.\n\nInitially, plain AP wrist radiographs, stored as DICOM files, were exported as JPEG files with a matrix size of 600 x 800 pixels using eWorld Viewer. This conversion standardized the image format and size, making them compatible with the machine-learning pipeline.\n\nThe dataset consisted of 2,340 AP wrist radiographs, which were divided into an original training dataset of 2,040 images and a test dataset of 300 images. The training dataset included 1,341 images with distal radius fractures (DRFs) and 699 images without DRFs. The test dataset contained 150 images with DRFs and 150 images without DRFs.\n\nTo enhance the training process, data augmentation techniques were applied to the original images. These techniques included random horizontal inversion, random offset within 10% of the height and width, random rotation within 30 degrees, 10% random scaling, and 15% random shearing. This augmentation increased the diversity of the training dataset, helping the models generalize better to new, unseen data.\n\nFor the Faster R-CNN model, two orthopedists manually annotated the regions of interest (ROIs) on each image using LabelImg. The ROI coordinates were recorded automatically during the annotation process. These annotations were crucial for training the Faster R-CNN to detect the distal radius regions accurately.\n\nThe annotated images were then used to train the Faster R-CNN model, which automatically annotated the ROIs on the AP wrist radiographs in both the training and test datasets. The ROIs were extracted as images, and the rest of each initial image was discarded to reduce unnecessary interference and noise.\n\nFor the Inception-v4 model, the ROIs extracted from the annotated images were resized to 200 x 200 pixels and stored as JPEG files. These resized ROIs were further augmented using the same techniques applied to the original images. The final training dataset for the Inception-v4 model consisted of 6,120 images, including 4,023 images with DRFs and 2,097 images without DRFs. Fifteen percent of this dataset was randomly selected for the validation process.\n\nIn summary, the data encoding and preprocessing involved converting DICOM files to JPEG, applying data augmentation, manual annotation of ROIs, and extracting and resizing the ROIs for training the machine-learning models. These steps ensured that the data was in a suitable format and enhanced the models' ability to detect DRFs accurately.",
  "optimization/parameters": "The optimization process for our models involved several key parameters. For the Faster R-CNN model, the training procedure featured stochastic gradient descent as the optimizer, with a batch size of 100 and a dropout rate of 0.5. The model underwent 40,000 iterations with an initial learning rate of 0.001, which decayed over time according to the formula Learning Rate = Learning Rate * 1/(1 + decay * epoch), where the weight decay was set to 0.0005.\n\nFor the Inception-v4 model, the training procedure also used stochastic gradient descent as the optimizer, with a batch size of 100 and a dropout rate of 0.5. The model was trained for 20,000 iterations with an initial learning rate of 0.001 and a fixed learning rate decay type.\n\nThe selection of these parameters was based on empirical evidence and common practices in the field of deep learning. The batch size and learning rate were chosen to balance computational efficiency and model convergence. The number of iterations was determined to ensure that the models had sufficient time to learn from the training data. The dropout rate was set to prevent overfitting, and the learning rate decay was implemented to fine-tune the model as training progressed.",
  "optimization/features": "The input features for our models were derived from wrist radiographs. Specifically, the features used were the regions of interest (ROIs) within the wrist radiographs, focusing on the distal radius regions. These ROIs were automatically annotated using the Faster R-CNN model, which was trained to detect these specific regions.\n\nFeature selection was implicitly performed by focusing on the distal radius regions, as these were the areas of interest for detecting distal radius fractures (DRFs). The initial dataset included images with and without DRFs, and the ROIs were extracted from these images. This process ensured that only the relevant parts of the images were used for training and testing the diagnostic model, Inception-v4.\n\nThe feature selection process was conducted using the training set only. The training dataset consisted of 1,341 images with DRFs and 699 images without DRFs. These images were augmented to create a final training dataset of 6,120 images, including 4,023 images with DRFs and 2,097 images without DRFs. The validation dataset was randomly selected from this augmented dataset, ensuring that the feature selection and model training were performed using the training set only. This approach helped in maintaining the integrity of the validation and test sets, preventing data leakage and ensuring robust model evaluation.",
  "optimization/fitting": "The Faster R-CNN and Inception-v4 models were trained using a substantial dataset to ensure robust performance. The original training dataset for Faster R-CNN consisted of 1,341 images with distal radius fractures (DRFs) and 699 images without DRFs. This dataset was augmented through various techniques, resulting in a final training dataset of 6,120 images. Similarly, the Inception-v4 model was trained on a dataset of 6,120 images, which included 4,023 images with DRFs and 2,097 images without DRFs. These augmented datasets were used to train the models, ensuring a large number of training points relative to the model parameters.\n\nTo address overfitting, several strategies were employed. First, data augmentation techniques such as random horizontal inversion, random offset, random rotation, random scaling, and random shearing were applied to the training images. This increased the diversity of the training data, making the models more generalizable. Second, dropout regularization with a rate of 0.5 was used during training. This technique randomly sets a fraction of input units to 0 at each update during training time, which helps prevent overfitting by reducing the model's reliance on any single feature. Additionally, the learning rate was decayed over time, which helps in fine-tuning the model parameters and prevents overfitting.\n\nTo rule out underfitting, the models were trained for a sufficient number of iterations (40,000 for Faster R-CNN and 20,000 for Inception-v4). The training process was monitored using validation datasets, which comprised 15% of the total dataset. The performance on the validation dataset was used to tune hyperparameters and ensure that the models were learning effectively. The use of stochastic gradient descent as the optimizer also helped in converging to a good solution by exploring the parameter space efficiently.\n\nThe performance of the models was evaluated using metrics such as mean square error (MSE) and Intersection over Union (IOU) for Faster R-CNN, and the area under the receiver operating characteristic curve (AUC) for Inception-v4. These metrics provided a comprehensive assessment of the models' accuracy and generalization capabilities. The results indicated that the models were well-trained, with high accuracy and low error rates, suggesting that both overfitting and underfitting were effectively managed.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting during the training of our convolutional neural network (CNN) models. One of the key methods used was dropout, which was set at a rate of 0.5. Dropout works by randomly setting a fraction of the input units to zero at each update during training time, which helps to prevent overfitting by ensuring that the model does not become too reliant on any single neuron.\n\nAdditionally, we utilized learning rate decay to gradually reduce the learning rate over time. This technique helps in fine-tuning the model by making smaller updates to the weights as training progresses, which can lead to more stable and generalizable performance.\n\nWe also augmented the training dataset through various transformations, including random horizontal inversion, random offset within 10% of the height and width, random rotation within 30 degrees, 10% random scaling, and 15% random shearing. Data augmentation helps to increase the diversity of the training data, making the model more robust and less likely to overfit to the specific examples in the training set.\n\nFurthermore, we used a validation dataset, which comprised 15% of the final training dataset, to monitor the model's performance during training. This allowed us to tune the hyperparameters and stop training when the performance on the validation set started to degrade, indicating potential overfitting.\n\nThese regularization techniques collectively contributed to the robustness and generalizability of our models, ensuring that they performed well on unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules for the models used in this study are reported in detail. For the Faster R-CNN model, the training procedure included parameters such as an optimizer using stochastic gradient descent, a batch size of 100, a dropout rate of 0.5, 40,000 iterations, an initial learning rate of 0.001, and a learning rate decay factor. Similarly, the Inception-v4 model was trained with stochastic gradient descent, a batch size of 100, a dropout rate of 0.5, 20,000 iterations, an initial learning rate of 0.001, and a fixed learning rate decay type.\n\nThe experimental environment details, including the use of TensorFlow 1.11.0 on an Ubuntu 16.04 operating system with an NVIDIA Titan X GPU, are also provided. This environment setup ensures reproducibility of the results.\n\nRegarding the availability of model files and optimization parameters, the study does not explicitly mention where these files can be accessed or under what license they are provided. However, the detailed reporting of the hyper-parameters and training procedures allows for the replication of the models and their optimization processes by other researchers.\n\nNot applicable.",
  "model/interpretability": "The model employed in this study, Inception-v4, is primarily a black-box model. This means that while it can provide highly accurate predictions, the internal workings and the specific features it uses to make these predictions are not easily interpretable. The model operates by learning complex patterns and features directly from the input data, which are not explicitly defined or easily understood by humans.\n\nThe Inception-v4 model was trained to distinguish images with distal radius fractures (DRFs) from normal images. It generates a score between 0 and 1, representing the likelihood of an image containing a DRF. This score is derived from the model's internal layers and neurons, which process the input image through a series of convolutions and transformations. However, the exact mechanisms by which the model arrives at this score are not transparent.\n\nTo enhance the interpretability of the model's decisions, we utilized Faster R-CNN for automatic annotation of the regions of interest (ROIs) in the images. This step ensured that the distal radius region was appropriately encased in the bounds, providing a clear focus for the Inception-v4 model. While Faster R-CNN helps in identifying the relevant areas in the images, it does not make the Inception-v4 model itself more interpretable.\n\nIn summary, the Inception-v4 model used in this study is a black-box model, and its decision-making process is not easily interpretable. The use of Faster R-CNN for ROI annotation aids in focusing the model's attention but does not provide transparency into the model's internal workings.",
  "model/output": "The model discussed in this publication involves two main components: Faster R-CNN and Inception-v4, both of which are convolutional neural networks (CNNs) used for different tasks.\n\nFaster R-CNN is primarily used for object detection, which is a classification task. It is trained to detect and annotate regions of interest (ROIs) in wrist radiographs, specifically the distal radius regions. The output of Faster R-CNN is a set of coordinates that define the boundaries of these ROIs. The performance of Faster R-CNN is evaluated using metrics such as the Intersection over Union (IOU), which measures the accuracy of the detected regions compared to the ground truth annotations provided by orthopedists.\n\nOn the other hand, Inception-v4 is used for image classification, which is also a classification task. It is trained to distinguish between images with distal radius fractures (DRFs) and those without. The output of Inception-v4 is a score between 0 and 1, representing the likelihood that an image contains a DRF. This score is used to generate a receiver operating characteristic (ROC) curve, and the area under the curve (AUC) is calculated to evaluate the model's performance. The optimal cut-off point for the score is determined to maximize the Youden Index, which balances sensitivity and specificity.\n\nIn summary, both models are used for classification tasks. Faster R-CNN classifies and detects specific regions in images, while Inception-v4 classifies entire images into one of two categories. The outputs of these models are used to evaluate their performance in detecting and diagnosing distal radius fractures.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed a comprehensive approach to assess the performance of the models and compare them with human experts. The evaluation process began with a test dataset consisting of 300 images, evenly split between images with fractures and those without. This dataset was used to evaluate the capacity of the trained Faster R-CNN model in automatic annotation of the region of interest (ROI) on images. Two orthopedists with extensive experience annotated each image\u2019s ROI as the ground truth bounds (GTBs), ensuring the whole distal radius was encased. The Faster R-CNN model then automatically detected the ROI, and the Intersection over Union (IOU) was calculated to measure the accuracy of the model's annotations. The mean square error (MSE) was also calculated to assess the loss in the automatic annotation process.\n\nFollowing the evaluation of Faster R-CNN, the annotated images were reviewed by orthopedists to confirm the accuracy of the ROI bounds. These images were then resized and stored as a new test dataset, which was used to evaluate the Inception-v4 model. The Inception-v4 model analyzed each image, generating a score indicating the likelihood of a fracture. The receiver operating characteristic (ROC) curve was generated, and the area under the curve (AUC) was determined to assess the model's diagnostic performance.\n\nTo compare the performance of the Inception-v4 model with human experts, a group of radiologists and orthopedists were assembled. Each image in the new test dataset was diagnosed by these professionals, and any disagreements within the groups were resolved by majority vote. The diagnostic performance of the Inception-v4 model was then statistically compared with that of the human groups using the Youden Index, sensitivity, specificity, and accuracy. Statistical analyses were performed using SPSS software, with significance levels set at p < 0.05. The study was approved by the Ningbo Lihuili Hospital Ethics Committee, and financial support was provided by the Ningbo Natural Science Fund.",
  "evaluation/measure": "In the evaluation of our models, we focused on several key performance metrics to ensure a comprehensive assessment. For the Faster R-CNN model, we primarily reported the Intersection over Union (IOU), which measures the accuracy of the automatic annotation of regions of interest (ROIs). An IOU value close to 1 indicates precise detection, while values below 0.5 are considered poor. Our model achieved an average IOU of 0.87, demonstrating high accuracy in detecting the distal radius regions.\n\nFor the Inception-v4 model, we utilized the receiver operating characteristic (ROC) curve and the area under the curve (AUC) to evaluate its diagnostic performance. The AUC provides a single scalar value that summarizes the model's ability to distinguish between images with fractures and those without. An AUC of 1 indicates perfect discrimination, while an AUC of 0.5 suggests no discriminative ability. Our Inception-v4 model achieved an AUC of 0.96, indicating excellent performance in discerning images with fractures from normal images.\n\nAdditionally, we calculated sensitivity, specificity, and accuracy at the optimal cut-off point to further assess the model's performance. These metrics are standard in the literature and provide a clear comparison with human experts. Sensitivity measures the true positive rate, specificity measures the true negative rate, and accuracy measures the overall correctness of the model's predictions. We compared these metrics with those of radiologists and orthopedists to evaluate the model's diagnostic performance relative to human experts.\n\nThe use of these metrics is representative of the current literature in medical image analysis, ensuring that our evaluation is both rigorous and comparable to other studies in the field. The combination of IOU for Faster R-CNN and ROC/AUC for Inception-v4 provides a thorough assessment of our models' capabilities in automatic annotation and diagnostic classification, respectively.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, we focused on evaluating the performance of our Inception-v4 model against human experts, specifically radiologists and orthopedists. This approach allowed us to assess the practical applicability of our model in a clinical setting.\n\nWe did not compare our model to simpler baselines. Our primary goal was to determine how well our deep learning model could perform in distinguishing images with distal radius fractures (DRFs) from normal images, and to compare this performance to that of medical professionals. The Inception-v4 model was chosen for its proven capabilities in image recognition tasks, and we trained it specifically for this diagnostic task.\n\nThe evaluation involved a detailed procedure where each image in the new test dataset was diagnosed by the model and by groups of radiologists and orthopedists. The results were then statistically compared to assess the diagnostic performance of the model relative to human experts. This comparison was crucial in understanding the potential of the model to assist or even replace human diagnosis in certain scenarios.",
  "evaluation/confidence": "The evaluation of our method includes performance metrics with confidence intervals. For instance, the average Intersection over Union (IOU) value of Faster R-CNN was reported with a 95% confidence interval (CI) of 0.86\u20130.87. Similarly, the Area Under the Curve (AUC) for the Inception-v4 model was provided with a CI of 0.94\u20130.99. These intervals give a range within which the true performance metrics are expected to lie, indicating the reliability of our results.\n\nStatistical significance was assessed using various tests. For example, Dunnett\u2019s test was employed to compare the performance of Inception-v4 with that of orthopedists and radiologists. This test helps determine if the differences in performance metrics, such as accuracy, sensitivity, and specificity, are statistically significant. The results indicated that there were statistically significant differences in some comparisons, suggesting that our method performs differently from the human groups in specific metrics.\n\nAdditionally, the significance level was set at p < 0.05 for all statistical tests, ensuring that the results are robust and not due to random chance. For demographic data, chi-square tests were used to compare sex distributions, and one-way analysis of variance (ANOVA) was used for age comparisons. These tests further support the validity of our findings and the confidence in our method's performance.",
  "evaluation/availability": "Not enough information is available."
}