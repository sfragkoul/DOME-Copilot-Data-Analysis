{
  "publication/title": "Streamlining follicular monitoring during controlled ovarian stimulation: a data-driven approach to efficient IVF care in the new era of social distancing.",
  "publication/authors": "Robertson I, Chmiel FP, Cheong Y",
  "publication/journal": "Human reproduction (Oxford, England)",
  "publication/year": "2021",
  "publication/pmid": "33147345",
  "publication/pmcid": "PMC7665450",
  "publication/doi": "10.1093/humrep/deaa251",
  "publication/tags": "- Machine Learning\n- IVF\n- Follicular Tracking\n- Trigger Timing\n- OHSS Prediction\n- Random Forest\n- Clinical Decision Support\n- Retrospective Study\n- Fertility Treatment\n- COVID-19 Impact on Healthcare\n- Ovarian Stimulation\n- Predictive Modeling\n- Data-Driven Medicine\n- Reproductive Health\n- Patient Monitoring",
  "dataset/provenance": "The dataset used in this study was sourced from a tertiary IVF center located in the South of England, UK. The data was collected from January 1, 2011, to November 30, 2019. It consists of fully anonymized retrospective electronic data from IVF and ICSI cycles, excluding fertility preservation, egg freezing, and altruistic egg donation cycles.\n\nThe dataset includes a total of 2322 cycles from 1875 women, with 9294 individual scans. The mean age of the patients was 33.57 years, ranging from 20 to 44 years. Among these, 1505 patients had one cycle of controlled ovarian stimulation (COS), 301 had two cycles, and 55 had three cycles. After excluding oocyte donors and fertility preservation cycles, data from 2128 cycles from 1731 patients were complete and suitable for analysis.\n\nThe dataset includes detailed patient information such as demographics, cycle characteristics, antral follicle count (AFC), diagnosis, anti-M\u00fcllerian hormone (AMH) where available, all follicle measurements taken during follicular tracking, medications and dosages used for stimulation, the type of oocyte maturation trigger, and the time/date of trigger administration. Outcome data includes the number of eggs collected, the number of embryos frozen, live birth from the fresh cycle, and cumulative live birth from embryos created from this cycle.\n\nThis dataset has not been used in previous papers by the community, as it is specific to this study and the IVF center from which it was sourced. The data was collected to inform centers contemplating monitoring protocol adjustments to minimize face-to-face interactions during the COVID-19 pandemic while maintaining accurate predictive power for optimal, safe, and personalized care.",
  "dataset/splits": "In our study, we employed a 5-fold cross-validation scheme to evaluate the performance of our machine learning models. This approach involved randomly dividing the dataset into five equal-sized subsets. The model was then trained on four of these subsets and evaluated on the remaining fifth subset, a process known as out-of-fold validation. This procedure was repeated five times, with each subset serving as the validation set once, resulting in five estimates of the model's generalization performance.\n\nEach of the five subsets contained an approximately equal number of data points, ensuring a balanced distribution across the folds. This method helps to provide a robust assessment of the model's predictive accuracy and generalizability. The metric used for evaluating the models' performance was the mean squared error (MSE) between the predicted trigger day and the observed trigger day. For each model, we reported the mean MSE across the five subsets along with the associated standard error.",
  "dataset/redundancy": "The dataset used in this study was split using a 5-fold cross-validation scheme at the treatment cycle level. This means the data were randomly divided into five equal-sized subsets. The model was trained on four of these subsets and evaluated on the fifth, out-of-fold subset. This process was repeated for all five possible permutations of the subsets, yielding five estimates of the model's generalization performance. This approach ensures that the training and test sets are independent, as each cycle is only used once for testing and the rest for training. The distribution of the dataset is representative of cycles from women undergoing stimulation prior to oocyte retrieval for IVF, ICSI, or oocyte donation, with a mean age of 33.57 years. The dataset includes detailed information on scan findings, oocyte numbers, and live birth rates, which is crucial for the predictive modeling of trigger timing and risk of ovarian hyperstimulation syndrome (OHSS). The dataset's size and detail allow for robust training and evaluation of machine learning models, aiming to optimize trigger timing and improve outcomes in assisted reproductive technologies.",
  "dataset/availability": "The dataset used in this study is not publicly available in a forum. However, it can be shared upon reasonable request to the corresponding author. This approach ensures that the data remains secure and confidential while allowing for potential collaboration and verification of the study's findings. The dataset includes fully anonymized retrospective electronic data on IVF and ICSI cycles, extracted from a tertiary IVF center in the South of England, UK, from January 1, 2011, to November 30, 2019. The data includes detailed patient information such as demographics, cycle characteristics, antral follicle count (AFC), diagnosis, anti-M\u00fcllerian hormone (AMH), follicle measurements, medications, trigger administration details, and outcome data. The sharing process will be managed to comply with ethical guidelines and data protection regulations, ensuring that the data is used responsibly and securely.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is the Random Forest algorithm. This includes both Random Forest Regressors and Random Forest Classifiers. Random Forest Regressors were employed to predict the day of trigger administration, while Random Forest Classifiers were used to predict the risk of ovarian hyperstimulation syndrome (OHSS).\n\nThe Random Forest algorithm is not new; it is a well-established ensemble learning method that combines multiple decision trees to improve predictive accuracy and control over-fitting. This algorithm is widely used in various fields due to its robustness and ability to handle large datasets with high dimensionality.\n\nThe choice to use Random Forest in this context was driven by its effectiveness in handling regression and classification tasks, as well as its ability to provide feature importance analysis, which helps in understanding the contribution of different variables to the predictions. The implementation of the Random Forest algorithm was done using the sci-kit learn Python library, which is a popular and well-documented toolkit for machine learning.\n\nThe focus of this study is on applying machine learning to optimize IVF treatment protocols, rather than on developing new machine-learning algorithms. Therefore, the algorithm was not published in a machine-learning journal but rather in a journal focused on reproductive medicine and fertility treatments. The primary goal was to demonstrate the practical application of existing machine-learning techniques to improve clinical outcomes in IVF, rather than to contribute novel algorithms to the field of machine learning.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data used for the machine-learning algorithms were fully anonymized retrospective electronic data extracted from a tertiary IVF center. This data included detailed patient information such as demographics, cycle characteristics, antral follicle count (AFC), diagnosis, anti-M\u00fcllerian hormone (AMH) levels, follicle measurements during follicular tracking, medications and dosages used for stimulation, the type of oocyte maturation trigger, and the time and date of trigger administration. Outcome data, including the number of eggs collected, number of embryos frozen, live birth from fresh cycle, and cumulative live birth from embryos created from this cycle, were also included.\n\nFor the machine learning models, data from individual ultrasound scans performed on specific cycle days were combined with demographic data. Missing values, which were less than 1% of the dataset (primarily patient age), were imputed using mean imputation within respective cross-validation folds. This ensured that the models could handle incomplete data without significant bias.\n\nThe primary outcomes focused on predicting the day of trigger administration and assessing the risk of ovarian hyperstimulation syndrome (OHSS). High response was defined as more than 18 follicles or follicles greater than 11 mm in size on the day of oocyte retrieval, as per the ESHRE ovarian stimulation guideline.\n\nTo evaluate the performance of the models, 5-fold cross-validation at the treatment cycle level was used. This involved randomly portioning the data into five equal-size subsets, training the model on four subsets, and evaluating it on the fifth subset. This process was repeated over the five possible permutations of the subsets, yielding five estimates of the models\u2019 generalization performance. The metric used for evaluating the models\u2019 performance in predicting the trigger day was the mean squared error (MSE) between the predicted and actual trigger day. For predicting the risk of OHSS, the area under the receiver operating characteristic curve (AUROC) was used.\n\nHyperparameter tuning was not performed to avoid overfitting the validation set, and reasonable model parameters were used, such as no restriction on tree depth. This approach ensured that the models were robust and generalizable to new data.",
  "optimization/parameters": "The models used in this study employed several input parameters to predict the day of trigger administration. These parameters included patient age, antral follicle count (AFC), and follicle count by size from ultrasound scans. The specific number of parameters (p) used in the model is not explicitly stated, as it can vary depending on the scan day and the number of follicles of each size observed. The models were constructed for each cycle day using the results of individual ultrasound scans performed on the given day, combined with demographic data.\n\nThe selection of these parameters was driven by their relevance to the clinical decision-making process for trigger timing. Patient age and AFC are known to influence ovarian response and trigger timing, while follicle count by size provides real-time data on follicular growth, which is crucial for determining the optimal trigger day. The models primarily use the number of follicles of each size to predict trigger timing, essentially learning the follicle growth rate and clinical decisions to trigger.\n\nThe models were evaluated using 5-fold cross-validation at the treatment cycle level, which helps to ensure that the selected parameters are robust and generalizable. The performance of the models was assessed using the mean squared error (MSE) between the predicted trigger day and the actual trigger day, providing a quantitative measure of the models' predictive accuracy. The models built using data from scans performed later in the cycle became much more predictive, significantly outperforming a baseline predictor that assumed each patient triggers on the mean day of trigger administration. This indicates that the selected parameters are effective in capturing the relevant information for predicting trigger timing.",
  "optimization/features": "The input features used in our models include patient age, antral follicle count (AFC), and follicle count by size for the current scan. These features are used to predict the day of trigger administration. Feature selection was not explicitly performed as part of our modeling process. The features were chosen based on their clinical relevance and availability in the dataset. The models were constructed for each cycle day using the results of individual ultrasound scans performed on the given day and combined with demographic data. Missing values, which were minimal (less than 1% for patient age), were imputed using mean imputation within respective cross-validation folds. This approach ensures that the feature set remains consistent and that any imputation is done in a way that does not introduce bias from the validation set.",
  "optimization/fitting": "The fitting method employed in this study utilized Random Forest Regressors and Classifiers, which are ensemble methods consisting of multiple decision trees. These models were implemented using the sci-kit learn Python library. The Random Forest Regressors were used to predict the day of trigger administration, while Random Forest Classifiers were used to predict the risk of ovarian hyperstimulation syndrome (OHSS).\n\nThe number of parameters in these models is inherently large due to the nature of decision trees, which can have many splits. However, the ensemble approach helps to mitigate the risk of overfitting by averaging the predictions of multiple trees. Additionally, hyperparameter tuning was not performed to ensure that the validation set was not overfitted. Instead, reasonable model parameters were used, such as no restriction on tree depth, which allowed the trees to grow fully and capture the complexity of the data.\n\nTo evaluate the performance of the models and rule out overfitting, 5-fold cross-validation was used at the treatment cycle level. This validation scheme randomly portioned the data into five equal-size subsets, training the model on four subsets and evaluating it on the fifth out-of-fold subset. This process was repeated over the five possible permutations of the subsets, yielding five estimates of the models' generalization performance. The mean squared error (MSE) between the predicted trigger day and the observed trigger day was used as the metric for evaluating the models' performance. The mean MSE across the five subsets and the associated standard error were reported for each model.\n\nUnderfitting was addressed by ensuring that the models had sufficient complexity to capture the underlying patterns in the data. The use of ensemble methods and the lack of restrictions on tree depth allowed the models to learn complex decision rules. Furthermore, the models were constructed for each cycle day using the results of individual ultrasound scans performed on the given day and combined with demographic data, providing a rich set of features for the models to learn from.\n\nMissing values, which were less than 1% of the patients' ages, were imputed using mean imputation within respective cross-validation folds. This approach helped to maintain the integrity of the data and ensure that the models were not underfitted due to missing information.",
  "optimization/regularization": "Regularization techniques were not explicitly mentioned in the context of the models used. However, to prevent overfitting, hyperparameter tuning was not performed. This decision was made to ensure that the validation set was not overfitted. Instead, reasonable model parameters were used, such as no restriction on tree depth. This approach helps in maintaining the generalizability of the models. Additionally, 5-fold cross-validation was employed at the treatment cycle level, which helps in providing a robust estimate of the model's performance on unseen data. This validation scheme involves randomly portioning the data into five equal-size subsets, training the model on four subsets, and evaluating it on the fifth subset. This process is repeated over the five possible permutations of the subsets, yielding five estimates of the model's generalization performance. The mean squared error (MSE) between the predicted trigger day and the observed trigger day was used as the metric for evaluating the models' generalization performance. Ultimately, for each model, the mean MSE across the five subsets and the associated standard error were reported.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are not explicitly detailed in the publication. However, we did use reasonable model parameters, such as no restriction on tree depth, to ensure we did not overfit the validation set. We employed Random Forest Regressors and Classifiers, with 100 individual decision trees in each ensemble, and used 5-fold cross-validation at the treatment cycle level to evaluate model performance. The metric used for evaluating the models\u2019 generalization performance was the mean squared error (MSE) for regression tasks and the area under the receiver operating characteristic curve (AUROC) for classification tasks.\n\nRegarding the availability of model files and optimization parameters, these are not publicly shared in the publication. The dataset used for this study will be shared on reasonable request to the corresponding author. However, specific details about the optimization schedule, model files, and exact hyper-parameter configurations are not provided in the paper.",
  "model/interpretability": "The models employed in this study are based on Random Forest Regressors and Classifiers, which are ensemble learning methods. These models are not entirely black-box, as they offer some level of interpretability through feature importance analysis. For instance, in predicting the trigger day, the models associate a lower antral follicle count (AFC) with a slightly later trigger day, although this is not a very strong predictor. The models primarily use the number of follicles of each size to predict trigger timing, essentially learning the follicle growth rate and clinical decisions to trigger.\n\nThe decision rules in these models are learned during training, where the algorithm selects decisions that minimize the mean squared error between the predicted and actual trigger day. This process allows for some transparency, as one can examine which features (e.g., follicle counts, patient age) are most influential in making predictions. However, the exact decision paths within each tree in the forest are complex and numerous, making the models less transparent than simpler, rule-based models.\n\nIn summary, while the models provide insights into feature importance, they are not fully transparent. The use of ensemble methods like Random Forests offers a balance between predictive power and interpretability, but the internal workings remain somewhat opaque.",
  "model/output": "The model employed in this study is primarily a regression model, specifically a Random Forest Regressor, used to predict the day of trigger administration. This model operates by creating an ensemble of decision tree regressors, which make a series of if-then-else decisions to forecast the trigger day for a given instance. The final prediction is the average of the trigger days predicted by 100 individual decision trees, with variance introduced through bootstrapping the training data.\n\nAdditionally, a binary classification model, the Random Forest Classifier, was developed to predict the risk of ovarian hyperstimulation syndrome (OHSS). This classifier provides the probability that a patient belongs to the class of being at risk of OHSS or not, based on the class fraction in the relevant leaf of the decision trees.\n\nThe performance of the regression model was evaluated using the mean squared error (MSE) between the predicted and actual trigger days, while the classification model's performance was assessed using the area under the receiver operating characteristic curve (AUROC). The regression model showed good predictive power for trigger timing, particularly on Days 5, 6, and 7 of stimulation. The classification model demonstrated moderate accuracy in predicting over-response, with a mean AUROC of 0.77.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our models was conducted using a robust 5-fold cross-validation scheme at the treatment cycle level. This approach involved randomly dividing the data into five equal subsets. The model was then trained on four of these subsets and evaluated on the fifth, out-of-fold subset. This process was repeated across all five possible permutations of the subsets, yielding five estimates of the models' generalization performance. The primary metric used for evaluating the models' performance was the mean squared error (MSE) between the predicted trigger day and the observed trigger day. For each model, we reported the mean MSE across the five subsets along with the associated standard error. This method ensured that our models were thoroughly tested and that their performance was reliable and generalizable. Additionally, we created a baseline trigger administration day predictor for comparison, which predicted the mean trigger day for all cycles in the training set that had a scan on the given cycle day. This baseline helped to contextualize the performance of our machine learning models.",
  "evaluation/measure": "For the evaluation of our models, we primarily focused on two key performance metrics: Mean Squared Error (MSE) and Area Under the Receiver Operating Characteristic Curve (AUROC).\n\nThe MSE was used to evaluate the performance of our Random Forest Regressors in predicting the day of trigger administration. This metric measures the average of the squares of the errors\u2014that is, the average squared difference between the estimated values and the actual value. By using MSE, we could quantify the accuracy of our predictions, with lower values indicating better performance. We reported the mean MSE across five subsets obtained through 5-fold cross-validation, along with the associated standard error, to provide a robust estimate of our models' generalization performance.\n\nFor predicting the risk of ovarian hyperstimulation syndrome (OHSS), we framed the problem as a binary classification task. In this context, we used the AUROC as our primary evaluation metric. The AUROC provides a single scalar value that represents the ability of the model to distinguish between the two classes (patients at risk of OHSS and those not at risk). An AUROC of 1 indicates perfect classification, while an AUROC of 0.5 suggests no discriminative ability. We evaluated the AUROC across validation folds to assess the consistency and reliability of our models' performance.\n\nThese metrics are widely used in the literature for evaluating regression and classification models, respectively. The MSE is a standard choice for regression tasks due to its sensitivity to larger errors, while the AUROC is a well-established metric for binary classification problems, particularly when dealing with imbalanced datasets. By reporting these metrics, we aim to provide a clear and representative assessment of our models' performance, enabling comparisons with other studies in the field.",
  "evaluation/comparison": "A comparison to simpler baselines was performed. Specifically, a baseline trigger administration day predictor was created. This baseline predictor estimates the mean trigger day for all cycles in the training set that had a scan on the given cycle day. This approach serves as a straightforward benchmark to evaluate the performance of the more complex machine learning models developed in the study. The evaluation metric used for this comparison was the mean squared error (MSE) between the predicted trigger day and the observed trigger day. This comparison helps to highlight the improvements and added value of the machine learning models over simpler, more intuitive methods.",
  "evaluation/confidence": "The evaluation of our models included confidence intervals for the performance metrics. For the trigger day prediction models, we reported the mean Mean Squared Error (MSE) across five subsets obtained through 5-fold cross-validation, along with the associated standard error. This provides an indication of the variability and confidence in the model's performance.\n\nFor the over-response predictive models, we evaluated performance using the Area Under the Receiver Operating Characteristic Curve (AUROC). The error bars in the evaluation figures represent the standard deviation of the model\u2019s performance across validation folds, which gives an indication of the confidence intervals of the model\u2019s performance.\n\nThe results demonstrate that models built using data from scans performed later in the cycle become much more predictive and significantly outperform the baseline trigger day prediction. From Day 5 of stimulation, there is very low standard deviation of model performance across the validation folds, indicating high confidence in the model's predictions.\n\nStatistical significance was assessed by comparing the performance of our models to a baseline predictor that simply assumes each patient triggers on the mean day of trigger administration of historical cycles. The models using data from scans performed on stimulation Days 4 and beyond significantly outperformed this baseline, as evidenced by the lower MSE and the close alignment of predicted trigger days to actual trigger days in the evaluation figures.\n\nOverall, the performance metrics and statistical analyses provide strong evidence that our models are superior to the baseline predictor and have high confidence in their predictions, particularly for scans performed later in the stimulation cycle.",
  "evaluation/availability": "The dataset used in this study will be shared upon reasonable request to the corresponding author. This approach ensures that the data can be accessed by other researchers for further validation or replication of the findings. However, the specific details about the license under which the data will be shared are not provided. This means that interested parties should contact the corresponding author to discuss the terms of data access and any potential restrictions that may apply."
}