{
  "publication/title": "Synergistic Approach of Interfacial Layer Engineering and READ-Voltage Optimization in HfO<sub>2</sub>-Based FeFETs for In-Memory-Computing Applications.",
  "publication/authors": "Raffel Y, De S, Lederer M, Olivo RR, Hoffmann R, Thunder S, Pirro L, Beyer S, Chohan T, K\u00e4mpfe T, Seidel K, Heitmann J",
  "publication/journal": "ACS applied electronic materials",
  "publication/year": "2022",
  "publication/pmid": "36439397",
  "publication/pmcid": "PMC9686141",
  "publication/doi": "10.1021/acsaelm.2c00771",
  "publication/tags": "- Ferroelectric Field-Effect Transistors (FeFETs)\n- Neuromorphic Computing\n- Low-Frequency Noise\n- Endurance and Retention\n- Interface Engineering\n- Synaptic Devices\n- Neural Network Simulation\n- MNIST Dataset\n- Inference Operations\n- Deep Learning Applications",
  "dataset/provenance": "The dataset used in our study is the MNIST dataset, which is a widely recognized dataset in the machine learning community. It consists of 70,000 grayscale images of handwritten digits, each of size 28x28 pixels. The dataset is split into 60,000 training images and 10,000 testing images. Each image is labeled with a digit from 0 to 9.\n\nThe MNIST dataset has been extensively used in previous research for training and testing various machine learning models, particularly in the field of neural networks and deep learning. Its simplicity and well-defined structure make it an ideal benchmark for evaluating the performance of new algorithms and hardware implementations.\n\nIn our experiments, we utilized the MNIST dataset to evaluate the impact of low-frequency noise and retention degradation on the inference accuracy of neural networks built with FeFET-based synaptic devices. The dataset's images were reshaped to a size of 20x20 pixels to simplify the hardware implementation task, resulting in a neural network architecture with 400 input nodes. The neural network's architecture also includes 100 hidden nodes and 10 output nodes, corresponding to the 10 digit classes.",
  "dataset/splits": "Not applicable.",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our work is the Adam optimizer, which is a widely-used class of stochastic gradient descent algorithms. Adam is known for its efficiency and effectiveness in training deep learning models, particularly in handling sparse gradients on noisy problems.\n\nThe Adam optimizer is not a new algorithm; it was introduced by Diederik P. Kingma and Jimmy Ba in their 2014 paper \"Adam: A Method for Stochastic Optimization.\" Given its established reputation and widespread adoption in the machine learning community, there was no necessity to publish it in a machine-learning journal within the context of our study.\n\nOur focus was on evaluating the impact of low-frequency noise and retention degradation on FeFETs, specifically in neuromorphic applications. The Adam optimizer was chosen for its robustness and ability to handle the complexities of our neural network simulations, ensuring that the optimization process was both efficient and reliable. This allowed us to concentrate on the primary objectives of our research, which involved assessing the performance of FeFET-based synaptic devices in neural network architectures.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, we focused on evaluating the performance of ferroelectric field-effect transistors (FeFETs) in neuromorphic computing applications, specifically for inference operations. For the neural network simulation, we used the MNIST dataset, which consists of handwritten digits. To simplify the hardware implementation, each image in the dataset was reshaped to a size of 20 \u00d7 20 pixels. This preprocessing step resulted in a neural network architecture with 400 input nodes, corresponding to the pixel values of the reshaped images.\n\nThe neural network architecture comprised three layers: an input layer with 400 nodes, a hidden layer with 100 nodes, and an output layer with 10 nodes, each representing a digit from 0 to 9. We considered offline training scenarios for the neural network, where the synaptic weights were optimized in software and then written into the hardware. This approach allowed us to focus on inference-only operations, which are crucial for maintaining high accuracy in the presence of noise and retention degradation.\n\nDuring the offline training process, we employed the back-propagation algorithm with the Adam optimizer to minimize the cost function. The step function was used as the activation function during forward propagation, while the sigmoid function was used during back-propagation. After training, the synaptic weights, represented by the channel conductance of the FeFETs, were updated on the hardware using a single-shot programming pulse. These weights were normalized between the minimum and maximum conductance values to ensure optimal performance during inference.",
  "optimization/parameters": "In our study, the model utilized several key input parameters to optimize the performance of FeFETs, particularly for neuromorphic computing applications. The primary parameters included the effective mobility (\u03bc_eff), the effective oxide capacitance (C_ox), and the Coulomb scattering factor (\u03b1). These parameters are crucial for characterizing the low-frequency noise behavior of the devices, which is essential for understanding their reliability and performance.\n\nThe selection of these parameters was based on their established significance in the field of semiconductor device physics and their direct impact on the noise characteristics of FeFETs. The effective mobility and oxide capacitance are fundamental properties that influence the device's electrical behavior, while the Coulomb scattering factor accounts for the interactions between charge carriers and defects in the material. By focusing on these parameters, we aimed to provide a comprehensive analysis of the noise sources and their mitigation strategies in FeFETs.\n\nAdditionally, the transconductance (g_m) of the MOSFET was considered, which is defined by the change in drain current (I_D) with respect to the gate-to-source voltage (V_gs). This parameter is mathematically represented as g_m = dI_D/dV_gs and is essential for understanding the device's amplification capabilities and noise performance.\n\nThe power spectral density (S_ID) and the input gate voltage noise (S_VG) were also critical parameters in our analysis. S_ID represents the drain current change in the frequency domain, translated via Fourier transformation, and is described by the equation S_ID = S_VG * g_m^2. This relationship highlights the importance of g_m in determining the noise characteristics of the device.\n\nIn summary, the model employed a set of well-established parameters that are pivotal for characterizing the noise behavior and performance of FeFETs. These parameters were selected based on their theoretical and experimental relevance, ensuring a robust and comprehensive analysis of the devices' reliability and suitability for neuromorphic computing applications.",
  "optimization/features": "Not applicable.",
  "optimization/fitting": "Not applicable.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "Not applicable.",
  "model/interpretability": "The model discussed in this publication is not a traditional machine learning model but rather a hardware-based implementation using FeFET (Ferroelectric Field-Effect Transistor) devices. These devices are designed to mimic the behavior of synapses in neural networks, making them highly interpretable in the context of neuromorphic computing.\n\nThe transparency of the model stems from its physical structure and operation. The FeFET devices have a well-defined architecture with a ferroelectric layer of silicon-doped hafnium oxide and an interfacial layer of either SiO2 or SiON. The behavior of these devices can be directly observed and measured, providing clear insights into their functioning.\n\nFor example, the conductance of the FeFET devices, which represents the synaptic weights in the neural network, can be experimentally calibrated and measured. This conductance value is used to simulate the performance of the neural network, making the relationship between the hardware and the neural network's behavior transparent.\n\nAdditionally, the impact of low-frequency noise and retention degradation on the devices can be evaluated, providing further interpretability. The devices with a SiON interface demonstrate high immunity to variations and maintain an inference accuracy of over 96% without retraining, which can be directly attributed to their physical properties.\n\nThe neural network architecture used in the simulations is also transparent. It consists of three layers: 400 input nodes, 100 hidden nodes, and 10 output nodes. The synaptic weights, in terms of channel conductance of FeFETs, are updated on the hardware using a single-shot programming pulse, making the process of weight update interpretable.\n\nIn summary, the model is not a black box but a transparent system where the physical properties of the FeFET devices and their behavior can be directly observed and measured, providing clear examples of interpretability.",
  "model/output": "The model used in our study is a classification model. Specifically, we employed a Multilayer Perceptron (MLP) neural network architecture for classifying handwritten digits from the MNIST dataset. The MLP consists of three layers: an input layer with 400 nodes, a hidden layer with 100 nodes, and an output layer with 10 nodes, each corresponding to a digit from 0 to 9. The network was trained offline using the back-propagation algorithm with the Adam optimizer to minimize the cost function. During forward-propagation, a step function was used as the activation function, while a sigmoid function was employed during back-propagation. After training, the synaptic weights, represented by the channel conductance of FeFETs, were updated on the hardware using a single-shot programming pulse. The weights were normalized between the minimum and maximum conductance values to ensure optimal performance during inference. The primary focus was on inference-only operations, given the power-hungry nature of online training and the need for high endurance. The model's performance was evaluated based on its ability to maintain high inference accuracy despite variations in device conductance, low-frequency noise, and retention degradation.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the Neurosim simulation platform used in our work is not publicly released. However, the neural network simulation was conducted using an experimentally calibrated conductance value with variation statistics to evaluate the impact of low-frequency noise and retention degradation on FeFETs, specifically for neuromorphic applications. The neural network architecture comprised three layers: 400 input nodes, 100 hidden nodes, and 10 output nodes. The back-propagation algorithm with the optimizer Adam was adopted to minimize the cost function during offline training. The step function was used as an activation function during forward-propagation, and the sigmoid function was used during back-propagation. After offline training, the synaptic weights, in terms of channel conductance of FeFETs, were updated on the hardware using a single-shot programming pulse. The synaptic weights were normalized between the minimum and maximum values.\n\nThe neural network was trained using the MNIST dataset, which consists of handwritten digits. The input images were reshaped to a size of 20 \u00d7 20 pixels to simplify the hardware implementation task. The performance of the FeFET-based synaptic devices was simulated using this architecture. The detailed method of low-frequency noise investigation is demonstrated in previous work.",
  "evaluation/method": "The evaluation method for our study involved a comprehensive approach to assess the impact of low-frequency noise and retention degradation on ferroelectric field-effect transistors (FeFETs), particularly for neuromorphic applications. We employed the Neurosim simulation platform to evaluate system-level performance. This platform allowed us to simulate the multilevel perception (MLP) neural network performance using the MNIST dataset.\n\nThe neural network architecture consisted of three layers: 400 input nodes, 100 hidden nodes, and 10 output nodes. We focused on offline training scenarios, as online training, while beneficial for mitigating conductance drift, is power-hungry and requires high endurance. The back-propagation algorithm with the Adam optimizer was used to minimize the cost function during offline training. The step function was used as the activation function during forward-propagation, and the sigmoid function was used during back-propagation.\n\nAfter offline training, the synaptic weights, represented by the channel conductance of FeFETs, were updated on the hardware using a single-shot programming pulse. These weights were normalized between the minimum and maximum conductance values. The experimentally calibrated conductance values, along with variation statistics, were used to simulate the neural network's performance.\n\nAdditionally, we conducted electrical characterization using a B1500A Semiconductor Analyzer. The devices were subjected to wake-up cycling with specific voltage pulses before READ-WRITE operations. The devices were programmed to binary levels using 500 ns pulses at the gate terminal, with positive and negative pulses programming the devices to low and high threshold voltage states, respectively. Each WRITE pulse was preceded by a RESET pulse to ensure proper programming.\n\nNoise and reliability characterization were performed using a ProPlus noise measurement system. We analyzed input-referred gate noise (SvG) and output drain current noise (SID) to understand the noise behavior of the devices. SvG is a crucial figure of merit that provides information for choosing the optimal operating point. The power spectral density was described using an analytical expression that includes terms for flat band voltage spectral density, effective mobility, effective oxide capacitance, and Coulomb scattering factor.\n\nFurthermore, endurance measurements were conducted using electric field cycling with amplitudes in the \u00b16 V range and a pulse width of 500 ns. This helped us assess the devices' reliability under stress conditions. The detailed method of low-frequency noise investigation is demonstrated in previous work.",
  "evaluation/measure": "In our evaluation, we focused on several key performance metrics to assess the reliability and effectiveness of our FeFET devices, particularly in the context of neuromorphic computing applications.\n\nWe began by examining the low-frequency noise characteristics, which are crucial for understanding the device's stability and performance. The noise spectrum was analyzed using the output power spectral density (SID) and the input gate voltage noise (SvG). These metrics provide insights into the intrinsic stochasticity and defect sites within the ferroelectric thin film, which can capture electrons or holes from the channel or gate side.\n\nEndurance and retention characteristics were also thoroughly evaluated. Endurance refers to the number of write cycles a device can withstand before degradation, while retention measures how well the device maintains its state over time. These metrics are essential for assessing the device's suitability for long-term data storage and repeated read/write operations.\n\nFor neuromorphic computing applications, we specifically looked at the impact of these performance metrics on neural network operations. We considered both training and inference operations. During training, the synaptic weights are constantly updated, necessitating high write endurance. For inference operations, stable data retention and low read variations are critical to ensure accurate and reliable performance without the need for repeated retraining.\n\nWe also evaluated the inference accuracy of multilevel perception (MLP) neural networks using the MNIST dataset. The baseline inference accuracy was established at 98.5%. We then assessed how device-to-device variation, low-frequency noise, and retention degradation affected this accuracy. Our findings showed that devices with a SiON interface demonstrated high immunity to these variations, maintaining an inference accuracy of over 96% without retraining, even in the presence of noise and retention degradation.\n\nAdditionally, we considered the on-current to off-current ratio (I_ON/I_OFF), which is a key indicator of the device's switching performance and energy efficiency. A high I_ON/I_OFF ratio is desirable for achieving better inference accuracy in neural networks.\n\nIn summary, our performance measures included low-frequency noise characteristics, endurance, retention, inference accuracy, and the on-current to off-current ratio. These metrics provide a comprehensive evaluation of our FeFET devices' reliability and effectiveness, particularly in the context of neuromorphic computing applications. Our results align with and build upon existing literature, demonstrating the robustness and potential of our devices for advanced computing tasks.",
  "evaluation/comparison": "In our evaluation, we focused on assessing the performance of our FeFET-based synaptic devices in the context of neural network applications, particularly for inference operations. We did not perform a direct comparison with publicly available methods or benchmark datasets in the traditional sense, as our work is more foundational, dealing with the fabrication and characterization of FeFET devices.\n\nHowever, we did compare the performance of FeFETs with different interfacial layers (SiO2 and SiON) to understand their impact on neural network accuracy. This internal comparison provided insights into how different device characteristics affect system-level performance.\n\nFor the neural network simulation, we used the MNIST dataset, which is a standard benchmark for handwritten digit recognition. The neural network architecture consisted of 400 input nodes, 100 hidden nodes, and 10 output nodes. We considered offline training scenarios, using the back-propagation algorithm with the Adam optimizer to minimize the cost function. The activation functions used were the step function for forward-propagation and the sigmoid function for back-propagation.\n\nWe evaluated the cumulative impact of device variation, low-frequency noise, and retention degradation on the inference accuracy of multilevel perception (MLP) neural networks. The software baseline for the inference operation was 98.5%. Our results showed that FeFETs with SiON-based interfacial layers demonstrated high immunity to variations and maintained an inference accuracy of over 96% without retraining for the MNIST dataset, even in the presence of noise and retention degradation.\n\nWhile we did not compare our methods to simpler baselines in the traditional machine learning sense, our work provides a foundational comparison of different FeFET configurations, which is crucial for understanding their potential in neuromorphic computing applications.",
  "evaluation/confidence": "The evaluation of the FeFET devices and their application in neuromorphic computing involves several performance metrics, but specific details about confidence intervals are not explicitly mentioned. However, the results presented are statistically significant in demonstrating the superiority of FeFETs with SiON interfaces over those with SiO2 interfaces, particularly in terms of endurance, retention, and inference accuracy.\n\nThe study evaluates the impact of low-frequency noise, retention degradation, and device variations on the inference accuracy of neural networks using the MNIST dataset. The software baseline for inference accuracy is established at 98.5%. The results show that FeFETs with SiON interfaces maintain an inference accuracy of over 96% after 10 years of programming without retraining, indicating high immunity to variations and degradation.\n\nThe statistical significance of these results is supported by the consistent performance of SiON-based FeFETs across different metrics. The high on-current to off-current ratio in these devices contributes to better inference accuracy. Additionally, the cumulative impact of device variation, flicker noise, and retention degradation is evaluated, further reinforcing the reliability and performance of SiON-based FeFETs.\n\nWhile explicit confidence intervals are not provided, the detailed experimental setup and the comprehensive evaluation of various factors suggest a robust statistical foundation for the claims made. The use of established methods for low-frequency noise investigation and the calibration of conductance values with variation statistics add to the credibility of the results.",
  "evaluation/availability": "Not enough information is available."
}