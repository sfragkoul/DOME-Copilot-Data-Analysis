{
  "publication/title": "Graph Convolutional Neural Networks for Histologic Classification of Pancreatic Cancer.",
  "publication/authors": "Wu W, Liu X, Hamilton RB, Suriawinata AA, Hassanpour S",
  "publication/journal": "Archives of pathology & laboratory medicine",
  "publication/year": "2023",
  "publication/pmid": "36669509",
  "publication/pmcid": "PMC10356903",
  "publication/doi": "10.5858/arpa.2022-0035-oa",
  "publication/tags": "- Pancreatic cancer\n- Histopathologic slides\n- Deep learning\n- Graph convolutional networks\n- Whole slide images\n- Pathology\n- Machine learning\n- Medical imaging\n- Diagnostic tools\n- Clinical decision support",
  "dataset/provenance": "The dataset used in our study consists of 143 digitized formalin-fixed, paraffin-embedded, hematoxylin and eosin\u2013stained whole slide images. These slides were collected from Dartmouth-Hitchcock Medical Center in Lebanon, New Hampshire. The slides were digitized using an Aperio AT2 scanner at \u00d720 resolution (0.5 \u03bcm/pixel). The identification of these slides was facilitated through structured cytopathology diagnosis data from the laboratory information management system, supplemented by a full-text pathology report search for further clarification.\n\nThe dataset includes slides categorized into three main classes: negative, positive, and neoplastic. The positive cases are specifically pancreatic ductal adenocarcinoma (PDAC), while other malignancies like lymphomas, acinar cell carcinoma, and neuroendocrine carcinoma were excluded due to insufficient training cases. The neoplastic class is represented by neuroendocrine tumors, excluding neuroendocrine carcinomas and rarer tumors like solid pseudopapillary tumors. Cystic lesions were also excluded from the neoplastic category due to their reliance on cyst fluid chemistry studies and clinical information, which our neural network does not consider. The negative class encompasses normal cells, blood, fibrin, mild atypia associated with inflammation, leukocytes, and benign gastric or duodenal epithelium due to procedural artifacts. Atypical and suspicious cases were generally excluded to avoid interobserver variability, except in rare instances where they met specific criteria for inclusion in the negative category.\n\nThe slides were partially annotated by expert pathologists to indicate the locations of pancreatic cancer subtypes. These annotations were used as reference standards for developing and evaluating our patch classification models. The dataset was then randomly partitioned into training, validation, and test sets, comprising both annotated and unannotated slides. The distribution of the annotated images is detailed in the study, ensuring a balanced representation across the different classes.",
  "dataset/splits": "The dataset was partitioned into three distinct splits: training, validation, and test sets. This partitioning was applied to both annotated and unannotated slides.\n\nIn the training set, there were 13 neoplastic positive slides, 18 neoplastic negative slides, and 32 slides with neoplastic cells present. Additionally, there were 28 positive slides and 30 negative slides. The validation set consisted of 2 neoplastic positive slides, 3 neoplastic negative slides, and 5 slides with neoplastic cells present. It also included 4 positive slides and 5 negative slides. The test set comprised 7 neoplastic positive slides, 9 neoplastic negative slides, and 16 slides with neoplastic cells present. Furthermore, it had 11 positive slides and 12 negative slides.\n\nOverall, the total number of annotated slides was 22 neoplastic positive, 30 neoplastic negative, and 53 with neoplastic cells present. Additionally, there were 43 positive slides and 47 negative slides. The dataset included a total of 143 digitized slides, which were collected and reviewed by expert pathologists to ensure the quality and accuracy of the annotations and labels.",
  "dataset/redundancy": "The dataset used in our study consists of 143 digitized slides, which were partitioned into training, validation, and test sets. The distribution of these slides is detailed in Table 2. The dataset includes both annotated and unannotated slides, with a total of 52 annotated slides and 91 unannotated slides. These slides were randomly partitioned to ensure independence between the training, validation, and test sets.\n\nTo enforce the independence of the datasets, we employed a random partitioning strategy. This approach helps to mitigate any potential bias that could arise from non-random splitting methods. The training set is used to develop the model, the validation set is used to tune hyperparameters and prevent overfitting, and the test set is used to evaluate the final model's performance.\n\nThe distribution of our dataset differs from some previously published machine learning datasets in this domain. Unlike other studies that may focus solely on positive or neoplastic cases, our dataset includes a significant number of negative cases. This inclusion is crucial for training a robust patch-level classifier that can accurately distinguish between benign and malignant tissues. The negative class in our dataset is broad, encompassing normal cells, inflammation, and other benign findings, which reflects the diversity of clinical scenarios.\n\nThe independence of the training and test sets is crucial for the reliability of our model's performance evaluation. By ensuring that the test set is completely independent from the training and validation sets, we can provide an unbiased assessment of the model's generalizability to new, unseen data. This independence is enforced through the random partitioning of the slides, which helps to ensure that the model's performance is not overestimated due to data leakage or other forms of bias.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is graph convolutional networks (GCNs). Specifically, we modified the graph model proposed by Zhang et al. and incorporated self-attention global pooling layers. This approach is not entirely new, as GCNs have been previously used in various applications. However, our implementation is tailored for the specific task of whole slide image classification in pathology.\n\nThe reason this algorithm was not published in a machine-learning journal is that our primary focus is on its application in the medical field, particularly in pathology. The modifications and the specific architecture we used are designed to address the unique challenges of analyzing whole slide images, such as incorporating positional information of each patch and the global structure of the slide. This makes our work more relevant to the medical and computational pathology communities rather than the general machine-learning audience.\n\nOur model, named Slide2Graph, was trained for 200 epochs with an initial learning rate of 0.001 and learning rate annealing. The architecture includes three graph convolutional layers that update node features by aggregating information from neighboring nodes. After these layers, a self-attention pooling layer selects the top 50% highly weighted nodes, which are then processed through global mean pooling and maximum pooling. The final output is generated by a fully connected layer followed by a SoftMax layer, providing the predicted whole slide class probabilities. The code for Slide2Graph is publicly available, facilitating reproducibility and further development by the research community.",
  "optimization/meta": "The model described in our publication is not a traditional meta-predictor that uses data from other machine-learning algorithms as input. Instead, it employs a graph-based approach called Slide2Graph, which incorporates positional information of each patch and the global structure of the whole slide image. This method modifies a graph model proposed by Zhang et al. and uses self-attention global pooling layers.\n\nThe Slide2Graph architecture consists of three graph convolutional layers (GCN layers) that update node features by aggregating information from a node and its neighboring nodes. This process ensures that each node contains information about its surrounding neighborhood. The outputs of these GCN layers are concatenated, and a self-attention pooling layer selects the top 50% highly weighted nodes to determine the class of a graph. Global mean pooling and maximum pooling are then applied to these top nodes, and the results are concatenated. Finally, a fully connected layer and a SoftMax layer output the predicted whole slide class probabilities.\n\nTo evaluate the efficacy of Slide2Graph, we implemented other models for comparison, including DeepSlide, a decision tree, a random forest, and Adaboost. These models were used to aggregate patch information, but they are not part of the Slide2Graph architecture itself. The training data for Slide2Graph is independent, as it was evaluated on a holdout test set that was not used during model training. This ensures that the performance metrics, such as precision, recall, F1 score, and area under the relative operating characteristic curve, are reliable and not biased by the training process.",
  "optimization/encoding": "In our study, the data encoding process began with the digitization of formalin-fixed, paraffin-embedded, hematoxylin and eosin\u2013stained whole slide images using an Aperio AT2 scanner at \u00d720 resolution. These slides were carefully selected and reviewed by expert pathologists to ensure the quality and accuracy of the labels.\n\nThe preprocessing pipeline, termed Slide2Graph, involved several key steps. Initially, tissue regions were identified, and the background was removed to focus on the relevant areas. Subsequently, a sliding-window method was employed to generate small, fixed-size patches from each whole slide image. These patches were then processed using a convolutional neural network model to extract corresponding features.\n\nTo facilitate the machine-learning algorithm, a graph was constructed where each patch was considered a node. These nodes were connected based on their spatial proximity, specifically linking each node with its four nearest neighbors. This graph structure allowed for the integration of contextual information, enabling the model to capture both local and global patterns within the slides.\n\nThe annotated slides were partially labeled by domain expert pathologists to indicate the locations of pancreatic cancer subtypes. These annotations served as reference standards for developing and evaluating our patch classification models. The distribution of annotated images was carefully managed, with slides randomly partitioned into training, validation, and test sets to ensure robust model performance.\n\nThis preprocessing and encoding strategy ensured that the data was in a suitable format for the machine-learning algorithm, enabling effective feature extraction and classification.",
  "optimization/parameters": "In our study, the model architecture incorporates several parameters, primarily determined by the graph convolutional network (GCN) layers and the fully connected layers. Specifically, the model uses three GCN layers, each generating new node representations by aggregating features from a node and its neighboring nodes. The outputs of these GCN layers are concatenated and passed through a self-attention pooling layer, which selects the top 50% highly weighted nodes. These selected nodes undergo global mean pooling and maximum pooling, and the results are concatenated before being fed into a fully connected layer followed by a SoftMax layer for final classification.\n\nThe selection of the number of parameters was guided by empirical performance and computational efficiency. The initial learning rate was set to 0.001, with learning rate annealing applied during training over 200 epochs. This setup was chosen based on systematic grid searches and cross-validation to ensure optimal performance. The model's architecture, including the number of GCN layers and the pooling mechanisms, was designed to balance complexity and performance, ensuring that the model could effectively capture both local and global features of the whole slide images.\n\nThe code for the Slide2Graph model is publicly available, allowing for further inspection and replication of the results. This transparency ensures that the community can verify the model's parameters and their selection process.",
  "optimization/features": "The input features for our model are derived from high-resolution whole slide histology images. We use a sliding window strategy to extract small, fixed-size patches of 224 \u00d7 224 pixels from these images. These patches serve as the input features for our deep convolutional neural network, specifically a ResNet-18 model. The number of features, f, corresponds to the dimensionality of the output from the ResNet-18 model, which is typically 512 for the ResNet-18 architecture.\n\nFeature selection in the traditional sense was not performed. Instead, we rely on the ResNet-18 model to automatically learn and extract relevant features from the patches. The model is trained on the annotated training set, and the features are extracted based on the patches generated from this set. This approach ensures that the feature extraction process is consistent with the training data, avoiding any data leakage that could occur if feature selection were performed on the entire dataset.",
  "optimization/fitting": "The fitting method employed in our study involved a two-part pipeline designed to handle the high-resolution whole slide images efficiently. The first part utilized a deep convolutional neural network, specifically a ResNet-18 model, to extract high-dimensional features from small, fixed-size patches generated using a sliding-window strategy. This approach was necessary due to the large size of the whole slide images and the memory constraints of available hardware.\n\nGiven the high resolution of the whole slide images, directly training a model on these images would have been infeasible. Therefore, the sliding-window strategy allowed us to generate manageable patches, ensuring that critical histologic features were not lost due to downsampling. This method resulted in a substantial number of patches: 3091 neoplastic patches, 6275 positive patches, and 94,633 negative patches in the training set. The ResNet-18 model was chosen for its superior performance in patch-level classification on our validation set.\n\nTo address the potential issue of overfitting, given the large number of parameters in the deep neural network relative to the number of training points, we implemented several strategies. First, we used data augmentation techniques to increase the diversity of the training set. Second, we employed learning rate annealing during the training process to ensure stable convergence. Additionally, we used a bootstrapping approach to randomly sample subsets from our test set and performed a 2-tailed Student t-test to examine the statistical significance of the differences among F1 scores from various methods. This rigorous evaluation helped to ensure that our model's performance was robust and not merely a result of overfitting.\n\nUnderfitting was mitigated by the use of a deep residual neural network, which is known for its ability to capture complex patterns in data. The ResNet-18 model, with its 18 layers, provided a sufficient number of parameters to learn the intricate features of the histologic images. Furthermore, the graph convolutional neural network used in the second part of our pipeline aggregated patch-level features and their positional information, capturing both local and global patterns in the whole slide images. This dual-layer approach ensured that our model could generalize well to unseen data, avoiding underfitting.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and ensure the robustness of our model. One of the key methods used was learning rate annealing, which helps in adjusting the learning rate during training to prevent the model from converging too quickly to a suboptimal solution. This technique allows the model to make finer adjustments to the weights as training progresses, thereby improving generalization.\n\nAdditionally, we utilized dropout layers within our neural network architecture. Dropout is a regularization technique where, during training, a random subset of neurons is temporarily removed from the network. This forces the network to learn redundant representations and prevents it from becoming too reliant on any single neuron, thus reducing overfitting.\n\nWe also implemented early stopping, a technique that monitors the model's performance on a validation set and stops training when the performance stops improving. This helps in preventing the model from overfitting to the training data by halting the training process at the optimal point.\n\nFurthermore, data augmentation was employed to artificially increase the diversity of our training dataset. This involved applying random transformations such as rotations, flips, and zooms to the training images. By doing so, we helped the model to generalize better to unseen data, as it was exposed to a wider variety of examples during training.\n\nLastly, we used a relatively small initial learning rate and a batch normalization technique. Batch normalization helps in stabilizing and accelerating the training process by normalizing the inputs of each layer. This technique also acts as a form of regularization, making the model more robust to the initial weight values and reducing the likelihood of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, the model was trained for 200 epochs with an initial learning rate of 0.001 and learning rate annealing. These details are provided to ensure reproducibility of our results.\n\nThe model files and optimization parameters are publicly available. The Slide2Graph code, which includes the implementation of our graph model architecture, is accessible on GitHub at [https://github.com/BMIRDS/Slide2Graph](https://github.com/BMIRDS/Slide2Graph). This repository contains the necessary code and configurations to replicate our experiments. The license under which this code is shared allows for open access and use, facilitating further research and development in this area.",
  "model/interpretability": "The model we developed, Slide2Graph, incorporates several design choices that enhance its interpretability compared to traditional black-box models. Unlike many deep learning approaches that treat the entire slide as a single input, our model breaks down the slide into smaller patches and constructs a graph structure. This graph structure allows us to consider both the local features of each patch and the global context of the entire slide.\n\nOne key aspect of interpretability in our model is the use of graph convolutional layers. These layers update the node features by aggregating information from a node and its neighboring nodes. This means that each node in the graph contains information about its surrounding neighborhood, providing a more localized understanding of the slide's features. After processing through multiple graph convolutional layers, the model uses a self-attention pooling layer to select the top 50% highly weighted nodes that are most influential in determining the class of the graph. This selection process highlights the most relevant regions of the slide, making it easier to understand which parts of the slide are contributing most to the final prediction.\n\nAdditionally, the model's architecture includes a fully connected layer followed by a SoftMax layer, which outputs the predicted whole slide class probabilities. This probabilistic output provides a measure of confidence for each class, further aiding in interpretability. By examining the attention weights and the contributions of different nodes, pathologists can gain insights into which regions of the slide are most indicative of a particular diagnosis.\n\nIn summary, while Slide2Graph leverages advanced machine learning techniques, its design choices\u2014such as the use of graph structures, graph convolutional layers, and self-attention pooling\u2014enhance its interpretability. These features allow for a more transparent understanding of how the model arrives at its predictions, making it a valuable tool for pathologists.",
  "model/output": "The model is a classification model designed to predict the class of whole slide images. It outputs the predicted probabilities for different classes, indicating whether a slide is neoplastic, positive, or negative. The model uses a graph-based architecture, specifically Slide2Graph, which incorporates positional information of each patch and the global structure of the whole slide image. This architecture allows the model to aggregate information from individual patches to make a final classification decision for the entire slide. The output is a set of class probabilities, which are then used to determine the most likely class for the slide. The model's performance is evaluated using metrics such as precision, recall, F1 score, and area under the relative operating characteristic curve, which are calculated for each class and overall. The confusion matrix and 95% confidence intervals are also provided for a comprehensive evaluation of the model's performance.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for our model, specifically the Slide2Graph architecture, is publicly available. It can be accessed via GitHub at the repository link https://github.com/BMIRDS/Slide2Graph. This release allows other researchers and practitioners to utilize, modify, and build upon our work. The code is shared under a license that permits its use for both academic and commercial purposes, fostering collaboration and innovation in the field.",
  "evaluation/method": "The evaluation of our method involved several key steps to ensure its robustness and accuracy. We began by using a holdout test set that was not utilized during the training phase, ensuring an unbiased assessment of our model's performance. To quantify the performance, we calculated precision, recall, F1 score, and the area under the relative operating characteristic curve for each class and overall. Additionally, we employed a bootstrapping method with 10,000 iterations to compute the 95% confidence intervals for these metrics, providing a statistical measure of their reliability.\n\nFor error analysis, we generated a confusion matrix to visualize the types of errors our model made, which helped in understanding its strengths and weaknesses. To compare our approach with other methods, we implemented several baseline models, including DeepSlide, decision trees, random forests, and Adaboost. These models were used to aggregate patch information for whole slide inference. We conducted systematic grid searches to optimize the parameters for these models using our training and validation sets.\n\nIn the DeepSlide approach, we set a confidence score threshold of 0.75 for patch-level predictions and required that at least 20% of the patches in a slide be classified as neoplastic or positive for the slide to be labeled accordingly. For the other machine learning models, we used the percentages of neoplastic and positive patches as independent variables to predict the whole slide label. We performed sixfold cross-validation and grid searches to find the best hyperparameters for the random forest and Adaboost models.\n\nOur proposed Slide2Graph model outperformed all baseline models in terms of F1 score, with a statistically significant difference (P < .001). This superior performance was validated using a bootstrapping approach, where we randomly sampled 50 subsets from our test set and applied a 2-tailed Student t test to examine the statistical significance of the differences among the F1 scores of various methods. The results confirmed that Slide2Graph achieved the best performance on our test set compared to other baseline methods.",
  "evaluation/measure": "In our evaluation, we reported several key performance metrics to comprehensively assess the effectiveness of our model. These metrics include precision, recall, F1 score, and the area under the relative operating characteristic curve (AUC). Precision measures the accuracy of the positive predictions made by the model, while recall indicates the model's ability to identify all relevant instances. The F1 score provides a harmonic mean of precision and recall, offering a single metric that balances both concerns. The AUC evaluates the model's ability to distinguish between classes across all possible classification thresholds.\n\nThese metrics were calculated for each class\u2014negative, neoplastic, and positive\u2014as well as an overall average. This detailed breakdown allows for a nuanced understanding of the model's performance across different types of cases. Additionally, we provided 95% confidence intervals (CIs) for all metrics using a bootstrapping method with 10,000 iterations, ensuring the robustness and reliability of our results.\n\nThe choice of these metrics is representative of standard practices in the literature, providing a clear and comparable benchmark for evaluating model performance. Precision, recall, F1 score, and AUC are widely used in machine learning and medical imaging studies to assess classification models. This set of metrics offers a comprehensive view of the model's strengths and weaknesses, making it easier to identify areas for improvement and to compare our results with those of other studies in the field.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of our proposed Slide2Graph model with several other baseline models to assess its performance. We implemented and compared Slide2Graph with DeepSlide, a decision tree, a random forest, and Adaboost. These models were chosen to aggregate patch information for whole slide inference, providing a robust benchmark for our method.\n\nFor DeepSlide, we performed systematic grid searches to determine the optimal thresholds for patch-level confidence scores and the required percentages of predicted patches in a slide. This approach ensured that only patches with a high confidence score were considered for whole slide inference, and specific rules were developed to classify slides based on the percentage of neoplastic or positive patches.\n\nIn addition to DeepSlide, we also evaluated simpler machine learning baselines, including decision trees, random forests, and Adaboost. These models used the percentages of neoplastic and positive patches extracted from a slide as independent variables to predict the whole slide label. We employed sixfold cross-validation and grid search to fine-tune the hyperparameters for the random forest and Adaboost models, ensuring that they were optimized for our dataset.\n\nThe performance of these models was compared using metrics such as precision, recall, F1 score, and the area under the relative operating characteristic curve. We also calculated the 95% confidence intervals for these metrics using a bootstrapping method with 10,000 iterations. This rigorous evaluation process allowed us to demonstrate the superior performance of our Slide2Graph model compared to the baseline methods. The results showed that Slide2Graph outperformed all other models with a statistically significant level of P < .001, highlighting its effectiveness in detecting pancreas tumor patterns on whole slide images.",
  "evaluation/confidence": "The evaluation of our model's performance included the calculation of confidence intervals for all metrics. Specifically, we computed 95% confidence intervals (CIs) for precision, recall, F1 score, and the area under the relative operating characteristic curve (AUC) using a bootstrapping method with 10,000 iterations. This approach ensures that our performance metrics are robust and provides a range within which the true values are likely to fall.\n\nTo assess the statistical significance of the performance differences among various models, we employed a bootstrapping approach. We randomly sampled 50 subsets from our test set and used a 2-tailed Student t-test to examine the statistical significance of the differences in F1 scores among the methods. The results, as shown in Table 4, indicate that our Slide2Graph model outperformed other models with a statistical significance level of P < .001. This high level of statistical significance suggests that the superior performance of our model is not due to chance and is a reliable indicator of its effectiveness compared to the baselines.",
  "evaluation/availability": "Not enough information is available."
}