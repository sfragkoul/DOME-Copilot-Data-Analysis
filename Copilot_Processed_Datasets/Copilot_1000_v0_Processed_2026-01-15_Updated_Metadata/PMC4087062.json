{
  "publication/title": "Developing and evaluating a machine learning based algorithm to predict the need of pediatric intensive care unit transfer for newly hospitalized children.",
  "publication/authors": "Zhai H, Brady P, Li Q, Lingren T, Ni Y, Wheeler DS, Solti I",
  "publication/journal": "Resuscitation",
  "publication/year": "2014",
  "publication/pmid": "24813568",
  "publication/pmcid": "PMC4087062",
  "publication/doi": "10.1016/j.resuscitation.2014.04.009",
  "publication/tags": "- Machine Learning\n- Pediatric Intensive Care Unit\n- Predictive Modeling\n- Logistic Regression\n- Clinical Decision Support\n- Electronic Health Records\n- Patient Transfer Prediction\n- Pediatric Care\n- Algorithm Development\n- Medical Data Analysis",
  "dataset/provenance": "The dataset used in this study was sourced from the electronic health records (EHRs) of Cincinnati Children\u2019s Hospital Medical Center (CCHMC). The data was collected from clinical providers between January 1, 2010, and August 31, 2012. During this period, there were 71,752 admissions to the inpatient wards, with 1,438 of these admissions resulting in transfers from the general wards to the Pediatric Intensive Care Unit (PICU).\n\nThe dataset comprised over 300,000,000 data points, encompassing 7,587 unique clinical elements as candidate predictors. These elements were filtered and processed through a six-step procedure to identify the most predictive clinical elements for the machine learning algorithm.\n\nThe data used in this study was not previously used in any other published paper or by the community. It was specifically collected and curated for the purpose of developing and evaluating a prediction algorithm for PICU transfers within the first 24 hours of admission. The dataset included a wide range of clinical measurements and assessments, which were leveraged to build a robust predictive model.",
  "dataset/splits": "There were two main data splits: a training set and a test set. The training set consisted of 90% of the cases, which included 473 cases and 473 controls. The test set consisted of 10% of the cases, which included 53 cases and 6,299 controls. The ratio of \"no-PICU transfer\" to \"24-hour PICU transfer\" was maintained at 119:1 in the test set to preserve the generalizability of the study's findings.",
  "dataset/redundancy": "The dataset used in this study was derived from electronic health records (EHR) data generated by clinical providers between January 1, 2010, and August 31, 2012. The dataset included 71,752 admissions to inpatient wards, of which 1,438 were later transferred to the Pediatric Intensive Care Unit (PICU). The unit of analysis was the encounter, not the patient.\n\nThe dataset was split into cases and controls. Cases were defined as encounters where patients were transferred to the PICU within the first 24 hours of admission. Controls were encounters where patients were not transferred to the PICU within the first 24 hours. This resulted in 526 case encounters and 6,772 control encounters.\n\nTo ensure the independence of the training and test sets, the dataset was split into two experimental datasets: a training set and a test set. The training set included 90% of the cases (473 cases) and an equal number of controls (473 controls). The test set included the remaining 10% of the cases (53 cases) and a significantly larger number of controls (6,299 controls). This split maintained a ratio of approximately 119:1 for \"no-PICU transfer\" to \"24-hour PICU transfer\" in the test set, preserving the generalizability of the study's findings.\n\nThe distribution of the dataset is notable for its imbalance, with a much higher number of control encounters compared to case encounters. This imbalance is reflective of the rarity of PICU transfers within the first 24 hours of admission. The training and test sets were designed to maintain this imbalance, ensuring that the model's performance could be evaluated in a real-world scenario where PICU transfers are relatively infrequent. This approach is consistent with many previously published machine learning datasets in healthcare, where the event of interest (e.g., disease onset, adverse event) is rare compared to the non-event.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is logistic regression. This choice was driven by its widespread use in clinical decision systems and the relative ease of interpreting its output. The algorithm was not newly developed for this study; rather, it was selected for its established effectiveness in similar contexts.\n\nThe logistic regression model was implemented using Weka 3.6.8 as the experimental platform. We employed a forward stepwise approach with Akaike\u2019s Information Criterion (AIC) to select the best model. This method ensures that the most relevant variables are included in the final model, enhancing its predictive accuracy.\n\nThe decision to use logistic regression was also influenced by the need to provide a clear and interpretable output. Unlike other machine-learning algorithms that might produce complex or black-box models, logistic regression offers a straightforward way to understand the relationship between the predictor variables and the outcome. This is particularly important in a clinical setting, where transparency and interpretability are crucial for gaining the trust of healthcare providers.\n\nIn summary, logistic regression was chosen for its proven track record in clinical applications, ease of interpretation, and the ability to handle the specific requirements of our dataset. The use of an established algorithm allowed us to focus on the clinical relevance and practical implementation of the model, rather than on the development of a new machine-learning technique.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It is a standalone logistic regression model designed to predict the likelihood of Pediatric Intensive Care Unit (PICU) transfer within the first 24 hours of admission. The model utilizes clinical data directly from the Electronic Health Record (EHR) rather than relying on predictions from other machine-learning algorithms as input.\n\nThe development process involved selecting 29 variables associated with 16 clinical elements, which were chosen based on their significance and availability in the EHR. These variables include vital signs, level of consciousness, pain assessments, and work of breathing, among others. The model was trained and validated using a dataset that underwent a 10-fold cross-validation step to ensure its performance on real-world data.\n\nThe training data consisted of encounters with both cases (patients transferred to the PICU) and controls (patients not transferred). The model's performance was evaluated against existing Pediatric Early Warning Scores (PEWS), such as Parshuram\u2019s Bedside PEWS and Monaghan\u2019s PEWS, demonstrating superior sensitivity, specificity, and Area Under the Curve (AUC) in both training and test sets.\n\nThe independence of the training data was maintained by ensuring that the model was evaluated on a test set that represented the real-world ratio of 24-hour transfer and non-transfer encounters. This approach helped in assessing the model's robustness and generalizability.\n\nIn summary, the model is a direct application of logistic regression to clinical data, without the use of meta-predictors or inputs from other machine-learning algorithms. The training data's independence was carefully managed to ensure reliable and valid results.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to transform raw electronic health record (EHR) data into a format suitable for model training. Initially, over 300 million data points from 71,752 encounters were collected, resulting in 7,587 unique clinical elements. These elements were filtered and sorted by frequency, retaining the top 400 most frequent elements. A pediatric hospitalist then reviewed these elements to identify 16 candidate clinical elements with predictive potential.\n\nFor each encounter, measurements for these 16 clinical elements were collected up to one hour before the transfer event for cases and within the first 24 hours for controls. This ensured a conservative approach by using a maximum of 23 hours of data for cases. The clinical elements included five continuous, two narrative, and nine nominal types.\n\nTo create independent variables, the most recent measurement within the study time-window was extracted for each clinical element. For the five continuous clinical elements, additional measurements such as the oldest, maximum, minimum, and mean values were calculated to represent the dynamic nature of patients' clinical conditions. This process resulted in 36 candidate measurements.\n\nEach of these 36 measurements was then categorized. For nominal elements, original labels were used. For continuous elements, categorization was based on predefined cut-off points identified from published work and guided by physicians. Narrative elements were categorized using keywords and synonyms provided by a physician. A new category, \"Not Available\" (\"N/A\"), was added to handle missing values, treating it as any other naturally occurring category.\n\nThis categorization process resulted in 155 dichotomous variables. Chi-square calculations were used to test the significance of each measurement in the training set, with all 36 measurements selected at a 0.05 p-value for developing the machine-learning algorithm. Logistic regression was chosen as the machine-learning algorithm due to its wide usage in clinical decision systems and ease of interpreting its output. A forward stepwise approach with Akaike\u2019s Information Criterion (AIC) was applied to select the best model.",
  "optimization/parameters": "In our study, we utilized a total of 29 variables derived from 16 clinical elements to develop our prediction model. These variables were selected through a meticulous process that involved expert clinician opinion, categorization, and machine learning techniques.\n\nThe selection of these variables began with the identification of the top 400 most frequent clinical elements from the electronic health records (EHR). A pediatric hospitalist then manually reviewed these elements to generate a list of 16 candidate clinical elements with predictive potential. These elements included a mix of continuous, narrative, and nominal data types.\n\nFor each encounter, we extracted the most recent measurement within a specified time window for each clinical element. For continuous elements, we created additional measurements, including the oldest, maximum, minimum, and mean values, to represent the dynamic nature of patients' clinical conditions. This process resulted in a total of 36 candidate measurements.\n\nThese measurements were then categorized based on published cut-off points and physician guidance. For nominal elements, the original labels were used, while continuous elements were categorized using defined cut-off points. Narrative elements were categorized based on keywords and synonyms provided by physicians. This categorization resulted in 155 dichotomous variables.\n\nTo ensure the significance of each measurement, we used Chi-square calculations in the training set. All 36 measurements were selected and used to develop the machine learning algorithm at a 0.05 p-value. Finally, we employed a forward stepwise approach with Akaike\u2019s Information Criterion (AIC) to select the best model, resulting in the inclusion of 29 variables in the final model.",
  "optimization/features": "In the optimization process, the input features for the machine learning algorithm were carefully selected and processed. Initially, a large set of clinical elements was considered, but this was narrowed down to 16 candidate clinical elements with predictive potential. These elements were chosen through a combination of frequency analysis and expert review by a pediatric hospitalist.\n\nFor each of these 16 clinical elements, various measurements were collected. For continuous elements, additional measurements such as the oldest, maximum, minimum, and mean values within the study time-window were created to capture the dynamic nature of patients' clinical conditions. This resulted in a total of 36 candidate measurements.\n\nFeature selection was performed using the training set only. Each of these 36 measurements was categorized and tested for significance using Chi-square calculations. All 36 measurements were found to be significant at a 0.05 p-value and were used to develop the machine learning algorithm. However, the final model included 29 variables associated with 13 of the 16 clinical elements, selected through a forward stepwise approach with Akaike\u2019s Information Criterion (AIC).\n\nThe final set of input features used in the model consisted of these 29 variables, which were derived from the initial 16 clinical elements. This process ensured that the features used were both relevant and significant in predicting PICU transfer.",
  "optimization/fitting": "The logistic regression model used in our study included 29 variables derived from 16 clinical elements. While this number of variables might seem substantial, it is important to note that the training set consisted of a large number of encounters, which helped mitigate the risk of overfitting. Specifically, the training set included 425 encounters with non-missing values for Monaghan\u2019s PEWS, and a total of 3,080 encounters in the test set. This large dataset provided a robust foundation for training the model.\n\nTo further ensure that overfitting was not an issue, we employed a forward stepwise approach with Akaike\u2019s Information Criterion (AIC) to select the best model. This method helps in identifying the most relevant variables and prevents the inclusion of unnecessary parameters that could lead to overfitting. Additionally, we used a 10-fold cross-validation step to evaluate the model's performance, which provided a more reliable estimate of its generalization capability.\n\nUnderfitting was addressed by carefully selecting the variables and ensuring that they were clinically relevant. The variables included vital signs, level of consciousness, pain assessments, and work of breathing, all of which have face validity in association with worsening patient status. This careful selection process ensured that the model captured the essential aspects of the clinical data, reducing the risk of underfitting.\n\nMoreover, the model's performance was compared against two existing Pediatric Early Warning Systems (PEWS), demonstrating statistically significant improvements in AUC and specificity. This comparison further validated the model's effectiveness and ruled out the possibility of underfitting, as the model outperformed established benchmarks.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model developed in our study is not a black box. Instead, it is designed to be transparent and interpretable. We utilized logistic regression as the machine learning algorithm, which is well-known for its interpretability. This choice was driven by the need for a model that could provide clear insights into the factors contributing to the prediction of PICU transfer.\n\nThe logistic regression model outputs a percentage likelihood of PICU transfer, which is straightforward to interpret. This likelihood is derived from a combination of 29 variables associated with 16 clinical elements. Each of these variables has a clear and measurable impact on the prediction, making it easy to understand which factors are influencing the model's decisions.\n\nFor example, variables such as vital signs, level of consciousness, pain assessments, and work of breathing were included in the model. These variables were selected because they have face validity in association with worsening patient status and are routinely collected by nurses during their clinical assessments. The model's transparency allows clinicians to see exactly which measurements are contributing to the prediction, enhancing trust and facilitating better clinical decision-making.\n\nMoreover, the model's output can be integrated into rapid response systems, providing multiple thresholds based on the percentage likelihood of PICU transfer. This integration allows for a more nuanced and actionable approach to patient care, where different likelihoods can trigger specific clinical responses. For instance, a likelihood of over 50% might prompt an automatic call for a multidisciplinary assessment, while a likelihood of over 95% could initiate immediate PICU transfer protocols.\n\nIn summary, the logistic regression model is designed to be transparent, providing clear and interpretable outputs that can be easily understood and acted upon by clinicians. This transparency is crucial for building trust in the model and ensuring that it can be effectively integrated into clinical workflows.",
  "model/output": "The model developed is a classification model. It uses logistic regression, a type of supervised learning algorithm, to predict the likelihood of a patient's transfer to the Pediatric Intensive Care Unit (PICU) within the first 24 hours of hospitalization. The output of the model is a binary-valued variable indicating the presence or absence of a PICU transfer event. Specifically, the model provides a percentage likelihood of PICU transfer, with a predicted positive being any combination of predictor variables that has an output greater than 0.5. The model's performance is evaluated using metrics such as sensitivity, specificity, positive predictive value (PPV), and the area under the curve (AUC), which are typical for classification tasks.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our prediction methods for PICU transfer involved several steps to ensure robustness and practical applicability. We employed a standard 10-fold cross-validation to assess the performance of our models. This approach helped in evaluating the models' ability to generalize to unseen data.\n\nTo gauge the models' performance on real-world data, we evaluated them on a testing set that reflected the actual ratio of 24-hour transfer and non-transfer encounters. This testing set was crucial for understanding how our models would perform in practical scenarios.\n\nIn addition to our primary models, we compared our results with established methods, specifically Parshuram\u2019s Bedside PEWS and Monaghan\u2019s PEWS. For Parshuram\u2019s Bedside PEWS, we included only five clinical elements available in our dataset, following a similar approach to previous work. For Monaghan\u2019s PEWS, we extracted the highest values recorded before the transfer event for cases and within the first 24 hours of hospitalization for controls. This ensured a fair comparison by using the same data points for evaluation.\n\nTo further assess the robustness of our logistic regression model, we conducted a timestamp experiment. This involved evaluating the model using cumulative clinical measurements at different points within the first 24 hours after admission. The model was run 24 times, each time using data up to a specific hour. This experiment helped in determining the optimal time window for making accurate predictions.\n\nThe performance of the models was measured using several metrics, including sensitivity, specificity, positive predictive value (PPV), and the area under the curve (AUC). These metrics provided a comprehensive view of the models' predictive accuracy and reliability. The results showed that our logistic regression model outperformed the existing PEWS methods in terms of AUC and specificity, with notable improvements in sensitivity as well.",
  "evaluation/measure": "In our study, we evaluated the performance of our predictive models using several key metrics to ensure a comprehensive assessment. The primary metrics reported include sensitivity, specificity, positive predictive value (PPV), and the area under the receiver operating characteristic curve (AUC). These metrics are widely recognized and used in the literature for evaluating the performance of predictive models, particularly in clinical settings.\n\nSensitivity, also known as recall or true positive rate, measures the proportion of actual positives that are correctly identified by the model. Specificity, or the true negative rate, measures the proportion of actual negatives that are correctly identified. These two metrics are crucial for understanding how well the model can distinguish between patients who will require PICU transfer and those who will not.\n\nThe positive predictive value (PPV) indicates the proportion of positive predictions that are actually correct. This metric is particularly important in clinical settings where the cost of false positives can be significant. Finally, the AUC provides a single scalar value that summarizes the performance of the model across all classification thresholds. A higher AUC indicates better overall performance.\n\nIn addition to these standard metrics, we also reported the 95% confidence intervals (CI) for each metric to provide a sense of the uncertainty around our estimates. This is important for understanding the reliability of our results and for comparing our model's performance to other studies in the literature.\n\nWe believe that this set of metrics is representative of the current standards in the field and provides a thorough evaluation of our model's performance. By including sensitivity, specificity, PPV, and AUC, we can offer a comprehensive view of how well our model performs in predicting PICU transfers. This approach allows for a fair comparison with other studies and ensures that our results are interpretable and actionable in a clinical context.",
  "evaluation/comparison": "In our evaluation, we compared our predictive model to established methods to ensure its robustness and practical applicability. We implemented a standard 10-fold cross-validation step to assess the performance of our models on real-world data. Specifically, we evaluated our models on a testing set that represented the real-world ratio of 24-hour transfer and non-transfer encounters.\n\nFor comparison, we evaluated two existing Pediatric Early Warning Systems (PEWS): Parshuram\u2019s Bedside PEWS and Monaghan\u2019s PEWS. When assessing Parshuram\u2019s Bedside PEWS, we included only five clinical elements available in our dataset, namely heart rate, systolic blood pressure, capillary refill time, respiratory rate, and transcutaneous oxygen saturation. These elements, except for respiratory effort and oxygen therapy, were also used in our machine learning algorithm. Monaghan\u2019s PEWS was calculated for nearly all admitted patients, and we extracted the highest PEWS values recorded before the transfer event for cases and within the first 24 hours of hospitalization for controls. This ensured a fair comparison by evaluating prediction results only on encounters with non-missing Monaghan\u2019s PEWS values.\n\nAdditionally, we conducted a timestamp experiment to assess the robustness of our logistic regression model. We evaluated the model using clinical measurements at different points within the first 24 hours after admission. Specifically, we ran the model 24 times using cumulative clinical measurements for each hour of the first 24 hours after admission on the training set. This approach allowed us to determine the optimal time window for accurate predictions.\n\nIn summary, our evaluation included comparisons to publicly available PEWS methods and simpler baselines, ensuring a comprehensive assessment of our model's performance.",
  "evaluation/confidence": "The evaluation of our predictive model for PICU transfer includes several key performance metrics, each accompanied by confidence intervals to provide a range of plausible values. These metrics include sensitivity, specificity, positive predictive value (PPV), and the area under the curve (AUC). The confidence intervals offer a measure of the reliability and precision of these estimates.\n\nStatistical significance is a crucial aspect of our evaluation. We compared our logistic regression model against two established Pediatric Early Warning Systems (PEWS): Bedside PEWS and Monaghan\u2019s PEWS. The results indicate that our model's AUC and specificity are statistically significantly improved in the test set. Specifically, the improvements in AUC and specificity when compared to Bedside PEWS and Monaghan\u2019s PEWS are statistically significant, with P-values less than 0.001. This suggests that the differences observed are unlikely to be due to chance.\n\nHowever, while our model's sensitivity numerically improved by 15.4% and 24.1% in the test set when compared to Bedside PEWS and Monaghan\u2019s PEWS, respectively, these improvements were not statistically significant. This lack of statistical significance is hypothesized to be due to the relatively small number of cases in the test set, which may have hindered the detection of statistical significance.\n\nOverall, the confidence intervals and statistical significance tests provide a robust framework for evaluating the performance and superiority of our predictive model. The statistically significant improvements in AUC and specificity, along with the confidence intervals for the performance metrics, enhance our confidence in the model's effectiveness and reliability.",
  "evaluation/availability": "Not enough information is available."
}