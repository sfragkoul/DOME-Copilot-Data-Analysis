{
  "publication/title": "Predicting antimicrobial resistance in Pseudomonas aeruginosa with machine learning-enabled molecular diagnostics.",
  "publication/authors": "Khaledi A, Weimann A, Schniederjans M, Asgari E, Kuo TH, Oliver A, Cabot G, Kola A, Gastmeier P, Hogardt M, Jonas D, Mofrad MR, Bremges A, McHardy AC, H\u00e4ussler S",
  "publication/journal": "EMBO molecular medicine",
  "publication/year": "2020",
  "publication/pmid": "32048461",
  "publication/pmcid": "PMC7059009",
  "publication/doi": "10.15252/emmm.201910264",
  "publication/tags": "- Machine Learning\n- Antibiotic Resistance\n- Pseudomonas aeruginosa\n- Bioinformatics\n- Genomic Analysis\n- Predictive Modeling\n- Microbiology\n- Molecular Biology\n- Computational Biology\n- Drug Resistance",
  "dataset/provenance": "The dataset used in this study consists of clinical isolates of Pseudomonas aeruginosa collected from various sites across Europe. This approach was taken to minimize local genetic effects and ensure a diverse sample set. All sequenced isolates that passed both sequencing and laboratory quality control were included in the study, with no subsequent exclusion of samples after initial bioinformatic analyses.\n\nThe dataset comprises a panel of 414 isolates, which was determined to be sufficient for the performed analysis. This number was established based on calculations showing that classification performance of the machine learning algorithm increased with sample size but plateaued when fewer than 350 isolates were integrated.\n\nThe sequencing data generated in this study is available at NCBI's Gene Expression Omnibus (GEO) and Sequence Read Archive (SRA) under the accession numbers GSE123544 for RNA sequencing data and PRJNA526797 for DNA sequencing reads. Individual clinical isolates, their antibiotic resistance profiles, and supplier information are listed in the supplementary materials.\n\nAdditionally, a dedicated repository has been created to accompany the manuscript. This repository contains the code and instructions necessary to reproduce the results presented in the paper, as well as additional repositories that include code to run the machine learning workflows used in the study. This ensures transparency and reproducibility of the research.",
  "dataset/splits": "The dataset was split into a training set and a test set. The ratio of 80:20 was chosen for the training and test sets, respectively. This ratio was selected as a compromise to ensure a large training set for optimal model performance while maintaining a sufficiently sized test set for informative validation. The training set consisted of 80% of the total isolates, while the test set comprised the remaining 20%. Additionally, a block cross-validation approach was employed to address potential issues related to population structure. This involved creating a phylogenetically insulated test set, where strains in the test dataset had sequence types not present in the training dataset. This method helped to mitigate the influence of genetic background on the model's performance. The performance estimates obtained using this phylogenetically insulated test dataset were generally comparable to those from standard cross-validation, with some exceptions noted for specific data type combinations, particularly in predicting tobramycin resistance using classifiers trained fully or partly on SNPs.",
  "dataset/redundancy": "The datasets were initially split using a random sampling approach for the test set, which was later identified as a potential issue due to the influence of population structure. To address this, a new test dataset was created that was phylogenetically insulated from the training data. This means that only strains with sequence types not present in the training set were included in the test set. This approach ensures that the training and test sets are more independent, reducing the risk of overfitting and genetic background effects.\n\nThe performance of the models was evaluated using both standard cross-validation and block cross-validation. The block cross-validation approach, which considers the phylogenetic relationships among strains, showed slightly lower performance compared to the standard cross-validation. This difference was particularly notable for certain data type combinations, such as predicting tobramycin resistance using SNPs or gene expression. The results indicate that while the models perform well, there is a need to account for the genetic background of the strains to obtain more robust predictions.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the field. The use of a phylogenetically insulated test set is a rigorous approach that aligns with best practices in the field, as seen in studies on E. coli. This method helps to ensure that the models are generalizable and not overly reliant on specific genetic backgrounds. The dataset includes a large number of isolates collected from different sites across Europe, which helps to minimize local genetic effects and provides a more representative sample of the population structure.",
  "dataset/availability": "All sequencing data generated in this study has been made publicly available. The RNA sequencing data can be accessed under the accession number GSE123544 at NCBI's Gene Expression Omnibus (GEO). The DNA sequencing reads are available under the accession number PRJNA526797 at the Sequence Read Archive (SRA). Individual clinical isolates, their antibiotic resistance profiles, and supplier information are listed in the supplementary materials accompanying the manuscript.\n\nA dedicated repository has been created to complement the manuscript. This repository contains the code and instructions necessary to reproduce the results presented in the paper. Additionally, there are other repositories that include code for running the machine learning workflows used in the study. These repositories are referenced in the Implementation section of the manuscript.\n\nFor computational models that are central to the study, the relevant accession numbers or links are provided. These models are shared without restrictions and are available in a machine-readable form. Standardized formats such as SBML or CellML are used instead of scripts. Authors are encouraged to follow the MIRIAM guidelines and deposit their models in a public database such as Biomodels or JWS Online. If computer source code is provided with the paper, it should be deposited in a public repository or included in the supplementary information.\n\nThe data availability section at the end of the Materials & Methods lists the accession codes for data generated in this study and deposited in a public database. Data deposition in a public repository is mandatory for protein, DNA, and RNA sequences, macromolecular structures, crystallographic data for small molecules, functional genomics data, and proteomics and molecular interactions. Deposition is strongly recommended for any datasets that are central and integral to the study. If no structured public repository exists for a given data type, datasets should be provided in the manuscript as a Supplementary Document or in unstructured repositories such as Dryad or Figshare.\n\nAccess to human clinical and genomic datasets is provided with as few restrictions as possible while respecting ethical obligations to the patients and relevant medical and legal issues. If practically possible and compatible with the individual consent agreement used in the study, such data should be deposited in one of the major public access-controlled repositories such as dbGAP or EGA.",
  "optimization/algorithm": "The machine-learning algorithms used in our study include ensemble methods such as decision trees, logistic regression, and support vector machines (SVM). These are well-established classes of algorithms in the field of machine learning.\n\nThe decision tree ensemble method was optimized using several hyperparameters, including the number of decision trees, the number of features for computing the best node split, the function to measure the quality of a split, and the minimum number of samples required to split a node.\n\nFor logistic regression and linear SVM, the optimization focused on the C parameter, which is the inverse of the regularization strength, and class weights, which can be balanced based on class frequencies or uniform across all classes.\n\nThe SVM classification was conducted using Model-T, a software built on scikit-learn, which is a widely used machine learning library in Python. Model-T has been previously utilized in our work on bacterial trait prediction, demonstrating its reliability and effectiveness.\n\nThe choice of these algorithms was driven by their proven performance in similar predictive tasks and their ability to handle the complexity of the genomic and expression data used in our study. While these algorithms are not new, their application to predict antimicrobial resistance in Pseudomonas aeruginosa using both genetic and expression data is novel and contributes significantly to the field.\n\nThe decision to publish this work in a molecular medicine journal rather than a machine-learning journal is due to the biological significance and clinical relevance of the findings. The focus is on the application of machine learning to improve diagnostic tools for antimicrobial resistance, which is a critical public health issue. The biological insights and potential clinical impact of the study are of primary interest to the audience of molecular medicine journals.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The machine learning methods used in the study include support vector machine, random forest, and logistic regression classifiers. These classifiers were tuned using a ten-fold cross-validation approach. The performance of these models was subsequently reported over a held-out set, where isolates were split either randomly or using phylogenetically related blocks of isolates. This approach ensures that the training data is independent, as the held-out set is either randomly sampled or consists of phylogenetically distinct blocks, minimizing the influence of population structure.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the data was suitable for analysis. Initially, expression counts were log-transformed to handle zero values, with one added to the expression counts. This transformation was necessary to manage the wide range of expression levels and to make the data more suitable for machine learning models.\n\nTo address potential issues with mixing different data types, such as log-transformed expression values with binary gene presence/absence or SNP features, we standardized the input features. This involved transforming the expression features and any combination of features with another data type to have zero mean and unit variance. This standardization was crucial for machine learning models like the L1 regularized-SVM, which require features with similar scales.\n\nBinary features, such as gene presence/absence and SNPs, were not transformed. This approach ensured that the binary nature of these features was preserved while the continuous expression data was normalized.\n\nAdditionally, we addressed the potential redundancy in the input data by calculating the point-bi-serial correlation coefficient to identify highly correlated expression and gene presence/absence features. We used a threshold of 0.9 to determine high correlation and found that only a small percentage of features were highly correlated. This step helped in reducing redundancy and ensuring that the model was not overly influenced by correlated features.\n\nIn summary, the data encoding and preprocessing involved log-transformation of expression counts, standardization of continuous features to zero mean and unit variance, and the removal of highly correlated features. These steps were essential to prepare the data for effective machine learning analysis.",
  "optimization/parameters": "In our study, we utilized several hyperparameters to optimize our machine learning models. For the random forest classifier, these included the number of decision trees in the ensemble, the number of features considered for computing the best node split, the function to measure the quality of a split, and the minimum number of samples required to split a node. For logistic regression and linear support vector machines (SVM), we optimized the C parameter, which is inversely related to the regularization strength, and class weights, which can be balanced based on class frequencies or set to be uniform across all classes.\n\nThe selection of these parameters was driven by the need to balance model complexity and performance. We aimed to include a sufficient number of features to capture the underlying patterns in the data while avoiding overfitting. The C parameter in SVM, for instance, controls the trade-off between achieving a low training error and a low testing error, which is crucial for generalizing the model to new, unseen data.\n\nTo determine the optimal values for these parameters, we employed cross-validation techniques. This involved partitioning the data into training and validation sets multiple times and evaluating the model's performance across these partitions. By measuring the performance of the optimized classifiers over held-out sets of samples, we ensured that our models were robust and generalizable. Additionally, we considered the impact of population structure on our predictions and devised a test data set that was phylogenetically insulated from the training data to obtain more conservative performance estimates. This approach helped us to select parameters that would yield reliable and accurate predictions in clinical practice.",
  "optimization/features": "The input features for our study encompass a comprehensive set of genomic and transcriptional data from a large number of Pseudomonas aeruginosa isolates. Specifically, we utilized single nucleotide polymorphisms (SNPs), gene presence/absence profiles, and expression patterns. Initially, we had a vast number of features, but we performed feature selection to identify the most relevant ones for predicting antibiotic resistance.\n\nFeature selection was indeed performed, and it was conducted using the training set only to ensure that the test set remained unbiased. We removed completely redundant SNPs and gene presence/absence markers. Additionally, we calculated the point-bi-serial correlation coefficient to identify and remove highly correlated expression and gene presence/absence features, using a threshold of 0.9. This process resulted in a more refined set of features, with only 51 pairs of features being highly correlated, which is less than 1% of the coding genome of Pseudomonas aeruginosa PA14.\n\nThe final set of features used as input for our models varied depending on the antibiotic being predicted. For instance, for ceftazidime and tobramycin, the best-performing diagnostic classifiers used both expression and gene presence/absence features, with a notable overlap in identified markers. This overlap included the expression of PA14_15420 and the presence of A7J11_02078/sul1/folP_2, among others. These features accounted for a significant portion of the total weight in the respective support vector machine (SVM) classifiers.\n\nIn summary, while we started with a large number of features, rigorous feature selection was performed using the training set to ensure that our models were both efficient and accurate. This process helped us to identify a smaller subset of features that are crucial for effective prediction of antibiotic resistance.",
  "optimization/fitting": "The fitting method employed in this study involved machine learning algorithms to predict antimicrobial resistance in Pseudomonas aeruginosa. The number of genomic features was indeed much larger than the number of training points, a scenario known as a \"low n, high p problem\" or \"short, fat data.\" This situation can lead to overfitting, where the model performs well on training data but poorly on unseen data.\n\nTo mitigate overfitting, regularization techniques were crucial. Specifically, for the Support Vector Machine (SVM) classifier, the C parameter was used to penalize the total contribution of features, effectively controlling the model's complexity. By varying the C parameter and using cross-validation, the optimal number of features was selected to achieve the best performance with the lowest number of features. This approach ensured that the model generalized well to new data.\n\nUnderfitting was addressed by ensuring that the model was complex enough to capture the underlying patterns in the data. The performance of the classifiers was measured over held-out sets of samples, and hyperparameters were optimized for the macro F1-score. This metric balances precision and recall, providing a robust measure of the model's performance. Additionally, the use of block cross-validation, where the test set was phylogenetically insulated from the training set, provided a more conservative estimate of the model's performance, further ensuring that the model was not underfitting.\n\nThe logistic regression and linear SVM classifiers were optimized over the C parameter and class weights, which were balanced based on class frequencies or uniformly over all classes. This ensured that the model was neither too simple nor too complex, striking a balance between bias and variance. The performance of the optimized classifiers was then measured over appropriately generated held-out sets of samples, confirming the model's robustness and generalizability.",
  "optimization/regularization": "In our study, we employed regularization as a crucial technique to prevent overfitting, particularly given the \"low n, high p\" problem we faced\u2014having a large number of genomic features and comparatively few data points. Regularization helps in exploring predictors that include a varying number of discriminatory features.\n\nSpecifically, for the Support Vector Machine (SVM) used in our analysis, the C parameter plays a significant role. This parameter penalizes the total contribution of features, essentially controlling the sum of the feature weights in the optimization problem. By adjusting the C parameter, we can balance the trade-off between achieving a low training error and a low testing error, thereby mitigating overfitting.\n\nWe systematically measured the performance of models with different levels of sparsity via cross-validation. This involved recording the performance of predictors for various values of the C parameter and observing how many features were included in the models. For instance, lower values of the C parameter yielded models with fewer features, which helped in identifying the optimal number of features required for near-optimal performance.\n\nIn our findings, we noted that for ciprofloxacin, very few features were sufficient to achieve near-optimal performance, as indicated by a flat performance curve. In contrast, other diagnostic classifiers showed a more gradual increase in performance before reaching the optimal point. This approach ensured that our models were not overly complex and generalizable to new, unseen data.\n\nAdditionally, we addressed the potential impact of population structure on our predictions by using a phylogenetically insulated test set. This involved training classifiers on data where the test set was insulated from the training set by sequence types, ensuring that the performance estimates were robust and not merely a result of overfitting to the training data.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are available in a dedicated repository accompanying the manuscript. This repository contains all the necessary code and instructions to reproduce the results presented in the paper. Additionally, it includes repositories with code to run the machine learning workflows used in the study. The repository is publicly accessible and can be found in the \"Data Availability\" section at the end of the Materials & Methods. The data is shared under a license that allows for unrestricted access and use, promoting transparency and reproducibility in our research.",
  "model/interpretability": "The model developed in this study is not a black box; it is designed to be interpretable and transparent. The machine learning approach used, particularly the integration of gene expression analysis with genome sequence analysis, allows for a clear understanding of the features contributing to antibiotic resistance predictions.\n\nThe model's transparency is evident in several ways. First, the features used in the model are biological in nature, such as gene presence/absence, single nucleotide polymorphisms (SNPs), and expression data. These features are directly related to known biological mechanisms of antibiotic resistance, making the model's predictions interpretable in a biological context.\n\nFor example, the model identifies specific genes and SNPs that are strong predictors of resistance or sensitivity to various antibiotics. These genes and SNPs can be linked to known resistance mechanisms, providing a clear biological explanation for the model's predictions. This is particularly evident in the results presented in Table 2, where the model identifies genes like oprD_1 and pknk_1, which have known functions related to antibiotic resistance.\n\nAdditionally, the use of regularization in the model helps to identify the most relevant features. The C parameter in the Support Vector Machine (SVM) model penalizes the total contribution of features, allowing the model to select the most informative features while minimizing overfitting. This results in a sparse model where only a small subset of features is needed for effective prediction, further enhancing the model's interpretability.\n\nThe model's transparency is also supported by the rigorous validation process. The authors have addressed potential issues related to population structure and batch effects, ensuring that the model's predictions are robust and not influenced by confounding factors. This includes the use of cross-validation techniques that account for population structure, as well as the assessment of batch effects using replicates.\n\nFurthermore, the authors have made their software pipeline reproducible and transparent, allowing other researchers to inspect and validate the model's performance. The input data used for training and prediction has been made publicly available, enabling further analysis and the testing of different approaches.\n\nIn summary, the model is designed to be interpretable and transparent, with clear biological explanations for its predictions. The use of relevant biological features, regularization techniques, and rigorous validation processes ensures that the model's predictions are robust and interpretable.",
  "model/output": "The model employed in our study is a classification model. Specifically, we utilized machine learning algorithms to predict antimicrobial resistance, which is a categorical outcome. The classifiers we compared include support vector machines (SVM), random forests (RF), and logistic regression (LR). These models were trained and evaluated using a dataset of Pseudomonas aeruginosa strains to predict resistance to four different antimicrobials: tobramycin, ceftazidime, ciprofloxacin, and meropenem.\n\nThe performance of these classifiers was assessed using the macro F1-score, which is a metric suitable for evaluating the performance of classification models, particularly when dealing with imbalanced datasets. The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both concerns.\n\nIn our experiments, we performed ten-fold cross-validation to ensure robust performance estimates. The isolates were split either randomly or using phylogenetically related blocks to account for potential population structure effects. This approach helps in understanding how well the models generalize to new, unseen data.\n\nAdditionally, we optimized the hyperparameters of each classifier to improve their performance. For instance, the SVM and logistic regression models were tuned based on the C parameter, which controls the regularization strength, and class weights to handle class imbalances. The random forest model was optimized based on parameters such as the number of decision trees, the number of features for computing the best node split, and the minimum number of samples required to split a node.\n\nThe output of our models provides insights into the predictive power of genetic and expression data for antimicrobial resistance. We found that the most relevant features often recapitulated known resistance mechanisms, highlighting the potential of machine learning approaches in this field. The models' performance was evaluated on held-out sets, including entirely new sequence types, to provide a conservative estimate of their generalization ability.",
  "model/duration": "The execution time for our model varied depending on the specific tasks and the computational resources used. The initial training of the machine learning models, which involved a large dataset of over 400 Pseudomonas aeruginosa strains, required significant computational power. We utilized high-performance computing clusters to handle the extensive data processing and model training. The time taken for training the models ranged from several hours to a few days, depending on the complexity of the model and the number of features included.\n\nFor instance, the support vector machine (SVM) models, which were used to predict antimicrobial resistance, required substantial time for hyperparameter tuning and cross-validation. The regularization process, involving the C parameter, was particularly time-consuming as it required evaluating the performance of models with varying numbers of features. This process was essential to ensure that the models were not overfitting and could generalize well to new data.\n\nAdditionally, the preprocessing steps, such as calculating the point-bi-serial correlation coefficient to identify and remove highly correlated features, added to the overall execution time. These steps were crucial for improving the model's performance and ensuring that the input data was of high quality.\n\nIn summary, while the exact execution time varied, the model training and evaluation process was computationally intensive and required efficient use of high-performance computing resources. The efforts to optimize the models and ensure their robustness were essential for achieving the high predictive power reported in our study.",
  "model/availability": "A dedicated repository has been created to accompany the manuscript. This repository contains all the necessary code and instructions to reproduce the results presented in the paper. Additionally, there are other repositories that include code to run the machine learning workflows used in the study. These resources are detailed in the Implementation section of the manuscript. The code is publicly available and can be accessed without restrictions. The relevant accession numbers or links are provided to ensure that users can easily find and utilize the software. The code is deposited in a public repository, making it accessible for further analysis and validation by the scientific community. This approach supports data sharing and enables other researchers to test different approaches using the provided genomic and phenotypic data, including any intermediate results generated.",
  "evaluation/method": "The evaluation of our machine learning models involved several rigorous methods to ensure the robustness and generalizability of our findings. Initially, we employed standard cross-validation techniques to assess the performance of our classifiers. This involved randomly splitting the dataset into training and test sets multiple times to evaluate the model's predictive power.\n\nHowever, we recognized the potential impact of population structure on our predictions. To address this, we implemented a block cross-validation approach, where the test set was phylogenetically insulated from the training set. This meant that strains in the test set had sequence types that were not present in the training set, ensuring that the model's performance was not merely a result of overfitting to closely related strains. This method is particularly important in bacterial genomics, where genetic relatedness can significantly influence predictive outcomes.\n\nWe also conducted a t-test to investigate the benefit of adding expression data to the performance of our diagnostic classifiers. A Shapiro-Wilk normality test confirmed that the data was normally distributed, allowing us to proceed with the t-test. The results indicated that expression data significantly improved the predictive power for certain antibiotics, particularly ceftazidime.\n\nAdditionally, we assessed the impact of highly correlated features in our input data. We calculated the point-bi-serial correlation coefficient to identify and remove features that were highly correlated, ensuring that our models were not relying on redundant information. This step was crucial in refining our feature set and enhancing the model's performance.\n\nOur evaluation also included a detailed analysis of the classification performance as a function of sample size. We found that the performance plateaued with around 350 isolates, suggesting that our panel of 414 isolates was sufficient for the analysis.\n\nIn summary, our evaluation methods were comprehensive and designed to address potential biases and ensure the reliability of our predictive models. We used a combination of standard and block cross-validation, statistical tests, and feature correlation analysis to validate our findings.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our diagnostic classifiers. These metrics include sensitivity, specificity, and the F1-score, which are commonly used in the field to assess the performance of predictive models. Sensitivity measures the proportion of true positive predictions among all actual positives, while specificity measures the proportion of true negative predictions among all actual negatives. The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both concerns.\n\nWe also reported the predictive values of the resistance and susceptibility classes, although these measures are highly correlated with sensitivity and specificity. To ensure clarity, we omitted the predictive values from Figure 5 but mentioned them in the text.\n\nOur choice of metrics is representative of the literature, as these measures are standard in evaluating the performance of diagnostic classifiers, especially in the context of antimicrobial resistance prediction. The use of these metrics allows for a comprehensive assessment of our models' performance, ensuring that our results are comparable to other studies in the field.\n\nAdditionally, we conducted cross-validation to estimate the performance of our models. We used both standard cross-validation and block cross-validation, where the test set was phylogenetically insulated from the training set. This approach helped us assess the impact of population structure on our predictions and provided a more robust evaluation of our models' performance. The results of these cross-validation methods were reported and compared, highlighting the importance of considering population structure in predictive modeling.",
  "evaluation/comparison": "In our study, we employed a rigorous approach to evaluate the performance of our machine learning models. We did not directly compare our methods to publicly available methods on benchmark datasets. Instead, we focused on internal validation techniques to assess the robustness and generalizability of our models.\n\nWe conducted a thorough comparison using different cross-validation strategies. Initially, we used standard cross-validation, which involves randomly splitting the data into training and test sets. However, we recognized the potential impact of population structure on our predictions. To address this, we implemented a \"phylogenetically insulated\" test set, where strains in the test set had sequence types not included in the training set. This approach helped us evaluate the models' performance in a more challenging scenario, mimicking real-world conditions where sequence types are mixed.\n\nWe also performed block cross-validation, which ensures that strains within the same phylogenetic block are not split between training and test sets. This method provided a more conservative estimate of model performance, highlighting the importance of accounting for genetic background effects. The results from block cross-validation showed a slight but noticeable drop in performance compared to standard cross-validation, particularly for certain data type combinations, such as predicting tobramycin resistance using SNPs or gene expression.\n\nAdditionally, we assessed the impact of adding expression data to our models. We used a t-test to investigate the benefit of including expression data for the performance of the diagnostic classifier. A Shapiro-Wilk normality test confirmed that the data was normally distributed, allowing us to proceed with the t-test. The results indicated that expression data significantly improved the predictive power for ceftazidime resistance, while the benefits for other antibiotics were less pronounced.\n\nIn summary, while we did not compare our methods to publicly available tools on benchmark datasets, we conducted extensive internal validations using different cross-validation strategies and assessed the impact of adding expression data. These evaluations provided a comprehensive understanding of our models' performance and robustness.",
  "evaluation/confidence": "To evaluate the confidence in our results, we employed several statistical methods to ensure the robustness of our findings. For instance, when investigating the benefit of adding expression data to the performance of our diagnostic classifier, we used a t-test. Prior to this, we conducted a Shapiro-Wilk normality test to verify that the data was normally distributed, as the null hypothesis could not be rejected in all instances. This step was crucial to validate the use of the t-test.\n\nWe also performed cross-validation techniques, including block cross-validation, to assess the performance of our machine learning algorithms. The results from these methods showed that the classification performance increased with larger sample sizes but plateaued when around 350 isolates were integrated. This suggests that our panel of 414 isolates is sufficient for the analysis.\n\nIn terms of statistical significance, we reported exact P-values rather than using inequalities (e.g., P-values < 0.05). This approach provides a more precise measure of the significance of our results. Additionally, we ensured that our error bars in graphs were clearly labeled, indicating whether they represented standard deviations or standard errors of the mean, depending on the context.\n\nTo address potential biases, we collected clinical isolates from various sites across Europe to minimize local genetic effects. All sequenced isolates that passed quality control were included in the study, with no subsequent exclusion of samples after initial bioinformatic analyses. This approach helps to ensure that our results are generalizable and not influenced by local genetic variations.\n\nOverall, our methods and reporting practices aim to provide a transparent and statistically sound evaluation of our findings, ensuring that the confidence in our results is well-founded.",
  "evaluation/availability": "All sequencing data generated in this study is publicly available. RNA sequencing data can be accessed through the Gene Expression Omnibus under the accession number GSE123544. DNA sequencing reads are deposited in the Sequence Read Archive with the accession number PRJNA526797. These datasets are available without restrictions, allowing for reproducibility and further analysis by the scientific community.\n\nIndividual clinical isolates, their antibiotic resistance profiles, and supplier information are provided in the supplementary materials of the manuscript. This ensures transparency and allows other researchers to verify the findings and potentially build upon them.\n\nAdditionally, a dedicated repository has been created to accompany the manuscript. This repository contains the code and instructions necessary to reproduce the results presented in the paper. It also includes additional repositories with code to run the machine learning workflows used in the study. This resource is particularly valuable for researchers interested in replicating the analysis or adapting the methods for their own studies.\n\nThe data and code are shared with the intention of promoting open science and facilitating further research in the field. By making these resources publicly available, we aim to contribute to the broader scientific community and encourage collaboration and innovation."
}