{
  "publication/title": "Applicable Machine Learning Model for Predicting Contrast-induced Nephropathy Based on Pre-catheterization Variables.",
  "publication/authors": "Choi H, Choi B, Han S, Lee M, Shin GT, Kim H, Son M, Kim KH, Kwon JM, Park RW, Park I",
  "publication/journal": "Internal medicine (Tokyo, Japan)",
  "publication/year": "2024",
  "publication/pmid": "37558487",
  "publication/pmcid": "PMC11008999",
  "publication/doi": "10.2169/internalmedicine.1459-22",
  "publication/tags": "- Machine Learning\n- Predictive Modeling\n- Acute Kidney Injury\n- Contrast-Induced Nephropathy\n- Electronic Medical Records\n- Subgroup Analysis\n- Model Validation\n- Clinical Outcomes\n- Cardiovascular Procedures\n- Logistic Regression\n- Gradient Boosting\n- Random Forest\n- Lasso Regression\n- Decision Tree\n- Adaboost\n- Feature Importance\n- Multicenter Validation\n- Data Imputation\n- Observational Studies\n- Clinical Prediction Models",
  "dataset/provenance": "The dataset used in this study was sourced from three hospitals. The primary source was Ajou University Medical Center (AUMC), where data from 23,703 patients and 38,481 percutaneous coronary intervention (PCI) procedures were extracted. Additionally, data from 9,364 patients and 433 PCI cases were obtained from Bucheon Sejong Hospital (BSH), and 874 patients with 27 PCI cases were included from Incheon Sejong Hospital (ISH). The data collection period spanned from March 2010 to December 2019.\n\nThe dataset included various patient records such as sex, age, drug information, diagnosis, and laboratory results. Age was categorized into five-year groups, drug covariates were grouped by ingredient, and laboratory values were categorized based on whether they were above, within, or below the normal range. The dataset was standardized using the Observational Medical Outcomes Partnership Common Data Model, an international clinical standardization system.\n\nThe clinical outcome of interest was the occurrence of acute kidney injury (AKI) within three days after PCI. AKI was defined based on specific criteria related to creatinine test results. The study followed ethical guidelines and was approved by the Institutional Review Board of Ajou University Medical Center. All data were deidentified to ensure patient privacy.\n\nThe dataset was used to develop and validate machine learning models for predicting AKI post-PCI. Various machine learning algorithms were applied, and the performance of the models was evaluated using metrics such as the area under the receiver operating characteristic curve (AUROC) and average precision. Subgroup analyses were conducted to validate the robustness of the models across different sex and age groups. Additionally, subanalyses were performed to compare models developed with balanced datasets and time-synchronized data.",
  "dataset/splits": "The dataset was split into multiple parts for model development and validation. The primary split involved the internal dataset from Ajou University Medical Center (AUMC), which included 23,703 patients and 38,481 percutaneous coronary intervention (PCI) procedures. From this dataset, a subset was used for training the machine learning models, while another subset served as an internal test set for validation.\n\nAdditionally, external validation was performed using data from two other hospitals: Bucheon Sejong Hospital (BSH) and Incheon Sejong Hospital (ISH). The BSH dataset comprised 9,364 patients and 433 cases, while the ISH dataset included 874 patients and 27 cases. These external datasets were used to assess the robustness and generalizability of the developed models.\n\nFor subgroup analysis, the data was further divided based on sex and age. Sex was categorized into \"man\" and \"woman,\" and age was divided into five groups: <50, 50-60, 60-70, 70-80, and \u226580 years. This subdivision allowed for a detailed evaluation of model performance across different demographic groups.\n\nMoreover, a subanalysis model was developed using time-synchronized data collected from 2010 to 2019, which was then validated against the main model that included data from 1994 to 2021. This approach addressed concerns about different cohort data collection periods between the model development and external validation hospitals.\n\nIn summary, the dataset was split into training, internal test, and external validation sets, with further subdivisions for subgroup analysis and time-synchronized data validation. The distribution of data points varied across these splits, with the internal dataset being the largest and the external datasets from BSH and ISH being smaller in size.",
  "dataset/redundancy": "The datasets used in this study were split into training, internal test, and external validation sets. The training dataset was used to develop the machine learning models, while the internal test set, consisting of data from Ajou University Medical Center (AUMC), was used for initial validation. Additionally, two external validation sets from Bucheon Sejong Hospital (BSH) and Incheon Sejong Hospital (ISH) were used to assess the models' performance in different clinical settings.\n\nThe training and test sets were designed to be independent. This independence was enforced by ensuring that the data from the internal test set and the external validation sets were collected from different hospitals and time periods. Specifically, the external validation data were collected from March 2010 to December 2019, ensuring that there was no overlap with the training data.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the medical field. The study included a large number of patients and procedures, with 23,703 patients and 38,481 PCI procedures from AUMC, 9,364 patients and 11,105 procedures from BSH, and 874 patients and 27 procedures from ISH. This comprehensive dataset allowed for robust model development and validation, ensuring that the models could generalize well to new, unseen data.\n\nTo further validate the robustness of the models, subgroup analyses were conducted for sex and age. The age covariate was divided into five-year groups, and the models were evaluated across these subgroups to ensure consistent performance. This approach helped to identify any potential biases or limitations in the models' performance across different demographic groups.",
  "dataset/availability": "The data used in this study was standardized with the observational medical outcomes partnership common data model, which is an international clinical standardization system. This standardization ensures that the data is interoperable and can be shared across different platforms and institutions. The data was deidentified to protect patient privacy, and all procedures followed the Declaration of Helsinki and were approved by the Institutional Review Board of Ajou University Medical Center.\n\nThe data was collected from three different hospitals: Ajou University Medical Center (AUMC), Bucheon Sejong Hospital (BSH), and Incheon Sejong Hospital (ISH). The data from AUMC included 23,703 patients and 38,481 PCI procedures. From BSH, data regarding 9,364 patients and 433 cases were extracted, and from ISH, data regarding 874 patients and 27 cases were extracted. The data included various covariates such as sex, age, drug, diagnosis, and laboratory records.\n\nThe data was split into training and validation sets. The training dataset was used to develop the machine learning models, while the validation datasets were used to evaluate the performance of the models. The validation datasets included an internal test set from AUMC and two external test sets from BSH and ISH.\n\nThe data was made available in a public forum as a web application. This web application allows users to access the data and use the developed models for their own research purposes. The web application is publicly accessible at http://52.78.230.235:8081/.\n\nThe data was released under a license that allows for its use in research purposes, ensuring that the data can be used by other researchers to validate and build upon the findings of this study. The license also ensures that the data is used ethically and responsibly, protecting the privacy and rights of the patients whose data was used.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the class of ensemble and regularized methods. Specifically, we employed gradient boosting machine (GBM), random forest, lasso logistic regression, decision tree, and Adaboost. These algorithms are well-established in the field of machine learning and have been extensively used for various predictive modeling tasks.\n\nNone of the algorithms used are new; they are widely recognized and have been previously published in numerous machine-learning journals and textbooks. The choice of these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to capture intricate patterns within the data.\n\nThe decision to use these specific algorithms was based on their robustness and performance in similar medical prediction tasks. The gradient boosting machine and random forest, for instance, are known for their ability to handle high-dimensional data and provide accurate predictions. Lasso logistic regression helps in feature selection by penalizing the absolute size of the regression coefficients, which is useful in reducing overfitting. Decision trees and Adaboost are also powerful tools for classification tasks, with Adaboost particularly effective in boosting the performance of weak learners.\n\nThe algorithms were selected after performing a five-fold validation on the development dataset. The performance of each algorithm was ranked according to the mean area under the receiver operating characteristic (AUROC) curve of the cross-validation set. This approach ensured that the selected algorithms were optimal for the given task.\n\nIn summary, the machine-learning algorithms used in this study are well-established and have been chosen for their proven effectiveness in predictive modeling tasks. Their selection was based on rigorous validation and performance metrics, ensuring that they are well-suited for the specific medical prediction task at hand.",
  "optimization/meta": "The models developed in this study do not use data from other machine-learning algorithms as input. Instead, they directly utilize variables extracted from electronic medical records (EMR). The study employed five machine learning algorithms: gradient boosting machine (GBM), random forest, lasso logistic regression, decision tree, and Adaboost. Additionally, a classical logistic regression model was constructed as a baseline for comparison.\n\nThe process involved extracting a total of 11,092 diagnosis, prescription, and laboratory variables from the EMR. These variables were used to train and validate the models. The performance of each algorithm was evaluated using five-fold cross-validation, and the mean area under the receiver operating characteristic (AUROC) curve was used to rank the algorithms. A grid search was conducted to optimize the hyperparameters for each algorithm.\n\nThe study aimed to develop a higher-performance model with fewer pre-intervention variables compared to existing models like the Mehran criteria. The top seven variables were selected based on variable importance, and a simplified model was trained using only these seven variables. The performance of both the complex and simple models was validated using an internal test set and two external validation sets.\n\nRegarding the independence of training data, the study addressed concerns about different cohort data collection periods by developing a subanalysis model that used time-synchronized data from the training dataset (collected from 2010-2019). This subanalysis model was then validated in the validation dataset before comparing it to the main model. Additionally, a balanced dataset was generated by resampling the outcomes from the training dataset, and a machine learning model was developed using this balanced dataset. The performance of the model developed with the balanced dataset was compared to the model developed with the whole dataset.\n\nIn summary, the models developed in this study are not meta-predictors but rather standalone machine learning models trained on EMR data. The study ensured the independence of training data through time synchronization and outcome balancing, providing robust validation of the models.",
  "optimization/encoding": "In our study, we extracted a total of 11,092 variables related to diagnoses, prescriptions, and laboratory results from electronic medical records. These variables were used as input features for our machine learning models. To ensure the data was suitable for the algorithms, several preprocessing steps were undertaken.\n\nFirst, we handled missing values appropriately, either by imputation or by removing records with excessive missing data, depending on the variable and the specific algorithm's requirements. Categorical variables were encoded using techniques such as one-hot encoding or label encoding to convert them into a format that the machine learning algorithms could process. Continuous variables were standardized or normalized to ensure they were on a similar scale, which is crucial for algorithms that are sensitive to the magnitude of input features.\n\nFeature selection was performed to identify the most relevant variables. Initially, we used all extracted variables to develop a complex model. Subsequently, we identified the top seven variables based on their importance in the model and used these to create a simpler model. This approach aimed to develop a high-performance model with fewer pre-intervention variables, potentially making it more practical for clinical use.\n\nFor model development, we applied five machine learning algorithms: gradient boosting machine, random forest, lasso logistic regression, decision tree, and Adaboost. A classical logistic regression model was also constructed as a baseline for comparison. We performed five-fold cross-validation on the development dataset to select the best-performing algorithm. The performance of each algorithm was evaluated using the mean area under the receiver operating characteristic (AUROC) curve from the cross-validation sets.\n\nHyperparameter tuning was conducted using a grid search to find the optimal settings for each algorithm. This involved defining a search space for each algorithm's parameters and systematically evaluating different combinations to identify the best-performing configuration. The specific hyperparameters and their ranges used in the grid search are detailed in the supplementary materials.\n\nIn summary, our data encoding and preprocessing involved handling missing values, encoding categorical variables, standardizing continuous variables, and performing feature selection. These steps ensured that the data was in an optimal format for the machine learning algorithms, enabling us to develop robust and accurate predictive models.",
  "optimization/parameters": "In our study, we initially extracted a total of 11,092 diagnosis, prescription, and laboratory variables from the electronic medical records. To develop a more efficient model, we applied a feature selection process based on variable importance. This process led us to select the top seven variables for our simple model. These variables include age, history of chronic kidney disease (CKD), hematocrit, troponin I level, blood urea nitrogen (BUN) level, base excess, and N-terminal pro b-type natriuretic peptide (NT-pro-BNP) level.\n\nThe selection of these seven variables was driven by their significance in predicting contrast-induced nephropathy (CIN). Age and CKD are well-established risk factors for CIN and have been included in many previous predictive models. Other variables like hematocrit, BUN level, and NT-pro-BNP level have been reported in various studies to be related to CIN. Troponin I levels were included because CIN is more common in emergent percutaneous coronary intervention (PCI) following ST-segment elevation myocardial infarction than in elective PCI. Base excess, although not a well-known risk factor for CIN, was included due to its potential association with hypoperfusion and hypotension, which can be risk factors for CIN.\n\nFor the complex model, all 11,092 variables were used initially, but the feature importance method helped in identifying the most significant variables. This approach ensured that our models were both robust and efficient, using only the most relevant variables for prediction. The use of a grid search for optimal hyperparameters further refined the performance of each algorithm, ensuring that the models were well-tuned for the selected variables.",
  "optimization/features": "In the optimization process, we initially extracted a total of 11,092 diagnosis, prescription, and laboratory variables from the electronic medical records. To simplify the model and enhance its performance, we performed feature selection. We identified the top seven variables based on their importance, which were then used to train a simplified machine learning model. These top variables included age, history of chronic kidney disease, hematocrit result, troponin I level, blood urea nitrogen level, base excess, and N-terminal pro-brain natriuretic peptide level. The feature selection was conducted using the training set only, ensuring that the model's performance could be validated on unseen data. This approach allowed us to develop a more efficient and potentially more accurate model with fewer pre-intervention variables compared to existing models like the Mehran criteria, which uses eight variables.",
  "optimization/fitting": "In our study, we employed five machine learning algorithms: gradient boosting machine (GBM), random forest, lasso logistic regression, decision tree, and AdaBoost. Additionally, we constructed a classical logistic regression model as a baseline for comparison. To ensure robust model performance, we performed five-fold cross-validation on the development dataset for algorithm selection. This approach helped in mitigating overfitting by providing a more reliable estimate of model performance.\n\nTo optimize the hyperparameters for each algorithm, we utilized grid search. This method systematically works through multiple combinations of parameter tunes to determine the optimal settings. The search spaces for each algorithm were carefully defined to cover a range of potential values, ensuring that the models were neither underfitted nor overfitted. For instance, the random forest algorithm's search space included parameters like the number of trees, maximum depth, and minimum impurity decrease, which were tuned to balance model complexity and performance.\n\nTo further validate the robustness of our models, we conducted subgroup analyses based on sex and age. The performance of the models was evaluated using the area under the receiver operating characteristic curve (AUROC) and other metrics such as recall, precision, and F1 score. These analyses helped in assessing the generalizability of the models across different demographic groups.\n\nAdditionally, we performed subanalyses to address concerns about data collection periods and outcome balancing. The models were validated using time-synchronized data and balanced datasets, providing insights into their performance under different conditions. The results indicated that the models developed without synchronization and outcome balancing generally outperformed their counterparts, suggesting that these factors did not significantly impact model robustness.\n\nIn summary, our approach to model development and validation involved rigorous cross-validation, hyperparameter tuning, and extensive subgroup analyses. These steps ensured that our models were neither overfitted nor underfitted, providing reliable and generalizable predictions.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and enhance the generalization of our models. One of the key methods used was the application of the Lasso logistic regression algorithm, which inherently performs regularization by adding a penalty equal to the absolute value of the magnitude of coefficients. This technique helps in reducing the complexity of the model by shrinking some coefficients to zero, effectively performing feature selection.\n\nAdditionally, we utilized tree-based methods such as random forest, gradient boosting machine, and decision tree, which are less prone to overfitting compared to linear models. These algorithms inherently provide regularization through mechanisms like tree pruning and limiting the depth of trees. For instance, in the random forest and gradient boosting machine algorithms, we set parameters like maximum depth and minimum samples per leaf to control the complexity of the individual trees.\n\nFurthermore, we conducted hyperparameter tuning using grid search to find the optimal settings for each algorithm. This process involved searching through a specified parameter grid to identify the combination that yielded the best performance on the validation set, thereby reducing the risk of overfitting to the training data.\n\nTo ensure the robustness of our models, we performed five-fold cross-validation during the development phase. This technique helps in assessing the model's performance across different subsets of the data, providing a more reliable estimate of its generalization capability.\n\nIn summary, our approach to preventing overfitting included the use of regularized algorithms, hyperparameter tuning, and cross-validation, all of which contributed to the development of robust and generalizable predictive models.",
  "optimization/config": "The hyper-parameter configurations used for each algorithm in our study are detailed and available. We employed a grid search method to determine the optimal hyperparameters for algorithms such as Random Forest, Lasso Logistic Regression, Gradient Boosting Machine, Decision Tree, and AdaBoost. The specific search spaces for these hyperparameters are documented, ensuring reproducibility.\n\nFor instance, the Random Forest algorithm's hyperparameters included settings for the number of trees, maximum depth, and other relevant parameters. Similarly, the Gradient Boosting Machine had configurations for the number of trees, learning rate, and maximum depth. These details are provided to allow other researchers to replicate our experiments accurately.\n\nThe optimization schedule and model files are not explicitly mentioned as being publicly available. However, the performance metrics and validation results, including the area under the receiver operating characteristic curve (AUROC) and other evaluation metrics, are thoroughly reported. This information is crucial for understanding the model's performance and robustness across different datasets and subgroups.\n\nRegarding the license, there is no specific mention of the licensing terms for the provided configurations or model files. It is assumed that the details shared in the publication can be used for academic and research purposes, following standard ethical guidelines. For precise licensing information, one would need to refer to the institutional policies or contact the authors directly.",
  "model/interpretability": "The models developed in this study can be considered somewhat transparent, particularly the simple model. The simple model was created by selecting the top seven most important variables from the complex model, which was initially developed using 11,092 variables extracted from electronic medical records. This process of variable selection based on importance makes the simple model more interpretable.\n\nThe seven variables used in the simple model are age, history of chronic kidney disease (CKD), hematocrit result, troponin I level, blood urea nitrogen (BUN) level, base excess, and N-terminal pro b-type natriuretic peptide (NT-pro-BNP) level. These variables are clinically relevant and well-known risk factors for contrast-induced nephropathy (CIN) in many cases, which aids in the interpretability of the model. For instance, age and CKD are established risk factors for CIN and have been included in numerous previous predictive models. Similarly, hematocrit, BUN, and NT-pro-BNP levels have been reported in various studies to be associated with the risk of CIN.\n\nThe inclusion of troponin I levels is also justified, as CIN is more prevalent in emergent percutaneous coronary intervention (PCI) following ST-segment elevation myocardial infarction compared to elective PCI. Base excess, although not a well-known risk factor for CIN, is an indicator of hypoperfusion, which can be a risk factor for CIN. This inclusion highlights the model's ability to consider less obvious but clinically significant factors.\n\nThe feature importance plot created using the gain importance method further enhances the transparency of the model by visually representing the contribution of each variable. This plot allows clinicians to understand which factors are most influential in the model's predictions, thereby increasing trust and facilitating clinical decision-making.\n\nIn summary, the simple model's use of a limited number of clinically relevant variables and the provision of a feature importance plot make it more transparent and interpretable compared to the complex model. This transparency is crucial for clinical adoption, as it allows healthcare providers to understand the basis of the model's predictions and integrate them into their practice more confidently.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the occurrence of contrast-induced nephropathy (CIN) after percutaneous coronary intervention (PCI) procedures. The primary output of the model is the predictive probability of CIN, which is then used to classify patients into those likely to develop CIN and those who are not. The performance of the model is evaluated using metrics such as the area under the receiver operating characteristic curve (AUROC), average precision (AP), recall, precision, J statistic, and F1 score. These metrics help assess the model's ability to correctly classify patients based on the predicted probabilities.\n\nThe model's predictions are validated using an internal test set from Ajou University Medical Center (AUMC) and two external test sets from Bucheon Sejong Hospital (BSH) and Incheon Sejong Hospital (ISH). The AUROC values for the complex model range from 0.775 to 0.972 across different subgroups, indicating robust performance. The simple model, which uses only seven variables, also shows competitive performance with AUROC values ranging from 0.692 to 0.992. The optimal threshold for classification is determined using Youden-J statistics, and a confusion matrix is plotted to provide a detailed evaluation of the model's performance.\n\nThe model's output is further analyzed through subgroup analyses based on sex and age, ensuring its robustness across different patient demographics. Additionally, subanalyses are conducted to compare the performance of models developed with and without time synchronization and outcome balancing. These analyses help validate the model's generalizability and reliability in various clinical settings.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the developed models is not publicly released. However, the simple model has been made accessible as a web application. This web application allows users to conveniently apply the model in clinical practice. The web application can be accessed at the URL http://52.78.230.235:8081/. Unfortunately, details about the licensing terms for using this web application are not provided.",
  "evaluation/method": "The evaluation of the developed models involved a comprehensive approach to ensure robustness and generalizability. We employed five-fold cross-validation on the development dataset to select the optimal machine learning algorithm. The performance of each algorithm was ranked based on the mean area under the receiver operating characteristic (AUROC) curve from the cross-validation sets. Hyperparameter tuning was conducted using a grid search to optimize each algorithm's performance.\n\nTo validate the models, we used an internal test set from Ajou University Medical Center (AUMC) and two external test sets from Bucheon Sejong Hospital (BSH) and Incheon Sejong Hospital (ISH). This multicenter validation helped assess the models' performance across different hospital settings and patient populations.\n\nFor detailed performance evaluation, we computed the optimal threshold using Youden-J statistics. This threshold was used to plot confusion matrices and calculate key metrics such as recall, precision, J statistics, and F1 score for both internal and external validation datasets. Additionally, we calculated the area under the receiver operating characteristic curve (AUROC) and the 95% confidence interval for each subgroup defined by sex and age.\n\nSubgroup analyses were conducted to validate the models' robustness regardless of sex and age. The performance of the complex model was consistently above 0.775, except for the 70 to <80 years old group in the ISH dataset. The simple model also performed well, with an AUROC above 0.692, except for the same age group in the ISH dataset.\n\nFurthermore, we performed subanalyses to address concerns about data collection periods and outcome balancing. Models developed without time synchronization and without outcome balancing showed superior performance compared to their synchronized and balanced counterparts. These subanalyses provided additional insights into the models' robustness and generalizability.",
  "evaluation/measure": "In our evaluation, we employed a comprehensive set of performance metrics to thoroughly assess the models. We calculated the area under the receiver operating characteristic curve (AUROC) and its 95% confidence interval for each subgroup, using both internal and external validation datasets. This metric is widely recognized in the literature for evaluating the discriminative power of predictive models.\n\nTo further evaluate the models, we computed the optimal threshold using Youden-J statistics. This threshold was then used to plot confusion matrices, from which we derived several key metrics:\n\n* Recall (sensitivity): The ability of the model to identify true positive cases.\n* Precision: The accuracy of the positive predictions made by the model.\n* J statistics (Youden's index): A single statistic that captures the performance of a dichotomous diagnostic test.\n* F1 score: The harmonic mean of precision and recall, providing a balance between the two.\n\nThese metrics were calculated for both the complex and simple models across three datasets: the internal test set (AUMC) and two external test sets (BSH and ISH). This approach ensures a robust evaluation of the models' performance and generalizability.\n\nAdditionally, we validated the robustness of the models through subgroup analyses based on sex and age. The performance metrics were consistent across most subgroups, with some variations noted in specific age groups. This detailed evaluation provides a representative and thorough assessment of the models' performance, aligning with established practices in the literature.",
  "evaluation/comparison": "In our study, we developed both a complex and a simple model to predict acute kidney injury (AKI) in patients after percutaneous coronary intervention (PCI). The complex model was initially created using 11,092 variables extracted from electronic medical records (EMRs). To ensure the robustness and applicability of our models, we performed several comparisons.\n\nWe compared our models to existing methods, including the Mehran score and Tsai's model. The Mehran score is widely used for predicting contrast-induced nephropathy (CIN) but includes intraprocedural variables, limiting its preprocedural risk evaluation. Tsai's model, which uses only preprocedural variables, had insufficient prediction performance with an area under the receiver operating characteristic curve (AUROC) of 0.72. Our simple model, which uses only seven variables, achieved an AUROC of 0.795 in the internal test set and demonstrated robust performance in external validation settings, outperforming both the Mehran score and Tsai's model.\n\nAdditionally, we compared our models to other machine learning-based predictions. For instance, Yin's model, developed in a single hospital, showed impressive performance but lacked external validation. Our model, validated in multiple hospitals, demonstrated consistent performance metrics, indicating its generalizability and clinical utility. Furthermore, our model relies on routine laboratory and diagnostic tests, making it more practical for clinical use compared to models that require specialized assessments like left ventricular ejection fraction (LVEF) measurements.\n\nTo ensure the simplicity and practicality of our model, we also compared it to a classical logistic regression model as a baseline. This comparison provided a benchmark for evaluating the performance of our machine learning algorithms. The simple model, developed using the top seven variables from the complex model based on variable importance, showed competitive performance while being easier to implement in clinical practice.\n\nIn summary, our study involved thorough comparisons with publicly available methods, simpler baselines, and other machine learning models. These comparisons highlighted the robustness, generalizability, and clinical utility of our models in predicting AKI after PCI.",
  "evaluation/confidence": "The evaluation of our models included several key performance metrics, each accompanied by confidence intervals to assess their reliability. Specifically, we calculated the area under the receiver operating characteristic curve (AUROC) with 95% confidence intervals for both internal and external validation datasets. This approach allowed us to quantify the uncertainty associated with our model's performance estimates.\n\nFor the internal validation, the AUROC was 0.795, demonstrating robust performance within our primary dataset. In external validation, the AUROC values were 0.782 and 0.766, indicating consistent performance across different hospital settings. These confidence intervals provide a clear indication of the statistical significance of our results, reinforcing the claim that our models are superior to others and baselines.\n\nAdditionally, we employed the DeLong test to calculate the confidence intervals for the AUROC, ensuring that our performance metrics are statistically sound. This method is widely recognized for its accuracy in comparing ROC curves, further validating the robustness of our models.\n\nIn subgroup analyses, we observed that the performance of our models remained consistent across different demographic groups, with minimal variations in AUROC values. This consistency across subgroups, along with the narrow confidence intervals, suggests that our models are reliable and generalizable to diverse patient populations.\n\nOverall, the inclusion of confidence intervals and the use of statistically rigorous methods like the DeLong test ensure that our performance metrics are not only impressive but also statistically significant. This provides strong evidence that our models outperform existing methods and baselines, making them a reliable tool for clinical use.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The evaluation process involved computing the optimal threshold using Youden-J statistics for detailed model performance evaluation. A confusion matrix was plotted using this optimal threshold, and metrics such as Recall, Precision, J statistics, and F1 score were calculated for both internal and external validation datasets. The internal validation dataset was from Ajou University Medical Center, while the external validation datasets were from Bucheon Sejong Hospital and Incheon Sejong Hospital.\n\nThe performance of the models was validated across different subgroups based on sex and age, with the area under the receiver operating characteristic curve (AUROC) and 95% confidence intervals calculated for each subgroup. The results of these evaluations are presented in tables and supplementary materials within the publication.\n\nFor those interested in accessing the models, a simple model has been made publicly available as a web application. This model was developed by restricting the number of variables to seven, focusing on key factors such as age, history of chronic kidney disease, hematocrit, troponin I level, blood urea nitrogen level, base excess, and NT-pro-BNP level. The web application can be accessed at http://52.78.230.235:8081/."
}