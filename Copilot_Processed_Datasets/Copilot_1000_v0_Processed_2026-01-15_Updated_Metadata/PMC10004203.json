{
  "publication/title": "Intelligent medical image grouping through interactive learning.",
  "publication/authors": "Guo X, Yu Q, Li R, Alm CO, Calvelli C, Shi P, Haake A",
  "publication/journal": "International journal of data science and analytics",
  "publication/year": "2016",
  "publication/pmid": "36908375",
  "publication/pmcid": "PMC10004203",
  "publication/doi": "10.1007/s41060-016-0021-2",
  "publication/tags": "- Machine Learning\n- Expert-in-the-Loop\n- Image Grouping\n- Topic Modeling\n- Visual Analytics\n- Dermatology\n- Medical Imaging\n- Interactive Learning\n- Matrix Factorization\n- Sparse Coding",
  "dataset/provenance": "The dataset used in our work was collected offline from expert inputs. Specifically, 16 physicians were involved in the data collection process. They were asked to inspect 48 medical images and describe the image content aloud, simulating a teaching scenario. During this process, their eye movements were recorded, which helped highlight perceptually important image regions. These eye fixation maps were then used to filter image features, specifically SIFT features. Additionally, the physicians' verbal descriptions of the images were transcribed and analyzed to extract medical concepts using MetaMap, a medical language processing resource. These concepts formed a high-dimensional feature space, where each image is described by the occurrences of these medical concepts.\n\nThe dataset leverages both visual and textual information to create a comprehensive representation of the medical images. The visual information is derived from the SIFT features filtered by eye fixation maps, while the textual information comes from the medical concepts extracted from the physicians' descriptions. This multimodal approach allows for a more nuanced understanding of the images, incorporating both perceptual and semantic information.\n\nThe dataset has been used in previous work, including a conference presentation where preliminary results were shared. The current journal paper builds upon this work, incorporating additional methods for constructing the neighboring matrix, updating the underlying model, and evaluating the system's responsiveness to expert inputs. The dataset and the methods developed around it aim to minimize human efforts and provide experts with a good starting point for grouping images, ultimately improving the efficiency and accuracy of medical image analysis.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not applicable",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our work leverages a Laplacian sparse coding approach. This method was chosen over alternatives like latent semantic analysis (LSA) or latent Dirichlet allocation (LDA) because it performs better in clustering images by object. LSA does not match the effectiveness of Laplacian sparse coding in this regard, while LDA can be inconsistent due to its reliance on initial specifications and random samples generated during iterations. This inconsistency makes it difficult to support incremental changes, which is a key requirement for our interactive machine learning framework.\n\nThe algorithm is not entirely new; it builds upon established techniques in the field of machine learning. However, its application in our specific context\u2014integrating expert feedback for image grouping in a medical domain\u2014is novel. The focus of our publication is on the interactive paradigm and the integration of expert knowledge, rather than the optimization algorithm itself. This is why the algorithm is discussed within the context of our broader system, rather than in a dedicated machine-learning journal.\n\nThe algorithm's role is to facilitate the learning process by incorporating expert constraints, which are transformed into updates for the model. This interactive process allows for the refinement of image groupings based on expert input, ensuring that the final model aligns with the expert's understanding of the data. The use of Laplacian sparse coding enables the model to retain the relationships between observed data points, which is crucial for the accuracy and relevance of the image groupings.",
  "optimization/meta": "The model does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it integrates expert knowledge directly into the learning process through constraints provided by domain experts. These constraints are used to update the model, allowing for a more accurate and semantically meaningful image grouping.\n\nThe paradigm involves several key components, including the topic-coefficient matrix (C) and the topic-basis matrix (Q), which are updated based on expert interactions. Experts can influence the model by indicating the least relevant images for a selected topic, which updates the coefficient matrix C. Similarly, experts can modify the topic-term distributions, updating the topic-basis matrix Q. This interactive process ensures that the model learns from both the data and the expert's domain knowledge.\n\nThe model's design focuses on balancing global and local constraints. Global constraints, such as the neighboring matrix, ensure that the learned topics retain the relationships between observed data points. Local constraints, on the other hand, allow experts to make localized changes to a small subset of image relations. This balance is achieved through an interactive process where the machine and the expert agree upon decisions.\n\nThe paradigm also considers the trade-off between the model's power to capture underlying semantics and its simplicity for real-time interactions. The current implementation allows for expert input in approximately one minute, with the learning algorithms converging within ten seconds on a single-core machine. Future work may involve approximated learning rules and online learning algorithms to handle new data points more efficiently.\n\nIn summary, the model does not rely on other machine-learning algorithms for input but rather on direct expert interactions. The training data's independence is maintained through the expert's constraints, ensuring that the model learns from both the data and the expert's knowledge.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure that the machine-learning algorithm could effectively learn from the multimodal expert dataset. We began by collecting data from 16 physicians who inspected 48 medical images. During this process, their eye movements were recorded, which highlighted perceptually important image regions. These eye fixation maps were used to filter image features, specifically Scale-Invariant Feature Transform (SIFT) features. This filtering step helped to focus on the most relevant visual information, which is particularly useful in knowledge-rich domains like medical imaging.\n\nIn parallel, the physicians' verbal descriptions of the images were transcribed. These descriptions provided insights into the experts' diagnostic understanding. Medical concepts were extracted from the transcriptions using MetaMap, a medical language processing resource. These concepts formed a high-dimensional feature space, where each image was described by the occurrences of these medical concepts.\n\nTo initialize the image grouping, we adopted a Laplacian sparse coding approach. This method was chosen over latent semantic analysis (LSA) or latent Dirichlet allocation (LDA) because it performs better in clustering images by object and supports incremental changes, which are essential for our expert-in-the-loop paradigm.\n\nThe objective function for our data fusion framework included matrices representing eye gaze-filtered image features and verbal features. The model was designed to be flexible, allowing for the incorporation of additional data modalities. The coefficient matrix stored the new image representations, each of which was a distribution of latent topics learned from the basis matrices. These matrices revealed the transformation from the original feature spaces to latent topics.\n\nThe constraints in the Laplacian sparse coding helped capture the underlying semantics behind observations in both modalities. A neighboring matrix indicated similarities between pairs of data instances, and a multimodal variation of the feature-sign search algorithm was developed to update elements of each data instance selectively. This approach tackled the non-derivativeness of the l1-norm, ensuring that the sparse codes learned reflected expert image understanding in the specific domain.",
  "optimization/parameters": "The model employs several parameters that are integral to its functioning and optimization. The primary parameters include those within the objective functions used for data fusion and topic modeling. Specifically, the matrices E and V represent eye gaze-filtered image features and verbal features, respectively. The coefficient matrix C stores the new image representations, while the basis matrices P and Q reveal the transformation from the original feature spaces to latent topics.\n\nThe selection of these parameters is guided by the need to capture underlying semantics in both visual and verbal data modalities. The sparsity constraint and graph-regularizer in the objective functions help in achieving this. The neighboring matrix W indicates similarities between pairs of data instances, and its update is crucial for maintaining the consistency of image groupings across interactions.\n\nAdditionally, parameters such as \u03b1 and \u03b2 in the objective functions control the influence of the graph-regularizer and sparsity constraint, respectively. These parameters are tuned to balance the trade-off between capturing the underlying semantics and achieving good responsiveness for real-time interactions.\n\nThe model also includes parameters for handling expert inputs, such as \u03b31 and \u03b32, which are used in penalty terms to encode expert interactions and updates to the topic-term distributions. These parameters ensure that the model can adapt to expert feedback and improve the accuracy of image groupings over successive interactions.\n\nIn summary, the model uses a set of parameters that are carefully selected and tuned to optimize the balance between semantic capture and responsiveness. The exact number of parameters can vary depending on the specific implementation and the size of the datasets used. However, the key parameters include those in the objective functions, the neighboring matrix, and the penalty terms for expert inputs.",
  "optimization/features": "The input features used in our optimization process are derived from multiple modalities. Specifically, we utilize eye gaze-filtered image features and verbal features. The eye gaze-filtered image features are represented by a matrix E, where the number of visual words is denoted by ne. The verbal features are represented by a matrix V, with nv indicating the number of verbal features. The total number of images is represented by m.\n\nFeature selection is implicitly performed through the use of eye gaze data to filter the image features. This process highlights perceptually important image regions, which are particularly useful in knowledge-rich domains. The verbal features are extracted from transcriptions of experts' descriptions of the images, focusing on medical concepts that provide insights into diagnostic understanding.\n\nThe feature selection process is conducted using an offline collected expert dataset, ensuring that the features are derived independently of the training set used in the optimization process. This approach helps in creating a robust initial image grouping that can be further refined through expert interactions.",
  "optimization/fitting": "In our optimization process, we employ a learning framework that incorporates expert inputs through constraints, which helps to manage the complexity of the model and prevent overfitting. The number of parameters in our model is indeed large, given the high-dimensional feature space derived from both visual and verbal data. However, the integration of expert constraints serves as a regularization mechanism, ensuring that the model does not overfit to the training data.\n\nTo rule out overfitting, we utilize a joint regularization strategy that minimizes the offset between the neighborhood in the topic space and that in the visualization space. This approach helps to maintain the consistency and relevance of the image groupings. Additionally, we plan to develop a penalty term to isolate images that are repeatedly skipped by the expert, further refining the model's performance.\n\nOn the other hand, underfitting is addressed through the interactive process where experts provide localized changes and constraints. These inputs are incorporated into the model, allowing it to capture the underlying semantics more accurately. The model is designed to be flexible and adaptive, with the ability to update parameters in the neighboring graph to reflect relative similarities among neighbors locally.\n\nThe balance between overfitting and underfitting is achieved through the iterative interaction between the machine and the expert. The model learns incrementally, ensuring that it remains responsive to new data points and expert inputs. This dynamic process helps to maintain the model's generalization capability while capturing the nuances of the data.",
  "optimization/regularization": "In our work, we employed several regularization techniques to prevent overfitting and ensure the robustness of our model. One key method involves the use of Laplacian sparse coding, which incorporates sparsity constraints and graph regularizers. These constraints help capture the underlying semantics in both visual and verbal modalities, ensuring that the learned representations are meaningful and generalizable.\n\nAdditionally, we developed a joint regularization strategy to minimize the offset between the neighborhood in the topic space and that in the visualization space. This approach ensures that the visualizations accurately reflect the learned topics, maintaining consistency between the two spaces.\n\nTo further enhance the model's flexibility and generalization, we plan to replace hard constraints with soft ones. This allows the parameters in the neighboring graph to be learned and adapted, reflecting the relative similarities among neighbors locally. Soft constraints also help balance the influences between offline collected expert data and online expert inputs, encoding expert interactions in a new penalty term.\n\nMoreover, we consider approximated learning rules for better responsiveness and online learning algorithms to handle new data points efficiently. These techniques collectively contribute to a more robust and adaptable model, capable of handling larger datasets and providing accurate visualizations.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model presented in this publication is designed with a strong emphasis on transparency and interpretability, making it far from a black-box system. The interactive machine learning paradigm explicitly involves experts in the loop, ensuring that their domain knowledge is integrated into the model. This approach allows for a clear understanding of how the model arrives at its groupings.\n\nOne of the key aspects of transparency in our model is the use of constraints derived from expert interactions. Experts encode their domain knowledge by grouping a subset of images, and these groupings are transformed into constraints that guide the model's learning process. This means that the decisions made by the model are directly influenced by the expert's input, making the reasoning process transparent.\n\nAdditionally, the model's visualization component plays a crucial role in interpretability. The visual analytics tools allow experts to explore both the global structure and local details of the image groupings. This visual feedback helps experts understand how their inputs affect the model and provides insights into the underlying patterns in the data.\n\nThe model also supports iterative refinement, where experts can provide constraints in multiple rounds. This iterative process allows for continuous improvement and adaptation of the model based on expert feedback, further enhancing its transparency. The stability of the model ensures that the final groupings are consistent as long as the same set of constraints is provided, regardless of the order in which they are applied.\n\nIn summary, the model's transparency is achieved through the integration of expert knowledge, the use of visual analytics for exploration, and an iterative refinement process. These features ensure that the model's decisions are interpretable and aligned with expert understanding.",
  "model/output": "The model is primarily focused on image grouping, which is a form of unsupervised learning rather than traditional classification or regression. It aims to organize dermatological images into meaningful groups based on various features and expert inputs. The paradigm involves an interactive process where experts provide constraints that guide the learning algorithm to refine the image groupings. This process helps in transforming image pixels into interpretable content, making it particularly useful in knowledge-rich domains like dermatology. The model updates incrementally with each interaction, ensuring consistency in image groupings and visualizations. The evaluation of the model shows improved performance in image grouping when compared to fully automated machine learning approaches, highlighting the effectiveness of incorporating expert knowledge.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our evaluation, we assessed the performance of image grouping using various metrics and experimental setups. We measured the percentage of images in a reference list that appeared within the top 5, 10, 15, and 20 retrieved neighbors. This evaluation was conducted across different modalities, including primary morphology terms, body location terms, correct diagnoses terms, SIFT features, SIFT features filtered by gaze features, and multimodal data.\n\nWe compared two main cases: fully automated learning and our paradigm, which incorporates expert input. For the fully automated learning case, we employed several clustering and topic modeling techniques, such as K-means, PCA, hierarchical clustering, latent semantic analysis, latent Dirichlet allocation, and Laplacian sparse coding. Each method was evaluated based on its ability to retrieve relevant images within the specified top N neighbors.\n\nOur paradigm, which integrates expert knowledge through an interactive process, demonstrated superior performance across all evaluated metrics. The expert input provided local constraints that helped achieve a better balance in image grouping. This interactive approach allowed for incremental improvements, making the system more responsive to expert inputs and enhancing its scalability.\n\nThe evaluation also included a comprehensive analysis of system responsiveness, scalability, and other implementation considerations. We presented preliminary results in a conference and extended our work in this journal paper, including additional methods for constructing the neighboring matrix and updating the underlying model. The interface design and supported expert image manipulations were also detailed, along with an expert-in-the-loop evaluation study.",
  "evaluation/measure": "In our evaluation, we focused on measuring the performance of image grouping by assessing the percentage of images in the reference list that appear within the top retrieved neighbors. Specifically, we reported performances for the top 5, 10, 15, and 20 retrieved neighbors. These metrics provide a comprehensive view of how well our paradigm and the fully automated learning case perform across different retrieval thresholds.\n\nThe choice of these metrics is representative of common practices in the literature for evaluating image retrieval and clustering tasks. By examining the top retrieved neighbors, we can gauge the effectiveness of our methods in capturing relevant images that match the reference groupings. This approach allows us to compare the performance of various modalities, including verbal features, SIFT features, and multimodal data, under different algorithms such as K-means, PCA, hierarchical clustering, latent semantic analysis, latent Dirichlet allocation, and Laplacian sparse coding.\n\nOur results highlight that multimodal features consistently achieve the best performance, indicating the importance of integrating multiple data sources. Additionally, the expert interactive constraints in our paradigm significantly enhance the image grouping results, particularly when using verbal features of correct diagnoses. This suggests that diagnoses are a primary factor considered by experts in grouping medical images.\n\nOverall, the reported metrics are designed to provide a clear and comparative assessment of our methods' effectiveness, aligning with established evaluation practices in the field.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated our paradigm against various fully automated learning methods using benchmark datasets. The comparison included several established techniques such as K-means, PCA, hierarchical clustering, latent semantic analysis, latent Dirichlet allocation, and Laplacian sparse coding. These methods were applied to different combinations of modalities, including primary morphology terms, body location terms, correct diagnoses terms, SIFT features, SIFT features filtered by gaze features, and multimodal data.\n\nWe assessed the performance of these methods by measuring the percentage of images in the reference list that appeared within the top 5, 10, 15, and 20 retrieved neighbors. This comprehensive evaluation allowed us to compare our paradigm's effectiveness against simpler baselines and more complex automated learning approaches. The results highlighted the strengths and weaknesses of each method, providing a clear picture of where our paradigm stands in relation to existing techniques.",
  "evaluation/confidence": "In our evaluation, we focused on the performance metrics related to the percentage of images in the reference list that appeared within the top 5, 10, 15, and 20 retrieved neighbors. These metrics were used to compare the effectiveness of different image grouping methods, including fully automated learning and our paradigm involving expert interaction.\n\nRegarding confidence intervals, our study did not explicitly report them for the performance metrics presented. The results were primarily shown as percentages, highlighting the best performances in bold. This approach allowed for a clear comparison between different methods and modalities.\n\nStatistical significance was not explicitly discussed in the context provided. However, the consistent outperforming of our paradigm over fully automated learning, particularly with verbal features of correct diagnosis, suggests a strong indication of superiority. The qualitative evaluation from the expert, who noticed improvements in each iteration, further supports the effectiveness of our approach.\n\nWhile we did not provide detailed statistical tests or p-values, the clear and consistent performance differences observed across various modalities and top retrieved neighbors imply that the results are likely statistically significant. Future work could involve more rigorous statistical analysis to quantify these observations and provide confidence intervals for the performance metrics.",
  "evaluation/availability": "The raw evaluation files for our study are not publicly available. The data used in our research includes sensitive medical images and expert annotations, which are subject to strict privacy and ethical considerations. Therefore, we have not released these files to the public.\n\nHowever, we have provided detailed descriptions of our evaluation methodology and results in the publication. This includes the performance metrics used to assess image grouping, such as the percentage of images in the reference list appearing within the top 5 retrieved neighbors. We have also included tables and figures that illustrate the effectiveness of our expert-in-the-loop paradigm compared to fully automated learning methods.\n\nFor researchers interested in replicating or building upon our work, we recommend following the methodologies described in our paper. While the raw data cannot be shared, the techniques and approaches outlined can be applied to similar datasets with appropriate ethical considerations and permissions."
}