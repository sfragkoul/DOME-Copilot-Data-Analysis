{
  "publication/title": "A machine learning based approach to identify carotid subclinical atherosclerosis endotypes.",
  "publication/authors": "Chen QS, Bergman O, Ziegler L, Baldassarre D, Veglia F, Tremoli E, Strawbridge RJ, Gallo A, Pirro M, Smit AJ, Kurl S, Savonen K, Lind L, Eriksson P, Gigante B",
  "publication/journal": "Cardiovascular research",
  "publication/year": "2023",
  "publication/pmid": "37475157",
  "publication/pmcid": "PMC10730242",
  "publication/doi": "10.1093/cvr/cvad106",
  "publication/tags": "- Machine Learning\n- Cardiovascular Disease\n- Endotypes\n- Subclinical Atherosclerosis\n- Data Analysis\n- Bioinformatics\n- Network Analysis\n- Predictive Modeling\n- Cardiovascular Imaging\n- Biomarkers",
  "dataset/provenance": "The dataset used in this study originates from the IMPROVE study, which was funded by the Vth European Union programme. This study involves seven recruiting centers across five European countries. The IMPROVE study is a well-known dataset in the cardiovascular research community, having been utilized in various previous research endeavors.\n\nThe dataset comprises a comprehensive set of 124 variables, which were processed using advanced analytical techniques. These variables were standardized using Z-standardization to ensure consistency and comparability across different measurements. The dataset includes continuous variables that were categorized as either low or high based on their standardized values relative to the mean.\n\nThe IMPROVE study has been a valuable resource for the scientific community, contributing to numerous studies and analyses in the field of cardiovascular health. The dataset's extensive use and the rigorous methods applied in this study ensure its reliability and relevance for further research.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in this study are not publicly released in a forum. However, they are available upon reasonable request to the corresponding author. This approach ensures that the data can be accessed by researchers for verification or further study while maintaining control over its distribution. There is no specific license mentioned for the data, but the availability upon request implies that appropriate use and citation would be expected. The enforcement of this policy is managed through direct communication with the corresponding author, who can evaluate the reasonableness of each request.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages a well-established machine-learning framework known as Optuna. Optuna is a next-generation hyperparameter optimization framework designed to streamline the process of tuning machine learning models. It is not a new algorithm but rather a sophisticated tool that enhances the efficiency and effectiveness of hyperparameter optimization.\n\nOptuna was developed to address the challenges associated with hyperparameter tuning, which is a critical step in building robust machine learning models. By automating the search for optimal hyperparameters, Optuna helps to improve model performance and reduce the time and effort required for manual tuning.\n\nThe decision to use Optuna in our study was driven by its proven track record and widespread adoption in the machine learning community. Optuna has been successfully applied in various domains, demonstrating its versatility and reliability. Its integration into our analytical pipeline was seamless, thanks to its compatibility with popular machine learning libraries and frameworks.\n\nWhile Optuna is a powerful tool for hyperparameter optimization, it is important to note that it is not a machine learning algorithm per se. Instead, it serves as a complementary tool that enhances the performance of existing machine learning models. Therefore, it was not published in a machine-learning journal but rather in the proceedings of a conference focused on knowledge discovery and data mining. This reflects its role as a tool for optimizing machine learning workflows rather than as a standalone algorithm.",
  "optimization/meta": "The model employed in this study is an ensemble stacking model, which serves as a meta-predictor. This means it uses the predictions from multiple machine-learning algorithms as input to make its final predictions. The stacking model integrates outputs from various base models to enhance predictive performance.\n\nThe base models contributing to the stacking model include a multilayer perceptron with an encoder (Encoder-MLP) for dimensional data reduction and feature representation learning. Additionally, weighted gene co-expression network analysis (WGCNA) is utilized to identify modules of co-expressed proteins, which helps in generalizing findings across different analytical platforms.\n\nThe training data for the stacking model is designed to be independent. This independence is crucial for ensuring that the model's performance is robust and that it generalizes well to new, unseen data. The stacking model was built to predict endotypes and test their replicability, with details provided in the supplementary material.\n\nPython 3.7 was used for endotype generation, prediction model building, and model interpretation. Key modules and frameworks utilized include scikit-learn, torch, Optuna, and SHAP. These tools were essential for developing and optimizing the stacking model, ensuring that it effectively integrates the outputs from various base models to improve predictive accuracy.",
  "optimization/encoding": "In our study, we employed a multilayer perceptron with an encoder (Encoder-MLP) to process the 124 variables included in the analysis. This approach was used for dimensional data reduction and to learn features related to carotid intima-media thickness (c-IMT) mean-max. The encoder ensured that features unrelated to c-IMT mean-max were filtered out, while those relevant to it were retained.\n\nAll continuous variables were standardized using Z-standardization. This process involved calculating the standardized value of each variable using the formula: (individual value \u2212 mean) / (standard deviation). Variables with values below the standardized mean were defined as low, while those above were defined as high.\n\nAdditionally, we utilized weighted gene co-expression network analysis (WGCNA) to identify modules of co-expressed proteins. This method helped generalize our findings, as proteins within the same module often share similar biological functions and can be used interchangeably across different analytical platforms. The modules were visualized using dendrograms, and heatmaps were generated to compare Z-standardized Olink CVD-I protein levels across the four identified endotypes.\n\nThe endotypes were defined using two methods: those generated by the Encoder-MLP and hierarchical clustering were termed original endotypes, while those replicated by the stacking model were termed predicted endotypes. The entire process, including endotype generation, prediction model building, and model interpretation, was conducted using Python 3.7. The key modules and frameworks utilized for model development included scikit-learn, torch, Optuna, and SHAP.",
  "optimization/parameters": "In our study, we utilized a multilayer perceptron with an encoder (Encoder-MLP) for dimensional data reduction and feature representation learning related to c-IMT mean-max. The number of parameters (p) in the model was determined through a combination of domain knowledge and empirical testing.\n\nWe started with an initial set of 124 variables, which were processed using the Encoder-MLP. This step ensured that features unrelated to c-IMT mean-max were filtered out, while those related were retained. The continuous variables were standardized using Z-standardization.\n\nTo select the optimal number of parameters, we employed a hyperparameter optimization framework. This framework systematically explored different configurations of the model, including the number of layers and neurons in the Encoder-MLP, to identify the setup that best balanced model complexity and performance. The optimization process was guided by metrics such as the global SHAP value, which helped in ranking the most relevant variables in the stacking model.\n\nAdditionally, we used weighted gene co-expression network analysis (WGCNA) to identify modules of co-expressed proteins. This approach helped in generalizing our findings, as proteins belonging to the same module might have similar biological functions and could be used interchangeably across analytical platforms.\n\nThe final model incorporated 124 variables, but the effective number of parameters was reduced through the dimensionality reduction and feature selection processes. This ensured that the model was both efficient and effective in capturing the relevant information for predicting endotypes.",
  "optimization/features": "The analysis utilized 124 variables as input features. These features underwent processing through a multilayer perceptron with an encoder, specifically designed for dimensional data reduction and learning features related to carotid intima-media thickness (c-IMT) mean-max. This step ensured that features unrelated to c-IMT mean-max were filtered out, retaining only those relevant to the analysis.\n\nFeature selection was implicitly performed during this process, as the encoder-multilayer perceptron (Encoder-MLP) focused on retaining features pertinent to c-IMT mean-max. This selection process was conducted using the training set, ensuring that the features retained were those that best represented the underlying patterns in the data related to c-IMT mean-max.",
  "optimization/fitting": "The fitting method employed in this study involved a multilayer perceptron with an encoder (Encoder-MLP) for dimensionality reduction and feature representation learning. This approach was crucial for handling the 124 variables included in the analysis, ensuring that features unrelated to the target variable (c-IMT mean-max) were filtered out.\n\nTo address the potential issue of overfitting, given the high number of variables relative to the training points, several strategies were implemented. Firstly, the data was standardized using Z-standardization, which helps in normalizing the features and making the model more robust. Secondly, the use of an encoder in the MLP helped in reducing the dimensionality of the data, focusing on the most relevant features and thereby mitigating overfitting. Additionally, the stacking model utilized for endotype prediction included hyperparameter optimization using Optuna, which systematically searched for the best model parameters to prevent overfitting. The model's performance was also validated through internal and external validation processes, ensuring its generalizability.\n\nUnderfitting was addressed by ensuring that the model was complex enough to capture the underlying patterns in the data. The use of a multilayer perceptron with an encoder allowed for the extraction of meaningful features from the data, reducing the risk of underfitting. Furthermore, the stacking model combined multiple learning algorithms, leveraging their strengths to improve predictive performance and reduce the likelihood of underfitting. The model's ability to predict endotypes accurately, as shown in the supplementary material, further supports the adequacy of the fitting method in capturing the necessary complexity in the data.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our models. One of the key methods used was dimensionality reduction through a multilayer perceptron with an encoder. This approach helped in filtering out features unrelated to the target variable, thereby focusing the analysis on the most relevant data.\n\nAdditionally, we utilized Z-standardization for all continuous variables. This process involved transforming the data so that each variable had a mean of zero and a standard deviation of one. This standardization helped in making the model more robust and less sensitive to the scale of the input features.\n\nWe also implemented an ensemble stacking model, which combined multiple learning algorithms to improve the overall performance and generalization of the predictions. This technique helped in reducing the risk of overfitting by leveraging the strengths of different models.\n\nFurthermore, we employed hyperparameter optimization using Optuna, a next-generation framework designed to efficiently search for the best hyperparameters. This process ensured that our models were tuned to perform optimally without overfitting to the training data.\n\nIn summary, our approach included dimensionality reduction, data standardization, ensemble modeling, and hyperparameter optimization to effectively prevent overfitting and enhance the reliability of our results.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available in the supplementary materials. Specifically, details about the hyper-parameter optimization process can be found in the supplementary section titled 'Building of the stacking model'. This section outlines the methods and parameters used for tuning the models, including the use of Optuna for hyper-parameter optimization.\n\nThe model files and optimization parameters are not explicitly provided as standalone files but are described within the supplementary materials. The Python code used for endotype generation, prediction model building, and model interpretation is available upon request. The code utilizes several Python modules and frameworks, including scikit-learn, torch, Optuna, and SHAP.\n\nRegarding the availability and licensing of the materials, the supplementary materials are accessible to readers of the publication. The code and specific configurations can be shared under reasonable request, adhering to standard academic sharing practices. However, there is no specific open-source license mentioned for the code or supplementary materials.",
  "model/interpretability": "The model employed in our study is not a black-box system. We have incorporated several techniques to ensure interpretability. One key method used is SHAP (SHapley Additive exPlanations) values. SHAP values provide a way to interpret the output of machine learning models by attributing the contribution of each feature to the prediction. A SHAP value of zero indicates that a variable does not contribute to the prediction of an endotype. Positive SHAP values suggest that a variable is likely a predictor of the endotype, while negative values indicate that the variable is not a predictor for that individual.\n\nAdditionally, we utilized Z-scores to standardize the variables, which helps in understanding the relative importance of each feature. Variables with Z-scores above zero are highlighted in red, indicating higher values, while those below zero are in light blue. This visualization aids in quickly identifying which variables have the most significant impact on the predictions.\n\nWe also ranked the variables' importance for each endotype using the global absolute SHAP values. This ranking allows us to see which features are most relevant for predicting each endotype, providing transparency into the model's decision-making process.\n\nFurthermore, we employed weighted gene co-expression network analysis (WGCNA) to identify modules of co-expressed proteins. This approach helps in generalizing our findings, as proteins within the same module may have similar biological functions and can be used interchangeably across different analytical platforms. The modules were visualized using dendrograms and heatmaps, which offer a clear representation of the relationships between variables and their contributions to the endotypes.\n\nIn summary, our model's interpretability is enhanced through the use of SHAP values, Z-scores, variable ranking, and WGCNA. These techniques collectively ensure that the model's predictions are transparent and understandable, allowing for clear insights into the underlying mechanisms driving the endotype predictions.",
  "model/output": "The model developed in this study is primarily a classification model. It is designed to predict endotypes, which are distinct subgroups within a population that share similar characteristics or disease mechanisms. The model uses an ensemble stacking approach to classify individuals into one of four endotypes. This classification is based on various variables and features, including those related to c-IMT mean-max, which were processed using a multilayer perceptron with an encoder for dimensional data reduction.\n\nThe model's performance is evaluated using metrics such as SHAP values, which measure the contribution of each variable to the prediction of an endotype. The SHAP values help in understanding the importance of different variables in the classification process. Additionally, the model's reliability in predicting different endotypes is assessed, with endotypes 1 and 4 being predicted more reliably compared to endotypes 2 and 3.\n\nThe stacking model integrates multiple layers of machine learning algorithms, including a multilayer perceptron for feature representation learning and a weighted gene co-expression network analysis to identify modules of co-expressed proteins. These modules are visualized using dendrograms and heatmaps, which aid in interpreting the relationships between variables and endotypes.\n\nIn summary, the model is a classification model that predicts endotypes using a combination of dimensional data reduction, feature representation learning, and ensemble stacking techniques. The model's output includes the classification of individuals into endotypes and the assessment of variable importance through SHAP values.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models and algorithms used in this study is not publicly released. However, the software and frameworks utilized for model development are well-documented and publicly available. These include scikit-learn, torch, Optuna, and SHAP. These tools are widely used in the machine learning community and can be accessed through their respective repositories. The specific implementations and configurations used in this study are detailed in the supplementary materials, allowing for reproducibility by researchers familiar with these tools. The Python version used for the endotype generation, prediction model building, and model interpretation is Python 3.7.",
  "evaluation/method": "The evaluation method employed for this study involved a comprehensive approach to ensure the robustness and replicability of the findings. An ensemble stacking model was built to predict endotypes, and their replicability was tested. This model integrated multiple layers of analysis, including dimensional data reduction using a multilayer perceptron with an encoder, and feature representation learning focused on specific cardiovascular metrics.\n\nThe evaluation process included internal and external validation steps. Internally, the model was validated using a replicated dataset from the same study, ensuring that the results were consistent within the dataset. This internal validation helped to confirm the stability and reliability of the endotypes identified.\n\nFor external validation, the model's performance was assessed using independent datasets, which provided an additional layer of confidence in the generalizability of the findings. This step is crucial for demonstrating that the identified endotypes are not merely artifacts of the specific dataset used for training but are indeed representative of broader patterns in the data.\n\nAdditionally, the study utilized various statistical methods to evaluate the association of endotypes with different cardiovascular outcomes. Linear regression models were employed to estimate the relationship between endotypes and specific cardiovascular measures, such as carotid intima-media thickness (c-IMT) and plaque characteristics. These models were adjusted for potential confounders to ensure that the observed associations were robust and not due to extraneous variables.\n\nSurvival analysis using Kaplan\u2013Meier curves and Cox proportional hazard models was conducted to assess the risk of atherosclerotic cardiovascular disease (ASCVD) associated with different endotypes. These analyses provided insights into the prognostic value of the identified endotypes, demonstrating their potential utility in clinical settings.\n\nOverall, the evaluation method combined internal and external validation with rigorous statistical analysis to ensure the reliability and generalizability of the findings. This multifaceted approach strengthens the confidence in the identified endotypes and their potential applications in cardiovascular research and clinical practice.",
  "evaluation/measure": "In the evaluation of our study, we focused on several key performance metrics to assess the effectiveness of our analytical pipeline and models. We utilized the net reclassification improvement (NRI) to evaluate the predictive effect of endotypes, which is a well-established metric in the literature for assessing the improvement in risk stratification. The 95% confidence intervals (CIs) of NRI were estimated using 1000-time bootstrapping resampling, ensuring the robustness of our findings.\n\nAdditionally, we employed hierarchical clustering to define four subclinical carotid atherosclerosis endotypes, which were validated through various steps in our analytical pipeline. The performance of our stacking model in predicting these endotypes was also evaluated, particularly in external datasets such as PIVUS and PACIFIC. This approach allowed us to assess the generalizability and replicability of our endotypes across different populations.\n\nThe use of SHAP (SHapley Additive exPlanations) values was crucial in ranking the most relevant variables for each endotype. This method provides a unified approach to interpreting model predictions, which is consistent with current best practices in the field. We also utilized weighted gene co-expression network analysis (WGCNA) to identify modules of co-expressed proteins, further enhancing the biological interpretability of our findings.\n\nOverall, the performance metrics reported in our study are representative of those commonly used in the literature for similar types of analyses. The combination of NRI, hierarchical clustering, SHAP values, and WGCNA ensures a comprehensive evaluation of our models and the biological relevance of the identified endotypes.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. However, we did validate our results using multiple datasets and analytical strategies.\n\nWe internally validated our results in the IMPROVE replicated dataset using the same outcome measures as the derived dataset. Additionally, we validated our findings in the PIVUS dataset, where we estimated the association of the predicted endotypes with various cardiovascular measures and risks.\n\nFor the PACIFIC dataset, due to the limited number of patients in each predicted endotype group, we did not perform an inference analysis. Instead, we looked at the distribution of high-risk coronary artery plaques and the prevalence of coronary heart disease.\n\nWe also compared the predictive performance of our endotypes with established risk prediction scores, such as SCORE2 and SCORE2-OP. We estimated the C-statistics and the net reclassification improvement (NRI) of our endotypes compared to these scores, as well as to other measures like c-IMT mean-max and the presence of carotid plaque.\n\nWhile we did not use simpler baselines for direct comparison, our analytical pipeline included dimensionality reduction and feature representation learning steps, which are more complex than simple baseline models. We also used an ensemble stacking model to predict the endotypes, which is a more advanced approach than using a single model.\n\nIn summary, although we did not compare our methods with publicly available benchmarks or simpler baselines directly, we validated our results using multiple datasets and analytical strategies, and we compared our endotypes' predictive performance with established risk prediction scores.",
  "evaluation/confidence": "The evaluation of our study includes several statistical analyses to ensure the robustness and significance of our findings. We employed linear regression models to estimate the association of endotypes with various carotid measures, both at baseline and after a 30-month follow-up. The estimates from these models are expressed as coefficients (\u03b2) and standard error (SE), providing a clear indication of the precision of our estimates.\n\nTo assess the risk of atherosclerotic cardiovascular disease (ASCVD), we used a Cox proportional hazard model. The results are presented as hazard ratios (HR) with 95% confidence intervals (CI), which allow us to evaluate the statistical significance and the range within which the true effect size is likely to lie.\n\nWe also performed internal validation within the IMPROVE study using a replicated dataset to ensure the reproducibility of our results. This step is crucial for confirming that our findings are not due to random chance.\n\nStatistical significance was set at a p-value of 0.05 for all inferences, ensuring that our results are robust and not likely due to random variation. Additionally, we used bootstrapping resampling to estimate the 95% confidence intervals of the Net Reclassification Improvement (NRI), further enhancing the reliability of our risk stratification improvement evaluations.\n\nIn summary, our evaluation includes comprehensive statistical analyses with confidence intervals and significance testing, ensuring that our claims about the superiority of our method are well-supported by the data.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, data can be obtained upon reasonable request to the corresponding author. This approach ensures that the data is used responsibly and in accordance with ethical guidelines. The code used to generate endotypes is available on GitHub, which allows for reproducibility and further analysis by other researchers. This transparency in code sharing supports the scientific community's efforts to validate and build upon the findings presented in the study."
}