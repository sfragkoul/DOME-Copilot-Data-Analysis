{
  "publication/title": "Using handpicked features in conjunction with ResNet-50 for improved detection of COVID-19 from chest X-ray images.",
  "publication/authors": "Rajpal S, Lakhyani N, Singh AK, Kohli R, Kumar N",
  "publication/journal": "Chaos, solitons, and fractals",
  "publication/year": "2021",
  "publication/pmid": "33589854",
  "publication/pmcid": "PMC7874964",
  "publication/doi": "10.1016/j.chaos.2021.110749",
  "publication/tags": "- COVID-19\n- Chest X-ray\n- Deep Learning\n- Medical Imaging\n- Classification\n- Gradient-weighted Class Activation Mapping\n- ResNet-50\n- Transfer Learning\n- Feature Extraction\n- Machine Learning\n- Radiology\n- Pneumonia\n- Diagnostic Tools\n- Neural Networks\n- Image Processing",
  "dataset/provenance": "The dataset used in our study is compiled from several publicly available sources to ensure a comprehensive and balanced collection of chest X-ray images. The primary sources include the COVID-19 Radiography Database from Kaggle, which contains 2905 samples, with 219 COVID-19, 1341 normal, and 1345 viral pneumonia images. Additionally, we utilized the COVID-19 Image Data Collection, comprising 760 samples, with 538 COVID-19, 14 ARDS, and 222 other disease images. The Fig. 1 COVID-chestxray-dataset and Actualmed-COVID-chestxray-dataset were also included, providing 53 and 150 COVID-19 samples, respectively.\n\nTo maintain uniformity, we focused on frontal view images, specifically Posterior-Anterior (PA) and Anteroposterior (AP) views. From the initial databases, we selected 520 COVID-19 images. To balance the dataset, we included an equal number of pneumonia and normal chest X-ray images. Specifically, 260 images each of bacterial and viral pneumonia were randomly selected from the Kaggle and Mendeley databases. Similarly, 520 normal images were randomly chosen from these databases. This resulted in a final dataset of 1560 chest X-ray images, which was randomized and used for 10-fold cross-validation.\n\nAn independent cohort was also created using a new dataset that was not used during the 10-fold cross-validation. This cohort includes 157 unique COVID-19 samples collected from the Fig. 1 and Actualmed datasets, along with an equal number of normal and pneumonia images from the Mendeley database. This independent cohort helps validate the model's performance and generalizability.",
  "dataset/splits": "The dataset used in our study was split into two main parts: a primary dataset for training and validation, and an independent cohort for testing.\n\nThe primary dataset comprised 1560 chest X-ray images, with an equal number of images for each of the three classes: COVID-19, normal, and pneumonia. Specifically, there were 520 images for each class. This dataset was used for 10-fold cross-validation, a technique where the data is divided into 10 subsets, or folds. In each fold, 90% of the data is used for training, and the remaining 10% is used for validation. To prevent overfitting, 10% of the training data is further reserved as a hold-out validation set.\n\nThe independent cohort, used for final testing and validation, consisted of 471 unique chest X-ray images. This cohort also had an equal number of images for each of the three classes, ensuring a balanced representation. This independent cohort was not used during the 10-fold cross-validation process, providing an unbiased evaluation of the model's performance.",
  "dataset/redundancy": "In our study, we utilized multiple publicly available datasets to construct a comprehensive and balanced dataset for training and validating our COVID-19 detection model. The datasets were split into training and independent test sets to ensure the robustness and generalizability of our model.\n\nThe training dataset comprised 1560 chest X-ray images, with an equal number of images for each class: COVID-19, normal, and pneumonia. This balanced distribution was achieved by selecting 520 images for each class from various sources, ensuring that the model was trained on a diverse and representative sample. To maintain uniformity, only frontal view images (Posterior-Anterior and Erect Anteroposterior views) were considered.\n\nTo enforce independence between the training and test sets, we created an independent cohort that was never used during the 10-fold cross-validation process. This cohort consisted of 471 unique chest X-ray images, with an equal number of images for each of the three classes. This approach ensured that the model's performance was evaluated on completely unseen data, providing a more reliable assessment of its real-world applicability.\n\nCompared to previously published machine learning datasets, our approach stands out due to the balanced and diverse nature of our training dataset. Many existing studies have used smaller and less balanced datasets, which can lead to overfitting and reduced generalizability. By including a large and balanced dataset, along with an independent test cohort, we aimed to address these shortcomings and provide a more robust and reliable model for COVID-19 detection.",
  "dataset/availability": "The data used in this study is publicly available through various sources. The COVID-19 Radiography Database, which includes 2905 samples, is accessible on Kaggle. Additionally, the COVID-19 Image Data Collection, comprising 760 samples, is also publicly available. The Fig. 1 COVID-chestxray-dataset and the Actualmed-COVID-chestxray-dataset, which include 53 and 150 COVID-19 samples respectively, are hosted on GitHub. The Mendeley database was used to supplement the dataset with normal and pneumonia images.\n\nTo ensure uniformity, only frontal view chest X-ray images (PA and AP views) were considered. The final dataset, consisting of 1560 chest X-ray images, was randomized and used for 10-fold cross-validation. An independent cohort, comprising 157 unique COVID-19 samples and an equal number of normal and pneumonia images from the Mendeley database, was also created for validation.\n\nThe data is freely available for research re-use and analysis in any form or by any means, with proper acknowledgement of the original source. This permission is granted for as long as the COVID-19 resource centre remains active. The datasets are licensed in a way that allows unrestricted access for research purposes, ensuring that the data can be used by the scientific community to advance research on COVID-19 detection and diagnosis.",
  "optimization/algorithm": "The machine-learning algorithm class used in our work is a hybrid approach that combines deep learning with traditional machine learning techniques. Specifically, we utilized a convolutional neural network (CNN) architecture, ResNet50, which is a well-established deep learning model known for its effectiveness in image classification tasks. This CNN is augmented with a neural network (NN) to enhance its learning capabilities.\n\nThe algorithm is not entirely new; it builds upon existing state-of-the-art models but introduces a novel hybrid approach. The combination of handpicked features with those obtained from the deep neural network is what sets our framework apart. This hybrid method aims to improve the detection of COVID-19 by leveraging the strengths of both traditional and deep learning techniques.\n\nThe reason this algorithm was not published in a machine-learning journal is that our primary focus is on its application in the medical domain, specifically for the detection of COVID-19 from chest X-ray images. The journal Chaos, Solitons and Fractals, where our work is published, is well-suited for interdisciplinary research that combines complex systems, nonlinear dynamics, and applications in various fields, including medical imaging. This platform allows us to highlight the practical implications and clinical relevance of our model, which is crucial for its adoption in healthcare settings.",
  "optimization/meta": "The model described in this publication can be considered a meta-predictor, as it combines features from multiple sources to enhance its classification performance. Specifically, it integrates features obtained from a deep learning model, ResNet-50, with handpicked features derived from conventional image processing methods.\n\nThe ResNet-50 model is used in a transfer learning approach, where it is trained on preprocessed chest X-ray (CXR) images to extract a vector of 2048 features. These features leverage the strengths of deep learning in capturing complex patterns in the images.\n\nIn parallel, the model extracts texture and frequency domain features from the CXR images, resulting in a set of 252 handpicked features. These features are further reduced to 64 using Principal Component Analysis (PCA) and then passed through a feed-forward neural network to obtain a set of 16 features.\n\nThe features from both the ResNet-50 model and the handpicked features are combined to form the input for the dense layer of the final module. This combined feature set is then fed into a softmax layer to yield the classification model.\n\nRegarding the independence of the training data, the model employs 10-fold cross-validation on a dataset comprising 1560 images, with 520 images belonging to each of the three classes: COVID-19, normal, and pneumonia. Additionally, the model is validated on an independent cohort of 471 CXR images, ensuring that the training and validation data are separate, which is crucial for assessing the model's generalizability and performance.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for ensuring uniformity and enhancing the performance of the machine-learning algorithm. Initially, chest X-ray (CXR) images were collected from various publicly available repositories. To maintain consistency, only frontal view images, specifically Posteroanterior (PA) and Erect Anteroposterior (AP) views, were considered.\n\nThe preprocessing involved several key steps. First, all images were resized and transformed from RGB to grayscale to ensure uniformity across different datasets. This step was essential for standardizing the input data for the machine-learning model. Following this, the images underwent min-max normalization to speed up the convergence process during training. This normalization technique scales the pixel values to a range between 0 and 1, which helps in stabilizing and accelerating the learning process.\n\nIn the first module of the proposed framework, the preprocessed CXR images of size 224 \u00d7 224 were passed through a 2D convolutional layer, transforming them into a format suitable for the ResNet-50 model. The ResNet-50, pre-trained on ImageNet, was used to extract a vector of 2048 features from these images. During training, random projective transformations such as scaling, shearing, zooming, and horizontal flipping were applied to the images to enhance the robustness and generalization capability of the network.\n\nThe second module focused on extracting handpicked features from the CXR images. These images were rescaled to 512 \u00d7 512 and subjected to texture and frequency domain analysis. Texture features were generated using the gray-level co-occurrence matrix (GLCM) and gray-level difference matrix (GLDM), along with statistical values computed in the spatial domain. Frequency features were obtained by applying Fast Fourier Transform (FFT) and two-level Discrete Wavelet Transform (DWT) to the images. These features were then concatenated to form a pool of 252 features, which were further reduced to 64 features using Principal Component Analysis (PCA). The reduced feature set was passed through a feed-forward neural network consisting of dense and dropout layers to obtain a final vector of 16 features.\n\nThe outputs from both modules were concatenated to form a vector of 2064 features, which were then passed through a dense layer employing the ReLU activation function to obtain a reduced set of 1000 features. These features served as input to a softmax layer, yielding the final classification model. The entire model was trained end-to-end, ensuring that the features extracted from both modules were effectively combined to improve the detection of COVID-19 from chest X-ray images.",
  "optimization/parameters": "In the proposed network, the input parameters are determined by the architecture of the model, which combines features from a pre-trained ResNet-50 model with handpicked features extracted from chest X-ray (CXR) images.\n\nThe first module of the network uses a ResNet-50 model pre-trained on ImageNet. This model has 25,636,712 parameters. The output of ResNet-50 is passed through a global average pooling layer, which reduces the dimensionality to 2048 features.\n\nThe second module involves a feature extraction unit that yields 252 features from CXR images rescaled to 512 \u00d7 512. These features are processed using Principal Component Analysis (PCA) to obtain a vector of 64 features. This feature vector is then passed through a feed-forward neural network consisting of two pairs of dense layers and dropout layers. The first pair reduces the features to 32, and the second pair further reduces them to 16. Each dense layer uses the ReLU activation function, and the dropout regularization factor is set to 0.50 to prevent overfitting.\n\nThe features from the first and second modules are concatenated to form a vector of 2064 features (2048 from ResNet-50 and 16 from the feature extraction unit). This combined feature set is passed to a dense layer with ReLU activation, reducing the features to 1000. Finally, these features are fed into a softmax layer for classification.\n\nThe selection of parameters was based on extensive experimentation and validation. The use of ResNet-50 provides a robust feature extraction capability, while the handpicked features from conventional image processing methods complement the deep learning features. The PCA and dropout layers help in reducing overfitting and improving the generalization of the model. The final architecture was chosen after evaluating various configurations to achieve the best performance in terms of accuracy and sensitivity.",
  "optimization/features": "In the proposed framework, the input features are derived from two main sources: a pre-trained ResNet-50 model and a feature extraction unit.\n\nThe first module utilizes transfer learning with ResNet-50, which is pre-trained on ImageNet. The output from ResNet-50 is passed through a global average pooling layer, resulting in a vector of 2048 features.\n\nThe second module involves a feature extraction unit that yields 252 features from chest X-ray (CXR) images rescaled to 512 \u00d7 512. These features are obtained through a combination of texture and frequency domain analyses. The texture features are generated using the spatial domain, gray-level co-occurrence matrix (GLCM), and gray-level difference matrix (GLDM). Frequency features are obtained by applying Fast Fourier Transform (FFT) and two-level Discrete Wavelet Transform (DWT) to the images. The resulting 252 features are then processed using Principal Component Analysis (PCA) to reduce them to a vector of 64 features. This feature vector is further passed through a feed-forward neural network consisting of dense and dropout layers, ultimately producing a vector of 16 features.\n\nThe features from both modules are concatenated, forming a vector of 2064 features (2048 from ResNet-50 and 16 from the feature extraction unit). These concatenated features are then passed to a dense layer with ReLU activation, reducing the feature set to 1000 features. This reduced set serves as the input to the final softmax layer for classification.\n\nFeature selection is performed using PCA on the 252 handpicked features to reduce dimensionality to 64 features. This selection process is done using the training set only, ensuring that the model generalizes well to unseen data. The use of PCA helps in mitigating overfitting and enhancing the model's performance by focusing on the most informative features.",
  "optimization/fitting": "The proposed framework employs a robust fitting method to ensure that the model generalizes well to unseen data. The model architecture includes a feature extraction unit that reduces the dimensionality of the feature set from 252 to 64 using Principal Component Analysis (PCA). This dimensionality reduction helps in mitigating the risk of overfitting, especially when the number of parameters is large compared to the number of training points.\n\nTo further prevent overfitting, dropout regularization with a factor of 0.50 is applied in the dense layers of the neural network. This technique randomly sets a fraction of the input units to zero during training, which helps in preventing the network from becoming too reliant on specific features and thus reduces overfitting.\n\nAdditionally, 10-fold cross-validation is performed to assess the model's performance and variability across different test data. This method ensures that the model is evaluated on multiple subsets of the data, providing a more reliable estimate of its generalization capability.\n\nTo avoid underfitting, the model architecture is designed with multiple dense layers and ReLU activation functions, which allow the network to learn complex patterns in the data. The use of a balanced dataset comprising 520 COVID-19, 520 normal, and 520 pneumonia chest X-ray images also contributes to the model's ability to learn effectively from the data.\n\nDuring training, 90% of the dataset is used for training, and 10% is reserved as a hold-out validation set within each fold. This approach ensures that the model is trained on a sufficient amount of data while still having a validation set to monitor performance and prevent underfitting.\n\nOverall, the combination of dimensionality reduction, dropout regularization, cross-validation, and a balanced dataset helps in achieving a good balance between underfitting and overfitting, leading to a model with high accuracy and sensitivity.",
  "optimization/regularization": "In our study, we implemented several regularization techniques to prevent overfitting and enhance the generalization capability of our model. One of the key methods used was dropout regularization. Specifically, we employed a dropout rate of 0.50 in the dense layers of our feed-forward neural network within the second module. This technique randomly sets a fraction of input units to zero at each update during training time, which helps to prevent the network from becoming too reliant on any single feature and thus reduces overfitting.\n\nAdditionally, we utilized data augmentation techniques during the training process. These included random projective transformations such as scaling, shearing, zooming, and horizontal flipping. By applying these transformations, we effectively increased the diversity of our training dataset, making the model more robust and less likely to overfit to the specific patterns in the original data.\n\nFurthermore, we performed 10-fold cross-validation to assess the model's performance and stability across different subsets of the data. This method ensures that the model is evaluated on multiple train-test splits, providing a more reliable estimate of its generalization performance. During each fold, we reserved 10% of the training data as a hold-out validation set to monitor the model's performance and adjust hyperparameters accordingly.\n\nThese regularization techniques collectively contributed to the model's high accuracy and sensitivity, demonstrating its effectiveness in classifying chest X-ray images into COVID-19, pneumonia, and normal categories.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The proposed model incorporates interpretability as a crucial aspect, particularly important in the medical domain. To achieve this, we employed Gradient-weighted Class Activation Mapping (Grad-CAM). This technique allows us to visualize the regions in chest X-ray images that are most influential in the model's decision-making process. By highlighting these regions, Grad-CAM provides a clear and intuitive way to understand which parts of the image the model focuses on when classifying an image as COVID-19, pneumonia, or normal.\n\nThe use of Grad-CAM ensures that the model is not a black box. Instead, it offers transparency by generating heatmaps that mark the lung regions affected in COVID-19 cases. These heatmaps serve as clinical evidence, making the model's predictions more interpretable and trustworthy for medical professionals. The red-colored regions in the heatmaps indicate the areas of interest that the model considers significant for its classification, while the blue-colored regions provide additional evidence supporting the identified class.\n\nThis interpretability is essential for validating the model's decisions and building confidence in its clinical applications. By providing visual explanations, Grad-CAM helps bridge the gap between the model's predictions and the underlying medical evidence, making the framework more reliable and useful in practical healthcare settings.",
  "model/output": "The model is a classification model designed to categorize chest X-ray images into three distinct classes: COVID-19, normal, and pneumonia. It employs a hybrid approach that combines handpicked features with those extracted from a deep neural network, enhancing its learning ability and detection capabilities.\n\nTo evaluate the model's performance, 10-fold cross-validation was conducted. This process involved dividing the dataset into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This procedure was repeated 10 times, each time using a different subset as the validation set. The model achieved an overall classification accuracy of 0.974 \u00b1 0.02, indicating high consistency across different validation folds.\n\nThe sensitivity of the model, which measures its ability to correctly identify positive cases, was also assessed. For COVID-19, the sensitivity was 0.987 \u00b1 0.05, for normal cases it was 0.963 \u00b1 0.05, and for pneumonia cases it was 0.973 \u00b1 0.04, all at a 95% confidence interval. These high sensitivity values demonstrate the model's effectiveness in accurately detecting each class.\n\nTo further validate the model's performance, it was tested on an independent chest X-ray cohort, achieving an overall accuracy of 0.979. This independent validation confirms the model's robustness and generalizability.\n\nThe model's outputs are not only accurate but also interpretable, which is crucial in the medical domain. Gradient-weighted Class Activation Mapping (Grad-CAM) was used to highlight the regions in the X-ray images that contributed most to the model's predictions. This visualization tool helps clinicians understand the basis for the model's classifications, providing valuable clinical evidence.\n\nIn summary, the model is a robust classification tool that accurately and reliably distinguishes between COVID-19, normal, and pneumonia cases in chest X-ray images. Its high accuracy and sensitivity, combined with interpretable outputs, make it a valuable asset in medical diagnostics.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "To evaluate the proposed model, a 10-fold cross-validation was conducted. This method involved dividing the dataset into 10 equal parts, or folds, and training the model on 9 folds while testing it on the remaining fold. This process was repeated 10 times, with each fold serving as the test set once. The overall classification accuracy achieved was 0.974 \u00b1 0.02, with sensitivities of 0.987 \u00b1 0.05 for COVID-19, 0.963 \u00b1 0.05 for normal, and 0.973 \u00b1 0.04 for pneumonia classes, all at a 95% confidence interval.\n\nTo further validate the model's effectiveness, it was tested on an independent Chest X-ray cohort, resulting in an overall classification accuracy of 0.979. This additional validation step ensured that the model's performance was consistent across different datasets.\n\nThe model's performance was also compared with state-of-the-art methods. The proposed framework demonstrated superior accuracy and sensitivity, highlighting its effectiveness in classifying chest X-ray images into COVID-19, normal, and pneumonia categories.\n\nTo enhance the interpretability of the results, Gradient-weighted Class Activation Mapping (Grad-CAM) was employed. This technique provided gradient-based localizations, which helped in identifying the specific regions in the lung images that contributed to the model's decisions. This is particularly important in the medical domain, where interpretability and clinical evidence are crucial.",
  "evaluation/measure": "In our evaluation, we reported several key performance metrics to comprehensively assess the effectiveness of our proposed framework. These metrics include overall accuracy, sensitivity, precision, recall, and F1-score. The overall accuracy of our model, as determined through 10-fold cross-validation, is 0.974 \u00b1 0.02. Sensitivity, which measures the true positive rate, is reported at 0.987 \u00b1 0.05 for COVID-19, 0.963 \u00b1 0.05 for normal cases, and 0.973 \u00b1 0.04 for pneumonia cases, all at a 95% confidence interval. Precision, recall, and F1-score are also provided, with values consistently high across the three classes, indicating robust performance.\n\nThese metrics are representative of the standards in the literature, as they cover a broad spectrum of evaluation criteria commonly used in medical imaging and classification tasks. Precision, recall, and F1-score are particularly important in medical diagnostics, where the cost of false positives and false negatives can be significant. Our framework's high values in these metrics demonstrate its reliability and effectiveness in distinguishing between COVID-19, normal, and pneumonia cases. Additionally, the use of 10-fold cross-validation ensures that our results are stable and generalizable, addressing a common shortcoming in many related works that often lack thorough validation procedures.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the effectiveness of our proposed framework by comparing it with various state-of-the-art machine learning and deep learning algorithms. This comparison was conducted using a balanced dataset comprising 520 COVID-19, 520 normal, and 520 pneumonia chest X-ray images. The comparison focused on overall accuracy and sensitivity in detecting COVID-19.\n\nOur proposed model, which combines ResNet50 with a neural network, achieved an overall accuracy of 0.974 \u00b1 0.02 and a sensitivity of 0.987 \u00b1 0.05 for COVID-19 detection. This performance was benchmarked against several other models, including ResNet50, VGGNet19, MobileNet v2, Xception, AlexNet, GoogLeNet, ResNet18, VGG16, and EfficientNet-B0, among others. The results showed that our model outperformed these state-of-the-art methods in terms of accuracy and sensitivity.\n\nFor instance, Bukharia et al. reported an accuracy of 0.982 using ResNet50 but did not provide sensitivity values. Apostolopoulos et al. achieved accuracies ranging from 0.86 to 0.94 using VGGNet19 and MobileNet v2, but their datasets were smaller and did not include pneumonia cases. Khan et al. used Xception and reported an accuracy of 0.902, while Khuzani et al. achieved 0.94 accuracy with a multilayer neural network but had a smaller dataset.\n\nAdditionally, we addressed the limitations of previous works by including a multi-class classification model that distinguishes between COVID-19, normal, and pneumonia cases. Most related works focused on binary classification (COVID-19 vs. normal), which does not account for the complexity of real-world scenarios where pneumonia is also a significant factor.\n\nFurthermore, we validated our model on an independent chest X-ray cohort, achieving an overall accuracy of 0.979. This validation step is crucial for establishing the robustness and generalizability of our model, which many previous studies lacked.\n\nIn summary, our comparison with publicly available methods and simpler baselines demonstrated that our proposed framework is superior in terms of accuracy and sensitivity. The inclusion of a multi-class classification model and validation on an independent cohort further underscores the effectiveness and reliability of our approach.",
  "evaluation/confidence": "The evaluation of our proposed framework includes a thorough assessment of its performance metrics, which are presented with confidence intervals to ensure robustness and reliability. For instance, the overall classification accuracy is reported as 0.974 \u00b1 0.02, indicating a high level of precision with minimal variance. Similarly, the sensitivity for COVID-19, normal, and pneumonia classes are provided with confidence intervals of 0.987 \u00b1 0.05, 0.963 \u00b1 0.05, and 0.973 \u00b1 0.04, respectively. These intervals are calculated at a 95% confidence level, ensuring that the results are statistically significant and not due to random chance.\n\nThe use of 10-fold cross-validation further strengthens the statistical significance of our findings. This method involves dividing the dataset into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. The consistent performance across all folds, as evidenced by the low standard deviation in the accuracy and sensitivity metrics, demonstrates the model's stability and generalizability.\n\nMoreover, the comparison with state-of-the-art methods reveals that our framework outperforms others in terms of accuracy and sensitivity. The statistical significance of these comparisons is supported by the confidence intervals, which show that the differences in performance are not due to random variation but reflect a genuine superiority of our method.\n\nIn summary, the performance metrics of our proposed framework are robust and statistically significant. The use of confidence intervals and 10-fold cross-validation ensures that the results are reliable and generalizable, providing strong evidence that our method is superior to existing approaches.",
  "evaluation/availability": "Not applicable."
}