{
  "publication/title": "Forecasting Risk of Future Rapid Glaucoma Worsening Using Early Visual Field, OCT, and Clinical Data.",
  "publication/authors": "Herbert P, Hou K, Bradley C, Hager G, Boland MV, Ramulu P, Unberath M, Yohannan J",
  "publication/journal": "Ophthalmology. Glaucoma",
  "publication/year": "2023",
  "publication/pmid": "36944385",
  "publication/pmcid": "PMC10509314",
  "publication/doi": "10.1016/j.ogla.2023.03.005",
  "publication/tags": "- Glaucoma\n- Machine Learning\n- Visual Field Testing\n- Optical Coherence Tomography\n- Predictive Modeling\n- Rapid Progressors\n- Ophthalmology\n- Deep Learning\n- Clinical Data\n- Neuroprotection Studies",
  "dataset/provenance": "The dataset used in this study was sourced from a clinical database containing eyes followed from 2013 to 2021. The selection criteria for the dataset included eyes that were followed for glaucoma or suspect status, had at least five reliable visual fields (VFs), one reliable baseline optical coherence tomography (OCT) scan, and one set of baseline clinical measurements. The dataset consists of approximately 4536 eyes, with a training split of about 3600 eyes. This dataset is relatively large and multimodal, including visual field data, OCT data, and clinical information such as intraocular pressure (IOP) and visual acuity. The dataset represents a real-world clinical population, which allows for better generalizability of the results to a treated clinical population. In previous work, a much larger dataset (n = 18,340) was used, but the current study demonstrates improved performance with a smaller, more focused dataset (n = 3,632). The inclusion of longitudinal data from follow-up visits further enriches the dataset, providing a comprehensive view of the patients' disease progression over time.",
  "dataset/splits": "The dataset was divided into three splits: training, validation, and test sets. The division was done using a stratified method to ensure an equal ratio of classes (rapid worsening eyes and non-rapid worsening eyes) in each set. The splits were as follows:\n\n* Training set: 80% of the data, which amounts to approximately 3600 eyes.\n* Validation set: 10% of the data.\n* Test set: 10% of the data.\n\nTo prevent data leakage, it was ensured that an individual patient could not have eyes in more than one set. This means that the eyes of a single patient were all placed in the same split, either training, validation, or test. The same training and test sets were used for all models to maintain consistency across different model evaluations.",
  "dataset/redundancy": "The datasets were split into training, validation, and test sets using an 80%/10%/10% ratio. This split was done using a stratified method to ensure that each set had an equal ratio of classes, specifically rapid worsening eyes and non-rapid worsening eyes. To prevent data leakage, it was ensured that an individual patient could not have eyes in more than one set. This means that the training, validation, and test sets are independent of each other. The same training and test sets were used for all models, ensuring consistency across different model evaluations. The distribution of the dataset, with approximately 3600 eyes in the training split, is relatively large compared to some previously published machine learning datasets in this domain. This size allows for robust training and better generalization of the models to a real-world clinical population. The dataset includes a mix of demographic, clinical, visual field, and OCT information, providing a comprehensive view for model training and evaluation.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is transformer-based deep learning networks. This approach is relatively new in the context of glaucoma progression prediction. Transformers have recently shown promising results in various domains, including natural language processing and computer vision, and are increasingly being explored as alternatives to traditional convolutional neural networks (CNNs) and recurrent neural networks (RNNs or LSTMs).\n\nThe reason this algorithm was not published in a machine-learning journal is that the primary focus of our work is on its application in ophthalmology, specifically in predicting future rapid worsening of glaucoma. The innovation lies in the application of transformers to multimodal data, including visual field (VF) tests, optical coherence tomography (OCT) scans, and clinical information, to improve the forecasting of glaucoma progression. This application is more aligned with the scope of ophthalmology and medical research journals, where the clinical relevance and impact on patient care are of paramount importance. The technical details of the transformer architecture and its implementation are secondary to the clinical outcomes and the potential for improving patient management.",
  "optimization/meta": "The models described in our study do not function as meta-predictors. They do not use data from other machine-learning algorithms as input. Instead, they directly utilize multimodal data, including visual field (VF) tests, optical coherence tomography (OCT) scans, and clinical information. The models are designed to forecast future rapid glaucoma worsening by integrating these various data sources.\n\nSeveral models were trained with different combinations of inputs:\n\n* Model V uses only baseline VF data.\n* Model VC includes baseline VF and clinical information.\n* Model VO incorporates baseline VF and OCT information.\n* Model VOC combines baseline VF, OCT, and clinical information.\n* Model V2 includes two VFs (baseline and one follow-up) and clinical information.\n* Model V2OC includes two VFs, OCT, and clinical information.\n* Model V3 includes three VFs (baseline and two follow-ups) and clinical information.\n* Model V3OC includes three VFs, OCT, and clinical information.\n\nEach model was trained and evaluated independently, ensuring that the training data for each model was distinct and did not overlap. This approach allowed for a clear comparison of model performance based on the different types and amounts of input data used. The models were evaluated using metrics such as the area under the receiver operating curve (AUC), precision-recall curves, and other performance metrics to assess their ability to forecast rapid glaucoma worsening.",
  "optimization/encoding": "The data encoding process involved transforming various types of input data into a format suitable for the machine-learning algorithm. Visual field (VF) data was converted into a 12x12 grid to align with the quadrant or clock hour divisions of the scan. Additionally, the total deviation values from the VF were radially imputed to create an analogous 12x12 grid. These grids were then stacked to form a three-channel image for each eye, which served as the input to the vision transformer. To account for the varying orientations of nerve fiber layers in different eyes, data augmentation techniques such as random cropping and rotation were applied to the OCT layers of the stacked image. This approach ensured that the vision transformer could learn the appropriate spatial relationships between the channels, despite the imperfect reflection of these relationships in the initial encoding.\n\nClinical information, global features of the VF (such as mean deviation and reliability indices), and global features from the OCT (including average retinal nerve fiber layer thickness, disc parameters, and signal strength) were combined into a vector of 34 features. This vector was then concatenated with the output of the vision transformer, which consisted of 64 extracted features. The combined vector was input into a fully connected classifier to predict the probability of future rapid worsening. This encoding process allowed the model to integrate multimodal data effectively, enhancing its predictive capabilities.",
  "optimization/parameters": "In our study, we utilized a vision transformer that extracted 64 features from the input images. These features were then combined with a vector of 34 additional features, which included clinical information, global features of the visual field (VF), and global features from the optical coherence tomography (OCT). This resulted in a total of 98 input parameters for our models.\n\nThe selection of these parameters was driven by the need to incorporate both spatial and temporal information to improve the accuracy of forecasting future rapid worsening of glaucoma. The vision transformer was chosen for its ability to handle spatial data effectively, while the additional clinical and global features provided contextual information that enhanced the model's predictive power. The inclusion of multiple VFs and OCT data further enriched the input parameters, allowing the models to learn from longitudinal data and multimodal inputs. This approach ensured that the models could capture a comprehensive view of the patient's condition, leading to improved performance in forecasting rapid progression.",
  "optimization/features": "The input features for the models vary depending on the specific model used. The total number of features (f) used as input ranges from 64 to 98, depending on the model. This includes features extracted from visual field (VF) data, optical coherence tomography (OCT) data, and clinical information.\n\nFor models that include VF data, the VF information is transformed into a 12x12 grid to match the quadrant or clock hour divisions of the scan. The total deviation values from the VF are radially imputed to fill out an analogous 12x12 grid. These grids are then stacked to form a three-channel image for each eye, which serves as input to the vision transformer. The vision transformer extracts 64 features from these images.\n\nIn addition to the VF features, some models incorporate clinical information and global features from the OCT. The clinical information vector includes 34 features, such as mean deviation (MD), reliability indices, average retinal nerve fiber layer (RNFL) thickness, disc parameters, and signal strength. These features are combined with the 64 features extracted by the vision transformer, resulting in a total of 98 features for models that include all types of data.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the selection of features was inherently done by choosing specific models that included different combinations of VF, OCT, and clinical data. The models were designed to evaluate the impact of different types of input data on performance. For example, Model V includes only baseline VF data, while Model V3OC includes baseline VF data, OCT data, clinical information, and data from three VF tests.\n\nThe splitting of the data into training, validation, and test sets was done in a stratified manner to ensure equal ratios of classes in each set. This splitting was performed identically for all models, ensuring that the same training and test sets were used across different models. This approach helps to maintain the integrity of the feature selection process and prevents data leakage between sets.",
  "optimization/fitting": "The models employed in this study utilized a vision transformer architecture, which inherently has a large number of parameters. To address potential overfitting, several strategies were implemented. Firstly, data augmentation techniques such as random cropping and rotation were applied to the optical coherence tomography (OCT) layers of the stacked images. This helped to increase the effective size of the training dataset and improve the model's generalization ability.\n\nAdditionally, the dataset was split into training, validation, and test sets using a stratified method to ensure equal ratios of classes in each set. This approach helped to prevent data leakage and ensured that the model's performance was evaluated on unseen data. The use of a validation set allowed for tuning of hyperparameters and early stopping to prevent overfitting.\n\nTo further mitigate overfitting, the models were evaluated using multiple performance metrics, including the area under the curve (AUC), precision-recall (PR) curves, precision, recall, F1-Score, and Youden\u2019s J-Index. These metrics provided a comprehensive assessment of the model's performance and helped to identify any signs of overfitting.\n\nUnderfitting was addressed by ensuring that the models were complex enough to capture the underlying patterns in the data. The inclusion of multimodal data, such as visual field (VF) data, clinical information, and OCT data, provided a rich set of features for the models to learn from. Additionally, the use of a transformer-based architecture allowed the models to capture complex relationships in the data.\n\nThe models were trained using a fully connected classifier, which combined the output of the vision transformer with a vector of clinical information. This approach ensured that the models had sufficient capacity to learn from the data and generalize to new, unseen examples. The use of a validation set also allowed for monitoring of the training process and adjustment of the model's complexity as needed.\n\nIn summary, the models employed in this study utilized a combination of data augmentation, stratified splitting, and comprehensive evaluation metrics to address potential overfitting. The inclusion of multimodal data and a transformer-based architecture ensured that the models had sufficient capacity to learn from the data and generalize to new examples, thereby mitigating underfitting.",
  "optimization/regularization": "Not enough information is available.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model developed in this study is primarily a black-box design, particularly due to its reliance on transformer-based deep learning architectures. These models, while powerful, do not inherently provide clear feature importance values as seen in simpler models like logistic regression or random forests. This lack of transparency makes it challenging to explain the model's decisions in a meaningful way.\n\nTo mitigate this, ablation studies were conducted. For instance, in one study, OCT data was removed from the model to examine the impact on results. This approach aimed to provide a clearer picture of how each piece of data affects the overall outcomes. However, it is important to note that these studies do not fully address the issue of interpretability.\n\nLooking ahead, exploring the transformer\u2019s multi-headed attention mechanism could offer additional insights into the model's decision-making process. This avenue of research could help reduce the black-box nature of the architecture, making it more understandable for clinicians. Improved interpretability is crucial as it allows clinicians to understand why certain patients are classified as rapid progressors, thereby enhancing decision-making and error detection. Ultimately, increasing the transparency of the model is likely to improve clinician and patient trust in the model's outputs.",
  "model/output": "The model is a classification model designed to predict the probability of future rapid worsening in glaucoma patients. It outputs a probability indicating the likelihood of an eye experiencing rapid visual field (VF) deterioration. This probability is derived from a fully connected classifier that processes a combined vector of features extracted from visual field data, optical coherence tomography (OCT) images, and clinical information. The model's performance is evaluated using metrics such as the area under the receiver operating curve (AUC), precision, recall, F1-Score, and Youden\u2019s J-Index, which are all relevant to classification tasks. The primary goal is to identify patients at high risk of rapid progression, enabling earlier intervention and better resource allocation in clinical settings.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "We evaluated our models using a stratified 80%/10%/10% split of the data into training, validation, and test sets. This ensured that each set had an equal ratio of rapid worsening eyes and non-rapid worsening eyes. To prevent data leakage, we ensured that eyes from the same patient were not present in more than one set.\n\nWe assessed model performance using several metrics, including the area under the curve (AUC) and precision-recall (PR) curves on the held-out test set. Additionally, we calculated precision, recall, F1-Score, and Youden\u2019s J-Index to provide a comprehensive evaluation. Recall and sensitivity were treated as identical metrics, while precision was defined as the fraction of true positives among all positives predicted by the model. The F1-Score, being the harmonic mean of precision and recall, was useful for combining these two metrics into one. Youden\u2019s J-Index, calculated as sensitivity plus specificity minus one, served as a summary measure of the ROC curve and helped in selecting optimal thresholds for classification.\n\nWe chose thresholds that optimized the F1-Score or Youden\u2019s J-Index, which led to equal weighting of precision and recall or sensitivity and specificity, respectively. However, it is important to note that these thresholds might differ in a clinical setting, where practitioners might prioritize higher sensitivity to avoid false negatives, i.e., missing eyes that are rapid progressors.\n\nWe also compared the performance of different models using the DeLong Test to determine if the differences in AUC scores were statistically significant. This allowed us to understand the impact of adding different types of data, such as additional visual fields, OCT information, and clinical data, on model performance.",
  "evaluation/measure": "In our study, we reported a comprehensive set of performance metrics to evaluate the effectiveness of our models in predicting rapid progression in glaucoma patients. The primary metrics we focused on include the Area Under the Curve (AUC) with 95% Confidence Intervals (CI), Sensitivity (also known as Recall), Specificity, Youden\u2019s J-Index, Precision, and the F1-Score. These metrics were calculated specifically for the positive class, which in our case refers to rapid progressors.\n\nThe AUC provides a measure of the model's ability to distinguish between rapid progressors and non-rapid progressors across all possible classification thresholds. Sensitivity and Specificity were calculated at the optimal point where sensitivity and specificity are equally weighted, using Youden\u2019s J-Index. Precision and Recall were determined at the point where the F1-Score is maximized, offering a balance between precision and recall.\n\nAdditionally, we performed intermodel AUC comparisons using the DeLong Test to determine if the performance differences between models were statistically significant. This allowed us to assess whether the improvements observed in certain models were due to genuine enhancements in predictive power rather than random variation.\n\nOur choice of metrics is representative of standard practices in the literature, ensuring that our results can be compared with other studies in the field. The inclusion of AUC, Sensitivity, Specificity, Precision, Recall, and F1-Score provides a thorough evaluation of model performance, covering aspects such as discrimination, calibration, and classification accuracy. This comprehensive approach helps to validate the robustness and reliability of our models in clinical settings.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. However, we did compare our models' performance to simpler baselines and previous methodologies used in similar tasks.\n\nOur models were evaluated against simpler baselines, such as models using only baseline visual field (VF) data. For instance, our previous work achieved a modest area under the curve (AUC) of 0.72 for predicting future rapid worsening with baseline VF data alone. This served as a baseline to measure the improvement gained by incorporating additional data and more advanced modeling techniques.\n\nWe also compared our current models to previous methodologies. For example, our previous deep learning model showed a marginally worse AUC of 0.71 compared to random forests. In contrast, our current Model V achieved a higher AUC of 0.74, demonstrating improvement even with a much smaller training dataset.\n\nAdditionally, we explored the use of multimodal data, including baseline clinical information and structural optical coherence tomography (OCT) data, along with longitudinal VF data. This approach allowed us to achieve better performance metrics, such as an AUC greater than 0.80 for detecting future rapid glaucoma worsening.\n\nIn summary, while we did not compare directly to publicly available methods on benchmark datasets, we did evaluate our models against simpler baselines and previous methodologies, showing significant improvements in performance.",
  "evaluation/confidence": "The evaluation of our models included several performance metrics, each accompanied by confidence intervals to provide a measure of uncertainty. For instance, the Area Under the Curve (AUC) scores for all models were reported with 95% confidence intervals, offering a range within which the true AUC value is likely to fall. This approach ensures that the reported performance is not overly optimistic and accounts for variability in the data.\n\nStatistical significance was assessed using the DeLong test, which compares the AUC scores between models. This test helps determine if the observed differences in performance are likely due to actual superiority of one model over another, rather than random chance. The p-values from these comparisons indicate that several models, particularly those incorporating additional visual fields (VFs) and multimodal data, show statistically significant improvements over baselines and simpler models. For example, models that included two or three VFs beyond the baseline (such as V3 and V3OC) demonstrated significantly better performance compared to models using baseline data alone.\n\nAdditionally, precision-recall (PR) curves and other metrics like Youden\u2019s J-Index, precision, recall, and F1-Score were evaluated to provide a comprehensive view of model performance. These metrics were calculated at optimal points to ensure balanced consideration of sensitivity and specificity. The results consistently showed that models with more longitudinal and multimodal data performed better, with statistically significant improvements in key metrics.\n\nOverall, the confidence intervals and statistical tests provide a robust framework for evaluating the models' performance, ensuring that the claimed superiority is supported by rigorous statistical analysis.",
  "evaluation/availability": "Not enough information is available."
}