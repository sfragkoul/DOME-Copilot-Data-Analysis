{
  "publication/title": "Improving Methods of Identifying Anaphylaxis for Medical Product Safety Surveillance Using Natural Language Processing and Machine Learning.",
  "publication/authors": "Carrell DS, Gruber S, Floyd JS, Bann MA, Cushing-Haugen KL, Johnson RL, Graham V, Cronkite DJ, Hazlehurst BL, Felcher AH, Bejan CA, Kennedy A, Shinde MU, Karami S, Ma Y, Stojanovic D, Zhao Y, Ball R, Nelson JC",
  "publication/journal": "American journal of epidemiology",
  "publication/year": "2023",
  "publication/pmid": "36331289",
  "publication/pmcid": "PMC9896464",
  "publication/doi": "10.1093/aje/kwac182",
  "publication/tags": "- Machine Learning\n- Natural Language Processing\n- Anaphylaxis\n- Electronic Health Records\n- Epidemiology\n- Bayesian Additive Regression Trees\n- Model Validation\n- Drug Safety Surveillance\n- Public Health Surveillance\n- Data-Adaptive Modeling",
  "dataset/provenance": "The dataset used in this study originated from two primary sources. The first source, referred to as path-1, consisted of paper print-outs of electronic health record (EHR) notes from 15 regional non-Kaiser medical facilities. These paper documents were converted into electronic text using Tesseract optical character recognition (OCR) software, followed by a context-sensitive spelling correction model trained on KPWA chart notes. The second source, path-2, along with the entire KPNW corpus, was extracted directly from the KPWA and KPNW Epic Clarity EHR databases in electronic format.\n\nThe study involved a total of 239 potential anaphylaxis events at KPWA, out of which 154 (64%) met clinical criteria for validated anaphylaxis. Additionally, 180 out of 277 (65%) potential events at KPNW met validation criteria. After excluding low-variation covariates, the dataset included 43 structured and 93 NLP-derived covariates.\n\nThe structured data and covariates were engineered based on clinical domain expertise, informatics knowledge, and observed distributions in KPWA data. A modest-sized set of 47 structured covariates was operationalized, including demographics, setting for the healthcare encounter, potential causes of anaphylaxis, clinical interventions, and history of allergic reactions or competing diagnoses.\n\nThe NLP-derived data and covariates were generated using a custom dictionary of concepts curated by clinical and informatics experts. This dictionary was augmented with Unified Medical Language System (UMLS) concepts from published anaphylaxis knowledge base articles. The final dictionary was enriched with synonyms from the UMLS Metathesaurus and synonyms discovered in KPWA charts through NLP-assisted manual review. The KPWA corpus for path-1 encounters was converted to electronic text using OCR and a spelling correction model, while the path-2 corpus and the entire KPNW corpus were extracted directly from the EHR databases.\n\nThe dataset used in this study has not been used in previous papers or by the community, as it was specifically curated for this research on identifying anaphylaxis using machine learning and NLP techniques. The structured and NLP-derived covariates were engineered to distinguish actual from potential anaphylaxis events in automated models.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "The datasets used in this study were split into development and validation sets to ensure independence between training and test sets. The development dataset originated from Kaiser Permanente Washington (KPWA), while the validation dataset came from Kaiser Permanente Northwest (KPNW). This split was designed to evaluate the generalizability of the models across different but related healthcare settings.\n\nTo enforce independence, the KPWA corpus included data from various inpatient facilities, ensuring a diverse range of clinical documentation styles and patient populations. The KPNW dataset, on the other hand, was extracted from a single, integrated care setting, providing a controlled environment for validation. This approach helped mitigate potential biases that could arise from using the same data for both training and testing.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the context of rare outcomes like anaphylaxis. The study included a rigorous validation process using established clinical criteria and cross-validated performance metrics. This ensured that the models were not overfitted to the training data and could generalize well to new, unseen data. The use of structured and natural language processing-derived data further enriched the dataset, providing a comprehensive set of covariates for model development.\n\nThe study also addressed potential limitations, such as the modest sample size and the challenges of documenting rare events like anaphylaxis. Despite these challenges, the datasets were carefully curated to include a diverse set of covariates, enhancing the robustness of the models. The external validation in KPNW data confirmed that the models, particularly the BART2-Retain-All model, performed well in identifying anaphylaxis cases, demonstrating their generalizability within the Kaiser network.",
  "dataset/availability": "The data used in this study are not publicly released. The study involved electronic health records (EHR) from Kaiser Permanente Washington (KPWA) and Kaiser Permanente Northwest (KPNW). The KPWA corpus for path-1 encounters originated as paper print-outs of EHR notes from 15 regional non-Kaiser medical facilities, which were converted to electronic text using optical character recognition and a context-sensitive spelling correction model. The KPWA path-2 corpus, along with the entire KPNW corpus, was extracted from the Epic Clarity EHR databases in electronic format.\n\nThe data were processed using SAS, version 9.4, and the KPWA NLP system to generate structured and NLP-derived covariates. The same SAS code and NLP system were transported to KPNW via GitHub to ensure identical processing of the data. This approach was taken to avoid missing data issues, and all covariates were defined as counts or binary indicators.\n\nThe study was conducted under the authority of the FDA as a public health surveillance activity, which exempted it from institutional review board oversight. This exemption was in accordance with specific regulations and guidelines, ensuring that the data handling and processing complied with the necessary legal and ethical standards.\n\nThe data were not released publicly due to the sensitive nature of the information and the need to protect patient privacy. The study focused on validating anaphylaxis events using established clinical criteria and rigorous methods, and the results were reported with reliable cross-validated performance metrics. The decision to keep the data private was enforced through compliance with regulatory requirements and the use of secure data handling practices.",
  "optimization/algorithm": "The machine-learning algorithms used in this study were not new. They were well-established methods in the field of machine learning. The algorithms evaluated included logistic regression, elastic net, gradient-boosting machines, Bayesian additive regression trees, and neural networks. These algorithms were chosen for their robustness and wide application in various predictive modeling tasks. Additionally, an ensemble method called Super Learner was used, which combines the predictions from the other 24 models to create an optimal weighted combination.\n\nThe decision to use these established algorithms rather than developing a new one was driven by the need for reliability and comparability. These algorithms have been extensively tested and validated in numerous studies, ensuring that their performance metrics are well-understood. This approach allowed for a thorough evaluation of how different machine-learning techniques perform when applied to structured and natural language processing (NLP)-derived data in the context of identifying anaphylaxis events.\n\nThe study focused on applying these algorithms to a rich, high-dimensional set of covariates, including both structured data and NLP-derived data. This approach highlighted the strengths of data-adaptive machine learning in improving the identification of rare health outcomes. The use of these established algorithms provided a solid foundation for comparing the performance of different modeling strategies and understanding their potential for enhancing public health surveillance methods.",
  "optimization/meta": "In our study, we employed a meta-predictor approach known as Super Learner. This method leverages the predictions from multiple machine-learning algorithms to create an optimal weighted combination. The Super Learner algorithm itself does not directly use the raw data but rather the outputs from the other 24 models we evaluated. These models included a variety of algorithms such as logistic regression, elastic net, gradient-boosting machines, Bayesian additive regression trees, and neural networks. Each of these algorithms was applied to different sets of covariates, including structured data only, structured data plus all NLP-derived covariates, and structured data plus a clinician-selected set of NLP covariates.\n\nThe Super Learner aggregates the predictions from these diverse models to improve overall performance. It is designed to enhance predictive accuracy by combining the strengths of different algorithms. The training data for each of the constituent models was independent, ensuring that the Super Learner's performance is not biased by overlapping data. This independence is crucial for the reliability and generalizability of the meta-predictor's results. The Super Learner did not significantly improve performance further than the best individual models, indicating that while it is a robust method, the individual models already achieved high levels of accuracy.",
  "optimization/encoding": "In our study, we employed several preprocessing steps to prepare the data for machine-learning algorithms. Initially, we excluded covariates with low variation, defined as those where 99% or more of the observations had the same value and had a correlation with the outcome of less than 0.05. This step helped in reducing noise and focusing on more informative features.\n\nWe developed models using three sets of covariates: structured covariates only, structured plus all 116 NLP-derived covariates, and structured plus 25 clinician-selected NLP covariates. For each set, we evaluated 25 machine-learning algorithms, which included pairings between three covariate selection strategies and eight different algorithms.\n\nThe three covariate selection strategies used were:\n\n1. Least absolute shrinkage and selection operator (LASSO), which retains covariates with nonzero coefficients in the model that minimized the cross-validated loss.\n2. Partitioning around medoids, which uses the silhouette width to identify the optimal number of clusters between 5 and 20, then retains each cluster medoid.\n3. Retaining all variables (Retain-All), where no covariate selection is performed.\n\nThe eight modeling algorithms evaluated were:\n\n1. Logistic regression\n2. Elastic net with area under the receiver operating characteristic curve (AUC) loss\n3. Gradient-boosting machine with a maximum tree depth of 4 and AUC loss\n4. Gradient-boosting machine with a maximum tree depth of 2 and AUC loss\n5. Bayesian additive regression trees with a regularization parameter k = 1\n6. Bayesian additive regression trees with a regularization parameter k = 2\n7. Neural network with 1 node in one hidden layer\n8. Neural network with 2 nodes in one hidden layer\n\nAdditionally, we used Super Learner, an ensemble method that calculates an optimal weighted combination of predictions from the other 24 models. This approach leverages the strengths of multiple algorithms to improve overall performance.\n\nPrior to modeling, we also standardized non-binary covariates by transforming them to have a mean of zero and a standard deviation of one. This step ensures that all features contribute equally to the model, preventing any single feature from dominating due to its scale.\n\nIn summary, our data encoding and preprocessing involved excluding low-variation covariates, standardizing non-binary features, and evaluating multiple covariate selection strategies and machine-learning algorithms to identify the best-performing models.",
  "optimization/parameters": "In our study, we developed several models using different sets of covariates. Initially, we had 47 structured covariates and 116 NLP-derived covariates. However, we excluded 27 covariates that had low variation, leaving us with 43 structured and 93 NLP-derived covariates.\n\nThe number of covariates included in each model varied depending on the covariate selection method used. We employed three different strategies: Least Absolute Shrinkage and Selection Operator (LASSO), Partitioning Around Medoids (PAM), and Retain-All. The LASSO method retained covariates with nonzero coefficients in the model that minimized the cross-validated loss. The PAM method used the silhouette width to identify the optimal number of clusters between 5 and 20, then retained each cluster medoid. The Retain-All method included all available covariates.\n\nFor structured data only, the number of covariates included in the models ranged from 7 to 43, depending on the selection method. When structured data were augmented with all NLP-derived data, the number of covariates ranged from 13 to 93. This variability allowed us to explore different model complexities and assess their impact on performance.\n\nThe selection of the number of covariates was driven by the goal of optimizing model performance while avoiding overfitting. Cross-validation was used to evaluate the performance of each model, ensuring that the selected covariates contributed meaningfully to the predictive power of the models.",
  "optimization/features": "In our study, we utilized three partially overlapping sets of covariates as input features for our models. The first set consisted of structured covariates only, totaling 43 after excluding low-variation features. The second set combined structured covariates with all 116 NLP-derived covariates, resulting in 159 features. The third set included structured covariates and a clinician-selected set of 25 NLP-derived covariates, amounting to 68 features.\n\nFeature selection was indeed performed using three different strategies: Least Absolute Shrinkage and Selection Operator (LASSO), Partitioning Around Medoids (PAM), and Retain-All. The LASSO method retained covariates with nonzero coefficients in the model that minimized the cross-validated loss. PAM used the silhouette width to identify the optimal number of clusters between 5 and 20, then retained each cluster medoid. The Retain-All strategy, as the name suggests, retained all variables.\n\nIt is crucial to note that feature selection was conducted using the training set only, ensuring that the validation process remained unbiased. This approach helped mitigate the risk of overfitting and ensured that the selected features were generalizable to new, unseen data.",
  "optimization/fitting": "In our study, we developed several models using three partially overlapping sets of covariates: structured covariates only, structured plus all 116 NLP-derived covariates, and structured plus 25 clinician-selected NLP covariates. For each set, we evaluated 25 machine-learning algorithms, which included pairings between three covariate selection strategies and eight algorithms. The covariate selection strategies were Least Absolute Shrinkage and Selection Operator (LASSO), Partitioning Around Medoids (PAM), and Retain-All. The algorithms ranged from logistic regression and elastic net to more complex methods like gradient-boosting machines, Bayesian additive regression trees (BART), and neural networks. Additionally, we used Super Learner, an ensemble method that combines predictions from the other 24 models.\n\nGiven the large number of covariates, particularly with the inclusion of NLP-derived features, the number of parameters was indeed much larger than the number of training points. To mitigate the risk of overfitting, we employed several strategies. First, we used cross-validation, specifically 15-fold cross-validation, to evaluate model performance. This technique helps in assessing how well the model generalizes to unseen data. Second, we applied regularization techniques such as LASSO, which penalizes large coefficients and can shrink some to zero, effectively performing feature selection. Third, we used ensemble methods like Super Learner, which can improve robustness and generalization by combining multiple models.\n\nTo address underfitting, we ensured that our models were complex enough to capture the underlying patterns in the data. For instance, we used gradient-boosting machines and neural networks, which are capable of modeling non-linear relationships. Additionally, we compared the performance of models with different levels of complexity, such as neural networks with one or two nodes in the hidden layer, to find an optimal balance between bias and variance.\n\nWe also excluded covariates with low variation, defined as having \u226599% of observations with the same value and a correlation with the outcome <0.05. This step helped in reducing noise and focusing on more informative features. Furthermore, we ranked predictors by variable importance, defined as the marginal mean difference in predicted probability associated with a change in the covariate. This approach allowed us to identify and focus on the most relevant features, further mitigating the risk of overfitting.\n\nIn summary, we employed a combination of regularization, cross-validation, ensemble methods, and careful feature selection to manage the risk of overfitting and underfitting in our models. These strategies ensured that our models were robust and generalizable to new data.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and enhance the generalization of our models. One of the key methods used was the Least Absolute Shrinkage and Selection Operator (LASSO). LASSO is a regression analysis method that performs both variable selection and regularization to enhance the prediction accuracy and interpretability of the statistical model it produces. It does this by adding a penalty equal to the absolute value of the magnitude of coefficients. This technique helps in reducing the complexity of the model by shrinking some of the coefficients to zero, effectively performing feature selection.\n\nAdditionally, we utilized Bayesian Additive Regression Trees (BART) with different regularization parameters. BART is a non-parametric model that combines the flexibility of regression trees with the robustness of Bayesian inference. The regularization parameters in BART control the complexity of the trees, preventing them from becoming too deep and thus reducing the risk of overfitting.\n\nAnother regularization technique we applied was the Elastic Net, which combines the penalties of both LASSO (L1 regularization) and Ridge regression (L2 regularization). This method is particularly useful when dealing with datasets that have a high number of correlated features, as it can handle multicollinearity more effectively than LASSO alone.\n\nFurthermore, we employed gradient-boosting machines with a maximum tree depth constraint. This technique builds an ensemble of decision trees in a sequential manner, where each new tree corrects the errors of the previous ones. By limiting the depth of the trees, we controlled the complexity of the model, thereby reducing the risk of overfitting.\n\nIn summary, our study incorporated multiple regularization techniques, including LASSO, BART with regularization parameters, Elastic Net, and gradient-boosting machines with depth constraints, to ensure that our models were robust and generalizable.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are not explicitly detailed in the main text. However, specific details about the algorithms and their configurations can be inferred from the descriptions provided. For instance, we used gradient-boosting machines with specified maximum tree depths, Bayesian additive regression trees with particular regularization parameters, and neural networks with defined hidden layers. These details are scattered throughout the text and can be pieced together to understand the configurations used.\n\nThe optimization schedule and model files are not directly reported in the main text or supplementary materials. The focus of the study was on comparing the performance of various machine-learning algorithms and covariate selection strategies rather than providing detailed technical specifications for reproducibility.\n\nRegarding the availability and licensing of the models and parameters, there is no explicit mention of where these can be accessed or under what license they might be available. The study primarily reports on the methods and results without providing direct access to the model files or optimization parameters.\n\nIn summary, while the study provides a comprehensive overview of the methods and results, it does not offer detailed hyper-parameter configurations, optimization schedules, or model files in a readily accessible format. For those interested in replicating the study, the descriptions of the algorithms and their configurations in the text can serve as a starting point, but additional information would be needed for full reproducibility.",
  "model/interpretability": "The models developed in this study vary in their interpretability, ranging from more transparent to black-box approaches. The logistic regression models, both main-terms and those with covariate selection strategies like LASSO and partitioning around medoids, are relatively transparent. These models provide clear coefficients for each covariate, indicating the direction and magnitude of their association with the outcome. For instance, a positive coefficient in logistic regression suggests a higher likelihood of the outcome with an increase in the covariate value, while a negative coefficient suggests the opposite.\n\nOther algorithms, such as gradient-boosting machines and neural networks, are considered more black-box models. These algorithms are powerful in capturing complex relationships within the data but do not provide straightforward interpretations of individual covariates. For example, gradient-boosting machines build an ensemble of decision trees, and neural networks involve layers of interconnected nodes that transform input data through complex, non-linear functions. While these models can achieve high predictive performance, understanding the contribution of each covariate to the final prediction is less intuitive.\n\nTo address the interpretability of the more complex models, variable importance rankings were calculated. These rankings indicate the marginal mean difference in predicted probability associated with a change in each covariate. For binary covariates, this change is a 1-unit increase, and for non-binary covariates, it is a 1-standard-deviation change. This approach helps in identifying which covariates are most influential in the model's predictions, providing some level of interpretability even for black-box models.\n\nIn summary, while some models offer clear interpretability through coefficients, others rely on variable importance rankings to shed light on the key predictors. This balance between transparency and predictive power is crucial in understanding and trusting the model's outputs.",
  "model/output": "The model developed is a classification model. It was designed to predict the case status of anaphylaxis, which is a binary outcome. The performance of the models was evaluated using metrics such as sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), F1 score, and F0.5 score. These metrics are typically used for classification tasks rather than regression tasks. The primary evaluation metric was the area under the receiver operating characteristic curve (AUC), which is a standard metric for assessing the performance of classification models. The models were built using various machine-learning algorithms, including logistic regression, gradient-boosting machines, Bayesian additive regression trees, and neural networks, all of which are commonly used for classification problems.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the models involved a comprehensive approach using cross-validation and external validation. For the initial evaluation, we employed 15-fold cross-validation within the Kaiser Permanente Washington (KPWA) data. This method helped us assess the models' performance by dividing the data into 15 subsets, training on 14, and validating on the remaining one, repeating this process 15 times.\n\nOur primary performance metric was the cross-validated area under the receiver operating characteristic curve (AUC), which was weighted to account for path-specific sampling probabilities. This metric provided a robust measure of the models' ability to discriminate between cases and non-cases.\n\nIn addition to the AUC, we calculated several other performance metrics for the best-performing models. These included sensitivity, specificity, positive predictive value (PPV), negative predictive value, F1 score, and F0.5 score. These metrics were evaluated at various cutpoints between the 10th and 95th quantiles of predicted risk, offering a detailed view of the models' performance across different thresholds.\n\nTo further validate our models, we conducted external validation using data from Kaiser Permanente Northwest (KPNW). This step was crucial for assessing the generalizability of our models to different populations and settings. The external validation confirmed that models incorporating both structured and natural language processing (NLP)-derived data performed best in identifying cases.\n\nThe evaluation also involved comparing different covariate selection strategies and modeling algorithms. We tested 25 machine-learning algorithms, including logistic regression, elastic net, gradient-boosting machines, Bayesian additive regression trees, and neural networks. Additionally, we used an ensemble method called Super Learner, which combined predictions from the other models to potentially improve performance.\n\nOverall, the evaluation process was rigorous and multifaceted, ensuring that the models were thoroughly tested and validated for their ability to predict anaphylaxis events accurately.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the models' effectiveness. The primary metric used was the cross-validated area under the receiver operating characteristic curve (AUC), which provides a single scalar value that represents the ability of the model to discriminate between positive and negative cases across all possible classification thresholds. This metric was weighted to account for path-specific sampling probabilities, ensuring that the results were representative of the entire study population.\n\nIn addition to the AUC, we calculated several other performance metrics for the best-performing models at various cutpoints of predicted risk. These metrics included sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), F1 score, and F0.5 score. Sensitivity measures the proportion of actual positives that are correctly identified by the model, while specificity measures the proportion of actual negatives that are correctly identified. PPV and NPV provide information about the precision of the positive and negative predictions, respectively. The F1 score is the harmonic mean of PPV and sensitivity, offering a balance between the two, while the F0.5 score gives more weight to PPV, which is particularly useful when the cost of false positives is high.\n\nThe cutpoints of predicted risk ranged from the 10th to the 95th quantile, allowing us to assess the models' performance across a wide range of operating points. This approach provides a nuanced understanding of how the models perform under different conditions and helps to identify the optimal trade-off between sensitivity and specificity for practical applications.\n\nThe set of metrics reported in this study is representative of those commonly used in the literature for evaluating machine-learning models, particularly in the context of rare outcomes such as anaphylaxis. The inclusion of multiple metrics ensures a thorough assessment of model performance, taking into account different aspects of predictive accuracy and the potential impact on clinical decision-making.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of various modeling approaches to evaluate their performance in predicting anaphylaxis. We developed several models using different sets of covariates: structured covariates only, structured plus all NLP-derived covariates, and structured plus a clinician-selected set of NLP covariates. For each set, we evaluated 25 machine-learning algorithms, which included pairings between three covariate selection strategies and eight different algorithms.\n\nThe covariate selection strategies we employed were Least Absolute Shrinkage and Selection Operator (LASSO), Partitioning Around Medoids (PAM), and Retain-All. The algorithms ranged from traditional logistic regression to more complex methods such as gradient-boosting machines, Bayesian additive regression trees (BART), and neural networks. Additionally, we used Super Learner, an ensemble method that combines predictions from the other models.\n\nOur primary analysis focused on comparing the best-performing model using only structured data covariates to the best-performing model that incorporated both structured data and all NLP-derived covariates. In secondary analyses, we evaluated the performance of the best model using structured data and the clinician-selected set of NLP-derived covariates, as well as a main-terms logistic regression model using only structured data covariates. This conventional model approach served as a baseline for comparison.\n\nThe performance of these models was assessed using cross-validated area under the receiver operating characteristic curve (AUC), which provided a robust measure of model accuracy. We also calculated additional metrics such as sensitivity, specificity, positive predictive value (PPV), negative predictive value, F1 score, and F0.5 score at various cutpoints of predicted risk. These metrics offered a comprehensive evaluation of model performance across different thresholds.\n\nThe results indicated that the addition of NLP-derived covariates significantly improved model performance. For instance, the best-performing structured data model, a neural network with LASSO variable selection, achieved a weighted cross-validated AUC of 0.62. In contrast, the best-performing model that included both structured and NLP-derived data, a BART2 model with LASSO variable selection, yielded a cross-validated AUC of 0.71. This improvement highlights the value of incorporating NLP-derived covariates in enhancing predictive accuracy.\n\nFurthermore, we validated our models in an external dataset from Kaiser Permanente Northwest (KPNW) to assess their generalizability. The BART2-Retain-All model demonstrated robust performance in both the development and validation datasets, confirming its ability to generalize well within the Kaiser network.\n\nIn summary, our study involved a rigorous comparison of various modeling approaches, including simpler baselines and more complex machine-learning methods. The results underscored the importance of integrating NLP-derived covariates to achieve superior predictive performance.",
  "evaluation/confidence": "The evaluation of our models included several performance metrics, with confidence intervals provided for the primary metric, the cross-validated area under the receiver operating characteristic curve (AUC). This metric was reported with 95% confidence intervals for various models, allowing for an assessment of the uncertainty around the point estimates.\n\nFor instance, the main-terms logistic regression model applied to structured data achieved a cross-validated AUC of 0.56 with a 95% confidence interval of 0.52 to 0.61. More sophisticated machine-learning methods applied to the same structured data improved the cross-validated AUC to 0.62, with a 95% confidence interval of 0.58 to 0.66. When traditional regression methods were applied to structured and all NLP-derived data, the cross-validated AUC improved to 0.66, with a 95% confidence interval of 0.62 to 0.71. The best-performing model, which incorporated both machine learning and NLP-derived data, achieved a cross-validated AUC of 0.71, with a 95% confidence interval of 0.66 to 0.76.\n\nStatistical significance was assessed to determine if the improvements in performance were meaningful. The addition of NLP-derived covariates clearly improved model performance, as evidenced by the higher AUC values and non-overlapping confidence intervals compared to models using only structured data. For example, the BART2 model with LASSO variable selection performed best when structured data were augmented with all NLP-derived data, yielding a cross-validated AUC of 0.71, which was significantly higher than the AUC of 0.62 achieved by the best-performing structured data model.\n\nIn summary, the performance metrics included confidence intervals, and the results indicated statistically significant improvements in model performance when NLP-derived covariates were added to structured data. This provides confidence in the superiority of the methods that incorporate both machine learning and NLP-derived data.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study was conducted under the authority of the FDA as a public health surveillance activity, which is not subject to institutional review board oversight. This indicates that the data used for evaluation is likely proprietary and restricted due to privacy and regulatory considerations. The evaluation focused on cross-validated performance metrics within the Kaiser Permanente network, specifically at Kaiser Permanente Washington (KPWA) and Kaiser Permanente Northwest (KPNW). The performance of the models was assessed using cross-validated area under the receiver operating characteristic curve (AUC) and other metrics such as sensitivity, specificity, positive predictive value, and negative predictive value. These evaluations were conducted internally and the detailed results, including the specific data used, are not released to the public."
}