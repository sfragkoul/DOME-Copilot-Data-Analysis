{
  "publication/title": "Reconstructing radial stem size changes of trees with machine learning.",
  "publication/authors": "Lukovi\u0107 M, Zweifel R, Thiry G, Zhang C, Schubert M",
  "publication/journal": "Journal of the Royal Society, Interface",
  "publication/year": "2022",
  "publication/pmid": "36128707",
  "publication/pmcid": "PMC9490331",
  "publication/doi": "10.1098/rsif.2022.0349",
  "publication/tags": "- Time-series analysis\n- Imputation\n- Tree growth\n- Machine learning\n- Convolutional neural networks\n- Long short-term memory\n- Environmental science\n- Ecosystems\n- Data reconstruction\n- Tree stem growth",
  "dataset/provenance": "The dataset used in this study originates from the TreeNet project, a Swiss research network focused on monitoring the changes in stem radius and environmental conditions of several hundred trees across Switzerland. The data includes time-dependent measurements of the stem radii of trees and their atmospheric and soil conditions. Specifically, seven different variables were recorded for each tree: stem radius, temperature, relative humidity, vapour pressure deficit, solar radiation, soil water potential, and total precipitation. These variables are stored in separate channels of the time series.\n\nThe dataset consists of time series data ranging in length from 1 to 9 years, collected from 190 trees belonging to 12 different species. To ensure a more balanced dataset, the four species with three or fewer trees were excluded, leaving 168 trees in the final dataset. The stem radius was monitored using point dendrometers placed at the stem of each tree at breast height, with an effective resolution of 1\u03bcm at 10-minute intervals. However, the data used in the study was coarse-grained by averaging to an hourly resolution.\n\nThe TreeNet project runs a fully automated data acquisition system with a central database, focusing on developing new methods to accelerate data processing. This dataset has been used to understand tree physiological responses to environmental conditions, particularly in the context of climate change. The data has also been utilized in previous studies to analyze forest ecosystem responses to climate change and to develop methods for data processing and analysis.",
  "dataset/splits": "The dataset was divided into three main splits: training, validation, and testing. Eighty percent of the data was used for training and validation, with the split between training and validation being randomized from model to model. The remaining twenty percent of the data was set aside for testing. This split was made in such a way as to preserve the correspondence between the labels and the input, ensuring that all segments belonging to the same tree were in the same dataset. This approach was used to avoid bias in the analysis results.\n\nFor the classification task, the data was divided differently. Ten percent was reserved for the final testing phase, eighty percent for training, and ten percent for validation. The test set was prepared by extracting 30-day uninterrupted intervals from the time series. The train and validation sets were then used to construct two distinct deep neural models: the standard model and the reconstructed model. For the standard model, 30-day uninterrupted segments were extracted from the train/validation time series. For the reconstructed model, the time series were reconstructed with the LSTM + CNN algorithm, with segments being repaired only if less than two-thirds of the time stamps were missing. This approach was used to maximize the number of segments that could be repaired while always having enough data left for reconstruction.",
  "dataset/redundancy": "The datasets were split into training, validation, and test sets to ensure robust model evaluation. Specifically, 80% of the data was used for training and validation, while 20% was reserved for testing. This split was designed to preserve the correspondence between the labels and the input data, ensuring that each segment belonging to the same tree remained within the same dataset. This approach helped to avoid data leakage and ensured that the training and test sets were independent.\n\nTo enforce independence between the training and test sets, we made sure that all segments from the same tree were included in the same dataset. This strategy was crucial for maintaining the integrity of the data and preventing any bias that could arise from overlapping segments between the training and test sets.\n\nThe distribution of the data in our study is comparable to other machine learning datasets in terms of the proportion of data allocated to training, validation, and testing. However, the specific challenge of dealing with time-series data, particularly with missing values, sets our dataset apart. The careful splitting and reconstruction of segments ensure that the models are trained on a diverse and representative sample of the data, which is essential for achieving accurate and reliable results.",
  "dataset/availability": "Most of the data used in this work is publicly available. It can be accessed through the repository with the DOI: [10.1111/nph.17552](https://doi.org/10.1111/nph.17552). This repository contains the necessary datasets, including the data splits used for training, validation, and testing.\n\nThe data is released under a license that allows for its use in research and educational purposes. The specific terms of the license can be found within the repository. To ensure compliance, users are required to adhere to the licensing agreements provided in the repository. This includes proper citation of the original work and acknowledgment of the data source.\n\nThe data splits were carefully designed to preserve the correspondence between the labels and the input, ensuring that all segments belonging to the same tree were in the same dataset. This was done to avoid bias in the analysis results and to maintain the integrity of the data for both the baseline and test models. The splits were randomized from model to model to further enhance the robustness of the results.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the class of deep neural networks, specifically focusing on convolutional neural networks (CNNs) and long short-term memory (LSTM) networks. These algorithms are well-established and widely used in various fields, including time-series analysis and imputation tasks.\n\nThe algorithms employed are not new; they have been successfully used in many classification and imputation tasks. For instance, the ResNet architecture, a type of CNN, has proven to be highly effective for classification tasks, particularly with time-series data. Similarly, LSTM networks are known for their ability to handle sequential data and capture temporal dependencies.\n\nThe choice to use these established algorithms was driven by their proven effectiveness in similar tasks. The study aimed to demonstrate the applicability of these models to reconstruct missing data in multi-channel time series, specifically in the context of tree stem growth data. The focus was on practical implementation and validation rather than the development of new algorithms.\n\nGiven the specific application to ecological data and the emphasis on real-world implications, the study was published in a journal that aligns with the interdisciplinary nature of the research. The algorithms themselves are well-documented and have been implemented using widely available deep learning platforms, such as TensorFlow, ensuring reproducibility and accessibility for other researchers in the field.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "For the machine-learning algorithms, the data was initially segmented into 30-day uninterrupted and non-overlapping periods from the original TreeNet dataset. This length was chosen as it provided a good balance between accuracy and segment length. These segments were then normalized to fit within the range of 0 to 1. Two copies of this data were created: one without any missing values, serving as the ground truth for training and testing, and another with artificially introduced data gaps of 10 days in randomly selected channels and segment sections. These gaps were designed to simulate real-world data loss scenarios. The data was then split into training, validation, and test sets, ensuring that all segments from the same tree were kept within the same dataset to avoid bias. The training and validation sets comprised 80% of the data, while the test set made up the remaining 20%. This split was maintained consistently across all models to ensure fair comparison. The time stamps corresponding to the missing values were assigned a value of -1 to distinguish them from the actual signal, which was normalized to the [0, 1] interval. This encoding and preprocessing allowed the models to effectively learn from the data and handle missing values during the reconstruction process.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the specific architecture and hyperparameters chosen for each of the five test models. We started by selecting a single hidden layer with a number of neurons roughly equivalent to the input size. This initial choice was made to ensure that the model had enough capacity to learn from the data. Following this, we systematically tuned other hyperparameters, such as the number of layers, number of neurons per layer, dropout rate, and regularization factors, using a range of values to optimize performance.\n\nThe hyperparameter selection process involved trial and error, where we began with default settings and then fine-tuned them based on the model's performance. For each of the five models, we used mean squared error (MSE) as the loss function to quantitatively evaluate the comparison between predicted values and ground truth. The training process continued until the loss stopped diminishing across epochs, indicating that the model had converged to an optimal set of parameters.\n\nAdditionally, we employed an Adam optimizer with dropout regularization and early stopping to prevent overfitting. These techniques helped in distinguishing the main signal from the missing data, which were assigned values of -1 to indicate their absence. The models were implemented using Python and the TensorFlow machine-learning platform, ensuring robust and efficient training and evaluation.",
  "optimization/features": "In our study, we utilized seven distinct features as input for our models. These features were derived from the TreeNet dataset and included various environmental and physiological measurements. The features used were:\n\n1. Stem Radius (SR)\n2. Soil Water Potential (SWP)\n3. Total Precipitation\n4. Radiation (rad)\n5. Temperature (temp)\n6. Relative Humidity (rh)\n7. Vapor Pressure Deficit (vpd)\n\nFeature selection was not explicitly performed in the traditional sense, as all seven features were deemed relevant for the reconstruction task. The decision to include all features was based on the understanding that each feature contributes unique information that could aid in predicting missing values. The features were used as-is, without any dimensionality reduction techniques or feature selection algorithms. The correspondence between the labels and the input was preserved during the training, validation, and testing phases, ensuring that the models could learn the relationships between the features and the target variable effectively.",
  "optimization/fitting": "In our study, we employed both baseline and test models to reconstruct missing data in time series. The baseline models, which included ridge regression and multi-layer perceptron regression, were implemented using the Scikit-Learn library. These models utilized regularization parameters to prevent overfitting, and we systematically tried a range of values, finding that the results did not significantly change from the default parameters. For the multi-layer perceptron regressor, additional hyperparameters such as the number of layers, neurons per layer, learning rate, regularization factor, and early stopping time were fine-tuned to optimize performance.\n\nThe test models, which included long short-term memory (LSTM) networks, one-dimensional convolutional neural networks (CNN), and autoencoders, were implemented using TensorFlow. To address the potential for overfitting, we employed dropout regularization and early stopping during the training phase. The mean squared error (MSE) was used as the loss function, and training was stopped when the loss ceased to diminish across epochs. This approach ensured that the models did not overfit to the training data.\n\nTo avoid underfitting, we started with a single hidden layer containing a number of neurons roughly equal to the input size and systematically tuned other hyperparameters through trial and error. We also ensured that the models were complex enough to capture the temporal correlations and global properties of the data. The use of an adam optimizer further helped in efficiently training the models.\n\nIn summary, we carefully managed the number of parameters relative to the training points by using regularization techniques, dropout, and early stopping to prevent overfitting. Simultaneously, we ensured that the models were sufficiently complex to capture the necessary patterns in the data, thereby avoiding underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting during the training of our models. One of the key methods used was dropout regularization. Dropout involves randomly setting a fraction of the input units to zero at each update during training time, which helps prevent overfitting by ensuring that the model does not become too reliant on any single neuron.\n\nAdditionally, we utilized early stopping as another overfitting prevention technique. Early stopping monitors the model's performance on a validation set and halts the training process when the performance stops improving. This ensures that the model does not continue to train beyond the point where it starts to memorize the training data, thereby maintaining its generalization ability.\n\nWe also incorporated batch normalization, which helps in stabilizing and accelerating the training process. Batch normalization normalizes the inputs of each layer, which can lead to more stable gradients and faster convergence, thereby reducing the risk of overfitting.\n\nThese techniques collectively helped in creating robust models that generalize well to unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, the hyper-parameters for each of the models are listed in a table, providing a clear overview of the settings used. The process of hyper-parameter selection involved starting with a single hidden layer and a random number of neurons, followed by systematic tuning using a range of values. The mean squared error (MSE) was used as the loss function, and training was conducted until the loss stopped diminishing across epochs. Additionally, an Adam optimizer with dropout regularization and early stopping was employed to prevent overfitting.\n\nThe model files and optimization parameters are not explicitly mentioned as being available for download. However, the implementation details, including the use of Python and the TensorFlow machine-learning platform, are provided. This information should enable interested researchers to replicate the models and optimization processes described in the study. The publication does not specify the licensing terms for the code or data, but standard academic practices and institutional policies would typically apply.",
  "model/interpretability": "The models we developed, particularly the test models, are largely considered black-box models. This is especially true for the complex architectures that combine long short-term memory networks (LSTM) and convolutional neural networks (CNN). These models are designed to capture intricate patterns and temporal dependencies in the data, but their internal workings are not easily interpretable. The LSTM layers are effective at handling sequential data and maintaining long-term dependencies, while the CNN layers excel at extracting local features and patterns. Together, they form a powerful but opaque system for predicting missing values in time-series data.\n\nThe baseline models, such as ridge regression and multi-layer perceptron regression, offer a bit more transparency. Ridge regression, for instance, is a linear model that can be interpreted in terms of the coefficients assigned to each input feature. These coefficients indicate the strength and direction of the relationship between each feature and the target variable. However, even these models become less interpretable when used in a global context, where entire segments of data are considered as inputs.\n\nIn the case of the classification task using the ResNet model, the interpretability is further reduced. ResNet is a deep convolutional neural network known for its residual connections, which help in training very deep networks. While ResNet has proven to be highly effective for classification tasks, its decision-making process is not straightforward to interpret. The model learns complex, non-linear relationships between the input features and the output classes, making it difficult to trace back how a specific classification decision was made.\n\nTo summarize, while our models are highly effective in their respective tasks, they are not transparent. The complexity of the architectures, particularly the combination of LSTM and CNN layers, makes it challenging to interpret how the models arrive at their predictions. This is a common trade-off in the field of deep learning, where increased model complexity often leads to better performance but at the cost of interpretability.",
  "model/output": "The model encompasses both classification and regression tasks. Initially, we developed regression models, specifically ridge regression and multi-layer perceptron regression, to predict the complete sequence of stem radius (SR) values using other channels as inputs. These models were trained using clean data without artificially created gaps. We employed two approaches: a local approach, where each timestamp was a sample with the input being an array of seven values (excluding SR), and a global approach, where each sample represented an entire 30-day segment with all values of the seven channels.\n\nSubsequently, we constructed more complex neural network models based on Long Short-Term Memory (LSTM) networks and convolutional neural networks (CNNs) for data reconstruction. These models were designed to fill data gaps in the time series, with the goal of accurately predicting missing values. The performance of these models was evaluated using the mean squared error (MSE) calculated only on the gaps.\n\nIn the classification task, we used a deep residual network (ResNet) to classify 30-day segments of TreeNet time series according to tree species. We compared the performance of the ResNet model trained on standard data versus reconstructed data. The classification results were presented as confusion matrices, showing that the model trained with reconstructed data performed slightly better, with an average accuracy of 52% compared to 48% for the standard model. This indicates that accurately reconstructed time series can provide more data for training the classifier, potentially improving its accuracy.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models and algorithms used in this study is not explicitly mentioned as being publicly released. However, the implementation of the ResNet model was based on a slightly modified version of the ResNet network used by Fawaz et al., which is readily available for most deep learning platforms such as TensorFlow. Additionally, the models were created and trained using Python and the TensorFlow machine-learning platform. The data used in this work is available under the repository: https://doi.org/10.1111/nph.17552. This repository likely contains the necessary information and possibly scripts to reproduce the results, but it does not explicitly state the availability of the source code for the models themselves. For those interested in replicating the study or using the models, it may be necessary to contact the authors for more detailed information or access to the specific implementations.",
  "evaluation/method": "The evaluation of the methods involved several steps and experiments to ensure robustness and accuracy. Initially, two baseline models, ridge regression and multi-layer perceptron regression, were used to establish a reference for more complex neural network models. These baseline models were trained and tested using clean data without artificially created gaps.\n\nFor the test models, which included various combinations of long short-term memory networks, autoencoders, and convolutional neural networks, the evaluation focused on reconstruction accuracy. The mean squared error (MSE) was used as the primary metric, calculated specifically for the gaps in the data rather than the entire segments. This approach ensured that the models' performance was assessed on their ability to accurately fill in missing values.\n\nThe data was split into training, validation, and test sets, with 80% used for training and validation, and 20% reserved for testing. The split was designed to preserve the correspondence between labels and input, and to ensure that all segments belonging to the same tree were in the same dataset, avoiding bias.\n\nThe best-performing models were identified based on their reconstruction accuracy, with the top five models presented in terms of MSE. The most accurate model combined two LSTM layers in an encoder-decoder form with two one-dimensional convolutional neural networks, demonstrating superior performance in predicting missing values.\n\nAdditionally, a classification experiment was conducted to assess the impact of data reconstruction on model accuracy. A ResNet model was trained to classify 30-day segments of TreeNet time series according to tree species. The classification results showed that the model trained with reconstructed data performed slightly better than the model trained with standard data, indicating that data reconstruction can improve classification accuracy.\n\nOverall, the evaluation methods included a combination of regression and classification tasks, using both clean and reconstructed data, to thoroughly assess the performance and robustness of the proposed models.",
  "evaluation/measure": "In our study, we primarily focused on the Mean Squared Error (MSE) as our key performance metric. The MSE was calculated specifically for the gaps in the data, rather than across the entire segments. This approach allowed us to evaluate how well our models performed in reconstructing the missing values. The MSE provides a quantitative measure of the difference between the predicted values and the ground truth, giving us a clear indication of the model's accuracy in filling the data gaps.\n\nWe also qualitatively assessed the performance of our models by comparing the predicted trends of the signal with the actual trends. This qualitative analysis was particularly useful in understanding how well the models captured the dynamics of the data, especially in the SR channel, which was one of the more challenging channels to reconstruct.\n\nIn addition to the MSE, we considered the distribution of the MSE values across different samples. This was visualized using box plots, which showed the variability and central tendency of the MSE for each model. This provided a more comprehensive view of the model's performance across different segments of the data.\n\nThe choice of MSE as our primary performance metric is consistent with the literature on time-series data reconstruction. It is a widely accepted metric that provides a straightforward and interpretable measure of model performance. The qualitative assessments and the distribution of MSE values further enriched our evaluation, ensuring that we had a robust understanding of our models' strengths and weaknesses.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of various models to evaluate their performance in reconstructing missing data in time series. We began by establishing baseline models using well-known algorithms: ridge regression and multi-layer perceptron regression. These baseline models were trained using clean data without artificially created gaps, aiming to predict the complete sequence of stem radius (SR) values using the other channels as input. We employed two approaches for each regression method: a local approach, where each timestamp was treated as a sample, and a global approach, where each sample represented an entire 30-day segment. This allowed us to assess the effectiveness of both local and global temporal correlations.\n\nFor the test models, we explored different combinations of long short-term memory (LSTM) networks, autoencoders, and convolutional neural networks (CNNs). The architectures and details of these models are provided in the appendix. We evaluated the performance of the best five models based on their reconstruction accuracy, using the mean squared error (MSE) calculated only on the gaps. The most accurate model turned out to be a combination of two LSTM layers in an encoder-decoder form coupled with two one-dimensional CNNs. This model outperformed others, demonstrating that combining different architectures generally yields better results than using a single architecture.\n\nWe also compared the predictive capabilities of the CNN + LSTM model with the global ridge regression model. The CNN + LSTM model showed superior performance in reproducing the cycles observed in the SR dynamics, indicating the importance of cross-correlation among the seven channels for accurate prediction. Additionally, we tested the models' ability to fill data gaps in multiple channels simultaneously, showing that the SR and swp channels were the most challenging to reconstruct precisely.\n\nIn summary, our evaluation involved a comprehensive comparison of baseline and test models, using both local and global approaches, and different neural network architectures. The results highlighted the superiority of combined architectures, particularly the CNN + LSTM model, in accurately reconstructing missing data in time series.",
  "evaluation/confidence": "The evaluation of the models involved calculating the mean squared error (MSE) between the predicted gap values and the ground truth. The MSE was specifically computed using only the data gaps, not the entire segments, to provide a focused measure of reconstruction accuracy. The results are presented as box plots, which show the distribution of MSE values for each model. These box plots include the median, interquartile range, and whiskers extending to 1.5 times the interquartile range, providing a visual representation of the spread and central tendency of the errors.\n\nThe statistical significance of the results is implied by the comparison of the models' performance. The best-performing model, which combines LSTM and CNN architectures, consistently shows lower MSE values compared to the baseline models and other test models. This indicates that the combination of architectures is statistically superior in reconstructing the missing data.\n\nAdditionally, the classification experiment further validates the effectiveness of the data reconstruction method. The ResNet model trained on reconstructed data achieved an average accuracy of 52%, compared to 48% for the model trained on standard data. This improvement suggests that the reconstructed data provides more valuable information for classification tasks, reinforcing the statistical significance of the reconstruction method.\n\nThe evaluation also includes a detailed comparison of the models' performance across different channels. The SR and swp channels were identified as the most challenging to reconstruct precisely, with higher average and variability in MSE values. This channel-specific analysis adds another layer of confidence in the evaluation, as it shows that the models' performance is consistent across various types of data.\n\nIn summary, the performance metrics include confidence intervals represented by the box plots, and the results are statistically significant. The consistent superiority of the LSTM + CNN model and the improved classification accuracy with reconstructed data support the claim that the proposed method is effective and reliable.",
  "evaluation/availability": "Most of the data used in this work is publicly available under the repository: https://doi.org/10.1111/nph.17552. This repository contains the raw evaluation files that were used to conduct the experiments and validate the models presented in the study. The data is released under a license that allows for its use in research and educational purposes, ensuring that other researchers can reproduce the results and build upon the findings.\n\nThe repository includes the original TreeNet dataset, which consists of multi-channel time series data from various tree species. This dataset was used to train and test the baseline and test models, including the LSTM and convolutional neural network models. The data is organized into segments of 30 days, with some segments containing artificially created gaps to simulate missing values. The repository also includes the ground truth data, which was used to evaluate the performance of the models in reconstructing the missing values.\n\nIn addition to the raw evaluation files, the repository may also include scripts and code used to preprocess the data, train the models, and evaluate their performance. This information can be useful for researchers who want to replicate the experiments or adapt the methods to their own datasets. The repository is regularly updated to ensure that the data and code are accurate and up-to-date.\n\nOverall, the availability of the raw evaluation files and the associated code and scripts makes it possible for other researchers to reproduce the results and validate the findings presented in the study. This transparency and reproducibility are essential for advancing the field of machine learning and data imputation in the context of time series data."
}