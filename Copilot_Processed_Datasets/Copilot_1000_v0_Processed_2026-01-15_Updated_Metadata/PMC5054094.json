{
  "publication/title": "Identification of microRNA precursors with support vector machine and string kernel.",
  "publication/authors": "Xu JH, Li F, Sun QF",
  "publication/journal": "Genomics, proteomics & bioinformatics",
  "publication/year": "2008",
  "publication/pmid": "18973868",
  "publication/pmcid": "PMC5054094",
  "publication/doi": "10.1016/s1672-0229(08)60027-3",
  "publication/tags": "- MicroRNA\n- Pre-miRNA\n- SVM classifier\n- String kernel\n- WLD\n- Computational biology\n- Bioinformatics\n- Machine learning\n- Sequence analysis\n- Secondary structures\n- Cross-validation\n- Grid search\n- Parameter optimization\n- miRNA identification\n- Biological sequences\n- Computational methods\n- Algorithm development\n- Data analysis\n- Experimental results\n- Statistical significance",
  "dataset/provenance": "The datasets used in this study were primarily sourced from the miRNA registry database, specifically release 5.0 from September 2004. This release contained 207 human miRNAs, which were utilized to construct the datasets for both true and false pre-miRNAs. The true miRNAs and pre-miRNAs of 12 species were included in this dataset.\n\nFor the positive training samples, 163 human miRNAs without multiple loop hairpin structures were used. Additionally, 30 miRNAs without multiple loops were used as test samples. These samples are referred to as \"TE-C-real\" in the study. Furthermore, 14 miRNAs with multiple loops, which were excluded in previous studies, were included as one of the test sets.\n\nThe artificially false pre-miRNAs were constructed from sequence segments that had similar stem-loop hairpin structures to true pre-miRNAs but were not recognized as such. These segments included protein coding regions of human RefSeq genes and gene regions on human chromosome 19. In total, 8,494 false pre-miRNAs were constructed from the protein coding sequences of human RefSeq genes. From this set, 168 were selected as negative training samples, and 1,000 were used as test samples, labeled as \"TE-C (pseudo).\" Additionally, 2,444 false pre-miRNAs with a length of 100 nucleotides were built from gene regions on human chromosome 19, forming the \"Conserved-hairpin\" test set.\n\nThe study utilized a total of 331 training samples, including 163 positive and 168 negative samples, to design the SVM classifier. The remaining 16 independent datasets were used to validate the identification accuracies of the method. These datasets included samples from various species, such as humans, animals, plants, and viruses, as well as artificially generated false pre-miRNAs.",
  "dataset/splits": "In our study, we utilized a total of 18 datasets, which were divided into training and test sets. The training set consisted of 331 samples, with 163 positive samples derived from true human pre-miRNAs and 168 negative samples from artificially false human pre-miRNAs. These samples were used to design the SVM classifier.\n\nThe test sets included 16 independent datasets from various sources:\n\n* Three human datasets: TE-C-real, Updated, and Multiple loops.\n* Eight animal datasets: Caenorhabditis briggsae, Caenorhabditis elegans, Drosophila melanogaster, Drosophila pseudoobsura, Danio rerio, Gallus gallus, Rattus norvegicus, and Mus musculus.\n* Two plant datasets: Arabidopsis thaliana and Oryza sativa.\n* One virus dataset: Epstein Barr Virus.\n* Two artificially false human pre-miRNA datasets: TE-C (pseudo) and Conserved-hairpin.\n\nThe sizes of these datasets varied significantly. For instance, the TE-C (pseudo) dataset contained 1,000 samples, while the Epstein Barr Virus dataset had only 5 samples. The length of the sequences in these datasets also varied, with the minimal length being 58 nucleotides and the maximal length being 263 nucleotides.\n\nTo ensure the reliability of our results, we employed a 5-fold cross-validation strategy with 5 different partitions. This approach helped us to determine the optimal algorithmic parameters and to enhance the statistical significance of our findings. The accuracies on the independent test sets were averaged over all classifiers designed using the optimal parameter pairs.",
  "dataset/redundancy": "The datasets used in this study were carefully curated to ensure independence between training and test sets. The training set consisted of 331 samples, including 163 positive samples from true human pre-miRNAs and 168 negative samples from artificially false human pre-miRNAs. These samples were used to design the SVM classifier.\n\nThe test sets were independent and comprised 16 datasets from various sources, including human, animal, plant, virus, and artificially false human pre-miRNAs. These datasets were not used in the training process, ensuring that the evaluation of the classifier's performance was unbiased.\n\nTo enforce independence, the training samples were selected from a specific database release, while the test sets included samples from the same and subsequent database releases. This approach ensured that the test sets contained samples that were not seen during the training phase.\n\nThe distribution of the datasets in this study is comparable to previously published machine learning datasets in the field. The training set size is similar to that used in previous studies, and the test sets cover a wide range of species and pre-miRNA types, providing a comprehensive evaluation of the classifier's performance.\n\nThe artificially false pre-miRNAs were constructed from sequence segments that had similar stem-loop hairpin structures to true pre-miRNAs but were not recognized as such. This ensured that the negative samples were challenging and representative of real-world scenarios.\n\nIn summary, the datasets were split into independent training and test sets, with careful consideration given to ensuring that the test sets were not used in the training process. This approach provides a robust evaluation of the classifier's performance and is comparable to previously published datasets in the field.",
  "dataset/availability": "The datasets used in this study are not publicly released in a forum. The datasets were collected and preprocessed specifically for this research. The positive training samples consisted of true human pre-miRNAs, while the negative training samples were artificially generated false human pre-miRNAs. These datasets were used to train and validate the SVM classifier. The specific details of the datasets, including the sequences and their secondary structures, are not made available publicly. The study focused on the methodology and performance of the classifier rather than the dissemination of the datasets themselves.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is Support Vector Machine (SVM). SVM is a well-established and widely used supervised learning method for classification and regression tasks. It is particularly effective in high-dimensional spaces and when the number of dimensions exceeds the number of samples.\n\nThe specific implementation of SVM in our work is not entirely new, but it incorporates a novel approach to handling sequence data. We utilize an exponential string kernel based on Weighted Levenshtein Distance (WLD) to directly process pre-miRNA sequences and their secondary structures. This kernel allows us to capture more discriminative and characteristic information hidden in the sequences, which is crucial for accurately identifying true and false pre-miRNAs.\n\nThe reason this method was not published in a machine-learning journal is that the primary focus of our study is on bioinformatics and the specific application of identifying microRNA precursors. The innovation lies in the application of the SVM with the exponential string kernel to this biological problem, rather than in the development of a new machine-learning algorithm per se. Our work demonstrates the effectiveness of this approach in a biological context, which is the main contribution of our study. The method was published in a bioinformatics journal because it addresses a significant problem in the field of genomics and provides a practical solution that can be applied to real-world biological data.",
  "optimization/meta": "The model described in this publication is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it employs a support vector machine (SVM) classifier combined with an exponential string kernel based on weighted Levenshtein distance (WLD). This approach directly handles sequence data, including pre-miRNA sequences and their secondary structures, without converting them into numerical features. The optimal algorithmic parameters for the SVM are determined through 5-fold cross-validation and a grid search scheme, ensuring robust and statistically significant results. The training data consists of 331 true and false human pre-miRNAs, with positive samples derived from true human pre-miRNAs and negative samples from artificially false human pre-miRNAs. The method's performance is evaluated on 16 independent test sets from various species, demonstrating its effectiveness in identifying true and false pre-miRNAs, particularly in animal species.",
  "optimization/encoding": "In our study, we directly utilized pre-miRNA sequences and their secondary structures as symbol sequences for encoding. Both the pre-miRNA sequences, which consist of the four nucleotides (A, U, G, C), and the secondary structure sequences, which include brackets and dots, were considered as inputs for our model. This approach allowed us to capture more discriminative and characteristic information hidden within the sequences and their structures.\n\nThe brackets in the secondary structure sequences indicate pairing between nucleotides, with a left bracket \"(\" denoting a nucleotide near the 5\u2032-end that pairs with another nucleotide at the 3\u2032-end, denoted by a right bracket \")\". Dots represent unpaired nucleotides. This encoding method enabled us to handle the data in its original sequence form, rather than converting it into numerical features, which can sometimes lead to a loss of important information.\n\nBy using this encoding strategy, we constructed an exponential string kernel based on the Weighted Levenshtein Distance (WLD) for Support Vector Machine (SVM). This kernel allowed us to directly process the sequence data, improving the identification accuracy of true and false pre-miRNAs, particularly those with multiple loops that were excluded in previous studies.",
  "optimization/parameters": "In our study, three key parameters were used in the model. These parameters include a regularization constant, denoted as C, and two width parameters of exponential kernels, denoted as \u03b3 sequence and \u03b3 structure. To simplify the computational process, the two kernel widths were set to be identical, meaning \u03b3 = \u03b3 sequence = \u03b3 structure.\n\nThe selection of these parameters was determined through a combination of 5-fold cross-validation and a grid search strategy. This approach involved executing 121 parameter pairs, with C and \u03b3 ranging from 2^0 to 2^10. The parameter pair that yielded the highest accuracy based on the left-out training samples was considered optimal. In cases where multiple optimal pairs were identified, all of them were utilized to design the SVM classifier using all training samples, and the accuracies on independent test sets were averaged across all classifiers. To enhance the statistical significance of our results, five realizations with different 5-fold partitions were performed, and the overall accuracies were averaged.",
  "optimization/features": "Not applicable",
  "optimization/fitting": "In our study, we employed a Support Vector Machine (SVM) classifier with an exponential string kernel to identify true and false pre-miRNAs. The SVM classifier was trained using 331 samples, consisting of 163 positive samples from true human pre-miRNAs and 168 negative samples from artificially false human pre-miRNAs.\n\nThe SVM model involves three key parameters: a regularization constant C and two width parameters of exponential kernels, \u03b3sequence and \u03b3structure. To simplify the process and save computational time, we set \u03b3sequence equal to \u03b3structure, effectively reducing the number of parameters to two.\n\nTo determine the optimal values for C and \u03b3, we used a 5-fold cross-validation scheme combined with a grid search strategy. This approach involved evaluating 121 parameter pairs (11 values for C and 11 values for \u03b3) to find the combination that yielded the highest accuracy on the left-out training samples. When multiple optimal pairs were identified, all were utilized to design the SVM classifier, and the accuracies on independent test sets were averaged across all classifiers.\n\nTo enhance the statistical significance of our results, we performed 5 realizations with different 5-fold partitions. This ensured that our findings were robust and not dependent on a specific partitioning of the data.\n\nBy using cross-validation and grid search, we effectively addressed the risk of overfitting. The cross-validation process helps to ensure that the model generalizes well to unseen data by evaluating its performance on multiple subsets of the training data. The grid search method systematically explores a range of parameter values, ensuring that the optimal parameters are selected based on their performance across different data partitions.\n\nAdditionally, the use of multiple realizations with different partitions further mitigates the risk of overfitting by providing a more comprehensive assessment of the model's performance. This approach also helps to rule out underfitting, as it ensures that the model is adequately complex to capture the underlying patterns in the data.\n\nIn summary, our fitting method involved a careful balance of model complexity and generalization performance, achieved through the use of cross-validation, grid search, and multiple realizations with different data partitions. This approach ensured that our SVM classifier was neither overfitted nor underfitted, providing reliable and statistically significant results.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our Support Vector Machine (SVM) classifier. One of the key methods used was the regularization technique, which is inherently part of the SVM algorithm. The regularization constant, denoted as C, plays a crucial role in balancing the trade-off between achieving a low training error and maintaining a simple model to avoid overfitting.\n\nTo determine the optimal value of the regularization constant C, along with the kernel width parameter \u03b3, we utilized a 5-fold cross-validation scheme combined with a grid search strategy. This approach involved dividing the training dataset into five subsets, training the model on four subsets, and validating it on the remaining subset. This process was repeated five times, each time using a different subset as the validation set. The grid search strategy systematically explored a range of values for C and \u03b3, specifically from 2^0 to 2^10, to find the parameter pair that yielded the highest accuracy on the left-out training samples.\n\nAdditionally, to enhance the statistical significance of our results, we performed five realizations with different 5-fold partitions. This ensured that our findings were not dependent on a particular split of the data and provided a more reliable estimate of the model's performance. By averaging the accuracies across these multiple realizations, we obtained a more robust and generalizable measure of our classifier's effectiveness.\n\nThese techniques collectively helped in preventing overfitting and ensured that our SVM classifier was well-generalized to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are thoroughly detailed within the publication. Specifically, we employed a Support Vector Machine (SVM) classifier with an exponential string kernel based on Weighted Longest Common Subsequence (WLD). The key parameters, including the regularization constant \\( C \\) and the kernel width \\( \\gamma \\), were optimized using a 5-fold cross-validation scheme combined with a grid search strategy. The values for \\( C \\) and \\( \\gamma \\) were systematically explored within the range of \\( 2^0 \\) to \\( 2^{10} \\), resulting in 121 parameter pairs being evaluated. The optimal parameter pairs were selected based on the highest accuracy achieved on the left-out training samples. To enhance statistical significance, five realizations with different 5-fold partitions were conducted, and the overall accuracies were averaged.\n\nThe software used for training the SVM classifier, including the quadratic programming solver \"pr-loqo\" in C language, is available at http://www.kernel-machines.org/software. Additionally, a function for WLD was developed in C language. The entire software was executed on a computer with a P4 3.0 GHz processor and 1,024 MB of RAM, using the VC6.0 compiler.\n\nThe datasets utilized in this study, which include both true and false pre-miRNA samples from various species, are available for download from http://bioinfo.au.tsinghua.edu.cn/mirnasvm. These datasets were used to train and validate our SVM classifier, ensuring that the results are reproducible and comparable to previous methods.\n\nThe publication also includes detailed tables summarizing the identification accuracies on various datasets, providing transparency and reproducibility of the results. The datasets and software are made available under licenses that permit their use for research purposes, ensuring that other researchers can replicate and build upon our findings.",
  "model/interpretability": "The model employed in this study is based on Support Vector Machines (SVM), which is inherently a black-box model. SVM operates by finding a hyperplane that best separates the data into different classes. The decision-making process involves solving a quadratic programming problem to determine the coefficients (\u03b1i) and the support vectors, which are the training samples that lie closest to the decision boundary.\n\nThe use of an exponential string kernel based on Weighted Longest Common Subsequence (WLD) further adds to the complexity. This kernel allows the model to consider both the pre-miRNA sequences and their secondary structures simultaneously, capturing more discriminative and characteristic information. However, this also means that the model's decisions are not easily interpretable. The kernel function transforms the input data into a higher-dimensional space where a linear separator can be found, but the transformation itself is not straightforward to interpret.\n\nThe model's transparency is limited by the nature of SVM and the string kernel. While the support vectors provide some insight into which training samples are most influential in defining the decision boundary, the exact reasons why a particular input is classified in a certain way are not immediately clear. The model's decisions are based on complex interactions between the input features as transformed by the kernel, making it difficult to provide clear, intuitive examples of how the model arrives at its predictions.",
  "model/output": "The model is a classification method. Specifically, it is a Support Vector Machine (SVM) designed for binary classification problems. The SVM classifier assigns new input vectors to one of two classes using a nonlinearly discriminant function. The training process involves determining optimal algorithmic parameters through 5-fold cross-validation and grid search, using a dataset of true and false human pre-miRNAs. The model's performance is evaluated on various test sets, including those from human, animal, plant, virus, and artificially false human pre-miRNAs. The results indicate that the method statistically outperforms previous methods on several datasets, particularly for animal species. However, caution is advised when applying the model to predict virus pre-miRNAs due to differences in sequence length and biological kingdom. The model considers both pre-miRNA sequences and their secondary structures to construct an exponential string kernel for SVM, aiming to improve identification performance.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The software developed for this study includes a quadratic programming solver named \"pr-loqo\" implemented in C language, which was used to train the SVM classifier. Additionally, a function for WLD was developed in C language. The entire software was executed on a computer with a P4 3.0 GHz processor and 1,024 MB of RAM, using the VC6.0 compiler.\n\nThe source code for the software is not explicitly mentioned as being publicly available. Therefore, it is not clear whether the source code has been released to the public. Similarly, there is no information provided about the availability of an executable, web server, virtual machine, or container instance to run the algorithm. Thus, it is not possible to specify where or how the software can be accessed or under what license it might be distributed.",
  "evaluation/method": "The method was evaluated using a combination of cross-validation and independent test sets. Initially, the optimal algorithmic parameters were determined through a 5-fold cross-validation scheme combined with a grid search. This process was repeated five times with different 5-fold partitions to enhance statistical significance. The training set consisted of 331 samples, including 163 positive samples from true human pre-miRNAs and 168 negative samples from artificially false human pre-miRNAs.\n\nAfter parameter optimization, the method was tested on 16 independent test sets, which included samples from various species: 3 human, 8 animal, 2 plant, 1 virus, and 2 artificially false human pre-miRNAs. The performance of the method was compared with that of Xue\u2019s method. The results showed that our method statistically outperformed Xue\u2019s method on 11 of these sets, including 3 human, 7 animal, and 1 false human pre-miRNA sets. Notably, our method demonstrated a satisfactory accuracy of 92.66% in identifying pre-miRNAs with multiple loops, which were excluded in previous studies. This indicates that our approach can effectively handle complex secondary structures. The evaluation process involved multiple realizations and different partitions to ensure the reliability and robustness of the results.",
  "evaluation/measure": "In our evaluation, we primarily report the identification accuracy as the key performance metric. This metric is presented as a percentage and reflects the proportion of correctly identified true and false pre-miRNAs out of the total number of samples in each test set. We provide the accuracy for multiple realizations with different 5-fold partitions to enhance the statistical significance of our results.\n\nAdditionally, we compare our method's performance against another existing method, specifically highlighting the number of test sets on which our approach outperforms the comparison method. This comparative analysis is crucial for demonstrating the effectiveness and reliability of our approach.\n\nThe reported metrics are representative of the literature in the field of pre-miRNA identification. Accuracy is a standard metric used to evaluate the performance of classification models, and comparing against established methods ensures that our results are benchmarked against existing standards. This approach allows for a clear assessment of our method's strengths and areas for improvement.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison with publicly available methods, specifically focusing on the work by Xue et al. We utilized the same datasets as those used in Xue et al. to ensure a fair and direct comparison. These datasets included a variety of species, encompassing humans, animals, plants, and viruses, providing a broad spectrum for assessing the performance of our method.\n\nOur approach involved training an SVM classifier using 331 true and false human pre-miRNAs. We determined the optimal algorithmic parameters through a combination of 5-fold cross-validation and a grid search scheme. This process was repeated with five different 5-fold partitions to enhance the statistical significance of our results.\n\nThe comparison revealed that our method outperformed Xue's method on 11 out of 16 independent test sets. These sets included human, animal, and artificially false human pre-miRNAs. Notably, our method demonstrated a satisfactory accuracy of 92.66% in identifying pre-miRNAs with multiple loop secondary structures, which were not considered in previous studies.\n\nHowever, our method showed lower accuracy for plant pre-miRNAs and was not suitable for detecting virus pre-miRNAs. This discrepancy could be attributed to the distinct characteristics between virus and human pre-miRNAs, as well as the variable lengths of plant pre-miRNAs.\n\nIn summary, our method, which combines SVM with an exponential string kernel based on weighted Levenshtein distance, effectively identifies true and false pre-miRNAs by leveraging both pre-miRNA sequences and their secondary structures. This approach provides a robust framework for pre-miRNA identification across various species, with notable improvements over existing methods.",
  "evaluation/confidence": "To ensure the reliability of our results, we employed several statistical techniques. We used 5-fold cross-validation combined with a grid search strategy to determine the optimal algorithmic parameters. This process was repeated five times with different 5-fold partitions to enhance the statistical significance of our findings. By averaging the accuracies across these realizations, we obtained more robust and reliable performance metrics.\n\nOur method was evaluated on 16 independent test sets, and we compared our results with those from a previous study. The statistical significance of our method's superiority was demonstrated by outperforming the baseline method on 11 out of these 16 sets. This includes datasets from human, animal, and artificially false human pre-miRNAs, indicating the broad applicability of our approach.\n\nAdditionally, we achieved a satisfactory accuracy of 92.66% in identifying pre-miRNAs with multiple loop secondary structures, which were not considered in previous studies. This further validates the effectiveness of our method in handling complex structures.\n\nThe use of multiple realizations with different partitions ensures that our results are not dependent on a specific data split, providing a more generalizable assessment of our method's performance. The consistent outperforming of the baseline method across various datasets and species underscores the statistical significance and reliability of our approach.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The datasets employed for training and testing our SVM classifier, including the pre-miRNA sequences and their secondary structures, were obtained from specific sources such as the miRNA registry database (release 5.0, Sep. 2004). These datasets can be accessed through the provided link: http://bioinfo.au.tsinghua.edu.cn/mirnasvm. However, the specific evaluation files generated during our experiments, such as the results of the 5-fold cross-validation and the identification accuracies on various test sets, are not publicly released.\n\nThe datasets used in our study include true miRNAs and pre-miRNAs from 12 species, as well as artificially generated false pre-miRNAs. These datasets were used to train and test our SVM classifier, which was designed to distinguish between true and false pre-miRNAs. The performance of our method was evaluated using various metrics, including identification accuracies on independent test sets from different species.\n\nWhile the raw evaluation files are not available, the methods and datasets used in our study are described in detail in the publication. This includes information on the preprocessing of datasets, the construction of the exponential string kernel based on WLD, and the training and evaluation of the SVM classifier. Researchers interested in replicating or building upon our work can refer to these descriptions and access the relevant datasets through the provided link."
}