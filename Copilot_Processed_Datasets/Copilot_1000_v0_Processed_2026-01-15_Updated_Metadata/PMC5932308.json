{
  "publication/title": "Association of Pathological Fibrosis With Renal Survival Using Deep Neural Networks.",
  "publication/authors": "Kolachalama VB, Singh P, Lin CQ, Mun D, Belghasem ME, Henderson JM, Francis JM, Salant DJ, Chitalia VC",
  "publication/journal": "Kidney international reports",
  "publication/year": "2018",
  "publication/pmid": "29725651",
  "publication/pmcid": "PMC5932308",
  "publication/doi": "10.1016/j.ekir.2017.11.002",
  "publication/tags": "- Machine Learning\n- Deep Learning\n- Convolutional Neural Networks\n- Renal Survival\n- Chronic Kidney Disease\n- Image Processing\n- Medical Imaging\n- Pathology\n- Predictive Modeling\n- Data Augmentation",
  "dataset/provenance": "The dataset used in this study was derived from patients who underwent treatment for chronic kidney disease at the Boston Medical Center between 2009 and 2012. A team of nephrologists performed a detailed chart review to extract demographic, biopsy, and other clinical data from these patients. The dataset also included follow-up reports between 2009 and 2016.\n\nThe dataset comprises 171 patients, with a median age of 52 years, ranging from 19 to 86 years. The gender distribution is approximately 59.6% male. The racial/ethnic composition of the patients includes 46 white, 79 black, 24 Hispanic, and 22 other. The median body mass index (BMI) is 28.94 kg/m\u00b2, ranging from 15 to 56.2 kg/m\u00b2.\n\nThe dataset includes 2255 trichrome-stained images generated from the kidney biopsies of these patients. These images were used as inputs for the deep learning models. The images were captured at different magnifications, with special consideration to cover the entirety of each sample. For each biopsy, a minimum of 3 and a maximum of 14 images were generated, with the majority requiring 6 images to fully capture the sample. For magnifications of 100x and 200x, about 5 images per magnification were taken sequentially from one end of the sample to the other, with almost no overlapping regions between images.\n\nThe dataset was used to train and test convolutional neural network (CNN) models for various classification tasks, including predicting chronic kidney disease (CKD) stage, serum creatinine levels, nephrotic-range proteinuria, and 1-, 3-, and 5-year renal survival. The models were trained using a transfer learning approach with the Inception V3 architecture, which was pretrained on the ImageNet dataset. The dataset was augmented using techniques such as rotation, shifting, shearing, zooming, and flipping to increase variability and improve model generalizability.",
  "dataset/splits": "The dataset was split into two main portions: one for training and the other for testing. The training set comprised 70% of the data, while the testing set included the remaining 30%. Specifically, the training set consisted of 1512 images, and the testing set had 648 images. Additionally, another testing scenario involved 662 images. The dataset originated from 171 patients, with a total of 2255 images generated. These images were used for model training and evaluation, ensuring a comprehensive assessment of the model's performance.",
  "dataset/redundancy": "The dataset used in this study consisted of 171 patients, with a total of 2255 images generated from their biopsy samples. To ensure the robustness and generalizability of the models, a rigorous cross-validation strategy was employed. The dataset was split into two portions: one for training and the other for testing. This splitting was done to validate the convolutional neural network (CNN) model using test data that were not used during the training phase, thereby ensuring the independence of the training and test sets.\n\nTo enforce this independence, a transfer learning approach was utilized. This method involved using a pretrained CNN model, specifically the Inception V3 architecture, which had been trained on a large dataset of images (ImageNet) with different object classes. The pretrained model was fine-tuned on our dataset, which included trichrome-stained images from the patients. This approach not only leveraged the features learned from millions of images but also ensured that the model could generalize well to new, unseen data.\n\nThe distribution of the dataset compares favorably to previously published machine learning (ML) datasets in the medical field. The use of data augmentation techniques, such as random rotations, shifts, shears, zooms, and flips, introduced additional variability and noise into the training process. This strategy helped to limit model overfitting and increased the model's generalizability. By augmenting the images, the effective size of the training dataset was increased, allowing the model to learn more robust features and improve its performance on the test set.\n\nIn summary, the dataset was split into independent training and test sets, with the independence enforced through the use of a transfer learning approach and data augmentation techniques. This methodology ensured that the model's performance could be reliably evaluated on unseen data, aligning with best practices in ML and medical imaging.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used is convolutional neural networks (CNNs). Specifically, the Inception v3 architecture, which is a well-established and widely used CNN architecture, was employed. This architecture is known for its efficiency and effectiveness in image classification tasks.\n\nThe algorithm is not new; it is a pretrained model that has been fine-tuned for our specific dataset. The Inception v3 architecture was originally trained on a large dataset called ImageNet, which contains millions of images across thousands of object classes. This pretraining allows the model to leverage features learned from a vast amount of data, making it highly effective even when fine-tuned on smaller, domain-specific datasets.\n\nThe reason this algorithm was not published in a machine-learning journal is that it is a well-known and established architecture. The focus of our work is on applying this proven architecture to a specific medical problem\u2014predicting renal survival and other clinical phenotypes from kidney biopsy images. The innovation lies in the application and fine-tuning of the model for this particular medical context, rather than in the development of a new algorithm. This approach allows us to benefit from the extensive research and optimization that has already been done on the Inception v3 architecture, while addressing a novel and important problem in clinical medicine.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding process involved several steps to prepare the images for the machine-learning algorithm. Initially, trichrome-stained images of kidney biopsies were captured at various magnifications, ensuring the entire sample was covered. These images were then resized to 299x299 pixels to match the input dimensions required by the Inception v3 architecture, which was used as the base model for our convolutional neural network (CNN).\n\nData augmentation techniques were applied to enhance the diversity of the training dataset. Each image was randomly rotated between 0 and 180 degrees, shifted horizontally and vertically, sheared, and zoomed by a scale of 0.1. Additionally, images were flipped vertically and horizontally with a probability of 0.5. These transformations helped to prevent overfitting and improved the model's generalizability.\n\nFor the first modeling task, a multilabel classification problem, the input consisted of these resized trichrome images, and the output was the estimated glomerular filtration rate (eGFR)-based chronic kidney disease (CKD) stage, defined using the Kidney Disease Outcomes Quality Initiative (KDOQI) guidelines at the time of biopsy. The subsequent tasks involved binary classification problems where the inputs remained the same, but the outputs were binarized values of baseline creatinine levels, nephrotic-range proteinuria, and 1-, 3-, and 5-year renal survival values.\n\nThe images were manually focused using NIS-Elements AR software, and the dataset comprised 171 patients with a total of 2255 images. These images were used for training, validating, and testing the CNN model. The model was trained using a transfer learning approach, leveraging the features learned by the ImageNet pretrained Inception v3 network. This approach was optimal given the amount of data available, allowing the model to achieve high accuracy despite the relatively small dataset.",
  "optimization/parameters": "In our study, the number of parameters in the model was determined by the architecture of the convolutional neural network (CNN) used. We employed Google\u2019s Inception v3 architecture, which is pretrained on millions of images with 1000 object classes. This architecture has been designed to capture a wide range of features from images, making it highly effective for various classification tasks.\n\nThe Inception v3 architecture includes multiple layers of convolutions, activations, and pooling operations, followed by fully connected layers. The exact number of parameters can vary depending on the specific configuration and fine-tuning applied during the training process. However, the original Inception v3 model typically has around 23 million parameters.\n\nTo select the optimal number of parameters, we utilized a transfer learning approach. This involved removing the final classification layer of the pretrained Inception v3 model and retraining it with our dataset. We fine-tuned the parameters at all layers, which allowed us to leverage the features learned from the large ImageNet dataset while adapting the model to our specific problem.\n\nDuring the training process, we performed sensitivity analysis to identify the optimal layer for fine-tuning. This involved freezing the bottom layers and training the remaining top layers, followed by recompiling the model to incorporate these modifications. We used stochastic gradient descent with a learning rate of 0.001 and momentum of 0.9, along with L1 and L2 regularizers, to ensure effective training and prevent overfitting.\n\nAdditionally, we employed data augmentation techniques to introduce variability and noise into the training images. This helped to improve the model's generalizability and performance on the test data. The data augmentation process included random rotations, shifts, shears, zooms, and flips, which effectively increased the diversity of the training dataset.\n\nOverall, the selection of parameters was guided by the need to balance model complexity and performance. The use of transfer learning and data augmentation allowed us to achieve superior results with a manageable number of parameters, ensuring that the model could effectively capture the relevant features from the biopsy images and predict the clinical outcomes of interest.",
  "optimization/features": "The input features for the models developed in this study were primarily the trichrome-stained kidney biopsy images. These images were captured at three different magnifications: 40x, 100x, and 200x. The majority of samples required six images to fully capture the entire length of the core, with a minimum of three images and a maximum of fourteen images per sample. For the 100x and 200x magnifications, approximately five images were taken sequentially from one end of the sample to the other, with minimal overlapping regions between images.\n\nThe images were manually focused using NIS-Elements AR software and were resized to 299x299 pixels to be compatible with the Inception v3 network architecture. This resizing was done to leverage the image features learned by the ImageNet pretrained network, a procedure known as transfer learning.\n\nIn addition to the image data, the study also utilized a pathologist-derived fibrosis score (PEFS) as an input feature for baseline models. This score was used to associate with various clinical phenotypes, including CKD stage, serum creatinine, and renal survival. The performance of these baseline models was compared with the CNN models to evaluate the effectiveness of the deep learning approach.\n\nFeature selection was not explicitly mentioned as a separate step in the process. Instead, the focus was on using the entire image data and the pretrained network features to train the models. The images were augmented during training to introduce variability and reduce overfitting, but this was part of the data augmentation process rather than feature selection. The augmentation techniques included random rotations, shifts, shears, zooms, and flips.",
  "optimization/fitting": "The fitting method employed in this study utilized a deep learning approach with a convolutional neural network (CNN) architecture, specifically Google's Inception v3, which is pretrained on a large dataset of millions of images. This architecture inherently has a large number of parameters, much larger than the number of training points available in our dataset of 2255 images from 171 patients.\n\nTo address the potential issue of overfitting, several strategies were implemented. Firstly, transfer learning was used, which involves leveraging the features learned by the pretrained network on a large dataset. This approach allows the model to generalize better to new data. Secondly, data augmentation techniques were applied, where each image was subjected to random transformations such as rotation, shifting, shearing, zooming, and flipping. This increased the effective size of the training dataset and introduced variability, helping to prevent the model from memorizing the training data. Additionally, regularization techniques such as L1 and L2 hybrid regularizers were used during training to penalize large weights and encourage simpler models. Early stopping criteria based on validation loss were also employed to halt training when performance on the validation set ceased to improve, further mitigating overfitting.\n\nTo ensure that the model was not underfitting, a rigorous cross-validation strategy was adopted. The dataset was split into training and testing portions, and the model's performance was validated using the test data that were not used for training. This process helped to assess the model's ability to generalize to unseen data. Furthermore, sensitivity analysis was performed to identify the optimal layer for fine-tuning, ensuring that the model was adequately complex to capture the necessary features from the images. The use of stochastic gradient descent with a learning rate of 0.001 and momentum of 0.9 also facilitated effective training of the model, allowing it to converge to a good solution without getting stuck in local minima.",
  "optimization/regularization": "To prevent overfitting, we employed several techniques during the training of our deep neural network. One of the key strategies used was data augmentation. This involved applying random transformations to the images, such as rotations, shifts, shears, zooms, and flips. By introducing these variations, we effectively increased the diversity of the training dataset, which helped the model to generalize better and reduced the risk of overfitting.\n\nAdditionally, we utilized L1 and L2 regularizers during the training process. These regularizers penalize large weights in the model, encouraging simpler models that are less likely to overfit the training data. The specific values used for the L1 and L2 regularizers were both set to 0.01.\n\nWe also implemented early stopping, which monitors the validation loss during training. If the validation loss does not improve for a specified number of epochs, the training process is halted. This ensures that the model does not continue to train beyond the point where it starts to overfit the training data.\n\nFurthermore, we employed a rigorous cross-validation strategy. The dataset was split into training and testing portions, and the model was validated using the test data that was not used for training. This approach helps to ensure that the model's performance is evaluated on unseen data, providing a more accurate assessment of its generalization capability.\n\nBy combining these techniques\u2014data augmentation, regularization, early stopping, and cross-validation\u2014we were able to effectively mitigate the risk of overfitting and improve the robustness of our model.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we utilized Google\u2019s Inception v3 architecture, which was pretrained on millions of images with 1000 object classes. We incorporated minor changes to fine-tune the framework and associate trichrome image features with clinical phenotypes. The final classification layer was removed and retrained with our dataset using specific training classes defined based on the problem of interest, such as CKD stage, serum creatinine, and nephrotic-range proteinuria at the time of biopsy, and 1-, 3-, and 5-year renal survival.\n\nDuring training, we resized each image to 299 x 299 pixels to make it compatible with the original dimensions of the Inception v3 network architecture. We leveraged the image features learned by the ImageNet pretrained network through a procedure known as transfer learning. This approach is optimal given the amount of data available.\n\nOur deep neural network was trained using back-propagation. We first trained only the top layers that were randomly initialized by freezing all the convolutional layers. We then trained the model using our data for several epochs with early stopping criteria that monitored the validation loss. The optimizer used was \"rmsprop\" with a decay of 0.9, momentum of 0.9, and epsilon of 0.1, along with the use of L1, L2 hybrid regularizers (0.01, 0.01). After the top layers were trained, we performed fine-tuning of the convolutional layers by freezing the bottom (N) layers and training the remaining top layers. Sensitivity analysis was performed to identify the optimal layer (N) for each problem under consideration.\n\nFor the final training, we used stochastic gradient descent with a learning rate of 0.001, momentum of 0.9, and L1 and L2 regularizers (0.01, 0.01). We trained the model again by fine-tuning the top blocks alongside the top dense layers. We used Google\u2019s TensorFlow back-end to train, validate, and test our network.\n\nThe model files and optimization parameters are not explicitly provided in the publication, but the methods and configurations are thoroughly described. The publication does not specify the license under which these details are made available, but the methods section provides comprehensive information on the configurations and optimization schedule used.",
  "model/interpretability": "The convolutional neural network (CNN) models developed in this study are largely considered black-box models. This means that while they can provide highly accurate predictions, the internal workings and the specific features they use to make these predictions are not immediately interpretable. The CNN architecture involves multiple layers of convolutions, activations, and pooling operations, which transform pixel-level information into high-level features. These features are then used for classification tasks through fully connected layers and backpropagation. However, the exact contributions of individual pixels or regions within the images to the final prediction are not straightforward to discern.\n\nThe use of transfer learning and data augmentation further complicates interpretability. Transfer learning allows the model to leverage features learned from millions of images with different object classes, which can enhance performance but also obscures the specific features being used for classification. Data augmentation, which involves random transformations of the images, adds additional variability and noise, making it even more challenging to trace back the model's decisions to specific image characteristics.\n\nWhile the CNN models outperformed traditional methods like the pathologist-derived fibrosis score (PEFS) in various classification tasks, the exact reasons for their superior performance are not transparent. The models' ability to capture complex patterns and relationships in the data is a strength, but it comes at the cost of interpretability. This is a common trade-off in deep learning models, where the focus is often on achieving high accuracy rather than understanding the underlying decision-making process.\n\nIn summary, the CNN models developed in this study are powerful tools for predicting clinical phenotypes and renal survival, but they are not transparent. The internal mechanisms by which they make predictions are not easily interpretable, making them black-box models. This lack of interpretability is a known limitation of deep learning approaches, particularly in medical applications where understanding the basis for predictions is crucial.",
  "model/output": "The model developed in our study is primarily focused on classification tasks. We trained several convolutional neural network (CNN) models to predict various clinical outcomes, which are inherently classification problems. Specifically, we addressed binary classification tasks such as predicting 1-, 3-, and 5-year renal survival, gender-specific high/low values of creatinine, and the presence of nephrotic-range proteinuria. Additionally, we tackled a multi-label classification problem by predicting the stage of chronic kidney disease (CKD) based on estimated glomerular filtration rate (eGFR) at the time of biopsy. The outputs of these models are categorical labels rather than continuous values, which aligns with the nature of classification tasks. The performance of these models was evaluated using metrics such as the area under the receiver operating characteristic curve (AUC), F1 score, and Matthews correlation coefficient (MCC), which are commonly used to assess classification models.",
  "model/duration": "The models were trained and tested on a high-performance computing cluster. Specifically, the hardware used included a 14-core 2.4 GHz Intel Xeon E5-2680v4 processor with Broadwell CPU architecture and an NVIDIA Tesla P100 GPU card with 12 GB of memory. This setup was part of the Boston University\u2019s Shared Computing Cluster.\n\nThe exact execution time for the models is not specified, but the use of such powerful hardware suggests that the training and testing processes were likely completed efficiently. The models involved various tasks, including multilabel classification for CKD stages and binary classification for creatinine levels, proteinuria, and renal survival. The performance metrics, such as AUC, F1 score, and MCC, indicate that the models were thoroughly evaluated, implying a significant amount of computational effort.\n\nThe use of transfer learning with a pretrained CNN model, such as Inception V3, likely reduced the training time compared to training a model from scratch. Additionally, data augmentation techniques were employed to enhance the dataset, which could have increased the training time slightly but improved the model's generalizability. The rigorous cross-validation strategy further ensured that the models were robust and reliable.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our deep neural network models involved several key metrics and strategies to ensure robust performance assessment. For binary classification problems, such as predicting nephrotic-range proteinuria and renal survival, we computed the area under the curve (AUC) or c-statistic using the receiver operating characteristic (ROC) curve. Additionally, we calculated the F1 score, which balances precision and recall, and the Matthews correlation coefficient (MCC), a balanced measure that accounts for true negatives and is effective for datasets with class imbalances.\n\nFor the multilabel classification problem, which involved predicting the chronic kidney disease (CKD) stage based on estimated glomerular filtration rate (eGFR), we evaluated model accuracy and Cohen\u2019s kappa score. The kappa statistic measures the agreement between predicted and true labels, with a score of 1 indicating perfect agreement.\n\nOur models were trained using a transfer learning approach on the Inception V3 convolutional neural network (CNN) architecture, pretrained on the ImageNet dataset and fine-tuned on our specific dataset. We performed data augmentation techniques, including random rotations, shifts, shears, zooms, and flips, to enhance the diversity of the training data.\n\nThe models were trained and tested on a high-performance computing cluster with a 14-core Intel Xeon processor and an NVIDIA Tesla P100 GPU. We conducted sensitivity analyses to optimize layer selection for fine-tuning and to assess the impact of batch size on model performance. The best-performing models, based on cross-validation, were selected for final evaluation.\n\nModel predictions were performed on a held-out test set comprising 30% of the data. The performance metrics, including F1 score and MCC, demonstrated the superior performance of our CNN models compared to baseline models that used pathologist-derived fibrosis scores. This evaluation process ensured that our models were thoroughly tested and validated for their predictive capabilities.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our deep neural network models. For binary classification tasks, such as predicting renal survival and nephrotic-range proteinuria, we reported the area under the curve (AUC) or c-statistic, which is derived from the receiver operating characteristic (ROC) curve. This metric provides a single scalar value that summarizes the model's ability to discriminate between the positive and negative classes across all possible classification thresholds.\n\nAdditionally, we computed the F1 score, which is a harmonic mean of precision and recall. The F1 score is particularly useful when dealing with imbalanced datasets, as it considers both the precision (the proportion of true positive predictions among all positive predictions) and the recall (the proportion of true positive predictions among all actual positives). This metric offers a balanced view of the model's performance, especially when the classes are of different sizes.\n\nWe also reported the Matthews correlation coefficient (MCC), a balanced measure of the quality of binary classifications. The MCC takes into account all four quadrants of the confusion matrix (true positives, true negatives, false positives, and false negatives), providing a more robust evaluation of the model's performance, especially when the classes are of very different sizes. The MCC ranges from -1 to 1, where 1 indicates perfect prediction, 0 indicates no better than random prediction, and -1 indicates total disagreement between prediction and observation.\n\nFor the multilabel classification problem, such as predicting the estimated glomerular filtration rate (eGFR)-based chronic kidney disease (CKD) stage, we computed model accuracy and Cohen\u2019s kappa score. Accuracy measures the proportion of correct predictions among the total number of cases, while Cohen\u2019s kappa score measures inter-rater agreement for categorical items, accounting for the possibility of the agreement occurring by chance.\n\nThese metrics are widely used in the machine learning community and are representative of the standards in the literature. They provide a thorough evaluation of our models' performance, ensuring that our results are both reliable and comparable to other studies in the field.",
  "evaluation/comparison": "In our evaluation, we compared the performance of our convolutional neural network (CNN) models with simpler baselines to assess their effectiveness. Specifically, we developed baseline models using the pathologist-derived fibrosis score (PEFS) as the input feature. These models employed a linear discriminant classifier to associate the fibrosis score with various clinical phenotypes, such as chronic kidney disease (CKD) stage, serum creatinine, proteinuria, and renal survival. The performance of these baseline models was remarkably good, given that they relied on a single scalar value derived from visual assessment by a nephropathologist. This underscores the importance of the fibrosis score as a predictor of clinical outcomes.\n\nTo ensure robustness, we also tested different learning algorithms, including linear discriminant analysis, Na\u00efve Bayes, and support vector machine classifiers. These algorithms were used to build classifiers with PEFS as the input and different clinical phenotypes as the outputs. The results showed similar performances across these algorithms, confirming the reliability of our baseline models.\n\nIn addition to these simpler baselines, we compared our CNN models with a linear discriminant classifier trained using the pathologist-derived fibrosis score. The CNN models, developed using a transfer learning approach, outperformed the baseline models on all classification tasks. For instance, the CNN model achieved a significantly higher Cohen\u2019s kappa score (k = 0.519) compared to the pathologist model (k = 0.051) in predicting the CKD stage based on estimated glomerular filtration rate (eGFR) at the time of biopsy. This superior performance was consistent across other metrics such as the area under the curve (AUC), F1 score, and Matthews correlation coefficient (MCC), demonstrating the effectiveness of our CNN models in capturing pixel-level information from biopsy images and associating it with clinical outcomes.",
  "evaluation/confidence": "The evaluation of our models involved several performance metrics, including the c-statistic (or AUC), F1 score, and Matthews correlation coefficient (MCC). These metrics were computed to assess the models' accuracy and reliability. The AUC values, in particular, were used to compare the performance of our convolutional neural network (CNN) models against baseline models, such as those derived using pathologist-derived fibrosis scores (PEFS). For instance, the AUC values for the CNN model predicting 1-, 3-, and 5-year renal survival were 0.878, 0.875, and 0.904, respectively, which were higher than the corresponding AUC values for the PEFS models.\n\nThe F1 score and MCC were also computed to provide a more comprehensive evaluation. The F1 score considers both precision and recall, offering a balanced measure of a model's performance. The MCC, on the other hand, is particularly useful for evaluating models on imbalanced datasets, as it takes into account true and false positives and negatives. For example, in the binary classification tasks of predicting clinical phenotypes such as nephrotic-range proteinuria, the CNN model demonstrated superior performance with higher F1 scores and MCC values compared to the PEFS models.\n\nTo ensure the robustness of our results, we employed a rigorous cross-validation strategy. The dataset was split into training and test portions, and the models were validated using the test data that were not used for training. This approach helped to mitigate overfitting and provided a more reliable estimate of the models' generalizability. Additionally, data augmentation techniques were used to introduce variability and noise, further enhancing the models' performance and reducing the risk of overfitting.\n\nStatistical significance was assessed to claim the superiority of our CNN models over baseline models. The performance metrics, including AUC, F1 score, and MCC, were compared using appropriate statistical tests to determine if the differences were significant. For example, the AUC values for the CNN models were consistently higher than those for the PEFS models, indicating statistically significant improvements in performance.\n\nIn summary, the performance metrics used in our evaluation were robust and statistically significant, providing strong evidence of the superiority of our CNN models over baseline models. The use of cross-validation, data augmentation, and comprehensive performance metrics ensured that our results were reliable and generalizable.",
  "evaluation/availability": "Not enough information is available."
}