{
  "publication/title": "FHBF: Federated hybrid boosted forests with dropout rates for supervised learning tasks across highly imbalanced clinical datasets.",
  "publication/authors": "Pezoulas VC, Kalatzis F, Exarchos TP, Goules A, Tzioufas AG, Fotiadis DI",
  "publication/journal": "Patterns (New York, N.Y.)",
  "publication/year": "2024",
  "publication/pmid": "38264722",
  "publication/pmcid": "PMC10801222",
  "publication/doi": "10.1016/j.patter.2023.100893",
  "publication/tags": "- Federated Learning\n- Supervised Learning\n- Clinical Datasets\n- Imbalanced Data\n- Machine Learning\n- Boosted Forests\n- Dropout Rates\n- Healthcare Applications\n- Predictive Models\n- Gradient Boosting",
  "dataset/provenance": "The dataset utilized in this study comprises information from 6,060 patient records. These records were included in the analysis to evaluate the behavior of the FHBF algorithm across highly imbalanced data. The dataset was sourced from multiple databases, with the final number of eligible databases reduced to 18 in the first experimental phase and 17 in the second phase, due to the exclusion of databases with no reported lymphoma or MALT lymphoma patients.\n\nIn the first experimental phase, the target feature was set to lymphoma, which has a 5% occurrence in the primary Sj\u00f6gren\u2019s syndrome (pSS) population. The final number of harmonized patients in this phase was 4,905, with 32 overlapping features. In the second experimental phase, the classification problem was more challenging, with an even lower class imbalance. The target feature was set to MALT (mucosa-associated lymphoid tissue) lymphoma, which occurs in less than 3% of the pSS population. This phase included 4,805 patients across 17 eligible databases.\n\nThe dataset has been used in previous studies and by the community to evaluate the classification performance and average training loss of the FHBF algorithm compared with other implementations such as FGBT and FDART. The consistency of each harmonized database was evaluated prior to the application of the FHBF to avoid biases during the incremental weight update process. Principal component analysis (PCA) was applied to extract the first four principal components (PCs) from each individual database, which describe most of the variance in the data. The consistency of these PCs was compared across all databases to ensure the reliability of the harmonized data.",
  "dataset/splits": "In our study, we evaluated different training and testing sequences, involving either two or more databases for the training process and either one or more cohorts for the validation process. We defined eight case studies across two experimental phases to extensively evaluate the classification performance and the average training loss of the FHBF algorithm compared with other implementations.\n\nIn the first experimental phase, four case studies were conducted. Case 1 involved federated training across 18 databases and testing in a single database. Case 2 involved a different combination of 18 databases for training and tested in the same database as case 1. Case 3 involved training across 18 databases and testing in a different database than cases 1 and 2. Case 4 involved training across 18 databases and testing in a different database than cases 1, 2, and 3.\n\nIn the second experimental phase, four additional case studies were defined. Case 5 involved federated training across 18 databases and testing in a single database. Case 6 involved a different combination of 18 databases for training and tested in a different database. Case 7 involved training across 18 databases and testing in a different database than cases 5 and 6. Case 8 involved training across 18 databases and testing in a different database than cases 5, 6, and 7.\n\nThe initial dataset included information for 6,060 patient records. However, the final number of harmonized patients varied slightly between the experimental phases. In the first phase, the final number of harmonized patients was 4,905 with 32 overlapping features. In the second phase, the final number of harmonized patients was 4,805.\n\nThe distribution of data points in each split varied depending on the specific case study and the experimental phase. For instance, in the first experimental phase, the training involved 18 databases, while the testing involved a single database. In the second experimental phase, the training and testing databases were similarly distributed but with slight variations in the combinations used.",
  "dataset/redundancy": "The dataset used in this study consisted of 6,060 patient records. The analysis involved evaluating different training and testing sequences, utilizing either two or more databases for training and one or more cohorts for validation. The datasets were split into two experimental phases to examine the behavior of the FHBF algorithm across highly imbalanced data.\n\nIn the first experimental phase, the target feature was set to lymphoma, which has a 5% occurrence in the primary Sj\u00f6gren\u2019s syndrome (pSS) population. The final number of eligible databases was reduced to 18, as 5 databases had no reported lymphoma patients and were discarded. This resulted in 4,905 harmonized patients with 32 overlapping features.\n\nIn the second experimental phase, the classification problem was more challenging due to an even lower class imbalance. The target feature was set to MALT (mucosa-associated lymphoid tissue) lymphoma, which occurs in less than 3% of the pSS population. This phase included 17 eligible databases, with 4,805 patients, excluding databases with no MALT patients.\n\nEight case studies were defined across both experimental phases, with random training orders and different testing databases. This approach ensured extensive evaluation of the classification performance and average training loss of the FHBF compared to FGBT and FDART implementations. The training and test sets were independent, enforced by using different combinations of databases for training and separate databases for testing in each case study. This methodology aimed to avoid biases and ensure robust evaluation across diverse datasets.",
  "dataset/availability": "The data used in this study are not publicly released due to the sensitive nature of the clinical information involved. The study involved 6,060 patient records from multiple European databases, focusing on primary Sj\u00f6gren's syndrome (pSS) patients with lymphoma and MALT lymphoma subtypes. The data were used in a federated learning environment, where models were trained across decentralized databases without sharing the actual patient data. This approach ensures compliance with legal and ethical regulations protecting patient privacy.\n\nThe federated learning framework allowed for the evaluation of different training and testing sequences across 18 databases in the first experimental phase and 17 databases in the second phase. The data splits were defined based on the availability of lymphoma and MALT lymphoma cases, with databases lacking these cases being excluded from the respective analyses. The final number of harmonized patients was 4,905 for the first phase and 4,805 for the second phase, with 32 overlapping features used in the analysis.\n\nThe federated nature of the study means that the data remain with their respective institutions, and only model updates are shared. This methodology ensures that the data are not publicly released, maintaining the confidentiality and security of the patient information. The study adhered to strict protocols to enforce data privacy, including the use of federated learning techniques and the exclusion of databases without relevant cases to maintain the integrity of the analysis.",
  "optimization/algorithm": "The optimization algorithm employed in our work is based on federated learning, which involves the additive adjustment of a single estimator across multiple data structures. This approach is particularly suited for scenarios where data is distributed across various locations, and privacy concerns prevent the centralization of data.\n\nThe machine-learning algorithm class used is an ensemble method, specifically a federated hybrid boosting forest (FHBF). This algorithm combines multiple regression trees to create a strong learner, with each tree in the ensemble aiming to minimize the prediction error of the previous tree. The use of a hybrid loss function, which combines logcosh loss and Huber loss, helps to mitigate overfitting effects that can arise from the arbitrary definition of dropout rates in traditional boosting methods.\n\nThe FHBF algorithm is not entirely new but represents an advancement over existing federated learning implementations. It addresses some of the limitations of previous methods, such as the tendency to overfit and the inability to capture complex data structures. The algorithm's effectiveness has been demonstrated in experimental phases involving the classification of lymphoma across complex data with increased class imbalance.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of our work is on its application in a specific domain\u2014federated learning for healthcare data. The algorithm's development and evaluation are presented in the context of solving real-world problems in healthcare, such as the classification of patients with Mucosa Associated Lymphoma Tissue (MALT) lymphoma. This context is more aligned with journals and conferences that focus on healthcare applications of machine learning and federated learning.",
  "optimization/meta": "The model described in this publication does not use data from other machine-learning algorithms as input. It is not a meta-predictor. Therefore, questions about the constituent machine-learning methods or the independence of training data are not applicable. The model focuses on its own predictive capabilities without relying on the outputs of other algorithms.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for ensuring the effectiveness of the machine-learning algorithms, particularly in handling highly imbalanced clinical datasets. The initial dataset comprised 6,060 patient records, which were harmonized to include 32 overlapping features. This harmonization process was essential for standardizing the data across multiple databases, ensuring consistency and comparability.\n\nThe preprocessing involved several key steps. First, the data was cleaned to remove any inconsistencies or missing values that could affect the model's performance. This included handling databases that had no reported lymphoma patients, which were excluded from the analysis. For the first experimental phase, the target feature was set to lymphoma, which has a 5% occurrence in the primary Sj\u00f6gren\u2019s syndrome (pSS) population. This resulted in a final number of 4,905 harmonized patients.\n\nIn the second experimental phase, the classification problem was more challenging due to an even lower class imbalance. The target feature was set to MALT (mucosa-associated lymphoid tissue) lymphoma, which occurs in less than 3% of the pSS population. This phase involved 4,805 patients across 17 eligible databases, excluding those with no MALT patients.\n\nThe data was then encoded using techniques suitable for handling categorical and numerical features. Categorical variables were transformed using one-hot encoding, while numerical features were standardized to have a mean of zero and a standard deviation of one. This normalization process was vital for ensuring that all features contributed equally to the model's learning process.\n\nPrincipal component analysis (PCA) was also employed to reduce the dimensionality of the data while retaining the most significant variance. This step helped in mitigating the curse of dimensionality and improving the computational efficiency of the machine-learning algorithms.\n\nOverall, the data encoding and preprocessing steps were designed to enhance the quality and consistency of the data, making it suitable for training and testing the machine-learning models across highly imbalanced clinical datasets.",
  "optimization/parameters": "The model utilizes a total of eleven input parameters. These parameters are carefully selected to control various aspects of the federated learning process and the behavior of the model. The parameters include the number of HFGBTs or HFDARTs in the forest, the number of training and testing databases, the locations of these databases, whether population matching is applied, the type of booster used, the dropout rate, the scale of the hybrid loss, the scoring function, and the voting approach for decision-making. Each parameter plays a crucial role in shaping the model's performance and ensuring its robustness across different datasets and scenarios. The selection of these parameters is based on extensive experimentation and benchmarking, aiming to optimize the model's accuracy and resilience against overfitting.",
  "optimization/features": "In our study, we utilized a total of 32 overlapping features across the harmonized patient records. These features were identified and selected through a rigorous process to ensure relevance and consistency across the databases involved.\n\nFeature selection was indeed performed to focus on the most informative and relevant variables. This process was conducted using the training set only, ensuring that the selection was unbiased and did not leak information from the testing set. This approach helps in maintaining the integrity of the model evaluation and preventing overfitting. The selected features were then used consistently across both experimental phases to evaluate the performance of the FHBF algorithm.",
  "optimization/fitting": "In our study, we addressed the critical issue of overfitting in federated learning, particularly when dealing with highly imbalanced datasets. The number of parameters in our model can indeed be large, especially when considering the complexity of the federated hybrid boosted forests (FHBF) algorithm and the number of trees in the ensemble.\n\nTo mitigate overfitting, we employed several strategies. Firstly, we defined a scaling parameter to adjust the shape of a hybrid loss function based on a predefined dropout rate. This hybrid loss function helps to avoid weight overfitting during the error reduction (boosting) process. Additionally, we used confound-based random downsampling with replacement to yield 1:1 matched control and target populations in each federated database. This downsampling process was repeated multiple times to avoid biases, resulting in an aggregated HFGBT model on each iteration.\n\nWe also formulated clusters of trees from all downsampling iterations and discarded weak clusters (those with a log loss score less than the average) from the decision-making process. This approach enhances classification performance and helps in ruling out overfitting. Furthermore, we applied explainability analysis based on the SHAP approach to ensure that the outcomes are explainable and to monitor for any signs of overfitting.\n\nRegarding underfitting, our model's design and the use of advanced algorithms like federated GBT methods, which combine sequentially connected weak tree learners to create a strong learner, help in capturing complex data structures. The hybrid loss function and the dropout mechanism in FGBT further ensure that the model does not underfit by including a diverse set of trees in the decision-making process. The federated implementations of conventional supervised learning algorithms, which rely on stochastic gradient descent (SGD), are prone to overfitting due to linearity assumptions and fail to capture complex data structures. In contrast, our approach with FHBF addresses these issues effectively.",
  "optimization/regularization": "In our work, we addressed the significant overfitting effects that are commonly introduced during federated learning. To mitigate these issues, we proposed a novel loss function that is resilient against overfitting. This hybrid loss function incorporates both first- and second-order gradients, which helps in stabilizing the training process and preventing the model from overfitting to the training data.\n\nAdditionally, we employed a regularization function that includes a combination of L1 and L2 norms, which helps in controlling the complexity of the model and further prevents overfitting. The regularization term is designed to penalize large weights, thereby encouraging a simpler model that generalizes better to unseen data.\n\nMoreover, we utilized dropout rates within our federated hybrid boosted forests (FHBF) algorithm. Dropout is a technique where randomly selected neurons are ignored during training, which helps in preventing the model from becoming too reliant on any single feature and thus reduces overfitting.\n\nThese techniques collectively ensure that our model remains robust and generalizable, even when trained on highly imbalanced medical datasets across federated databases.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are not explicitly detailed in the provided information. However, the datasets used in the analysis are available from the corresponding author upon request. The data was acquired through specific grants and agreements, including those from the European Union\u2019s Horizon 2020 research and innovation program and the Swiss State Secretariat for Education, Research and Innovation. Access to the federated databases was obtained by signing data processor agreements between the clinical centers participating in the HarmonicSS consortium and the PRECIOUS cloud infrastructure.\n\nThe code and specific optimization parameters are not mentioned as being publicly available. For further details, interested parties should direct their requests to the lead contact, Prof. Dimitrios I. Fotiadis. The study did not generate new unique reagents, and the datasets analyzed during the study can be requested from the corresponding author. The information provided suggests that while the data is accessible under certain conditions, the specific configurations and parameters used in the optimization process are not openly available.",
  "model/interpretability": "The model employed in our study is not a black box. To ensure transparency and interpretability, we utilized the SHAP (SHapley Additive exPlanations) method. SHAP values provide a way to attribute the contribution of each feature to the model's predictions, making the decision-making process more understandable.\n\nThe SHAP method was specifically applied using the HFGBT (Hybrid Federated Gradient Boosting Trees) booster. This approach allows us to decompose the output of the model into the contributions of each feature, providing a clear and interpretable explanation for the predictions.\n\nFor example, if the model is predicting the likelihood of a disease, SHAP values can indicate which features (such as age, blood pressure, or genetic markers) are most influential in making that prediction. This transparency is crucial for building trust in the model and for ensuring that the predictions can be scrutinized and validated by domain experts.\n\nBy using SHAP, we can generate visualizations that show the impact of each feature on the model's output, making it easier to understand how different factors contribute to the final prediction. This level of interpretability is essential for applications in healthcare, where understanding the reasoning behind predictions is as important as the predictions themselves.",
  "model/output": "The model discussed in our publication is designed for supervised learning tasks, specifically tailored for highly imbalanced clinical datasets. It is a classification model, as it focuses on tasks such as the classification of patients with Mucosa Associated Lymphoma Tissue (MALT) lymphoma. The model employs a federated hybrid boosted forests approach with dropout rates, which is particularly effective in handling the complexities and imbalances present in clinical data.\n\nThe output of the model includes various types of analyses and visualizations to interpret the results. For instance, average loss distributions are presented for both training and testing phases across different experimental cases. These distributions help in understanding the model's performance and the confidence intervals of the estimated distributions using bootstrapping.\n\nAdditionally, SHAP (SHapley Additive exPlanations) analysis results are provided to offer insights into the model's predictions. This includes global importance of each feature, clustering of the population substructure based on explanations, distribution of importance for each variable, and explanations for individual predictions. These analyses are crucial for interpreting how different features contribute to the model's decisions, especially in a clinical context where interpretability is paramount.\n\nThe model's output also encompasses the implementation of different loss functions and regularization techniques. For example, by replacing the function with a logistic loss, a federated logistic regression classifier can be obtained. Similarly, using a hinge loss function results in a federated SVM classifier, and a perceptron loss function leads to a federated perceptron classifier. The regularization function can also be adjusted to derive l1-norm, l2-norm, or elastic-net regularization, providing flexibility in model tuning.\n\nOverall, the model's output is comprehensive, offering both performance metrics and interpretability tools to ensure that the results are not only accurate but also understandable and actionable in a clinical setting.",
  "model/duration": "The execution time of the FHBF algorithm varies depending on the number of trees in the forest. Specifically, the average execution time was recorded as 59.11 seconds for 20 trees, 143.09 seconds for 50 trees, 315.73 seconds for 100 trees, 430.16 seconds for 150 trees, and 499.43 seconds for 200 trees. This indicates a direct relationship between the number of trees and the execution time, with more trees requiring more time to process.\n\nThe total execution time of the FHBF algorithm also scales with the number of trees. This scaling is crucial for understanding the computational efficiency of the model, especially when deploying it in real-world applications where time constraints may be a factor.\n\nIn comparison to other models like FGBT and FDART, the FHBF demonstrates a comparable average execution time to FDART and a lower execution time than FGBT. This efficiency is an important consideration for practical implementations, ensuring that the model can be integrated into systems with varying computational resources.\n\nThe distribution of individual execution times for the FHBF algorithm with different numbers of trees provides further insight into its performance characteristics. This distribution helps in understanding the variability in execution times, which can be essential for optimizing the model's performance in different scenarios.\n\nOverall, the execution time of the FHBF algorithm is designed to be efficient, balancing the need for accurate predictions with the computational resources available. This makes it a viable option for applications requiring both speed and accuracy.",
  "model/availability": "The source code for the implementation and validation of the FHBF algorithm is publicly available. It can be accessed via GitHub at the following URL: https://github.com/vpz4/FHBF/tree/main. The repository contains the necessary code to run the algorithm, allowing others to replicate the experiments and utilize the FHBF for their own research or applications. The specific details regarding the licensing terms are not provided, but the code is intended for use by the research community and beyond.",
  "evaluation/method": "The evaluation of the method involved a comprehensive analysis using patient records and multiple databases. The study included 6,060 patient records, with different training and testing sequences evaluated across various databases. Two experimental phases were defined to examine the behavior of the FHBF algorithm across highly imbalanced data.\n\nIn the first experimental phase, the target feature was set to lymphoma, which has a 5% occurrence in the primary Sj\u00f6gren\u2019s syndrome (pSS) population. The final number of eligible databases was reduced to 18, as 5 databases had no reported lymphoma patients and were discarded from the analysis. The final number of harmonized patients was reduced to 4,905 with 32 overlapping features. Four case studies were defined with random training orders and different testing databases to evaluate the classification performance and the average training loss of the FHBF compared with the FGBT and FDART implementations.\n\nIn the second experimental phase, the classification problem was more difficult, with an even lower class imbalance. The target feature was set to MALT (mucosa-associated lymphoid tissue) lymphoma, which has an occurrence of less than 3% in the pSS population. The final number of eligible databases was 17, with 4,805 patients. Four additional case studies were defined with random training orders and different testing databases.\n\nThe consistency of each harmonized database was evaluated prior to the application of the FHBF to avoid biases during the incremental weight update process. Principal-component analysis (PCA) was applied on each individual harmonized database to extract the first four principal components (PCs) as those that describe most of the variance in each database. The consistency of the four PCs from each individual database was compared with the PCs across the total databases using either the Student\u2019s t test or the Wilcoxon rank-sum test based on the normality estimations obtained by the Shapiro-Wilk test for normality. No significant differences were observed between IPC1 and PC1 per database, confirming the consistency of the harmonized data. The same stands for IPC3 and PC3. Only one statistically significant difference was observed between PC2 and IPC2 and between IPC4 and PC4 in the PARIS (p < 0.05) and UMCU (p < 0.05) databases, respectively.",
  "evaluation/measure": "In the evaluation of our proposed method, we focused on several key performance metrics to comprehensively assess the effectiveness of our approach. Primarily, we reported sensitivity and specificity for each case study, providing a clear picture of the model's ability to correctly identify positive instances (sensitivity) and negative instances (specificity). These metrics are crucial for evaluating the performance of classifiers, especially in imbalanced datasets where the minority class is of particular interest.\n\nAdditionally, we examined the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curves. The AUC provides a single scalar value that summarizes the performance of the classifier across all classification thresholds, offering a more holistic view of the model's discriminative power.\n\nWe also considered the average log loss during both training and testing procedures. Log loss is a widely used metric that measures the performance of a classification model where the prediction input is a probability value between 0 and 1. Lower log loss values indicate better model performance, and we specifically highlighted the resilience of our method against overfitting by showing the lowest average training loss across all cases.\n\nThese metrics collectively provide a robust evaluation framework, aligning with standard practices in the literature for assessing classification models, particularly in the context of imbalanced datasets. The choice of these metrics ensures that our evaluation is both thorough and representative of the model's true performance.",
  "evaluation/comparison": "In the evaluation of our methods, we conducted a comprehensive comparison with other algorithms to assess their performance. Specifically, we compared our Federated Hybrid Boosting Framework (FHBF) with two other implementations: Federated Gradient Boosting Trees (FGBT) and Federated Dropouts Additive Regression Trees (FDART). These comparisons were performed across eight case studies, which involved different training and testing sequences using multiple databases.\n\nThe FGBT algorithm was configured with a 'gbtree' booster, a 'binary:logistic' objective, and 'logloss' as the evaluation metric. The parameters were updated incrementally, where the model trained on one database was updated on the subsequent database. For the FDART algorithm, the booster was set to 'dart', with the same objective and evaluation metric as FGBT, and dropout rates of 0.1 and 0.2 were used.\n\nOur FHBF algorithm utilized a hybrid loss function with a scale set to 0.1, matching the dropout rate. The number of rounds was set to 20 for evaluation purposes, and the booster was configured as 'HFDART', which is essentially the FDART with a customized hybrid loss. This setup allowed us to directly compare the performance of FHBF against FGBT and FDART under similar conditions.\n\nThe results indicated that FHBF yielded similar or better performance in both experimental phases. Even in cases where FDART with a dropout rate of 0.1 performed poorly, FHBF showed improved sensitivity and specificity. For instance, in cases 1, 5, and 7, FHBF demonstrated higher sensitivity and specificity compared to FDART. In other cases, such as 4 and 8, the performance was comparable. Notably, FHBF maintained its superior performance in the second experimental phase, where class imbalance was more pronounced, highlighting its robustness against overfitting.\n\nThe increased performance of FHBF was also confirmed by the higher Area Under the Curve (AUC) in the Receiver Operating Characteristic (ROC) curves. Additionally, FHBF exhibited the lowest average training loss across all cases, further emphasizing its resilience against overfitting. These comparisons provide a clear benchmark of FHBF's effectiveness relative to established methods.",
  "evaluation/confidence": "In the evaluation of our method, we employed statistical tests to assess the significance of our results. Specifically, we compared the performance of our IPCs against PCs from individual databases using either the Student\u2019s t test or the Wilcoxon rank-sum test, depending on the normality of the data as determined by the Shapiro-Wilk test.\n\nThe results indicated that there were no significant differences between IPC1 and PC1, as well as between IPC3 and PC3, across all databases. This consistency confirms the reliability of our harmonized data. However, we did observe one statistically significant difference between PC2 and IPC2 in the PARIS database, and between IPC4 and PC4 in the UMCU database, both with a p-value of less than 0.05. These findings suggest that while our method generally performs comparably to the baseline, there are specific instances where it shows statistically significant improvements or differences.",
  "evaluation/availability": "The raw evaluation files for our study are not publicly available. The evaluation process involved sensitive patient data, and to maintain privacy and comply with ethical guidelines, the data has not been released. However, detailed information about the evaluation methodology, including the experimental design, case studies, and performance metrics, is provided in the supplemental materials accompanying the publication. These materials include descriptions of the databases used, the training and testing sequences, and the results obtained from the different experimental phases. Researchers interested in replicating or building upon our work can refer to these detailed descriptions to understand the evaluation process thoroughly. For specific inquiries or collaborations, please contact the corresponding authors."
}