{
  "publication/title": "Integrating radiomics with clinical data for enhanced prediction of vertebral fracture risk.",
  "publication/authors": "Saravi B, Zink A, Tabukashvili E, G\u00fczel HE, \u00dclk\u00fcmen S, Couillard-Despres S, Lang GM, Hassel F",
  "publication/journal": "Frontiers in bioengineering and biotechnology",
  "publication/year": "2024",
  "publication/pmid": "39650236",
  "publication/pmcid": "PMC11620855",
  "publication/doi": "10.3389/fbioe.2024.1485364",
  "publication/tags": "- Radiomics\n- Vertebral Fracture\n- Machine Learning\n- Bone Health\n- Osteoporosis\n- CT Scans\n- Predictive Modeling\n- Clinical Data Integration\n- Fracture Risk Assessment\n- Biomedical Imaging",
  "dataset/provenance": "The dataset utilized for this study was derived from the \"MDCT vertebra segmentation and localization dataset,\" which was published as part of the Verse2019 challenge. This dataset was collected following approval from the local institutional review board of the Technical University of Munich, with a waiver of written informed consent. The data comprised CT images from two retrospective studies. The inclusion criteria for the first study required the availability of lumbar dual-energy x-ray absorptiometry (DXA) and a CT scan of the lumbar region, both performed within one year. For the second study, inclusion required a non-enhanced CT scan of the entire spine. Additional patient selection criteria included being over 30 years of age and having no history of bone metastases.\n\nA total of 104 CT image series from 104 patients were selected based on imaging requirements and the availability of DXA T-values from the dataset. The scans were obtained between January 2013 and November 2017 and included indications such as acute back pain, suspected spinal fractures, cancer staging, chronic back pain, and postoperative examinations. The dataset has been used in previous research and by the community, contributing to advancements in vertebral segmentation and localization.",
  "dataset/splits": "In our study, we utilized two primary datasets: a training dataset and an external validation dataset.\n\nThe training dataset was used to develop and refine our segmentation and prediction models. It consisted of a large number of CT image series, although the exact number of data points is not specified here. This dataset was crucial for training our machine learning models and ensuring their robustness.\n\nThe external validation dataset comprised 20 CT image series from 20 consecutive patients. These patients were diagnosed with both osteoporotic and non-osteoporotic fractures and underwent CT examinations and dual-energy x-ray absorptiometry (DXA) scans. The patients were recruited from the Department of Spine Surgery at Loretto-Hospital Freiburg, an affiliated hospital of the University Medical Center Freiburg, Germany. Ethical approval was obtained from the local Ethics Committee Freiburg, Germany. This dataset was used to evaluate the performance of our models in a real-world setting, ensuring that they generalize well to new, unseen data.",
  "dataset/redundancy": "The datasets were split using stratified 5-fold cross-validation to ensure that the distribution of classes was consistent across each fold. This method helps to maintain the independence of the training and test sets, as each fold serves as a test set while the remaining folds are used for training. The stratification process ensures that the class distribution in each fold is representative of the overall dataset, which is crucial for evaluating model performance, especially when dealing with imbalanced classes.\n\nTo enforce the independence of the training and test sets, we implemented a pipeline that included a standardization step. This pipeline ensured that all transformations were applied consistently across both the training and validation datasets. Additionally, class weighting was used during training to ensure that minority classes were appropriately represented, further enhancing the generalizability of the models.\n\nThe distribution of our dataset compares favorably to previously published machine learning datasets in the field. Our cohort comprised 124 participants, with an average age of 71.46 \u00b1 10.46 years, ranging from 50.6 to 92.7 years. This demographic distribution is representative of the population typically studied in similar research, ensuring that our findings are relevant and applicable to broader clinical settings. The use of stratified cross-validation and consistent preprocessing steps helps to mitigate potential biases and ensures that our models are robust and reliable.",
  "dataset/availability": "The dataset utilized in this study is publicly available. It was derived from the \"MDCT vertebra segmentation and localization dataset,\" which was published as part of the Verse2019 challenge. This dataset is accessible to the public and includes CT images from two retrospective studies. The data collection was approved by the local institutional review board of the Technical University of Munich, with a waiver of written informed consent.\n\nThe dataset comprises CT images from various multidetector CT scanners, including models from Philips and Siemens. The images were collected in helical mode, with specific parameters such as a peak tube voltage of 120 kVp and a slice thickness of 0.9\u20131 mm. The dataset includes indications such as acute back pain, suspected spinal fractures, cancer staging, chronic back pain, and postoperative examinations.\n\nThe data was anonymized and converted to Neuroimaging Informatics Technology Initiative (NIfTI) format to reduce computational demands. A deep learning framework, employing a fully convolutional neural network (CNN), was used for segmentation. The dataset is available under terms that allow for its use, distribution, or reproduction in other forums, provided that the original authors and the copyright owner are credited, and the original publication in this journal is cited.\n\nThe dataset includes 104 CT image series from 104 patients, selected based on specific imaging requirements and the availability of DXA T-values. The scans were obtained between January 2013 and November 2017, ensuring a comprehensive and diverse set of data for analysis. The dataset's availability and the use of public forums ensure that the data can be accessed and utilized by other researchers, promoting transparency and reproducibility in scientific research.",
  "optimization/algorithm": "The optimization algorithm employed in our study is Bayesian Optimization, specifically implemented through the BayesSearchCV method from the scikit-optimize library. This approach was chosen for its efficiency in exploring the hyperparameter space, particularly for complex models with multiple hyperparameters. It is not a new machine-learning algorithm but rather a well-established method for hyperparameter tuning. The decision to use this method was driven by its proven effectiveness in enhancing model performance by systematically searching for the optimal set of hyperparameters. The choice of this optimization technique was made to ensure that our models could achieve the best possible predictive accuracy and generalizability. The use of Bayesian Optimization allowed us to efficiently navigate the hyperparameter landscape, which is crucial for models like Random Forest, Gradient Boosting, Support Vector, and XGBoost that have numerous tunable parameters. This method's efficiency and effectiveness in handling complex models made it an ideal choice for our study, focusing on predicting vertebral fracture risk using radiomics and clinical features.",
  "optimization/meta": "The study did not employ a meta-predictor approach. Instead, individual machine learning models were developed and optimized separately. These models included RandomForest, GradientBoosting, SupportVector, and XGBoost. Each model was trained and evaluated independently using a pipeline that included standardization and the machine learning algorithm itself. Hyperparameter optimization was performed using Bayesian Optimization through the BayesSearchCV method, ensuring efficient exploration of the hyperparameter space.\n\nThe models were trained on datasets that included clinical features and, in some cases, radiomics features extracted from CT scans. The performance of these models was evaluated using metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Square Error (RMSE), and the coefficient of determination (R\u00b2). The inclusion of radiomics features significantly improved the predictive accuracy of the models, particularly for tasks such as predicting the number of fractures, fracture grading, and fracture shape.\n\nThe training data for each model was carefully managed to ensure independence. Stratified cross-validation was used to mitigate the impact of class imbalance and improve the generalizability of the models. This approach involved dividing the data into stratified folds, where each fold maintained the same proportion of class labels as the original dataset. This method helped to ensure that the models were trained and validated on independent data, reducing the risk of overfitting and enhancing the reliability of the results.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for ensuring the consistency and effectiveness of the machine learning models. Radiomics features were extracted from segmented vertebrae using the 3D Slicer software platform, encompassing a wide range of categories such as first-order statistics, shape-based features, gray level co-occurrence matrix (GLCM), gray level run length matrix (GLRLM), gray level size zone matrix (GLSZM), neighboring gray-tone difference matrix (NGTDM), and gray level dependence matrix (GLDM). These features capture various aspects of the image, including size, shape, texture, and intensity distributions.\n\nClinical features considered included patient age at CT scan, sex, and DXA T-value. Both clinical and radiomics data were standardized using the StandardScaler from scikit-learn to ensure that all features contributed equally to the analysis without bias due to differing scales. This standardization step was integrated into a pipeline for each model, which also included the machine learning algorithm itself. This approach ensured that all transformations were applied consistently across the training and validation datasets.\n\nHyperparameter optimization was performed using Bayesian Optimization through the BayesSearchCV method from the scikit-optimize library. This method was chosen for its efficiency in exploring the hyperparameter space, particularly for complex models with multiple hyperparameters. The optimization process was set to 30 iterations, with stratified 5-fold cross-validation to evaluate model performance. Class weighting was implemented to ensure that minority classes were appropriately represented during training, mitigating the impact of class imbalance on model performance and improving generalizability.",
  "optimization/parameters": "In our study, the number of parameters (p) used in the model varied depending on the specific machine learning algorithm employed. For the RandomForest model, the hyperparameters tuned included the number of estimators, maximum tree depth, minimum samples required to split an internal node, and minimum samples required at a leaf node. The GradientBoosting model had parameters such as the number of boosting stages, learning rate, maximum tree depth, minimum samples to split an internal node, and minimum samples at a leaf node. The SupportVector model focused on the regularization parameter and the epsilon in the epsilon-SVR model. Lastly, the XGBoost model included the number of boosting rounds, maximum tree depth, learning rate, subsample ratio, and column subsample by tree.\n\nThe selection of these parameters was guided by the need to optimize model performance while mitigating overfitting and underfitting. Bayesian Optimization through the BayesSearchCV method from the scikit-optimize library was utilized for this purpose. This method was chosen for its efficiency in exploring the hyperparameter space, particularly for complex models with multiple hyperparameters. The optimization process was set to 30 iterations, with a stratified 5-fold cross-validation to evaluate model performance. This approach ensured that the selected parameters were robust and generalizable across different subsets of the data.",
  "optimization/features": "The input features used in our study encompassed both clinical and radiomics data. The clinical features included patient age at CT scan, sex, and the DXA T-value. Radiomics features, extracted from segmented vertebral regions, covered a wide array of morphological and textural characteristics. These features were categorized into first-order statistics, shape-based features (both 3D and 2D), gray level co-occurrence matrix (GLCM), gray level run length matrix (GLRLM), gray level size zone matrix (GLSZM), neighboring gray-tone difference matrix (NGTDM), and gray level dependence matrix (GLDM).\n\nFeature selection was performed using a tree-based method, specifically the Random Forest model. This method was employed to identify the most predictive features by generating feature importance scores. The selection process was crucial for understanding which features were most informative in predicting the outcomes of interest, such as the number of fractures, fracture grading, and fracture shape. The feature selection was conducted using the training set only, ensuring that the validation set remained unbiased and unaffected by the selection process. This approach helped in mitigating overfitting and enhancing the generalizability of the models.",
  "optimization/fitting": "In our study, we employed several strategies to address potential overfitting and underfitting issues. The number of parameters in our models was indeed large, especially when considering the combination of clinical and radiomics features. To mitigate overfitting, we utilized Bayesian Optimization through the BayesSearchCV method, which efficiently explores the hyperparameter space and helps in selecting the optimal parameters. This method is particularly effective for complex models with multiple hyperparameters, ensuring that the models generalize well to unseen data.\n\nAdditionally, we implemented class weighting to ensure that minority classes were appropriately represented during training. This approach, combined with stratified 5-fold cross-validation, helped in mitigating the impact of class imbalance on model performance. The cross-validation process allowed us to evaluate the models' performance on different subsets of the data, providing a more robust estimate of their generalizability.\n\nTo further rule out overfitting, we monitored the performance metrics on both the training and validation datasets. The consistent performance across these datasets indicated that our models were not overfitting to the training data. Moreover, the use of radiomics features, which were selected based on their importance scores from a Random Forest model, ensured that only the most predictive features were included in the final models. This feature selection process helped in reducing the dimensionality of the data and improving the models' performance.\n\nUnderfitting was addressed by using ensemble methods such as Random Forest, Gradient Boosting, and XGBoost, which are known for their ability to capture complex interactions among features. The hyperparameter optimization process also played a crucial role in ensuring that the models were not too simplistic. The significant improvements in performance metrics when radiomics features were included further indicated that the models were not underfitting.\n\nIn summary, our approach involved a combination of hyperparameter optimization, class weighting, stratified cross-validation, and feature selection to address potential overfitting and underfitting issues. The consistent performance across training and validation datasets, along with the improvements observed with the inclusion of radiomics features, provided confidence in the robustness and generalizability of our models.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our models. One key method was the use of class weighting during the training process. This approach helped to balance the impact of minority classes, ensuring that they were appropriately represented and reducing the risk of the model being biased towards the majority class.\n\nAdditionally, we implemented stratified cross-validation, which is a technique that preserves the percentage of samples for each class in both the training and validation sets. This method helps to maintain the class distribution across folds, providing a more reliable estimate of model performance and reducing the likelihood of overfitting.\n\nAnother crucial step was the use of Bayesian Optimization through the BayesSearchCV method from the scikit-optimize library. This technique efficiently explores the hyperparameter space, helping to find the optimal set of hyperparameters that generalize well to unseen data. By setting the optimization process to 30 iterations with stratified 5-fold cross-validation, we ensured that our models were thoroughly evaluated and tuned for better performance and generalization.\n\nFurthermore, we created pipelines that included standardization steps along with the machine learning models. This ensured that all transformations were applied consistently across the training and validation datasets, maintaining the integrity of the data and reducing the risk of overfitting.\n\nThese combined techniques helped to mitigate the impact of class imbalance and improve the generalizability of our models, ensuring that they performed well on both training and validation datasets.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and optimization parameters used in our study are fully reported within the publication. We utilized Bayesian Optimization through the BayesSearchCV method from the scikit-optimize library, which was chosen for its efficiency in exploring the hyperparameter space, especially for complex models with multiple hyperparameters. The optimization process was set to 30 iterations, with stratified 5-fold cross-validation to evaluate model performance.\n\nFor each model, specific hyperparameters were tuned:\n\n* RandomForest: number of estimators, maximum tree depth, minimum samples required to split an internal node, and minimum samples required at a leaf node.\n* GradientBoosting: number of boosting stages, learning rate, maximum tree depth, minimum samples to split an internal node, and minimum samples at a leaf node.\n* SupportVector: regularization parameter and the epsilon in the epsilon-SVR model.\n* XGBoost: number of boosting rounds, maximum tree depth, learning rate, subsample ratio, and column subsample by tree.\n\nThese details are provided to ensure reproducibility and transparency in our methodology. The code and specific model files are not directly available in the publication, but the methods and parameters are described in sufficient detail for others to replicate the optimization process. The publication is distributed under the terms of the Creative Commons Attribution License (CC BY), which permits use, distribution, or reproduction in other forums, provided the original authors and the copyright owner are credited and the original publication in this journal is cited.",
  "model/interpretability": "The models employed in this study, particularly the machine learning algorithms like RandomForest, GradientBoosting, SupportVector, and XGBoost, are generally considered to be more interpretable compared to deep learning models. These models provide insights into feature importance, which allows for a better understanding of the factors contributing to the predictions.\n\nFor instance, the RandomForest model highlights that features such as Age at CT (age_ct) and Dependence Entropy are crucial in predicting fracture risk. Age at CT had the highest importance score, underscoring the critical role of patient age in assessing bone health and fracture susceptibility. Dependence Entropy, a texture feature, reflects the randomness of gray-level intensity distributions, providing insights into the structural heterogeneity of bone.\n\nAdditionally, clinical measures like DXA T-value, which indicates bone mineral density, and shape features like Surface Volume Ratio, which relates to the bone's surface area relative to its volume, were also significant. These features were selected based on their relevance and scores, demonstrating their contribution to the model's decision-making process.\n\nThe integration of radiomics features, such as Total Energy, Median, Busyness, and Kurtosis, further enhanced the models' predictive capabilities. These features offer detailed texture information critical for understanding complex bone shapes and predicting fracture shapes.\n\nOverall, the models used in this study are not black boxes. They provide transparent insights into which features are most important for making predictions, thereby aiding in the interpretability of the results. This transparency is essential for clinical applications, where understanding the underlying factors contributing to predictions is crucial for trust and adoption.",
  "model/output": "The models developed in this study are regression models. They are designed to predict continuous outcomes related to vertebral fractures, specifically the number of fractures, fracture grading, and fracture shape. The performance of these models is evaluated using regression metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Square Error (RMSE), and the coefficient of determination (R\u00b2). These metrics provide a comprehensive assessment of the models' predictive accuracy and their ability to fit the data.\n\nThe models include RandomForest, GradientBoosting, SupportVector, and XGBoost. Each model was optimized using Bayesian Optimization through the BayesSearchCV method, which efficiently explores the hyperparameter space. Hyperparameters such as the number of estimators, maximum tree depth, learning rate, and others were tuned to improve model performance. Additionally, class weighting and stratified cross-validation were implemented to address class imbalance and enhance generalizability.\n\nThe inclusion of radiomics features, extracted from CT scans, significantly improved the models' predictive capabilities. For instance, the RandomForest model with combined clinical and radiomics features achieved a reduced MAE and RMSE, along with a higher R\u00b2 value, indicating better predictive accuracy. Similarly, the XGBoost model demonstrated the highest accuracy among all evaluated models, with the lowest MAE and RMSE, and the highest R\u00b2 value.\n\nKey features that contributed to the models' decision-making process include Age at CT, Dependence Entropy, DXA T-value, Surface Volume Ratio, Total Energy, Mean, Median, and Busyness. These features were selected based on their importance scores, highlighting their relevance in predicting fracture risk and shape. The integration of these radiomics features with clinical data enhanced the models' ability to assess vertebral fracture risk accurately.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the software used in this study is not publicly released. However, the segmentation algorithm utilized the 3D Slicer software platform, which is an open-source software available for download. The specific framework used within 3D Slicer is the nnU-Net framework, which is also open-source and can be accessed through its respective repositories. The segmentation process involved the use of this framework, which automatically configures hyperparameters based on the dataset's characteristics. Additionally, the initial segmentations were refined by a radiologist using the Segment Editor module in 3D Slicer software. While the specific code used for the analysis is not available, the tools and frameworks employed are open-source and can be accessed by the community.",
  "evaluation/method": "The evaluation method employed in this study was comprehensive and multifaceted, ensuring robust assessment of the models' performance. A pipeline was created for each model, incorporating standardization and the machine learning model itself, to ensure consistent transformations across training and validation datasets.\n\nHyperparameter optimization was conducted using Bayesian Optimization through the BayesSearchCV method from the scikit-optimize library. This approach was chosen for its efficiency in exploring the hyperparameter space, particularly for complex models with multiple hyperparameters. The optimization process was set to 30 iterations, with stratified 5-fold cross-validation to evaluate model performance.\n\nFor the RandomForest model, hyperparameters such as the number of estimators, maximum tree depth, minimum samples required to split an internal node, and minimum samples required at a leaf node were tuned. Similarly, for the GradientBoosting model, parameters like the number of boosting stages, learning rate, maximum tree depth, minimum samples to split an internal node, and minimum samples at a leaf node were optimized. The SupportVector model focused on the regularization parameter and the epsilon in the epsilon-SVR model. The XGBoost model tuned parameters including the number of boosting rounds, maximum tree depth, learning rate, subsample ratio, and column subsample by tree.\n\nClass weighting was implemented to ensure that minority classes were appropriately represented during training. This, along with stratified cross-validation, helped mitigate the impact of class imbalance on model performance and improved generalizability.\n\nRadiomics feature selection was performed using a tree-based method, specifically the Random Forest model. The feature importance scores generated by the model were utilized to identify the most predictive features. This step was crucial in understanding which features (clinical and radiomics) were most informative in predicting the outcomes of interest: the number of fractures, fracture grading, and fracture shape.\n\nModel performance was evaluated using a variety of metrics: Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Square Error (RMSE), and the coefficient of determination (R\u00b2). These metrics provided a comprehensive assessment of the models\u2019 predictive accuracy, error margins, and overall fit to the data. The correlations were calculated using Pearson or Spearman methods, based on the results of the Shapiro-Wilk normality test. A p-value of less than 0.05 was considered statistically significant. All analyses were performed in Python.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our models in predicting vertebral fracture risk. The primary metrics reported include Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Square Error (RMSE), and the coefficient of determination (R\u00b2). These metrics were chosen for their ability to provide a thorough assessment of model accuracy, error margins, and overall fit to the data.\n\nMAE measures the average magnitude of errors in a set of predictions, without considering their direction. It provides a straightforward indication of prediction accuracy. MSE, on the other hand, quantifies the average of the squares of the errors, giving more weight to larger errors. RMSE is the square root of MSE, providing an error metric in the same units as the target variable, which is particularly useful for interpreting the scale of errors. The R\u00b2 value indicates the proportion of variance in the dependent variable that is predictable from the independent variables, offering insight into the model's explanatory power.\n\nThese metrics were evaluated on both train/test and validation datasets, ensuring that the models' performance was assessed not only on the data they were trained on but also on unseen data. This approach helps in understanding the models' generalizability and robustness.\n\nThe inclusion of these metrics is representative of standard practices in the literature, where a combination of error metrics and explanatory power measures is commonly used. This set of metrics allows for a balanced evaluation of model performance, capturing both the accuracy of predictions and the models' ability to generalize to new data. Additionally, the use of stratified cross-validation and class weighting further ensures that the performance metrics are reliable and not biased by class imbalances in the dataset.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on evaluating the predictive capability of radiomics features extracted from CT scans in combination with clinical data for assessing vertebral fracture risk.\n\nWe did, however, compare the performance of different machine learning models, both with and without the inclusion of radiomics features. This comparison allowed us to assess the impact of radiomics on model performance. The models we evaluated included RandomForest, GradientBoosting, SupportVector, and XGBoost. Each model was trained and validated using a pipeline that included standardization and hyperparameter optimization through Bayesian Optimization.\n\nFor each model, we tuned specific hyperparameters to optimize performance. For example, in the RandomForest model, we adjusted the number of estimators, maximum tree depth, minimum samples required to split an internal node, and minimum samples required at a leaf node. Similarly, we fine-tuned parameters for GradientBoosting, SupportVector, and XGBoost models.\n\nThe performance metrics used for evaluation included Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Square Error (RMSE), and the coefficient of determination (R\u00b2). These metrics provided a comprehensive assessment of the models' predictive accuracy, error margins, and overall fit to the data.\n\nWe also implemented class weighting and stratified cross-validation to mitigate the impact of class imbalance and improve the generalizability of the models. The optimization process was set to 30 iterations, with a stratified 5-fold cross-validation to evaluate model performance.\n\nIn summary, while we did not compare our methods to publicly available benchmarks, we conducted a thorough comparison of different machine learning models and evaluated their performance with and without radiomics features. This approach allowed us to demonstrate the significant improvements in predictive accuracy achieved by incorporating radiomics features.",
  "evaluation/confidence": "The evaluation of our models included a comprehensive assessment using various performance metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Square Error (RMSE), and the coefficient of determination (R\u00b2). These metrics provided a thorough understanding of the models' predictive accuracy, error margins, and overall fit to the data.\n\nTo ensure the statistical significance of our results, we employed Pearson or Spearman correlation methods, depending on the outcomes of the Shapiro-Wilk normality test. A p-value of less than 0.05 was considered statistically significant. This approach allowed us to confidently claim the superiority of our methods over baselines and other models.\n\nHowever, confidence intervals for the performance metrics were not explicitly provided in the results. The statistical significance of the improvements observed when incorporating radiomics features was established through the rigorous evaluation process, including stratified cross-validation and hyperparameter optimization. This methodology ensured that the enhancements in model performance were robust and generalizable.",
  "evaluation/availability": "The raw data supporting the conclusions of this article will be made available by the authors, without undue reservation. This ensures that other researchers can access and verify the findings presented in the study. The data is shared in accordance with accepted academic practices, promoting transparency and reproducibility in scientific research."
}