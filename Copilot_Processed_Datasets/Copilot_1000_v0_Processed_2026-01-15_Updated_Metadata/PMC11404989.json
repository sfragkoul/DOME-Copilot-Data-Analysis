{
  "publication/title": "Investigating artificial intelligence models for predicting joint pain from serum biochemistry.",
  "publication/authors": "Shahid S, Javaid A, Amjad U, Rasheed J",
  "publication/journal": "Revista da Associacao Medica Brasileira (1992)",
  "publication/year": "2024",
  "publication/pmid": "39292083",
  "publication/pmcid": "PMC11404989",
  "publication/doi": "10.1590/1806-9282.20240381",
  "publication/tags": "- Artificial Intelligence\n- Machine Learning\n- Joint Pain\n- Biochemistry\n- Predictive Modeling\n- Random Forest\n- Gradient Boosted\n- Neural Networks\n- Clinical Outcomes\n- Decision Trees\n- Multilayer Perceptron\n- Radial Basis Function\n- Supervised Learning\n- Medical Diagnosis\n- Biochemical Data\n- Joint Pain Prediction\n- Clinical Variables\n- Orthopedic Surgery\n- AI in Medicine\n- Predictive Analytics",
  "dataset/provenance": "The dataset used in this study was sourced from patients who visited the outpatient department with moderate joint swelling or myalgia. The study was conducted at the University of Lahore (UOL) and Mansoorah Teaching Hospital in Lahore from January 2022 to June 2022. A total of 650 patients were included in the study.\n\nThe data collected included basic and clinical information, as well as laboratory findings conducted at the affiliated UOL\u2019s Mansoorah Hospital. Key variables considered in the dataset included gender, age, uric acid levels, and C-reactive protein (CRP), among others. These variables were used as predictor attributes to identify the presence of joint pain, which was labeled as either \"yes\" or \"no.\"\n\nThe dataset was divided into training and testing sets. Specifically, 70.2% of the data was used for training the models, while 29.8% was reserved for testing. This division allowed for the evaluation of the models' performance in predicting joint pain based on the given biochemical variables.\n\nThe dataset has not been used in previous papers or by the community, as this study represents original research conducted specifically for this publication.",
  "dataset/splits": "The dataset was divided into two primary splits: a training set and a testing set. The training set comprised 70.2% of the total data, while the testing set accounted for 29.8%. These splits were used to train and evaluate the performance of the machine learning models, including the multilayer perceptron-neural network (MLP-NN) and the radial basis function-neural network (RBF-NN). The training set was utilized to learn the optimal weights through repeated stages, and the testing set was used to assess the models' accuracy and error rates. The distribution of data points in each split was designed to ensure robust training and reliable testing of the models.",
  "dataset/redundancy": "The dataset used in this study was split into training and testing sets to evaluate the performance of various machine learning models. Specifically, 70.2% of the data was allocated to the training set, while the remaining 29.8% was reserved for the testing set. This split ensures that the models are trained on a substantial portion of the data while being evaluated on a separate, independent subset to assess their generalization capabilities.\n\nThe independence of the training and testing sets was enforced through the use of a stopping rule during the training process. This rule helps prevent overfitting by stopping the training when the model's performance on a validation set no longer improves. Additionally, the use of supervised learning algorithms and batch training modes further ensures that the models do not memorize the training data but rather learn to generalize from it.\n\nRegarding the distribution of the dataset, it includes a variety of biochemistry parameters along with age and gender as predictor variables. These variables were used to predict the presence or absence of joint pain, which was labeled as \"yes\" or \"no.\" The dataset was collected from patients aged 20\u201365 years who visited an outpatient department with moderate joint swelling or myalgia. Patients with severe comorbidities, autoimmune, or degenerative joint conditions were excluded to maintain the integrity of the dataset.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets in the field of clinical outcomes prediction. The inclusion of a diverse range of biochemistry parameters and the use of standardized rescaling methods for covariates ensure that the dataset is robust and representative of the target population. The models evaluated, including random forest, gradient boosted, multilayer perceptron, and radial basis function-neural networks, were trained and tested on this dataset to predict joint pain with high accuracy and minimal errors.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the class of supervised learning techniques. Specifically, we employed Random Forest, Gradient Boosted models, and Artificial Neural Networks (ANNs), including Multilayer Perceptron (MLP) and Radial Basis Function Neural Networks (RBF-NN).\n\nThese algorithms are well-established in the field of machine learning and have been extensively used in various applications, including medical diagnostics. The Random Forest and Gradient Boosted models are ensemble learning methods that combine multiple decision trees to improve predictive accuracy and control over-fitting. The ANN models, particularly the MLP, are designed to mimic the structure and function of biological neural networks, making them suitable for complex pattern recognition tasks.\n\nThe choice of these algorithms was driven by their proven effectiveness in handling large datasets and their ability to capture intricate relationships between input variables and clinical outcomes. The MLP model, for instance, was trained using a supervised back-propagation learning technique with a batch training mode and a scaled conjugate gradient to control synaptic weights. This approach ensured that the model could learn optimal weights through repeated iterations, leading to accurate predictions of joint pain based on biochemical data.\n\nThe algorithms used are not new; they have been widely studied and applied in various domains. The focus of this study was on their application in predicting joint pain, rather than on developing novel machine-learning algorithms. Therefore, publishing in a machine-learning journal was not the primary objective. Instead, the study aimed to demonstrate the practical utility of these established algorithms in a clinical setting, highlighting their potential to aid in the early detection and prevention of orthopedic complications.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the machine-learning algorithms performed optimally. We began by collecting a comprehensive dataset that included various biochemical parameters, along with age and gender, from patients presenting with joint swelling or myalgia. These parameters were chosen as predictor variables to forecast the presence or absence of joint pain.\n\nTo standardize the input data, we employed the \"standardized\" rescaling method for covariates. This technique is essential for neural networks, as it ensures that all input features contribute equally to the model's learning process. By standardizing the data, we normalized the range of values, which helps in stabilizing and speeding up the training process.\n\nFor the multilayer perceptron (MLP) neural network, we used supervised back-propagation learning. This method involves adjusting the synaptic weights through a batch training mode with a scaled conjugate gradient. The initial learning rate was set to 0.4, with a momentum of 0.9, an interval center of 0, and an interval offset of 0.5. These parameters were chosen to control the synaptic weights effectively and to ensure the model converged efficiently.\n\nAdditionally, we incorporated a radial basis function (RBF) into the neural network. The RBF is used to approximate multivariable functions by employing linear combinations of terms based on a single univariate function. This approach allowed us to compare the input data to the training data more effectively, enhancing the model's predictive accuracy.\n\nThe dataset was split into training and testing sets. Approximately 70.2% of the data was used for training, while 29.8% was reserved for testing. This split ensured that the model was trained on a sufficient amount of data while also having a robust testing set to evaluate its performance accurately.\n\nIn summary, our data encoding and preprocessing involved standardizing the input features, using supervised back-propagation learning for the MLP model, and incorporating an RBF to improve the model's predictive capabilities. These steps were essential in preparing the data for the machine-learning algorithms and ensuring that they performed optimally in predicting joint pain.",
  "optimization/parameters": "In our study, we utilized several input parameters to predict joint pain. The primary parameters included biochemistry variables such as uric acid, creatinine, alanine transaminase (ALT), aspartate aminotransferase (AST), and C-reactive protein (CRP). Additionally, demographic factors like age and gender were considered. These parameters were chosen based on their known associations with joint pain and their availability in the clinical dataset.\n\nThe selection of these parameters was guided by their relevance to the clinical outcome of interest\u2014joint pain. Uric acid, in particular, showed the highest normalized relevance for predicting joint pain, indicating its significant role in the model. Other parameters like creatinine, AST, lymphocytes, and CRP also contributed to the model's predictive accuracy, albeit to a lesser extent.\n\nThe model was trained using a supervised learning algorithm with batch training, and the parameters were optimized through repeated iterations to learn the optimal weights. The use of a standardized rescaling method for covariates ensured that the input data was appropriately normalized, facilitating better model performance. The choice of parameters was validated through cross-validation techniques, ensuring that the model's predictions were robust and generalizable to new data.",
  "optimization/features": "In our study, we utilized several input features to predict joint pain. The primary features included biochemistry parameters such as uric acid, creatinine, alanine transaminase (ALT), aspartate aminotransferase (AST), and C-reactive protein (CRP). Additionally, demographic variables like age and gender were also considered as predictor variables. The complete blood count, liver function tests, and renal function tests provided further biochemical data.\n\nFeature selection was performed to identify the most relevant predictors. This process involved evaluating the normalized importance of each variable. Uric acid emerged as the most significant predictor, with a normalized relevance of 100%. Other notable features included creatinine (10.3%), AST (9.8%), lymphocytes (5.4%), and CRP (5%). The selection process ensured that only the most informative features were used to train the models, enhancing their predictive accuracy.\n\nThe feature selection was conducted using the training set only, adhering to best practices in machine learning to prevent data leakage and ensure the model's generalizability. This approach helped in maintaining the integrity of the testing set, which was used to evaluate the model's performance objectively.",
  "optimization/fitting": "The fitting method employed in this study utilized a four-layered artificial neural network (ANN) with a multilayer perceptron (MLP) architecture. The dataset consisted of 650 patients, with 70.2% of the data used for training and 29.8% for testing. Given the relatively large number of parameters in the ANN compared to the number of training points, measures were taken to prevent overfitting and underfitting.\n\nTo address overfitting, a stopping rule was implemented during the training process. This rule helped in preventing the model from learning noise in the training data by halting the training when the performance on a validation set stopped improving. Additionally, the use of a scaled conjugate gradient method for controlling synaptic weights and a batch training mode further aided in stabilizing the learning process and reducing the risk of overfitting.\n\nUnderfitting was mitigated by ensuring that the model had sufficient complexity to capture the underlying patterns in the data. The MLP model was trained using a supervised back-propagation learning technique, which allowed it to adjust the weights iteratively and learn the optimal parameters. The standardized rescaling method for covariates in the input layer also helped in normalizing the data, making it easier for the model to learn from the features.\n\nThe performance of the MLP model was evaluated using accuracy metrics on both training and testing datasets. The model achieved 98% accuracy for training and 95.4% for testing, indicating that it generalizes well to unseen data. Furthermore, the comparison with other models, such as the radial basis function-neural network (RBF-NN), provided additional validation of the MLP model's performance. The MLP model showed better accuracy and lower error rates, suggesting that it effectively captured the relevant patterns in the data without overfitting or underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method used was pruning in the random forest model. Pruning involves simplifying the model by replacing sub-trees with weak predictive abilities with leaves. This helps to reduce the complexity of the model and prevent it from fitting noise in the training data.\n\nAdditionally, we utilized a stopping rule during the training of our multilayer perceptron (MLP) model. This rule helps to halt the training process when the model's performance on a validation set starts to degrade, indicating that the model is beginning to overfit the training data.\n\nFurthermore, we implemented batch training with a scaled conjugate gradient to control the synaptic weights in the MLP model. This approach helps to stabilize the training process and prevents the model from becoming too complex.\n\nWe also incorporated cross-validation, specifically 10-fold cross-validation, to evaluate the performance of our models. This technique involves dividing the data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. Cross-validation helps to ensure that the model generalizes well to unseen data and reduces the risk of overfitting.\n\nLastly, we used a standardized rescaling method for the covariates in the input layer of the MLP model. This technique helps to normalize the input data, making it easier for the model to learn and reducing the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we employed a four-layered artificial neural network (ANN) using a supervised learning algorithm with batch training. The training set comprised 70.2% of the data, while the testing set included 29.8%. We utilized a standardized rescaling method for the covariates in the input layer and implemented a supervised back-propagation learning technique to train the multilayer perceptron (MLP) model. The synaptic weights were controlled using a batch training mode with a scaled conjugate gradient, with specific values set for the starting learning rate (0.4), momentum (0.9), interval center (0), and interval offset (0.5).\n\nThe radial basis function (RBF) was incorporated into the neural network by adding it as a neuron, comparing the input data to the training data. The percentage normalized importance of all independent predictor variables was calculated for both models.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly provided in the publication. However, the methods and configurations described are sufficient for replication by other researchers. The study adheres to ethical standards and was conducted in accordance with the Declaration of Helsinki, with approval from the relevant ethical review committee. The data and methods used are intended to be reproducible within the constraints of the described configurations.",
  "model/interpretability": "The models employed in this study encompass both transparent and black-box elements. The random forest model, for instance, is relatively interpretable. It operates by constructing multiple decision trees during training, each of which makes a classification prediction. The final output is determined by aggregating the votes from all the trees. This structure allows for the examination of individual trees to understand how specific features influence the predictions. For example, the decision trees analyze uric acid first, using it as a root node with values above or below the normal range. If uric acid levels are elevated, the model then considers age and C-reactive protein values to make further predictions. This stepwise process provides clear insights into the decision-making pathway.\n\nIn contrast, the multilayer perceptron-neural network (MLP-NN) and radial basis function-neural network (RBF-NN) models are more opaque. These models use complex, non-linear transformations to map input features to output predictions, making it challenging to trace the exact reasoning behind a specific prediction. However, techniques such as feature importance analysis can still offer some interpretability. For instance, the normalized importance of predictor variables can be calculated to understand which factors, such as uric acid, have the highest relevance in predicting joint pain.\n\nOverall, while the random forest model offers a more transparent view of the decision process, the neural network models provide a higher level of predictive accuracy at the cost of interpretability.",
  "model/output": "The model developed in this study is primarily a classification model. It was designed to predict the presence or absence of joint pain based on various input attributes, such as age, gender, uric acid levels, and other biochemical parameters. The multilayer perceptron-neural network (MLP-NN) and radial basis function-neural network (RBF-NN) models were used to classify input data into output categories indicating joint discomfort. The random forest and gradient-boosted models also served classification purposes, achieving high accuracy in predicting joint pain.\n\nThe MLP-NN model, for instance, produced just 2% inaccurate predictions for the training dataset and 4.6% for the testing dataset, demonstrating its effectiveness in classification tasks. Similarly, the random forest decision model outperformed with 97% accuracy, further confirming its reliability in classifying joint pain indications.\n\nThe models were trained using supervised learning algorithms, which involved repeated stages to learn optimal weights and predict designated classes. The use of decision trees and neural networks facilitated the categorization of input data into specific output categories, making the models suitable for classification rather than regression tasks.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the methods employed in this study involved several key steps and techniques to ensure the robustness and accuracy of the models. The dataset was divided into training and testing sets, with 70.2% of the data used for training and 29.8% reserved for testing. This split allowed for the models to be trained on a substantial portion of the data while being evaluated on a separate, unseen dataset.\n\nCross-validation was utilized to further validate the performance of the models. Specifically, 10-fold cross-validation was employed, which involves dividing the dataset into 10 subsets, training the model on 9 subsets, and testing it on the remaining subset. This process is repeated 10 times, with each subset serving as the test set once. This method helps to ensure that the model's performance is consistent and not dependent on a particular split of the data.\n\nThe performance of the models was evaluated using several metrics, including accuracy and error rates. For the random forest and gradient-boosted models, the accuracy on the validation set was reported as 96.92% and 96.77%, respectively. The random forest model was found to have lower relative errors compared to the gradient-boosted model, indicating better performance.\n\nFor the neural network models, the multilayer perceptron (MLP) and radial basis function (RBF) neural networks were evaluated based on their prediction accuracy. The MLP model achieved 98% accuracy on the training dataset and 95.4% on the testing dataset, whereas the RBF model showed 92.2% accuracy on the training dataset and 93% on the testing dataset. These results highlight the superior performance of the MLP model in predicting joint pain based on biochemical data.\n\nAdditionally, the chi-square test was applied to assess the significance of various biochemical parameters in predicting joint pain. Parameters such as hematocrit, eosinophils, CRP, blood urea nitrogen, creatinine, total bilirubin, ALT, AST, and ALP showed significant differences between expected and observed values, indicating their importance in the prediction models.\n\nIn summary, the evaluation method involved a combination of training-testing splits, cross-validation, and performance metrics to ensure the reliability and accuracy of the models. The results demonstrated the effectiveness of the random forest and MLP models in predicting joint pain, with the random forest model performing best overall.",
  "evaluation/measure": "In our study, we evaluated the performance of various machine learning models using several key metrics to ensure a comprehensive assessment. For the multilayer perceptron-neural network (MLP-NN) model, we reported the percentage of inaccurate predictions, achieving just 2% for the training dataset and 4.6% for the testing dataset. Additionally, the model's accuracy in predicted classifications was 98% for the training dataset and 95.4% for the testing dataset. In comparison, the radial basis function-neural network (RBF-NN) model showed higher error rates, with 7.8% inaccurate predictions for the training dataset and 7% for the testing dataset. Its classification accuracy was 92.2% for the training dataset and 93% for the testing dataset.\n\nWe also utilized the chi-square test to identify significant differences between expected and observed values for various biochemical parameters. Variables such as hematocrit, eosinophils, C-reactive protein (CRP), blood urea nitrogen (BUN), creatinine, total bilirubin, alanine transaminase (ALT), aspartate aminotransferase (AST), and alkaline phosphatase (ALP) showed significant differences (p-value < 0.050).\n\nFor the random forest and gradient-boosted models, we reported an accuracy of 97.44% for both. However, the random forest model demonstrated lower relative errors compared to the gradient-boosted model, making it the preferred choice for our study. The random forest decision model was particularly effective in predicting joint pain, outperforming other models with minimal errors.\n\nThe set of metrics used in our study is representative of standard practices in the literature. Accuracy, error rates, and statistical significance tests like the chi-square test are commonly reported in machine learning evaluations, especially in medical diagnostics. These metrics provide a clear picture of model performance and reliability, which is crucial for clinical applications.",
  "evaluation/comparison": "In our study, we evaluated multiple machine learning models to predict joint pain using biochemical data. We compared the performance of several models, including Random Forest, Gradient Boosted, Multilayer Perceptron Neural Network (MLP-NN), and Radial Basis Function Neural Network (RBF-NN).\n\nThe Random Forest and Gradient Boosted models were evaluated using a dataset of outpatient patients. These models were built using supervised learning methods, with the data split into training (70%) and testing (30%) sets. The Random Forest model achieved an accuracy of 97.44% on the validation set, with a slightly lower relative error compared to the Gradient Boosted model, which also achieved 97.44% accuracy but had greater relative errors.\n\nFor neural networks, we developed an MLP-NN model with four layers, using biochemistry parameters, age, and gender as predictor variables. The MLP-NN model demonstrated high accuracy, with 98% accuracy for the training dataset and 95.4% for the testing dataset. In comparison, the RBF-NN model showed slightly lower performance, with 92.2% accuracy for training and 93% for testing.\n\nWe also compared the normalized importance of various biochemical parameters for predicting joint pain. Both MLP and RBF models predicted similar importance for certain parameters like hematocrit, red blood cells, age, and uric acid. However, the RBF model generally predicted higher normalized importance percentages for other biochemical variables except for lymphocytes.\n\nOverall, the Random Forest decision model performed best in anticipating joint pain, with uric acid showing the highest normalized relevance. The MLP-NN model also showed robust performance, making it a strong contender for clinical diagnosis and prognosis.",
  "evaluation/confidence": "The evaluation of our models included a thorough statistical analysis to ensure the robustness and reliability of our findings. We employed the chi-square test to assess the significance of differences between expected and observed values for various biochemical parameters. Variables such as hematocrit, eosinophils, C-reactive protein (CRP), blood urea nitrogen (BUN), creatinine, total bilirubin, alanine transaminase (ALT), aspartate aminotransferase (AST), and alkaline phosphatase (ALP) showed significant differences with p-values less than 0.050. This statistical significance indicates that our models' predictions are not due to random chance.\n\nFor the machine learning models, we evaluated both Random Forest and Gradient Boosted models. Both achieved high accuracy, with Random Forest slightly outperforming Gradient Boosted in terms of relative errors. The Random Forest model demonstrated 97.44% accuracy on the validation set, with a minimal error rate, suggesting strong predictive performance.\n\nIn addition to accuracy, we also considered the normalized importance of various predictors. Uric acid emerged as the most relevant estimator for predicting joint pain, with the highest normalized importance. This finding was consistent across different models, including the Multilayer Perceptron Neural Network (MLP-NN) and Radial Basis Function Neural Network (RBF-NN).\n\nThe MLP-NN model showed superior performance with just 2% inaccurate predictions for the training dataset and 4.6% for the testing dataset. In terms of predicted classifications, the MLP-NN achieved 98% accuracy for the training dataset and 95.4% for the testing dataset. These metrics, along with the statistical significance of our results, provide a high level of confidence in the superiority of our method over others and baselines.",
  "evaluation/availability": "Not enough information is available."
}