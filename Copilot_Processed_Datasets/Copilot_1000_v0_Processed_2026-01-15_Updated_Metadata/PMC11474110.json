{
  "publication/title": "Comprehensive data optimization and risk prediction framework: machine learning methods for inflammatory bowel disease prediction based on the human gut microbiome data.",
  "publication/authors": "Peng Y, Liu Y, Liu Y, Wang J",
  "publication/journal": "Frontiers in microbiology",
  "publication/year": "2024",
  "publication/pmid": "39411443",
  "publication/pmcid": "PMC11474110",
  "publication/doi": "10.3389/fmicb.2024.1483084",
  "publication/tags": "- Microbiome\n- Inflammatory Bowel Disease\n- Machine Learning\n- Data Imputation\n- Dimensionality Reduction\n- Risk Prediction\n- High-Dimensional Data\n- Sparse Data\n- Model Optimization\n- Gut Microbiome",
  "dataset/provenance": "The dataset utilized in this study originates from the Inflammatory Bowel Disease Multi\u2019omics Database (IBDMDB) within the Integrated Human Microbiome Project (iHMP). This dataset focuses on microbial community structure and diversity, analyzed through 16S rRNA gene sequencing, specifically targeting the V4 region. The dataset comprises 178 participant records, with 137 individuals diagnosed with Inflammatory Bowel Disease (IBD) and 41 without. Each record consists of 983 fields, where one field indicates the presence or absence of IBD, and the remaining 982 fields represent various microbial features. The dataset is characterized by high-dimensional sparsity, with many feature values being close to zero or exactly zero, leading to a sparse data distribution in high-dimensional space. Despite this overall sparsity, some samples exhibit high abundance in specific feature dimensions, creating locally dense regions. Additionally, approximately 491 features have missing values, with missing rates ranging from 0.56% to 9.55%, and an average missing rate of 5.27%. This dataset has been used in previous research and by the community to study the association between the human gut microbiome and IBD pathogenesis, making it a valuable resource for risk prediction and diagnostic tools.",
  "dataset/splits": "Not applicable.",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "The dataset used in this study is from the Inflammatory Bowel Disease Multi\u2019omics Database (IBDMDB) within the Integrated Human Microbiome Project (iHMP). This dataset is publicly available and was originally published by Lloyd-Price et al. in 2019. The dataset comprises 178 participant records, with 137 individuals having Inflammatory Bowel Disease (IBD) and 41 without. Each record consists of 983 fields, where one field indicates the presence or absence of IBD, and the remaining 982 fields represent various microbial features.\n\nThe dataset is characterized by high-dimensional sparsity, the presence of missing values, differences in class distribution, and local density regions. Approximately 491 features have missing values, with missing rates ranging from 0.56% to 9.55%, and an average missing rate of 5.27%. The missing data mechanism in this dataset is likely Missing At Random (MAR), as confirmed by correlation matrix analysis and grouped statistical analysis with hypothesis testing.\n\nThe dataset was analyzed using 16S rRNA gene sequencing, specifically targeting the V4 region, to study microbial community structure and diversity. The data was optimized through imputation methods, including TOI, to handle the missing values effectively. The imputed data was validated by comparing it with original data distributions, ensuring that the imputation preserved the key distributional properties of the original dataset.\n\nThe dataset and its splits are not explicitly released in a separate public forum by our study. However, the original source, the IBDMDB within the iHMP, is publicly accessible. The specific splits and preprocessing steps used in our study are detailed in the methodology section of the publication, ensuring reproducibility. The use of public datasets like IBDMDB adheres to standard practices in the field, promoting transparency and reproducibility in research.",
  "optimization/algorithm": "The optimization algorithm discussed in this publication is an enhanced version of the Aquila Optimizer (AO), referred to as the Improved Aquila Optimizer (IAO). This algorithm is not entirely new but represents a significant improvement over the traditional AO. The IAO incorporates multiple strategies to address the limitations of the original AO, such as insufficient exploration in the early stages and the risk of getting trapped in local optima.\n\nThe IAO uses a Dynamically Adjusted Sobol Sequence (DASS) for population initialization, which enhances the diversity of the initial population. This method dynamically adjusts the search space based on feature importance, providing broader coverage of the solution space and maintaining better population diversity. Additionally, the IAO introduces adaptive parameter adjustment and dynamic mutation rates to balance global exploration and local exploitation effectively.\n\nThe IAO also employs a position update strategy that includes exploration and exploitation strategies. The exploration strategy updates the position of individuals using the current best individual as a reference point, expanding the search space and enhancing global exploration capabilities. The exploitation strategy, on the other hand, focuses on refining the search within the current solution space.\n\nThe IAO is used in conjunction with a Random Forest (RF) model for risk prediction. The RF model is an ensemble learning method known for its robustness and ability to handle complex datasets. The IAO-RF model demonstrates notable advantages in global hyperparameter tuning, improving model performance, particularly with complex datasets characterized by enhanced accuracy and stability.\n\nThe reason this optimization algorithm was not published in a machine-learning journal is that the focus of this publication is on its application in the context of microbiome data analysis and risk prediction for Inflammatory Bowel Disease (IBD). The enhancements to the AO are presented as part of a comprehensive framework, the CDORPF, which includes data optimization and risk prediction modules. The framework addresses specific challenges related to high-dimensional, sparse, and incomplete microbiome datasets, making it highly relevant to the field of microbiology and healthcare.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. Instead, it employs a comprehensive data optimization and risk prediction framework named CDORPF. This framework is divided into two main modules: data optimization and risk prediction.\n\nIn the data optimization module, triple optimization imputation (TOI) is used to handle missing data while preserving the biological characteristics of the gut microbiome data. Following this, an importance-weighted variational autoencoder with integrated evaluation (IWV AE) method is applied. This method incorporates feature importance ranking and a comprehensive scoring approach based on variational autoencoders (VAE) to enhance the dimensionality reduction process by retaining critical features. This results in a more complete dataset and a low-dimensional representation, which lays a solid foundation for improving algorithm efficiency and accuracy.\n\nIn the risk prediction module, the optimized data is classified using a random forest (RF) model. The improved aquila optimizer (IAO), enhanced with multiple strategies, is employed for global hyperparameter optimization of the RF model. The effectiveness of the CDORPF framework has been validated through multiple comparative experiments.\n\nThe framework does not aggregate predictions from multiple machine learning algorithms. Instead, it focuses on optimizing the data and using a single RF model for prediction. The training data used in the framework is independent, as it involves preprocessing steps that handle missing values and reduce dimensionality before applying the RF model. This ensures that the model is trained on a refined and optimized dataset, leading to improved accuracy and reliability.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithm. Initially, we addressed the issue of missing values in the dataset, which is common in microbiome data. Traditional methods like KNN and MICE were initially explored but yielded suboptimal results. To better preserve the biological characteristics of the gut microbiome data, we developed the Triple Optimization Imputation (TOI) method. This method integrates KNN, MICE, and Bayesian ridge regression, allowing it to capture both linear and nonlinear relationships in the data. The TOI method involved iterative refinement, where in each iteration, a Bayesian ridge regression model was used to predict and update the imputed values. This process continued until convergence was achieved or the maximum number of iterations was reached, resulting in a complete dataset that maintained its structural integrity.\n\nFollowing imputation, we focused on dimensionality reduction to handle the high dimensionality of the 16S rRNA data, which can introduce noise and hinder downstream predictions. We proposed the Importance-Weighted Variational Autoencoder (IWV AE) method, which builds on the Variational Autoencoder (V AE) framework. The IWV AE method incorporates feature importance ranking and a comprehensive scoring mechanism. Feature importance scores were computed using a Random Forest (RF) model, and features were ranked accordingly. This step ensured that the most critical features for classification tasks were prioritized. The V AE encoder mapped high-dimensional data into a low-dimensional latent space, while the decoder reconstructed the high-dimensional data from this latent representation. The reconstruction error was calculated to evaluate the effectiveness of the dimensionality reduction.\n\nThe preliminary screening stage involved selecting features at intervals of 1/10 of the total dimensionality. The trained V AE model was used to calculate reconstruction errors, and the RF model was trained on the dimensionally reduced data. Classification accuracy was evaluated through cross-validation, and a comprehensive score was introduced to balance reconstruction error and classification accuracy. This score helped avoid bias and overfitting, ensuring an optimal balance between preserving data features and predictive capability. The optimal latent dimensions were then recorded and further refined in a detailed screening phase to ensure precision. This comprehensive approach to data encoding and preprocessing laid a solid foundation for improving the efficiency and accuracy of the machine-learning algorithm.",
  "optimization/parameters": "In our study, we focused on optimizing four key hyperparameters for the Random Forest (RF) model. These parameters are n_estimators, max_depth, min_samples_split, and min_samples_leaf. The selection of these parameters was based on their significant impact on the model's performance and stability. The n_estimators parameter determines the number of trees in the forest, with an optimal range identified around 120 trees. The max_depth parameter controls the maximum depth of the trees, with a depth of 10 found to be optimal. The min_samples_split parameter, which sets the minimum number of samples required to split an internal node, was optimized around a value of 5. Lastly, the min_samples_leaf parameter, specifying the minimum number of samples required to be at a leaf node, was set to 2 for optimal performance. These parameters were chosen and tuned using the IAO (Improved Ant Optimization) method, which demonstrated superior performance compared to other optimization techniques such as Random Search (RS), Grid Search (GS), and Bayesian Optimization (BO). The IAO method effectively balances global exploration and local exploitation, ensuring that the model achieves high accuracy, precision, recall, and F1 scores.",
  "optimization/features": "The input features for our model are derived from a high-dimensional microbiome dataset. Initially, the dataset consists of a matrix with N original data items and M feature dimensions. However, to address the challenges of high dimensionality and sparsity, we employed a feature selection process.\n\nFeature selection was performed using a variational autoencoder (VAE) model during the preliminary screening stage. Features were selected at intervals of 1/10 of the total dimensionality. The VAE model was used to calculate reconstruction errors, and a random forest (RF) was trained on the dimensionally reduced data. This process involved evaluating classification accuracy through cross-validation and balancing reconstruction error and classification accuracy to avoid bias and overfitting.\n\nThe optimal latent dimensions were identified and recorded based on a comprehensive score that standardized both the reconstruction error and classification accuracy. This score ensured an optimal balance between preserving data features and predictive capability.\n\nIn the refined screening phase, a more detailed screening was conducted around the optimal latent dimensions identified in the preliminary screening. All steps from the preliminary screening were repeated within this refined range to ensure precision.\n\nTherefore, the number of features used as input varies depending on the dimensionality reduction process, but the final selected features are those that provide the best balance between reconstruction error and classification accuracy. Feature selection was performed using the training set only, ensuring that the model's performance is evaluated on unseen data during cross-validation.",
  "optimization/fitting": "In our study, we addressed the challenge of high-dimensional microbiome data, where the number of parameters often exceeds the number of training points. To mitigate overfitting, we employed several strategies.\n\nFirstly, we utilized a sensitivity analysis to understand the impact of different hyperparameters on the model's performance. This analysis helped us identify the optimal settings for parameters such as n_estimators, max_depth, min_samples_split, and min_samples_leaf, ensuring that the model was neither too complex nor too simple.\n\nSecondly, we implemented a comprehensive framework named CDORPF, which integrates data imputation, dimensionality reduction, and risk prediction. This framework effectively handles the incompleteness, high dimensionality, and sparsity of microbiome datasets, preserving the inherent structure of the data and minimizing biases from missing values.\n\nAdditionally, we employed the IAO (Improved Ant Optimization) algorithm to optimize the hyperparameters of our Random Forest (RF) model. IAO incorporates multiple strategies to enhance the traditional Ant Optimization, achieving a balance between local and global optimization. This includes dynamic adjustment of the search space based on feature importance, adaptive parameter adjustment, and dynamic mutation rates, which prevent the model from becoming trapped in local optima and ensure robust performance.\n\nTo further validate the effectiveness of our approach, we conducted comparative experiments using different optimization methods, including Random Search (RS), Grid Search (GS), and Bayesian Optimization (BO). The results demonstrated that IAO outperformed all other methods across various metrics, including accuracy, precision, recall, and F1-score, indicating that our model is well-tuned and not overfitted.\n\nMoreover, we ensured that our model did not underfit by carefully selecting and tuning the hyperparameters. The sensitivity analysis provided insights into how changes in hyperparameters affected model performance, guiding us to choose settings that captured the complexity of the data without oversimplifying it.\n\nIn summary, our approach combines sensitivity analysis, advanced optimization techniques, and a comprehensive framework to address the challenges of high-dimensional microbiome data, ensuring that our model is neither overfitted nor underfitted.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One key method involved the use of regularization during the data imputation process. Specifically, we integrated ridge regression into our triple optimization imputation (TOI) approach. Ridge regression helps mitigate the issue of multicollinearity in high-dimensional datasets by adding a penalty term to the regression equation. This regularization technique prevents the model from overfitting to noise in the sparse microbiome data, thereby enhancing the stability and accuracy of the imputation results.\n\nAdditionally, we utilized an importance-weighted variational autoencoder (IWV AE) for dimensionality reduction. This method not only reduces the dimensionality of the data but also retains critical features by incorporating feature importance ranking. By focusing on the most relevant features, we minimize the risk of overfitting and improve the model's generalization capability.\n\nFurthermore, our sensitivity analysis of hyperparameters for the random forest (RF) model played a crucial role in preventing overfitting. By carefully tuning parameters such as n_estimators, max_depth, min_samples_split, and min_samples_leaf, we ensured that the model was neither too complex nor too simple. For instance, we found that increasing n_estimators beyond a certain point did not significantly improve accuracy, indicating that adding more trees could lead to overfitting. Similarly, setting appropriate values for min_samples_split and min_samples_leaf helped in balancing the model's complexity and predictive power.\n\nOverall, these techniques collectively contributed to the stability and robustness of our model, ensuring that it performed well on both training and validation datasets.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and optimization parameters used in our study are thoroughly detailed within the publication. Specifically, the sensitivity analysis of key hyperparameters such as n_estimators, max_depth, min_samples_split, and min_samples_leaf for the Random Forest (RF) model is presented in Figure 8. This figure illustrates how varying these parameters affects the model's accuracy, providing insights into optimal settings.\n\nThe optimization process employed the Improved Aquila Optimizer (IAO) to fine-tune these hyperparameters. The results of this optimization are summarized in Table 3, which lists the optimized values for each hyperparameter. Additionally, comparative experiments using different optimization methods\u2014including Random Search (RS), Grid Search (GS), and Bayesian Optimization (BO)\u2014are detailed in Table 4. This table highlights the superior performance of IAO in achieving high accuracy, precision, recall, and F1 scores.\n\nThe dataset used for these experiments is publicly available and can be accessed via the provided link. This ensures reproducibility and allows other researchers to validate or build upon our findings. The data availability statement specifies the source of the dataset, making it accessible for further research.\n\nRegarding model files and optimization parameters, while the specific files are not directly linked within the publication, the detailed descriptions and results provided offer a comprehensive guide for replicating the experiments. The methods section outlines the steps taken for data optimization and risk prediction, including the use of triple optimization imputation (TOI) and the importance-weighted variational autoencoder (IWV AE) for dimensionality reduction. The risk prediction module utilizes the RF model with hyperparameters optimized through IAO.\n\nIn summary, while the exact model files may not be directly downloadable from the publication, the detailed methodology, results, and publicly available dataset enable full reproducibility of the experiments. The optimization parameters and configurations are clearly reported, ensuring transparency and facilitating further research.",
  "model/interpretability": "The CDORPF framework, while highly effective in handling complex microbiome data, is primarily designed for predictive performance rather than interpretability. The model integrates several components, including data imputation, dimensionality reduction, and risk prediction, which collectively enhance its accuracy and reliability. However, these components often contribute to a more black-box nature, making it challenging to provide clear, transparent examples of how specific decisions are made within the model.\n\nThe use of advanced techniques such as the Iterative Adaptive Optimization (IAO) for hyperparameter tuning and the Conditional Data Optimization and Risk Prediction Framework (CDORPF) itself focuses on optimizing performance metrics like accuracy, precision, recall, and F1-score. These optimizations are crucial for handling high-dimensional, sparse, and incomplete microbiome datasets but do not inherently provide interpretability.\n\nFor instance, the Random Forest (RF) model, which is a key component of CDORPF, is known for its ensemble nature, combining multiple decision trees. While individual decision trees can be interpreted, the ensemble nature of RF makes it difficult to trace back specific predictions to clear, understandable rules. Similarly, dimensionality reduction techniques like IWVAE (Improved Wasserstein Variational Autoencoder) and imputation methods like TOI (Two-Stage Imputation) are designed to enhance data quality and model performance but do not offer straightforward interpretability.\n\nIn summary, while CDORPF excels in predictive performance and handling complex datasets, it is not designed with interpretability as a primary goal. The model's components and optimizations contribute to its effectiveness but also to its black-box nature, making it challenging to provide transparent examples of its decision-making process.",
  "model/output": "The model is a classification model. It is designed to predict the risk of Inflammatory Bowel Disease (IBD) based on optimized data. The primary output of the model is a classification of samples into two categories: IBD patients and healthy controls. The model's performance is evaluated using metrics such as accuracy, precision, recall, and F1-score, which are commonly used in classification tasks. The Random Forest (RF) model, a key component of the CDORPF framework, is employed for this classification task. The framework includes data optimization steps, such as imputation and dimensionality reduction, to enhance the model's predictive capability. The final output is a classification result that indicates whether a given sample is likely to be from an IBD patient or a healthy control.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our method involved several key steps and experiments to ensure its robustness and effectiveness. Initially, we conducted comparative experiments using different imputation methods, including KNN, MICE, and our proposed TOI method. We employed various classifier models such as logistic regression (LR), SVM, MLP, XGBoost, LightGBM, and Random Forest (RF) to assess the performance of these imputation techniques. The results, presented in Figure 6, demonstrated that the TOI method achieved the highest AUC values across most models, indicating superior classification performance and excellent generalization capability.\n\nFollowing data imputation, we performed dimensionality reduction experiments using PCA, VAE, and our proposed IWVAE method. The RF model was applied to the full feature set as well as to the feature sets reduced by these methods. The experimental results, shown in Table 2, highlighted that IWVAE outperformed the other methods across all metrics, including accuracy, precision, recall, and F1 score. This demonstrated IWVAE's superior feature extraction capabilities and its significant enhancement of overall model performance.\n\nAdditionally, we conducted sensitivity analyses to evaluate the model\u2019s response to variations in input parameters and to identify key factors influencing performance. This involved tuning hyperparameters of the RF model, with the results presented in Table 3, which showed the optimal settings for parameters such as the number of trees, maximum depth, and minimum samples required to split an internal node.\n\nTo further validate the effectiveness of our approach, we compared it with several widely used models and optimization methods. For imputation methods, we used classifiers like LR, SVM, MLP, XGBoost, LightGBM, and RF. For dimensionality reduction, we compared PCA, VAE, and IWVAE. For parameter optimization, we evaluated methods such as Random Search (RS), Grid Search (GS), and Bayesian Optimization (BO). The results, presented in Table 4, showed that our optimized RF model achieved the highest performance metrics, confirming its superiority.\n\nOverall, the evaluation process involved rigorous comparative experiments, sensitivity analyses, and hyperparameter tuning to ensure that our method was thoroughly validated and optimized for handling high-dimensional, sparse datasets with missing values.",
  "evaluation/measure": "In our evaluation, we employed a comprehensive set of performance metrics to thoroughly assess the effectiveness of our models. These metrics include accuracy, precision, recall, F1-score, and the area under the receiver operating characteristic curve (AUC). These metrics are widely recognized and used in the literature, ensuring that our evaluation is both rigorous and comparable to other studies in the field.\n\nAccuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. It provides a general sense of how often the model is correct.\n\nPrecision, also known as the positive predictive value, indicates the proportion of true positive results among all positive results predicted by the model. It is crucial for understanding the reliability of positive predictions.\n\nRecall, or sensitivity, measures the proportion of actual positives that are correctly identified by the model. It is essential for evaluating the model's ability to detect positive cases.\n\nThe F1-score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. It is particularly useful when dealing with imbalanced datasets.\n\nThe AUC evaluates the model's ability to distinguish between classes across all possible classification thresholds. A higher AUC indicates better model performance.\n\nBy reporting these metrics, we ensure a comprehensive evaluation of our models' performance, covering various aspects of predictive accuracy and reliability. This set of metrics is representative of the standards in the literature, allowing for meaningful comparisons with other studies.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the effectiveness of our proposed models by comparing them with several widely used methods across different categories. For assessing the performance of imputation methods, we employed classifiers such as logistic regression, support vector machines, multilayer perceptron, XGBoost, LightGBM, and random forest. These classifiers were applied to datasets imputed using different techniques, including traditional methods like K-nearest neighbors (KNN) and multiple imputation by chained equations (MICE), as well as our proposed technique of order-invariant imputation (TOI). The results demonstrated that TOI achieved the highest area under the curve (AUC) values for most classifiers, indicating superior classification performance and robustness.\n\nFor dimensionality reduction, we compared our proposed method, the information-weighted variational autoencoder (IWV AE), against principal component analysis (PCA) and variational autoencoder (VAE). The experimental results showed that IWV AE outperformed both PCA and VAE across all metrics, including accuracy, precision, recall, and F1 score. This highlights the superior feature extraction capabilities of IWV AE, which significantly enhances the overall performance of the model.\n\nAdditionally, we compared different parameter optimization methods, including random search (RS), grid search (GS), and Bayesian optimization (BO), against our proposed intelligent adaptive optimization (IAO). The results indicated that IAO achieved the highest performance metrics, demonstrating its effectiveness in optimizing the hyperparameters of the random forest model.\n\nIn summary, our methods were rigorously compared against publicly available and simpler baseline methods on benchmark datasets, ensuring a comprehensive evaluation of their effectiveness. The comparisons across various imputation, dimensionality reduction, and parameter optimization techniques underscored the superiority of our proposed approaches in handling high-dimensional, sparse, and missing data, thereby enhancing the predictive performance of our models.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files used in this study are publicly available. The data can be accessed through the Integrated Human Microbiome Project (iHMP) database, specifically from the Inflammatory Bowel Disease Multi\u2019omics Database (IBDMDB). The dataset includes microbial community structure and diversity analyzed using 16S rRNA gene sequencing, targeting the V4 region. It comprises 178 participant records, with 137 individuals having Inflammatory Bowel Disease (IBD) and 41 without. Each record consists of 983 fields, where one field indicates the presence or absence of IBD, and the remaining 982 fields represent various microbial features.\n\nThe dataset is characterized by high-dimensional sparsity, the presence of missing values, differences in class distribution, and local density regions. Approximately 491 features have missing values, with missing rates ranging from 0.56% to 9.55%, and an average missing rate of 5.27%. This dataset is crucial for evaluating the effectiveness of the proposed CDORPF framework, which addresses issues related to incompleteness, high dimensionality, and sparsity within microbiome datasets.\n\nFor those interested in accessing the data, it is available at https://hmpdacc.org/ihmp/. The data is provided under terms that allow for its use in research, ensuring that it can be utilized by other researchers to validate and build upon the findings presented in this study."
}