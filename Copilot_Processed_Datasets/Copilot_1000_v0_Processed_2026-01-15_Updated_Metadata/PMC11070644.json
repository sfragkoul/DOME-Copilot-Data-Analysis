{
  "publication/title": "Prediction of breast cancer and axillary positive-node response to neoadjuvant chemotherapy based on multi-parametric magnetic resonance imaging radiomics models.",
  "publication/authors": "Lin Y, Wang J, Li M, Zhou C, Hu Y, Wang M, Zhang X",
  "publication/journal": "Breast (Edinburgh, Scotland)",
  "publication/year": "2024",
  "publication/pmid": "38696854",
  "publication/pmcid": "PMC11070644",
  "publication/doi": "10.1016/j.breast.2024.103737",
  "publication/tags": "- Breast cancer\n- Radiomics\n- Machine learning\n- MRI\n- Predictive modeling\n- Neoadjuvant chemotherapy\n- Pathological complete response\n- Axillary lymph node response\n- Feature extraction\n- Clinical outcomes\n\nNot applicable",
  "dataset/provenance": "The dataset used in this study was sourced from breast cancer patients treated with neoadjuvant chemotherapy (NAC) and subsequent surgery. The data collection period spanned from March 2016 to May 2021. Initially, 653 patients were retrieved, but after applying eligibility and exclusion criteria, 268 patients were enrolled. Among these, 240 patients had clinical positive-node status.\n\nThe eligibility criteria included having core needle biopsy-proven invasive breast cancer, completion of systemic NAC treatment followed by surgery, and having undergone a breast MRI with a 3.0-T MRI prior to NAC within 2 weeks. Additionally, baseline data had to be complete. Exclusion criteria involved bilateral breast cancer, non-adherence to the standard NAC regimen, inadequate MRI quality, or the presence of metastatic disease or other malignant tumors.\n\nThe dataset includes baseline data such as age, menopausal status, clinical tumor TNM staging, and histopathological results including tumor type, receptor status, and Ki-67 index. The study aimed to identify radiomics predictors of axillary positive-node complete response (apCR) and breast pathological complete response (bpCR) based on multi-parametric MRI and to construct machine learning models combined with clinicopathologic characteristics. This dataset has not been used in previous publications by the community.",
  "dataset/splits": "The dataset was split into a training set and a testing set using stratified nested cross-validation (NCV). This process involved a 5-fold outer loop and a 4-fold inner loop. In each cross-validation iteration, 80% of the patients were used as the training dataset, while the remaining 20% served as the independent testing dataset. The inner loop further divided the training set into four folds, with 64% of the patients used for training the machine learning model and the remaining 16% used to determine the best hyperparameters. This entire construction process was repeated 100 times to ensure the robustness of the model. The distribution of data points in each split maintained a similar positive-negative sample ratio.",
  "dataset/redundancy": "The dataset used in this study consisted of 268 consecutive patients. To ensure the robustness and reliability of the models, the patients were randomly split into a training set and a testing set using stratified nested cross-validation (NCV). This method helps maintain a similar positive-negative sample ratio in both sets.\n\nThe NCV framework included a 5-fold outer loop and a 4-fold inner loop. In each cross-validation iteration, 80% of the patients were used as the training dataset, while the remaining 20% served as the independent testing dataset. This independence was enforced by ensuring that the testing set was not used during the training phase, thus preventing data leakage and ensuring an unbiased evaluation of model performance.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the field of breast cancer prediction. The use of NCV ensures that the models are trained and tested on diverse subsets of the data, which helps in generalizing the findings and reducing the risk of overfitting. This approach is particularly important in medical research, where the reliability and reproducibility of results are crucial.\n\nNot applicable",
  "dataset/availability": "The data used in this study is not publicly available. However, it can be obtained from the corresponding author upon reasonable request. This approach ensures that the data is shared responsibly and ethically, adhering to the guidelines set by the Institutional Review Board of the First Affiliated Hospital, Sun Yat-sen University. The data includes detailed clinicopathologic characteristics and radiomics features extracted from MRI scans of breast cancer patients treated with neoadjuvant chemotherapy. The specific data splits used for training and testing the models are not released publicly to maintain the integrity and confidentiality of the patient information. This method of data sharing allows for verification and potential replication of the study's findings while protecting patient privacy.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and widely recognized in the field. The algorithms employed include support vector machine (SVM) with a radial kernel, logistic regression (LR), random forest (RF), ada-boost classifier (ABC), linear discriminant analysis (LDA), and multi-layer perception (MLP). These algorithms are not new but are robust and commonly used for classification tasks in various domains, including medical imaging and predictive modeling.\n\nThe choice of these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to capture intricate patterns within the data. The SVM, for instance, is known for its effectiveness in high-dimensional spaces and is particularly useful when the number of dimensions exceeds the number of samples. Logistic regression is a straightforward yet powerful algorithm for binary classification problems. Random forest and ada-boost are ensemble methods that combine multiple decision trees to improve predictive accuracy and control over-fitting. Linear discriminant analysis is useful for finding a linear combination of features that characterizes or separates two or more classes. Multi-layer perception, a type of artificial neural network, is capable of modeling complex relationships within the data.\n\nGiven that these algorithms are well-documented and have been extensively studied, there was no need to publish them in a machine-learning journal. Instead, the focus of this study was on applying these algorithms to a specific medical problem\u2014predicting breast cancer and axillary positive-node response to neoadjuvant chemotherapy (NAC)\u2014and evaluating their performance in this context. The study aimed to demonstrate the practical utility of these algorithms in a clinical setting, rather than introducing new machine-learning techniques.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the robustness and accuracy of our machine-learning models. Initially, we extracted a comprehensive set of radiomics features from MRI images using the Pyradiomics module. This process involved resampling the voxel size of each MRI image to 1 \u00d7 1 \u00d7 1 mm\u00b3 and fixing the bin width to a scale of 25 to ensure uniform scaling ratios across images. We also applied Laplacian Gaussian and wavelet-based filters to the original MRI images to extract additional features.\n\nWe obtained a total of 4198 features for each patient by combining radiomics features with clinicopathologic indexes such as age, menopausal status, IHC reports, and clinical anatomic TNM stage. To handle the high dimensionality and potential multicollinearity among these features, we performed feature normalization. This involved subtracting the mean value from each feature value and then dividing by the standard deviation, resulting in features with a mean of 0 and a standard deviation of 1.\n\nTo select the most relevant features, we employed analysis of variance (ANOVA) and least absolute shrinkage and selection operator (LASSO) logistic regression. These methods helped identify features most correlated with the clinical outcomes of interest, specifically breast pathological complete response (bpCR) and axillary pathological complete response (apCR). We also assessed the multicollinearity of selected features using correlation analysis, ensuring that features with a correlation coefficient above 0.8 or below -0.8 were not included simultaneously.\n\nThe patients were randomly split into training and testing sets using stratified nested cross-validation (NCV) with a similar positive-negative sample ratio. The NCV framework included a 5-fold outer loop and a 4-fold inner loop, ensuring that the models were trained and validated on diverse subsets of the data. This rigorous preprocessing and encoding pipeline enabled us to construct robust and reliable machine-learning models for predicting breast cancer and axillary node responses to neoadjuvant chemotherapy.",
  "optimization/parameters": "In our study, we utilized a total of 4198 radiomics features extracted from MRI images for each patient. To identify the most significant features, we employed analysis of variance (ANOVA) and least absolute shrinkage and selection operator (LASSO) regression. This process resulted in the selection of 24 optimal features for predicting axillary positive-node response and 28 features for predicting breast cancer response. These selected features included a mix of radiomics and clinicopathologic characteristics. The feature selection process ensured that the chosen features were independent of each other, with no correlation coefficient values exceeding 0.8. This rigorous selection method helped in building robust machine learning models for predicting clinical outcomes.",
  "optimization/features": "In the study, a total of 4198 features were initially extracted for each patient. These features included both radiomics and clinicopathologic features. To ensure the most relevant features were used, feature selection was performed using the training set only. This process involved using analysis of variance (ANOVA) and least absolute shrinkage and selection operator (LASSO) logistic regression to identify the features most correlated with the clinical outcomes of interest. For predicting axillary positive-node response, 24 optimal features were selected. For predicting breast cancer response, 28 optimal features were chosen. The selected features were then used to construct the machine learning models, ensuring that the feature selection process was conducted solely on the training data to maintain the integrity and robustness of the models.",
  "optimization/fitting": "The study employed a robust approach to ensure that the models were neither overfitting nor underfitting the data. To address the potential issue of having a large number of parameters relative to the number of training points, several strategies were implemented.\n\nFirstly, feature selection techniques such as ANOVA and LASSO logistic regression were used to identify the most significant features. This process helped in reducing the dimensionality of the feature set, ensuring that only the most relevant features were used in model construction. By doing so, the risk of overfitting was mitigated.\n\nSecondly, the models were developed using a stratified nested cross-validation (NCV) framework. This involved an outer loop with 5 folds and an inner loop with 4 folds. The inner loop was used for hyperparameter tuning via grid searching, while the outer loop provided an independent testing set to evaluate model performance. This approach ensured that the models were trained and validated on different subsets of the data, reducing the likelihood of overfitting.\n\nAdditionally, the entire model construction process was repeated 100 times to ensure the robustness and stability of the models. This repetition helped in assessing the consistency of the models' performance across multiple iterations.\n\nTo rule out underfitting, multiple classification algorithms were employed, including support vector machine (SVM) with a radial kernel, logistic regression (LR), random forest (RF), ada-boost classifier (ABC), linear discriminant analysis (LDA), and multi-layer perception (MLP). The use of diverse algorithms ensured that the models could capture a wide range of patterns in the data, reducing the risk of underfitting.\n\nFurthermore, the performance of the models was evaluated using multiple metrics, including the area under the receiver operating characteristic curve (AUC), accuracy, sensitivity, and specificity. The multi-layer perception (MLP) model, in particular, demonstrated strong performance metrics, indicating that the models were well-fitted to the data without underfitting.\n\nIn summary, the combination of feature selection, nested cross-validation, repeated model construction, and the use of multiple classification algorithms ensured that the models were neither overfitting nor underfitting the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was stratified nested cross-validation (NCV), which involved a 5-fold outer loop and a 4-fold inner loop. This approach helped in effectively splitting the data into training and testing sets while maintaining a similar positive-negative sample ratio, thus reducing the risk of overfitting.\n\nAdditionally, we utilized feature selection techniques such as analysis of variance (ANOVA) and least absolute shrinkage and selection operator (LASSO) logistic regression. These methods helped in identifying the most significant features correlated with the clinical outcomes, thereby reducing the dimensionality of the feature set and mitigating overfitting.\n\nFurthermore, we implemented a grid-searching strategy within the inner loop of the cross-validation to optimize the hyperparameters of our models. This process ensured that the models were tuned to the best possible parameters, enhancing their generalization capability.\n\nTo further validate the stability and robustness of our models, we repeated the entire construction process 100 times. This extensive repetition helped in confirming that our models were not overfitting to any specific subset of the data.\n\nOverall, these techniques collectively contributed to the prevention of overfitting and ensured that our models were reliable and generalizable.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are not explicitly detailed in the provided information. However, it is mentioned that a grid-searching strategy and a 5-fold cross-validation were employed to choose the best optimal hyperparameters fitting the model. This process was repeated 100 times to ensure the robustness of the model.\n\nRegarding the model files and optimization parameters, there is no specific mention of where these can be accessed or under what license. The study focuses on the methodology and results of using machine learning models to predict breast cancer and axillary positive-node response to neoadjuvant chemotherapy, rather than providing access to the actual model files or optimization parameters.\n\nFor those interested in replicating the study or using similar models, the detailed methodology and feature selection processes are described, which can serve as a guide. However, the exact configurations and parameters used for optimization are not made publicly available in the provided information.",
  "model/interpretability": "The models developed in our study are not entirely black-box systems. To ensure interpretability, we evaluated the feature importance in our models based on their weight importance. For the prediction of both axillary positive-node complete response (apCR) and breast pathological complete response (bpCR), all selected features were found to be necessary as they significantly influenced the model output.\n\nTo quantify the contribution of each feature, we used the Kernel-explainer method in the SHAP (SHapley Additive exPlanations) package. This method calculates the contribution weighting coefficient of each feature in our predictive models. This approach provides a way to understand the impact of individual features on the model's predictions, making the models more transparent.\n\nFurthermore, we ranked the feature weight importance for two groups of features: clinicopathologic characteristics and radiomics features. From the ranks, it was evident that clinicopathologic characteristics had a high proportion in the prediction model contribution for both apCR and bpCR models. This indicates that clinical factors play a significant role in the model's decision-making process, alongside radiomics features.\n\nBy providing these insights into feature importance, we aim to enhance the interpretability of our models, allowing clinicians to understand which factors are most influential in predicting treatment responses. This transparency is crucial for building trust in the model's predictions and for integrating the model into clinical decision-making processes.",
  "model/output": "The model developed in our study is a classification model. It is designed to predict two specific outcomes: axillary pathological complete response (apCR) and breast pathological complete response (bpCR) in patients undergoing neoadjuvant chemotherapy (NAC). The model uses various machine learning algorithms, including support vector machine (SVM), logistic regression (LR), random forest (RF), ada-boost classifier (ABC), linear discriminant analysis (LDA), and multi-layer perception (MLP). These algorithms were evaluated based on their ability to classify patients into those who achieved complete response and those who did not.\n\nThe performance of the models was assessed using several metrics, including the area under the receiver operating characteristic curve (AUC), accuracy, sensitivity, and specificity. For instance, the MLP model demonstrated strong classification performance, with an AUC of 0.825 for predicting apCR and 0.852 for predicting bpCR. These metrics indicate the model's effectiveness in correctly classifying patients based on their response to NAC.\n\nThe models were constructed using a combination of radiomics features extracted from multi-parametric MRI and clinicopathologic characteristics. This integration allowed for a comprehensive assessment of the features that contribute to the prediction of treatment response. The use of 100-round 5-fold nested cross-validation ensured the stability and robustness of the models, providing reliable classification outcomes.\n\nIn summary, the output of our model is a classification of patients into those who are likely to achieve a complete response to NAC and those who are not, based on a combination of radiomics and clinical features. This classification can aid in personalized treatment planning and improve the assessment of treatment efficacy.",
  "model/duration": "The execution time for the model development process was not explicitly stated. However, the robustness of the models was ensured by repeating the entire construction process 100 times using stratified nested cross-validation with a 5-fold outer loop and a 4-fold inner loop. This extensive validation process would have required a significant amount of computational time, but the exact duration is not specified. The models were developed using six robust classification algorithms, including support vector machine, logistic regression, random forest, ada-boost classifier, linear discriminant analysis, and multi-layer perception. The performance of each model was evaluated using metrics such as the receiver operating characteristic curve, area under the curve, accuracy, sensitivity, and specificity. The multi-layer perception model outperformed the other algorithms in predicting both axillary pathological complete response and breast pathological complete response.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in our study was designed to ensure the robustness and reliability of our models. We utilized a stratified nested cross-validation (NCV) approach, which involved a 5-fold outer loop and a 4-fold inner loop. This method helped in maintaining a similar positive-negative sample ratio across different folds.\n\nIn each cross-validation iteration, 80% of the patients were used as the training dataset to establish the model, while the remaining 20% served as the independent testing dataset to assess model performance. Within the inner loop, a grid-searching strategy with 5-fold cross-validation was used to select the optimal hyperparameters for the models. This involved training the machine learning model on four folds (64% of the patients) and using the remaining fold (16% of the patients) to determine the best hyperparameters.\n\nTo further ensure the robustness of our models, the entire construction process was repeated 100 times. This extensive repetition helped in validating the consistency and reliability of our models' performance.\n\nWe evaluated the models using several metrics, including the receiver operating characteristic (ROC) curve, the area under the curve (AUC), accuracy, sensitivity, and specificity. These metrics provided a comprehensive assessment of the models' performance in predicting the outcomes of interest.\n\nAdditionally, we used the Kernel-explainer method in the SHAP Package to calculate the contribution weighting coefficient of each feature in our predictive models. This allowed us to understand the importance of individual features in the model's predictions.",
  "evaluation/measure": "In our study, we employed several performance metrics to comprehensively evaluate the effectiveness of our models. These metrics include the receiver operating characteristic (ROC) curve, the area under the curve (AUC), accuracy, sensitivity, and specificity. The ROC curve provides a visual representation of the trade-off between the true positive rate and the false positive rate, while the AUC quantifies the overall ability of the model to discriminate between positive and negative classes. Accuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, also known as the true positive rate, indicates the model's ability to correctly identify positive cases, whereas specificity, or the true negative rate, reflects the model's capability to correctly identify negative cases.\n\nThese metrics are widely recognized and used in the literature for evaluating the performance of machine learning models, particularly in medical and diagnostic contexts. They offer a robust framework for assessing the reliability and effectiveness of our models in predicting clinical outcomes. By reporting these metrics, we aim to provide a clear and comprehensive understanding of our models' performance, enabling comparisons with other studies and ensuring transparency in our evaluation process.",
  "evaluation/comparison": "Not applicable. The study focused on developing and evaluating models using specific clinicopathologic and radiomics features to predict breast cancer and axillary positive-node response to neoadjuvant chemotherapy (NAC). The evaluation involved comparing different types of models (clinical, radiomics, and combined radiomics-clinical) using various machine learning algorithms. However, there was no mention of comparing these models to publicly available methods or simpler baselines on benchmark datasets. The primary emphasis was on the performance of the developed models within the study's framework.",
  "evaluation/confidence": "The evaluation of our models included several performance metrics, and we ensured that these metrics were robust and reliable. For instance, the area under the curve (AUC) for our models was reported with 95% confidence intervals. This provides a clear indication of the precision of our AUC estimates.\n\nStatistical significance was assessed using appropriate tests. For example, the Delong test was used to compare the AUCs of different models, ensuring that any claimed superiority was backed by statistical evidence. Additionally, we used the Mann-Whitney U test or Student's t-test for continuous variables and the Chi-squared test or Fisher's exact test for categorical variables, all of which helped in determining the statistical significance of our findings.\n\nTo further ensure the stability and robustness of our models, we employed a 100-round 5-fold nested cross-validation (NCV). This rigorous approach helped in validating the performance of our models across multiple iterations, reducing the risk of overfitting and providing a more reliable estimate of their generalizability.\n\nIn summary, our evaluation process included confidence intervals for key metrics and statistical tests to ensure the significance of our results. This comprehensive approach allows us to confidently claim the superiority of our methods over others and baselines.",
  "evaluation/availability": "Not applicable."
}