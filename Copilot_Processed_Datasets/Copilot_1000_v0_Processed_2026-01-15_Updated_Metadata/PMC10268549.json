{
  "publication/title": "Predicting Domestic Abuse (Fairly) and Police Risk Assessment.",
  "publication/authors": "Turner E, Brown G, Medina-Ariza J",
  "publication/journal": "Psychosocial intervention",
  "publication/year": "2022",
  "publication/pmid": "37361012",
  "publication/pmcid": "PMC10268549",
  "publication/doi": "10.5093/pi2022a11",
  "publication/tags": "- Violence dom\u00e9stica\n- Evaluaci\u00f3n de riesgos\n- Aprendizaje autom\u00e1tico\n- Justicia algor\u00edtmica\n- Polic\u00eda\n- Predicting Domestic Abuse\n- Police Risk Assessment\n- Machine Learning Models\n- Logistic Regression\n- Risk Prediction",
  "dataset/provenance": "The dataset used in this study was provided by a large metropolitan police force in the UK. This dataset includes all individuals within the jurisdiction who had an administrative (police) record of domestic abuse during a specific period. The data covers approximately 350,000 domestic violence incidents reported between 2011 and 2016.\n\nFrom this larger dataset, an analytical dataset of around 84,000 index events was constructed. The selection criteria for this analytical dataset included complete data in key fields such as abuser and victim identifiers, victim-to-abuser relationship type, and data linking fields that permitted identification of whether charges were associated with an incident. Only cases where officer risk grading had been specified were retained, which accounted for 84% of the incidents. Missing data in other fields was handled, but the aforementioned fields were considered integral to the analysis.\n\nThe dataset focuses on the primary victim at the index incident, as they would have provided the answers to the DASH questionnaire, which is victim-focused. A small proportion of dyads (abuser-victim pairs) were recorded as being involved in more than one incident in a day. Since the exact times of these incidents were unknown, only one incident per day was kept to avoid potential duplication. Additionally, cases where either the victim or abuser were dead or too ill were excluded, accounting for a tiny proportion (0.01%) of the data.\n\nThe final dataset was split into intimate partner violence (IPV) and non-IPV cases. The IPV dataset included current/ex spouse and partner, girlfriend, and boyfriend relationship types, forming approximately 60,000 unique dyads. The non-IPV dataset contained around 24,000 unique dyads, which mostly entail adolescent-to-parent violence. This split was necessary because different risk factors are relevant when predicting IPV and non-IPV events, although UK protocols for risk assessment do not distinguish between various forms of abuse.",
  "dataset/splits": "The dataset was split into two main categories: intimate partner violence (IPV) and non-IPV cases. The IPV dataset included relationships such as current/ex spouse and partner, girlfriend, and boyfriend, comprising approximately 60,000 unique abuser-victim dyads. The non-IPV dataset, which primarily involved adolescent-to-parent violence, contained around 24,000 unique dyads. This split was necessary because different risk factors are relevant when predicting IPV and non-IPV events, despite UK protocols for risk assessment not distinguishing between various forms of abuse.\n\nThe dataset construction involved several steps to ensure the integrity and representativeness of the data. Initially, the dataset included around 84,000 index events from a larger pool of approximately 350,000 domestic violence incidents reported between 2011 and 2016. Only incidents with complete data in key fields, such as abuser and victim identifiers, victim-to-abuser relationship type, and officer risk grading, were retained. This resulted in complete data for 84% of the incidents.\n\nTo handle multiple incidents involving the same abuser-victim pair on a single day, only one incident was kept, and the rest were excluded. Additionally, cases where either the victim or abuser was dead or too ill were removed. The final dataset focused on incidents occurring between 2013 and 2015, excluding incidents from 2011, 2012, and 2016, which further reduced the number of incidents by 46%.\n\nFor the remaining abuser-victim pairs, 37% were involved in more than one incident. In such cases, one incident was randomly selected to represent the index incident, ensuring that the dataset was representative of the variety of incidents police encounter daily. This approach also allowed for the evaluation of the importance of domestic abuse history in predicting subsequent incidents. By using only one event per dyad, the assumption of independence of observations was preserved, which is a requirement for logistic regression modeling.",
  "dataset/redundancy": "The dataset was constructed from approximately 350,000 domestic violence incidents responded to by a large metropolitan police force in the UK between 2011 and 2016. To create the analytical dataset of around 84,000 index events, we focused on incidents with complete data in key fields, such as abuser and victim identifiers, relationship type, and data linking fields for incident charges. We ensured that officer risk grading was specified for all retained cases, which accounted for 84% of the incidents.\n\nTo address redundancy, we handled cases where abuser-victim pairs were involved in multiple incidents in a single day. Since the exact times of these incidents were unknown, we could not determine their order and thus could not distinguish between duplicated records and genuine multiple incidents. Consequently, we retained only one incident per day for such pairs. Additionally, we excluded a minimal proportion of cases where either the victim or abuser was deceased or too ill to be considered.\n\nWe further reduced the dataset by focusing on incidents occurring between 2013 and 2015, excluding those from 2011, 2012, and 2016. This reduction was necessary to create predictor variables from two years of domestic abuse history and to define the outcome based on subsequent incidents within one year after the index incident. This timeframe restriction led to a 46% reduction in the number of incidents.\n\nFor abuser-victim pairs involved in more than one incident, we randomly selected one incident to represent the index incident. This approach ensured that the dataset was representative of the variety of incidents police encounter daily and allowed for the evaluation of the importance of domestic abuse history in predicting subsequent incidents. By using only one event per dyad, we preserved the assumption of independence of observations, which is crucial for logistic regression modeling.\n\nThe dataset was then split into intimate partner violence (IPV) and non-IPV cases. The IPV dataset included approximately 60,000 unique dyads, while the non-IPV dataset contained around 24,000 unique dyads. This split was necessary because different risk factors are likely relevant for predicting IPV and non-IPV events, despite UK protocols for risk assessment not distinguishing between various forms of abuse.\n\nThe training and test sets were ensured to be independent by using only one event per abuser-victim dyad. This method prevented any overlap between the sets, maintaining the integrity of the model's predictive capabilities. The distribution of the dataset compares favorably to previously published machine learning datasets in the domain of domestic abuse prediction, as it includes a comprehensive set of predictor variables drawn from police and census data. These variables have been identified in the literature as relevant risk factors, ensuring the robustness and relevance of the dataset for predictive modeling.",
  "dataset/availability": "The data used in this study is not publicly available. The dataset was provided by a large metropolitan police force in the UK, and its release is restricted due to confidentiality and ethical considerations. Our data sharing agreement and University Ethics Committee approval prevent us from identifying the police force providing the data or making the dataset publicly accessible.\n\nThe data includes all people in the jurisdictional population for which there was an administrative (police) record of domestic abuse between 2011 and 2016. The original dataset consisted of approximately 350,000 domestic violence incidents, but the analytical dataset was constructed from around 84,000 index events. This subset was chosen based on the completeness of key fields such as abuser and victim identifiers, victim-to-abuser relationship type, and data linking fields that permitted identification of whether or not there were charges associated with an incident.\n\nThe dataset was further refined by focusing on the primary victim at the index incident, excluding cases where either the victim or abuser was dead or too ill, and ensuring that only one incident per abuser-victim pair was included to preserve the assumption of independence of observations. The dataset was also split into intimate partner violence (IPV) and non-IPV cases to account for different risk factors relevant to each type of abuse.\n\nGiven the sensitive nature of the data, it is not released in a public forum. The restrictions on data availability were enforced through legal agreements and ethical guidelines to protect the privacy and security of the individuals involved.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study primarily focuses on logistic regression with elastic net regularization. This method is well-established in the field of statistics and machine learning, known for its effectiveness in both variable selection and regularization. It combines the penalties of Lasso (L1) and Ridge (L2) regression, providing a balanced approach to handling multicollinearity and feature selection.\n\nIn addition to logistic regression, we also explored other machine learning models, including naive Bayes, tree-augmented naive Bayes, random forests, gradient boosting, and weighted subspace random forests. These models were chosen for their diverse approaches to classification and their ability to handle different types of data and relationships within the data.\n\nThe weighted subspace random forests, in particular, were included because they often outperform traditional random forests when dealing with a large number of unimportant variables. This method assigns varying probabilities for subset selection to each variable based on its relationship with the outcome, ensuring that variables with weak relations are less likely to be selected.\n\nThe discretization methods used for naive Bayes and tree-augmented naive Bayes included FUSINTER and proportional discretization. FUSINTER is a supervised method that considers the dependent variable when choosing cut-points, while proportional discretization is unsupervised and aims to balance bias and variance by adjusting the number of cut-points.\n\nThe choice of these algorithms was driven by the need to identify high-risk victims of domestic abuse accurately. The logistic regression with elastic net regularization emerged as the best-performing model, with area under the curve (AUC) values of .748 for intimate partner violence (IPV) and .763 for non-IPV samples. These results indicate that our models surpassed the expected performance of domestic abuse risk assessment instruments deemed 'good' in the literature.\n\nThe algorithms used are not new but are well-established in the field. They were chosen for their proven effectiveness in handling the specific challenges of domestic abuse prediction. The focus of our study was on improving the process for identifying at-risk victims rather than developing new machine-learning models. Therefore, the algorithms were implemented and evaluated within the context of our specific research goals, and the results were published in a relevant journal focusing on psychosocial intervention.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms employed. We began by addressing missing data, which was minimal but present in certain fields. Specifically, postcodes were missing for 5.9% of the records, preventing the identification of geographical demographics in those cases. Additionally, there were small proportions of missing data for the age and gender of both victims and abusers, each below 6%. To handle these missing values, we utilized multivariate imputation by chained equations, a robust method that helps maintain the integrity of the dataset.\n\nNumeric variables required discretization before applying naive Bayes and tree-augmented naive Bayes methods. We compared two discretization techniques: FUSINTER and proportional discretization. FUSINTER is a supervised method that considers the dependent variable when selecting cut-points, whereas proportional discretization is unsupervised and aims to minimize the risk of classifying instances using intervals that contain decision boundaries. This approach balances bias and variance, ensuring more accurate model performance.\n\nVariable selection was necessary for logistic regression, naive Bayes, and tree-augmented naive Bayes models. For logistic regression, we employed the elastic net, an embedded feature selection method that combines the penalties of Lasso and Ridge regression. This technique helps in regularizing the model and selecting relevant variables. For naive Bayes and tree-augmented naive Bayes, we applied forward feature selection to identify the most influential predictors.\n\nTo evaluate the relative influence of variables in the logistic regression models, we reported standardized coefficients. Standardization involved subtracting the mean and dividing by two standard deviations, which allowed for a fair comparison of variables with different units. Additionally, we provided odds-ratios for the unstandardized variables to maintain the interpretability of meaningful units, such as age in years.\n\nOverall, our data encoding and preprocessing steps were designed to enhance the performance and interpretability of the machine-learning models, ensuring that they could effectively rank individuals based on their risk of revictimization.",
  "optimization/parameters": "In our study, we employed logistic regression with elastic net regularization, which is an embedded feature selection method. This approach allowed us to handle a large number of potential predictors efficiently. For the intimate partner violence (IPV) model, a total of 80 variables were selected, while the non-IPV model included 17 variables. These variables were chosen based on their influence and relevance to the model.\n\nThe selection of variables was guided by the elastic net method, which inherently performs feature selection by shrinking some coefficients to zero. This process helps in identifying the most relevant predictors while mitigating overfitting. Additionally, we applied forward feature selection for naive Bayes and tree-augmented naive Bayes models, ensuring that only the most influential variables were retained.\n\nTo ensure the robustness of our model, we used 500-times 2-fold cross-validation. This rigorous validation process helped in assessing the model's performance and stability. The mean and standard deviation of the area under the ROC curve (AUC) were reported, providing a comprehensive evaluation of the model's predictive capability.\n\nIn summary, the number of parameters (p) used in the model varied between the IPV and non-IPV datasets, with 80 and 17 variables selected respectively. The selection of these parameters was driven by the elastic net regularization method and validated through extensive cross-validation.",
  "optimization/features": "In our study, we utilized a comprehensive set of variables to build our predictive models. The exact number of features (f) varied between the intimate partner violence (IPV) and non-IPV models, with 80 and 17 variables selected respectively. Feature selection was indeed performed using elastic net regularization for the logistic regression model, which is an embedded feature selection method. This process was conducted using only the training set to ensure that the selection was unbiased and to prevent data leakage. Additionally, forward feature selection was applied for naive Bayes and tree-augmented naive Bayes models. The goal of these feature selection techniques was to identify the most influential variables and to improve the model's predictive performance.",
  "optimization/fitting": "The fitting method employed in this study utilized logistic regression paired with elastic net regularization. This approach is particularly useful when dealing with a large number of predictors relative to the number of training points, as it helps to mitigate overfitting by imposing penalties on the coefficients.\n\nElastic net regularization combines the properties of both L1 (lasso) and L2 (ridge) regularization, which helps in selecting relevant variables and shrinking the coefficients of less important ones. This method was chosen to handle the high-dimensional data effectively, ensuring that the model generalizes well to unseen data.\n\nTo rule out overfitting, we employed 500-times 2-fold cross-validation. This rigorous cross-validation technique ensures that the model's performance is evaluated on multiple splits of the data, providing a robust estimate of its generalization capability. The mean and standard deviation of the model's performance metrics, such as the area under the ROC curve (AUC), were computed across these validation folds. This process helps in assessing the model's stability and reliability, ensuring that it does not overfit to the training data.\n\nUnderfitting was addressed by carefully selecting the most influential variables and ensuring that the model complexity was appropriate for the data. The elastic net regularization method helps in balancing the bias-variance trade-off, preventing the model from being too simple and thus underfitting the data. Additionally, the use of standardised coefficients allowed for a fair comparison of the relative influence of different predictors, ensuring that the model captures the essential patterns in the data without being overly simplistic.\n\nIn summary, the fitting method involved logistic regression with elastic net regularization, supported by extensive cross-validation to rule out overfitting and underfitting. This approach ensures that the model is both robust and generalizable, providing reliable predictions for revictimisation risk.",
  "optimization/regularization": "In our study, we employed regularization techniques to prevent overfitting. Specifically, we used the elastic net method, which combines the penalties of both L1 (lasso) and L2 (ridge) regularization. This approach helps to shrink some coefficients to zero, effectively performing variable selection, and others to small values, thereby reducing the complexity of the model and preventing overfitting. The elastic net was integrated with logistic regression to balance between bias and variance, ensuring that our model generalizes well to unseen data. Additionally, we utilized 500-times 2-fold cross-validation to further validate the model's performance and stability. This rigorous cross-validation process helps to ensure that the model's predictions are reliable and not merely a result of overfitting to the training data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule are reported in the publication. The best-performing model, logistic regression with elastic net, was selected based on cross-validation results. For algorithms with hyperparameters that required tuning, a nested set of cross-validation was used to determine the best hyperparameters associated with the highest cross-validated AUC.\n\nThe model files and specific optimization parameters are not explicitly detailed in the text. However, the process involved building models on training data and evaluating them on separate test data that was unseen during the training stage. The mean and standard deviation of AUCs from cross-validation results are provided, along with the rate-wise mean ROC curve and 95% confidence intervals across cross-validation runs.\n\nRegarding availability and licensing, the publication does not specify where the model files or detailed optimization parameters can be accessed or under what license they might be available. Therefore, it is not clear if these resources are publicly accessible or if there are any restrictions on their use.",
  "model/interpretability": "The model employed in this study is not a black-box but rather a transparent one, primarily due to the use of logistic regression with elastic net regularization. This type of model is inherently interpretable, as it provides clear insights into the relationship between the predictors and the outcome.\n\nOne of the key aspects of interpretability in our model is the ability to identify the most influential variables. In our case, almost all of the most influential variables were static, with a particular emphasis on variables related to the abuser's criminal history. This dominance in count and influence highlights the significance of these factors in predicting revictimisation.\n\nThe model selected a total of 80 variables for the intimate partner violence (IPV) sample and 17 variables for the non-IPV sample. However, not all variables were equally important. To focus on the most relevant predictors, we presented only those with a standardized coefficient magnitude exceeding 0.1. This approach ensures that the most impactful variables are clearly visible and can be easily interpreted.\n\nAdditionally, to enhance the linearity in the logit, all variables pertaining to crime counts and mean Office for National Statistics (ONS) scores were log-transformed. This transformation aids in making the model more interpretable by ensuring that the relationships between the predictors and the outcome are more linear and easier to understand.\n\nIt is also worth noting that elastic net logistic regression does not return p-values or confidence intervals on the coefficients. This means that the interpretation of the model focuses on the magnitude and direction of the coefficients rather than statistical significance. This approach allows for a more straightforward understanding of how each variable contributes to the prediction of revictimisation.\n\nIn summary, the model's transparency is achieved through the use of logistic regression with elastic net regularization, which provides clear insights into the most influential variables and their relationships with the outcome. This makes the model interpretable and easier to understand, even for those who may not have extensive background knowledge in machine learning.",
  "model/output": "The model developed is a classification model. It is designed to predict the likelihood of revictimisation in cases of domestic abuse. The primary metric used to evaluate the model's performance is the area under the curve (AUC) of the receiver operating characteristic (ROC) curve, which is a standard measure for classification models. The model's goal is to rank cases in order of risk, with a higher AUC indicating better performance in distinguishing between revictimisation and non-revictimisation cases.\n\nThe model was built using logistic regression with elastic net regularization, which is a technique commonly used in classification tasks to prevent overfitting and improve the model's generalizability. The performance of the model was assessed using cross-validation, where the data was split into training and test sets multiple times to ensure robust evaluation.\n\nThe model's output includes the predicted probability of revictimisation for each case, allowing for the identification of high-risk victims. The true positive rate represents the rate of revictimisation detection, while the false positive rate indicates the rate of false alarms. The positive predictive value, which is the proportion of revictimisation predictions that were correct, is also considered.\n\nThe model's calibration was evaluated to ensure that the predicted probabilities align with the actual revictimisation rates. Overall, the model is well-calibrated, with the mean predicted probability of revictimisation closely matching the observed prevalence. However, there is less confidence in predictions for cases with higher predicted probabilities due to the limited data in those regions.\n\nThe most influential predictors in the model are static variables, with a significant emphasis on the abuser's criminal history. These variables were selected based on their standardized coefficients, indicating their importance in the model. The model's performance was compared to existing domestic abuse risk assessment instruments, and it surpassed expected performance, demonstrating its effectiveness in identifying high-risk victims.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed for our study involved a rigorous process to ensure the robustness and reliability of our models. We utilized cross-validation to estimate the performance of each algorithm. This involved building models on training data and evaluating them on separate test data that the models had not seen during the training stage. By doing this, we aimed to simulate real-world scenarios where the model would encounter unseen data.\n\nWe reported the mean and standard deviation of the Area Under the Curve (AUC) metrics obtained from the cross-validation results. Additionally, we provided the rate-wise mean Receiver Operating Characteristic (ROC) curve with 95% confidence intervals across the cross-validation runs. This approach allowed us to assess the consistency and reliability of our models' performance.\n\nFor algorithms that required hyperparameter tuning, we employed a nested set of cross-validation. The best hyperparameters were selected based on the highest nested cross-validated AUC. This method ensured that the hyperparameters were optimized in a way that minimized the risk of overfitting to the training data.\n\nIn our evaluation, the true positive rate represented the rate of revictimization detection, while the false positive rate represented the rate of false alarms. The AUC metric indicated the probability that the classifier would rank a revictimization case above a non-revictimization case. We also considered the positive predictive value, which is the proportion of revictimization predictions that were correct.\n\nTo address potential issues of unfairness, we examined model performance across different population subgroups. These subgroups were defined based on officer-defined ethnicity of the victim and the Index of Multiple Deprivation (IMD) ranking. The IMD served as a proxy for social demographics. We compared model calibration, revictimization rates, true and false positive rates, and positive predictive value across these subgroups to ensure that our models performed fairly and ethically.",
  "evaluation/measure": "In the evaluation of our models, several key performance metrics were reported to ensure a comprehensive assessment of their effectiveness in predicting domestic abuse revictimisation. The primary metric used was the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve. The AUC provides a single scalar value that summarizes the model's ability to rank cases in order of risk, with an AUC of 1 indicating perfect separation of high-risk cases and an AUC of 0.5 indicating performance no better than random classification.\n\nThe mean and standard deviation of AUCs were reported across multiple cross-validation runs to provide a robust estimate of model performance. Additionally, the rate-wise mean ROC curve with 95% confidence intervals was provided to visualize the performance across different thresholds.\n\nBeyond AUC, other important metrics were also considered. The true positive rate (TPR), which represents the rate of revictimisation detection, and the false positive rate (FPR), which indicates the rate of false alarms, were analyzed. These metrics are crucial for understanding the model's effectiveness in identifying true high-risk cases while minimizing false positives.\n\nThe positive predictive value (PPV), which is the proportion of revictimisation predictions that were correct, was also reported. This metric is particularly important for assessing the practical utility of the model in real-world applications, as it indicates how often the model's predictions of revictimisation are accurate.\n\nThe performance metrics reported are representative of those commonly used in the literature for evaluating predictive models in similar domains. The focus on AUC, TPR, FPR, and PPV ensures that the model's performance is assessed from multiple angles, providing a comprehensive view of its strengths and limitations. This approach aligns with best practices in the field and allows for meaningful comparisons with other studies and models.",
  "evaluation/comparison": "In the evaluation of our models, we conducted a thorough comparison with various statistical and machine learning algorithms to assess their performance in predicting domestic abuse. We benchmarked six different models, including logistic regression with elastic net, naive Bayes, augmented naive Bayes, random forest, gradient boosting, and weighted subspace random forest. These models were evaluated using cross-validation on both intimate partner violence (IPV) and non-intimate partner violence (non-IPV) datasets.\n\nThe performance of these models was measured using the area under the curve (AUC) of the receiver operating characteristic (ROC) curve. The AUC provides a comprehensive measure of how well the models rank cases in order of risk. An AUC of 0.5 indicates random classification, while an AUC of 1 indicates perfect separation of high-risk cases.\n\nLogistic regression with elastic net emerged as the best-performing model, achieving an AUC of 0.748 for the IPV sample and 0.763 for the non-IPV sample. This model outperformed other algorithms, demonstrating its superior ability to identify high-risk victims. The comparison with simpler baselines, such as naive Bayes and random forest, highlighted the effectiveness of logistic regression with elastic net in this context.\n\nAdditionally, we compared our model's performance with the DASH tool, which is commonly used for domestic abuse risk assessment. The DASH form contributed almost nothing to the model's predictive capability. We rebuilt the model excluding the DASH questions, DASH risk grading, and related variables, and found negligible differences in performance. This indicates that the data already available in police databases is much more effective for risk prediction than the DASH tool.\n\nIn summary, our evaluation involved a comprehensive comparison with publicly available methods and simpler baselines. Logistic regression with elastic net was identified as the most effective model for predicting domestic abuse, surpassing the performance of the DASH tool and other machine learning algorithms.",
  "evaluation/confidence": "The evaluation of our models involved rigorous cross-validation techniques to ensure robust performance metrics. We employed 500-times 2-fold cross-validation, which provides a comprehensive assessment of model performance by repeatedly splitting the data into training and test sets. This method allows us to report the mean and standard deviation of the area under the curve (AUC) metrics, offering a clear view of the model's consistency and reliability.\n\nConfidence intervals are indeed provided for the AUC metrics, which are crucial for understanding the variability and reliability of our results. These intervals help in assessing whether the observed performance differences are statistically significant. For instance, the logistic regression model with elastic net regularization, which was the best-performing model, achieved an AUC of .748 for intimate partner violence (IPV) and .763 for non-IPV samples. The standard deviations for these AUCs were .004 and .017, respectively, indicating a high level of confidence in these estimates.\n\nThe statistical significance of our results is further supported by the comparison with existing domestic abuse risk assessment instruments. Models deemed 'good' in the literature achieve AUCs in the range of .67 to .73. Our models surpassed this range, with AUCs of .748 and .763, clearly indicating superior performance. This suggests that our approach provides a significant improvement over current methods, including the DASH tool, which contributes almost nothing to the model's predictive power.\n\nAdditionally, the model calibration was assessed to ensure that the predicted probabilities align with the observed revictimization rates. The model demonstrated good calibration, with the mean predicted probability of revictimization closely matching the observed prevalence. This further bolsters the confidence in our model's predictions, especially in the lower probability regions where the majority of cases fall.\n\nIn summary, the performance metrics are accompanied by confidence intervals, and the results are statistically significant, demonstrating the superiority of our models over existing baselines and other methods. The use of cross-validation and the provision of detailed performance statistics ensure that our claims of improved predictive accuracy are well-founded.",
  "evaluation/availability": "Not enough information is available."
}