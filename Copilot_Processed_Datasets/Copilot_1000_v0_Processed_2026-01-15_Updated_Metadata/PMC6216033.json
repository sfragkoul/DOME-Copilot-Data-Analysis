{
  "publication/title": "BERMP: a cross-species classifier for predicting m<sup>6</sup>A sites by integrating a deep learning algorithm and a random forest approach.",
  "publication/authors": "Huang Y, He N, Chen Y, Chen Z, Li L",
  "publication/journal": "International journal of biological sciences",
  "publication/year": "2018",
  "publication/pmid": "30416381",
  "publication/pmcid": "PMC6216033",
  "publication/doi": "10.7150/ijbs.27819",
  "publication/tags": "- m6A site prediction\n- Deep learning\n- Machine learning\n- Bidirectional GRU\n- Cross-species prediction\n- Feature encoding\n- Logistic regression\n- Performance assessment\n- Nucleic acid content\n- Sequence patterns\n\nNot sure if the following tags are present in the article, but they are relevant to the content:\n\n- RNA modification\n- Bioinformatics\n- Computational biology\n- Predictive modeling\n- Data integration\n- Model training\n- Cross-validation\n- Sensitivity and specificity",
  "dataset/provenance": "The datasets used in this study were derived from previous publications. For mammalian datasets, the data was split into five groups, with four used for training and one for independent testing. Specifically, four-fifths of the total samples were used for training, and the remaining one-fifth for testing. For Saccharomyces cerevisiae datasets, 2200 samples were used for training, and 414 for independent testing. For Arabidopsis thaliana datasets, 4200 samples were used for training, and 836 for independent testing. Each training set was further divided for five-fold or ten-fold cross-validation. The cross-validation and independent test datasets are available for download. The data processing flows for Arabidopsis thaliana and Mammalia were excerpted from previous studies.",
  "dataset/splits": "In our study, we employed a multi-split approach to ensure robust training and validation of our models. For mammalian datasets, we divided the data into five groups, using four for training and one for independent testing. Specifically, four-fifths of the total samples were allocated for training, while the remaining one-fifth was reserved for independent testing.\n\nFor the Saccharomyces cerevisiae datasets, we randomly selected 2200 samples for training and 414 samples for independent testing. The training set was further divided into ten groups for ten-fold cross-validation, where nine groups (1980 samples) were used for training and one group (220 samples) was used for validation.\n\nIn the case of Arabidopsis thaliana datasets, we selected 4200 samples for training and 836 samples for independent testing. The training set was divided into five groups for five-fold cross-validation, with four groups used for training and one group for validation in each fold.\n\nThis approach ensures that our models are thoroughly validated and can generalize well to unseen data. The cross-validation and independent test datasets are available for further exploration and validation.",
  "dataset/redundancy": "The datasets were split into five groups, with four used for training and one for independent testing. For mammalian datasets, four-fifths of the samples were randomly selected for training, and the remaining one-fifth for independent testing. For Saccharomyces cerevisiae datasets, 2200 samples were randomly selected for training, and the remaining 414 for independent testing. For Arabidopsis thaliana datasets, 4200 samples were randomly selected for training, and the remaining 836 for independent testing.\n\nThe training and test sets are independent. This independence was enforced by randomly selecting samples for each set without overlap. Each training set was further divided for five-fold or ten-fold cross-validation, ensuring that the validation process was robust and that the model's performance was evaluated on unseen data.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in terms of size and diversity. The mammalian datasets are notably larger, with over 550,000 samples, which allows for more robust training and validation. The yeast and Arabidopsis thaliana datasets, while smaller, are still substantial and representative of their respective species. This ensures that the models trained on these datasets are generalizable and perform well across different species.",
  "dataset/availability": "The datasets used in this study are publicly available. Specifically, the cross-validation and independent test datasets can be accessed at a dedicated URL. The data processing flows for certain species, such as Mammalia and Arabidopsis thaliana, were excerpted from previous studies. The datasets themselves were derived from earlier publications, ensuring a robust and verified source.\n\nThe data is made available to facilitate reproducibility and further research. The specific URL provided for download ensures that users can access the necessary datasets for their own analyses. This approach promotes transparency and allows other researchers to validate or build upon the findings presented in this study. The datasets include the splits used for training and testing, which were carefully selected to ensure a comprehensive evaluation of the prediction models. The availability of these datasets supports the reproducibility of the results and encourages further exploration in the field.",
  "optimization/algorithm": "The optimization algorithm employed in our study is the Adam algorithm. This is a widely-used optimization technique for training deep learning models. It is not a new algorithm; it was introduced by Diederik P. Kingma and Jimmy Ba in 2014. The Adam algorithm combines the advantages of two other extensions of stochastic gradient descent. Specifically, it computes adaptive learning rates for each parameter. This means it can converge faster and more efficiently than traditional stochastic gradient descent.\n\nThe reason it was not published in a machine-learning journal is that it is a well-established algorithm in the field of machine learning and deep learning. Its effectiveness and efficiency have been thoroughly validated in numerous studies and applications. Therefore, it is commonly used as a standard optimization technique in various research areas, including bioinformatics.\n\nIn our work, the Adam algorithm was used to optimize the categorical cross-entropy loss function during the training of our deep learning classifiers. This choice was made to ensure efficient and effective training of our models, which are designed to predict m6A modification sites in RNA sequences. The algorithm's ability to adapt learning rates for each parameter helped in achieving better convergence and performance of our classifiers.",
  "optimization/meta": "The model indeed uses data from other machine-learning algorithms as input. Specifically, the meta-predictor integrates the outputs of individual classifiers to calculate the prediction score. The classifiers involved in this integration include a bidirectional GRU (BGRU)-based deep learning classifier and an ENAC-based random forest (RF) classifier.\n\nThe BGRU-based classifier is a deep learning approach that utilizes a bidirectional GRU with word embedding, which has demonstrated superior performance in predicting m6A sites. The ENAC-based RF classifier, on the other hand, employs the ENAC encoding scheme, which has been shown to perform best among different feature encodings.\n\nThe integration of these classifiers is achieved through a logistic regression approach. The weights and bias for each classifier output are obtained using a regression process, with the final score denoting the confidence level of the central adenosine to be methylated. This logistic model is trained and tested using a ten-fold cross-validation approach, ensuring that the training data is independent.\n\nThe performance of the meta-predictor, referred to as BERMP, is assessed through various metrics such as Sensitivity, Specificity, Accuracy, and Matthews Correlation Coefficient. Additionally, the area under the receiver-operating-characteristic curve (AUC) and the AUC with a <10% false positive rate (AUC01) are calculated to evaluate the overall performance of the predictors. The meta-predictor BERMP has shown competitive performance in both cross-validation and independent tests, indicating its effectiveness in predicting m6A sites.",
  "optimization/encoding": "In our study, we employed several feature encoding schemes to preprocess the data for our machine-learning algorithms. One of the key encodings we used is the Enhanced Nucleic Acid Composition (ENAC) encoding. This encoding reflects the frequency of the four types of nucleotides (A, G, C, U/T) surrounding the m6A sites in sequence fragments of a specific window size. The ENAC encoding scheme involves calculating the frequency of nucleic acids in a window that continuously slides from the N-terminal to the C-terminal of each RNA fragment in the dataset. The feature vector dimension of the ENAC encoding is determined by the size of the sliding window and the window size.\n\nWe also utilized other common encoding schemes such as Kmer, K-spaced nucleotide pair frequencies (KSNPF), and Pseudo dinucleotide composition (PseDNC). These encodings were selected based on their usage in state-of-the-art m6A predictors.\n\nFor the deep learning classifiers, the input layer receives a sequence of nucleotides with an optimized size, which are then converted into three-dimensional word vectors in the embedding layer. This conversion helps in representing the nucleotide properties according to their relative positions in the sequence.\n\nIn the recurrent layer, we used either a unidirectional GRU (UGRU) block or a bidirectional GRU (BGRU) block, depending on the classifier. The BGRU-based classifier contains two GRUs, each with 64 units, while the UGRU-based classifier has a single GRU block with 64 units.\n\nAdditionally, we ensured that the sequence length was consistent for all fragments by adding a gap symbol ('-') if the m6A/non-m6A sites appeared at one terminal of the sequence. This preprocessing step is crucial for maintaining uniformity in the input data for the machine-learning algorithms.",
  "optimization/parameters": "In our study, the optimization of window sizes was crucial for the performance of our classifiers. For the Arabidopsis thaliana dataset, we evaluated various sliding window sizes (S) ranging from 2 to 14, while keeping the sequence window size (W) fixed at 101. The sliding window size of 8 was selected for algorithm development as it yielded the highest AUC and AUC01 values.\n\nFor the Mammalian full transcript dataset, we tested different combinations of sliding window sizes (S) from 1 to 14 and sequence window sizes (W) from 51 to 101. The optimal parameters were found to be a sliding window size of 2 and a sequence window size of 81, which provided the best performance metrics.\n\nSimilarly, for the Mammalian mature mRNA dataset, we explored the same range of window sizes. The optimal configuration was determined to be a sliding window size of 2 and a sequence window size of 61, which also resulted in the highest AUC and AUC01 values.\n\nAdditionally, for the BGRU-based classifier, specific window sizes were optimized for different prediction modes. The full transcript mode used a window size of 301, while the mature mRNA mode utilized a window size of 191.\n\nThese parameters were selected based on extensive evaluations and cross-validation to ensure the best possible performance of our models. The chosen window sizes were those that maximized the AUC and AUC01 values, indicating superior predictive accuracy and reliability.",
  "optimization/features": "In the optimization of our classifiers, the input features are derived from nucleotide sequences. Specifically, a sequence of nucleotides with an optimized size is used as categorical features. The exact size of this sequence window varies depending on the dataset and prediction mode. For instance, in the full transcript mode for mammalian datasets, a window size of 301 nucleotides is used, while in the mature mRNA mode, a window size of 191 nucleotides is employed.\n\nThe sequences are processed through an embedding layer where each nucleotide, including a gap symbol ('-'), is converted into a three-dimensional word vector. This conversion helps in representing the nucleotide properties according to their relative positions in the sequence.\n\nFeature selection was not explicitly performed in the traditional sense of reducing the number of features. Instead, the focus was on optimizing the window size of the nucleotide sequences to ensure that the most relevant information is captured. This optimization was done using cross-validation techniques on the training set to determine the best window sizes that yield the highest performance metrics, such as AUC and AUC01.\n\nThe optimization process involved evaluating different window sizes and selecting those that provided the best performance. For example, for the Arabidopsis thaliana dataset, a sliding window size of 8 was chosen based on the highest AUC and AUC01 values. Similarly, for the mammalian full transcript dataset, a sliding window size of 2 and a sequence window size of 81 were selected. These optimizations were conducted using the training data to ensure that the models generalize well to unseen data.",
  "optimization/fitting": "In our study, we employed deep learning classifiers based on Gated Recurrent Units (GRUs), specifically unidirectional GRU (UGRU) and bidirectional GRU (BGRU) architectures. These classifiers consisted of five layers, including an input layer, an embedding layer, a recurrent layer, a fully connected layer, and an output layer.\n\nTo address the potential issue of overfitting, given the complexity of our models and the number of parameters involved, we implemented several strategies. Firstly, we used a dropout rate of 20% between different layers. Dropout is a regularization technique that helps prevent overfitting by randomly setting a fraction of input units to zero at each update during training time, which prevents units from co-adapting too much.\n\nAdditionally, we utilized a large batch size of 1024 during training to ensure a high rate of convergence. The total training process contained 1000 epochs, which allowed the loss function value to converge effectively. We also employed the Adam optimization algorithm to minimize the categorical cross-entropy loss function, which is well-suited for large datasets and high-dimensional parameter spaces.\n\nTo further validate the performance and generalization of our models, we conducted cross-validation. Specifically, we used ten-fold cross-validation for training and testing the weights and bias of the logistic regression model that integrated the outputs of individual classifiers. This approach helped ensure that our models were not overfitting to the training data and could generalize well to unseen data.\n\nRegarding underfitting, we ensured that our models were sufficiently complex to capture the underlying patterns in the data. The use of GRU layers, which are capable of learning long-term dependencies, along with the optimization of window sizes for different datasets, helped in capturing the necessary features. The performance metrics, including Sensitivity, Specificity, Accuracy, and Matthews Correlation Coefficient, along with the area under the ROC curve (AUC) and AUC01, indicated that our models were not underfitting and were able to make accurate predictions.",
  "optimization/regularization": "In our study, we employed dropout as a regularization method to prevent overfitting. Specifically, we applied a dropout rate of 20% between different layers of our neural network models. This technique involves randomly setting a fraction of the input units to zero at each update during training time, which helps to prevent the model from becoming too reliant on any single feature and thus improves its generalization to unseen data. Additionally, we used a batch size of 1024 during training to ensure a high rate of convergence, further aiding in the prevention of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule are detailed within the publication. The configurations include the use of GRU-based deep learning classifiers, with specific architectures for both unidirectional and bidirectional GRU models. The models consist of five layers: an input layer, an embedding layer, a recurrent layer, a fully connected layer, and an output layer. The training process employed the Adam algorithm to optimize the categorical cross-entropy loss function, with a dropout rate of 20% to prevent overfitting. The batch size was set to 1024, and the training process included 1000 epochs to ensure convergence.\n\nThe optimized window sizes for different datasets are also provided. For the mammalian mature mRNA dataset, a sliding window size of 2 and a sequence window size of 61 were selected. For the Arabidopsis thaliana dataset, a sliding window size of 8 was chosen with a fixed sequence window size of 101. For the mammalian full transcript dataset, a sliding window size of 2 and a sequence window size of 81 were used.\n\nThe model files and specific optimization parameters are not explicitly mentioned as being available for download. However, the datasets used for cross-validation and independent testing are accessible at a provided URL. The publication does not specify the license under which these resources are made available, but they are intended for further research and validation purposes.\n\nThe integration of classifier outputs and performance assessment metrics, such as Sensitivity, Specificity, Accuracy, and Matthews Correlation Coefficient, are also discussed, providing a comprehensive view of the model's configuration and optimization process.",
  "model/interpretability": "The models discussed in this publication primarily rely on deep learning architectures, specifically Gated Recurrent Units (GRUs), which are known for their ability to capture sequential dependencies in data. These models, particularly the Bidirectional GRU (BGRU) and Unidirectional GRU (UGRU), can be considered somewhat of a black box due to their complex, multilayered structure. The internal workings of these neural networks are not easily interpretable, as they involve numerous parameters and non-linear transformations.\n\nHowever, the integration of these deep learning models with traditional machine learning classifiers, such as the Random Forest (RF) with ENAC encoding, adds a layer of interpretability. The ENAC encoding scheme, for instance, converts nucleotide sequences into a format that can be understood and processed by the RF classifier. This encoding is designed to capture specific biological properties of the nucleotides, making it more interpretable than the raw sequence data.\n\nMoreover, the logistic regression approach used to combine the outputs of different classifiers provides a way to understand the contribution of each classifier to the final prediction. The weights assigned to each classifier's output in the logistic regression model can be examined to gauge their relative importance. This integration allows for a more transparent understanding of how different features and models contribute to the final prediction of m6A sites.\n\nIn summary, while the deep learning components of the models are largely black box, the use of interpretable encoding schemes and the integration of traditional machine learning classifiers add a degree of transparency. This hybrid approach aims to balance the predictive power of deep learning with the interpretability of traditional machine learning methods.",
  "model/output": "The model is a classification model designed to predict m6A modification sites. It outputs the probability of a sequence being positive for m6A modification. If this probability exceeds a specified threshold, the sequence is predicted as positive. The output layer of the model uses a sigmoid activation function to produce this probability. The model employs two types of classifiers: a unidirectional GRU (UGRU)-based classifier and a bidirectional GRU (BGRU)-based classifier. Both classifiers have five layers, including an input layer, an embedding layer, a recurrent layer, a fully connected layer, and an output layer. The output of these classifiers is integrated using a logistic regression approach to calculate the prediction score, which denotes the confidence level of the central adenosine being methylated. The model's performance is evaluated using metrics such as Sensitivity, Specificity, Accuracy, and Matthews Correlation Coefficient, as well as the area under the ROC curve (AUC) and AUC with a low false positive rate (AUC01). The model is implemented using TensorFlow and trained with the Adam algorithm to optimize the categorical cross-entropy loss function. Dropout is used between layers to prevent overfitting, and the batch size is set to 1024 to ensure a high rate of convergence. The training process includes 1000 epochs to ensure the loss function value converges.",
  "model/duration": "The execution time of the model varied depending on the dataset size and the specific configuration used. For the mammalian datasets, which were significantly larger, the model required more time to train due to the increased data size. The training process involved 1000 epochs with a batch size of 1024, which ensured a high rate of convergence. The use of a dropout rate of 20% between layers helped prevent overfitting, but it also added to the computational complexity.\n\nThe model was implemented using TensorFlow, which is known for its efficiency in handling deep learning tasks. Despite the complexity introduced by the bidirectional GRU (BGRU) architecture, the model demonstrated robust performance, particularly with larger datasets. The integration of the BGRU and the ENAC-encoding RF classifier through a logistic regression approach further enhanced the model's accuracy and reliability.\n\nFor smaller datasets, such as those from yeast, the execution time was comparatively shorter. This is because the model's performance is sensitive to the size of the training data, and smaller datasets required less computational resources. The five-fold cross-validation process also contributed to the overall execution time, but it was necessary to ensure the model's generalizability and performance across different datasets.\n\nIn summary, the execution time of the model was influenced by the dataset size and the architectural complexity. The use of TensorFlow and the logistic regression approach for integrating classifiers ensured efficient and effective model training, even for large-scale datasets.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the prediction methods involved a combination of cross-validation and independent testing. For the datasets, four-fifths of the samples were used for training, while the remaining one-fifth was reserved for independent testing. This approach ensured that the models were evaluated on data they had not seen during training.\n\nCross-validation was employed to assess the performance of the classifiers. Specifically, five-fold or ten-fold cross-validation was used. In this process, the training set was divided into multiple folds. For instance, in the case of Saccharomyces cerevisiae, the training set of 2200 samples was divided into 10 groups. Nine of these groups were used for training, while the remaining group was used for validation. This procedure was repeated such that each group served as the validation set once, providing a comprehensive evaluation of the model's performance.\n\nThe performance of the predictors was compared using metrics such as the Area Under the Curve (AUC) and AUC01 values. These metrics were calculated for different datasets, including those from Mammalia and Arabidopsis thaliana. Statistical comparisons were made between the adjacent datasets to determine if there were significant differences in performance. A paired student\u2019s t-test was used to calculate the P-values, with a horizontal line representing no statistical difference (P > 0.05).\n\nAdditionally, the relationship between data size and prediction performance was examined. The AUC and AUC01 values were calculated using four different data sizes\u2014all, one-fifth, one-tenth, and one-fiftieth of the dataset\u2014via five-fold cross-validation. This analysis helped to understand how the amount of data affects the predictive accuracy of the classifiers.",
  "evaluation/measure": "In the evaluation of our predictors, we employed several key performance metrics to comprehensively assess their effectiveness. These metrics include Sensitivity (Sn), Specificity (Sp), Accuracy (Ac), and Matthews Correlation Coefficient (MCC). Sensitivity measures the proportion of true positive predictions among actual positives, while Specificity measures the proportion of true negative predictions among actual negatives. Accuracy provides an overall measure of correct predictions, and MCC offers a balanced measure that considers true and false positives and negatives.\n\nAdditionally, we utilized the area under the receiver-operating-characteristic (ROC) curve (AUC) to evaluate the overall performance of the predictors. The ROC curve plots the true positive rate against the false positive rate at various threshold settings, providing a visual representation of the trade-off between sensitivity and specificity. We also calculated the AUC with a false positive rate of less than 10% (AUC01), which is particularly relevant in practical applications where maintaining a low false positive rate is crucial.\n\nThese metrics are widely recognized and used in the literature for evaluating the performance of predictive models, ensuring that our evaluation is both rigorous and comparable to other studies in the field. The combination of these metrics provides a thorough assessment of our predictors' ability to accurately identify m6A sites, making our results reliable and informative.",
  "evaluation/comparison": "In the evaluation of our method, we conducted a thorough comparison with both publicly available methods and simpler baselines to ensure the robustness and superiority of our approach.\n\nFor the publicly available methods, we compared our BERMP framework with several established predictors across three different species: Saccharomyces cerevisiae, Arabidopsis thaliana, and Mammalia. For Mammalia, we chose SRAMP, a well-documented predictor known for its good performance. In the case of Arabidopsis thaliana, we selected RFAthM6A, another predictor with reported high accuracy. For Saccharomyces cerevisiae, we compared BERMP with pRNAm-PC, M6A-HPCS, and RAM-NPPS, all of which were developed using the same dataset containing 1307 positive and 1307 negative samples. The results showed that BERMP outperformed these predictors in terms of sensitivity, accuracy, Matthew's Correlation Coefficient (MCC), and Area Under the Curve (AUC) values. Specifically, BERMP demonstrated improvements of 8.7%, 3.8%, and 2.0% in AUC compared to M6A-HPCS, pRNAm-PC, and RAM-NPPS, respectively.\n\nIn addition to comparing with publicly available methods, we also evaluated BERMP against simpler baselines. We integrated a bidirectional GRU-based deep learning approach (BGRU) with a random forest method using extended nucleic acid content (RF ENAC). This combination was chosen because BGRU showed better performance for large datasets, particularly mammalian datasets, while RF ENAC was superior for smaller datasets, such as those from yeast. By combining these two methods, BERMP was able to leverage the strengths of both, resulting in a more robust and accurate predictor across different species.\n\nFurthermore, we tested the performance of BERMP using different data sizes to understand its sensitivity to the size of the training dataset. We calculated the AUC and AUC01 values using four different data sizes (all, one-fifth, one-tenth, and one-fiftieth) via five-fold cross-validation. The results indicated that BERMP maintained high performance even with reduced dataset sizes, although the overall performance increased with larger training datasets. This analysis highlighted the advantage of BERMP in handling both large and small datasets, making it a versatile tool for m6A site prediction across various species.",
  "evaluation/confidence": "In the evaluation of our predictors, we employed several performance metrics, including Sensitivity, Specificity, Accuracy, and Matthews Correlation Coefficient. These metrics were calculated at certain thresholds to assess the performance of each predictor.\n\nTo ensure the robustness of our results, we utilized cross-validation techniques. Specifically, we performed five-fold cross-validation to calculate the AUC and AUC01 values for our predictors. This method helps to provide a more reliable estimate of model performance by reducing the risk of overfitting and ensuring that the results are generalizable to new, unseen data.\n\nStatistical significance was assessed using paired student\u2019s t-tests to compare the AUC or AUC01 values between adjacent datasets for each algorithm. A horizontal line in our figures represents no statistical difference (P > 0.05), indicating that any differences observed below this threshold are not considered significant.\n\nAdditionally, we integrated the outputs of individual classifiers using a logistic regression approach to calculate the prediction score. This integration process involved training and testing the weights and bias in a ten-fold cross-validation, further enhancing the confidence in our results.\n\nWhile we have not explicitly provided confidence intervals for the performance metrics, the use of cross-validation and statistical testing methods ensures that our results are robust and that any claims of superiority over other methods and baselines are supported by rigorous statistical analysis.",
  "evaluation/availability": "The datasets used for cross-validation and independent testing in our study are publicly available. They can be accessed via a specific download link. This ensures that other researchers can replicate our results and further build upon our work. The datasets include various species and are divided into training and testing sets as described in our methodology. The availability of these datasets promotes transparency and reproducibility in our research."
}