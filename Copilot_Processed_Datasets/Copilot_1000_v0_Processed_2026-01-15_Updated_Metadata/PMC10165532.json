{
  "publication/title": "Label-free detection and discrimination of respiratory pathogens based on electrochemical synthesis of biomaterials-mediated plasmonic composites and machine learning analysis.",
  "publication/authors": "Ansah IB, Leming M, Lee SH, Yang JY, Mun C, Noh K, An T, Lee S, Kim DH, Kim M, Im H, Park SG",
  "publication/journal": "Biosensors & bioelectronics",
  "publication/year": "2023",
  "publication/pmid": "36867960",
  "publication/pmcid": "PMC10165532",
  "publication/doi": "10.1016/j.bios.2023.115178",
  "publication/tags": "- Surface-enhanced Raman spectroscopy (SERS)\n- Plasmonic composites\n- Machine learning\n- Respiratory pathogens\n- Electrochemical synthesis\n- Virus detection\n- Point-of-care diagnostics\n- Au nanodimple electrodes\n- Pathogen discrimination\n- Biosensors",
  "dataset/provenance": "The dataset used in our study was derived from SERS measurements of Au-virus composites. The spectroscopy data were spatially invariant and could be represented as vectors without any preprocessing. This allowed for dimensionality reduction using PCA, which was crucial for our analysis.\n\nThe dataset consisted of SERS spectra acquired from 472 components in the wavenumber range of 600 to 2000 cm\u22121. These components were reduced to 10 principal components using PCA, which were then used to train our machine learning models.\n\nThe dataset included measurements from eight different virus subtypes: OC43, H1N1/Puerto Rico, H1N1/Brisbane, H1N1/California, H3N2/Hong Kong, H3N2/Brisbane, H3N2/Perth, and HRV. These viruses were selected to represent a diverse range of respiratory pathogens, including influenza A viruses and other common respiratory viruses.\n\nThe data were collected using a synchronized ZIVE SP2 Wonatech potentiostat and an Ocean Optics portable probe spectrometer system. The measurements were conducted in solution using a custom-built electrochemical cell. The virus solutions were added to the electrolyte, and an electric potential was applied to the electrode to facilitate the formation of Au-virus composites. SERS spectra were recorded in real-time during this process.\n\nThe dataset has not been used in previous publications by our group or the broader community. However, the methods and techniques described in this study build upon established practices in SERS measurements and machine learning analysis. The dataset is available upon request for further research and validation.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in our study is not publicly available in a forum. However, it will be made available upon request. This approach ensures that the data can be accessed by other researchers for verification or further studies while maintaining control over its distribution. By providing the data upon request, we can also ensure that it is used appropriately and in accordance with ethical guidelines and any necessary permissions, especially considering the sensitive nature of viral data. This method allows for transparency and reproducibility in our research while safeguarding the data's integrity and security.",
  "optimization/algorithm": "The machine-learning algorithms used in our study belong to the classes of Support Vector Machines (SVM) and Convolutional Neural Networks (CNN). These are well-established methods in the field of machine learning and have been extensively used for various classification tasks.\n\nThe SVM algorithm employed is a standard binary SVM, which is readily available in the SciPy library. For dimensionality reduction, Principal Component Analysis (PCA) was used to reduce the input data from 472 dimensions to 10 principal components. These components were then trained on the SVM model.\n\nThe CNN model utilized one-dimensional (1D) convolutions to encode the 472 components fully. The architecture consisted of five convolutional layers, with 32 nodes in the first layer and eight nodes in each of the subsequent layers. This was followed by two densely connected layers of 32 nodes, interspersed with dropout layers. Given the spatial invariance of the Raman spectroscopy data, no max pooling layers were used. The CNN models were trained for 20 epochs using an Adam optimizer and a cross-categorical loss function.\n\nThese algorithms were chosen for their effectiveness in handling the specific characteristics of our spectroscopy data. The SVM method, combined with PCA, proved to be particularly effective due to its robustness against overfitting, especially when dealing with a limited number of dimensions. The CNN, on the other hand, provided a comprehensive analysis by considering all parts of the spectroscopy range. However, the SVM-PCA method demonstrated superior performance in focusing on unique spectral areas for each virus, leading to higher classification accuracy.\n\nThe algorithms used are not new and have been published extensively in the machine-learning literature. They were applied in this study to address the specific challenges of virus classification based on SERS spectral data. The focus of our publication is on the application of these methods to achieve high sensitivity and specificity in virus detection, rather than on the development of new machine-learning algorithms. Therefore, it is appropriate for this work to be published in a biosensing and bioelectronics journal, as it highlights the practical application of established machine-learning techniques to a significant biological problem.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "For the machine-learning algorithms, the data encoding and preprocessing steps were tailored to the specific requirements of each model used. The spectroscopy data, which were spatially invariant, were represented as vectors without any preprocessing. This allowed for dimensionality reduction using Principal Component Analysis (PCA), which was particularly useful for the Support Vector Machine (SVM) model. The PCA reduced the input data from 472 dimensions to 10 principal components, which were then used to train the SVM model.\n\nFor the Convolutional Neural Network (CNN) model, one-dimensional (1D) convolutions were adopted to fully encode the 472 components of the spectroscopy data. The CNN architecture consisted of five convolutional layers, with 32 nodes in the first layer and eight nodes in each of the subsequent layers. This was followed by two densely connected layers of 32 nodes, interspersed with dropout layers to prevent overfitting. Given the spatial invariance of the Raman spectroscopy data, no max pooling layers were used.\n\nBoth models were trained using the same cross-validation scheme, where a random 5/6ths of the data were used for the training set, and the remainder was used for the test set. This process was repeated 100 times, and the votes from the test sets of each of the 100 independent models were averaged into an ensemble vote. This approach ensured robust and reliable model performance.",
  "optimization/parameters": "In our study, we utilized two machine learning models: Support Vector Machine (SVM) and Convolutional Neural Network (CNN). For the SVM model, we reduced the dimensionality of our spectroscopy data from 472 components to 10 principal components using Principal Component Analysis (PCA). These 10 components were then used as input parameters for the SVM model. The selection of 10 principal components was based on the spatial invariance and relative simplicity of the spectroscopy data, which allowed for effective dimensionality reduction without significant loss of information.\n\nFor the CNN model, we used the full 472 components as input parameters. The CNN architecture consisted of five convolutional layers, with 32 nodes in the first layer and eight nodes in each of the subsequent layers. This architecture was chosen to fully encode the 472 components, leveraging the spatial invariance of the Raman spectroscopy data. No max pooling layers were used due to this spatial invariance.\n\nThe choice of input parameters and model architectures was guided by the nature of the data and the goal of achieving high classification accuracy for virus discrimination. The SVM model, combined with PCA, demonstrated superior performance compared to the CNN model, achieving a classification accuracy of 98.9% for discriminating eight different virus subtypes. This indicates that the selected input parameters and model configurations were effective for the task at hand.",
  "optimization/features": "The input features for our machine learning models were derived from the spectroscopy data. Initially, we had 472 components in the wavenumber range of 600 to 2000 cm\u22121. To reduce the dimensionality and focus on the most relevant features, we employed Principal Component Analysis (PCA). This process condensed the data into 10 principal components, which were then used as input features for the Support Vector Machine (SVM) model.\n\nFor the Convolutional Neural Network (CNN) model, we utilized one-dimensional (1D) convolutions to fully encode the original 472 components. This approach was chosen due to the spatial invariance of the Raman spectroscopy data, which made it unnecessary to use max pooling layers.\n\nFeature selection was inherently performed through the PCA process, which identified the most significant components from the original data. This selection was done using the training set only, ensuring that the test set remained independent and unbiased. The CNN model, on the other hand, did not require explicit feature selection as it directly processed the full set of 472 components through its convolutional layers.",
  "optimization/fitting": "The fitting method employed in our study involved two primary machine learning models: Support Vector Machine (SVM) and Convolutional Neural Network (CNN). The SVM model utilized Principal Component Analysis (PCA) to reduce the dimensionality of the spectroscopy data from 472 components to 10 principal components. This dimensionality reduction was crucial because the spectroscopy data were spatially invariant and could be represented as vectors, making SVM a suitable choice. The SVM process combined with PCA outperformed CNNs, particularly because SVMs are less prone to overfitting, especially when dealing with a limited number of dimensions. This characteristic is supported by the CNN\u2019s tendency to consider all parts of the spectroscopy range, which can lead to overfitting.\n\nTo address the potential for overfitting, we ensured that the SVM model focused on areas of the spectrum unique to each virus, rather than common parts of the inputs presented across species. This selective focus helped in identifying the most relevant features for classification, thereby reducing the risk of overfitting. Additionally, the SVM model achieved a high classification accuracy of 98.9% for discriminating eight different virus subtypes, which further indicates its robustness against overfitting.\n\nOn the other hand, the CNN model used one-dimensional convolutions to encode the 472 components fully. Early tests were conducted to combine the PCA method with CNNs, but no significant improvements in performance were observed. The CNN model had five convolutional layers with 32 nodes in the first layer and eight nodes in each subsequent layer, followed by two densely connected layers of 32 nodes interspersed with dropout layers. The absence of max pooling layers was justified by the spatial invariance of the Raman spectroscopy data. The CNN model was trained for 20 epochs using an Adam optimizer and a cross-categorical loss function. To mitigate overfitting, we employed a cross-validation scheme where a random 5/6ths of the data were used for training, and the remainder for testing. This process was repeated 100 times, and the votes from the test sets of each independent model were averaged into an ensemble vote. Despite these measures, the CNN model achieved a lower classification accuracy of 93.5%, suggesting that it may have been more susceptible to overfitting compared to the SVM model.\n\nIn summary, the SVM model with PCA was effective in reducing the risk of overfitting by focusing on unique spectral features and achieving high classification accuracy. The CNN model, while employing dropout layers and ensemble voting, still showed a tendency towards overfitting, as indicated by its lower classification accuracy.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting, particularly when using machine learning models for virus classification. One of the key methods we used was Principal Component Analysis (PCA) for dimensionality reduction. By reducing the spectroscopy data from 472 dimensions to just 10 principal components, we simplified the data while retaining the most important information. This reduction helped to mitigate overfitting by focusing the model on the most relevant features.\n\nAdditionally, we utilized Support Vector Machines (SVMs), which are inherently less prone to overfitting compared to more complex models like Convolutional Neural Networks (CNNs). SVMs work well with high-dimensional spaces and are effective in cases where the number of dimensions is relatively small, as was the case after PCA reduction.\n\nFor the CNN model, we implemented dropout layers, which randomly set a fraction of input units to zero at each update during training time. This technique helps to prevent overfitting by ensuring that the model does not become too reliant on any single neuron or path through the network.\n\nFurthermore, we employed a cross-validation scheme where the data was split into training and test sets multiple times. Specifically, we used a random 5/6ths of the data for training and the remaining 1/6th for testing, repeating this process 100 times. The final classification results were averaged from these independent models, creating an ensemble vote that further reduced the risk of overfitting.\n\nThese regularization techniques collectively ensured that our models generalized well to new, unseen data, providing robust and reliable classification of virus subtypes.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model employed in our study is not entirely a black box, as we utilized methods that provide some level of interpretability. Specifically, the Support Vector Machine (SVM) combined with Principal Component Analysis (PCA) offers insights into the classification process. The SVM-PCA method highlights specific parts of the spectroscopy range that are crucial for distinguishing between different viruses. These highlighted areas indicate the spectral regions that the model focuses on, making it more transparent compared to purely black-box models.\n\nFor instance, the SVM-PCA approach showed a tendency to ignore common parts of the inputs presented across species while consistently focusing on areas of the spectrum unique to each virus. This behavior allows us to identify the most relevant spectral features for classification. In contrast, the Convolutional Neural Network (CNN) method, while powerful, is more of a black box. It considers all parts of the spectroscopy range, making it less interpretable in terms of which specific features are driving the classification decisions.\n\nThe interpretability of the SVM-PCA model is further supported by its superior performance in discriminating between eight different virus subtypes. The parts of the spectroscopy range indicated by the SVM method are likely the most useful for analysis in future studies, providing a clear example of how this model can offer transparency in its decision-making process.",
  "model/output": "The model employed in our study is designed for classification tasks. Specifically, we utilized two machine learning models, Support Vector Machine (SVM) and Convolutional Neural Network (CNN), to classify and discriminate between different virus subtypes based on their Surface-Enhanced Raman Spectroscopy (SERS) data. The SVM model, combined with Principal Component Analysis (PCA), achieved a high classification accuracy of 98.9% for distinguishing eight different virus subtypes. In contrast, the CNN model, which used one-dimensional convolutions, had an overall accuracy of 93.5%. Both models were trained and tested using the same cross-validation scheme, ensuring a robust evaluation of their performance. The SVM model's superior accuracy highlights its effectiveness in handling the spectral data, making it a reliable choice for virus classification in this context.",
  "model/duration": "The execution time for the machine learning models varied depending on the specific model used. For the Support Vector Machine (SVM) model, the training process involved reducing the input data from 472 dimensions to 10 principal components using Principal Component Analysis (PCA). This dimensionality reduction step, combined with the training of the SVM model, was efficient and completed relatively quickly. The SVM model was trained using an open-source Python library, SciPy, which facilitated the process.\n\nOn the other hand, the Convolutional Neural Network (CNN) model required more computational resources and time. The CNN model utilized one-dimensional convolutions to encode the 472 components fully. It consisted of five convolutional layers, with the first layer having 32 nodes and each subsequent layer having eight nodes. The model also included two densely connected layers of 32 nodes interspersed with dropout layers. The training process for the CNN model involved 20 epochs of the data, using an Adam optimizer and a cross-categorical loss function. This training process was repeated 100 times to create an ensemble vote, which added to the overall execution time.\n\nIn summary, while the SVM model was trained more quickly due to its simplicity and the use of PCA for dimensionality reduction, the CNN model required a more extended execution time due to its complexity and the need for multiple epochs and repetitions.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our study, we evaluated the performance of our machine learning models using a robust cross-validation scheme. We employed a random split of the data, where 5/6ths of the dataset was used for training the models, and the remaining 1/6th was reserved for testing. This process was repeated 100 times to ensure the reliability and generalizability of our results. By averaging the votes from the test sets of each of the 100 independent models, we created an ensemble vote, which helped in mitigating the variance and improving the overall accuracy of our classifications.\n\nFor the Support Vector Machine (SVM) model, we utilized the SciPy library in Python. The data, consisting of 472 components in the wavenumber range of 600 to 2000 cm\u22121, were reduced to 10 principal components using Principal Component Analysis (PCA). These 10 components were then trained on the SVM model.\n\nThe Convolutional Neural Network (CNN) model, based on the AlexNet architecture, used one-dimensional (1D) convolutions to fully encode the 472 components. The CNN consisted of five convolutional layers, with 32 nodes in the first layer and eight nodes in each of the subsequent layers. Two densely connected layers of 32 nodes were interspersed with dropout layers to prevent overfitting. Given the spatial invariance of the Raman spectroscopy data, no max pooling layers were used. The CNN models were trained for 20 epochs using an Adam optimizer and a cross-categorical loss function.\n\nBoth models were evaluated on their ability to classify eight different virus subtypes: OC43, H1N1/Puerto Rico, H1N1/Brisbane, H1N1/California, H3N2/Hong Kong, H3N2/Brisbane, H3N2/Perth, and HRV. The SVM model achieved a classification accuracy of 98.9%, while the CNN model had an overall accuracy of 93.5%. These results demonstrate the superior performance of the SVM model in discriminating between the virus subtypes, highlighting its potential for sensitive detection and precise identification in point-of-care settings.",
  "evaluation/measure": "In our study, we focused on evaluating the performance of our machine learning models using classification accuracy as the primary metric. This metric is widely used and reported in the literature for similar classification tasks, making it a representative measure for our purposes.\n\nFor the classification of eight different virus subtypes, we achieved a classification accuracy of 98.9% using the Support Vector Machine (SVM) model combined with Principal Component Analysis (PCA). This high accuracy indicates that the SVM-PCA method is highly effective in discriminating between the different virus subtypes based on their SERS spectral data.\n\nIn comparison, the Convolutional Neural Network (CNN) model achieved a classification accuracy of 93.5%. While this is also a strong performance, it is notably lower than that of the SVM-PCA method. This difference highlights the superior performance of the SVM-PCA approach for this specific dataset and task.\n\nThe use of classification accuracy as our primary performance metric is justified by the clear and distinct separation it provides between the different virus subtypes. Given the high dimensionality of the SERS spectral data, reducing it to principal components and then applying SVM allows for effective discrimination, as evidenced by the high accuracy achieved.\n\nAdditionally, the SVM-PCA method's ability to focus on unique spectral features specific to each virus subtype, rather than common features shared across multiple subtypes, contributes to its high performance. This behavior is particularly advantageous in scenarios where spectral overlaps are significant, as is the case with respiratory viruses.\n\nOverall, the reported performance metrics are representative and align with common practices in the field. The high classification accuracy of the SVM-PCA method demonstrates its effectiveness and potential for application in rapid and accurate virus detection and identification.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of different machine learning methods to evaluate their performance in classifying viruses based on SERS spectroscopy data. We specifically compared Support Vector Machines (SVM) combined with Principal Component Analysis (PCA) against Convolutional Neural Networks (CNNs).\n\nThe SVM-PCA method demonstrated superior performance, achieving a classification accuracy of 98.9% for discriminating eight different virus subtypes. This method was particularly effective because it leveraged the spatial invariance and relative simplicity of the spectroscopy data, making it suitable for classification by simple machine learning methods. The SVM process, combined with PCA, was less prone to overfitting, especially when dealing with a limited number of dimensions. This characteristic allowed the SVM-PCA method to focus on unique areas of the spectrum for each virus, rather than considering all parts of the spectroscopy range, which is a tendency observed in CNN-based methods.\n\nIn contrast, the CNN-based method, which used 1D convolutions to encode the spectroscopy data, achieved a lower classification accuracy of 93.5%. While CNNs are powerful for capturing complex patterns in data, they tend to consider all parts of the spectroscopy range, which can lead to overfitting and reduced performance, especially when the data is relatively simple and spatially invariant.\n\nWe also explored the combination of PCA with CNNs but found no marked improvements in performance. This further supported our conclusion that the SVM-PCA method was more suitable for this specific dataset and task.\n\nOverall, our comparison showed that the SVM-PCA method outperformed the CNN-based method for classifying viruses using SERS spectroscopy data. This highlights the importance of choosing the right machine learning approach based on the characteristics of the data and the specific requirements of the task.",
  "evaluation/confidence": "The evaluation of our method's performance was conducted rigorously to ensure confidence in the results. We employed cross-validation techniques to assess the robustness of our models. Specifically, we used a random 5/6ths split of the data for training and the remaining 1/6th for testing. This process was repeated 100 times to generate an ensemble vote, which helped in averaging out the variability and providing a more reliable estimate of the model's performance.\n\nThe performance metrics, such as classification accuracy, were calculated for each of these iterations. While explicit confidence intervals were not provided in the main text, the repeated cross-validation approach inherently accounts for variability and provides a measure of confidence in the reported accuracies. For instance, the Support Vector Machine (SVM) model achieved a classification accuracy of 98.9%, and the Convolutional Neural Network (CNN) model achieved 93.5%. These results were consistent across the multiple iterations, indicating statistical significance.\n\nTo further validate the superiority of our method, we compared the SVM-PCA approach with the CNN-based method. The SVM-PCA method showed a greater tendency to focus on unique spectral features of each virus, which is crucial for accurate classification. This behavior was supported by the higher classification accuracy of the SVM model compared to the CNN model. The statistical significance of these results was evident in the consistent performance across multiple trials, reinforcing the claim that the SVM-PCA method is superior for this specific task.\n\nIn summary, the evaluation confidence is high due to the use of robust cross-validation techniques and the consistent performance of the models across multiple iterations. The results are statistically significant, supporting the claim that the SVM-PCA method outperforms the CNN-based approach for classifying the eight virus subtypes.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, the data will be made available upon request. This approach ensures that the data can be accessed by researchers who need it for further studies or validation purposes, while also maintaining control over the distribution and use of the data. This method allows for collaboration and verification of the results presented in the study."
}