{
  "publication/title": "In silico insights into the design of novel NR2B-selective NMDA receptor antagonists: QSAR modeling, ADME-toxicity predictions, molecular docking, and molecular dynamics investigations.",
  "publication/authors": "El Fadili M, Er-Rajy M, Mujwar S, Ajala A, Bouzammit R, Kara M, Abuelizz HA, Er-Rahmani S, Elhallaoui M",
  "publication/journal": "BMC chemistry",
  "publication/year": "2024",
  "publication/pmid": "39085870",
  "publication/pmcid": "PMC11293250",
  "publication/doi": "10.1186/s13065-024-01248-6",
  "publication/tags": "- Multiple Linear Regression\n- Principal Component Analysis\n- QSAR Studies\n- Molecular Docking\n- Artificial Neural Networks\n- External Validation\n- Descriptor Selection\n- Statistical Validation\n- Predictive Modeling\n- Correlation Analysis",
  "dataset/provenance": "The dataset used in our study was sourced from a recently published work by K. Anan et al. This dataset includes NR2B-selective antagonists that inhibit the binding of [3H]-ifenprodil to rat brain membranes. The dataset consists of 32 compounds, each represented by a matrix of 32 rows (active molecules) and 14 columns (molecular descriptors). This dataset was divided into two subsets: a training set comprising 26 compounds, which was used to build the model, and a test set comprising 6 compounds, which was used to validate the established model. The dataset was chosen to establish QSAR mathematical models, focusing on the correlation and interaction of various molecular descriptors to predict the activity of these compounds.",
  "dataset/splits": "The dataset used in this study consists of 32 compounds, which were divided into two main subsets: a training set and a test set. The training set includes 26 compounds, which were used to build the QSAR model. The test set comprises 6 compounds, which were used to validate the established model. This division ensures that the model is both robust and generalizable, as it is tested on data that was not used during the training phase. The distribution of data points is therefore 26 in the training set and 6 in the test set.",
  "dataset/redundancy": "The dataset used in this study was split into a training set and a test set to evaluate the performance and generalizability of the predictive models. The training set consisted of 26 compounds, while the test set included six new molecules. This split was designed to ensure that the training and test sets were independent, thereby providing an unbiased evaluation of the models' predictive capabilities.\n\nTo enforce the independence of the training and test sets, we employed external validation techniques. This involved testing the six new molecules in the test set on a training set basis, ensuring that the models were not overfitted to the training data. The results of this external validation were presented in a table, demonstrating the correlation between observed and predicted activities for the test set molecules.\n\nThe distribution of the dataset was carefully considered to minimize redundancy and ensure that the models were robust and reliable. Principal Component Analysis (PCA) was used to reduce the number of molecular descriptors, eliminating correlated variables and retaining only the most relevant descriptors. This step was crucial in making the studied information less redundant and ensuring that the models were based on independent variables.\n\nThe models were further validated using cross-validation techniques, specifically the leave-one-out procedure. This involved removing each observation from the training set one at a time and predicting the biological activity of the deleted sample. This process was repeated for each observation, ensuring that every compound was tested exactly once. The cross-validation results indicated that the models had high internal predictivity and were reliable.\n\nIn summary, the dataset was split into independent training and test sets, with external validation and cross-validation techniques used to ensure the models' robustness and reliability. The distribution of the dataset was optimized using PCA to minimize redundancy and enhance the models' predictive capabilities.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study falls under the class of artificial neural networks (ANNs), specifically utilizing a three-layer neural network architecture. This approach is well-established in the field of quantitative structure-activity relationship (QSAR) studies for developing non-linear relationships between molecular descriptors and biological activities.\n\nThe ANN used in this work is not a novel algorithm. It is a widely recognized and extensively used method in computational chemistry and drug discovery. The choice of ANN for this study was driven by its proven efficacy in handling complex, non-linear relationships in data, which is crucial for accurately predicting biological activities based on molecular descriptors.\n\nGiven that the ANN is a well-known technique, it was not necessary to publish the algorithm itself in a machine-learning journal. Instead, the focus of our publication is on the application of this established method to a specific problem in chemistry, namely the prediction of the effects of NMDA antagonists on the rat brain. The innovation lies in the application and validation of the ANN in this particular context, rather than in the development of a new algorithm.",
  "optimization/meta": "The models developed in this study do not function as meta-predictors. Instead, they are standalone quantitative structure-activity relationship (QSAR) models. These models were created using various machine-learning techniques, including multiple linear regression (MLR), partial least squares regression (PLSR), and multiple nonlinear regression (MNLR). Additionally, an artificial neural network (ANN) was employed to capture non-linear relationships between molecular descriptors and biological activity.\n\nEach of these models was trained and validated independently. The training data for each model consisted of a set of molecular descriptors derived from a principal component analysis (PCA). The models were then validated using both external validation techniques and cross-validation methods, such as the leave-one-out procedure. This ensured that the models were reliable and had good predictive power.\n\nThe external validation involved testing the models on a separate set of molecules that were not included in the training set. This step is crucial for assessing the generalizability of the models to new, unseen data. The cross-validation technique, specifically the leave-one-out method, involved removing one observation at a time from the training set and predicting the biological activity of the removed sample. This process was repeated for each observation in the training set, ensuring that each observation was tested exactly once.\n\nThe results of these validation techniques demonstrated that the models had high statistical quality and were capable of accurately predicting the biological activity of the molecules in the test set. The correlation coefficients for the external validation were all above 0.6, indicating that the models were externally validated and could be used to predict the experimental activity of new compounds.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, the number of molecular descriptors was minimized using Principal Component Analysis (PCA) to reduce redundancy and transform the original correlated variables into independent principal components. This step was crucial for making the studied information less redundant and more manageable.\n\nFollowing PCA, the linear variation of these independent variables with the dependent variable, which in this case was analgesic activity, was tested using the Multiple Linear Regression (MLR) method with stepwise selection. This approach was particularly useful given the high collinearity between predictors and the fact that there were more predictors than observations.\n\nThe molecular descriptors selected through this process were then used as input parameters for both Multiple Nonlinear Regression (MNLR) and Artificial Neural Network (ANN) models. These models were developed to predict the effects of NMDA antagonists on the rat brain. Each model was evaluated based on statistical criteria such as the correlation coefficient (R), determination coefficient (R2), and adjusted coefficient (R2adj), all of which should tend towards 1 for a good model. Additionally, the mean square error (MSE) should be minimal, and the Fisher value (F) should be inferior to the critical value of Fisher, with a probability value (p-value) ideally under 5% for a 95% confidence level.\n\nTo ensure the reliability and generalizability of the models, external validation techniques were employed. This involved testing six new molecules constituting the test set and using cross-validation methods, including the \"leave-one-out\" procedure. This conventional procedure helps in examining the reliability of the developed models by removing a single example from the training set and applying the model to the remaining N-1 compounds. Furthermore, the \"Y-randomization\" technique was used to validate the models, ensuring the security of the statistical tests by randomly selecting a new training set from the full set of compounds.",
  "optimization/parameters": "In our study, we utilized five input parameters for our models. These parameters were selected through a rigorous process involving Principal Component Analysis (PCA) to minimize the number of molecular descriptors and reduce redundancy. The PCA was performed using XLSTAT 2014 software, which helped in transforming the original correlated variables into independent principal components. Following this, we employed the Multiple Linear Regression (MLR) method with stepwise selection to test the linear variation of these independent variables with the dependent variable, which in this case was the analgesic activity. The descriptors chosen were E lumo, \u03c7, \u00b5, Log P, and % O. These descriptors were found to significantly influence the biological activity, as indicated by their respective probabilities corresponding to the slope of each variable being less than 5%. The selection of these parameters ensured that our models were robust and capable of predicting the effects of NMDA antagonists on the rat brain effectively.",
  "optimization/features": "In our study, we initially considered a large number of molecular descriptors. To reduce redundancy and enhance the quality of our models, we performed feature selection using Principal Component Analysis (PCA). This step was crucial for minimizing the number of descriptors while retaining the most relevant information.\n\nWe used PCA to transform the original set of correlated variables into a smaller set of uncorrelated principal components. This process helped us identify and eliminate descriptors that shared similar information, thereby reducing the dimensionality of our data.\n\nAfter applying PCA, we selected five major uncorrelated descriptors. These descriptors were chosen based on their contribution to the principal components, with the first two components explaining a significant portion of the total variability in the data. This selection was performed using the training set only, ensuring that the feature selection process did not introduce any bias from the test set.\n\nThe five selected descriptors were then used as input features for our predictive models, including multiple linear regression (MLR), multiple nonlinear regression (MNLR), and artificial neural networks (ANN). This careful feature selection process helped improve the performance and generalizability of our models.",
  "optimization/fitting": "In our study, we employed several techniques to address the challenges of over-fitting and under-fitting, particularly given the complexity of our models and the number of descriptors relative to the training points.\n\nInitially, we minimized the number of molecular descriptors using Principal Component Analysis (PCA). This step was crucial in reducing redundancy and transforming the original correlated variables into independent principal components. This not only simplified the model but also helped in mitigating over-fitting by focusing on the most relevant information.\n\nFor the regression analysis, we utilized Multiple Linear Regression (MLR) with stepwise selection. This method ensured that only the most significant predictors were included, further reducing the risk of over-fitting. Additionally, we employed Partial Least Squares (PLS) regression, which is particularly effective when the number of predictors exceeds the number of observations and when there is strong collinearity among predictors. PLS helps in handling such scenarios by projecting the predictors and the response variables into a new space, thereby improving the model's predictive power and robustness.\n\nTo validate our models and ensure they were not over-fitted, we implemented several cross-validation techniques. The \"leave-one-out\" procedure was used, where each compound in the training set was temporarily removed, and the model was retrained and tested on the remaining data. This process was repeated for all compounds, providing a comprehensive assessment of the model's reliability.\n\nFurthermore, we conducted external validation using a separate test set of six new compounds. This step was essential in evaluating the generalizability of our models and ensuring they performed well on unseen data, thus ruling out over-fitting.\n\nTo address under-fitting, we ensured that our models were complex enough to capture the underlying patterns in the data. We used non-linear regression techniques, such as Multiple Non-linear Regression (MNLR) and Artificial Neural Networks (ANN), which are capable of modeling more intricate relationships. The statistical criteria we employed, including the correlation coefficient (R), determination coefficient (R\u00b2), and adjusted determination coefficient (R\u00b2adj), helped in assessing the model's fit and ensuring it was not too simplistic.\n\nIn summary, by combining PCA for dimensionality reduction, using robust regression techniques like PLS and MLR, and implementing rigorous validation methods, we effectively managed to avoid both over-fitting and under-fitting in our models.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was cross-validation, specifically the leave-one-out procedure. This technique involves removing each observation one at a time and testing the model's predictive power on the excluded data. By doing this iteratively, we ensured that each observation was used as a test case exactly once, providing a comprehensive evaluation of the model's internal validity.\n\nAdditionally, we performed external validation to assess the generalizability of our models. This involved testing the models on a separate test set of six new molecules, which were not part of the training data. The results of this validation showed high external validation correlation coefficients, indicating that our models can accurately predict the activities of unseen compounds.\n\nTo further validate the models, we conducted a Y-randomization test. This test involves randomly shuffling the response data (Y) and rebuilding the models to check if the high predictive power is due to a chance correlation. The results of this test confirmed that our models are robust and not the result of random chance, as the randomized models showed significantly lower predictive power compared to the original models.\n\nMoreover, we defined the applicability domain (AD) for our best QSAR model using leverage analysis. This analysis helps identify outliers that might affect the model's performance. The results showed that all compounds in the training and test sets had leverage values below the warning leverage threshold, indicating that the model's predictions are reliable and not influenced by outliers.\n\nThese techniques collectively ensured that our models are not overfitted and can be reliably used for predicting the activities of new compounds.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models developed in this study are not entirely black-box, as they incorporate interpretable techniques alongside more complex methods. The Multiple Linear Regression (MLR) model, for instance, is inherently transparent. It provides a clear equation that relates the predicted biological activity (pIC50) to specific molecular descriptors. This equation includes coefficients that indicate the contribution of each descriptor to the predicted activity, making it straightforward to interpret the influence of individual variables.\n\nFor example, the final QSAR model generated by the Multiple Non-Linear Regression (MNLR) technique presents a mathematical function that combines both linear and quadratic terms of the descriptors. This function allows for a detailed understanding of how changes in each descriptor affect the predicted biological activity. The coefficients in this equation provide insights into the strength and direction of these relationships.\n\nAdditionally, the Artificial Neural Network (ANN) model, while more complex, can still offer some level of interpretability. The architecture of the ANN used in this study includes an input layer with neurons representing selected descriptors, a hidden layer, and an output layer representing the observed activity values. Although the internal workings of the ANN are less transparent than those of the linear models, the relationships between the input descriptors and the output can be analyzed to some extent. Techniques such as sensitivity analysis can be employed to understand the impact of each descriptor on the network's predictions.\n\nIn summary, while some of the models used in this study are more complex and less transparent than others, efforts have been made to ensure that the relationships between molecular descriptors and biological activity are interpretable. The use of linear models and the analysis of ANN architectures contribute to the overall transparency of the predictive models developed.",
  "model/output": "The model developed in this study is a regression model, specifically a Quantitative Structure-Activity Relationship (QSAR) model. The primary goal was to predict the biological activity (pIC50) of molecules based on various molecular descriptors. Several regression techniques were employed, including Multiple Linear Regression (MLR), Partial Least Squares Regression (PLSR), Multiple Non-Linear Regression (MNLR), and Artificial Neural Networks (ANN).\n\nThe MLR model established a linear relationship between the biological activity and five key descriptors: E_lumo, \u03c7, \u00b5, Log P, and % O. The model's statistical criteria, such as the determination coefficient (R\u00b2 = 0.860) and the adjusted determination coefficient (R\u00b2_adjusted = 0.826), indicated a strong predictive competence with a minimal standard error (RMSE = 0.272).\n\nThe PLSR model also aimed to establish a linear relationship but with an emphasis on minimizing variance. It yielded a model with a determination coefficient of R\u00b2 = 0.758 and an RMSE of 0.314, demonstrating good adjustment and predictive power.\n\nThe MNLR model, on the other hand, explored non-linear relationships between the descriptors and the biological activity. This model showed a high coefficient of determination (R\u00b2 = 0.885) and a minimal RMSE of 0.285, indicating a strong non-linear relationship and good predictive competence.\n\nThe ANN model utilized a three-layer neural network to develop a non-linear relationship between the activity and molecular descriptors. The results showed an exceptionally high correlation of 97.7% between observed and predicted values, with a minimal residual standard error of 0.189 and an external validation correlation coefficient of 98.1%. This suggests that the descriptors selected by MLR are relevant and that the model has high statistical quality.\n\nOverall, the models developed are regression models aimed at predicting continuous biological activity values based on molecular descriptors. The use of multiple regression techniques allowed for a comprehensive evaluation of the relationships between the descriptors and the biological activity, ensuring robust and reliable predictions.",
  "model/duration": "The execution time for the models varied depending on the technique used. The artificial neural network (ANN) model, implemented using JMP version 8.0 software, operated over 16 turns with a convergence criterion of 0.00001 and a maximum of 75 iterations. This process involved an over-adjustment penalty of 2.734. The ANN model demonstrated a high correlation of 97.7% between observed and predicted values, with a minimal coded residual standard error of 0.189 and an external validation correlation coefficient of 98.1%.\n\nThe multiple linear regression (MLR) model, partial least squares regression (PLSR) model, and multiple non-linear regression (MNLR) model were also evaluated for their execution times. The MLR model showed an external validation correlation coefficient of 0.703, with an adjustment coefficient of 0.629 and a minimal root mean square error (RMSE) of 0.460. The PLSR model achieved an external validation correlation coefficient of 0.851, indicating a strong predictive power. The MNLR model, which involved a non-linear combination of descriptors, resulted in an external validation correlation coefficient of 98.1%, demonstrating its robustness and predictive competence.\n\nCross-validation with the leave-one-out procedure was applied to measure the effectiveness and reliability of the QSAR models. This technique involved removing each observation at each iteration, ensuring that each observation was tested exactly once. The quadratic coefficient (r2cv) obtained was 0.785, indicating that the models were reliable and had better internal predictivity. The Y-randomization test further confirmed the robustness of the models, ensuring that the results were not due to random chance.\n\nIn summary, the execution times for the models varied, with the ANN model requiring multiple iterations and turns to achieve convergence. The MLR, PLSR, and MNLR models also demonstrated efficient execution times, with high correlation coefficients and minimal errors, indicating their strong predictive capabilities.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several rigorous techniques to ensure the reliability and generalizability of the models. External validation was conducted using a test set of six new molecules, which were not part of the training data. This process involved comparing the observed and predicted activities, resulting in external validation correlation coefficients (R2ext) that demonstrated the models' predictive accuracy. For instance, the multiple linear regression (MLR) method yielded an R2ext of 0.703, while the partial least squares regression (PLSR) method achieved an R2ext of 0.851. These values indicate strong predictive performance.\n\nCross-validation with the leave-one-out procedure was also employed to assess the models' internal validity. This technique involved removing each observation one at a time and predicting its activity based on the remaining data. The quadratic coefficient (q2 or r2cv) obtained from this process was greater than 0.5, confirming the models' robustness and internal predictivity. Specifically, an r2cv value of 0.785 was achieved, which is well above the threshold of 0.5, indicating reliable internal validation.\n\nAdditionally, the Y-randomization test was performed to further validate the models. This test involved randomly shuffling the response data and rebuilding the models to ensure that the predictive power was not due to chance. The results showed that the randomized models had much lower R2 values compared to the original models, and the randomization constant (cR2p) was superior to 0.5, confirming the models' robustness and reliability.\n\nThe applicability domain was also defined to ensure that the models' predictions were reliable within a specific chemical space. Leverage analysis was conducted, and all compounds in the training and test sets had leverage values below the warning leverage (h*), indicating the absence of outliers and reliable predictions.\n\nOverall, the evaluation methods included external validation, cross-validation, Y-randomization test, and applicability domain analysis. These techniques collectively ensured that the models were reliable, robust, and capable of making accurate predictions within the defined chemical space.",
  "evaluation/measure": "In the evaluation of our predictive models, several performance metrics were reported to ensure a comprehensive assessment of their accuracy and reliability. For external validation, we focused on the correlation coefficient (R\u00b2ext), which measures the agreement between observed and predicted activities. This metric was reported for multiple regression models, with values such as 0.703, 0.851, and 0.778, indicating a strong predictive performance. Additionally, the adjustment coefficient (R\u00b2adjusted) and the root mean square error (RMSE) were provided to assess the model's fit and the accuracy of predictions, respectively. These metrics are widely used in the literature and are considered representative for evaluating the generalizability of predictive models.\n\nCross-validation was performed using the leave-one-out procedure, which is a standard method in the field. The quadratic coefficient (r\u00b2cv) was reported, with a value of 0.785, demonstrating the internal validity and robustness of the models. This metric is crucial for ensuring that the models are not overfitted and can generalize well to new data.\n\nThe Y-randomization test was also conducted to validate the models' robustness. This test involves randomizing the response data and checking if the models still perform well, which helps to ensure that the predictions are not due to chance. The results, including the average r, r\u00b2, Q\u00b2cv, and cR\u00b2p values, confirmed the reliability of the models.\n\nFurthermore, the applicability domain was defined using leverage analysis, which is a common approach to determine the chemical space where the models make reliable predictions. This analysis helps to identify any outliers that might affect the model's performance and ensures that the predictions are valid within the defined domain.\n\nOverall, the reported performance metrics are representative of those used in the literature and provide a thorough evaluation of the models' accuracy, reliability, and generalizability.",
  "evaluation/comparison": "In our study, we employed multiple regression techniques to develop predictive models for the effects of NMDA antagonists on the rat brain. To ensure the robustness and generalizability of our models, we conducted a thorough comparison using various validation techniques.\n\nWe utilized external validation on a test set of six new molecules to evaluate the accuracy of our predictive models. This approach is crucial for assessing the models' performance on unseen data, ensuring their applicability in clinical practice. The external validation results, presented in Table 5, demonstrate the effectiveness of our models, with external validation correlation coefficients (R\u00b2ext) ranging from 0.703 to 0.981. These values indicate that our models can reliably predict the observed activities of the test set molecules.\n\nIn addition to external validation, we applied cross-validation using the leave-one-out procedure. This method involves removing one observation at a time and predicting its biological activity based on the remaining data. The cross-validation results, summarized in Table 6, show a high quadratic coefficient (r\u00b2cv = 0.785), suggesting that our models are reliable and have strong internal predictivity.\n\nTo further validate our models, we performed a Y-randomization test. This technique involves randomly shuffling the dependent variable values and re-evaluating the models to ensure that the predictive power is not due to chance correlations. The Y-randomization test confirmed the robustness of our models, as the randomized models did not show significant predictive power.\n\nWe also compared our models to simpler baselines, such as multiple linear regression (MLR) and partial least squares regression (PLSR). The results, presented in Figures 5, 6, and 7, show that our models, developed using multiple nonlinear regression (MNLR) and artificial neural networks (ANN), outperformed the simpler baselines in terms of predictive accuracy and robustness.\n\nIn summary, our study involved a comprehensive comparison of different regression techniques and validation methods to ensure the reliability and generalizability of our predictive models. The use of external validation, cross-validation, and Y-randomization tests, along with comparisons to simpler baselines, provides strong evidence for the effectiveness of our models in predicting the effects of NMDA antagonists on the rat brain.",
  "evaluation/confidence": "To evaluate the confidence in our results, several validation techniques were employed to ensure the robustness and reliability of our QSAR models. The Y-randomization test, also known as the permutation test, was used to assess the model's robustness. This test involves repetitive randomizations of the response data (Y) to derive new models. The statistical regression data from these random models showed that the average R\u00b2 was significantly lower than that of the original data, indicating that our model is not due to random chance. The randomization constant (cR\u00b2p) was found to be superior to 0.5, further confirming the model's validity.\n\nCross-validation with the leave-one-out procedure was applied to measure the effectiveness and reliability of the QSAR models. This technique involves removing each observation once and predicting the biological activity of the deleted sample. The quadratic coefficient (r\u00b2cv) obtained was greater than 0.5, suggesting that the models have better internal predictivity. However, it is important to note that while cross-validation is essential, it is not sufficient on its own. Therefore, the Y-randomization test was necessary to complement this validation.\n\nExternal validation was performed to evaluate the accuracy and generalizability of the predictive models. Six new molecules constituting the test set were tested on a training set basis. The external validation correlation coefficients (R\u00b2ext) for the models established by MLR, PLSR, and MNLR methods were all greater than 0.6, indicating that the models are externally validated and can precisely predict experimental activity.\n\nThe applicability domain (AD) was defined to ensure that the model makes predictions with a given reliability within the chemical space. Leverage analysis, illustrated by the William diagram, was performed to identify any outliers that might affect the model's performance. The results showed that all chemical compounds in the training and test sets had leverage values inferior to the warning leverage (h*), indicating the absence of outliers and confirming the reliability of the predictions.\n\nIn summary, the performance metrics of our QSAR models were thoroughly validated using multiple techniques, including Y-randomization, cross-validation, and external validation. The results are statistically significant, and the models demonstrate robustness and reliability in predicting the biological activity of the compounds studied.",
  "evaluation/availability": "Not enough information is available."
}