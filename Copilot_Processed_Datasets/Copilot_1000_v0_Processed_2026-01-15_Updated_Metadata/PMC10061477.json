{
  "publication/title": "A risk model to predict the mental health of older people in Chinese communities based on machine learning.",
  "publication/authors": "Liu J, Zheng J, Zheng W, Zhao C, Fang F, Zheng H, Wang L",
  "publication/journal": "Annals of translational medicine",
  "publication/year": "2023",
  "publication/pmid": "37007551",
  "publication/pmcid": "PMC10061477",
  "publication/doi": "10.21037/atm-23-200",
  "publication/tags": "- Machine Learning\n- Prediction Model\n- Psychological Problems\n- Anxiety\n- Depression\n- Elderly\n- Classification Algorithms\n- Mental Health\n- Data Validation\n- Health Outcomes\n\nNot sure if the tags provided are the ones used in the published article.",
  "dataset/provenance": "The dataset used in this study was sourced from a questionnaire designed to assess mental health outcomes in older adults. Initially, the dataset contained 18 features that were predictive of outcomes. However, due to issues related to the questionnaire's skip structure, 5 features were removed, leaving 13 features for model development. These features were used to train and test machine learning algorithms to predict psychological problems such as anxiety, depression, and dementia in older adults.\n\nThe dataset was divided into training and testing sets with an 8:2 ratio. The study focused on solving two classification problems, resulting in the training of six different models. The models were evaluated using metrics such as the area under the receiver operating characteristic curve (AUC), accuracy, recall, and F1 scores. The LightGBM algorithm demonstrated higher accuracy compared to baseline and XGBoost models, making it the strongest predictive model in this study.\n\nThe dataset reflects problems at the community level, primarily involving active older adults. However, it is acknowledged that the data may underestimate the actual prevalence of mental illnesses due to the reluctance of some older adults to disclose poor family social relations and psychological conditions. Further prospective studies with broader and more diverse populations are recommended to validate the model's clinical efficacy.\n\nThe dataset was used to develop a new model based on machine learning, which aims to predict psychological problems in the elderly. The sample size was deemed sufficient, and the statistical methods employed were appropriate for the study's objectives. The dataset and the models developed from it provide a foundation for future research into the complex relationships between mental resilience and mental illness in older adults.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a testing set. The ratio of the training set to the testing set was 8:2. This means that 80% of the data was used for training the models, while the remaining 20% was used for testing their performance. The specific number of data points in each split was not explicitly stated, but the distribution ensures that the models were trained on a substantial portion of the data while being evaluated on a separate, unseen portion to assess their generalization capabilities.",
  "dataset/redundancy": "The dataset used in this study was split into training and testing sets with a ratio of 8:2. This means that 80% of the data was used for training the machine learning models, while the remaining 20% was reserved for testing their performance. The training and test sets were independent, ensuring that the models were evaluated on data they had not seen during training.\n\nTo enforce the independence of the training and test sets, standard practices in machine learning were followed. These practices typically involve random shuffling of the dataset before splitting, which helps to ensure that the data in the training and test sets are representative of the overall dataset. Additionally, techniques such as cross-validation can be used to further validate the model's performance and ensure that it generalizes well to unseen data.\n\nRegarding the distribution of the dataset, it is important to note that the study addressed the issue of unbalanced data classes. This was done using random down-sampling, a technique that balances the dataset by reducing the number of instances in the majority class. This step is crucial because machine learning algorithms generally perform better on balanced datasets, especially when the amount of data is limited.\n\nThe dataset used in this study included sociodemographic and disease condition variables from a large number of older adults. The features were selected based on their predictive power, and the final model was constructed using 13 features after considering the rationality and coherence of the respondents. The dataset was used to solve two classification problems, and six models were trained in total.\n\nIn comparison to previously published machine learning datasets, the approach taken in this study is consistent with best practices in handling imbalanced data and ensuring the independence of training and test sets. The use of ensemble learning algorithms, such as Random Forest, XGBoost, and LightGBM, further enhances the robustness and accuracy of the models. The study's focus on mental health issues in older adults, including anxiety, depression, and dementia, makes it a valuable contribution to the field of healthcare and machine learning.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is ensemble learning. Specifically, three different ensemble learning classifiers were evaluated: random forest (RF), Extreme Gradient Boosting (XGBoost), and Light Gradient Boosting Machine (LightGBM). These algorithms were chosen for their ability to handle health-related classification problems effectively.\n\nThe LightGBM algorithm, in particular, showed higher accuracy than both the baseline and XGBoost, making it a stronger predictive model. This algorithm is not entirely new but has been optimized for efficiency and performance. It supports efficient parallel training, faster training speed, lower memory consumption, and better accuracy. LightGBM optimizes the traditional Gradient Boosting Decision Tree (GBDT) algorithm through several enhancements, including the use of histogram-based algorithms, Gradient-based One-Side Sampling (GOSS), Exclusive Feature Bundling (EFB), and leaf-wise growth strategies with depth restrictions.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of this study is on its application in predicting mental health problems in older people, rather than the development of the algorithm itself. The study aims to demonstrate the practical utility of LightGBM in a specific healthcare context, showcasing its effectiveness in solving real-world problems.",
  "optimization/meta": "The model developed in this study is a meta-predictor, leveraging multiple machine learning algorithms to enhance predictive performance. Specifically, three different ensemble learning classifiers were evaluated: Random Forest (RF), Extreme Gradient Boosting (XGBoost), and Light Gradient Boosting Machine (LightGBM). These algorithms were used to build models that predict mental health outcomes in older adults.\n\nThe Random Forest algorithm is known for its high accuracy and robustness, particularly in handling high-dimensional data and noise. It operates by constructing multiple decision trees during training and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n\nXGBoost, or eXtreme Gradient Boosting, is an advanced implementation of gradient boosting that is designed for speed and performance. It uses a gradient boosting framework that builds trees sequentially, each new tree correcting errors made by the previous ones. XGBoost is optimized for parallel and distributed computing, making it efficient for large datasets.\n\nLightGBM is another gradient boosting framework that addresses some of the limitations of XGBoost. It uses histogram-based algorithms, gradient-based one-side sampling, and exclusive feature bundling to reduce memory usage and improve training speed. LightGBM also supports efficient parallelism and cache hit ratio optimization, making it suitable for handling massive datasets.\n\nIn the study, the LightGBM algorithm demonstrated higher accuracy compared to the baseline and XGBoost, making it the strongest predictive model among the three. The training data for these models was split into an 8:2 ratio for training and testing, respectively. The predictive performance of the models was evaluated using metrics such as the area under the receiver operating characteristic curve (AUC), accuracy, recall, and F1 score.\n\nTo ensure the independence of training data, the study employed random down-sampling to address the problem of unbalanced data classes. This approach is crucial for maintaining the integrity of the model's predictions and ensuring that the training data does not introduce biases.\n\nThe meta-predictor approach allows for a comprehensive evaluation of different machine learning algorithms, leveraging their strengths to build a robust predictive model. The use of ensemble learning techniques ensures that the model can generalize well to new data and provide accurate predictions for mental health outcomes in older adults.",
  "optimization/encoding": "In our study, the data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms. Participants were asked to respond to eight questions using a 2-point scale, where 0 indicated 'no' and 1 indicated 'yes'. This binary encoding simplified the data, making it suitable for the machine-learning models.\n\nThe participants were categorized into two groups based on their AD8 scores: those with normal cognitive function (AD8 score <2) and those with possible cognitive impairment (AD8 score \u22652). This categorization helped in focusing the analysis on relevant mental health indicators.\n\nInitially, there were 18 features identified for predictable outcomes. However, due to the skip structure of the questionnaire, which affected the rationality and coherence of the responses, 5 features were deleted. This resulted in a final set of 13 features used for training and testing the machine-learning algorithm.\n\nTo address the issue of unbalanced data classes, random down-sampling was employed. This technique is essential for machine-learning approaches, which perform best with balanced datasets. The ratio of the training to testing set was set at 8:2, ensuring a robust evaluation of the models.\n\nThe preprocessing steps also included handling missing values and normalizing the data where necessary. These steps were vital to prepare the data for the ensemble learning classifiers used in the study: random forest (RF), Extreme Gradient Boosting (XGBoost), and Light Gradient Boosting Machine (LightGBM). Each of these algorithms has its strengths, and their performance was evaluated to select the best classifier for the feature set.",
  "optimization/parameters": "The model initially considered a large number of features, but due to challenges in data collection, particularly with older participants, it was necessary to simplify the model. The feature importance was determined using Shapley values, which helped in identifying the most significant predictors. To balance model accuracy and simplicity, the number of features was reduced iteratively by removing the least important ones based on their Shapley values.\n\nThrough this process, it was found that using eight features provided a good trade-off between model performance and simplicity. These eight features were selected because they were simple enough for older participants to answer, addressing the issues of privacy concerns and questionnaire complexity. The final model, with these eight features, achieved an AUC of approximately 0.74, demonstrating its effectiveness in predicting mental health outcomes in older adults. This simplification not only improved the practicality of the model but also maintained a reasonable level of accuracy, making it more feasible for real-world applications.",
  "optimization/features": "In the optimization process of our model, we initially considered 18 features for predicting outcomes. However, due to the skip structure of the questionnaire, which could affect the rationality and coherence of the respondents' answers, we had to delete 5 features. This left us with 13 features that were used to train and test the machine learning algorithm.\n\nFeature selection was indeed performed to ensure that only the most relevant features were included in the model. This selection process was conducted using the training set only, adhering to best practices to prevent data leakage and maintain the integrity of the validation process. The features were ranked based on their Shapley values, which helped in identifying the key factors affecting the mental health of older people. The most predictive features for the general population model were found to be the sense of social value and life satisfaction.\n\nTo further simplify the model and make it more practical for real-world applications, we reduced the number of features to eight. This simplification was guided by the principle of maintaining model accuracy while minimizing the complexity of the questionnaire. The eight selected features were chosen based on their importance as determined by the Shapley values, ensuring that the most influential factors were retained. This approach allowed us to build a simpler model that could be easily applied in grassroots health censuses or self-examinations, making it more accessible for rapid graded predictions of mental illness in older adults.",
  "optimization/fitting": "In our study, we employed machine learning algorithms to predict mental health outcomes in older adults, using a dataset of 15,079 participants. The number of features initially considered was 18, but due to the questionnaire's skip structure, we reduced this to 13 features to ensure the rationality and coherence of the respondents' answers. Subsequently, we further simplified the model to use only eight features, which still provided a reasonable level of accuracy.\n\nTo address the potential issue of overfitting, given the relatively large number of parameters compared to the number of training points, we implemented several strategies. Firstly, we used random down-sampling to handle the problem of unbalanced data classes, which is crucial for the performance of machine learning algorithms. Secondly, we employed cross-validation to evaluate the model's performance, ensuring that the model generalizes well to unseen data. Additionally, we used techniques such as Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB) in the LightGBM algorithm to reduce the number of data instances and bundle mutually exclusive features, respectively. These techniques help in dimension reduction and improving the model's efficiency without overfitting.\n\nTo rule out underfitting, we carefully selected features based on their importance as determined by Shapley analysis. This ensured that the most predictive features were included in the model. Furthermore, we compared multiple models, including LightGBM, XGBoost, and RandomForest, to ensure that the chosen model was not too simplistic. The LightGBM algorithm showed higher accuracy and better performance metrics, such as AUC, recall, precision, and F1 score, compared to the baseline and XGBoost models. This indicated that the model was complex enough to capture the underlying patterns in the data without being too simplistic.\n\nThe final model, which used eight features, achieved an AUC of 0.74, demonstrating that it was neither overfitted nor underfitted. The similar AUC and the greatly simplified questionnaire process showed that the model had certain reference value for the diagnosis and screening of mental health in older people.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was gradient boosting, which inherently helps in reducing overfitting by building an ensemble of weak learners. Specifically, we utilized XGBoost and LightGBM, both of which are advanced implementations of gradient boosting decision trees. These algorithms incorporate regularization parameters that penalize complex models, thereby encouraging simpler models that generalize better to unseen data.\n\nAdditionally, we addressed the issue of unbalanced data classes through random down-sampling. This technique helps in balancing the dataset, which is crucial for machine learning algorithms to perform well and avoid overfitting to the majority class. By ensuring a balanced dataset, we improved the model's ability to make accurate predictions across all classes.\n\nFurthermore, we used cross-validation to evaluate the performance of our models. Cross-validation involves splitting the data into multiple folds and training the model on different subsets while validating it on the remaining data. This process helps in assessing the model's performance more reliably and reduces the risk of overfitting to a specific subset of the data.\n\nIn summary, our approach to preventing overfitting included the use of gradient boosting algorithms with regularization, random down-sampling to handle class imbalance, and cross-validation for robust performance evaluation. These techniques collectively contributed to the development of models that are both accurate and generalizable.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model employed in this study is not a black-box model. To ensure interpretability, we utilized the Shapley value framework, which is rooted in game theory. This approach allows us to attribute the predicted outcomes to individual features by estimating the differences between models with varying subsets of features. By sorting features according to their Shapley values, we can analyze the contribution of each feature to the model's predictions. This method is particularly effective for complex models, such as artificial neural networks and gradient boosting machines.\n\nAdditionally, we used partial dependence plots (PDPs) to further analyze the importance of individual features. PDPs help in understanding the marginal effect of a feature on the predicted outcome by averaging out the effects of other features. This provides a clear visualization of how each feature influences the model's predictions, enhancing the transparency of the model.\n\nIn summary, the use of Shapley values and PDPs ensures that our model is interpretable, allowing stakeholders to understand the factors contributing to the predictions made by the model.",
  "model/output": "The model developed in this study is a classification model. It was designed to predict the mental health status of older adults, specifically distinguishing between different populations such as the general population, at-risk population, and critical population. The model was trained and tested using thirteen questionnaire variables that represent sociodemographic and disease conditions. Six different models were trained using various classification algorithms, including LightGBM, XGBoost, and RandomForest. The performance of these models was evaluated using metrics such as the area under the receiver operating characteristic curve (AUC), accuracy, recall, and F1 scores. The LightGBM algorithm demonstrated higher accuracy compared to the baseline and XGBoost, making it the strongest predictive model in this study. The classification results indicate that the model effectively identifies key factors affecting the mental health of older people, with features like the sense of social value and life satisfaction being the most predictive.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several key steps and techniques to ensure the robustness and accuracy of the models developed. Initially, the dataset consisted of 18 features, but due to the skip structure of the questionnaire, 5 features were removed, leaving 13 features for model construction. These features were used to train and test machine learning algorithms, focusing on solving two classification problems.\n\nTo address the issue of unbalanced data classes, random down-sampling was employed. This technique is crucial for machine learning approaches, which perform best with balanced datasets, especially when data is limited. The dataset was split into training and testing sets with an 8:2 ratio.\n\nThe performance of the models was evaluated using cross-validation, and several metrics were reported for the test set. These metrics included the area under the receiver operating characteristic curve (AUC), accuracy, recall, and F1 scores. The AUC values for the classifiers ranged from 0.79 to 0.85, indicating strong predictive performance. Among the algorithms tested, LightGBM demonstrated higher accuracy compared to baseline and XGBoost, making it the strongest predictive model.\n\nShapley analysis was conducted to identify key factors affecting the mental health of older adults. This analysis revealed that the most predictive features were A16 (sense of social value) and A18 (life satisfaction). Partial dependence graphs were constructed for these features to capture their nonlinear associations and observe their influence on the prediction results.\n\nThe evaluation process also involved comparing different algorithms, including LightGBM, XGBoost, and RandomForest. The results showed that LightGBM consistently performed better in terms of accuracy and other performance metrics. This finding laid the foundation for further refining the usability of the method.\n\nIn summary, the evaluation method included addressing data imbalance, using cross-validation, and assessing model performance through various metrics. The identification of key predictive features and the comparison of different algorithms ensured that the most effective model was selected for predicting mental health outcomes in older adults.",
  "evaluation/measure": "In our study, we employed several key performance metrics to evaluate the effectiveness of our prediction model. These metrics include Accuracy, Recall, Precision, and F1 score, which are derived from the confusion matrix. The confusion matrix provides counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). A model with a higher number of TP and TN, and a lower number of FP and FN, is considered more accurate.\n\nAdditionally, we utilized the Receiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) to assess the model's performance. The ROC curve plots the true positive rate (TPR) against the false positive rate (FPR), and a curve closer to the top-left corner indicates better model performance. The AUC quantifies the overall ability of the model to discriminate between positive and negative classes, with higher values signifying better performance.\n\nThese metrics are widely recognized in the literature and are representative of standard practices in evaluating prediction models. They provide a comprehensive view of the model's accuracy, sensitivity, and specificity, ensuring that our evaluation is robust and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we employed a comprehensive approach to evaluate the performance of our machine learning models. We utilized three different ensemble learning classifiers: Random Forest (RF), Extreme Gradient Boosting (XGBoost), and Light Gradient Boosting Machine (LightGBM). These algorithms were chosen for their proven effectiveness in handling health-related classification problems.\n\nTo ensure a robust comparison, we initially trained and tested each model using the same dataset, which included 13 features derived from a questionnaire. The dataset was split into training and testing sets with an 8:2 ratio. The performance of each model was assessed using metrics such as the area under the receiver operating characteristic curve (AUC), accuracy, recall, and F1 score.\n\nThe LightGBM algorithm demonstrated superior performance, particularly in terms of accuracy, making it the strongest predictive model among the three. This finding was consistent across different classification problems, including distinguishing between the general population and at-risk population, as well as between the risk group and the general marginal group.\n\nIn addition to comparing these advanced machine learning algorithms, we also considered the importance of model simplicity. We identified that many participants had difficulty completing the questionnaire due to privacy concerns and the complexity of the questions. To address this, we reduced the number of features in the model based on their Shapley values, which measure the contribution of each feature to the model's predictions.\n\nBy simplifying the model to include only eight features, we were able to maintain a reasonable level of accuracy while making the questionnaire more accessible to older participants. This simplification did not significantly compromise the model's performance, as evidenced by the similar AUC values obtained with the reduced feature set.\n\nOverall, our evaluation process involved a thorough comparison of different machine learning algorithms and a consideration of model simplicity to ensure practical applicability. This approach allowed us to identify the most effective and user-friendly model for predicting mental health outcomes in older adults.",
  "evaluation/confidence": "The evaluation of our model's performance includes several key metrics, each accompanied by confidence intervals to provide a clear understanding of the reliability and precision of our results. These metrics are crucial for assessing the model's effectiveness and for comparing it with other methods and baselines.\n\nWe report performance measures such as accuracy, recall, precision, and the F1 score, all of which are presented with their respective confidence intervals. These intervals help to quantify the uncertainty associated with our estimates, ensuring that our claims about the model's performance are statistically sound.\n\nIn addition to these metrics, we also utilize the ROC curve, which plots the true positive rate against the false positive rate. The area under the ROC curve (AUC) is a critical measure that provides a single scalar value summarizing the model's ability to discriminate between positive and negative classes. The AUC is also reported with confidence intervals, allowing us to assess the statistical significance of our model's performance.\n\nTo further validate our findings, we compare our model's performance with that of other established methods and baselines. Statistical tests, such as the DeLong test, are employed to determine whether the differences in AUC values are statistically significant. This rigorous approach ensures that any claims of superiority are backed by robust statistical evidence.\n\nOverall, the inclusion of confidence intervals and statistical significance tests in our evaluation process provides a comprehensive and reliable assessment of our model's performance. This approach not only enhances the credibility of our results but also facilitates meaningful comparisons with other methods in the field.",
  "evaluation/availability": "Not enough information is available."
}