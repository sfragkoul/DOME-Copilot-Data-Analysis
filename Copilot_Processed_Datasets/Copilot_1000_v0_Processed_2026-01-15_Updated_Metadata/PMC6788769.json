{
  "publication/title": "Group sparse reduced rank regression for neuroimaging genetic study.",
  "publication/authors": "Zhu X, Suk HI, Shen D",
  "publication/journal": "World wide web",
  "publication/year": "2019",
  "publication/pmid": "31607788",
  "publication/pmcid": "PMC6788769",
  "publication/doi": "10.1007/s11280-018-0637-3",
  "publication/tags": "- Neuroimaging\n- Genetics\n- Machine Learning\n- Regression Analysis\n- Cross-Validation\n- Phenotypes\n- Genotypes\n- Brain Imaging\n- Alzheimer's Disease\n- Data Analysis",
  "dataset/provenance": "The dataset used in our study was obtained from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) database. This dataset is publicly available and can be accessed at http://adni.loni.usc.edu. The ADNI database is a well-known resource in the field of Alzheimer's disease research, providing up-to-date information and data for various studies.\n\nOur study utilized samples from 737 non-Hispanic Caucasian participants. These participants were categorized into three groups: 171 with Alzheimer's Disease (AD), 362 with Mild Cognitive Impairment (MCI), and 204 healthy Normal Controls (NC). Each participant had both MRI scans and genotype data available.\n\nThe MRI data consisted of raw Digital Imaging and Communications in Medicine (DICOM) scans, which were processed using a standard protocol. This protocol included spatial distortion correction, bias field correction, skull-stripping, cerebellum removal, intensity inhomogeneity correction, segmentation, and registration. The final output was gray matter volume measures of 93 cortical and subcortical regions for each MRI scan.\n\nThe genotype data was obtained from the ADNI Phase 1 cohort, which included 620,901 SNPs and copy number variations. The SNP of the APOE\u03b54 variant, which is separately genotyped by ADNI, was also included in our dataset. The genotype data underwent quality control and imputation steps to ensure its reliability. After processing, we obtained 2098 SNPs from 153 genes within a boundary of 20KB.\n\nThe ADNI dataset has been used in numerous previous studies and by the community, making it a valuable resource for Alzheimer's disease research. Our study builds upon this foundation by comparing our method with state-of-the-art techniques to advance the understanding of the associations between genotypes and neuroimaging phenotypes in Alzheimer's disease.",
  "dataset/splits": "In our study, we employed a three-fold cross-validation scheme to evaluate the performance of our method and competing approaches. This involved partitioning the entire dataset into three subsets. In each iteration of the cross-validation, one subset was designated as the testing dataset, while the remaining two subsets were combined to form the training set.\n\nThe dataset consisted of 737 non-Hispanic Caucasian participants, which were divided into three groups: 171 with Alzheimer's Disease (AD), 362 with Mild Cognitive Impairment (MCI), and 204 healthy Normal Controls (NC). These participants were genotyped, and their MRI scans were processed to obtain gray matter volume measures of 93 cortical and subcortical regions.\n\nFor model selection within the training set, we further employed a five-fold nested cross-validation. This process was repeated ten times, and the averaged results across these repetitions were reported. This rigorous cross-validation strategy ensured that our findings were robust and generalizable.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in this work were obtained from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) database. The ADNI dataset is publicly available and can be accessed through the ADNI website. The specific URL for accessing the data is http://adni.loni.usc.edu. For the most up-to-date information, users can refer to the ADNI website at www.adni-info.org.\n\nThe dataset includes both SNP and MRI data from 737 non-Hispanic Caucasian participants, comprising 171 individuals with Alzheimer's Disease (AD), 362 with Mild Cognitive Impairment (MCI), and 204 healthy Normal Controls (NC). These participants were genotyped by ADNI.\n\nThe MRI scans were processed using a standard protocol, which included spatial distortion correction, bias field correction, skull-stripping, cerebellum removal, intensity inhomogeneity correction, segmentation, and registration. The processed data resulted in gray matter volume measures of 93 cortical and subcortical regions for each MRI scan.\n\nThe genotype data were obtained from the ADNI Phase 1 cohort, which includes 620,901 SNPs and copy number variations. The SNP data underwent quality control and imputation steps to ensure data integrity. The final dataset included 2098 SNPs from 153 genes, within a boundary of 20K base pairs of Alzheimer\u2019s disease candidate genes listed on the AlzGene database.\n\nThe data splits used in the experiments followed a three-fold cross-validation scheme, where the dataset was partitioned into three subsets. One subset was used as the testing dataset, while the remaining two subsets were used as the training set. This process was repeated ten times, and the averaged results were reported.\n\nThe data and the methods used are publicly available, and the experiments can be replicated by following the procedures outlined in the publication. The ADNI dataset is released under a license that allows for research use, ensuring that the data can be accessed and utilized by the scientific community for further studies.",
  "optimization/algorithm": "The optimization algorithm employed in our work falls under the class of iterative optimization techniques, specifically utilizing the Iteratively Reweighted Least Square (IRLS) framework. This method is well-established in the field of statistical learning and optimization.\n\nThe algorithm is not entirely new; it builds upon existing principles of IRLS, which is a widely recognized approach for solving optimization problems with sparsity constraints. The novelty lies in the specific application and integration of this framework within our model, which combines reduced rank constraints with group sparsity constraints. This integration is tailored to address the unique challenges of feature selection in neuroimaging genetic studies.\n\nThe decision to publish this work in a journal focused on neuroimaging and genetics, rather than a machine-learning journal, is driven by the specific application and the domain expertise required to interpret the results. The primary focus of our research is on the biological and medical implications of the findings, particularly in the context of Alzheimer's Disease Neuroimaging Initiative (ADNI) data. While the optimization technique itself is rooted in machine learning, the innovative aspect of our work is the application of this technique to a complex biological dataset, demonstrating its practical utility in a real-world scenario. This interdisciplinary approach is crucial for advancing both the methodological and applied aspects of neuroimaging genetics.",
  "optimization/meta": "The optimization process described does not involve a meta-predictor. Instead, it focuses on an iterative optimization procedure using Iteratively Reweighted Least Square (IRLS) to update parameters b, B, and A. This process does not incorporate data from other machine-learning algorithms as input. The method directly optimizes the parameters within a single framework that combines reduced rank constraint and group sparsity constraints.\n\nThe optimization involves three main steps:\n\n1. Updating b with fixed B and A.\n2. Updating B with fixed b and A.\n3. Updating A with fixed b and B.\n\nThese steps are repeated until convergence. The process ensures that the selected regions of interest (ROIs) are associated with the selected single nucleotide polymorphisms (SNPs), conducting feature selection and subspace learning simultaneously.\n\nThe training data used in this optimization process is independent and comes from a three-fold cross-validation scheme. This scheme partitions the dataset into three subsets, where one subset is used as the testing dataset and the remaining two subsets are used as the training set. Additionally, a five-fold nested cross-validation is conducted within the training set for model selection. This ensures that the training data is independent and that the model's performance is evaluated robustly.",
  "optimization/encoding": "The data used in our study consisted of both SNP and MRI data obtained from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) database. The MRI scans were processed using a standard protocol, which included spatial distortion correction, bias field correction, skull-stripping, cerebellum removal, intensity inhomogeneity correction, segmentation, and registration. This processing pipeline allowed us to obtain gray matter volume measures of 93 cortical and subcortical regions for each MRI scan, characterizing the anatomy of the brain.\n\nFor the SNP data, we used the genotype data of non-Hispanic Caucasian participants from the ADNI Phase 1 cohort. The genotyping was performed using the Human610-Quad Bead-Chip, which includes 620,901 SNPs and copy number variations. The SNP data underwent a quality control step and an imputation step. The quality control step involved call rate checks, gender checks, sibling pair identification, Hardy-Weinberg equilibrium tests, marker removal by minor allele frequency, and population stratification. The imputation step filled in incomplete SNPs with the modal value. After these steps, we obtained 2098 SNPs from 153 genes within a boundary of 20K base pairs of Alzheimer\u2019s disease candidate genes listed on the AlzGene database.\n\nThe processed MRI data and SNP data were then used as inputs for our machine-learning algorithm. The MRI data was encoded as gray matter volume measures for the 93 regions of interest (ROIs), while the SNP data was encoded as the presence or absence of specific genetic variants. This encoding allowed our algorithm to analyze the relationships between genetic variations and brain anatomy, facilitating the identification of statistically meaningful associations.",
  "optimization/parameters": "In our optimization process, we primarily focus on updating three key parameters: b, B, and A. These parameters are iteratively updated using the Iteratively Reweighted Least Square (IRLS) method until convergence.\n\nThe parameter b is updated first with fixed B and A. This step involves minimizing the difference between the observed data Y and the predicted data XBAT, adjusted by an error term ebT. The update rule for b is derived by setting the derivative of the objective function with respect to b to zero, resulting in a closed-form solution.\n\nNext, B is updated with fixed b and A. This step involves substituting the updated b into the objective function and minimizing the resulting expression. The update for B is subject to a constraint that ensures the orthogonality of A.\n\nFinally, A is updated with fixed b and B. This step also involves minimizing the objective function, with the constraint that A remains orthogonal.\n\nThe number of parameters in our model is not explicitly stated as a single value p, as it depends on the dimensions of the matrices involved. However, the dimensions of these matrices are determined by the data, specifically the number of single nucleotide polymorphisms (SNPs) and regions of interest (ROIs) being analyzed.\n\nThe selection of the parameters is conducted through a model selection process using five-fold nested cross-validation. This process involves tuning the parameters over a range of values to find the combination that yields the best results in terms of Root-Mean-Square Error (RMSE) for the testing datasets. Specifically, we tuned the parameters with the range of {10\u22125, \u2026, 105}, and varied the rank number r in {2, 4, \u2026, 20} for our proposed method. Additionally, we selected the top {20, 40, 60, \u2026, 180, 200} genotypes to predict the phenotypes in our experiments.",
  "optimization/features": "The input features used in our study consist of single nucleotide polymorphisms (SNPs) and brain regions of interest (ROIs). Specifically, we started with 2098 SNPs from 153 genes, which were obtained after a quality control and imputation process from the ADNI Phase 1 cohort. These SNPs were selected within a boundary of 20K base pairs of Alzheimer\u2019s disease candidate genes.\n\nFeature selection was indeed performed as part of our optimization process. This involved discarding regressors or response variables whose corresponding coefficients in matrices B or A were zeros. The selection process ensured that only statistically meaningful ROIs from the brain imaging data (Y) and associated SNPs from the genetic data (X) were retained. This feature selection was conducted iteratively, alternating between subspace learning and group sparsity constraints, until the objective function converged. The iterative learning process ensured that the selected ROIs were associated with the selected SNPs, improving the performance by avoiding noise in the data.\n\nThe feature selection was performed using the training set only. We employed a three-fold cross-validation scheme, where the dataset was partitioned into three subsets. One subset was used as the testing dataset, while the remaining two subsets were used as the training set. Within the training set, a five-fold nested cross-validation was conducted for model selection, ensuring that the feature selection and parameter tuning were done solely on the training data. This approach helped in evaluating the generalizability of our method and preventing overfitting.",
  "optimization/fitting": "The fitting method employed in our study involves an optimization process that iteratively updates parameters to minimize an objective function. This process is designed to handle high-dimensional data, where the number of parameters can indeed be much larger than the number of training points.\n\nTo address the potential issue of over-fitting, we incorporated regularization techniques. Specifically, we used group sparsity constraints and reduced rank constraints. The group sparsity constraints help in selecting a subset of relevant features (SNPs and ROIs) by driving some coefficients to zero, thus effectively reducing the model complexity. The reduced rank constraints further aid in dimensionality reduction by imposing a low-rank structure on the data, which helps in capturing the underlying relationships without overfitting to the noise in the data.\n\nAdditionally, we employed a nested cross-validation scheme within our three-fold cross-validation setup. This nested structure ensures that the model selection process is robust and that the chosen parameters generalize well to unseen data. By repeating the entire process ten times and reporting the averaged results, we further mitigate the risk of overfitting and ensure the stability and reliability of our findings.\n\nTo rule out under-fitting, we carefully tuned the parameters within a specified range and varied the rank number. This thorough parameter tuning process helps in finding the optimal balance between bias and variance, ensuring that the model is neither too simple nor too complex. The experimental results, which show statistically significant improvements over competing methods, indicate that our model is well-fitted to the data without underfitting.",
  "optimization/regularization": "In our optimization process, we employed several regularization techniques to prevent overfitting and improve the generalization of our model. Specifically, we utilized both reduced rank constraints and group sparsity constraints.\n\nThe reduced rank constraint helps in conducting subspace learning on both the regressor matrix (X) and the response matrix (Y). This constraint ensures that the predicted matrix (Y) has a rank less than r, implying that each column of Y can be represented by a linear combination of at most r latent response variables. This approach considers the intra-relations within the genotypes and phenotypes, effectively reducing the complexity of the model and preventing overfitting.\n\nAdditionally, we incorporated group sparsity constraints through the use of \ud835\udcc12,1-norm regularizers on the coefficient matrices B and A. These regularizers penalize the coefficients in a row-wise manner, promoting joint selection or un-selection of the regressors and response variables. This results in a sparse solution where only the most relevant features (SNPs) and response variables (ROIs) are selected, further reducing the risk of overfitting.\n\nThe combination of these constraints allows our model to effectively handle high-dimensional data by selecting a subset of statistically meaningful brain regions of interest (ROIs) and single nucleotide polymorphisms (SNPs). This iterative process of feature selection and subspace learning continues until the objective function converges, yielding optimal results for both feature selection and subspace learning.",
  "optimization/config": "The optimization process involves iteratively updating parameters b, B, and A using the Iteratively Reweighted Least Square (IRLS) method. The specific steps for updating each parameter are detailed in the optimization subsection. The update rules for b, B, and A are provided in equations (8), (13), and (15) respectively. These equations outline the mathematical formulations used to adjust the parameters during each iteration.\n\nThe pseudo code of the optimization algorithm is provided in Algorithm 1, which includes the initialization and iterative update steps. This algorithm ensures that the objective function monotonically decreases after each iteration, leading to convergence.\n\nRegarding the availability of hyper-parameter configurations, optimization schedule, model files, and optimization parameters, the publication does not explicitly mention where these details can be accessed or under what license. Therefore, it is not clear if these configurations are publicly available or if there are any restrictions on their use.",
  "model/interpretability": "The model proposed in this work is designed to be interpretable, rather than a black-box model. The interpretability stems from the formulation of the BW-CGA study as a regression problem, where genotypes and phenotypes are treated as regressors and responses, respectively. By finding optimal weight coefficients in a regularized linear regression model, the method aims to discover inherent relations between phenotypes and genotypes. These relations are interpretable through the linear feature selection model used.\n\nThe model incorporates a reduced rank constraint on the weight coefficient matrix, which helps in transforming variables into subspaces where relations among variables are more easily understood. This constraint is based on the rationale that high-dimensional data often have reduced rank structures due to inherent noise and redundancy. Additionally, a group sparsity constraint (an \ud835\udcc12,1-norm regularizer) is applied to each reduced rank matrix, ensuring that highly informative phenotypes and genotypes are selected. This joint use of reduced rank and group sparsity constraints helps in selecting a subset of brain regions and a subset of genotypes that show high associations.\n\nThe feature selection process is conducted by discarding regressors or response variables whose corresponding coefficients are zero. This results in the selection of a subset of brain regions (ROIs) from the phenotypes and a subset of single nucleotide polymorphisms (SNPs) from the genotypes. The selected ROIs and SNPs are statistically meaningful and associated with each other, providing a clear interpretation of the model's predictions.\n\nThe optimization process involves iteratively updating parameters to ensure convergence, using methods like Iteratively Reweighted Least Square (IRLS). This iterative learning yields optimal results for both feature selection and subspace learning, further enhancing the model's interpretability. The selected features and their associations are visually presented in figures, such as Figure 2 and Figure 3, which show the 'Frequency' value and coefficients of the selected SNPs and ROIs. These visualizations help in understanding which specific genotypes and phenotypes are most informative and how they are related.\n\nIn summary, the model's transparency is achieved through the use of interpretable linear regression, reduced rank constraints, and group sparsity constraints. These elements work together to select and highlight meaningful associations between genotypes and phenotypes, making the model's predictions understandable and interpretable.",
  "model/output": "The model presented in this work is a regression model. Specifically, it is a sparse reduced rank regression model designed for neuroimaging genetic studies. The model regards genotypes as regressors and phenotypes as responses, aiming to discover inherent relations between them and predict phenotypes from new genotype samples. The regression framework includes a reduced rank constraint on the weight coefficient matrix and a group sparsity constraint to select informative phenotypes and genotypes. The output of the model includes the selected subsets of brain regions (ROIs) and genotypes (SNPs) that show high associations, as well as the predicted phenotypes. The performance of the model is evaluated using metrics such as Root-Mean-Square Error (RMSE) and the frequency of selected genotypes and phenotypes. The model's output is visualized through figures that present the RMSE performance, the frequency of selected SNPs and ROIs, and the coefficients of the selected features. The iterative optimization process ensures that the selected ROIs are associated with the selected SNPs, providing a meaningful and interpretable output for neuroimaging genetic studies.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our evaluation, we employed a three-fold cross-validation scheme to assess all the methods. The dataset was divided into three subsets, with one subset used for testing and the remaining two for training. For model selection within the training set, we conducted a five-fold nested cross-validation to identify the parameter combination that yielded the best Root-Mean-Square Error (RMSE) results for the testing datasets. This entire process was repeated ten times for each method, and the results were averaged across these repetitions.\n\nDuring model selection, we tuned the parameters across a range of values from 10^-5 to 10^5. Additionally, we varied the rank number r from 2 to 20 for our proposed method. We also selected the top 20, 40, 60, up to 200 genotypes to predict the phenotypes, following established literature.\n\nTwo primary evaluation metrics were used: Root-Mean-Square Error (RMSE) and 'Frequency'. RMSE measures the prediction accuracy, while 'Frequency' indicates how often specific genotypes or phenotypes were selected across all experiments, with values ranging from 0 to 1.",
  "evaluation/measure": "In our evaluation, we utilized two primary performance metrics to assess the effectiveness of our methods. The first metric is the Root-Mean-Square Error (RMSE), which measures the average magnitude of the errors between predicted and actual values. RMSE is a widely used metric in regression tasks and provides a clear indication of the prediction accuracy. Lower RMSE values signify better performance.\n\nThe second metric is 'Frequency', which represents the frequency with which specific genotypes (or phenotypes) are selected across all experiments. This metric ranges from 0 to 1 and helps in understanding the consistency and reliability of the selected features. A higher frequency indicates that the selected genotypes or phenotypes are more consistently important across different experimental runs.\n\nThese metrics are representative of the literature in the field of neuroimaging genetics. RMSE is a standard metric for evaluating prediction accuracy, while 'Frequency' provides insights into the stability and robustness of feature selection. Together, these metrics offer a comprehensive evaluation of our methods' performance in predicting phenotypes from genotypes.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our proposed method against several publicly available and widely used methods in both statistical learning and medical image analysis. These competing methods included standard regularized Multi-output Linear Regression (MLR), sparse feature selection with an \ud835\udcc12, 1-norm regularizer (L21), Group sparse Feature Selection (GFS), sparse Canonical Correlation Analysis (CCA), and sparse Reduced-Rank Regression (RRR). These methods represent a range of approaches, from widely used techniques to state-of-the-art methods specifically designed for neuroimaging genetic studies.\n\nAdditionally, we included a simpler baseline method, which is a special case of our proposed method. This baseline method removes the group sparsity constraint, focusing solely on SNP selection without considering feature selection on genotype data. This comparison allowed us to demonstrate the effectiveness of our method's combined approach of reduced rank constraint and group sparsity constraint.\n\nThe evaluation was performed on the ADNI Phase 1 cohort dataset, which consists of 620,901 SNPs processed through quality control and imputation steps. We selected 2098 SNPs from 153 genes within a 20KB boundary for our experiments. The comparison involved using three-fold cross-validation and five-fold nested cross-validation for model selection, ensuring robust and reliable results. We evaluated the methods using two metrics: Root-Mean-Square Error (RMSE) and 'Frequency', which measures the frequency of selected genotypes or phenotypes across experiments.\n\nOur results showed that our method achieved the best RMSE performance, indicating superior predictive accuracy for phenotypes. The method also demonstrated statistical significance in improvements over the competing methods, as evidenced by paired-sample t-tests. Furthermore, the stability and effectiveness of our method were highlighted, showing its superiority in combining reduced rank and group sparsity constraints. The baseline method, while simpler, still outperformed other competing methods, underscoring the importance of simultaneous selection of genotypes and phenotypes in neuroimaging genetic studies.",
  "evaluation/confidence": "In our evaluation, we employed a robust three-fold cross-validation scheme, repeated ten times, to ensure the reliability of our results. This approach allowed us to report averaged results, providing a stable measure of performance.\n\nTo assess the statistical significance of our method's superiority, we conducted paired-sample t-tests. The p-values between our method and each of the competing methods were found to be less than 0.00001, indicating a highly significant improvement. This statistical analysis confirms that our method's performance is not due to random chance but reflects a genuine advantage.\n\nAdditionally, we used two evaluation metrics: Root-Mean-Square Error (RMSE) and 'Frequency'. The RMSE results included both mean and standard deviation, offering a clear view of the performance variability. The 'Frequency' metric, ranging from 0 to 1, provided insights into the consistency of genotype and phenotype selection across experiments.\n\nThe RMSE performance of all methods showed a clear trend: as the number of selected genotypes (SNPs) increased, the RMSE decreased, indicating better predictive performance. Our method consistently achieved the best RMSE results, demonstrating its effectiveness in phenotype prediction.\n\nFurthermore, the stability of our method was superior, highlighting the benefits of combining a reduced rank constraint with a group sparsity constraint. This stability is crucial for reliable and consistent performance in real-world applications.\n\nIn summary, our evaluation provides strong evidence of our method's superior performance, supported by statistical significance and robust performance metrics.",
  "evaluation/availability": "Not enough information is available."
}