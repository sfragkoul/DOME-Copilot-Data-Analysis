{
  "publication/title": "Image Segmentation of Operative Neuroanatomy Into Tissue Categories Using a Machine Learning Construct and Its Role in Neurosurgical Training.",
  "publication/authors": "Witten AJ, Patel N, Cohen-Gadol A",
  "publication/journal": "Operative neurosurgery (Hagerstown, Md.)",
  "publication/year": "2022",
  "publication/pmid": "36103318",
  "publication/pmcid": "PMC10637405",
  "publication/doi": "10.1227/ons.0000000000000322",
  "publication/tags": "- Artificial Intelligence\n- Convolutional Neural Network\n- Image Segmentation\n- Neuroanatomy\n- Neurosurgery\n- Machine Learning\n- Deep Learning\n- Medical Imaging\n- Surgical Training\n- ResNet\n- PSPNet\n- Data Augmentation\n- Medical Education\n- Surgical Procedures\n- Neural Networks",
  "dataset/provenance": "The dataset used in this study was primarily composed of gross neuroanatomy images. These images were sourced from a comprehensive collection provided by The Neurosurgical Atlas. After cleaning the data, approximately 879 uniquely labeled neuroanatomy images were identified for use. These images were then separated into training, validation, and testing datasets. Specifically, 679 images were allocated for training, 100 for validation, and 100 for testing. The dataset includes a variety of neuroanatomic structures and their corresponding labels, which were used to train and validate our convolutional neural network (CNN) for image segmentation. The dataset was chosen for its detailed and comprehensive labeling of neuroanatomic structures, which is crucial for accurate segmentation and classification tasks.",
  "dataset/splits": "The dataset used in this study was composed of approximately 879 uniquely labeled neuroanatomy images. These images were divided into three distinct splits: training, validation, and testing. The training set consisted of 679 images, which were used to train the convolutional neural network (CNN). The validation set contained 100 images and was utilized to tune the model's hyperparameters and prevent overfitting during the training process. Similarly, the testing set also comprised 100 images, which were used to evaluate the final performance of the trained model. The allocation of images to these splits was done randomly to ensure a representative distribution across all categories.",
  "dataset/redundancy": "The dataset used in this study consisted of approximately 879 uniquely labeled neuroanatomy images, which were separated into training, validation, and testing subsets. The training set comprised 679 images, while the validation and test sets each contained 100 images. These allocations were made randomly to ensure independence between the training and test sets.\n\nTo enforce the independence of the datasets, the images were randomly assigned to each subset. This randomization process helped to mitigate any potential biases that could arise from non-random splitting. The goal was to create a robust model that could generalize well to unseen data, which is crucial for the reliability of the segmentation network in clinical settings.\n\nThe distribution of the dataset reflects a right-skewed overall distribution, indicating that the data is not completely uniform. Many images were only partially labeled, and a few had significant amounts of unlabeled space. These unlabeled spaces were ignored during training to prevent skewing the results. The dataset's heterogeneity and the presence of unlabeled spaces highlight the challenges in creating a perfectly balanced dataset for neuroanatomy segmentation.\n\nCompared to previously published machine learning datasets, our dataset is relatively small, consisting of 879 images. This size is a limitation, as a larger and more diverse dataset would likely yield better network performance. Increasing the size of the training dataset can reduce spurious correlations that cause overfitting and improve the generalizability of the model. However, despite its size, our dataset has shown promising results, with a total pixel accuracy of 91.8% on the test set, demonstrating the model's robustness in performing anatomic category recognition.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used is convolutional neural networks (CNNs). Specifically, the network architecture is based on a pyramidal scene-parsing network (PSPNet) with a ResNet-152 backbone. This combination leverages the strengths of both architectures: the ResNet backbone enables the computation of a deeper feature set, while the PSPNet allows the network to learn its environment and objects in a larger general context.\n\nThe algorithm is not entirely new, as both ResNet and PSPNet are established architectures in the field of computer vision. However, the specific implementation and modifications made for this application are novel. The decision to publish in a neurosurgical journal rather than a machine-learning journal is likely due to the focus on the application of this technology in neurosurgical training and practice. The primary goal is to demonstrate the potential of this AI model in enhancing the operating experience during complex neurosurgical cases, rather than introducing a new machine-learning algorithm.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding process involved cleaning and identifying approximately 879 uniquely labeled neuroanatomy images. These images were then divided into training, validation, and testing datasets. The training dataset consisted of 679 images, while the validation and test datasets each contained 100 images.\n\nData augmentation techniques, including rotation, random cropping, blurring, and flipping, were applied iteratively to enhance the generalizability of the network and prevent overfitting. These techniques helped in creating a more robust model by introducing variations in the training data.\n\nUnlabeled spaces within the images were ignored during training to avoid skewing the results. This approach ensured that the model focused on the labeled regions, which were crucial for accurate segmentation.\n\nThe network was designed to handle multiclass segmentation, allowing it to classify various neuroanatomic structures. The training process involved using an Adam optimizer with differential learning rates for the ResNet backbone and the PSPNet, both with a weight decay of 10^-4. A batch size of 4 was used during training.\n\nThe model was trained for over 300 epochs, with validation performed every 5 epochs. This extensive training period, which took 48 hours on an NVIDIA V100 graphics processing unit, ensured that the network could accurately segment neuroanatomy images. The final model demonstrated a total pixel accuracy of 91.8% on the test dataset, indicating its robustness in performing anatomic category recognition in a clinical setting.",
  "optimization/parameters": "The model utilized a residual neural network (ResNet) backbone, specifically ResNet-152, which is composed of 152 residual blocks. This backbone was integrated with a pyramidal scene-parsing network (PSPNet) to form the complete architecture. The ResNet-152 backbone is known for its depth and complexity, enabling the computation of a deeper feature set. The PSPNet component further enhances the model's ability to learn global associations within the images.\n\nThe selection of the ResNet-152 backbone was driven by its proven effectiveness in extracting detailed features from images, which is crucial for accurate segmentation in neuroanatomy. The PSPNet was chosen for its ability to capture contextual information, allowing the model to understand the relationships between different parts of an image. This combination ensures that the model can accurately segment and classify various neuroanatomic structures.\n\nThe training process involved using an Adams optimizer with differential learning rates. A learning rate of 10^-5 was used for the ResNet backbone, focusing on more conserved features, while a learning rate of 10^-4 was used for the PSPNet, allowing it to develop more diverse features for classification. A batch size of 4 was employed during training.\n\nThe model was trained for over 300 epochs, which took approximately 48 hours on an NVIDIA V100 graphics processing unit. Data augmentation techniques, including rotation, random cropping, blurring, and flipping, were applied to increase the generalizability of the network and prevent overfitting. The dataset was divided into 679 training images, 100 validation images, and 100 test images, allocated randomly.\n\nThe model demonstrated excellent accuracy, achieving a total testing accuracy of 91.8% on previously unseen images. The training accuracy was 97.1%, and the validation accuracy was 94.5%, indicating the model's robustness in performing anatomic category recognition in a clinical setting.",
  "optimization/features": "The input features for our convolutional neural network (CNN) model consist of color images with three channels, corresponding to the red, green, and blue components. Therefore, the number of input features (f) is three.\n\nFeature selection in the traditional sense was not performed, as our model directly processes the raw pixel data of the images. However, the network architecture itself can be seen as a form of feature extraction and selection. The ResNet backbone, composed of multiple residual blocks, learns to extract relevant features from the input images. These features are then passed to the pyramidal scene-parsing network (PSPNet), which further refines and selects the most informative features for the segmentation task.\n\nThe training process involved splitting the dataset into training, validation, and testing sets. The feature extraction and selection were performed using the training set only, ensuring that the validation and testing sets remained unseen during this process. This approach helps to prevent data leakage and ensures that the model's performance on the validation and testing sets is a true reflection of its generalization capability.",
  "optimization/fitting": "The model employed in this study is a convolutional neural network (CNN) based on a pyramidal scene-parsing network (PSPNet) with a ResNet-152 backbone. This architecture is designed to handle a large number of parameters, which is typical for deep learning models aiming to capture complex patterns in image data.\n\nThe number of parameters in the model is indeed much larger than the number of training points. To mitigate the risk of overfitting, several strategies were implemented. Firstly, data augmentation techniques such as rotation, random cropping, blurring, and flipping were used to increase the diversity of the training data. This helps the model to generalize better by preventing it from memorizing the training examples.\n\nSecondly, a differential learning rate was employed, with a higher learning rate for the ResNet backbone to focus on more conserved features and a lower learning rate for the PSPNet to develop more diverse features for classification. This approach helps in fine-tuning the model more effectively.\n\nAdditionally, the model was trained for over 300 epochs, with validation checks every 5 epochs. This extensive training period, combined with the use of a validation set, ensures that the model's performance is monitored closely, and adjustments can be made to prevent overfitting.\n\nTo rule out underfitting, the model's performance was evaluated on a separate test set that was not seen during training. The model achieved a total testing accuracy of 91.8% on 100 previously unseen images, indicating that it has learned the underlying patterns in the data effectively. The training accuracy was 97.1%, and the validation accuracy was 94.5%, suggesting that the model is well-fitted to the data without being overly complex.\n\nOverall, the combination of data augmentation, differential learning rates, extensive training, and rigorous validation ensures that the model is neither overfitted nor underfitted, providing robust performance in segmenting neuroanatomy images.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and enhance the generalizability of our model. One key method was data augmentation, which involved applying transformations such as rotation, random cropping, blurring, and flipping to the training images. These augmentations helped to artificially increase the diversity of the training dataset, making the model more robust and less likely to overfit to the specific characteristics of the original images.\n\nAdditionally, we used a differential learning rate strategy during training. This approach involved setting different learning rates for the ResNet backbone and the PSPNet components of our network. By focusing the ResNet on more conserved features and allowing the PSPNet to develop more diverse features, we aimed to improve the model's ability to generalize to new, unseen data.\n\nAnother important technique was the use of an auxiliary branch in the PSPNet architecture. This branch helped to prevent vanishing gradients during training, which can be a common issue in deep neural networks. By including this auxiliary branch, we ensured that the network could effectively learn and propagate gradients throughout its layers, leading to better performance and reduced overfitting.\n\nFurthermore, we trained the network for an extended period of over 300 epochs, which allowed the model to thoroughly learn from the training data without becoming overly specialized to it. This long training duration, combined with the use of a validation dataset to monitor performance, helped to ensure that the model generalized well to new data.\n\nLastly, the use of a weight decay regularization term in the optimizer helped to prevent the model from becoming too complex and overfitting to the training data. This regularization technique encouraged the model to keep its weights small, which in turn promoted better generalization to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we utilized an Adam optimizer with differential learning rates of 10^-5 for the ResNet backbone and 10^-4 for the PSPNet, both with a weight decay of 10^-4. The network was trained for over 300 epochs with a batch size of 4, and validation was performed every 5 epochs. Data augmentation techniques such as rotation, random cropping, blurring, and flipping were employed to enhance the generalizability of the model and prevent overfitting.\n\nThe model files and optimization parameters are not directly available in the publication. However, the network was implemented using open-source PyTorch segmentation tools, which are accessible at https://github.com/yassouali/pytorch_segmentation. Modifications were made to the provided code to suit our specific application. The license for these tools is typically open-source, allowing for modification and use under specified conditions.\n\nFor those interested in replicating or building upon our work, the detailed configurations and methods are provided in the publication, and the open-source tools offer a foundation for further development.",
  "model/interpretability": "The model employed in our study is not entirely a black box, as it incorporates several design elements that enhance its interpretability. The architecture is based on a Pyramid Scene Parsing Network (PSPNet) with a ResNet-152 backbone, which allows for a hierarchical understanding of the input images. This structure enables the model to learn both local and global features, making it more interpretable compared to purely convolutional networks.\n\nOne key aspect of interpretability in our model is the use of convolutional layers and max pooling operations. These operations help in extracting and downsampling features from the input images, respectively. By visualizing the activation maps of these layers, one can gain insights into which parts of the image the model is focusing on for its predictions. For instance, specific convolutional layers can highlight regions of interest, such as the vein of Trolard or the surface of the Sylvian fissure, demonstrating the model's ability to identify and classify neuroanatomic structures accurately.\n\nAdditionally, the model's performance on validation and testing datasets provides further evidence of its interpretability. The high accuracy rates, particularly in correctly labeling surgical neuroanatomy, indicate that the model has learned meaningful patterns and features from the training data. For example, the model's ability to correctly identify the vein of Trolard in surgical images, despite the lack of surgical images in the training dataset, showcases its capability to generalize and interpret new, unseen data.\n\nMoreover, the use of data augmentation techniques, such as rotation, random cropping, blurring, and flipping, helps in making the model more robust and interpretable. These techniques ensure that the model does not overfit to specific patterns in the training data and can generalize well to new, unseen images. The learning curves for both the training and validation phases further support the model's interpretability, as they show consistent improvement and convergence over epochs.\n\nIn summary, while the model is complex and involves deep learning techniques, it is not a black box. The use of a hierarchical architecture, convolutional layer visualizations, and data augmentation techniques all contribute to its interpretability. The model's high accuracy and ability to generalize to new data further demonstrate its capacity to provide meaningful and interpretable results in the context of neuroanatomic image segmentation.",
  "model/output": "The model is designed for image segmentation, which is a type of classification task at the pixel level. It is built using the PSPNet framework with a ResNet-152 backbone. This network accepts a color image with three channels and predicts the probability that each pixel belongs to a given class. The model was trained to recognize various neuroanatomic structures, such as arteries, tissue, nerves, veins, and bone.\n\nThe output of the model is a segmented image where each pixel is classified into one of the predefined categories. The performance of the model was evaluated using several metrics, including accuracy, intersection-over-union (IoU), dice similarity coefficient (DSC), precision, recall, and boundary F1 (BF) score. These metrics provide a comprehensive assessment of the model's ability to correctly identify and segment different types of neuroanatomic structures.\n\nDuring testing, the model achieved a total testing accuracy of 91.8% on 100 previously unseen images. The training accuracy was 97.1%, and the validation accuracy was 94.5%. These results indicate that the model is robust and capable of performing anatomic category recognition in a clinical setting. However, there are some limitations, such as the mislabeling of cranioplasty clips as veins due to their similar color to arterial tissue. The model also struggled with classifying structures that transition between different tissue types, such as nerves and brain tissue.\n\nThe confusion matrix further illustrates the model's performance, showing clear identification of most tissue types. The least accurate results were observed in distinguishing between nerve and brain tissue, which is likely due to the challenges in manually labeling the transition points between these tissues. Despite these limitations, the model demonstrates promising results for image segmentation in operative neurosurgery, with potential applications in neurosurgical training and real-time identification of neuroanatomic structures during surgery.",
  "model/duration": "The model was trained for over 300 epochs, which took 48 hours on an NVIDIA V100 graphics processing unit. The network was tested using a validation data set every 5 epochs during training.",
  "model/availability": "The source code for the network was implemented using the open-source PyTorch segmentation tools, which are available on GitHub. Modifications were made to the original code to suit our specific application. The code is not publicly released. Not applicable.",
  "evaluation/method": "The evaluation of our method involved several key metrics to assess the segmentation performance quantitatively. These metrics included pixel accuracy, intersection-over-union (IoU), the Dice similarity coefficient (DSC), precision, recall, and the boundary F1 (BF) score. All evaluations were conducted using Python scripts.\n\nPixel accuracy was calculated as the percentage of correctly classified pixels, using the formula:\n\npixel accuracy = TP + TN / TP + TN + FP + FN\n\nwhere TP and TN represent true-positive and true-negative instances, respectively, and FP and FN represent false-positive and false-negative instances, respectively.\n\nIoU was determined by the area of overlap between the predicted segmentation and the labeled images over the area of union between them. The formula used was:\n\nIoU = intersection / union = TP / TP + FP + FN\n\nThe DSC, also known as the S\u00f8rensen\u2013Dice coefficient, measured the similarity between the predicted and actual labels. It was calculated as:\n\nDSC = 2TP / 2TP + FP + FN\n\nPrecision was defined as the fraction of retrieved true-positive instances among all retrieved positive instances, calculated as:\n\nprecision = TP / TP + FP\n\nRecall represented the retrieved true-positive instances among all relevant instances, calculated as:\n\nrecall = TP / TP + FN\n\nThe BF score, which characterizes the harmonic mean of precision and recall, was calculated using the formula:\n\nBF score = 2TP / 2TP + FN + FP\n\nThe dataset was first analyzed to understand the distribution of labeling. The network was then trained, and its performance was assessed using these metrics. The results showed a total pixel accuracy of 97.1% during training and 91.8% during testing. The highest accuracy by class was for labeled arteries (94.5%), and the lowest was for nerve tissue (90.1%). The vein label had the highest DSC, while arterial tissue led in IoU, precision, recall, and BF score. Additionally, a confusion matrix was used to show class-wise prediction performance, providing a detailed view of how well the network performed across different tissue types.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to quantitatively assess the segmentation performance of our model. These metrics include pixel accuracy, intersection-over-union (IoU), the Dice similarity coefficient (DSC), precision, recall, and the boundary F1 (BF) score. These metrics are widely recognized and used in the literature for evaluating segmentation tasks, ensuring that our evaluation is representative and comparable to other studies in the field.\n\nPixel accuracy measures the percentage of correctly classified pixels, providing a straightforward indication of overall segmentation correctness. IoU assesses the overlap between the predicted segmentation and the ground truth, offering insight into the spatial accuracy of the segmentation. The DSC, also known as the S\u00f8rensen\u2013Dice coefficient, evaluates the similarity between the predicted and actual labels, which is crucial for understanding the model's performance in capturing the true structure of the tissues.\n\nPrecision and recall are essential for understanding the model's ability to correctly identify positive instances. Precision indicates the fraction of true-positive instances among all retrieved positive instances, while recall measures the fraction of true-positive instances among all relevant instances. These metrics are particularly important for ensuring that the model does not miss critical structures.\n\nFinally, the BF score, which is the harmonic mean of precision and recall, provides a balanced measure of the model's performance, especially in scenarios where there is an imbalance between precision and recall. This metric is particularly useful for evaluating the model's performance at the boundaries of the segmented regions, which is often a challenging aspect of segmentation tasks.\n\nBy reporting these metrics, we aim to provide a thorough and comprehensive evaluation of our model's performance, ensuring that it meets the high standards required for clinical applications. These metrics collectively offer a robust assessment of the model's accuracy, reliability, and effectiveness in segmenting neuroanatomy images.",
  "evaluation/comparison": "Not enough information is available.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study utilized a dataset derived from The Neurosurgical Atlas, which provided a comprehensive collection of neuroanatomic structures and their corresponding labels. However, the specific images and annotations used for training, validation, and testing are not released publicly. The dataset consisted of approximately 879 uniquely labeled neuroanatomy images, which were divided into training, validation, and testing subsets. The evaluation metrics and results, such as pixel accuracy, intersection-over-union (IoU), Dice similarity coefficient (DSC), precision, recall, and boundary F1 (BF) score, were reported in the study. These metrics were calculated using scripts written in Python, but the actual scripts and raw data used for these calculations are not made available to the public. The study emphasizes the importance of a larger and more diverse dataset for improving network performance, but the current dataset remains proprietary and is not accessible for external evaluation or replication."
}