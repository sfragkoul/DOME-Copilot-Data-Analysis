{
  "publication/title": "Prediction of adolescent idiopathic scoliosis with machine learning algorithms using brain volumetric measurements.",
  "publication/authors": "Payas A, Kocaman H, Y\u0131ld\u0131r\u0131m H, Bat\u0131n S",
  "publication/journal": "JOR spine",
  "publication/year": "2024",
  "publication/pmid": "39011367",
  "publication/pmcid": "PMC11247394",
  "publication/doi": "10.1002/jsp2.1355",
  "publication/tags": "- Adolescent Idiopathic Scoliosis\n- Machine Learning\n- Brain Volume Data\n- Statistical Analysis\n- Cross-Sectional Study\n- Cerebral Hemisphere Measurements\n- Random Forest Algorithm\n- Logistic Regression\n- Support Vector Machines\n- Multilayer Perceptron",
  "dataset/provenance": "The dataset used in this study was sourced from MRI and Diffusion Tensor Imaging (DTI) scans of participants. The MRI imaging was performed using a 3 Tesla Siemens Magnetom Skyra device. The participants included 63 individuals, comprising 32 with Adolescent Idiopathic Scoliosis (AIS) and 31 healthy individuals. The study focused on female participants to minimize gender and hormonal differences, as AIS is more prevalent in females.\n\nThe MRI and DTI sequence settings included parameters such as TR = 4900 ms, TE = 95 ms, Number-of-Slice = 36, Flip Angle = 90\u00b0, FOV = 230 x 230 mm\u00b2, matrix = 128 x 128, and slice thickness = 3.0 mm, resulting in a voxel size of 1.8 x 1.8 x 3.5 mm. The images were converted to DICOM format and processed using MriStudio and MriCloud software. The segmentation process involved converting the MRI data to \".dpf\" format and then to \".zip\" format for uploading and processing on the Braingps.mricloud.org website. The processed files were downloaded and opened in the ROIEditor program for measurements and calculations across 168 brain regions.\n\nThe volume data from these 168 brain regions were used to train and test machine learning algorithms aimed at predicting the onset of AIS by comparing data from AIS patients and healthy individuals. The dataset was randomly shuffled and split into training and test sets, with the training data further divided using five times repeated 5-fold cross-validation. This approach ensured a comprehensive evaluation of the machine learning models' performance.",
  "dataset/splits": "The dataset was randomly shuffled and split into two main parts: training data and test data. The training data constituted two-thirds of the total dataset, while the test data made up the remaining one-third. For training the models, a five times repeated 5-fold cross-validation was employed. This means the training data was further divided into five folds, and the process was repeated five times to ensure robustness and reliability of the results. This approach helps in evaluating the performance of the machine learning algorithms more comprehensively by providing multiple training and validation iterations. The test data, which was not used during the training phase, was reserved for evaluating the final performance of the models.",
  "dataset/redundancy": "The dataset used in this study consisted of volume data from 169 different brain regions of individuals with Adolescent Idiopathic Scoliosis (AIS) and healthy individuals. The dataset was randomly shuffled and then split into two parts: two-thirds for training and one-third for testing. This split ensures that the training and test sets are independent, which is crucial for evaluating the generalizability of the machine learning models.\n\nTo enforce the independence of the training and test sets, a five-times repeated 5-fold cross-validation was used during the training phase. This method involves dividing the training data into five folds, training the model on four folds, and validating it on the remaining fold. This process is repeated five times, each time with a different fold as the validation set. The final model is then tested on the independent test set, which was not used during the training or validation phases.\n\nThe distribution of the data in this study is comparable to previously published machine learning datasets in terms of the volume measurements from brain regions. The data were preprocessed using standardization, which involves centering and scaling the data to eliminate the effect of unit variations. This preprocessing step is standard practice in machine learning to ensure that all features contribute equally to the model's performance.\n\nThe statistical significance of the measurements was assessed using appropriate tests, such as the independent samples t-test, Welch's test, or Mann\u2013Whitney U test, depending on whether the assumptions of normality and homogeneity of variance were met. This rigorous statistical analysis ensures that the findings are robust and reliable.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the class of supervised learning algorithms, specifically designed for classification tasks. The algorithms evaluated include logistic regression, naive bayes, k-nearest neighbors, support vector machines, random forest, linear discriminant analysis, multilayer perceptron, C5.0, bagging, extreme gradient boosting, and multivariate adaptive regression splines.\n\nNone of the algorithms used are new; they are well-established and widely recognized in the field of machine learning. The choice of these algorithms was driven by their proven effectiveness in various classification tasks and their ability to handle different types of data.\n\nThe decision to use these established algorithms rather than proposing a new one was influenced by the specific goals of the study. The primary aim was to estimate adolescent idiopathic scoliosis (AIS) using machine learning algorithms, comparing their performance against traditional statistical approaches. Given the complexity and importance of the task, it was crucial to rely on algorithms that have been extensively tested and validated in similar contexts.\n\nThe study focused on applying these algorithms to volume data obtained from different brain regions of individuals with AIS and healthy controls. The performance of these algorithms was evaluated using metrics such as accuracy, area under the receiver operating characteristic curve (AUC), F1-score, and Brier score. The results demonstrated that the random forest algorithm performed best in both training and test data, indicating its robustness and reliability for this particular application.\n\nThe algorithms were implemented using R software, along with the tidyverse and tidymodels packages, which provided the necessary tools for data preprocessing, model training, and performance evaluation. The use of these packages ensured that the implementation was efficient and reproducible, adhering to best practices in machine learning research.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for ensuring the effectiveness of the machine-learning algorithms. The dataset consisted of volume data obtained from 169 different brain regions of individuals with Adolescent Idiopathic Scoliosis (AIS) and healthy individuals. To prepare this data for analysis, several preprocessing steps were undertaken.\n\nFirst, the data was randomly shuffled to ensure that the distribution of samples was uniform. This step is essential to prevent any bias that might arise from the order in which the data was collected. Following the shuffling, the dataset was split into training and test sets. Specifically, two-thirds of the data were allocated for training the models, while the remaining one-third was reserved for testing their performance.\n\nStandardization was applied to the data to eliminate the effect of unit variations. This process involved centering and scaling the data, which is a common technique to ensure that all features contribute equally to the model's learning process. Standardization helps in improving the convergence speed of gradient descent algorithms and can lead to better model performance.\n\nFor the machine-learning algorithms, a grid search was employed to tune the parameters. This involved testing thirty possible combinations of parameters for each model to identify the optimal settings. The grid search method systematically works through multiple combinations of parameter tunes, cross-validating as it goes to determine which tune gives the best performance.\n\nThe performance of the models was evaluated using several criteria, including Accuracy, AUC (Area Under the Curve), F1-score, and Brier score, all with confidence intervals. These metrics provided a comprehensive assessment of the models' predictive capabilities. The ROC curve, which illustrates the trade-off between the true positive rate and the false positive rate across various threshold settings, was particularly useful in determining the best model.\n\nIn summary, the data encoding and preprocessing involved shuffling, splitting, standardization, and parameter tuning through grid search. These steps were essential in preparing the data for effective machine-learning analysis and ensuring robust model performance.",
  "optimization/parameters": "In our study, we evaluated eleven different machine learning algorithms, each with its own set of parameters. To ensure optimal performance, we employed a grid search method to tune these parameters. For each model, we tested thirty possible combinations of parameters. This systematic approach allowed us to identify the most effective parameter settings for each algorithm.\n\nThe specific number of parameters (p) varied depending on the algorithm. For instance, algorithms like Support Vector Machines (SVM) and Random Forest (RF) typically have more tunable parameters compared to simpler models like Logistic Regression. The grid search process involved evaluating different values for each parameter to find the combination that yielded the best performance metrics, such as accuracy, AUC, F1-score, and Brier score.\n\nThe selection of parameters was guided by the goal of maximizing model performance while avoiding overfitting. By using repeated 5-fold cross-validation on the training data, we ensured that the selected parameters were robust and generalizable. This method helped us to balance the trade-off between model complexity and performance, leading to reliable and effective models for predicting whether an individual has scoliosis or is healthy based on cerebral hemisphere measurements.",
  "optimization/features": "The study utilized volume data from 169 different brain regions as input features. Feature selection was inherently performed through the machine learning algorithms, particularly the Random Forest (RF) algorithm, which provided internal variable importance scores. These scores helped identify the most significant factors contributing to the prediction of Adolescent Idiopathic Scoliosis (AIS). The feature selection process was conducted using the training data only, ensuring that the test data remained independent and unbiased. The top features identified included measurements from regions such as the corticospinal tract, body of corpus callosum, splenium of corpus callosum, cerebellum, and pons. These selected features were crucial in distinguishing between healthy individuals and those with AIS.",
  "optimization/fitting": "In our study, we employed a comprehensive approach to ensure that our machine learning models were neither overfitting nor underfitting the data. We utilized a dataset that was randomly shuffled and split into two-thirds training data and one-third test data. This split helped in evaluating the model's performance on unseen data, thereby mitigating the risk of overfitting.\n\nTo further prevent overfitting, we used five times repeated 5-fold cross-validation during the training process. This technique ensures that each data point is used for both training and validation, providing a robust estimate of model performance. Additionally, we performed a grid search with thirty possible combinations of hyperparameters for each machine learning model. This exhaustive search helped in identifying the optimal set of parameters that generalized well to the test data.\n\nThe data preprocessing step involved standardization, which included centering and scaling the data. This step was crucial in eliminating the effect of unit variations and ensuring that all features contributed equally to the model's performance.\n\nTo rule out underfitting, we evaluated multiple machine learning algorithms, including logistic regression, naive bayes, k-nearest neighbors, support vector machines, random forest, linear discriminant analysis, multilayer perceptron, C5.0, bagging, extreme gradient boosting, and MARS. The performance of these algorithms was assessed using metrics such as accuracy, AUC, F1-score, and Brier score. The random forest algorithm emerged as the best-performing model, indicating that it effectively captured the underlying patterns in the data without being too simplistic.\n\nIn summary, our approach involved rigorous cross-validation, hyperparameter tuning, and the evaluation of multiple algorithms to ensure that our models were neither overfitting nor underfitting the data. The use of standardized data and comprehensive performance metrics further supported the robustness of our findings.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One of the key methods used was cross-validation. Specifically, we utilized five times repeated 5-fold cross-validation during the training phase. This approach helps to provide a more accurate estimate of model performance and reduces the risk of overfitting by ensuring that the model is evaluated on multiple subsets of the data.\n\nAdditionally, we performed parameter tuning using a grid search, testing thirty possible combinations for each machine learning model. This systematic approach to hyperparameter optimization helps to find the best parameters that generalize well to unseen data, further mitigating the risk of overfitting.\n\nAnother important technique we implemented was data preprocessing. We standardized the data by centering and scaling, which eliminates the effect of unit variations and ensures that all features contribute equally to the model training process. This preprocessing step is crucial for algorithms that are sensitive to the scale of the input features, such as support vector machines and neural networks.\n\nFurthermore, we evaluated multiple machine learning algorithms, including logistic regression, naive bayes, k-nearest neighbors, support vector machines, random forest, linear discriminant analysis, multilayer perceptron, C5.0, bagging, extreme gradient boosting, and MARS. By comparing the performance of these diverse algorithms, we were able to select the most robust model for our specific task, which in this case was the random forest algorithm. The random forest itself is an ensemble method that inherently reduces overfitting by averaging the results of multiple decision trees.\n\nIn summary, our study incorporated cross-validation, hyperparameter tuning, data standardization, and the evaluation of multiple machine learning algorithms to prevent overfitting and ensure the reliability of our results.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in the publication. Specifically, we utilized a grid search to tune the parameters corresponding to each machine learning model, testing thirty possible combinations. The data preprocessing steps, including standardization (centering and scaling), are also detailed to ensure reproducibility.\n\nThe performance metrics, such as Accuracy, AUC, F1-score, and Brier score, along with their confidence intervals, are provided for both the training and test datasets. These metrics are crucial for understanding the model's performance and generalizability.\n\nThe model files and specific configurations are not directly available in the main text but can be inferred from the described methods and results. For instance, the Random Forest (RF) algorithm demonstrated the best performance, with high Accuracy, AUC, and F1-score values, and a low Brier score. The variable importance plot generated by the RF algorithm highlights the most significant factors influencing the classification of individuals with scoliosis or healthy controls.\n\nWhile the exact model files are not provided, the detailed methodology and results offer a comprehensive guide for replicating the study. The use of R software, along with tidyverse and tidymodels packages, ensures that the analysis can be reproduced by other researchers. The statistical tests and machine learning algorithms employed are standard and well-documented, facilitating reproducibility.\n\nFor access to the specific model files and detailed configurations, readers are encouraged to refer to the supplementary materials or contact the authors directly. The study adheres to ethical guidelines and ensures that the data and methods are transparent and reproducible.",
  "model/interpretability": "The model employed in this study is not a black box. The Random Forest (RF) algorithm, which demonstrated the best performance, provides insights into the importance of various features. This transparency is evident through the variable importance plot generated using the RF algorithm's internal variable importance scores. This plot highlights the twenty most important factors influencing whether an individual has scoliosis or is healthy. The top five factors identified include the corticospinal tract, body of corpus callosum, splenium of corpus callosum, cerebellum, and pons measurements. These variables are not only statistically significant but also align with known neural pathways involved in intra-hemispheric and inter-hemispheric connectivity, particularly in areas associated with vision and motor control. This alignment underscores the model's ability to identify biologically relevant features, making it more interpretable and trustworthy. Additionally, the model's performance metrics, such as high accuracy, AUC, and F1-score, along with a low Brier score, further support its reliability and interpretability.",
  "model/output": "The model is a classification model. It is designed to predict whether an individual has scoliosis or is healthy. The performance of the model is evaluated using metrics such as accuracy, area under the receiver operating curve (AUC), Brier score, and F1-score. These metrics are commonly used to assess the performance of classification models. The model's output is a binary classification, indicating the presence or absence of scoliosis. The Random Forest algorithm, which is one of the models used, provides variable importance scores that help identify the most significant factors contributing to the classification. The results are presented in the form of a variable importance plot and a receiver operating characteristic (ROC) curve, which are typical for classification tasks. The model's performance is compared across different algorithms, and the best-performing model is highlighted based on these classification metrics.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the machine learning algorithms used in this study is not publicly released. The analysis was conducted using R software, along with the tidyverse and tidymodels packages. These packages are open-source and freely available for use. However, the specific scripts and models developed for this research are not provided as a standalone executable, web server, virtual machine, or container instance. The statistical tests were performed using IBM SPSS 25 and MedCalc 21, which are commercial software tools. The results and performance metrics of the algorithms are presented in the publication, but the underlying code and models are not publicly accessible.",
  "evaluation/method": "In the evaluation of our machine learning algorithms, we employed a comprehensive two-stage approach. Initially, we conducted performance comparisons among eleven different machine learning algorithms. These algorithms included logistic regression, naive bayes, k nearest neighbors, support vector machines, random forest, linear discriminant analysis, multilayer perceptron, C5.0, bagging, extreme gradient boosting, and MARS.\n\nThe dataset was randomly shuffled and split into two parts: two-thirds for training and the remaining one-third for testing. To ensure robust training, we utilized five times repeated 5-fold cross-validation on the training data. This method involved dividing the training data into five folds, training the model on four folds, and validating it on the remaining fold. This process was repeated five times, with each fold serving as the validation set once. Additionally, we used an independent test set to evaluate the final performance of the models.\n\nData preprocessing was crucial in our evaluation method. We applied standardization to the data, which involved centering and scaling, to eliminate the effect of unit variations. This step ensured that all features contributed equally to the model's performance.\n\nFor hyperparameter tuning, we employed a grid search, testing thirty possible combinations for each machine learning model. This systematic approach helped in identifying the optimal parameters that maximized the model's performance.\n\nThe performance of the models was evaluated using several criteria: Accuracy, AUC (area under the ROC curve), F1 score, and Brier score, all with confidence intervals. These metrics provided a comprehensive assessment of the models' predictive capabilities. The ROC curve was particularly useful in evaluating the trade-off between the true positive rate and the false positive rate across various threshold settings.\n\nThe results of our analysis were obtained using R software, along with the tidyverse and tidymodels packages. These tools facilitated the implementation and evaluation of the machine learning algorithms, ensuring that our findings were reliable and reproducible.",
  "evaluation/measure": "In our study, we evaluated the performance of various machine learning algorithms using a comprehensive set of metrics to ensure a thorough assessment. The primary metrics reported include Accuracy, AUC (Area Under the Curve), Brier Score, and F1-score, all accompanied by their respective 95% confidence intervals. These metrics were chosen for their ability to provide a well-rounded evaluation of model performance.\n\nAccuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. It is a straightforward metric that gives an overall sense of how well the model is performing.\n\nThe AUC, or Area Under the Receiver Operating Characteristic Curve, is a critical metric for evaluating the performance of classification models. It provides a single scalar value that represents the quality of the model's predictions irrespective of the chosen classification threshold. A higher AUC indicates better model performance.\n\nThe Brier Score is a measure of the accuracy of probabilistic predictions. It quantifies the mean squared difference between predicted probabilities and the actual binary outcomes. Lower Brier Scores indicate better predictive performance.\n\nThe F1-score is the harmonic mean of precision and recall, providing a balance between the two. It is particularly useful when dealing with imbalanced datasets, as it considers both false positives and false negatives. A higher F1-score indicates better performance in terms of precision and recall.\n\nThese metrics are widely used in the literature and are considered representative of model performance across various domains. By reporting these metrics, we aim to provide a clear and comprehensive evaluation of our models, allowing for easy comparison with other studies in the field.",
  "evaluation/comparison": "In the evaluation phase of our study, we conducted comprehensive performance comparisons of various machine learning algorithms. We assessed eleven different machine learning algorithms, including logistic regression, naive bayes, k-nearest neighbors, support vector machines, random forest, linear discriminant analysis, multilayer perceptron, C5.0, bagging, extreme gradient boosting, and MARS. These algorithms were evaluated based on their performance metrics, such as accuracy, AUC (area under the ROC curve), F1 score, and Brier score, with confidence intervals.\n\nThe data set was randomly shuffled and split into two parts: two-thirds for training and the remaining one-third for testing. The training process involved five times repeated 5-fold cross-validation to ensure robustness. The data were preprocessed using standardization to eliminate the effect of unit variations. A grid search was employed to tune the parameters of each machine learning model, testing thirty possible combinations to optimize performance.\n\nThe performance of these algorithms was measured using the aforementioned metrics. The results indicated that the random forest algorithm consistently performed the best across both the training and test datasets. It achieved high accuracy, AUC, and F1-score values while maintaining a low Brier score, which is indicative of a well-performing model. The ROC curve obtained using the test data further supported the reasonable and good performance of the random forest algorithm.\n\nIn addition to the machine learning comparisons, statistical tests were conducted to compare cerebral hemisphere measurements between healthy and patient groups. These tests included independent samples t-test, Welch's test, and Mann\u2013Whitney U test, depending on the assumptions of normality and homogeneity of variance. The statistical significance value was set at 0.05.\n\nThe variable importance plot generated using the random forest algorithm's internal variable importance scores highlighted the most significant factors influencing the classification of individuals as having scoliosis or being healthy. The top five important factors included measurements of the corticospinal tract, body of corpus callosum, splenium of corpus callosum, cerebellum, and pons.\n\nOverall, the evaluation phase involved rigorous comparisons of machine learning algorithms and statistical tests to ensure the reliability and validity of the findings. The random forest algorithm emerged as the best-performing model, providing high accuracy and robust performance metrics.",
  "evaluation/confidence": "The evaluation of the machine learning algorithms in this study includes a comprehensive assessment of performance metrics with associated confidence intervals. This approach ensures that the results are statistically robust and provides a clear indication of the reliability of the findings.\n\nPerformance metrics such as accuracy, AUC (Area Under the Curve), Brier score, and F1-score are presented with 95% confidence intervals. These intervals help to understand the variability and precision of the estimates, making it possible to assess the statistical significance of the results. For instance, the Random Forest algorithm, which demonstrated superior performance, has confidence intervals that indicate a high level of accuracy and AUC, reinforcing the claim that it is a strong model.\n\nStatistical significance is a crucial aspect of evaluating the superiority of the methods used. The study employs various statistical tests, including the independent samples t-test, Welch's test, and the Mann-Whitney U test, depending on the assumptions of normality and homogeneity of variance. These tests ensure that the differences observed between the models and baselines are not due to random chance. The significance value is set at 0.05, which is a standard threshold in many scientific disciplines.\n\nThe use of repeated 5-fold cross-validation further strengthens the evaluation process. This technique helps to mitigate overfitting and provides a more generalizable assessment of model performance. The results from the cross-validation are consistent with those from the independent test set, indicating that the models are robust and not overly tuned to the training data.\n\nIn summary, the performance metrics with confidence intervals, along with the statistical tests and cross-validation techniques used, provide a solid foundation for claiming that the methods evaluated are superior to others and baselines. The results are statistically significant and reliable, supporting the conclusions drawn from the study.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study focused on comparing machine learning algorithms to predict the onset of Adolescent Idiopathic Scoliosis (AIS) using brain volume data. The performance of these algorithms was evaluated using metrics such as Accuracy, AUC, F1-score, and Brier score. The results, including performance metrics and variable importance plots, are presented within the publication. However, the specific datasets and raw evaluation files used in the analysis are not released publicly. Therefore, access to these files is not available for external verification or further analysis."
}