{
  "publication/title": "Machine learning optimized polygenic scores for blood cell traits identify sex-specific trajectories and genetic correlations with disease.",
  "publication/authors": "Xu Y, Vuckovic D, Ritchie SC, Akbari P, Jiang T, Grealey J, Butterworth AS, Ouwehand WH, Roberts DJ, Di Angelantonio E, Danesh J, Soranzo N, Inouye M",
  "publication/journal": "Cell genomics",
  "publication/year": "2022",
  "publication/pmid": "35072137",
  "publication/pmcid": "PMC8758502",
  "publication/doi": "10.1016/j.xgen.2021.100086",
  "publication/tags": "- Machine Learning\n- Polygenic Scores\n- Blood Cell Traits\n- Genetic Correlation\n- Disease Prediction\n- Sex-Specific Trajectories\n- Genetic Architecture\n- UK Biobank\n- INTERVAL Study\n- Data Availability",
  "dataset/provenance": "The dataset utilized in this study primarily consists of summary statistics from various genome-wide association studies (GWAS). The UK Biobank (UKB) summary statistics for blood cell traits were sourced from Vuckovic et al., 2020, and are available under the accession numbers GCST90002379-GCST90002407 in the GWAS Catalog.\n\nAdditionally, the study incorporated GWAS summary statistics for several diseases, including schizophrenia from Ruderfer et al., 2018, Crohn\u2019s disease from Liu et al., 2015, rheumatoid arthritis from Okada et al., 2014, allergic disease from Ferreira et al., 2017, and asthma from Demenais et al., 2018. These datasets are also accessible through the GWAS Catalog under specific accession numbers.\n\nThe study also utilized the CAD meta-GRS from Inouye et al., 2018, available under the accession number PGS000018 in the PGS Catalog.\n\nThe number of data points varies depending on the specific dataset used, but the UK Biobank summary statistics, for instance, encompass a large cohort of individuals, providing robust data for analysis.\n\nThese datasets have been previously used in the scientific community for various genetic studies, ensuring their reliability and relevance for the current research. The integration of these well-established datasets allows for comprehensive analysis and validation of the findings presented in this manuscript.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not applicable",
  "dataset/availability": "The data generated from this study, specifically the polygenic score (PGS) models of blood cell traits, have been made publicly available. These models can be accessed through the PGS Catalog under the accession numbers PGS000088 to PGS000113. This ensures that the data is freely accessible to the research community for further analysis and validation.\n\nThe original code used in the study has also been deposited on GitHub, providing transparency and reproducibility. The repository can be found at https://github.com/xuyu-cam/PGS-BC-Traits-Using-ML-DL. This allows other researchers to review the methods and potentially replicate the findings.\n\nRegarding the enforcement of data availability, the deposition of data in the PGS Catalog and code on GitHub serves as a standard practice in the scientific community to ensure that the data and methods are accessible and can be verified by others. This approach promotes transparency and reproducibility, which are crucial for the integrity of scientific research.",
  "optimization/algorithm": "The machine-learning algorithms used in our study include elastic net (EN), Bayesian ridge (BR), multilayer perceptron (MLP), and convolutional neural network (CNN). These are well-established classes of machine-learning algorithms, with EN and BR being linear models and MLP and CNN being types of neural networks.\n\nThe algorithms used are not new; they have been extensively studied and applied in various fields. The choice of these algorithms was driven by their proven effectiveness in handling complex data and their ability to capture intricate patterns, which is crucial for genomic prediction tasks.\n\nThe focus of our work is on the application of these machine-learning methods to develop polygenic scores (PGS) for blood cell traits, rather than on the development of new machine-learning algorithms. Therefore, it is appropriate for our findings to be published in a genomics journal, as our contributions lie in the innovative application of these methods to genetic data, rather than in the creation of new machine-learning techniques.",
  "optimization/meta": "The meta-predictor approach was not explicitly mentioned in the provided information. However, the study evaluated the relative performance of six different methods to develop polygenic scores (PGS) for blood cell traits. These methods included a standard method of pruning and thresholding (P+T) and five learning methods: LDpred2, elastic net (EN), Bayesian ridge (BR), multilayer perceptron (MLP), and convolutional neural network (CNN).\n\nThe study did not indicate that any of these methods used data from other machine-learning algorithms as input. Each method was evaluated independently for its ability to predict blood cell traits. The performance of these methods was measured using Pearson r, and the models were trained and tested using a five-fold cross-validation approach on the UK Biobank (UKB) samples, with external validation using the INTERVAL cohort.\n\nThe training and internal testing covered the whole UKB cohort, ensuring that the evaluation was unbiased. The P+T method was also tested on the five different UKB testing sets and the whole INTERVAL cohort. Hyperparameter tuning was conducted for each method to optimize performance. For example, a genetic algorithm was used to search for the optimal MLP and CNN architectures, and grid search was used for hyperparameter tuning in the BR method.\n\nThe study did not provide details on whether the training data for each method was completely independent. However, the use of five-fold cross-validation suggests that the data was partitioned in a way to ensure that each model was trained and tested on different subsets of the data, which helps to avoid overfitting and ensures that the results are generalizable.",
  "optimization/encoding": "Not enough information is available.",
  "optimization/parameters": "In our study, the number of parameters (p) used in the model varied depending on the specific method employed for developing polygenic scores (PGS) for blood cell traits. We evaluated six different methods, including a standard method of pruning and thresholding (P+T) and five learning methods: LDpred2, elastic net (EN), Bayesian ridge (BR), multilayer perceptron (MLP), and convolutional neural network (CNN).\n\nFor the elastic net (EN) method, implemented using SNPNET, the parameter \u03b1 was set to 0.5. The parameter \u03bb was tuned for each trait and each variant set using 10% of the training samples as a validation set.\n\nIn the Bayesian ridge (BR) method, a grid search was conducted to identify two appropriate gamma distributions, involving the selection of parameters \u03b11, \u03b12, l1, and l2. The grid search was performed across a specified set on the training set, with 10% of the samples used as a validation set. This method was implemented using the scikit-learn package.\n\nFor LDpred2, the default options of hyper-parameters for h2, p, and sparsity were applied. A grid search, which is commonly used and top-performing, was employed to learn PGS of these blood cell traits. Summary statistics from GWAS were used in the variants selection step, and 10,000 randomly selected samples from the training data were used to obtain the variants-variants correlation matrix.\n\nFor the multilayer perceptron (MLP) and convolutional neural network (CNN) methods, a genetic algorithm was used to search for the optimal architectures and other hyperparameters. This included the number of layers, the number of neurons at each layer, activation functions, optimizers, and dropouts. The genetic algorithm was applied on the training set, with 10% of the samples used as a validation set. These models were implemented using Keras.\n\nIn summary, the selection of parameters (p) was method-specific and involved validation sets to ensure optimal performance. The exact number of parameters varied based on the complexity of the model and the specific hyperparameter tuning strategies employed.",
  "optimization/features": "In our study, the input features for the machine learning models were single nucleotide polymorphisms (SNPs). The exact number of SNPs used as input features varied depending on the method and the specific trait being analyzed. For methods like LDpred2, the number of SNPs was determined through a variants selection step using summary statistics from genome-wide association studies (GWAS). This process inherently involves feature selection, as it focuses on variants that show some level of association with the trait of interest.\n\nFor the elastic net (EN) and Bayesian ridge (BR) methods, feature selection was performed using the training set only. Specifically, 10% of the training samples were used as a validation set to tune the regularization parameter (\u03bb) for EN and to identify appropriate gamma distributions for BR. This approach ensures that the feature selection process does not introduce bias from the test set.\n\nFor the multilayer perceptron (MLP) and convolutional neural network (CNN) methods, a genetic algorithm was used to search for the optimal network architectures and other hyperparameters. This process also involved feature selection, as the algorithm aimed to identify the most relevant SNPs and network configurations that best predict the blood cell traits. Again, this was done using only the training set, with 10% of the samples reserved for validation to tune the hyperparameters.\n\nIn summary, feature selection was performed for all methods, and it was done using the training set only to avoid evaluation bias. The number of input features (SNPs) varied by method and trait, but the selection process ensured that only the most relevant variants were used to build the polygenic scores.",
  "optimization/fitting": "In our study, we employed several strategies to address both overfitting and underfitting, ensuring robust model performance.\n\nTo mitigate overfitting, we utilized pooling layers in our convolutional neural networks (CNNs). These layers replace each representation at a certain location with a summary statistic of the nearby output units, helping to generalize the representations and reduce the chance of overfitting. Additionally, we implemented dropout layers in our multi-layer perceptrons (MLPs) and CNNs, which randomly set a fraction of input units to zero during training, further preventing overfitting.\n\nWe also employed cross-validation techniques to evaluate model performance. For each trait and learning method, we randomly and equally partitioned the UK Biobank (UKB) samples into five portions. Any four portions (80% of the samples) were used as training data, while the remaining 20% served as internal test data. This process was repeated five times, ensuring that the training and internal testing covered the entire UKB cohort. This approach helped to avoid evaluation bias and provided a more reliable estimate of model performance.\n\nTo further validate our models, we conducted external validation using the whole INTERVAL cohort. This step ensured that our models generalized well to independent datasets, reducing the risk of overfitting to the training data.\n\nTo address underfitting, we used a genetic algorithm to search for the optimal MLP and CNN architectures, as well as other hyperparameters. This approach allowed us to explore a wide range of network configurations, including the number of layers, neurons per layer, activation functions, optimizers, and dropout rates. By tuning these hyperparameters, we aimed to find the best architecture for each task, ensuring that our models were complex enough to capture the underlying patterns in the data.\n\nMoreover, we employed grid search and validation sets to tune hyperparameters for different methods, such as Elastic Net (EN) and Bayesian Ridge (BR). For EN, we set the regularization parameter alpha to 0.5 and used 10% of the training samples as a validation set to tune the l1 ratio for each trait and variant set. For BR, we conducted a grid search across a specified set of values to identify appropriate gamma distributions, using 10% of the training samples as a validation set.\n\nIn summary, we implemented various techniques to balance model complexity and prevent overfitting and underfitting. These strategies included the use of pooling and dropout layers, cross-validation, external validation, and hyperparameter tuning through genetic algorithms and grid search.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the generalization of our models. One key method involved the use of pooling layers in our convolutional neural networks (CNNs). These pooling layers help to reduce the dimensionality of the data and summarize the nearby output units, which aids in generalizing the representations and mitigating overfitting.\n\nAdditionally, we utilized dropout layers in our multilayer perceptrons (MLPs) and CNNs. Dropout is a regularization technique where a random subset of neurons is temporarily removed during training, which helps to prevent the model from becoming too reliant on specific neurons and thus reduces overfitting.\n\nWe also implemented cross-validation to evaluate the performance of our models. Specifically, we partitioned the UK Biobank (UKB) samples into five portions, using four portions for training and one portion for testing. This process was repeated five times, ensuring that each portion was used as the test set once. This approach allowed us to cover the entire UKB cohort for training and internal testing, effectively avoiding evaluation bias.\n\nFurthermore, we conducted hyperparameter tuning using validation sets. For example, in the elastic net (EN) method, we set a parameter to 0.5 and used 10% of the training samples as a validation set to tune another parameter for each trait and variant set. Similarly, for the Bayesian ridge (BR) method, we performed a grid search on the training set, using 10% of the samples as a validation set to identify appropriate gamma distributions.\n\nIn summary, our study incorporated pooling layers, dropout layers, cross-validation, and hyperparameter tuning with validation sets to prevent overfitting and enhance the robustness of our models.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are indeed available. All the generated polygenic score (PGS) models for blood cell traits have been deposited in the PGS Catalog and are publicly accessible under the accession numbers PGS000088 to PGS000113. This ensures that the models can be reviewed and utilized by other researchers.\n\nAdditionally, all the original code used in this study has been deposited on GitHub. The repository, accessible at https://github.com/xuyu-cam/PGS-BC-Traits-Using-ML-DL, contains the scripts and configurations necessary to replicate the experiments and understand the optimization processes. This includes details on the hyper-parameter tuning, the genetic algorithm used for searching optimal architectures, and the implementation of machine learning and deep learning models using Keras.\n\nThe data and code are provided under open access, allowing for transparency and reproducibility in research. This approach facilitates further investigation and application of the methods described in our study.",
  "model/interpretability": "The model employed in this study is not a blackbox. It incorporates a three-layer multi-layer perceptron (MLP), which is a type of neural network that can be interpreted to some extent. The output of this MLP is defined as y = f3(f1(SNP1, SNP2, SNP3), f2(SNP1, SNP2, SNP3)), where f1, f2, and f3 are functions applied to the input single nucleotide polymorphisms (SNPs). This structure allows for a clear understanding of how the input features are transformed through each layer to produce the final output.\n\nAdditionally, the study uses various visualization techniques to enhance interpretability. For instance, the effect sizes of variants are compared between different models and analyses, providing insights into how different variants contribute to the traits being studied. Variants are marked with different symbols and colors based on their minor allele frequency (MAF), interactions, and correlations, making it easier to interpret their significance and relationships.\n\nThe use of mean effect sizes from multiple trained models further adds to the transparency, as it provides a more robust estimate of the variant effects. This approach helps in understanding the consistency and reliability of the model's predictions across different training instances. Overall, the model and the accompanying analyses are designed to be interpretable, allowing researchers to gain insights into the genetic factors influencing the traits of interest.",
  "model/output": "The output of our model is designed to predict a continuous value, indicating that our model is a regression model rather than a classification model. Specifically, the output is derived from a three-layer multilayer perceptron (MLP), where the final layer produces a single value. This value is the result of a series of transformations applied to the input features, which in our case are single nucleotide polymorphisms (SNPs). The transformations involve weighted linear sums followed by activation functions, allowing the model to capture complex, non-linear relationships in the data. The goal is to minimize the difference between the predicted values and the actual values in the training data, using a back-propagation algorithm to adjust the model's weights accordingly. This approach enables the model to learn the underlying patterns in the data and make accurate predictions on new, unseen data.",
  "model/duration": "The execution time for our models varied depending on the specific architecture and the computational resources used. For the multi-layer perceptrons (MLPs) and convolutional neural networks (CNNs), we employed a genetic algorithm to optimize the network architectures and hyperparameters. This process was computationally intensive and required significant time, as it involved training and evaluating multiple model configurations.\n\nThe training of each model was performed using 80% of the UK Biobank (UKB) samples, with the remaining 20% used for internal testing. Additionally, we validated the models using the entire INTERVAL cohort. The hyperparameter tuning, particularly for methods like SNPNET and LDpred2, involved grid searches and validation on subsets of the training data. For instance, SNPNET used 10% of the training samples for validation to tune the regularization parameter, while LDpred2 utilized a grid search across a range of hyperparameters.\n\nThe use of convolutional and pooling layers in CNNs added complexity to the training process, requiring more time compared to simpler models. We also implemented dropout layers to mitigate overfitting, which further influenced the training duration. The overall execution time for training and validating these models spanned several days to weeks, depending on the specific configurations and the computational power available.\n\nIn summary, the execution time for our models was substantial, reflecting the complexity of the tasks and the thoroughness of our hyperparameter tuning and validation processes.",
  "model/availability": "The source code for the algorithms and models used in this study has been made publicly available. All original code can be accessed on GitHub at the following repository: https://github.com/xuyu-cam/PGS-BC-Traits-Using-ML-DL. This repository contains the necessary scripts and implementations to reproduce the results presented in the manuscript.\n\nRegarding the execution of the algorithms, specific details on running the code are provided within the repository. This includes instructions on setting up the environment, dependencies, and any necessary configurations. The code is released under a permissive license, allowing for both academic and commercial use, subject to the terms specified in the repository.\n\nAdditionally, several software tools and libraries were utilized in the development and analysis of the models. These include R, Python, scikit-learn, Keras, SNPNET, LDpred2, PLINK, and Bcftools. Each of these tools has its own documentation and licensing information, which can be found on their respective websites. For instance, R and Python are open-source and freely available, while scikit-learn and Keras are also open-source and can be installed via package managers like pip or conda. SNPNET and LDpred2 are available on GitHub, and PLINK and Bcftools have their own dedicated websites with download and usage instructions.",
  "evaluation/method": "The evaluation of the methods involved a comprehensive approach to ensure robustness and generalizability. For each trait and learning method, the UK Biobank (UKB) samples were randomly and equally partitioned into five portions. Four of these portions (80% of the samples) were used as training data to learn a model, while the remaining 20% of the UKB samples served as the internal test set. Additionally, an external validation was conducted using the entire INTERVAL cohort. This process was repeated five times, resulting in five different models for each learning method and trait, each with performance measurements for both the internal UKB test and the external INTERVAL test. This strategy ensured that the training and internal testing covered the entire UKB cohort, effectively avoiding evaluation bias.\n\nThe performance of various polygenic scoring methods was measured using Pearson r. The P+T method was also tested on the five different UKB testing sets and the entire INTERVAL cohort. This rigorous evaluation framework provided a thorough assessment of the models' performance and generalizability.",
  "evaluation/measure": "In our evaluation, we primarily used the Pearson correlation coefficient (Pearson r) to measure the performance of various polygenic scoring methods. This metric is widely recognized and used in the literature for assessing the predictive performance of genomic models. It provides a straightforward and interpretable measure of the linear relationship between predicted and observed trait values.\n\nWe applied this performance metric across different traits and learning methods. For each trait and method, we partitioned the UK Biobank (UKB) samples into five portions, using four portions (80% of the samples) for training and the remaining 20% for internal testing. Additionally, we performed external validation using the entire INTERVAL cohort. This approach ensured that our training and internal testing covered the whole UKB cohort, helping to avoid evaluation bias.\n\nBy obtaining five different models for each learning method and trait, we could assess the consistency and robustness of our performance measurements. This comprehensive evaluation strategy allowed us to compare the performance of different methods, including P+T, Elastic Net (EN), and LDpred2, across various variant sets and traits. The use of Pearson r as our primary performance metric aligns with established practices in the field, ensuring that our results are comparable to other studies in genomic prediction.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of various polygenic scoring (PGS) methods to evaluate their performance in predicting blood cell traits. We assessed six different methods: pruning and thresholding (P+T), LDpred2, elastic net (EN), Bayesian ridge (BR), multilayer perceptron (MLP), and convolutional neural network (CNN).\n\nTo ensure a robust evaluation, we used the UK Biobank (UKB) samples and the INTERVAL cohort. For each trait and learning method, we randomly partitioned the UKB samples into five portions. Four of these portions (80% of the samples) were used as training data to learn a model, while the remaining 20% were used for internal testing. Additionally, we performed external validation using the entire INTERVAL cohort. This approach allowed us to cover the whole UKB cohort for training and internal testing, effectively avoiding evaluation bias.\n\nThe P+T method, which directly applies a weighted sum on a given variant set with effect sizes from genome-wide association studies (GWAS), was also tested on the five different UKB testing sets and the entire INTERVAL cohort. This method relies on linkage disequilibrium (LD) pruning to remove correlations among variants, but it must balance removing correlated variants with keeping predictive variants using hard cutoff thresholds.\n\nFor the learning methods, we employed different techniques to tune hyperparameters, which are crucial for model performance. For example, we used a genetic algorithm to search for the optimal MLP and CNN architectures, as well as other hyperparameters such as the number of layers, neurons, activation functions, optimizers, and dropouts. We also conducted grid searches for methods like Bayesian ridge to identify appropriate gamma distributions.\n\nIn summary, our evaluation included a comparison to publicly available methods like LDpred2 and simpler baselines like P+T. We used benchmark datasets and rigorous validation techniques to ensure the reliability and generalizability of our findings.",
  "evaluation/confidence": "In our study, we employed Pearson r to measure the performance of various polygenic scoring methods. To ensure the robustness of our results, we used a rigorous cross-validation approach. For each trait and each learning method, we randomly and equally partitioned the UK Biobank (UKB) samples into five portions. We then used any four portions (80% of the samples) as training data to learn a model and tested the model's performance on the remaining 20% of UKB samples. Additionally, we performed external validation using the entire INTERVAL cohort. This process was repeated five times, ensuring that the training and internal testing covered the whole UKB cohort, thereby avoiding evaluation bias.\n\nThe P+T method was also tested on the five different UKB testing sets and the entire INTERVAL cohort. This comprehensive evaluation strategy allowed us to obtain multiple performance measurements for each learning method and trait, providing a more reliable assessment of model performance.\n\nStatistical significance was a key consideration in our analysis. We reported p-values at different thresholds (e.g., < 1e\u22126, < 1e\u22124, < 1e\u22122, < 1) to indicate the significance of the results. These thresholds helped us to identify which variant sets and methods showed statistically significant performance improvements.\n\nHyperparameter tuning was another critical aspect of our evaluation. We used grid search and genetic algorithms to optimize the hyperparameters for different methods, including EN, BR, and LDpred2. For example, in the EN method implemented using SNPNET, we set \u03b1 to 0.5 and used 10% of the training samples as a validation set to tune \u03bb for each trait and variant set. Similarly, for BR, we conducted a grid search across a specified set of values to identify appropriate gamma distributions. This meticulous tuning process ensured that our models were optimized for performance.\n\nIn summary, our evaluation approach included robust cross-validation, statistical significance testing, and thorough hyperparameter tuning. These measures collectively enhanced the confidence in our performance metrics and the claims of superiority for the methods evaluated.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being available. However, the generated PGS models of blood cell traits have been deposited at the PGS Catalog and are publicly available under accession numbers PGS000088 - PGS000113. Additionally, all original code has been deposited at GitHub. The specific license under which these resources are released is not specified."
}