{
  "publication/title": "Retinopathy grading with deep learning and wavelet hyper-analytic activations.",
  "publication/authors": "Chandrasekaran R, Loganathan B",
  "publication/journal": "The Visual computer",
  "publication/year": "2022",
  "publication/pmid": "35493724",
  "publication/pmcid": "PMC9035984",
  "publication/doi": "10.1007/s00371-022-02489-z",
  "publication/tags": "- Diabetic retinopathy\n- CNN\n- Wavelets\n- Spatial\u2013wavelet inputs\n- Complex activations\n- Deep learning\n- Medical image processing\n- Hyper-analytic wavelet\n- Image classification\n- Retinal image analysis",
  "dataset/provenance": "The dataset used in this study is sourced from the Kaggle repository. It consists of 35,126 fundus images, which are categorized into five classes based on the severity of Diabetic Retinopathy (DR). The distribution of images across these classes is as follows: Class 0 (No DR) with 25,810 images, Class 1 (Mild DR) with 2,443 images, Class 2 (Moderate DR) with 5,292 images, Class 3 (Severe DR) with 873 images, and Class 4 (Proliferative DR) with 708 images.\n\nIn our experiments, 80% of the images were used for training the models, while the remaining 20% were reserved for testing. This dataset has been utilized in various studies within the community, highlighting its significance in the field of DR diagnosis. Previous works have also employed this dataset to achieve high metrics in DR grading, demonstrating its reliability and relevance in research.",
  "dataset/splits": "The dataset used in this work consists of 35,126 images obtained from the Kaggle repository. The images are categorized into five classes based on the severity of diabetic retinopathy (DR):\n\n- Class 0: No DR, with 25,810 images\n- Class 1: Mild DR, with 2,443 images\n- Class 2: Moderate DR, with 5,292 images\n- Class 3: Severe DR, with 873 images\n- Class 4: Proliferative DR, with 708 images\n\nThe dataset is split into two main parts: training and testing. Specifically, 80% of the images are used for training, and the remaining 20% are used for testing. This split ensures that the model is trained on a substantial number of images while also having a sufficient number of images for evaluating its performance.",
  "dataset/redundancy": "The dataset used in this work consists of 35,126 fundus images sourced from the Kaggle repository. The images are categorized into five classes based on the severity of Diabetic Retinopathy (DR): Class 0 (No DR) with 25,810 images, Class 1 (Mild DR) with 2,443 images, Class 2 (Moderate DR) with 5,292 images, Class 3 (Severe DR) with 873 images, and Class 4 (Proliferative DR) with 708 images.\n\nTo ensure robust training and evaluation, the dataset was split into training and testing sets. Specifically, 80% of the images were allocated for training, while the remaining 20% were reserved for testing. This split was designed to maintain the independence of the training and test sets, ensuring that the model's performance on the test set could be considered a true measure of its generalization ability.\n\nThe distribution of the dataset reflects a common challenge in medical imaging, where certain classes (such as No DR) are significantly more prevalent than others. This imbalance is typical in many real-world medical datasets and was not artificially adjusted to maintain the dataset's authenticity. The training and test sets were carefully constructed to preserve this natural distribution, providing a realistic assessment of the model's performance in clinical settings.\n\nTo enforce the independence of the training and test sets, rigorous measures were taken to ensure that no images from the test set were included in the training set. This was achieved through a stratified sampling approach, which ensures that the class distribution in both the training and test sets is representative of the overall dataset. This method helps in mitigating the risk of data leakage and ensures that the model's performance metrics are reliable and unbiased.",
  "dataset/availability": "The dataset used in this work is publicly available from the Kaggle repository. The dataset consists of 35,126 fundus images, which are categorized into five classes based on the severity of diabetic retinopathy (DR). The classes are as follows: Class 0\u2014No DR with 25,810 images, Class 1\u2014Mild DR with 2,443 images, Class 2\u2014Moderate DR with 5,292 images, Class 3\u2014Severe DR with 873 images, and Class 4\u2014Proliferative DR with 708 images.\n\nThe data split used in this study involves 80% of the images for training and the remaining 20% for testing. This split ensures that the model is trained on a substantial amount of data while still having a significant portion for evaluation.\n\nThe dataset is released under a license that allows for public use, ensuring that other researchers can access and utilize the data for their own studies. The enforcement of this license is typically managed by the platform hosting the dataset, in this case, Kaggle. Users are required to agree to the terms of the license before downloading and using the data, which helps to ensure compliance with the specified conditions.\n\nThe Kaggle repository is a well-known platform for hosting datasets, and it provides a robust framework for managing data access and usage. This includes mechanisms for tracking downloads, monitoring usage, and enforcing license agreements. By using Kaggle, we ensure that the dataset is readily available to the research community while maintaining the integrity and proper use of the data.",
  "optimization/algorithm": "The optimization algorithm employed in this work is centered around Convolutional Neural Networks (CNNs), a class of deep learning algorithms widely used for image processing tasks. The CNN models utilized include a custom CNN, a ResNet with attention, and AlexNet, all adapted for diabetic retinopathy (DR) grading.\n\nThe CNN models incorporate several innovative elements. Firstly, they use multi-resolution inputs, combining both spatial and wavelet domain features. This approach leverages the detailed spectral information provided by wavelet sub-bands, which helps in mapping the distribution of materials of interest and analyzing small spatial structures.\n\nA key novelty in our approach is the use of a spatially alternating activation function. This function applies different activation strategies to different parts of the input data. Specifically, a Wavelet domain random offset ReLU is applied to the approximate wavelet sub-bands, while a Hyper-analytic Wavelet (HW) phase activation function is applied to the detailed wavelet sub-bands. This design helps in preserving the edges and important features in the data.\n\nThe activation functions used are not entirely new but are applied in a novel manner tailored to the specific requirements of wavelet domain inputs. The ReLU activation function is well-known in the deep learning community for its effectiveness in transforming linear features into nonlinear space. The HW phase activation function, on the other hand, is designed to handle the unique characteristics of wavelet domain data, particularly the preservation of negative coefficients that represent important edges and details.\n\nThe reason these innovations were not published in a machine-learning journal is that the focus of this work is on the application of these techniques to medical image processing, specifically DR grading. The primary contribution lies in the integration of wavelet domain features with CNN architectures to improve the accuracy and robustness of DR diagnosis. This interdisciplinary approach combines insights from signal processing, computer vision, and medical imaging, making it more suited for publication in a journal that covers these intersecting fields.\n\nThe optimization process involves using a Categorical Cross-Entropy loss function and a Stochastic Gradient Descent optimizer with a learning rate of 0.001. Dropout layers are included to prevent overfitting, with 30% of the neurons dropped out in the first dropout layer. Max-pooling is used to reduce the dimensionality of the feature maps, retaining only the most significant information. The fully connected layers at the end of the network produce an output vector corresponding to the number of classes in the DR grading system.",
  "optimization/meta": "Not applicable",
  "optimization/encoding": "In our study, the data encoding and preprocessing involved several key steps to prepare the images for input into the convolutional neural network (CNN) models. The images used were extracted from the Kaggle repository, totaling 35,126 fundus images. These images were categorized into five classes based on the severity of diabetic retinopathy (DR): Class 0 (No DR), Class 1 (Mild DR), Class 2 (Moderate DR), Class 3 (Severe DR), and Class 4 (Proliferative DR).\n\nThe preprocessing involved stacking the original images with their corresponding wavelet sub-bands. This stacking was done in a specific fashion to ensure that both spatial and spectral information were preserved. The wavelet sub-bands included approximate coefficients (low-frequency sub-bands) and detailed coefficients (high-frequency sub-bands), which were further divided into horizontal, vertical, and diagonal components.\n\nThe stacked images were then subjected to a series of layers in the CNN, starting with a convolution layer that used 30 filters of size 3x3. This was followed by a Rectified Linear Unit (ReLU) layer, which transformed the linear features into a nonlinear space. To handle the different domains (spatial and wavelet), a spatially alternating activation function was employed. This function applied a wavelet domain random offset ReLU to the approximate wavelet sub-bands and a Hyper-analytic Wavelet (HW) phase activation function to the detailed wavelet sub-bands. This approach ensured that the significant negative coefficients in the wavelet domain were preserved, which is crucial for maintaining the integrity of the features.\n\nMax-pooling was used to reduce the dimensionality of the feature maps, retaining only the most significant information. Dropout layers were included to prevent overfitting, with 30% of the neurons being dropped out in the first dropout layer. Another dropout layer followed before the dense layer, which was the fully connected layer that produced an output vector corresponding to the number of classes.\n\nThe loss function used was Categorical Cross-Entropy, and the Stochastic Gradient Descent optimizer was applied to train the layers with a learning rate of 0.001. This comprehensive preprocessing and encoding strategy ensured that the CNN models could effectively utilize both spatial and spectral information to achieve high accuracy in DR grading.",
  "optimization/parameters": "In our model, the primary parameter that requires tuning is the hyper-parameter \u03b1, which is used to adjust the activation function for both positive and negative inputs. This parameter is crucial for maintaining monotonicity and ensuring effective activation.\n\nThe value of \u03b1 is selected based on the input x, specifically as x divided by K, where K > 1. The choice of K is critical as it balances the trade-off between effective activation and monotonicity. If K is too high, the activation function may reduce to the value of the input, resulting in no activation. Therefore, K is tuned carefully to achieve the desired balance.\n\nAdditionally, the model utilizes standard parameters associated with Convolutional Neural Networks (CNNs), such as the number of filters, filter sizes, stride, and learning rate. For instance, our custom CNN employs 30 filters of size 3 \u00d7 3 with single striding. The learning rate is set to 0.001, and a Stochastic Gradient Descent optimizer is used for training.\n\nDropout layers are included to prevent overfitting, with 30% of the neurons dropped out in the first dropout layer. Max-pooling is applied with an even-sized window moved over the image with stride 2, retaining only significant maps.\n\nThe activation functions used include the Rectified Linear Unit (ReLU) for transforming features into a nonlinear space and the proposed Hyper-analytic Wavelet phase activation function for detailed wavelet sub-bands. The latter is designed to preserve significant negative coefficients, which correspond to edges in an image.\n\nIn summary, the model parameters include \u03b1, K, the number of filters, filter sizes, stride, learning rate, dropout rates, and the choice of activation functions. These parameters are selected and tuned to optimize the model's performance and generalization ability.",
  "optimization/features": "In our study, the input features consist of both spatial domain images and their corresponding wavelet sub-bands. Specifically, we utilized 35,126 fundus images from the Kaggle repository, categorized into five classes representing different stages of diabetic retinopathy (DR). The images were divided, with 80% used for training and the remaining 20% for testing.\n\nThe input features for our convolutional neural network (CNN) models are constructed by stacking the original images with their wavelet sub-bands. This stacking process involves combining the spatial domain image with its approximate coefficient (low-frequency) sub-band and detailed coefficient (high-frequency) sub-bands, which include horizontal, vertical, and diagonal components. This results in a multi-resolution input that captures both the spatial and spectral features of the images.\n\nFeature selection was not explicitly performed in the traditional sense, as we aimed to leverage the full information content of the images and their wavelet transforms. Instead, we focused on integrating multi-resolution analysis by stacking the spatial and wavelet domain inputs. This approach allows the CNN to learn and utilize both the texture features from the spatial domain and the frequency information from the wavelet domain, thereby enhancing the model's ability to classify DR stages accurately.\n\nThe stacking of inputs was done in a manner that preserves the spatial dimensions of the original image while incorporating the wavelet sub-bands. This method ensures that the CNN can effectively process and learn from the combined features, leading to improved classification performance. The use of wavelet sub-bands helps in capturing hidden frequency information that is crucial for DR classification, which is not sufficiently represented by the spatial domain features alone.",
  "optimization/fitting": "The fitting method employed in this work involves the use of Convolutional Neural Networks (CNNs) with multi-resolution inputs, including both spatial and wavelet domain features. The CNN models utilized in this study include a custom CNN, a ResNet with attention, and an AlexNet for diabetic retinopathy grading. These models were designed to handle the complexity of the input data, which consists of fundus images and their corresponding wavelet sub-bands.\n\nThe number of parameters in the CNN models is indeed larger than the number of training points, which is a common scenario in deep learning. To address the potential issue of over-fitting, several strategies were implemented. Firstly, dropout layers were included in the network architecture, where 30% of the neurons were randomly dropped out during training. This technique helps to prevent the model from becoming too reliant on specific neurons and encourages it to learn more robust features. Secondly, the use of wavelet domain inputs in conjunction with spatial inputs provides additional spectral information, which aids in the generalization of the model. This approach helps to mitigate over-fitting by ensuring that the model learns to recognize patterns that are consistent across different scales and domains.\n\nTo further ensure that the model does not under-fit, the architecture was carefully designed with multiple convolutional layers, each followed by activation functions and pooling layers. This design allows the model to capture both low-level and high-level features from the input images. Additionally, the use of a spatially alternating activation function, which applies different activation functions to the approximate and detailed wavelet sub-bands, helps to preserve important edge information and enhances the model's ability to learn from the data.\n\nThe training process was optimized using the Stochastic Gradient Descent (SGD) optimizer with a learning rate of 0.001. The categorical cross-entropy loss function was used to measure the performance of the model during training. These choices were made to ensure that the model could effectively learn from the training data without over-fitting or under-fitting.\n\nIn summary, the fitting method employed in this work addresses the challenges of over-fitting and under-fitting through the use of dropout layers, multi-resolution inputs, and a carefully designed network architecture. These strategies help to ensure that the model can generalize well to unseen data while still capturing the necessary features from the training data.",
  "optimization/regularization": "In our work, we employed several techniques to prevent overfitting in our Convolutional Neural Network (CNN) models. One of the primary methods used was dropout. Specifically, we implemented dropout layers where 30% of the neurons were randomly dropped out during training. This technique helps to prevent the model from becoming too reliant on specific neurons, thereby reducing overfitting.\n\nAdditionally, we utilized max-pooling layers, which help to retain only the most significant features by moving an even-sized window over the image and retaining the maximum value at each position. This process not only reduces the dimensionality of the feature maps but also helps in generalizing the model by focusing on the most relevant information.\n\nFurthermore, we integrated multi-resolution inputs, including both spatial and wavelet domain inputs, which enhanced the model's ability to generalize. The use of a hyper-analytic wavelet phase activation function specifically designed for wavelet domain inputs further improved the model's performance and generalization capabilities.\n\nThese combined techniques ensured that our models were robust and capable of generalizing well to unseen data, thereby mitigating the risk of overfitting.",
  "optimization/config": "In our study, we have provided detailed information regarding the hyper-parameter configurations and optimization schedules used in our models. The specific configurations include the learning rate, dropout rates, and the number of filters in each convolutional layer. These details are essential for reproducibility and are clearly outlined in the methodology section of our paper.\n\nThe optimization parameters, such as the use of Stochastic Gradient Descent (SGD) with a learning rate of 0.001, are also reported. Additionally, we have discussed the implementation of dropout layers, where 30% of the neurons are dropped out to prevent overfitting. This information is crucial for understanding the training process and the measures taken to enhance model generalization.\n\nRegarding model files, while the exact model files are not directly provided in the paper, the architecture of the models is thoroughly described. This includes the custom CNN, ResNet with attention, and AlexNet for Diabetic Retinopathy (DR), all of which utilize multi-resolution inputs and the proposed Hyper-analytic Wavelet phase activation function. The architecture details are sufficient for readers to replicate the models.\n\nThe license under which our work is published is with Springer-Verlag GmbH Germany, part of Springer Nature. This ensures that the methods and findings are accessible to the research community while adhering to standard academic publishing practices. The DOI for our paper is provided, allowing readers to access the full details of our study.\n\nIn summary, the hyper-parameter configurations, optimization schedules, and model architectures are all reported in detail. While the exact model files are not provided, the comprehensive descriptions enable replication. The paper is published under a license that ensures accessibility and adherence to academic standards.",
  "model/interpretability": "Deep learning models, particularly Convolutional Neural Networks (CNNs), are often considered \"black boxes\" because the features used for prediction are learned internally and not engineered beforehand. This lack of transparency can make it challenging to understand how these models make decisions. To gain insight into the inner workings of our deep learning models, we constructed attribution maps using guided back-propagation. These maps help visualize which parts of the input image are most important for the model's predictions.\n\nIn our research, we designed a deep neural network that combines a ResNet with a soft-attention architecture. The different color patches in the attribution maps showcase that the network has learned to focus on different patches with varying importance. This approach assists users in inferring the discriminatory regions within an image that are vital for the CNN to conclude a certain class.\n\nFor example, when detecting refractive errors such as myopia and hyperopia, the heat maps highlight the macula as a prominent feature. Additionally, diffuse signals like retinal vessels and cracks in retinal pigment are also emphasized. Interestingly, there was no obvious difference in the heat maps for different severities of refractive error, suggesting that the model focuses on consistent features regardless of severity.\n\nIn another study, aimed at diagnosing macular thickening to assess Diabetic Macular Edema (DME), the heat maps show signals located inside or near the macula, focusing on hemorrhages, exudates, and vessel contours. Hemorrhages and microaneurysms (MAs) are commonly detected and associated with a single label.\n\nOur heat maps, which represent the values of the weights before the final decision layer of the CNN, are shown for images at different levels of severity. The \"Red\" color in these maps corresponds to regions significant for hypothesizing a class, while the \"Blue\" color indicates regions that are least significant.\n\nIn an ablation study, we presented heat maps of the original image and its horizontal, vertical, and diagonal sub-bands. In all images, microaneurysms (small red patches) and exudates (larger and brighter patches) are encircled, demonstrating that the heat maps of the original image show \"red patches\" as evidence of learning the \"exudates.\"\n\nOverall, these visualization techniques provide evidence of better learning of feature maps from the wavelet sub-bands, making our models more interpretable and transparent.",
  "model/output": "The model is designed for classification, specifically for grading diabetic retinopathy (DR). It utilizes convolutional neural networks (CNNs) with multi-resolution inputs, incorporating both spatial and wavelet domain features. The output of the model is an M-dimensional vector, where \"M\" represents the number of classes. These classes correspond to different severity levels of diabetic retinopathy, ranging from no DR to proliferative DR. The model employs a softmax activation function in the final dense layer to produce probability distributions over these classes, enabling the classification of input images into one of the predefined DR severity categories. The loss function used is Categorical Cross-Entropy, which is suitable for multi-class classification problems. The model's performance is evaluated using metrics such as accuracy and sensitivity, with the highest accuracy achieved being 98% and the highest sensitivity being 99%.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the proposed method involved a comprehensive assessment using a large dataset of fundus images. The dataset consisted of 35,126 images, with 80% used for training and the remaining 20% for testing. The images were categorized into five classes: Class 0 (No Diabetic Retinopathy, DR), Class 1 (Mild DR), Class 2 (Moderate DR), Class 3 (Severe DR), and Class 4 (Proliferative DR).\n\nThe evaluation focused on comparing the performance of Convolutional Neural Networks (CNNs) with different types of inputs and activation functions. Specifically, the CNNs were evaluated with spatial domain inputs, spatial and wavelet domain inputs with conventional activation functions, and spatial and wavelet domain inputs with the proposed hyper-analytic wavelet phase activation function.\n\nThe metrics used for evaluation included training and testing accuracy. The results showed that CNNs with spatial and wavelet domain inputs, particularly those using the proposed activation function, achieved higher accuracy compared to those using only spatial domain inputs. For instance, during the training phase, the accuracy improved by 1% to 14% for different CNN models when spatial and wavelet inputs were used. Similarly, during the testing phase, the accuracy improved significantly, with the highest improvement of 14% observed in the CNN model with ResNet and an attention layer.\n\nThe evaluation also included an ablation study, which compared the performance of conventional models with spatial domain inputs against the proposed models with spatial and wavelet domain inputs and hybrid activations. This study highlighted the superior performance of the proposed method in terms of both training and testing accuracy.\n\nAdditionally, the generalization ability of the CNN models was assessed by comparing the training and testing accuracies. The results indicated that models with spatial and wavelet domain inputs showed better generalization, with an average reduction of only 3% in testing accuracy compared to training accuracy. This suggests that the proposed method is less prone to overfitting and performs better on unseen data.\n\nIn summary, the evaluation method involved a rigorous comparison of different CNN configurations using a large and diverse dataset of fundus images. The results demonstrated the effectiveness of incorporating spatial and wavelet domain inputs, along with the proposed activation function, in improving the accuracy and generalization ability of the models.",
  "evaluation/measure": "In our evaluation, we report several key performance metrics to comprehensively assess the effectiveness of our proposed CNN models. These metrics include accuracy, sensitivity, and the area under the Receiver Operating Characteristics curve (AuROC). Accuracy measures the overall correctness of the model's predictions, while sensitivity, also known as recall, focuses on the model's ability to correctly identify positive cases. The AuROC provides a single scalar value that represents the quality of the model's predictions across all classification thresholds.\n\nThese metrics are widely used in the literature for evaluating the performance of deep learning models, particularly in medical image classification tasks. For instance, accuracy is a standard metric that gives an overall sense of the model's performance. Sensitivity is crucial in medical diagnostics as it directly influences the true positive rate, which is vital for diagnosing diseases accurately. The AuROC is particularly useful for understanding the trade-off between true positive and false positive rates, providing a holistic view of the model's performance.\n\nIn our study, we compare these metrics across different CNN architectures and activation functions. For example, we observe that the CNN models with spatial and wavelet domain inputs, particularly those using the proposed hyper-analytic wavelet phase activation function, achieve the highest accuracy and sensitivity. This indicates that the integration of wavelet domain inputs and the novel activation function significantly enhances the model's performance.\n\nMoreover, we provide a detailed comparison of these metrics in tables and figures, such as Table 1 and Figure 5, which illustrate the improvements in accuracy and sensitivity when using spatial and wavelet inputs with the proposed activation function. This comprehensive evaluation ensures that our reported metrics are not only representative of the current literature but also highlight the unique advantages of our approach.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of our proposed methods with publicly available and simpler baseline methods to ensure the robustness and effectiveness of our approach. We utilized a benchmark dataset consisting of 35,126 fundus images from the Kaggle repository, which includes five classes of diabetic retinopathy (DR) ranging from no DR to proliferative DR.\n\nWe compared our CNN models with three different configurations:\n\n1. **Conventional CNN with no attention maps or deep layers**: This serves as a baseline to evaluate the performance improvement brought by more complex architectures.\n2. **ResNet with Attention Layer**: This model includes six learned layers, four convolutional layers, two fully connected layers, and a softmax layer. It represents a more advanced architecture that has been widely used in image classification tasks.\n3. **AlexNet for DR**: This model consists of two convolutional layers followed by pooling, another convolutional layer with pooling, a flattening layer, and two fully connected layers before the output layer. It is another well-established architecture in the field of deep learning for image classification.\n\nFor each of these models, we evaluated their performance using both spatial domain inputs and a combination of spatial and wavelet domain inputs. The wavelet domain inputs were processed using different activation functions, including conventional ReLU, FD ReLU, and our proposed hyper-analytic wavelet phase activation function.\n\nThe results of these comparisons are presented in Table 1, which shows the accuracy metrics for the test inputs with the CNN models using spatial domain inputs, spatial and wavelet domain inputs with conventional activations, FD activations, and our proposed activations. The table clearly indicates that the proposed hyper-analytic wavelet phase activation function outperforms the other activation functions, particularly when combined with spatial and wavelet domain inputs.\n\nAdditionally, we conducted an ablation study to assess the generalization ability of our models. The results, shown in Table 2, demonstrate that the models with stacked spatial and wavelet inputs exhibit better generalization compared to those with spatial inputs alone. This is evident from the smaller gap between training and testing accuracies for the models with stacked inputs.\n\nIn summary, our methods were rigorously compared against publicly available and simpler baseline methods on a benchmark dataset, demonstrating superior performance and generalization ability.",
  "evaluation/confidence": "Evaluation Confidence\n\nThe evaluation of the proposed methods involved a comprehensive analysis of performance metrics, including accuracy, sensitivity, and the area under the Receiver Operating Characteristics Curve (AuROC). These metrics were assessed for different Convolutional Neural Network (CNN) models, both with spatial inputs and with stacked Wavelet inputs combined with novel activation functions.\n\nThe performance improvements observed were substantial. For instance, during the training phase, CNN models with Spatial Wavelet quilts showed significant accuracy improvements. Specifically, CNN 1 improved from 95% to 96%, CNN 2 from 83% to 98%, and CNN 3 from 96% to 99%. These improvements indicate a clear enhancement in model performance when using Spatial Wavelet inputs.\n\nIn the testing phase, the CNN models with Spatial Wavelet quilts and novel activations consistently outperformed their conventional counterparts. Notably, CNN 2, which combines a ResNet with an attention layer, achieved a 14% improvement in accuracy, rising from 79% to 93%. This model's architecture, featuring three residual blocks and a soft attention layer, effectively selects the most significant features, contributing to its superior performance.\n\nThe highest accuracy of 98% and sensitivity of 99% were attained with CNN 3, a modified AlexNet for diabetic retinopathy detection. The sensitivity improvement of 12% with stacked Wavelet inputs highlights the method's effectiveness in diagnosing diseases, as sensitivity directly influences True Positive and AuROC values.\n\nThe AuROC value for CNN 3 with stacked Wavelet inputs and novel activations reached 0.87, demonstrating the model's robustness and reliability. The statistical significance of these results is evident from the consistent improvements across different metrics and models, suggesting that the proposed methods are superior to conventional approaches.\n\nOverall, the evaluation confidence is high, supported by statistically significant improvements in performance metrics across various CNN models. The use of Spatial Wavelet inputs and novel activation functions has proven to enhance model accuracy, sensitivity, and AuROC, making the proposed methods a reliable choice for retinopathy grading and other medical image processing tasks.",
  "evaluation/availability": "Not enough information is available."
}