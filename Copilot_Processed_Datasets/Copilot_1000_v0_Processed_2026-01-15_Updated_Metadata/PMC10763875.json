{
  "publication/title": "A novel interpretative tool for early prediction of low cardiac output syndrome after valve surgery: online machine learning models.",
  "publication/authors": "Hong L, Feng T, Qiu R, Lin S, Xue Y, Huang K, Chen C, Wang J, Xie R, Song S, Zhang C, Zou J",
  "publication/journal": "Annals of medicine",
  "publication/year": "2023",
  "publication/pmid": "38128272",
  "publication/pmcid": "PMC10763875",
  "publication/doi": "10.1080/07853890.2023.2293244",
  "publication/tags": "- Machine Learning\n- Low Cardiac Output Syndrome\n- Valve Surgery\n- Predictive Modeling\n- Clinical Outcomes\n- Postoperative Care\n- Support Vector Machines\n- Model Interpretation\n- Data Validation\n- Medical Research",
  "dataset/provenance": "The dataset used in this study was sourced from a cohort of patients undergoing valve surgery. A total of 2218 patients were included in the analysis. The dataset included various preoperative, intraoperative, and postoperative variables, such as hemodynamic measurements, laboratory results, and patient demographics. Some patients were excluded due to being under 18 years old, dying within 6 hours post-surgery, or lacking relevant information. The median age of the enrolled patients was 63 years, with 56.4% being male. The dataset was divided into training and testing cohorts in an 8:2 ratio to facilitate model development and internal validation. The training cohort was used for feature selection, model training, and parameter tuning, while the test cohort was used for internal validation to assess the model's generalization ability. The dataset included variables such as mean arterial pressure, systolic blood pressure, diastolic blood pressure, and lactate levels, which were collected and processed to evaluate the risk of low cardiac output syndrome (LCOS) post-surgery.",
  "dataset/splits": "The dataset was divided into two main splits: a training cohort and a testing cohort. The ratio used for this split was 8:2, meaning 80% of the data was allocated to the training cohort, and the remaining 20% was used for the testing cohort. This division ensured that both cohorts had a similar proportion of patients with low cardiac output syndrome (lcOs). The training cohort was utilized for feature selection, model training, and parameter tuning. In contrast, the test cohort served solely as an internal validation cohort to evaluate the model's generalization ability internally.",
  "dataset/redundancy": "The dataset was split into training and testing cohorts in an 8:2 ratio. This means that 80% of the data was used for training the models, while the remaining 20% was reserved for testing. The split was done randomly, but it was ensured that both cohorts had a similar proportion of patients with low cardiac output syndrome (lcOs). This approach helps to maintain the independence of the training and test sets, which is crucial for evaluating the generalization ability of the models.\n\nThe training cohort was used for feature selection, model training, and parameter tuning. In contrast, the test cohort was used solely as an internal validation set to assess the model's performance on unseen data. This separation ensures that the models are not overfitted to the training data and can generalize well to new, unseen cases.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the medical field. By maintaining a similar proportion of lcOs patients in both cohorts, the study aims to create a balanced and representative dataset. This balance is essential for training robust models that can accurately predict lcOs in patients undergoing valve surgery. The use of techniques like k-nearest neighbor imputation for missing values, Z-score normalization for continuous variables, and one-hot encoding for categorical variables further ensures that the dataset is well-prepared for model training.",
  "dataset/availability": "The original contributions presented in this study are included in the supplementary materials. Further inquiries regarding the data can be directed to the corresponding authors. The data availability statement does not specify whether the data is released in a public forum or under a specific license. However, it is clear that the data is not publicly available and that access to it is controlled through the corresponding authors. This approach ensures that the data is shared in a controlled manner, allowing for verification of the study's findings while maintaining privacy and security.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and widely recognized in the field. Specifically, we employed five different algorithms: logistic regression, support vector machine, random forest classifier, extreme gradient boosting, and deep neural network. These algorithms are part of the broader class of supervised learning techniques, which are commonly used for classification tasks in machine learning.\n\nThe algorithms utilized are not new; they have been extensively studied and applied in various domains. The choice of these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to provide robust predictive performance. The support vector machine algorithm, in particular, demonstrated the highest area under the curve and the best calibration capability among the five algorithms tested.\n\nThe decision to use these established algorithms was based on their reliability and the need for a solid foundation for our predictive models. While these algorithms are not novel, their application in the context of predicting low cardiac output syndrome after valve surgery is significant. This study focuses on the practical implementation and interpretation of these models in a clinical setting, rather than the development of new machine-learning algorithms. The emphasis is on leveraging existing techniques to address a specific medical challenge, ensuring that the models are interpretable and clinically useful.",
  "optimization/meta": "The models developed in this study do not function as meta-predictors. Instead, they are standalone machine learning models designed to predict low cardiac output syndrome (LCOS) in patients undergoing valve surgery. The study employed five different machine learning algorithms: logistic regression (LR), support vector machine (SVM), random forest classifier (RFC), extreme gradient boosting (XGB), and deep neural network (DNN). Each of these algorithms was used to construct both preoperative and postoperative models.\n\nThe training and testing data were carefully divided to ensure independence. The dataset was randomly split into training and testing cohorts in an 8:2 ratio, with a similar proportion of patients with LCOS in each cohort. This division ensures that the training cohort was used for feature selection, model training, and parameter tuning, while the test cohort served solely as an internal validation cohort to evaluate the generalization ability of the models.\n\nThe models were evaluated based on their predictive performance, using metrics such as the area under the receiver operating characteristic curve (AUROC), sensitivity, specificity, accuracy, and Brier scores. The SVM algorithm demonstrated the highest discriminative performance among the five algorithms for both preoperative and postoperative models. The AUROC and Brier scores for the postoperative model using SVM were particularly notable, indicating good predictive and calibration capabilities.\n\nIn summary, the models developed in this study are not meta-predictors but rather individual machine learning models trained and evaluated on independent datasets. The use of multiple algorithms allowed for a comprehensive comparison, with SVM emerging as the most effective for predicting LCOS.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the quality and compatibility of the dataset for machine learning algorithms. Variables with more than 10% missing data were removed to maintain data integrity. Missing values were then imputed using the k-nearest neighbor algorithm, which effectively estimates and fills in the missing data points based on similar data entries.\n\nContinuous variables were normalized using Z-score normalization. This technique standardizes the data by subtracting the mean and dividing by the standard deviation, ensuring that all continuous variables contribute equally to the model's performance. Categorical variables were converted using one-hot encoding, a method that transforms categorical data into a binary matrix, making it suitable for machine learning algorithms.\n\nThe dataset was randomly split into training and testing cohorts in an 8:2 ratio, ensuring a similar proportion of patients with low cardiac output syndrome (lcOs) in each cohort. The training cohort was used for feature selection, model training, and parameter tuning, while the test cohort served as an internal validation set to evaluate the model's generalization ability.\n\nFeature selection was performed using the least absolute shrinkage and selection operator (lasso) regression. This method helps in identifying and retaining the most relevant features by shrinking less important coefficients to zero, thereby reducing overfitting and improving the predictive performance of the machine learning models. The feature selection process was implemented using the scikit-learn Python package.\n\nIn summary, our data encoding and preprocessing steps involved removing variables with excessive missing data, imputing missing values, normalizing continuous variables, and encoding categorical variables. These steps were essential in preparing a robust dataset for building and evaluating machine learning models to predict lcOs in patients undergoing valve surgery.",
  "optimization/parameters": "In our study, the number of input parameters (p) used in the models varied between the preoperative and postoperative predictions. For the preoperative model, six variables were selected: age, ejection fraction, left ventricular diastolic diameter, direct bilirubin, left ventricular posterior wall thickness, and BMI. For the postoperative model, a more extensive set of variables was incorporated, including lactate, ejection fraction, age, left ventricular diastolic diameter, direct bilirubin, time for MAP <65, BMI, urea, left ventricular posterior wall thickness, time for MAP <60, total protein, ACTIME, TWA-CVP (thresholds = 12), time for CVP >16, TWA-MAP (thresholds = 55), and time for CVP >20.\n\nThe selection of these parameters was performed using the LASSO (Least Absolute Shrinkage and Selection Operator) regression technique. This method is particularly useful for feature selection in high-dimensional data by shrinking the coefficients of less important features to zero. By adjusting the hyperparameter lambda (\u03bb), the LASSO algorithm effectively excludes extraneous features, retaining only those with nonzero coefficients, which are deemed significant for predicting the outcome. This process ensures that the models are built with the most relevant variables, reducing the risk of overfitting and enhancing predictive performance.",
  "optimization/features": "The input features for the models were selected using the least absolute shrinkage and selection operator (LASSO) regression. This process was performed separately for the preoperative and postoperative models.\n\nFor the preoperative model, six variables were identified as significant: age, ejection fraction, left ventricular diastolic diameter, direct bilirubin, left ventricular posterior wall thickness, and body mass index.\n\nFor the postoperative model, a more extensive set of variables was included, encompassing preoperative, intraoperative, and postoperative baseline variables. The final set of features for the postoperative model consisted of lactate, ejection fraction, age, left ventricular diastolic diameter, direct bilirubin, time for mean arterial pressure below 65, body mass index, urea, left ventricular posterior wall thickness, time for mean arterial pressure below 60, total protein, activated clotting time, time-weighted area under the curve for central venous pressure above 12, time for central venous pressure above 16, time-weighted area under the curve for mean arterial pressure below 55, and time for central venous pressure above 20.\n\nFeature selection was conducted using the training cohort only, ensuring that the test cohort remained independent for internal validation. This approach helps to evaluate the generalization ability of the models and prevents data leakage, which could otherwise lead to overfitting and biased performance estimates.",
  "optimization/fitting": "In our study, we employed several strategies to address potential overfitting and underfitting issues. The number of parameters in our models was indeed larger than the number of training points, which is a common scenario in machine learning, especially with complex models like deep neural networks.\n\nTo mitigate overfitting, we utilized a combination of techniques. First, we performed feature selection using the least absolute shrinkage and selection operator (LASSO) regression. This method helps in reducing the dimensionality of the data by shrinking the coefficients of less important features to zero, thereby excluding extraneous features and focusing on the most relevant ones.\n\nAdditionally, we implemented 10-fold cross-validation during the hyperparameter optimization process. This technique ensures that the model generalizes well to unseen data by training and validating the model on different subsets of the data.\n\nWe also employed a cost-sensitive learning approach to handle the potential impact of unbalanced data on model performance. This method helps in reducing patient misclassification by assigning different costs to different types of errors.\n\nTo further enhance the model's generalization ability, we used a training-to-testing cohort ratio of 8:2, ensuring that the test cohort was used only for internal validation. This approach helps in evaluating the model's performance on data it has not seen during training.\n\nRegarding underfitting, we used multiple algorithms, including logistic regression, support vector machine, random forest classifier, extreme gradient boosting, and deep neural network. By comparing the performance of these different algorithms, we could ensure that the model was not too simplistic to capture the underlying patterns in the data.\n\nMoreover, we evaluated the models using various metrics such as the area under the receiver operating characteristic curve (AUC), sensitivity, specificity, accuracy, and Brier scores. This comprehensive evaluation helped us in selecting the optimal model that balances both predictive and calibration capabilities.\n\nIn summary, our approach to fitting the models involved careful feature selection, cross-validation, cost-sensitive learning, and the use of multiple algorithms. These strategies collectively helped us in addressing both overfitting and underfitting issues, ensuring robust and reliable model performance.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One key method used was feature selection through LASSO (Least Absolute Shrinkage and Selection Operator) regression. This technique helps in identifying and retaining only the most relevant features by shrinking the coefficients of less important variables to zero, thereby reducing the complexity of the model and mitigating overfitting.\n\nAdditionally, we utilized cross-validation, specifically 10-fold cross-validation, during the hyperparameter optimization process. This approach involves dividing the dataset into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. Cross-validation helps in assessing the model's performance more reliably and ensures that it generalizes well to unseen data.\n\nFurthermore, we implemented a cost-sensitive learning approach to handle the potential impact of unbalanced data on model performance. This method adjusts the learning algorithm to account for the misclassification costs, thereby improving the model's ability to correctly classify minority classes and reducing the risk of overfitting to the majority class.\n\nBy combining these techniques, we aimed to build models that are not only accurate but also robust and generalizable, capable of predicting low cardiac output syndrome effectively in clinical settings.",
  "optimization/config": "The hyperparameter configurations and optimization schedule used in our study are available. We employed a grid search algorithm combined with 10-fold cross-validation to optimize the hyperparameters for each model, aiming to achieve the highest area under the curve (AUC) of the receiver operating characteristic (ROC). This process ensured that the models were finely tuned for optimal performance.\n\nThe model files, including the trained preoperative and postoperative models, are saved using the pickle package in Python. These files are essential for replicating the results and deploying the models in practical applications. The backend server-side of our web application is developed using Flask, a popular framework for serving Python web applications. The entire web application is deployed on an Nginx server, ensuring robust and scalable performance.\n\nRegarding the availability and licensing of these resources, the specific details about the model files and the optimization parameters are not explicitly mentioned in the provided information. However, it is implied that the methods and tools used are standard and widely accessible in the machine learning community. For precise information on licensing and access, one would need to refer to the supplementary materials or contact the authors directly.",
  "model/interpretability": "Traditional machine learning models are often criticized for their lack of interpretability, functioning as \"black boxes\" where the training and validation processes obscure the underlying decision-making mechanisms. To address this issue, we introduced the Shapley Additive ExPlanations (SHAP) method to enhance the interpretability and visualization of our models. SHAP values are instrumental in quantifying the association between a variable and its marginal contribution to an individual's final risk prediction. By reporting the average absolute SHAP value for all patients, we can determine the SHAP value of a variable, providing a clear and interpretable measure of feature importance.\n\nFor instance, in our preoperative model, the three most important features, as identified by SHAP values, are age, ejection fraction (EF), and left ventricular diastolic diameter (LVDd). This means that these variables have the most significant impact on predicting the risk of low cardiac output syndrome (LCOS). Similarly, in the postoperative model, the top features include EF, lactate, age, LVDd, direct bilirubin, and the time for mean arterial pressure (MAP) below 65. These features are ranked in order of their impact on the prediction of LCOS.\n\nVisualizations, such as feature importance ranking graphs and beeswarm plots, further illustrate the distribution of SHAP values for various features. In these plots, redder colors indicate higher values of the sample variable, and higher SHAP values correspond to a greater risk of LCOS. For example, in the preoperative model, older age, lower EF, and larger LVDd are associated with a higher risk of LCOS. In the postoperative model, patients with lower EF, higher lactate levels, older age, larger LVDd, higher direct bilirubin, and longer durations of MAP below 65 are at a higher risk.\n\nBy leveraging SHAP, we have made our models more transparent and interpretable, allowing clinicians to understand the key factors contributing to the risk of LCOS. This transparency is crucial for building trust in the model's predictions and for facilitating informed decision-making in clinical settings.",
  "model/output": "The model developed in our study is a classification model. It is designed to predict the occurrence of low cardiac output syndrome (LCOS) after valve surgery. Specifically, it categorizes patients into two groups: those who are at risk of developing LCOS and those who are not. The model uses various machine learning algorithms, including logistic regression, random forest classifier, support vector machine, extreme gradient boosting, and deep neural networks, to achieve this classification. The performance of these algorithms was evaluated using metrics such as the area under the receiver operating characteristic curve (AUC-ROC), sensitivity, specificity, accuracy, and Brier scores. The support vector machine (SVM) algorithm demonstrated the best performance, particularly in terms of AUC and calibration capability. The model's output provides a risk prediction for each patient, aiding clinicians in early detection and intervention for high-risk patients.",
  "model/duration": "The execution time of our model was evaluated to ensure its practicality in a clinical setting. The actual test took between 65.03 and 163.86 seconds from the moment variables were entered until the results were displayed. This efficient processing time allows for quick predictions, enabling clinicians to obtain results immediately and make timely interventions. The model's design prioritizes speed without compromising accuracy, making it suitable for use in critical care scenarios where rapid decision-making is essential.",
  "model/availability": "The source code for our models is not publicly released. However, the models themselves are deployed and accessible via a web application. This web application is hosted on an Nginx server, with the back-end server-side written in Flask, a framework for serving Python web applications. The models were saved and loaded using the pickle package in Python, ensuring that they can be easily integrated into the web application for practical use. This deployment allows clinicians to utilize the models for early prediction of low cardiac output syndrome risk after valve surgery, facilitating timely intervention and prevention.",
  "evaluation/method": "The evaluation method for our study involved several key steps to ensure the robustness and reliability of our machine learning models. We began by splitting the data into training and testing datasets in an 80% to 20% ratio, allowing for internal validation of our models. Additionally, cross-validation was performed during the training phase to further validate the models' performance.\n\nThe predictive performance of the models was primarily evaluated using the area under the curve (AUC) of the receiver operating characteristic (ROC). We compared the ROC curves of different models using Delong\u2019s test and calculated the sensitivity, specificity, and accuracy of each model at the optimal threshold determined by the maximum Youden index. The calibration performance of the models was assessed using calibration curves and Brier scores.\n\nTo select the optimal model, we considered both predictive and calibration capabilities. We also compared the predictive performance of our preoperative and postoperative models with the vasoactive-inotropic score (VIS), a commonly used clinical tool for assessing low cardiac output syndrome (LCOS).\n\nIn summary, our evaluation method included data splitting, cross-validation, ROC analysis, sensitivity, specificity, accuracy calculations, calibration assessment, and comparison with existing clinical tools to ensure comprehensive and reliable evaluation of our models.",
  "evaluation/measure": "In our study, we employed several performance metrics to comprehensively evaluate the predictive capabilities of our machine learning models for low cardiac output syndrome (LCOS). The primary metric used was the area under the curve (AUC) of the receiver operating characteristic (ROC), which provides a measure of the model's ability to discriminate between patients who will develop LCOS and those who will not. This metric is widely recognized and used in the literature for evaluating predictive models, ensuring our results are comparable with other studies.\n\nIn addition to the AUC, we reported sensitivity, specificity, and accuracy at the optimal threshold determined by the maximum Youden index. Sensitivity measures the true positive rate, specificity measures the true negative rate, and accuracy provides an overall measure of correct predictions. These metrics offer a detailed view of the model's performance across different aspects.\n\nTo assess the calibration of our models, we used Brier scores and calibration curves. The Brier score quantifies the accuracy of probabilistic predictions, with lower scores indicating better calibration. Calibration curves visually represent how well the predicted probabilities match the actual outcomes, providing insight into the reliability of the model's predictions.\n\nWe also compared our models' performance with the vasoactive-inotropic score (VIS), a commonly used clinical tool for assessing LCOS. This comparison helps contextualize the practical value of our models in a clinical setting.\n\nOverall, the set of metrics we reported is representative of the standards in the literature, ensuring that our evaluation is thorough and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of our machine learning models with established methods to evaluate their performance in predicting low cardiac output syndrome (LCOS). We applied five different algorithms\u2014logistic regression (LR), random forest classifier (RFC), support vector machine (SVM), extreme gradient boosting (XGB), and deep neural network (DNN)\u2014to construct our models. The performance of these models was evaluated using the area under the curve (AUC) of the receiver operating characteristic (ROC) and Brier scores.\n\nFor the preoperative model, the SVM algorithm exhibited the highest AUC of 0.786 and the best calibration capability among the five algorithms. Although the SVM algorithm did not show the highest discriminative performance, this parameter did not differ much between the five algorithms. Consequently, SVM was adopted as the algorithm for the final prediction model.\n\nIn addition to comparing different algorithms, we also compared the predictive performance of our preoperative and postoperative models with the vasoactive-inotropic score (VIS), a commonly used tool for clinical assessment of LCOS. The AUC of the VIS score was 0.776, which was significantly lower than that of our postoperative model, which had an AUC of 0.863. This comparison demonstrated the superior performance of our models in predicting LCOS.\n\nFurthermore, we evaluated the sensitivity, specificity, and accuracy of each model at the optimal threshold determined by the maximum Youden index on the ROC curves. The calibration performance of the models was also assessed based on calibration curves and Brier scores. The optimal model was selected by considering both predictive and calibration capabilities.\n\nIn summary, our study involved a comprehensive comparison of different machine learning algorithms and established clinical tools, providing a robust evaluation of our models' performance in predicting LCOS.",
  "evaluation/confidence": "The evaluation of our models included several performance metrics, each accompanied by confidence intervals to provide a range within which the true value is likely to lie. For instance, the area under the curve (AUC) of the receiver operating characteristic (ROC) for our models is reported with 95% confidence intervals. This allows for a more nuanced understanding of the model's performance beyond a single point estimate.\n\nStatistical significance was assessed using Delong's test to compare the ROC curves of different models. This test helps determine whether the observed differences in AUC values are statistically significant, thereby providing confidence in claiming that one model is superior to others. Additionally, the p-values associated with these comparisons were reported, further supporting the statistical rigor of our evaluations.\n\nThe calibration performance of the models was evaluated using Brier scores, which measure the accuracy of probabilistic predictions. Lower Brier scores indicate better calibration, and these scores were also reported with confidence intervals. This ensures that the calibration performance is not just based on a single metric but is supported by a range of possible values.\n\nIn summary, the performance metrics in our study are robustly evaluated with confidence intervals and statistical tests, providing a comprehensive and reliable assessment of model performance. This approach ensures that our claims of superiority over other methods and baselines are well-founded and statistically significant.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The original contributions presented in the study are included in the supplementary materials. Further inquiries regarding the data can be directed to the corresponding authors. The study was conducted in accordance with ethical guidelines, and the protocol was approved by the ethics committee of Nanjing First Hospital, Nanjing Medical University. The authors declare no commercial or financial relationships that could be construed as a potential conflict of interest. The study was funded by the Nanjing Key Medical Science and Technology Development Foundation and the National Natural Science Foundation of China."
}