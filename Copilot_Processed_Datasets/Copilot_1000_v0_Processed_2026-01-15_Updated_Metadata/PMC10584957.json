{
  "publication/title": "Assisting schizophrenia diagnosis using clinical electroencephalography and interpretable graph neural networks: a real-world and cross-site study.",
  "publication/authors": "Jiang H, Chen P, Sun Z, Liang C, Xue R, Zhao L, Wang Q, Li X, Deng W, Gao Z, Huang F, Huang S, Zhang Y, Li T",
  "publication/journal": "Neuropsychopharmacology : official publication of the American College of Neuropsychopharmacology",
  "publication/year": "2023",
  "publication/pmid": "37491671",
  "publication/pmcid": "PMC10584957",
  "publication/doi": "10.1038/s41386-023-01658-5",
  "publication/tags": "- EEG\n- Schizophrenia\n- Machine Learning\n- Cross-site Evaluation\n- Graph Convolutional Networks\n- EEG Biomarkers\n- Mental Health\n- Data Processing\n- Model Performance\n- Transfer Learning",
  "dataset/provenance": "The datasets used in this study were collected from four different sources. The TUH dataset, which contains EEG recordings from healthy controls (HCs), was sourced from the open-accessed TUH Abnormal EEG Corpus. This dataset includes 508 HCs, filtered from over 1400 subjects with normal neurological pathology. The Chengdu dataset comprises 93 inpatients diagnosed with schizophrenia (SCZ) and 95 HCs, recruited from West China Hospital. The Hangzhou dataset includes 194 SCZ inpatients and 56 HCs, collected from Hangzhou Seventh Hospital. The Moscow dataset, obtained from an open-access EEG database, consists of 39 HCs and 45 SCZ individuals, recorded by researchers at the Mental Health Research Center and M.V. Lomonosov Moscow State University.\n\nAll EEG data were collected in an eyes-closed resting state using the standard 10\u201320 EEG montage. The TUH dataset had original recordings ranging from 15 to 48 minutes, sampled with 33 channels at 250 Hz. The Chengdu dataset recorded 3 minutes of clinical EEG data using a 16-channel device at 128 Hz. The Hangzhou dataset was acquired with a 19-channel device at 256 Hz, while the Moscow dataset consisted of 1-minute recordings from 16 channels, sampled at 128 Hz.\n\nThe TUH dataset was used as an auxiliary resource for transfer learning, while the Chengdu dataset was primarily used to build automatic diagnosis models. The Hangzhou and Moscow datasets were utilized for cross-site performance validation. The study protocol was approved by the Ethical Committee of the West China Hospital and the Hangzhou Seventh People\u2019s Hospital, ensuring ethical standards were met.",
  "dataset/splits": "In our study, we utilized four distinct EEG datasets, each collected from different sites. The datasets were split based on the demographic characteristics of the participants, specifically focusing on healthy controls (HC) and schizophrenia patients (SCZ). The demographic statistics for each dataset are detailed in a table.\n\nThe TUH dataset consists of 508 healthy controls. The Chengdu dataset includes 95 healthy controls and 93 schizophrenia patients. The Moscow dataset comprises 39 healthy controls and 45 schizophrenia patients. The Hangzhou dataset is more varied, with different ratios of healthy controls to schizophrenia patients: 56 HCs and 194 SCZs (1:3 ratio), 112 HCs and 112 SCZs (1:1 ratio), 56 HCs and 56 SCZs (1:1 ratio), 28 HCs and 19 SCZs (2:1 ratio), and 19 HCs and 19 SCZs (3:1 ratio).\n\nFor the EEG samples, the original recordings varied in length across datasets. The TUH dataset had recordings ranging from 15 to 45 minutes, while the Chengdu and Hangzhou datasets had 3-minute recordings, and the Moscow dataset had 1-minute recordings. After preprocessing, 2-minute samples were generated for each participant in the Chengdu and Hangzhou datasets, resulting in a total of 3232 2-minute TUH samples used in the experiment. The Moscow dataset had an average sample length of 56 seconds after preprocessing.\n\nThe study also involved intra-site validation using five-fold cross-validation on the Chengdu dataset. Additionally, cross-site evaluations were conducted on the Hangzhou and Moscow datasets using classification models trained on the entire Chengdu dataset. This approach allowed us to assess the performance stability and transportability of the models across different sites and population heterogeneities.",
  "dataset/redundancy": "The study utilized four distinct EEG datasets, each collected from different sites and populations. The datasets were split based on their intended use in the study: one for model training and the others for cross-site validation.\n\nThe TUH dataset, consisting of 508 healthy controls (HCs), was used exclusively as an auxiliary resource for transfer learning. This dataset was filtered from a larger pool of subjects, ensuring that only those with normal neurological pathology and no history of mental disorders were included. The TUH dataset was not used for direct training or testing of the diagnosis models but rather to enhance the model's performance through transfer learning techniques.\n\nThe Chengdu dataset, comprising 93 schizophrenia (SCZ) patients and 95 HCs, was used to build the automatic diagnosis models. This dataset was collected from inpatients at West China Hospital and healthy controls recruited through advertisements. The Chengdu dataset served as the primary training set for developing the diagnosis models.\n\nThe Hangzhou and Moscow datasets were used for cross-site performance validation. The Hangzhou dataset included 194 SCZ patients and 56 HCs, while the Moscow dataset consisted of 45 SCZ patients and 39 HCs. These datasets were collected from different hospitals and populations, ensuring that the training and test sets were independent. The Hangzhou dataset was used to test the model's performance stability with diverse SCZ prevalence and population heterogeneity. The Moscow dataset, consisting of adolescent males, was used to evaluate the model's transportability from adult data to adolescent data.\n\nThe independence of the training and test sets was enforced by ensuring that the datasets were collected from different sites and populations. This approach minimized the risk of data leakage and ensured that the model's performance could be generalized to new, unseen data. The distribution of the datasets compares favorably to previously published machine learning datasets in the field of EEG-based diagnosis, as it includes a diverse range of subjects and ensures the independence of training and test sets.",
  "dataset/availability": "The TUH dataset is publicly accessible at https://isip.piconepress.com/projects/tuh_eeg/html/downloads.shtml. The Moscow dataset can be accessed at http://brain.bio.msu.ru/eeg_schizophrenia.htm. Both datasets are open-accessed, allowing researchers to freely use the data for their studies.\n\nThe Chengdu and Hangzhou clinical EEG datasets, which support the findings of this study, are accessible upon reasonable request from the corresponding author. Access to these datasets is contingent upon approval from the Institutional Review Board of the West China Hospital and the Hangzhou Seventh People\u2019s Hospital. This ensures that the data is used ethically and in accordance with the guidelines set by these institutions. The datasets are not publicly available to maintain the privacy and confidentiality of the participants involved in the study.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is Graph Convolutional Networks (GCNs). This approach is employed for EEG graph representation and diagnosis classification. The GCN model is not entirely new, as GCNs have been established in the literature for various applications, including those involving graph-structured data. However, our implementation and application to EEG data, particularly for schizophrenia diagnosis, represent a novel contribution to the field.\n\nThe reason this work was not published in a machine-learning journal is that the primary focus of our study is on the application of machine learning techniques to neurophysiological data, specifically EEG signals, for medical diagnosis. The innovation lies in the integration of GCNs with transfer learning strategies tailored for EEG data, aiming to improve the diagnosis of schizophrenia across different datasets and sites. This interdisciplinary approach, combining machine learning with neuroscience and clinical applications, is more aligned with journals that focus on neuroinformatics, biomedical engineering, or psychiatric research. The emphasis is on the practical implications and the potential for clinical translation rather than the development of a new machine-learning algorithm per se.",
  "optimization/meta": "The model employs a meta-learning approach, which involves two meta-learners designed to enhance performance. These meta-learners include a schizophrenia (SCZ) classifier based on contrastive learning and a domain discriminator aimed at aligning feature distributions across different datasets.\n\nThe SCZ classifier comprises a fully-connected layer with two output units, connected to a Softmax activation function. The InfoNCE loss function of contrastive learning is used to improve classification performance by minimizing the distance between a given sample and samples of the same class while maximizing the distance between a given sample and samples from another class.\n\nSimilarly, the domain discriminator also consists of a fully-connected layer with two output units, connected with a Softmax activation function. This component helps in reducing the variance of the marginal distribution of the source and target data, thereby promoting domain-invariant features.\n\nThe training process involves integrating healthy control (HC) data from the TUH dataset into the training data as an augmentation. In each iteration, every 300 epochs, HCs from the TUH dataset are randomly sampled in equivalent size and added to the training data. This augmentation enriches the HC patterns in the training data, and the two meta-learners are used to incrementally optimize performance.\n\nThe model does not use data from other machine-learning algorithms as input. Instead, it leverages contrastive learning and domain adaptation techniques to improve the generalization and transportability of the diagnosis model across different datasets. The training data for the meta-learners are independent, with HC data from the TUH dataset being randomly sampled and integrated into the training process to enhance the model's performance.",
  "optimization/encoding": "The data encoding process involved extracting both frequency-domain and time-domain features from the EEG signals. For the frequency domain, absolute power and differential entropy were computed across five different rhythms: delta (1\u20134 Hz), theta (4\u20138 Hz), alpha (8\u201313 Hz), beta (13\u201330 Hz), and gamma (30\u201344 Hz). This resulted in 10 frequency features per channel. In the time domain, features such as energy, amplitude, mean value, variance, first and second order differences of signals, Hjorth parameters (activity, mobility, complexity), and Petrosian fractal dimension were extracted. Additionally, demographic features like age and gender were included to account for individual discrepancies in EEG signals.\n\nTo address variability introduced by different EEG devices across sites, a min-max normalization technique was applied to all extracted features independently for each dataset. This step helped mitigate confounding factors and ensured fair comparisons across datasets.\n\nThe EEG signals were recorded using the standard 10\u201320 montage and were collected in an eyes-closed resting state. The original recordings varied in length: 15\u201345 minutes for the TUH dataset, 3 minutes for the Chengdu and Hangzhou datasets, and 1 minute for the Moscow dataset. These signals underwent automated data cleaning, transformation, and splitting before being used in experiments. For the TUH dataset, recordings between the 3rd and 15th minutes were used to avoid interference from electromyography and electrooculogram signals and to minimize fatigue effects. The recordings were then split into 2-minute samples with a 1-minute overlap, resulting in 3232 samples for the TUH dataset and 2-minute samples for the Chengdu and Hangzhou datasets. The Moscow dataset had an average sample length of 56 seconds after preprocessing.\n\nThe preprocessing pipeline included bandpass filtering (0.1\u201340 Hz), removal of artifactual channels, application of the Multiple Artifact Rejection Algorithm (MARA) for artifact correction, and channel interpolation and referencing. This standardized approach ensured consistent data quality across all datasets.",
  "optimization/parameters": "The model utilizes a combination of frequency and time-domain features extracted from EEG signals, along with demographic features such as gender and age. Specifically, the features include power and differential entropy (DE) across five frequency bands and 16 EEG channels, time-domain features across 16 channels, and demographic features.\n\nThe selection of these features was guided by their relevance to schizophrenia (SCZ) diagnosis and their ability to differentiate between healthy controls (HCs) and SCZ patients. The importance of these features was evaluated using SHAP values, which indicated the evolution of feature importance from one dataset to another through the transfer learning process.\n\nThe model's performance was optimized by balancing the classifier loss and the domain discriminator loss using tradeoff parameters. This approach ensured that the model could effectively generalize across different datasets with varying population heterogeneity.\n\nNot sure about the exact number of parameters used in the model, as it depends on the specific implementation and configuration of the graph convolutional network (GCN) and other machine learning algorithms employed. However, the features used in the model provide a comprehensive representation of the EEG data, enabling robust and stable performance in SCZ diagnosis.",
  "optimization/features": "In our study, we utilized a comprehensive set of features extracted from EEG signals to ensure robust and generalizable diagnosis models. Specifically, we focused on two main types of features: frequency-domain and time-domain features, along with demographic features.\n\nFor frequency-domain features, we extracted absolute power and differential entropy (DE) across five different rhythms: delta (1\u20134 Hz), theta (4\u20138 Hz), alpha (8\u201313 Hz), beta (13\u201330 Hz), and gamma (30\u201344 Hz). This resulted in 10 different frequency features for each of the 16 EEG channels, totaling 160 frequency-domain features.\n\nTime-domain features included energy, amplitude, mean value, variance, first and second order differences of signals, first and second order differences of normalized signals, Hjorth activity, Hjorth mobility, Hjorth complexity, and Petrosian fractal dimension. These features were also extracted for each of the 16 channels, resulting in 14 time-domain features per channel, totaling 224 time-domain features.\n\nAdditionally, we considered demographic features such as age and gender, which are crucial due to the significant individual variability in EEG signals.\n\nTo address measurement variability between datasets, we implemented a min-max normalization technique for all extracted features in each dataset independently. This step helped mitigate confounding factors and ensured fair comparisons across datasets.\n\nFeature selection was not explicitly performed as a separate step. Instead, we relied on the model's ability to learn and prioritize the most relevant features during training. The model was primarily trained using data from Chengdu, with augmentation from the TUH dataset, and validated across different sites, including Hangzhou and Moscow. This approach allowed the model to adapt and generalize across diverse datasets, ensuring the robustness of the selected features.",
  "optimization/fitting": "In our study, we employed a Graph Convolutional Network (GCN) model for the diagnosis of schizophrenia using EEG data. The model was trained using the Chengdu dataset, with augmentation from the TUH dataset. To address the potential issue of overfitting, given the complexity of the model and the number of parameters, we implemented several strategies.\n\nFirstly, we used a simple left-right symmetric graph structure for the EEG channels, which helped in reducing the complexity of the model. More complicated graph structures with dynamic edge weights between node pairs were also tested, but they resulted in a drop in cross-site AUC by about 3%. This indicated that complex EEG networks can lead to overfitting and reduce the generalizability of the diagnosis model.\n\nSecondly, we incorporated transfer learning (TL) strategies to improve the model's performance and generalizability. The TL strategies included pre-training on the TUH dataset, meta-learning, and domain adaptation. These strategies helped in stabilizing the performance across different sites and SCZ prevalence, demonstrating the potential of using TL to scale up the dissemination of EEG-based diagnosis tools.\n\nTo ensure that the model was not underfitting, we evaluated its performance on multiple datasets and under different conditions. The model showed steady performance improvements with the incremental implementation of different TL strategies. Additionally, the model's performance was compared with other deep learning and machine learning methods, and it demonstrated strong stability and robustness for different SCZ prevalence.\n\nIn summary, we addressed the potential issues of overfitting and underfitting by using a simple graph structure, implementing TL strategies, and evaluating the model's performance under various conditions. These measures helped in ensuring the model's generalizability and robustness.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and enhance the generalizability of our models. One key approach was the use of transfer learning (TL) strategies, which helped to improve the performance and stability of our models across different datasets and sites. Specifically, we integrated TL strategies such as pre-training (PT), meta-learning (META), and a combination of both (PT+META) into our graph convolutional network (GCN) model. These strategies significantly improved the model's performance, as evidenced by the continuous enhancements in AUC and accuracy metrics.\n\nAdditionally, we explored different graph structures for EEG representations. While more complex graph structures initially seemed promising, they were found to lead to overfitting and reduced generalizability. Therefore, we opted for a simpler, left-right symmetric graph structure, which demonstrated better cross-site performance. This choice was crucial in maintaining the model's robustness and stability across various datasets.\n\nFurthermore, we utilized a Box\u2013Cox transformation to address the long-tailed distribution of time-domain characteristics in the TUH EEG corpus. This transformation made the distribution more similar to a normal distribution, which helped in reducing overfitting and improving the model's performance.\n\nIn summary, our regularization methods included the strategic use of transfer learning, the selection of a simpler graph structure, and the application of the Box\u2013Cox transformation. These techniques collectively contributed to the prevention of overfitting and ensured the model's robustness and generalizability across different datasets and sites.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are reported in the supplementary materials of the publication. Detailed experiment configurations and hyper-parameters are provided in the \"Training and parameters\" section of the supplementary materials. This includes information on the training process, the parameters used for the models, and the optimization strategies employed.\n\nThe supplementary materials also include tables and figures that provide additional details on the performance metrics and the evaluation criteria used. These materials are essential for replicating the experiments and understanding the optimization process.\n\nRegarding the availability and licensing, the supplementary materials are typically made available alongside the publication. The specific licensing details would depend on the journal's policies, but generally, the supplementary materials are accessible to readers and researchers for further study and replication of the results. For precise licensing information, one would need to refer to the journal's guidelines or the specific publication details.",
  "model/interpretability": "The model developed in this study is designed to be interpretable, moving away from the black-box nature of many machine learning algorithms. This interpretability is crucial for understanding and trusting the results, especially in clinical applications.\n\nTo achieve this, we employed Explainable AI (XAI) techniques. Specifically, we used SHAP (SHapley Additive exPlanations) values to interpret the model's predictions. SHAP values provide a way to attribute the output of the model to the input features, making it clear which features are most important in the decision-making process.\n\nIn our study, we visualized the SHAP values of the top 15 features before and after applying transfer learning (TL). This visualization showed how the importance of different features evolved as the model adapted from one dataset to another. For instance, frequency features were prominent in one dataset, while time-domain features gained significance in another, alongside frequency features. This adaptation highlights the model's ability to learn and prioritize relevant features across different clinical settings.\n\nMoreover, certain features, such as theta and alpha band power, remained consistently important across datasets. This consistency suggests that these features are robust biomarkers for schizophrenia pathology. Additionally, specific EEG channels (e.g., Fp1, Fp2, P4, O1, and O2) were commonly identified as important, indicating potential spatial-temporal abnormalities associated with schizophrenia.\n\nBy making the model interpretable, we not only enhance its trustworthiness but also provide insights into the underlying mechanisms of schizophrenia, which can be valuable for further research and clinical practice.",
  "model/output": "The model is a classification model designed to diagnose schizophrenia (SCZ) using EEG data. It employs a graph convolutional network (GCN) to process EEG graph representations. The model includes a schizophrenia classifier and a domain discriminator, both utilizing fully-connected layers with Softmax activation functions. The classifier is trained using the InfoNCE loss function from contrastive learning, which aims to enhance classification performance by minimizing the distance between samples of the same class while maximizing the distance between samples of different classes. The domain discriminator helps align feature distributions across different datasets, promoting better generalization across sites.\n\nThe model's performance is evaluated using standard metrics such as AUC, ACC, sensitivity, specificity, precision, and F1 score. These metrics provide a comprehensive assessment of the model's diagnostic capabilities. The evaluation includes intra-site and cross-site assessments, with the latter focusing on the model's transportability and performance stability across different datasets from various sites. The cross-site evaluation involves testing the model on datasets from Hangzhou and Moscow, which have different demographic characteristics and EEG sample lengths compared to the primary training dataset from Chengdu.\n\nThe model's performance is further enhanced through transfer learning (TL) strategies, which include pre-training on the TUH dataset and using meta-learners for incremental optimization. These strategies help improve the model's generalization and stability, particularly in cross-site evaluations. The results demonstrate that the model achieves high diagnostic accuracy and robustness, making it a promising tool for schizophrenia diagnosis using EEG data.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code of the proposed approach is publicly accessible. It can be found on GitHub at the repository named \"Diagnosis-based-EEG-A-cross-site-study\". Additionally, the codes for traditional machine learning algorithms used in this study are available on the scikit-learn GitHub repository. The code used for SHAP analysis, which was employed to interpret the model, is also available on GitHub. All these resources are open-source and can be utilized by researchers and practitioners interested in replicating or building upon the work presented in this study.",
  "evaluation/method": "The evaluation method employed in this study was comprehensive and multifaceted, ensuring robust assessment of the proposed models. Initially, an intra-site evaluation was conducted using five-fold cross-validation on the Chengdu dataset. This approach involved partitioning the data into five subsets, training the model on four subsets, and validating it on the remaining subset, repeating this process five times to ensure thorough validation.\n\nFollowing the intra-site evaluation, a cross-site evaluation was performed using the Hangzhou and Moscow datasets. The models were trained on the entire Chengdu dataset and then tested on these external datasets to assess their transportability and performance stability across different sites. This evaluation included subsets of the Hangzhou dataset with varying ratios of healthy controls (HCs) to schizophrenia patients (SCZs) to test performance stability with diverse SCZ prevalence and population heterogeneity.\n\nAdditionally, the performance of models with and without transfer learning (TL) strategies was compared to evaluate the original cross-site performance and potential improvements after implementing TL strategies. Various deep learning (DL) algorithms, such as EEGNet, LSTM, and GAT, as well as machine learning (ML) algorithms like SVM, logistic regression, Random Forest, and XGBoost, were also implemented for performance comparison. These algorithms used different feature sets, with raw EEG signals for EEGNet and LSTM, and the same feature set as the proposed GCN for GAT and all ML algorithms.\n\nThe evaluation criteria included standard metrics such as AUC, ACC, sensitivity, specificity, precision, and F1 score, providing a comprehensive assessment of the models' performance. All experiments were repeated 50 times, and the mean and standard deviation of each evaluation criterion were reported to ensure reliability. Statistical analyses were conducted using IBM SPSS Statistics 22 and R (version 4.1), with significance set at p < 0.05. This rigorous evaluation method ensured that the models were thoroughly tested and validated across different datasets and conditions.",
  "evaluation/measure": "The performance metrics reported in this study are comprehensive and widely accepted in the field of machine learning and medical diagnostics. These metrics include the Area Under the Curve (AUC), Accuracy (ACC), sensitivity, specificity, precision, and the F1 score. These metrics provide a thorough evaluation of the model's performance from various perspectives.\n\nThe AUC measures the model's ability to distinguish between classes, providing a single scalar value that represents the performance across all classification thresholds. Accuracy indicates the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, also known as recall, measures the proportion of actual positives that are correctly identified by the model. Specificity measures the proportion of actual negatives that are correctly identified. Precision indicates the proportion of positive identifications that are actually correct. The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both concerns.\n\nThese metrics are standard in the literature and are used to ensure that the model's performance is evaluated objectively and comprehensively. The use of these metrics allows for a clear comparison with other studies and ensures that the model's performance is assessed from multiple angles, providing a robust evaluation of its diagnostic capabilities.",
  "evaluation/comparison": "In the evaluation of our proposed methods, a comprehensive comparison was conducted with various publicly available deep learning (DL) and machine learning (ML) methods on benchmark datasets. Specifically, we compared our GCN model with EEG_TL implementation against several established algorithms, including EEGNet, LSTM, GAT, SVM, Logistic Regression, Random Forest, and XGBoost. These methods were chosen due to their common use in EEG-based classification tasks.\n\nFor the DL algorithms, EEGNet utilized raw time series of EEG signals as features, while LSTM and GAT, along with all ML algorithms, used the same set of features as our proposed GCN. This ensured a fair comparison across different methodologies.\n\nThe performance of these methods was evaluated using standard metrics such as AUC, ACC, sensitivity, specificity, precision, and F1 score. The evaluation criteria provided an objective assessment from multiple aspects, ensuring that the comparison was thorough and unbiased.\n\nIn addition to comparing with advanced DL and ML methods, we also evaluated simpler baselines to understand the relative performance improvements. For instance, Logistic Regression, a simpler ML algorithm, was included in the comparison to assess the baseline performance. This approach helped in highlighting the advantages of more complex models like GCN with EEG_TL implementation.\n\nThe results of these comparisons are depicted in various figures and tables within the publication, providing a clear visualization of how our proposed methods stack up against existing techniques. Supplementary materials, including detailed performance tables and figures, offer additional insights into the comparative analysis.",
  "evaluation/confidence": "The evaluation of our models included standard metrics such as AUC, ACC, sensitivity, specificity, precision, and F1 score. These metrics were reported with their mean and standard deviation values, providing a measure of confidence in the performance estimates. The experiments were repeated 50 times to ensure robustness and reliability of the results.\n\nStatistical significance was assessed using appropriate tests. For categorical variables, the \u03c7\u00b2 test was used, while continuous variables were compared using a two-tailed two-sample t-test, assuming a normal distribution of the data. The significance level was set at p < 0.05 for all tests. This rigorous statistical analysis ensures that the observed performance improvements are not due to random chance.\n\nIn the cross-site evaluation, the performance of different methods was compared, and statistically significant improvements were noted. For instance, the EEG_TL framework showed significant performance gains over other methods, with improvements in AUC and ACC that were statistically significant. This indicates that the proposed methods are not only effective but also superior to baseline and other compared methods.\n\nAdditionally, the performance stability across different prevalence ratios and datasets was evaluated. The base GCN model demonstrated strong stability and robustness, with mean cross-site AUCs and ACCs that were consistently higher than those of other methods like logistic regression. This further supports the claim that the proposed methods are reliable and generalizable across different conditions and populations.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The Chengdu and Hangzhou clinical EEG datasets, which were used for evaluation, can be accessed from the corresponding author upon reasonable request. Approval from the Institutional Review Board of the West China Hospital or the Hangzhou Seventh People\u2019s Hospital is required to access these datasets. The TUH and Moscow datasets, used as auxiliary resources and for cross-site performance validation, are open-accessed and can be found at the following links:\n\n* TUH dataset: [TUH Abnormal EEG Corpus](https://isip.piconepress.com/projects/tuh_eeg/html/downloads.shtml)\n* Moscow dataset: [Moscow EEG Database](http://brain.bio.msu.ru/eeg_schizophrenia.htm)\n\nThe codes used for the evaluation, including the proposed approach and traditional machine learning algorithms, are publicly available. The code for the proposed approach can be found on GitHub at [Diagnosis-based EEG: A cross-site study](https://github.com/ChenPeiyin/Diagnosis-based-EEG-A-cross-site-study). The codes for traditional machine learning algorithms are available at [scikit-learn](https://github.com/scikit-learn/scikit-learn). Additionally, the code used for SHAP analysis is available at [SHAP](https://github.com/slundberg/shap)."
}