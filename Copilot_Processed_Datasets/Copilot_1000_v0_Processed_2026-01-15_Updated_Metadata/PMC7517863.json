{
  "publication/title": "An artificial neural network-based model to predict chronic kidney disease in aged cats.",
  "publication/authors": "Biourge V, Delmotte S, Feugier A, Bradley R, McAllister M, Elliott J",
  "publication/journal": "Journal of veterinary internal medicine",
  "publication/year": "2020",
  "publication/pmid": "32893924",
  "publication/pmcid": "PMC7517863",
  "publication/doi": "10.1111/jvim.15892",
  "publication/tags": "- Chronic Kidney Disease\n- Feline Medicine\n- Artificial Neural Networks\n- Predictive Modeling\n- Veterinary Diagnostics\n- Disease Prediction\n- Machine Learning in Medicine\n- Cat Health\n- Renal Disease\n- Big Data in Veterinary Science",
  "dataset/provenance": "The dataset used in this study originates from two primary sources: the Royal Veterinary College (RVC) and Banfield Pet Hospitals. The RVC contributed two datasets, RVC1 and RVC2, which included cats that were at least 7 years old and considered healthy based on various health screenings. The RVC1 dataset consisted of 218 cats, while the RVC2 dataset included 60 cats. These datasets encompassed a wide range of numerical and non-numerical variables collected at multiple time points, describing the cats' environment, signalment, clinical examination findings, vaccination history, packed cell volume, plasma biochemistry, urinalysis, and biomarkers such as parathyroid hormone (PTH) and fibroblast growth factor 23 (FGF-23).\n\nThe Banfield dataset was derived from electronic medical records of cats visiting Banfield Pet Hospitals between January 1995 and June 2016. From these records, 1510 cats with chronic kidney disease (CKD) were identified based on specific criteria, including plasma creatinine concentration, urine specific gravity (USG), and medical notes. Controls were cats that had not been diagnosed with CKD and remained free of the disease for at least two years beyond the last data point provided to the model.\n\nIn total, the study analyzed 10,576 visits, with 672 visits from the RVC1 dataset, 60 visits from the RVC2 dataset, and 9,844 visits from the Banfield dataset. The datasets included visits excluded due to missing data or cats younger than 7 years. The baseline characteristics of the cats showed that most were neutered, with a higher proportion of female cats. The mean age of the cats ranged from 11.1 years in the Banfield dataset to 13.2 years in the RVC1 dataset.\n\nThe datasets were used in three phases of model development. Phase 1 involved the initial development of the model using the RVC1 dataset. Phase 2 tested the model's performance using the Banfield and RVC2 datasets as independent data. Phase 3 consisted of a new cycle of training and validation using all three datasets combined. The model aimed to predict the occurrence of CKD based on a limited set of data, including creatinine, USG, and urea, which were selected as input variables due to their discriminant weight in factor discriminant analysis.",
  "dataset/splits": "The dataset was divided into three main splits for the modeling process. The first split, referred to as the RVC1 dataset, consisted of data from 218 cats and was used to design and build the initial model. However, due to missing data and age restrictions, only 672 visits from this dataset were included in the analysis.\n\nThe second split, known as the RVC2 dataset, included data from 60 cats. This dataset, along with a third split from Banfield containing data from 3486 cats, was used to validate the model in phase 2. After excluding visits due to missing data and age restrictions, 60 visits from the RVC2 dataset and 9844 visits from the Banfield dataset were analyzed.\n\nIn phase 3, the model was regenerated using a combined dataset that included RVC1, RVC2, and Banfield data. This combined dataset encompassed a total of 10,576 visits for analysis. The distribution of cats across the datasets showed that most were neutered, with the proportion of female cats ranging from 51% to 56%. The mean age of cats varied, with the Banfield dataset having the youngest mean age of 11.1 years and the RVC1 dataset the oldest at 13.2 years. Over the study period, 76% of cats in the RVC1 dataset and 57% of Banfield cats remained nonazotemic.",
  "dataset/redundancy": "The datasets were split into three main groups: RVC1, RVC2, and Banfield. The RVC1 dataset, consisting of data from 218 cats, was used to design and build the initial model. The RVC2 dataset, with data from 60 cats, and the Banfield dataset, with data from 3486 cats, were used to validate the model in phase 2 as independent datasets.\n\nThe training and test sets were kept independent by using different groups of cats for each phase. The RVC1 dataset was used exclusively for the initial model development, while the RVC2 and Banfield datasets were used for validation. This independence was enforced by ensuring that the cats in the validation datasets were not part of the training dataset.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in terms of the thoroughness of the data collection and the diversity of the variables included. The datasets encompassed a large set of numerical and non-numerical variables per time point, describing the cat's environment, signalment, clinical examination findings, vaccination history, packed cell volume, plasma biochemistry, urinalysis, and biomarkers. This comprehensive approach ensures that the model is robust and generalizable.\n\nVisits were excluded due to missing data or if the cats were younger than 7 years, ensuring that only high-quality, relevant data was used for analysis. This resulted in a total of 10,576 visits for analysis, with 672 from RVC1, 60 from RVC2, and 9844 from Banfield. The baseline characteristics of the cats, such as age and gender distribution, were summarized to provide a clear picture of the dataset composition.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used is artificial neural networks (ANNs), specifically multilayer feed-forward neural networks, also known as multilayer perceptrons (MLPs). These networks incorporate a back-propagation algorithm for training.\n\nThe algorithm is not new; it is a well-established method in the field of machine learning and has been widely used in various applications, including medical diagnostics and prognostics. The choice of ANN for this study was driven by its ability to handle large and complex datasets, making it suitable for the era of \"big data.\" ANNs are particularly useful in medical fields where early diagnosis can significantly impact outcomes, such as in cancer, neurology, and cardiology. In veterinary medicine, ANNs have been applied to predict disease development in cats, validating their effectiveness in this domain.\n\nThe decision to use ANNs in this study was motivated by their proven track record in medical diagnostics and their ability to harness the value of extensive datasets. The focus of the publication is on the application of ANNs to predict chronic kidney disease (CKD) in cats, rather than on the development of a new machine-learning algorithm. Therefore, it was published in a veterinary or medical journal rather than a machine-learning journal. The emphasis is on the practical application and validation of ANNs in a specific medical context, demonstrating their utility in improving diagnostic and prognostic capabilities in veterinary medicine.",
  "optimization/meta": "The model employs a meta-predictor approach, utilizing data from multiple submodels as input. These submodels are constructed using artificial neural networks (ANNs), specifically multilayer feed-forward neural networks known as multilayer perceptrons (MLPs). The ANNs incorporate a back-propagation algorithm for training.\n\nThe submodels are designed to predict the occurrence of chronic kidney disease (CKD) within various time frames: current status (M0), and future occurrences within 3 (M3), 6 (M6), 9 (M9), 12 (M12), or more than 12 months (M12+). Each submodel is trained independently using data from a single visit, ensuring that the training data for each submodel is distinct.\n\nThe meta-predictor aggregates the outputs of these submodels to make a final prediction. This approach allows for the integration of predictions from different time frames, providing a comprehensive assessment of CKD risk. The independence of the submodels is maintained by training each on separate visit data, which helps in avoiding overfitting and ensures that the predictions are robust across different time points.",
  "optimization/encoding": "The data encoding and preprocessing involved several steps to ensure the dataset was suitable for the machine-learning algorithm. Initially, the raw dataset was cleaned, reducing it from 116 to 16 variables. This process excluded variables with over 40% missing data, qualitative variables with minimal relevance to chronic kidney disease (CKD), and blood variables that were not consistently present. The remaining variables included age, creatinine, urine specific gravity (USG), chloride, total plasma protein, phosphate, total plasma calcium, albumin, globulin, urea, alanine transaminase, alkaline phosphatase, bilirubin, cholesterol, sodium, and potassium.\n\nFactor discriminant analysis (FDA) was then applied to these 16 variables to select the best predictors for each time range. This analysis was conducted separately on each submodel dataset to ensure that the most relevant predictors were retained for each specific time frame.\n\nThe input data used to build the submodels consisted of visits rather than the cat's history. Each visit was considered independently, with the CKD status at the time of the visit termed the M0 status. Once a cat became CKD positive at any visit, it was automatically entered into the model with a positive status for all subsequent visits, regardless of the predictions from following submodels.\n\nThe correlations between the selected variables were examined to ensure they were low enough to exclude potential modeling problems associated with covariation of predictors. The correlations between creatinine and USG, as well as between urea and urine specific gravity, varied between -0.2 and -0.3. The correlation between creatinine and urea was approximately 0.54, which was deemed acceptable for the modeling process.\n\nFor the artificial neural network (ANN) modeling, a multilayer feed-forward neural network, known as a multilayer perceptron (MLP), was used. This network incorporated a back-propagation algorithm. The input/output vector pairs were presented to the network for the training process. The network calculated an output vector for each input vector, and the error term was derived by comparing this output vector with the actual output vector. The weights and biases were updated using this error term to minimize the error.\n\nA 10-fold cross-validation approach with 5 repetitions was used for each submodel, and the 20 best ANNs were selected to form an \"ensemble model.\" During the training step, the internal parameters of the MLP were tuned simultaneously in a full factorial design. These parameters included the number of hidden layers, the number of neurons in the hidden layer, and the decay rate. The number of neurons in the hidden layer was tested from 2 to 30, and the decay rate varied from 0.001 to 0.1. Receiver operator curves (ROCs) were generated for the validation datasets to plot the true positive rate (sensitivity) against the false positive rate (1-specificity) across various thresholds. Optimal parameter values were selected based on the area under the curve (AUC) of the ROCs as the measure of model accuracy.",
  "optimization/parameters": "The model utilized 16 input parameters. These parameters were selected through a rigorous process of cleaning the raw dataset, which initially contained 116 variables. Variables with more than 40% missing data were excluded, as were qualitative variables deemed minimally relevant to chronic kidney disease (CKD) or those with a paucity of abnormal findings. Additionally, blood variables that were not consistently present with other variables to form complete sets of measures were removed. The remaining variables included age, creatinine, urine specific gravity (USG), chloride, total plasma protein, phosphate, total plasma calcium, albumin, globulin, urea, alanine transaminase, alkaline phosphatase, bilirubin, cholesterol, sodium, and potassium. Factor discriminant analysis (FDA) with a threshold set to 0.5 was applied separately to each submodel dataset to ensure that the best predictors for each time range were retained. This methodical approach ensured that the most relevant and reliable variables were used in the model.",
  "optimization/features": "The input features for the model were selected through a process that began with a raw dataset containing 116 variables. This dataset was cleaned to exclude variables with more than 40% missing data, qualitative variables with minimal relevance to chronic kidney disease (CKD), and blood variables that were not consistently present. This reduction left 16 variables, which included age, creatinine, urine specific gravity (USG), chloride, total plasma protein, phosphate, total plasma calcium, albumin, globulin, urea, alanine transaminase, alkaline phosphatase, bilirubin, cholesterol, sodium, and potassium.\n\nFeature selection was performed using factor discriminant analysis (FDA) with a threshold set to 0.5. This analysis was applied separately to each submodel dataset to ensure that the best predictors for each time range were retained. The variables that had a discriminant weight of 0.5 or greater were retained as input variables for the predictive model. For each of the time-delineated submodels (M0-M12+), the same three variables\u2014creatinine, USG, and urea\u2014had a discriminant weight of 0.5 or greater and were retained as input variables.\n\nThe correlations between the selected variables were also considered to exclude potential modeling problems associated with covariation of predictors. The correlations between creatinine and USG, as well as between urea and urine specific gravity, varied between -0.2 and -0.3. The correlation between creatinine and urea was approximately 0.54, which were low enough to proceed without issues related to covariation.",
  "optimization/fitting": "The model employed a multilayer feed-forward neural network, specifically a multilayer perceptron (MLP), with a back-propagation algorithm. This approach involved presenting input/output vector pairs to the network during the training process. The network calculated an output vector for each input vector, and the error term was derived by comparing this output with the actual output. The weights and biases were updated using this error term to minimize the error.\n\nTo address the potential issue of overfitting, a 10-fold cross-validation approach with 5 repetitions was used for each submodel. This method ensured that the model's performance was evaluated on multiple subsets of the data, reducing the likelihood of overfitting to any single subset. Additionally, the 20 best artificial neural networks (ANNs) were selected each time to form an \"ensemble model,\" which further helped in mitigating overfitting by averaging the predictions of multiple models.\n\nThe internal parameters of the MLP, including the number of hidden layers, the number of neurons in the hidden layer, and the decay, were tuned simultaneously in a full factorial design. This design tested all combinations for each submodel, ensuring that the model was optimized for the given data. The number of neurons in the hidden layer was tested from 2 to 30, and the decay was set to vary from 0.001 to 0.1. This range of parameters helped in finding the optimal configuration that balanced model complexity and performance.\n\nReceiver operator curves (ROCs) were generated for the validation datasets by plotting the true positive rate (sensitivity) against the false positive rate (1-specificity) across various thresholds. Optimal parameter values were selected based on the area under the curve (AUC) of the ROCs, which served as the measure of model accuracy. This approach ensured that the model was not underfitted by selecting parameters that maximized the AUC, indicating a good balance between sensitivity and specificity.\n\nThe final calculation applied the best submodels to all the initial data based on one visit from each cat. This process was repeated using the different visits for each cat, and the average values for each variable (sensitivity, specificity, positive predictive value, and negative predictive value) were reported. This method ensured that the model's performance was robust and generalizable across different visits and cats.",
  "optimization/regularization": "A regularization method was not explicitly mentioned in the provided information. However, techniques to prevent overfitting were employed during the modeling process. A 10-fold cross-validation approach with 5 repetitions was used for each submodel, and the 20 best artificial neural networks (ANNs) were selected each time to form an \"ensemble model.\" This ensemble approach helps to reduce overfitting by averaging the predictions of multiple models, thereby improving the generalization performance. Additionally, the internal parameters of the multilayer perceptron (MLP) were tuned simultaneously in a full factorial design that tested all combinations for each submodel. This thorough tuning process helps to ensure that the model is robust and not overly fitted to the training data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are available through the R software and its associated libraries. Specifically, we utilized the MASS, ADE4, CARET, pROC, and NNET libraries for modeling and machine learning. The configurations and parameters were determined through a 10-fold cross-validation approach with 5 repetitions for each submodel, ensuring robust model selection. The best submodels were selected based on the area under the curve (AUC) of the ROCs, which served as the measure of model accuracy.\n\nThe optimization schedule involved a multilayer feed-forward neural network, known as a multilayer perceptron (MLP), incorporating a back-propagation algorithm. During the training process, the internal parameters of the MLP were tuned simultaneously in a full factorial design. This design tested all combinations for each submodel, including the number of hidden layers, the number of neurons in the hidden layer, and the decay rate. The neuron numbers were tested from 2 to 30, and the decay was set to vary from 0.001 to 0.1.\n\nThe model files and specific configurations are not directly provided in a standalone format but are inherent in the R scripts and the libraries used. The R software itself is open-source and freely available under the GNU General Public License. The libraries used are also open-source and can be accessed through the Comprehensive R Archive Network (CRAN). This ensures that the methods and configurations are reproducible by other researchers.\n\nFor detailed information on the specific parameters and configurations, one would need to refer to the R scripts and the documentation of the libraries mentioned. The scripts and data used in our study can be made available upon request, adhering to the ethical guidelines and consent protocols approved by the Ethics and Welfare Committee of the Royal Veterinary College and the Royal Canin ethics committee.",
  "model/interpretability": "The model developed is based on artificial neural networks (ANNs), which are inherently complex and often considered black-box models. This means that the internal workings of the model are not easily interpretable, and the relationships between input variables and predictions are not straightforward to understand.\n\nThe model uses a multilayer feed-forward neural network, specifically a multilayer perceptron (MLP), which involves multiple layers of neurons. The relationships between the input variables (such as age, creatinine, urine specific gravity, etc.) and the output predictions are determined through a process of training and validation, where the model adjusts its internal parameters to minimize prediction errors. This process does not provide a clear, human-readable explanation of how the model arrives at its predictions.\n\nHowever, the model's performance and the importance of different input variables can be analyzed to some extent. For example, factor discriminant analysis (FDA) was used to select the most relevant variables for each submodel, ensuring that the best predictors for each time range were retained. This process helps in identifying which variables are most influential in the model's predictions.\n\nAdditionally, receiver operator curves (ROCs) were generated to evaluate the model's performance, and optimal parameter values were selected based on the area under the curve (AUC) of the ROCs. This provides a measure of the model's accuracy and helps in understanding its overall performance.\n\nWhile the model itself is not transparent, the process of variable selection and performance evaluation provides some insights into its behavior. The model's predictions are based on mathematical relationships derived from the training data, and while these relationships are not easily interpretable, they allow the model to make accurate predictions about the development of chronic kidney disease in cats.",
  "model/output": "The model developed is a classification model designed to predict the occurrence of chronic kidney disease (CKD) in cats. It uses artificial neural network (ANN) modeling, specifically a multilayer feed-forward neural network known as a multilayer perceptron (MLP), with a back-propagation algorithm. The model takes input data from a single visit, including various laboratory variables, and outputs a prediction of whether a cat will develop CKD within specific time frames (3, 6, 9, 12, or more than 12 months).\n\nThe model's output is a probability that indicates the likelihood of a cat developing CKD. This probability is then used to classify the cat as either positive or negative for CKD based on predefined decision thresholds. Two strategies were employed to set these thresholds: Strategy 1 aims to maximize both sensitivity and specificity, while Strategy 2 prioritizes high specificity and positive predictive value (PPV), potentially at the cost of sensitivity.\n\nThe model's performance is evaluated using metrics such as sensitivity, specificity, PPV, and negative predictive value (NPV). The accuracy of the model is approximately 90% for most submodels, with the exception of the submodel predicting CKD beyond 12 months, which has an accuracy closer to 80%. The model's outputs are used to identify cats at risk of developing CKD, aiding in early diagnosis and intervention.\n\nThe model was trained and validated using datasets from different sources, ensuring its robustness and generalizability. The final model was designed to be practical for veterinarians, using commonly measured laboratory variables and requiring only a single visit's data for predictions. This makes it a valuable tool for early detection and management of CKD in cats.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The modeling software used for this study is publicly available. All calculations were carried out using R software, specifically version 3.3.3. The R software is open-source and can be accessed via the provided link. Several libraries dedicated to modeling and machine learning were utilized, including MASS for discriminant analysis, ADE4 for multivariate analysis, CARET for data modeling, pROC for ROC curves, and NNET for artificial neural networks. These libraries are also available through the R package system. The source code for these libraries is open-source, allowing for reproducibility and further development by the scientific community. The specific versions of the libraries used in this study can be found in the respective links provided.",
  "evaluation/method": "The evaluation method for the model involved a multi-phase approach to ensure robustness and accuracy. Initially, the model was developed using the RVC1 dataset, where input variables were selected, and the model underwent training and validation. This phase focused on optimizing specificity and sensitivity.\n\nIn the second phase, the model's performance was tested using independent datasets from Banfield and RVC2. This step was crucial for assessing the model's generalizability and accuracy in predicting chronic kidney disease (CKD) status across different populations. The evaluation metrics included accuracy, sensitivity, specificity, positive predictive values (PPVs), and negative predictive values (NPVs).\n\nA second strategy was also tested, which optimized specificity and PPV but at the cost of decreasing sensitivity and NPV. This approach provided a trade-off analysis, showing how different optimization strategies affected the model's performance.\n\nThe third phase involved a new cycle of training and validation using all three datasets (RVC1, RVC2, and Banfield). This phase aimed to reinitialize and optimize the model, ensuring that it could handle a larger and more diverse set of data. The performance of the model in each phase was summarized in detailed tables, providing a comprehensive view of its accuracy, sensitivity, and specificity across different submodels.\n\nReceiver operator curves (ROCs) were generated to illustrate the model's accuracy, sensitivity, and specificity. These curves were plotted for various thresholds, and the area under the curve (AUC) was used as a measure of model accuracy. The final calculation applied the best submodels to all initial data based on one visit from each cat, and the process was repeated for different visits to report average values for each variable.\n\nOverall, the evaluation method was rigorous, involving multiple phases of training, validation, and optimization using diverse datasets. This approach ensured that the model was thoroughly tested and validated, providing reliable predictions of CKD status.",
  "evaluation/measure": "The performance of the model was evaluated using several key metrics to ensure a comprehensive assessment of its predictive capabilities. The primary metrics reported include accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). These metrics were chosen to provide a well-rounded evaluation of the model's performance across different phases of development.\n\nAccuracy measures the overall correctness of the model's predictions, indicating the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, also known as recall, assesses the model's ability to correctly identify positive cases, which is crucial for detecting chronic kidney disease (CKD) in cats. Specificity evaluates the model's ability to correctly identify negative cases, ensuring that healthy cats are not falsely diagnosed with CKD.\n\nThe positive predictive value (PPV) indicates the probability that a positive test result is a true positive, while the negative predictive value (NPV) indicates the probability that a negative test result is a true negative. These metrics are particularly important in the context of CKD prediction, as they help to understand the reliability of the model's predictions in real-world scenarios.\n\nThe reported metrics are representative of standard practices in the literature, providing a clear and comparable evaluation of the model's performance. The use of ROC curves further illustrates the trade-off between sensitivity and specificity, offering a visual representation of the model's diagnostic ability. Additionally, the model's performance was assessed using two different strategies: one optimizing both specificity and sensitivity, and another focusing on maximizing specificity and PPV at the cost of sensitivity and NPV. This dual approach ensures that the model can be tailored to different clinical needs and priorities.",
  "evaluation/comparison": "A comparison to other methods was performed, although not explicitly on benchmark datasets. The model's performance was contrasted with another artificial neural network (ANN) model for chronic kidney disease (CKD) prediction in cats, which had different input variables and a different predictive window. This other model relied on changes over time in plasma creatinine, blood urea nitrogen (BUN) concentrations, and urine specific gravity (USG), whereas our model was designed to make predictions based on a single visit.\n\nAdditionally, the model's performance was compared to traditional univariate and multivariate logistic regression models used in other studies. These models identified clinical variables associated with the subsequent development of azotemia, but our ANN model demonstrated higher specificity and sensitivity in certain phases, suggesting it may be more suitable for a screening test.\n\nThe comparison to simpler baselines was not explicitly mentioned. However, the model's performance was evaluated using standard metrics such as sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV), which are commonly used to assess the performance of diagnostic tests and can be considered a form of baseline comparison. The model's performance was also evaluated using the area under the curve (AUC) of the receiver operating characteristic (ROC) curves, which provides a single scalar value that summarizes the model's performance across all classification thresholds.",
  "evaluation/confidence": "The evaluation of the model's performance included the use of confidence intervals to assess the reliability of the results. Specifically, the receiver operator curve (ROC) curves for the submodels included dashed lines that delineate the 95% confidence intervals, which were obtained using bootstrapping. This statistical method helps to provide a range within which the true performance metrics are likely to fall, giving a measure of the uncertainty associated with the estimates.\n\nThe performance metrics, such as accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV), were calculated for different submodels and phases of the study. The results indicate that the model maintained high accuracy and specificity across various phases, with sensitivity and PPV varying depending on the strategy used for setting decision thresholds.\n\nThe statistical significance of the model's performance was not explicitly stated in terms of p-values or other significance tests comparing it to baselines or other methods. However, the use of confidence intervals and the consistent performance across different datasets and phases suggest that the model's results are robust and reliable. The model's ability to retain high accuracy when tested on independent datasets, such as the Banfield and RVC2 data in phase 2, further supports its generalizability and superiority over simpler approaches.",
  "evaluation/availability": "Not enough information is available."
}