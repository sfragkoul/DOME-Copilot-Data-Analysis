{
  "publication/title": "Machine learning models for non-invasive glucose measurement: towards diabetes management in smart healthcare.",
  "publication/authors": "Agrawal H, Jain P, Joshi AM",
  "publication/journal": "Health and technology",
  "publication/year": "2022",
  "publication/pmid": "35996737",
  "publication/pmcid": "PMC9386205",
  "publication/doi": "10.1007/s12553-022-00690-7",
  "publication/tags": "- Continuous glucose measurement\n- Classification\n- Regression\n- Smart healthcare\n- Machine learning\n- Non-invasive glucose measurement\n- Diabetes management\n- Predictive modeling\n- Healthcare technology\n- IoMT framework",
  "dataset/provenance": "In our study, we utilized two distinct datasets to evaluate our machine learning models for diabetes detection and glucose level prediction. The first dataset is the Pima Indian Diabetes Dataset (PIDD), which is an open-source dataset obtained from the UCI repository. This dataset comprises medical details of 768 female patients. It has been widely used in the community for similar research purposes, making it a reliable source for comparison with previous works.\n\nThe second dataset is the iGLU dataset, which was collected using a non-invasive iGLU device. This dataset consists of 99 instances and includes measurements from three fingers, with emitters and detectors placed underneath the surface of the pads. The design of the iGLU device ensures minimal interference from external factors like sweat, thereby reducing the probability of flawed measurements.\n\nBoth datasets underwent rigorous preprocessing steps, including data cleaning to handle missing values and feature scaling to optimize model performance. The PIDD dataset was auto-scaled, while a custom scaling function was developed for the iGLU dataset to ensure the best fit for our models. These preprocessing steps were crucial in preparing the data for effective implementation of various machine learning algorithms.",
  "dataset/splits": "The dataset was split into two parts: training and testing. The distribution used was 80:20, meaning 80% of the data was used for training the models, and the remaining 20% was used for testing. This split was chosen to achieve the best results and to minimize the risk of overfitting. Other distributions were found to be more prone to overfitting.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The datasets used in this study are both publicly available. The first dataset, known as the Pima Indian Diabetes Dataset (PIDD), was acquired from the UCI Machine Learning Repository. This dataset contains medical details of 768 female patients and is openly accessible for research purposes. The attributes and descriptions of this dataset are provided in the publication for clarity.\n\nThe second dataset, referred to as the iGLU dataset, was collected using a non-invasive iGLU device. This dataset includes measurements from three fingers, with the data accumulated by placing the fingers between emitters and detectors lined with pads. The design of the pads ensures that the measurements are not affected by external factors such as sweat, reducing the probability of flawed measurements. The attributes and descriptions of this dataset are also detailed in the publication.\n\nBoth datasets were used to evaluate the performance of various machine learning algorithms for diabetes detection and glucose level prediction. The PIDD dataset is widely used in the research community, and its availability is enforced through the terms of use provided by the UCI Machine Learning Repository. The iGLU dataset, while collected specifically for this study, follows similar principles of data sharing and is made available to the research community under appropriate licensing agreements.",
  "optimization/algorithm": "The optimization algorithm used in our study falls under the category of machine learning algorithms, specifically focusing on both regression and classification techniques. These algorithms are well-established in the field of machine learning and have been extensively used in various applications, including healthcare.\n\nThe algorithms employed in our research are not new; they are widely recognized and have been previously published in numerous machine learning and data science journals. The choice of these algorithms was driven by their proven effectiveness in handling similar datasets and their ability to provide reliable results for our specific use case, which involves the detection of diabetes and the prediction of glucose levels.\n\nThe decision to use these established algorithms was made to ensure the robustness and reliability of our model. By leveraging well-known machine learning techniques, we aimed to create a non-invasive, wearable, painless, precise, and low-cost device with high accuracy. The algorithms were selected based on their performance metrics, such as mean absolute error (MAE), root mean square error (RMSE), and accuracy score, which are crucial for evaluating the effectiveness of regression models. For classification tasks, additional metrics like recall, F1-measure, and ROC curves were used to compare the performance of different algorithms.\n\nThe algorithms used include Logistic Regression, Support Vector Machine (SVM) Linear, Random Forest, Neural Network, XGBoost, Gaussian Naive Bayes, and K-Nearest Neighbors (KNN). Each of these algorithms was applied to the datasets acquired from the UCI repository (PIDD) and the iGLU device, and their performance was evaluated and compared. The results demonstrated that algorithms like Random Forest and Logistic Regression achieved the highest AUC scores, indicating their superior performance in classifying the data.\n\nIn summary, the optimization algorithm class used in our study is that of machine learning algorithms, specifically regression and classification techniques. These algorithms are not new but are well-established and have been chosen for their proven effectiveness and reliability in handling similar datasets. The focus was on creating a robust and accurate model for diabetes detection and glucose level prediction, leveraging the strengths of these established machine learning techniques.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data preprocessing was a crucial step before applying machine learning algorithms. This process involved two main steps: data cleaning and feature scaling.\n\nFirst, we addressed missing values in the datasets. This step is essential to ensure that the algorithms can process the data accurately. For the Pima Indian Diabetes Dataset (PIDD), we used auto-scaling to normalize the data. This technique adjusts the values of the features to a standard scale, which helps in improving the performance of the machine learning models. For the iGLU dataset, we developed a custom scaling function tailored to fit the data into the models optimally.\n\nAfter cleaning and scaling, we selected the relevant features and labels for our models. This step involved identifying the attributes that are most significant for predicting diabetes and glucose levels. For PIDD, the attributes included number of times pregnant, plasma glucose concentration, diastolic blood pressure, skin fold thickness, serum insulin, BMI, diabetes pedigree function, and age. For the iGLU dataset, the attributes were voltages from three channels and the predicted value.\n\nThe preprocessing steps ensured that the data was in a suitable format for the machine learning algorithms. This preparation is vital for achieving accurate and reliable results in diabetes detection and glucose level prediction.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "In the optimization process, the number of input features (f) varies depending on the dataset used. For the Pima Indian Diabetes Dataset (PIDD), eight features are utilized as inputs. These features include the number of times pregnant, plasma glucose concentration, diastolic blood pressure, skin fold thickness, 2-hour serum insulin, BMI, diabetes pedigree function, and age. For the iGLU dataset, three features are used, which are the voltages from three different channels.\n\nFeature selection was performed as part of the data preprocessing steps. This process involved filling missing values, selecting relevant features, and scaling the data. For the PIDD, auto-scaling was applied, while for the iGLU dataset, a custom scaling function was developed to ensure the data fit the model optimally. The feature selection was conducted using the training set only, ensuring that the testing set remained unbiased and unaffected by the feature selection process. This approach helps in maintaining the integrity of the model evaluation and preventing data leakage.",
  "optimization/fitting": "In our study, we employed various algorithms to fit our models, and we carefully considered the balance between the number of parameters and the number of training points to avoid both overfitting and underfitting.\n\nFor some algorithms, such as Linear Regression, Polynomial Regression, Support Vector Regression, and Neural Networks, we observed that they did not perform well on our dataset, achieving accuracies of less than 30%. This poor performance can be attributed to the small size of our dataset, which makes these algorithms prone to overfitting. To mitigate this, we focused on simpler models like Decision Trees, Random Forests, and XGBoost, which provided better accuracy and were less likely to overfit.\n\nWe also ensured that our models were not underfitting by evaluating their performance on both training and testing datasets. For instance, the Decision Tree algorithm achieved a testing accuracy of 70.64%, which is the highest among all the algorithms we tested. This indicates that the model is complex enough to capture the underlying patterns in the data without being too simplistic.\n\nAdditionally, we used metrics such as Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) to further evaluate the performance of our regression models. The Decision Tree algorithm had the lowest MAE and RMSE, indicating that it provided the most accurate predictions.\n\nIn summary, we carefully selected our algorithms and evaluated their performance to ensure that we avoided both overfitting and underfitting. Our focus on simpler models and thorough evaluation metrics helped us achieve robust and reliable results.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key strategies involved splitting the dataset into training and testing sets in an 80:20 ratio. This distribution helped in reducing the chances of overfitting by ensuring that the model was trained on a sufficient amount of data while being validated on a separate, unseen dataset.\n\nAdditionally, we utilized cross-validation techniques to further validate the performance of our models. Cross-validation helps in assessing how the statistical analysis will generalize to an independent data set. This method involves partitioning the data into subsets, training the model on some subsets, and validating it on the remaining subsets. By repeating this process multiple times, we could obtain a more reliable estimate of the model's performance.\n\nWe also experimented with different algorithms and compared their performance metrics, such as accuracy, precision, recall, and AUC-ROC. This comparative analysis allowed us to identify the most effective models and avoid over-reliance on any single algorithm. For instance, Random Forest and Logistic Regression both demonstrated high AUC values of 0.87, indicating their effectiveness as classifiers.\n\nFurthermore, we calculated confusion metrics and ROC curves for various algorithms to evaluate their performance comprehensively. These metrics provided insights into the true positive, true negative, false positive, and false negative rates, helping us to fine-tune the models and prevent overfitting.\n\nIn summary, our approach to preventing overfitting involved careful data splitting, cross-validation, comparative analysis of multiple algorithms, and thorough evaluation using various performance metrics. These techniques ensured that our models were robust and generalizable to new, unseen data.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in this study encompass both transparent and black-box algorithms. Transparent models, such as Decision Trees and Logistic Regression, offer clear interpretability. Decision Trees, for instance, provide a visual representation of the decision-making process, where each node represents a feature, and branches represent the possible outcomes. This makes it straightforward to understand how the model arrives at its predictions. Logistic Regression, on the other hand, provides coefficients that indicate the strength and direction of the relationship between each feature and the target variable, allowing for easy interpretation of feature importance.\n\nIn contrast, black-box models like Neural Networks and Random Forests are more complex and less interpretable. These models involve multiple layers and nodes (in the case of Neural Networks) or ensembles of trees (in the case of Random Forests), making it challenging to trace the decision-making process. However, techniques such as feature importance scores and partial dependence plots can be used to gain some insights into these models.\n\nFor example, in the case of Random Forests, feature importance scores can be derived by measuring the average reduction in impurity (such as Gini impurity or entropy) brought by each feature across all trees in the forest. This provides a ranking of features based on their contribution to the model's predictions. Similarly, partial dependence plots can illustrate the relationship between a feature and the predicted outcome, marginalizing over the values of other features.\n\nOverall, while some models offer straightforward interpretability, others require additional techniques to uncover their inner workings. The choice of model depends on the trade-off between accuracy and interpretability, with transparent models being preferred when explainability is crucial.",
  "model/output": "The model encompasses both classification and regression tasks. Various algorithms, including Neural Network, XGBoost, Decision Tree, Random Forest, and Support Vector Machine, were applied as both classifiers and regressors. For classification, metrics such as Accuracy, Confusion Matrix, Precision, Recall, and ROC-AUC curve were used to evaluate performance. In regression tasks, the model's effectiveness was assessed using Accuracy, Mean Absolute Error (MAE), and Root Mean Square Error (RMSE).\n\nIn the classification context, the dataset was split into 80:20 for training and testing to achieve the best results. The Random Forest and Logistic Regression algorithms demonstrated the highest AUC of 0.87, indicating their superior performance as classifiers. The Decision Tree algorithm achieved the highest testing accuracy of 70.64% with the lowest mean absolute error and root mean square error among all algorithms.\n\nFor regression, the Gradient Boost algorithm showed the lowest relative absolute error, while the Decision Tree algorithm provided the best testing score of 70.64% with a significantly shorter training time compared to other algorithms. The Neural Network regressor, although taking the longest time to run, did not yield favorable results.\n\nOverall, the model's performance was evaluated using a comprehensive set of metrics, providing a clear indication of the best algorithms for both classification and regression tasks. The results highlight the importance of selecting appropriate algorithms based on the specific requirements of the task at hand.",
  "model/duration": "The execution time of the models varied significantly depending on the algorithm used. For classification tasks, all algorithms completed training in less than one second. Among these, the Decision Tree algorithm was notably fast, taking only 0.002 seconds to train. In contrast, the Neural Network algorithm required the most time, taking approximately 2.16 seconds for training. For regression tasks, the Neural Network algorithm again took the longest time, around 0.56 seconds, while the Polynomial Regression algorithm was the fastest, completing in just 0.001 seconds. The Random Forest algorithm, despite being one of the more accurate models, took around 0.16 seconds for training, which is relatively efficient given its performance. These variations in execution time are crucial for selecting the appropriate algorithm based on the specific requirements of the task, balancing accuracy and computational efficiency.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the proposed methods involved applying various machine learning algorithms to two datasets: the Pima Indian Diabetes Dataset (PIDD) and the iGLU dataset. For both datasets, the data was split into training and testing sets, with an 80-20 distribution to ensure robust evaluation.\n\nFor the PIDD dataset, several classification algorithms were applied, including Logistic Regression, Gaussian Naive Bayes, Support Vector Machine (Linear), Gradient Boost, Neural Network, K-Nearest Neighbors (KNN), Random Forest, and Decision Tree. The performance of these algorithms was evaluated using metrics such as Accuracy, ROC-AUC, Confusion Matrix, Precision, Recall, and F1-Score. The confusion matrix provided a summary of predictions, including True Positives, False Negatives, False Positives, and True Negatives. The ROC-AUC curve was plotted to measure the performance of the classifiers at various threshold settings, with a higher AUC indicating better performance.\n\nFor the iGLU dataset, regression algorithms were evaluated using metrics such as Accuracy, Mean Absolute Error (MAE), and Root Mean Square Error (RMSE). MAE measured the average absolute differences between actual and predicted values, while RMSE measured the square root of the average of squared errors, providing a quadratic scoring rule that reflects the closeness of predicted values to actual values.\n\nComparative analysis was conducted by forming tables and charts to compare the performance of different algorithms. For the PIDD dataset, Random Forest achieved the highest accuracy of 84%, while Logistic Regression performed effectively with 82% accuracy and faster training time. For the iGLU dataset, Decision Tree achieved the highest accuracy of 70.64%, with the lowest MAE and RMSE among all algorithms. Gradient Boost Regression also showed promising results with a low relative absolute error.\n\nAdditionally, the Clarke Error Grid analysis was conducted on the iGLU dataset to validate the feasibility of the proposed solution, with 100% of the values lying in the clinically accepted A-B zone. This analysis further supported the accuracy and reliability of the proposed machine learning models for non-invasive blood glucose measurement.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our models. For classification tasks, we reported Accuracy, Confusion Matrix, Precision, Recall, and the ROC-AUC curve. These metrics provide a thorough assessment of model performance. Accuracy measures the overall correctness of predictions, while the Confusion Matrix offers a detailed breakdown of true positives, true negatives, false positives, and false negatives. Precision indicates the proportion of positive identifications that were actually correct, and Recall (or Sensitivity) measures the proportion of actual positives that were correctly identified. The ROC-AUC curve evaluates the model's ability to distinguish between classes across various threshold settings, with a higher AUC indicating better performance.\n\nFor regression tasks, we utilized Accuracy, Mean Absolute Error (MAE), and Root Mean Square Error (RMSE). Accuracy in regression context measures the overall fit of the model to the data. MAE provides the average magnitude of errors in a set of predictions, without considering their direction, while RMSE gives the square root of the average of squared errors, providing a more significant penalty for larger errors.\n\nAdditionally, we calculated the F-1 Score, which combines Precision and Recall into a single metric, offering a balanced measure of a model's performance, especially useful when dealing with imbalanced datasets.\n\nThese metrics are widely recognized and used in the literature, ensuring that our evaluation is representative and comparable to other studies in the field. By reporting these metrics, we aim to provide a clear and comprehensive understanding of our models' performance, facilitating comparisons with other research and practical applications.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of various machine learning algorithms to assess their performance on different datasets. We applied multiple classification and regression algorithms, including Logistic Regression, Gaussian Naive Bayes, Support Vector Machine (Linear), Gradient Boost, Neural Network, K-Nearest Neighbors (KNN), Random Forest, and Decision Tree. This approach allowed us to benchmark our methods against publicly available algorithms and simpler baselines.\n\nFor the Pima Indian diabetes dataset, we distributed the data between 80-20% for training and testing. We evaluated the algorithms based on parameters such as accuracy, ROC-AUC, confusion metrics, precision, recall, and F1-score. The Random Forest algorithm demonstrated the highest accuracy at 84%, followed closely by Logistic Regression at 82%. These results indicate that Random Forest and Logistic Regression are robust classifiers for this dataset.\n\nFor the iGLU dataset, we also used an 80-20 split for training and testing to minimize overfitting. The Decision Tree algorithm achieved the highest accuracy of 70.64%, with the lowest mean absolute error (MAE) of 7.89% and root mean square error (RMSE) of 8.56%. Gradient Boost regression showed the lowest relative absolute error at 6.82%. These findings highlight the effectiveness of Decision Tree and Gradient Boost regression for this particular dataset.\n\nIn addition to accuracy, we considered other performance metrics such as MAE, RMSE, and training time. For instance, while Neural Network regressors took the longest time to run, Decision Tree provided the best testing score of 70.64% with significantly less training time. This comparison underscores the importance of evaluating multiple metrics to select the best algorithm for a given task.\n\nOverall, our evaluation involved a thorough comparison of different algorithms, including simpler baselines, to ensure that our methods are robust and reliable. This approach provides a clear indication of the best algorithms for detecting diabetes and predicting glucose values, contributing to the advancement of healthcare technologies.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}