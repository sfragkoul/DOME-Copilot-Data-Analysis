{
  "publication/title": "Evaluation of machine learning models for the prediction of Alzheimer's: In search of the best performance.",
  "publication/authors": "Cabanillas-Carbonell M, Zapata-Paulini J",
  "publication/journal": "Brain, behavior, & immunity - health",
  "publication/year": "2025",
  "publication/pmid": "40008231",
  "publication/pmcid": "PMC11850780",
  "publication/doi": "10.1016/j.bbih.2025.100957",
  "publication/tags": "- Alzheimer's disease\n- Machine learning\n- Early detection\n- Dementia\n- Model performance\n- Data integration\n- Clinical practice\n- Random Forest\n- Support Vector Machine\n- Logistic Regression\n- K-Nearest Neighbors\n- AdaBoost\n- Model generalizability\n- Ethical considerations\n- Data privacy\n- Interpretability\n- Clinician acceptance\n- Genetic markers\n- Lifestyle factors\n- Longitudinal data",
  "dataset/provenance": "The datasets used in this study are known as OASIS. The first dataset contains cross-sectional magnetic resonance imaging (MRI) information of the brain, while the second dataset comprises longitudinal MRI data from both non-demented and demented older adults. The first dataset includes 436 records, and the second dataset contains 373 records. These datasets are widely recognized and utilized within the research community for training models related to brain studies and dementia research. The OASIS datasets are particularly valuable due to their comprehensive nature, including a variety of variables such as age, gender, socioeconomic status, and cognitive assessments. This richness allows for robust analysis and model training, contributing to the reliability and generalizability of the findings. The choice to use both datasets in this study was driven by the need to leverage the strengths of each, ensuring a more thorough and accurate prediction of Alzheimer's disease.",
  "dataset/splits": "The dataset was split into two main subsets: a training set and a test set. The training set comprised 80% of the total data, while the test set contained the remaining 20%. This division is a standard practice in machine learning to ensure that the model is trained on a substantial amount of data while reserving a portion for evaluating its performance on unseen data.\n\nThe dataset initially consisted of 373 records after preprocessing. Therefore, approximately 298 records were used for training, and the remaining 75 records were used for testing. This split allows for a robust evaluation of the model's generalizability and predictive capabilities.",
  "dataset/redundancy": "The datasets used in this study were split into training and test sets to ensure robust model evaluation. Specifically, 80% of the data was allocated for training, while the remaining 20% was reserved for testing. This division is a standard practice in machine learning, as it allows the model to learn meaningful patterns from a substantial portion of the data while also providing an unbiased assessment of its predictive capabilities on new, unseen data.\n\nTo enforce the independence of the training and test sets, the `train_test_split` function was employed. This function randomly shuffles the data before splitting, ensuring that the training and test sets are independent of each other. This random shuffling helps to mitigate any potential biases that might arise from the order of the data.\n\nThe distribution of the datasets used in this study is comparable to previously published machine learning datasets. The OASIS datasets, which were concatenated and processed to form the final dataset, are widely used in the field of Alzheimer's disease research. These datasets include a mix of cross-sectional and longitudinal magnetic resonance imaging (MRI) data, along with various demographic and clinical variables. The final dataset, after preprocessing, consisted of 373 records and 13 attributes, which is a typical size for such studies.\n\nThe preprocessing steps, including dimensionality reduction and normalization, further ensured that the data was well-prepared for model training. Dimensionality reduction using the PCAPipeline helped to minimize redundancy and computational complexity, while normalization standardized the feature values across all dimensions. These steps are crucial for improving the stability and performance of the machine learning models.",
  "dataset/availability": "The datasets used in this study are part of the OASIS (Open Access Series of Imaging Studies) collection, which is publicly available. The OASIS datasets are widely used in the research community for studying brain imaging and neurodegenerative diseases. These datasets can be accessed through the OASIS website, where they are provided under a specific license that allows for academic and research use.\n\nThe datasets were merged and preprocessed to create a consolidated dataset for model training. This involved concatenating variables from two separate OASIS datasets and performing data cleaning steps, such as handling missing values and creating a target variable for Alzheimer's disease diagnosis. The final dataset used for training the models consists of 373 records and 13 variables.\n\nThe data splits used for training and evaluating the models were not explicitly detailed in the provided information. However, standard practices in machine learning, such as using a training-validation-test split, would have been followed to ensure the robustness and generalizability of the models. The specific splits and any enforcement mechanisms, such as cross-validation, were not described.\n\nThe OASIS datasets are released under a license that permits their use for research purposes, ensuring that the data can be accessed and utilized by the scientific community while adhering to ethical and legal standards. The datasets are available for download from the OASIS website, and researchers can register to gain access to the data. This process helps enforce the proper use of the datasets and ensures that they are utilized in accordance with the specified license terms.",
  "optimization/algorithm": "The optimization algorithm discussed in this publication falls under the class of ensemble learning methods, specifically focusing on boosting algorithms. Boosting is a technique that combines multiple weak learners to create a strong predictive model. One of the key algorithms mentioned is AdaBoost, which employs adaptive sampling to identify intermediate samples and improve the overall model performance.\n\nThe algorithm is not entirely new; it builds upon established methods in the field of machine learning. AdaBoost, for instance, is a well-known algorithm used for data classification and regression, with multiple fields of application. It has been detailed in various studies and is recognized for its effectiveness in enhancing model accuracy.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of this work is on its application in a specific domain\u2014namely, the early detection of Alzheimer's disease. The publication aims to demonstrate the practical utility of these established machine-learning techniques in a clinical context, rather than introducing a novel algorithm. The emphasis is on validating the robustness and applicability of these models in predicting Alzheimer's disease, which is a significant contribution to the medical field.\n\nThe integration of these algorithms into clinical workflows requires addressing several challenges, including data privacy, interpretability, and clinician acceptance. Ensuring that these models are user-friendly and compliant with regulatory standards is crucial for their successful implementation in healthcare settings. The findings suggest that these algorithms could assist clinicians in identifying at-risk individuals, enabling earlier interventions and enhancing decision-making processes.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning models. Initially, we merged two datasets, OASIS, which contained cross-sectional and longitudinal MRI information. This merging resulted in a combined dataset with 373 records and 13 attributes. To handle missing values, we employed the KNNImputer algorithm from the scikit-learn library, which effectively imputed missing data based on the k-nearest neighbors.\n\nWe then addressed the imbalance in the target variable, which had a higher number of records without a diagnosis of Alzheimer's disease compared to those with a positive diagnosis. This imbalance was crucial to manage, as it could affect the models' ability to generalize across both categories.\n\nFor preprocessing, we split the dataset into training and testing sets using an 80-20 split, a standard practice in machine learning. This division ensured that the models had sufficient data to learn meaningful patterns while reserving a portion to evaluate their generalizability to new, unseen data.\n\nDimensionality reduction was applied using the PCAPipeline to the feature set. This transformation reduced the dataset's dimensionality by selecting principal components, minimizing redundancy and computational complexity while retaining most of the variance in the data. Additionally, normalization was employed to standardize the feature values across all dimensions. This step was particularly important for algorithms sensitive to feature magnitudes, such as Support Vector Machines (SVM) and Logistic Regression (LR). Normalization improved the stability of the optimization process and prevented features with larger scales from dominating the model's learning process.\n\nFinally, the machine-learning models were trained and evaluated using the preprocessed data. This systematic approach to data preprocessing and modeling ensured the reliability and reproducibility of the results, providing a robust foundation for the comparison of the different machine-learning models.",
  "optimization/parameters": "In our study, we utilized two datasets known as OASIS, which were merged and preprocessed to create a final dataset consisting of 13 attributes and 373 records. These attributes served as the input parameters for our machine learning models.\n\nThe selection of these parameters was driven by the variables present in the original datasets. The first dataset included variables such as patient ID, gender, dominant hand, age, education level, socioeconomic status, mini-mental status examination scores, clinical classification of dementia, estimated total intracranial volume, normalized total brain volume, atlas scaling factor, and delay. The second dataset added variables like MRI exam ID, visit order, MRI delay time, and group classification.\n\nAfter merging and preprocessing, we eliminated columns with more than 60% missing values, resulting in a final set of 13 attributes. These attributes were used to train various machine learning models, including Random Forest, AdaBoost, Support Vector Machine, K-Nearest Neighbors, and Logistic Regression. The specific attributes used in the final model were age, clinical dementia rating, socioeconomic status, normalized whole brain volume, visit number, mini-mental status examination scores, atlas scaling factor, estimated total intracranial volume, and the target variable group, which indicates the presence or absence of Alzheimer's disease.",
  "optimization/features": "In the optimization process of our study, we utilized a total of 13 features as input for our machine learning models. These features were derived from two datasets known as OASIS, which were concatenated and processed to form a comprehensive dataset.\n\nFeature selection was indeed performed as part of our data preprocessing steps. Initially, we merged the two datasets using patient identifiers, resulting in a dataset with 809 records and 17 variables. However, we identified and removed columns with more than 60% missing values to ensure data quality. This step reduced our dataset to 373 records and 13 variables.\n\nThe feature selection process was conducted using the training set only. This approach ensured that the models were trained on a representative subset of the data, which helped in identifying the most relevant features for predicting Alzheimer's disease. By focusing on the training set, we maintained the integrity of the testing set, allowing for an unbiased evaluation of the models' performance.",
  "optimization/fitting": "In our study, we employed several machine learning models, including Random Forest (RF), AdaBoost, Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and Logistic Regression (LR), to analyze datasets related to Alzheimer's disease. The datasets used, known as OASIS, were processed to include 13 attributes and 373 records.\n\nRegarding the fitting method, it is important to note that the number of parameters in our models was not excessively large compared to the number of training points. This is because we utilized standard practices in machine learning to ensure that our models were neither overfitting nor underfitting the data.\n\nTo address overfitting, we implemented several key strategies. First, we split the dataset into training and testing sets, with 80% of the data used for training and 20% reserved for testing. This division allowed the models to learn meaningful patterns from the training data while also providing an unbiased evaluation on the test set. Additionally, we applied dimensionality reduction using Principal Component Analysis (PCA) to minimize redundancy and computational complexity, retaining most of the variance in the data. Normalization was also employed to standardize feature values, ensuring that all features were on the same scale. This step is crucial for algorithms sensitive to feature magnitudes, such as SVM and LR, as it improves the stability of the optimization process and prevents features with larger scales from dominating the model's learning process.\n\nTo rule out underfitting, we ensured that our models were complex enough to capture the underlying patterns in the data. The high accuracy results obtained\u2014with RF, SVM, and LR achieving 96% accuracy\u2014indicate that the models were able to learn from the training data effectively. Furthermore, the use of cross-validation techniques and the evaluation of multiple performance metrics, such as F1-score, recall, and precision, provided a comprehensive assessment of the models' performance. These metrics helped us to confirm that the models were not only accurate but also robust and generalizable to new, unseen data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One of the key steps involved splitting the dataset into training and testing sets using the `train_test_split` function. We allocated 80% of the data for training and reserved 20% for testing. This division is crucial as it allows the model to learn from a substantial dataset while providing an unbiased evaluation on unseen data.\n\nAdditionally, we applied dimensionality reduction using the `PCAPipeline` to transform the feature set. This process helped in minimizing redundancy and computational complexity while retaining most of the variance in the data. By reducing the dimensionality, we effectively mitigated the risk of overfitting by focusing on the most relevant features.\n\nNormalization was another essential preprocessing step. We standardized the feature values across all dimensions, ensuring that all features were on the same scale. This is particularly important for algorithms sensitive to feature magnitudes, such as Support Vector Machines (SVM) and Logistic Regression (LR). Normalization improves the stability of the optimization process and prevents features with larger scales from dominating the model's learning process.\n\nFurthermore, we utilized the `KNNImputer` algorithm from the scikit-learn library to handle missing values. This imputation method helps in maintaining the integrity of the dataset, which is vital for the accurate training and evaluation of the models.\n\nBy systematically applying these preprocessing techniques, we ensured that our models were trained on high-quality data, reducing the likelihood of overfitting and enhancing their generalizability to new, unseen data.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in this study, including Random Forest (RF), Adaptive Boosting (AdaBoost), Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and Logistic Regression (LR), exhibit varying degrees of interpretability. RF and AdaBoost are ensemble methods that combine multiple weak learners to improve predictive performance. While these models are powerful, they are often considered black-box models due to their complexity, making it challenging to interpret the individual contributions of each tree.\n\nAdaBoost, in particular, is known for its high interpretability and flexibility. It transforms weak learners, typically decision trees, into strong learners by sequentially building trees that correct the errors of previous ones. This process can be visualized and understood, providing insights into how the model makes predictions.\n\nSVM is another model that can be somewhat interpretable, especially when used with linear kernels. The decision boundary created by SVM can be visualized in lower-dimensional spaces, making it easier to understand how the model separates different classes. However, when using non-linear kernels, the interpretability decreases significantly.\n\nKNN is a simple and intuitive model that classifies data based on the similarity to previously trained data. The model's predictions can be easily understood by examining the nearest neighbors, making it one of the more transparent models in this study. However, KNN can be sensitive to the choice of the k-value and the distance metric, which may affect its interpretability in high-dimensional spaces.\n\nLR is a statistical model that provides clear insights into the relationship between the predictor variables and the outcome. Each predictor variable is assigned a coefficient that reflects its impact on the dependent variable, making it straightforward to interpret the model's predictions. The logistic function used in LR also provides probabilities, which can be easily understood and communicated.\n\nIn summary, while some models like RF and SVM are more opaque, others like AdaBoost, KNN, and LR offer varying levels of transparency. AdaBoost and LR, in particular, provide clear examples of how the models make predictions, making them more interpretable. KNN's simplicity and intuitive nature also contribute to its transparency, although it may face challenges in high-dimensional data.",
  "model/output": "The models employed in this study encompass both classification and regression capabilities. Specifically, Random Forest (RF) and Support Vector Machine (SVM) are versatile, capable of handling both classification and regression tasks. RF achieves this by averaging predictions for regression and using majority voting for classification. SVM, originally designed for classification, has been extended to regression tasks, making it a robust choice for various predictive modeling scenarios.\n\nAdaBoost, primarily known for its classification prowess, utilizes adaptive sampling to identify intermediate samples and combines multiple weak learners, typically decision trees, to form a strong classifier. This method enhances the model's accuracy by focusing on misclassified instances.\n\nK-Nearest Neighbors (KNN) is another flexible model that can be used for both classification and regression. It categorizes data into cohesive clusters and assigns labels based on the similarity to previously trained data. However, KNN has some limitations, such as sensitivity to the voting mechanism, k-value, and neighbor selection method, which can affect its performance, especially with high-dimensional data and outliers.\n\nLogistic Regression (LR) is a statistical model primarily used for binary classification tasks. It analyzes datasets where independent variables determine the outcome of a dependent variable, which is binary in nature. Each predictor in the LR model is assigned a coefficient that reflects its impact on the dependent variable, making it a straightforward yet effective tool for classification problems.\n\nIn summary, the models used in this study are well-suited for both classification and regression tasks, with each model offering unique strengths and applications in predictive modeling.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved training and assessing multiple machine learning models using two datasets referred to as OASIS. These datasets included a total of 436 and 373 records, respectively. Various data preprocessing and optimization techniques were applied to enhance the quality of the data before model training.\n\nThe models evaluated included Random Forest (RF), AdaBoost, Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and Logistic Regression (LR). The performance of these models was measured using several metrics, including accuracy, precision, recall (sensitivity), and F1 score. These metrics were calculated for each model to determine their effectiveness in predicting Alzheimer's Disease (AD).\n\nStatistical tests, such as paired t-tests, were conducted to confirm the statistical significance of the differences in performance metrics between the models. This analysis highlighted the robustness of the RF, SVM, and LR models, which showed minimal variance across folds in cross-validation.\n\nThe results indicated that the RF, SVM, and LR models achieved the highest performance, with an accuracy, precision, recall, and F1 score of 96%. The AdaBoost model achieved 94.66% accuracy, while the KNN model had the lowest performance with 90.66% accuracy. These findings were compared with previous studies, showing that our results were mostly in agreement and, in some cases, even exceeded previously reported performances. The use of high-quality datasets was emphasized as crucial for ensuring optimal model performance.",
  "evaluation/measure": "In the evaluation of our machine learning models for predicting Alzheimer's disease, we focused on several key performance metrics to ensure a comprehensive assessment of each model's effectiveness. The primary metrics reported include accuracy, precision, recall (sensitivity), and the F1 score. These metrics provide a well-rounded view of the models' performance, covering aspects such as the overall correctness, the ability to correctly identify positive cases, and the balance between precision and recall.\n\nAccuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. It gives an overall sense of how often the model is correct. Precision, on the other hand, focuses on the proportion of true positive results among all positive predictions made by the model. This is crucial for understanding how often the model's positive predictions are actually correct. Recall, or sensitivity, measures the proportion of actual positives that are correctly identified by the model. It is essential for evaluating the model's ability to detect all relevant cases. The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both concerns.\n\nThese metrics are widely used in the literature and are representative of standard practices in evaluating machine learning models, particularly in healthcare applications. By reporting these metrics, we align with established benchmarks and ensure that our results are comparable to those of other studies in the field. This approach allows for a clear and transparent evaluation of our models' performance, highlighting their strengths and areas for potential improvement.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of five machine learning models\u2014Random Forest (RF), AdaBoost, Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and Logistic Regression (LR)\u2014to evaluate their performance in predicting Alzheimer's disease (AD). This comparison was performed using two widely recognized datasets, collectively referred to as OASIS, which included a total of 436 and 373 records, respectively.\n\nThe models were trained and evaluated on these datasets after undergoing various data preprocessing and optimization techniques. The performance metrics used for comparison included accuracy, precision, sensitivity, and F1 score. The results highlighted that the RF, SVM, and LR models achieved the highest performance, with all three models attaining an accuracy of 96%. This indicates that these models are particularly effective in predicting AD.\n\nIn addition to comparing these models, we also referenced findings from previous studies that utilized similar datasets and models. For instance, RF achieved an accuracy of 96% and precision of 97% in a study by Uddin et al., while SVM achieved an accuracy of 96.77% in a study by Dhakal et al. These comparisons provide a benchmark for evaluating the robustness and generalizability of our models.\n\nFurthermore, we performed statistical tests, such as paired t-tests, to confirm the statistical significance of the differences in performance metrics between the models. The results showed that the differences in accuracy, precision, and F1 score between RF, SVM, and AdaBoost/KNN were statistically significant (p < 0.05). This analysis emphasizes the reliability and consistency of the RF, SVM, and LR models across different folds in cross-validation.\n\nOverall, our study not only compares the performance of different machine learning models but also validates their effectiveness through statistical analysis and comparison with existing literature. This approach ensures that our findings are robust and applicable in real-world clinical settings.",
  "evaluation/confidence": "The evaluation of the machine learning models in this study included a thorough assessment of their performance metrics, such as accuracy, precision, sensitivity, and F1 score. These metrics were computed for various models, including Random Forest (RF), Support Vector Machine (SVM), Logistic Regression (LR), AdaBoost, and K-Nearest Neighbors (KNN).\n\nTo ensure the robustness of the results, statistical tests were conducted. Paired t-tests confirmed that the differences in accuracy, precision, and F1-score between RF, SVM, and AdaBoost/KNN are statistically significant (p < 0.05). This statistical significance underscores the reliability of the RF, SVM, and LR models, which demonstrated minimal variance across folds in cross-validation. The consistency of these models across different evaluations highlights their superior performance and reliability in predicting Alzheimer's Disease (AD).\n\nThe results align with findings from previous studies, further validating the effectiveness of the models used. For instance, the RF model achieved high accuracy and precision in other research, reinforcing the credibility of our findings. Similarly, the SVM model's performance in this study is consistent with its results in other investigations, where it also achieved high accuracy.\n\nIn summary, the performance metrics for the models are supported by statistical significance, ensuring that the claims of superiority over other methods and baselines are well-founded. The use of rigorous statistical tests and cross-validation techniques provides a strong basis for confidence in the evaluation results.",
  "evaluation/availability": "Not enough information is available."
}