{
  "publication/title": "Karst vegetation coverage detection using UAV multispectral vegetation indices and machine learning algorithm.",
  "publication/authors": "Pan W, Wang X, Sun Y, Wang J, Li Y, Li S",
  "publication/journal": "Plant methods",
  "publication/year": "2023",
  "publication/pmid": "36691062",
  "publication/pmcid": "PMC9869541",
  "publication/doi": "10.1186/s13007-023-00982-7",
  "publication/tags": "- Plant Methods\n- Vegetation indices\n- Machine learning\n- UAV imaging\n- Karst vegetation\n- Model accuracy\n- Remote sensing\n- Data collection\n- Environmental monitoring\n- Deep learning",
  "dataset/provenance": "The dataset used in this study was collected using a commercial DJI Phantom 4 multispectral UAV equipped with one Red-Green-Blue (RGB) sensor and five multispectral sensors. The UAV was used to capture images of a study area with a vegetation cover mainly consisting of cypress, miscellaneous shrubs, and Miscanthus. The images were captured at different altitudes\u2014100 m, 200 m, and 400 m\u2014on a clear and windless day to ensure consistent environmental conditions. The flight altitudes were chosen to cover different surface areas, with the higher altitudes covering larger areas. The total flight mission took about two hours to complete.\n\nThe UAV's real-time kinematics (RTK) positioning system, based on satellite navigation, reduced the error of satellite-based position data to the centimeter level. This system, along with the camera's ability to record illuminance information and adjust radiometric reflectance calibration, ensured high-quality data collection. The raw images were georeferenced using the RTK system, and Pix4D mapper software was used to stitch the images into complete, georeferenced spectral reflectance and Vegetation Index (VI) mosaics. Aerial triangulation and bundle adjustment were performed to determine the individual orientations of each image, considering lens distortion and multiple ground control points (GCPs).\n\nThe dataset includes high-resolution GeoTIFF images of reflectance and VIs, generated at the same location but at different flight altitudes. This comprehensive dataset was used to train and validate various machine learning models, including Random Forest (RF), Support Vector Machine (SVM), Gradient Boosting Machine (GBM), and Deep Learning (DL) models. The models were established on test data with flight altitudes of 100 m, 200 m, and 400 m, respectively. The dataset's diversity in flight altitudes and high-quality imagery ensured robust model training and validation.",
  "dataset/splits": "The dataset was divided into two main splits: the training set and the validation set. The training set comprises 80% of the total data, while the validation set contains the remaining 20%. These splits were used to develop and evaluate the models, ensuring that the models were trained on a substantial portion of the data while also having a separate set to validate their performance.",
  "dataset/redundancy": "The datasets used in this study were divided into a training set and a validation set. Specifically, 80% of the data was allocated to the training set, while the remaining 20% was reserved for the validation set. This split ensures that the training and test sets are independent, which is crucial for evaluating the performance of the models.\n\nTo enforce the independence of the training and test sets, standard practices in machine learning were followed. The data was randomly shuffled before splitting to ensure that the distribution of classes and features was similar in both sets. This random shuffling helps to mitigate any potential biases that might arise from the order of the data.\n\nThe distribution of the datasets in this study is comparable to previously published machine learning datasets. The 80-20 split is a common practice in the field, as it provides a sufficient amount of data for training the models while also reserving a meaningful portion for validation. This approach allows for a robust evaluation of the models' performance and generalizability.\n\nIn summary, the datasets were carefully split to ensure independence between the training and test sets, following established practices in machine learning. This split is consistent with the standards seen in other published works, ensuring the reliability and validity of the results.",
  "dataset/availability": "The data splits used in our study, specifically the training set and validation set, were not released in a public forum. The training set constituted 80% of the data, while the validation set made up the remaining 20%. The data was divided in this manner to ensure a robust evaluation of our models. However, the specific datasets and their splits were not made publicly available due to the proprietary nature of the data and the need to maintain the integrity of the research process. The models were constructed and evaluated using this internal data split, and the results were reported accordingly. The enforcement of this data split was managed internally within the research team to ensure consistency and accuracy in the model training and validation processes.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are ensemble classifiers and deep learning architectures. Specifically, the algorithms employed include Random Forest (RF), Support Vector Machine (SVM), Gradient Boosting Machine (GBM), and Deep Learning (DL). These algorithms are well-established in the field of machine learning and have been widely applied in various domains, including remote sensing and spectral analysis.\n\nThe algorithms used are not new; they have been extensively studied and utilized in the scientific community. Random Forest, for instance, is known for its ability to handle high-dimensional data and its robustness against overfitting. Support Vector Machines are renowned for their effectiveness in classification tasks, especially in high-dimensional spaces. Gradient Boosting Machines are popular for their ability to build predictive models by combining weak learners, and Deep Learning architectures are celebrated for their capacity to learn complex features from data through multiple layers of nonlinearity.\n\nThe choice of these algorithms was driven by their proven track records in similar applications. The study focuses on their application in detecting karst vegetation coverage using UAV multispectral imagery. The algorithms were implemented using established packages in R software, such as randomForest for RF, caret for SVM, and h2o for GBM and DL. The decision to use these specific algorithms was based on their demonstrated effectiveness in handling the types of data and problems encountered in this research.\n\nThe study's primary contribution lies in the application of these machine-learning algorithms to the specific problem of karst vegetation detection rather than the development of new algorithms. The results highlight the potential of these methods in improving the accuracy and efficiency of vegetation coverage detection, which is crucial for environmental monitoring and management.",
  "optimization/meta": "In our study, we did not employ a meta-predictor approach. Instead, we focused on developing and evaluating individual machine learning models, specifically Random Forest (RF), Support Vector Machine (SVM), Gradient Boosting Machine (GBM), and Deep Learning (DL). Each of these models was trained and tested independently using data from different flight altitudes (100 m, 200 m, and 400 m).\n\nThe data used for training and validation was divided into an 80% training set and a 20% validation set. This division ensured that the training data was independent for each model. The models were constructed using specific packages in R software: the randomForest package for RF, the caret package for SVM, and the h2o package for GBM and DL.\n\nTo determine the best model, we evaluated the performance of each model across all flight altitudes. The best models from each flight altitude data were used to validate the prediction accuracy of the remaining data. This process involved testing the models on both the test set and the real set (full original images at the respective flight altitudes). The model with the highest overall accuracy and best performance in detecting karst vegetation was identified as the best model.\n\nIn summary, our approach did not involve using data from other machine-learning algorithms as input for a meta-predictor. Each model was developed and evaluated independently, with clear separation of training and validation data to ensure robustness and reliability.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for ensuring the effectiveness of the machine-learning algorithms employed. The dataset was divided into a training set and a validation set, with 80% allocated for training and the remaining 20% for validation. This split ensured that the models were trained on a substantial amount of data while also having a sufficient validation set to assess their performance accurately.\n\nThe data used for modeling included flight altitudes of 100 meters, 200 meters, and 400 meters. Various machine-learning models, including Random Forest (RF), Support Vector Machine (SVM), Gradient Boosting Machine (GBM), and Deep Learning (DL), were established on this test data. The modeling process was conducted using R software, with specific packages utilized for each model. The randomForest package was used for RF modeling, the caret package for SVM modeling, and the h2o package for both GBM and DL modeling.\n\nPreprocessing steps involved selecting significant variables to develop predictive models. This step was essential for reducing the burden of data collection and improving predictive efficiency. The h2o package in R software was employed to perform significant variable selection on the model with the highest overall accuracy, thereby determining the best predictors for the model.\n\nAdditionally, spectral vegetation indices were selected as potential methods for predicting photosynthetic pigment content. These indices, derived from reflectance spectra, were used to estimate the physiology of individual trees or populations at the stand level. Various vegetation indices, such as NDVI, OSAVI, GNDVI, and others, were tested to identify the most effective ones for our study.\n\nIn summary, the data encoding and preprocessing involved dividing the dataset, selecting significant variables, and utilizing specific software packages for modeling. These steps were essential for ensuring the accuracy and efficiency of the machine-learning algorithms used in our research.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the specific model and the flight altitude data. We employed several machine learning models, including Random Forest (RF), Support Vector Machine (SVM), Gradient Boosting Machine (GBM), and Deep Learning (DL). Each of these models has its own set of parameters that were optimized during the training process.\n\nTo determine the best predictors for the model, we utilized variable selection techniques. This process is crucial because many datasets contain hundreds or thousands of possible predictors. By selecting the most important variables, we not only reduce the burden of data collection but also improve the predictive efficiency of the models. We performed significant variable selection using the h2o package in R software on the model with the highest overall accuracy. This helped us identify the best predictors for each model.\n\nThe selection of parameters was guided by statistical characteristics such as importance and accuracy. For instance, the variable importance measure provided by the Random Forest model was widely used to select the best variables for classifying specific target classes. Similarly, the Gradient Boosting Machine algorithm was used to train weak classifiers based on the negative gradient information of the loss function, ensuring that the most relevant features were included in the model.\n\nIn summary, the number of parameters used in the model was dynamically determined through variable selection techniques, ensuring that only the most relevant predictors were included. This approach enhanced the model's performance and accuracy across different flight altitudes.",
  "optimization/features": "In our study, we utilized a comprehensive set of spectral vegetation indices as input features for our models. These indices were derived from multispectral imagery captured by unmanned aerial vehicles (UAVs) at various flight altitudes. The specific indices included in our analysis are detailed in Table 3 of our publication, encompassing a range of spectral bands such as near-infrared (NIR), red edge (E), red (R), green (G), and blue (B).\n\nFeature selection was indeed performed to enhance the predictive efficiency of our models. This process is crucial, especially when dealing with datasets that contain hundreds or thousands of potential predictors. By identifying the most important variables, we aimed to reduce the burden of data collection and improve the overall performance of our models.\n\nThe feature selection was conducted using the h2o package in R software. This package was employed to perform significant variable selection on the model that exhibited the highest overall accuracy. The goal was to determine the best predictors for the model, thereby ensuring that only the most relevant features were used in the final analysis.\n\nTo maintain the integrity of our model validation process, the feature selection was performed exclusively on the training set. This approach helps to prevent data leakage and ensures that the selected features are truly indicative of the model's performance on unseen data. By adhering to this practice, we aimed to achieve a robust and reliable model that can generalize well to new datasets.",
  "optimization/fitting": "In our study, we employed several machine learning models, including Random Forest (RF), Support Vector Machine (SVM), Gradient Boosting Machine (GBM), and Deep Learning (DL). Each of these models has its own strengths and mechanisms for handling overfitting and underfitting.\n\nFor the Random Forest model, we utilized the randomForest package in R. This ensemble method generates multiple decision trees using randomly selected training samples and subsets of variables. This approach inherently reduces overfitting by averaging the results of multiple trees, making it robust to high-dimensional data and insensitive to overfitting. The variable importance measure provided by RF was also crucial in selecting the best predictors, ensuring that the model was not underfitting by missing significant variables.\n\nThe Support Vector Machine (SVM) model, implemented using the caret package in R, is known for its ability to handle large samples and high-dimensional spaces. SVM reduces the complexity of the approximation function while ensuring accuracy, which helps in avoiding overfitting. The model's performance was evaluated using a confusion matrix, which provided insights into the prediction accuracy and classification error, helping to rule out underfitting by ensuring that the model captured the essential patterns in the data.\n\nThe Gradient Boosting Machine (GBM) algorithm, used with the h2o package in R, builds models sequentially, focusing on the errors made by previous models. This iterative process helps in reducing both overfitting and underfitting. The GBM model was trained according to the negative gradient information of the loss function, ensuring that each new weak classifier improved the overall model performance. The model's accuracy was verified using a confusion matrix, which helped in assessing the model's predictive ability and ensuring that it was neither overfitting nor underfitting.\n\nDeep Learning (DL) models, also implemented using the h2o package in R, are characterized by multiple layers of nonlinearity. This architecture allows the model to learn deep features of the input data hierarchically, making it sensitive to minute details and insensitive to irrelevant changes. The DL models were first initialized through unsupervised training and then adjusted in a supervised manner, ensuring that high-level features were learned from low-level features. This process helped in avoiding underfitting by capturing complex patterns in the data. The model's performance was evaluated using metrics such as overall accuracy, recall, and F1 score, which provided a comprehensive assessment of the model's predictive ability and helped in ruling out overfitting.\n\nIn summary, our study employed various techniques to address overfitting and underfitting in different machine learning models. The use of ensemble methods, iterative training processes, and comprehensive evaluation metrics ensured that our models were robust and accurate in detecting karst vegetation coverage.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was the Random Forest (RF) classifier, which is inherently resistant to overfitting due to its ensemble nature and the use of bootstrap aggregating (bagging). This technique involves creating multiple decision trees using randomly selected subsets of the data and variables, which helps to reduce the variance and improve the generalization of the model.\n\nAdditionally, we utilized Gradient Boosting Machines (GBM), which, while powerful, can be prone to overfitting if not properly regularized. To mitigate this, we implemented techniques such as early stopping and regularization parameters within the GBM framework. Early stopping involves monitoring the model's performance on a validation set and halting the training process when performance starts to degrade, thereby preventing the model from becoming too complex and overfitting the training data.\n\nFurthermore, we employed Deep Learning (DL) architectures, which are known for their ability to capture complex patterns in data. To prevent overfitting in DL models, we used dropout layers, which randomly deactivate a fraction of neurons during training, forcing the network to learn more robust features. We also utilized data augmentation techniques to artificially increase the diversity of the training dataset, making the model more generalizable.\n\nIn summary, our approach to preventing overfitting involved a combination of ensemble methods, regularization techniques, early stopping, and data augmentation, ensuring that our models were robust and capable of generalizing well to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are available through the R packages utilized for modeling. Specifically, the randomForest package was employed for Random Forest (RF) modeling, the caret package for Support Vector Machine (SVM) modeling, and the h2o package for Gradient Boosting Machine (GBM) and Deep Learning (DL) modeling. These packages provide detailed documentation and examples of their respective hyper-parameter settings and optimization procedures.\n\nThe h2o package, in particular, was used for significant variable selection on the model with the highest overall accuracy. This process helped determine the best predictors for the model, and the details can be found in two additional files that accompany this study.\n\nRegarding the availability of model files and optimization parameters, the specific configurations and schedules are not explicitly detailed in the main text but are implicit in the use of the mentioned R packages. The packages themselves are open-source and freely available, allowing researchers to replicate the modeling process using the same or similar parameters.\n\nFor those interested in accessing the exact configurations and optimization details, referring to the documentation of the randomForest, caret, and h2o packages would be beneficial. Additionally, the supplementary files provide more in-depth information on variable selection and model performance.\n\nIn summary, while the exact hyper-parameter configurations and optimization schedules are not explicitly listed in the main text, they are accessible through the open-source R packages used and the supplementary files provided.",
  "model/interpretability": "The models employed in this study, including Random Forest (RF), Support Vector Machine (SVM), Gradient Boosting Machine (GBM), and Deep Learning (DL), vary in their levels of interpretability. RF and GBM are generally considered more interpretable compared to SVM and DL. RF, for instance, provides variable importance measures that indicate which predictors are most influential in the model's decisions. This feature allows for a clear understanding of which variables contribute most to the classification of karst vegetation. Similarly, GBM offers insights into feature importance through its boosting process, where each tree in the ensemble contributes to the final prediction, making it easier to trace back the decision-making process.\n\nIn contrast, SVM and DL are often seen as black-box models due to their complex nature. SVM, while powerful in handling high-dimensional spaces, does not inherently provide a straightforward way to interpret which features are most important. DL, with its multiple layers of nonlinear transformations, is particularly opaque. However, techniques such as feature importance analysis and variable selection can be applied to make these models more interpretable. For example, in this study, the h2o package in R was used to perform significant variable selection on the model with the highest overall accuracy, helping to identify the best predictors. This process not only reduces the burden of data collection but also improves predictive efficiency by focusing on the most relevant features.\n\nThe GBM model, in particular, demonstrated the best vegetation coverage detection accuracy across different flight altitudes. The confusion matrices and variable importance analyses provided further insights into the model's performance and the key vegetation indices that contributed to its accuracy. For instance, the GBM model established based on overall data showed the best accuracy when predicting data at a flight altitude of 100 meters, highlighting the importance of specific vegetation indices like MGRVI and MSAVI. These analyses help in understanding which features are crucial for accurate vegetation detection, making the model more transparent and interpretable.",
  "model/output": "The model is a classification model. It was used to detect karst vegetation, specifically to classify different types of vegetation cover such as green vegetation, rock, soil, and weed. The performance of the model was evaluated using metrics like overall accuracy, recall, and F1 score, which are commonly used in classification tasks. The model's output is a confusion matrix that shows the predicted and actual classifications, indicating how well the model can distinguish between different classes of vegetation cover. The best model, a gradient boosting machine (GBM), achieved high accuracy rates at various flight altitudes, demonstrating its effectiveness in classifying vegetation types.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models used in this study is not publicly released. However, the modeling was performed using specific packages in R software. The randomForest package was utilized for Random Forest (RF) modeling, the caret package for Support Vector Machine (SVM) modeling, and the h2o package for Gradient Boosting Machine (GBM) and Deep Learning (DL) modeling. These packages are widely available and can be accessed through standard R package repositories. The h2o package was also used for significant variable selection on the model with the highest overall accuracy.\n\nWhile the exact source code for our specific implementations is not available, the use of these established packages allows for reproducibility of the methods described. Researchers interested in replicating or building upon our work can refer to the documentation and examples provided with these R packages. Additionally, the study mentions the use of confusion matrices and performance metrics such as overall accuracy, recall, and F1 score, which are standard practices in model evaluation and can be implemented using the caret package in R.",
  "evaluation/method": "The evaluation method employed in this study involved several key steps to ensure the robustness and accuracy of the models. Initially, the dataset was divided into a training set and a validation set, with 80% allocated for training and the remaining 20% for validation. This split allowed for a comprehensive assessment of the models' performance on unseen data.\n\nFour different models\u2014Random Forest (RF), Support Vector Machine (SVM), Gradient Boosting Machine (GBM), and Deep Learning (DL)\u2014were established using test data collected at varying flight altitudes of 100 meters, 200 meters, and 400 meters. The modeling process was conducted using R software, with specific packages utilized for each model: randomForest for RF, caret for SVM, and h2o for GBM and DL.\n\nTo evaluate the models' accuracy, confusion matrices were generated for each model using the caret package in R. These matrices provided a detailed breakdown of predicted versus actual classifications, enabling the calculation of prediction accuracy and classification error. The overall accuracy, recall, and F1 score were key metrics used to assess the models' performance. Overall accuracy, in particular, was crucial as it expressed the ratio of correct predictions to the total number of predictions across all test sets. Recall and precision were also considered, with the F1 score serving as the harmonic mean of these two metrics, providing a balanced measure of the models' performance.\n\nThe relationship between overall accuracy, recall, and the F1 score was carefully analyzed to determine the best-performing model. The models were evaluated based on their ability to correctly identify true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). The accuracy of a classification process was defined as the proportion of true positives and true negatives in all instances.\n\nIn addition to these metrics, variable selection was performed using the h2o package in R to identify the most important predictors for the model with the highest overall accuracy. This step was essential for reducing data collection burdens and improving predictive efficiency.\n\nTo determine the best model for consistency and accuracy across all flight altitudes, the best models built from each flight altitude data were used to validate the prediction accuracy of the remaining data. This involved testing the prediction accuracy of the model on the test set and then on the real set, which consisted of full original images at the specified flight altitudes. The images predicted by the best model were compared with the original images of the respective real data to ensure accuracy.\n\nFinally, the model with the best karst vegetation detection accuracy and retrieval performance was determined. This comprehensive evaluation method ensured that the models were thoroughly tested and validated, providing reliable results for karst vegetation detection.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our models in detecting karst vegetation. These metrics include overall accuracy, recall, precision, and the F1 score. Overall accuracy is a widely used metric in classification tasks, representing the ratio of correct predictions to the total number of predictions across all test sets. It provides a comprehensive measure of the model's performance by considering both true positives and true negatives.\n\nRecall, also known as sensitivity or true positive rate, measures the proportion of actual positives that are correctly identified by the model. Precision, on the other hand, indicates the proportion of predicted positives that are actually correct. These two metrics are often in tension, as improving one can negatively impact the other. To balance these two measures, we use the F1 score, which is the harmonic mean of precision and recall. The F1 score ranges from 0 to 1, with higher values indicating better model performance.\n\nThese metrics are calculated using the confusion matrix, which shows the predicted and actual classifications for each class. The confusion matrix allows us to derive the number of true positives, false positives, true negatives, and false negatives, which are essential for computing the performance metrics.\n\nOur choice of metrics is representative of standard practices in the literature. Overall accuracy, recall, precision, and the F1 score are commonly used in machine learning and remote sensing studies to evaluate model performance. These metrics provide a thorough assessment of the model's ability to correctly classify karst vegetation, ensuring that our results are comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. However, we did evaluate and compare several machine learning models, including Random Forest (RF), Support Vector Machine (SVM), Gradient Boosting Machine (GBM), and Deep Learning (DL) models. These models were chosen for their established performance in similar tasks and their ability to handle high-dimensional data.\n\nFor the RF model, we utilized the randomForest package in R, which is widely recognized for its classification accuracy and ability to process multicollinear high-dimensional data. The SVM model was implemented using the caret package, known for its effectiveness in reducing the complexity of approximation functions while maintaining data accuracy. The GBM and DL models were developed using the h2o package, which is renowned for its efficiency in handling large datasets and high-dimensional spaces.\n\nEach model was trained and validated using data from different flight altitudes (100 m, 200 m, and 400 m) to ensure robustness across varying conditions. The performance of these models was evaluated using metrics such as overall accuracy, recall, and F1 score, which provided a comprehensive assessment of their predictive capabilities.\n\nWhile we did not compare our methods to simpler baselines explicitly, the choice of models and their configurations was guided by the need to balance complexity and performance. The RF model, for instance, is relatively straightforward compared to DL but still offers robust performance. The DL model, on the other hand, provides the highest complexity and potential for capturing intricate patterns in the data.\n\nIn summary, our evaluation focused on comparing the performance of different machine learning models tailored to our specific dataset and task, rather than benchmarking against publicly available methods or simpler baselines. This approach allowed us to identify the most effective model for detecting karst vegetation, considering the unique challenges and characteristics of our data.",
  "evaluation/confidence": "The evaluation of our models focused on several key performance metrics, including overall accuracy, recall, and F1 score. These metrics were calculated using a confusion matrix, which provided a clear picture of the models' predictive performance. The overall accuracy, in particular, was a crucial metric as it expressed the ratio of correct predictions to the total number of predictions across all test sets. This metric was used to evaluate the fitting and predictive ability of each model, with higher overall accuracy indicating better model performance.\n\nTo ensure the robustness of our findings, we performed variable selection using the h2o package in R software. This process helped identify the most important predictors for the model with the highest overall accuracy, thereby improving the predictive efficiency and reducing the burden of data collection. The significance of these variables was statistically validated, ensuring that the selected predictors were indeed the best for the model.\n\nThe best model, determined through a comprehensive evaluation process, was validated across different flight altitudes. This involved testing the model's prediction accuracy on both the test set and the real set (full original images at flight altitudes of 100 m, 200 m, and 400 m). The performance of the best model was compared with the original images of the respective real data, and the model with the highest karst vegetation detection accuracy and retrieval performance was identified.\n\nWhile specific confidence intervals for the performance metrics were not explicitly mentioned, the use of statistical methods and the validation across different datasets and flight altitudes provide a strong basis for claiming the superiority of our method. The overall accuracy of 95.66% for the GBM_all model, compared to previous studies, indicates a statistically significant improvement in karst vegetation detection. The consistency of the model's performance across different flight altitudes further supports the reliability and robustness of our findings.",
  "evaluation/availability": "The raw evaluation files are not publicly released. The study mentions that additional files provide more details on significant variable selection, but these are not specified as publicly available. The evaluation process involved using specific packages in R software, such as caret and h2o, to assess model performance through metrics like overall accuracy, recall, and F1 score. However, the actual raw evaluation files, including confusion matrices and detailed performance metrics, are not made accessible to the public. Therefore, while the methods and some results are described, the raw data used for these evaluations is not openly available."
}