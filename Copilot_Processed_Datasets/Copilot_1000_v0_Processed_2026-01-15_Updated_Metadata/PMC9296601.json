{
  "publication/title": "Development and Validation of a Mortality Prediction Model in Extremely Low Gestational Age Neonates.",
  "publication/authors": "Moreira A, Benvenuto D, Fox-Good C, Alayli Y, Evans M, Jonsson B, Hakansson S, Harper N, Kim J, Norman M, Bruschettini M",
  "publication/journal": "Neonatology",
  "publication/year": "2022",
  "publication/pmid": "35598593",
  "publication/pmcid": "PMC9296601",
  "publication/doi": "10.1159/000524729",
  "publication/tags": "- Neonatology\n- Mortality Prediction\n- Extremely Low Gestational Age Neonates\n- Clinical Prediction Model\n- Neonatal Care\n- Premature Infants\n- Neonatal Mortality\n- Gestational Age\n- Birthweight\n- Apgar Score",
  "dataset/provenance": "The dataset used in our study originates from the Swedish Neonatal Quality Register, a comprehensive national registry that collects data on very preterm neonates. This register is known for its high-quality and detailed clinical information, making it an ideal source for developing and validating prediction models.\n\nOur dataset consists of 3,752 data points, which were divided into a training set (80% of the data, comprising 2,454 survivors and 548 non-survivors) and a test set (20% of the data, comprising 613 survivors and 137 non-survivors). These data points include a variety of clinical variables that are typically captured within the first hour of life, such as gestational age, birth weight, Apgar scores, and details about delivery room resuscitation.\n\nThe variables selected for our study were chosen apriori by practicing neonatologists through collaborative discussions. This ensures that the variables are clinically relevant and universally obtainable. The dataset has been used in previous research, contributing to the body of knowledge in neonatology. The Swedish Neonatal Quality Register is a well-established resource in the community, providing valuable insights into the care and outcomes of very preterm neonates.\n\nThe dataset's robustness and the thorough validation process enhance the reliability of our prediction model, which is designed to assist clinicians in making critical decisions regarding the care of preterm neonates.",
  "dataset/splits": "The dataset was split into two main parts: a training set and a test set. The training set comprised 80% of the total data, while the test set contained the remaining 20%. Specifically, the training set included 2,454 survivors and 548 non-survivors, totaling 3,002 data points. The test set consisted of 613 survivors and 137 non-survivors, making up 750 data points. This split was done randomly to ensure that the models could be trained and validated effectively. The test set was particularly used to assess the internal validity of the models.",
  "dataset/redundancy": "The dataset used in this study consisted of 3,752 Swedish infants born extremely low gestational age (ELGA). To create and test our predictive models, the samples were randomly split into a training set (80%) and a test set (20%). This split was done to assess the internal validity of the models. The training set was used to build the models, while the test set was used to evaluate their performance.\n\nThe training and test sets were designed to be independent. This independence was enforced through random sampling, ensuring that the data in the test set was not used during the model training process. This approach helps to prevent overfitting and provides a more reliable estimate of the model's performance on unseen data.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the field of neonatology. The cohort included a diverse range of gestational ages, birthweights, and other clinical variables, which are crucial for building robust predictive models. The median gestational age was 25.0 weeks, and the median birthweight was 780 grams, with a mortality rate of 18%. These characteristics are representative of the population of ELGA neonates, making the dataset suitable for developing and validating predictive models in this context.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study primarily utilizes logistic regression, which is a well-established machine-learning algorithm class. This method is widely used for binary classification problems, making it suitable for our predictive modeling tasks.\n\nThe algorithm is not new; it has been extensively used and validated in various fields, including medical research. The choice of logistic regression was driven by its interpretability and effectiveness in handling binary outcomes, which is crucial for predicting mortality in extremely low gestational age neonates.\n\nGiven that logistic regression is a mature and widely accepted method, there was no need to publish it in a machine-learning journal. Instead, our focus was on applying this established technique to a specific medical context, ensuring that the model is clinically relevant and practically useful. The algorithm's performance was rigorously tested and validated, demonstrating its effectiveness in predicting outcomes for preterm neonates.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. The process began with a random forest selection to reduce the dimension of predictors. This was followed by backward and forward selection to refine the model. The final model, referred to as the 'BAG' model, includes three variables: birthweight, Apgar score at 5 minutes, and gestational age. These variables were selected based on statistical criteria such as p-values, Akaike Information Criterion (AIC), and variance inflation factors to ensure low collinearity and a manageable number of variables. The training data was split into a training set (80%) and a test set (20%) to assess the internal validity of the models. This ensures that the training data is independent for each subset, maintaining the integrity of the model's performance evaluation.",
  "optimization/encoding": "To prepare the data for our machine-learning algorithm, we began by handling missing values, as detailed in supplemental files. We then split the samples randomly into a training set (80%) and a test set (20%). The test set was used to assess the internal validity of the models.\n\nFor feature selection, we initially employed a random forest selection method. This approach helped us to identify the most relevant predictors. Following this, we applied backward and forward selection techniques to refine the set of predictors. The criteria for retaining variables included a p-value less than 5%, maintaining an Akaike Information Criterion (AIC) comparable to the model using all variables, ensuring low collinearity (variance inflation factors less than 4), and keeping the number of variables manageable for clinical translation.\n\nThe final model, referred to as the 'BAG' model, included three key variables: birthweight, Apgar score at 5 minutes, and gestational age. These variables were selected based on their statistical significance and their ability to provide a good fit to the data, as indicated by the AIC. The variance inflation factors for these variables ranged between 1.1 and 2.0, confirming low collinearity.\n\nTo evaluate the performance of our predictive models, we computed the area under the curve (AUC) of the receiver operating characteristic curve (ROC) with 95% confidence intervals. This metric allowed us to assess the discriminatory ability of our models. Additionally, we compared our model to several references, including gestational age alone, birthweight alone, the Extremely Preterm Birth Outcomes Tool from the National Institute of Child Health and Human Development, and the CRIB II score.\n\nIn summary, our data encoding and preprocessing involved careful selection and refinement of predictors using statistical methods, ensuring that the final model was both statistically robust and clinically relevant.",
  "optimization/parameters": "The model initially considered a set of predictive variables selected through a random forest approach. This method helped in reducing the dimensionality of the predictors. Following this, a backward and forward selection process was employed to refine the variables. The final model retained three key parameters: birthweight, Apgar score at 5 minutes, and gestational age. These variables were chosen because they met specific criteria, including having a p-value less than 5%, maintaining a comparable Akaike Information Criterion (AIC) to the model with all variables, exhibiting low collinearity (variance inflation factors less than 4), and being manageable for translation into a clinical tool. The derived model, referred to as the 'BAG' model, thus uses these three parameters to predict outcomes in extremely low gestational age neonates.",
  "optimization/features": "The final model, referred to as the 'BAG' model, utilizes three input features: birthweight, Apgar score at 5 minutes, and gestational age. Initially, a random forest selection method was employed to reduce the dimensionality of predictors. This process retained 10 predictive variables. Subsequently, backward and forward selection techniques were applied to further refine the model. Variables were retained if they had a p-value less than 5%, maintained a comparable Akaike Information Criterion (AIC) to the model using all variables, exhibited low collinearity (variance inflation factors less than 4), and were manageable in number for clinical translation. After this selection process, the final model included the three aforementioned variables.\n\nFeature selection was indeed performed, and it was conducted using only the training set, ensuring that the test set remained independent for model validation. This approach helps to prevent overfitting and ensures that the model's performance is generalizable to new, unseen data.",
  "optimization/fitting": "The fitting method employed in our study involved a careful balance to avoid both overfitting and underfitting. Initially, we utilized a random forest selection to reduce the dimensionality of predictors, which helped in managing a large number of potential variables relative to the training points. This step ensured that we did not have an excessively high number of parameters compared to the number of training points, thus mitigating the risk of overfitting.\n\nTo further refine the model, we employed backward and forward selection techniques. This process involved retaining variables that had a p-value less than 5%, maintained a comparable Akaike Information Criterion (AIC) to the model with all variables, exhibited low collinearity (variance inflation factors less than 4), and resulted in a manageable number of variables for clinical translation. The AIC is a crucial metric for model selection, as it balances the goodness of fit with the complexity of the model, thereby helping to prevent overfitting.\n\nAdditionally, we conducted a calibration curve with bootstraps of 1000 resamples to assess the accuracy between estimated and observed outcomes. This step ensured that our model was well-calibrated and not underfitting the data. A decision curve analysis was also performed to demonstrate the clinical benefit of using the prediction model compared to no model, further validating the model's utility.\n\nThe final model, referred to as the BAG model, included three key variables: birthweight, Apgar score at 5 minutes, and gestational age. These variables were selected based on their significant contribution to the model's predictive power and their clinical relevance. The variance inflation factors for these variables ranged between 1.1 and 2.0, indicating low collinearity and ensuring that the model was robust and not overfitted.\n\nIn summary, the fitting method involved a systematic approach to variable selection and model refinement, utilizing statistical metrics and validation techniques to ensure that the model was neither overfitted nor underfitted. This process resulted in a parsimonious and clinically relevant prediction model.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our predictive model. Initially, we used random forest selection to reduce the dimensionality of predictors, which helps in identifying the most relevant variables and mitigating the risk of overfitting by focusing on a subset of important features.\n\nFollowing the random forest selection, we applied backward and forward selection methods. These techniques systematically evaluate the inclusion and exclusion of variables based on statistical criteria, such as p-values and the Akaike Information Criterion (AIC). By retaining only variables that significantly contributed to the model's predictive power and maintaining a low AIC, we aimed to create a parsimonious model that generalizes well to new data.\n\nAdditionally, we assessed collinearity among the retained variables using variance inflation factors (VIF). Variables with VIF values less than 4 were considered to have low collinearity, ensuring that the model did not rely too heavily on redundant information, which could lead to overfitting.\n\nTo further validate our model, we split the data into training and test sets, with 80% of the samples used for model development and 20% reserved for internal validation. This approach allowed us to evaluate the model's performance on unseen data, providing a more reliable estimate of its generalizability.\n\nMoreover, we conducted a calibration curve with bootstrapping to assess the accuracy between estimated and observed outcomes, ensuring that our model's predictions were well-calibrated. This step is crucial for preventing overfitting, as it helps in understanding how well the predicted probabilities align with the actual outcomes.\n\nOverall, these regularization techniques\u2014including feature selection, model simplification, collinearity assessment, and internal validation\u2014were integral to our approach, ensuring that our predictive model was robust and generalizable.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model developed in this study is designed to be transparent and interpretable. The final predictive model, referred to as the 'BAG' model, includes only three variables: birthweight, Apgar score at 5 minutes, and gestational age. These variables are readily accessible and clinically meaningful, making the model easy to understand and apply in a clinical setting.\n\nThe process of variable selection involved using random forest selection followed by backward and forward logistic regression modeling. This method ensured that the retained variables had a p-value less than 5%, maintained a comparable Akaike Information Criterion (AIC) to the model with all variables, exhibited low collinearity (variance inflation factors less than 4), and were manageable in number for practical use.\n\nThe transparency of the model is further enhanced by the use of a dynamic nomogram, created using RShiny, which provides a user-friendly web application. This nomogram allows clinicians to input the values of the three variables and immediately see the predicted outcome, making the model's predictions clear and actionable.\n\nAdditionally, the model's performance was assessed using the area under the curve (AUC) of the receiver operating characteristic curve (ROC), providing a quantitative measure of its discriminatory ability. The model was compared to other established tools, such as gestational age alone, birthweight alone, the Extremely Preterm Birth Outcomes Tool, and the CRIB II score, demonstrating its competitive performance.\n\nIn summary, the BAG model is transparent and interpretable, relying on clinically relevant variables and providing clear, actionable predictions through a user-friendly web application.",
  "model/output": "The model developed is a logistic regression model, which is a type of classification model. It is used to predict the probability of a binary outcome, specifically neonatal mortality. The model was derived using random forest selection to reduce the dimension of predictors, followed by backward and forward logistic regression modeling. The final model, referred to as the 'BAG' model, includes three variables: birthweight, Apgar score at 5 minutes, and gestational age. These variables were selected based on their statistical significance (p-value < 0.05), low collinearity (variance inflation factors between 1.1 and 2.0), and their contribution to the model's predictive performance, as indicated by the Akaike Information Criterion (AIC).\n\nThe model's performance was evaluated using the area under the curve (AUC) of the receiver operating characteristic (ROC) curve. In the development cohort, the BAG model achieved an AUC of 76.9%, which was significantly higher than the AUCs of models using gestational age alone (71.3%) or birthweight alone (73.1%). In the validation cohort, the BAG model had an AUC of 68.9%, which was not significantly different from the AUCs of the gestational age model (62.8%), birthweight model (68.4%), or the CRIB II score (65.8%). However, when the validation cohort was subset to include neonates with a birthweight between 401 and 1,000 grams and a gestational age between 22 and 25 weeks, the BAG model had an AUC of 80.5%, which was comparable to the NICHD model (AUC = 80.2%) and higher than the CRIB II score (AUC = 69.4%).\n\nThe model's calibration was assessed using a calibration curve with bootstraps of 1000 resamples, and a decision curve analysis was conducted to demonstrate the clinical benefit of using the prediction model. Additionally, the model was translated into a dynamic nomogram using RShiny to create a web application for clinical use.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the prediction model has not been released publicly. However, the model has been translated into a dynamic nomogram, which is accessible as a web application. This web application, known as the BAG model, is publicly available and can be accessed via a URL. The web application provides an easy-to-use interface for clinicians and researchers to utilize the prediction model. The application is built using RShiny, which allows for interactive and dynamic data visualization and analysis. The web application is designed to be user-friendly, enabling users to input relevant clinical data and receive predictions based on the model. The specific details about the licensing of the web application are not provided.",
  "evaluation/method": "The evaluation method for our predictive models involved several key steps to ensure robustness and validity. Initially, the dataset was randomly split into a training set comprising 80% of the samples and a test set with the remaining 20%. This split allowed us to train our models on a substantial portion of the data while reserving a separate set for unbiased evaluation.\n\nTo assess the internal validity of the models, we utilized the test set. The prediction performance was measured by computing the area under the curve (AUC) of the receiver operating characteristic curve (ROC), along with 95% confidence intervals (CI). This metric provided a comprehensive evaluation of the models' ability to discriminate between outcomes.\n\nIn addition to the AUC, we compared our model's discriminatory ability against four established references: gestational age alone, birthweight alone, the Extremely Preterm Birth Outcomes Tool from the United States\u2019 National Institute of Child Health and Human Development, and the CRIB II score. This comparison helped contextualize the performance of our model within existing clinical tools.\n\nTo further validate our model, we conducted a calibration curve using bootstraps of 1000 resamples. This process assessed the accuracy between the estimated and observed number of outcome events, ensuring that our model's predictions were well-calibrated.\n\nWe also performed a decision curve analysis to demonstrate the clinical benefit of using our prediction model compared to no prediction model. This analysis provided insights into whether our model offered practical advantages in a clinical setting.\n\nLastly, we translated our prediction model into a dynamic nomogram using RShiny, creating a web application that allows for easy and interactive use of the model in clinical practice. This tool facilitates the translation of our findings into actionable insights for healthcare providers.",
  "evaluation/measure": "In the evaluation of our predictive models, several key performance metrics were reported to assess their effectiveness in predicting neonatal mortality. The primary metric used was the area under the curve (AUC) of the receiver operating characteristic curve (ROC), which provides a measure of the model's ability to discriminate between outcomes. The AUC was reported with 95% confidence intervals (CI) to indicate the reliability of the estimates.\n\nIn addition to the AUC, detailed metrics of model performance were provided, including sensitivity, specificity, and other relevant statistical measures. These metrics were presented for both the training and test sets, ensuring a comprehensive evaluation of the models' internal validity.\n\nThe models were compared against several references, including gestational age alone, birthweight alone, the Extremely Preterm Birth Outcomes Tool from the United States\u2019 National Institute of Child Health and Human Development, and the CRIB II score. This comparison allowed us to contextualize the performance of our models within the existing literature and established tools.\n\nTo further validate the models, a calibration curve with bootstraps of 1000 resamples was conducted. This assessment helped to evaluate the accuracy between the estimated and observed number of outcome events, ensuring that the models were well-calibrated.\n\nA decision curve analysis was also performed to demonstrate the clinical benefit of using the prediction model compared to no prediction model. This analysis is crucial for understanding the practical utility of the models in a clinical setting.\n\nThe variance inflation factors were used to assess collinearity among the predictors, ensuring that the models were robust and that the predictors were independent of each other. The Akaike information criterion (AIC) was employed to select the optimal model, with the model having the lowest AIC being theoretically the best-fitting model.\n\nOverall, the reported performance metrics are representative of standard practices in the field, providing a thorough evaluation of the models' predictive capabilities and their potential clinical utility. The use of multiple comparison tools and detailed statistical measures ensures that the evaluation is comprehensive and aligned with established methodologies in the literature.",
  "evaluation/comparison": "To evaluate the performance of our prediction model, we conducted a thorough comparison with several publicly available methods and simpler baselines. We assessed the discriminatory ability of our model against four references: gestational age alone, birthweight alone, the Extremely Preterm Birth Outcomes Tool from the United States\u2019 National Institute of Child Health and Human Development, and the CRIB II score. These comparisons were crucial for understanding how our model, which we refer to as the BAG model, performed relative to established and simpler predictive tools.\n\nThe BAG model was derived using a combination of birthweight, Apgar score at 5 minutes, and gestational age. We measured the prediction performance by computing the area under the curve (AUC) of the receiver operating characteristic curve (ROC) with 95% confidence intervals (CI). The AUC for the BAG model was 76.9% (95% CI, 72.6\u201381.3%) in the training set, which was significantly higher than the AUCs for gestational age alone (71.3%) and birthweight alone (73.1%). This indicated that the BAG model provided a better discriminatory ability compared to these simpler baselines.\n\nIn the validation cohort, the BAG model yielded an AUC of 68.9% (95% CI, 47.9\u201389.8%), which was comparable to the AUCs for gestational age (62.8%) and birthweight (68.4%). Additionally, we compared the BAG model to the CRIB II score, which resulted in an AUC of 65.8%. The comparisons using DeLong\u2019s statistical test showed no significant difference between the BAG model and the other models in the validation cohort.\n\nFurthermore, we subset the validation cohort to include neonates with a birthweight between 401 and 1,000 grams and a gestational age between 22 and 25 weeks. In this subset, the NICHD model provided an AUC of 80.2%, while the BAG model yielded an AUC of 80.5%. The CRIB II score resulted in an AUC of 69.4%. Again, no significant difference was appreciated in the AUCs using DeLong\u2019s statistical test.\n\nThese comparisons demonstrate that the BAG model performs competitively with publicly available methods and simpler baselines, providing a robust tool for predicting neonatal mortality in extremely low gestational age neonates.",
  "evaluation/confidence": "The evaluation of our predictive models included several key metrics to assess their performance and confidence. We computed the area under the curve (AUC) of the receiver operating characteristic curve (ROC) with 95% confidence intervals (CI) to measure prediction performance. This approach provided a clear indication of the models' discriminatory ability and the reliability of their predictions.\n\nTo determine the statistical significance of our models, we employed DeLong\u2019s test. This test allowed us to compare the AUCs between different models, including our BAG model and other references such as gestational age alone, birthweight alone, the Extremely Preterm Birth Outcomes Tool, and the CRIB II score. The results of DeLong\u2019s test were significant in some comparisons, indicating that our BAG model outperformed certain baselines, particularly gestational age and birthweight alone, in the training cohort. However, in the validation cohort, the differences were not statistically significant when compared to other models.\n\nAdditionally, we conducted a calibration curve with bootstraps of 1000 resamples to assess the accuracy between estimated and observed outcome events. This step ensured that our model's predictions were well-calibrated and reliable. Furthermore, a decision curve analysis was built to demonstrate the clinical benefit of using our prediction model compared to no prediction model. This analysis provided insights into the practical utility of our model in clinical settings.\n\nOverall, the performance metrics, including the AUC with 95% CI, and the statistical tests conducted, such as DeLong\u2019s test, calibration curves, and decision curve analysis, provided a comprehensive evaluation of our models' confidence and superiority over other methods and baselines.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, data is available upon reasonable request to the Swedish Neonatal Quality Register. This ensures that the data can be accessed for further research or validation purposes while maintaining control over its distribution and use. The specific details regarding the data request process can be obtained by contacting the appropriate authorities or institutions associated with the register."
}