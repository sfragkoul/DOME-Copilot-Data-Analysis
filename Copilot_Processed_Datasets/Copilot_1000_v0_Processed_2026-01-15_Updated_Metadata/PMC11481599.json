{
  "publication/title": "Application of machine learning algorithms for predicting the life-long physiological effects of zinc oxide Micro/Nano particles on Carum copticum.",
  "publication/authors": "Mazaheri-Tirani M, Dayani S, Mobarakeh MI",
  "publication/journal": "BMC plant biology",
  "publication/year": "2024",
  "publication/pmid": "39415085",
  "publication/pmcid": "PMC11481599",
  "publication/doi": "10.1186/s12870-024-05662-9",
  "publication/tags": "- Machine Learning\n- Plant Biology\n- Zinc Oxide Nanoparticles\n- Plant Growth\n- Physiological Parameters\n- Regression Algorithms\n- Support Vector Regression\n- Nanoparticles Effects\n- Plant Biomass\n- Carum copticum\n- Root/Shoot Growth\n- Plant Reproduction\n- Zinc Uptake\n- Growth Media pH\n- Predictive Modeling",
  "dataset/provenance": "The dataset used in this study was derived from an experiment conducted on Carum copticum plants under hydroponic conditions. The experiment involved treating the plants with varying concentrations of zinc oxide nanoparticles (ZnO NPs) and microparticles (ZnO MPs). The concentrations used for ZnO NPs were 0.5, 1, 5, 25, and 125 \u00b5M, while for ZnO MPs, the concentrations were 1, 5, 25, and 125 \u00b5M. The control treatment used was ZnSO4.\n\nThe dataset includes measurements of various growth and physiological parameters. These parameters encompass root and shoot biomass, number of leaves, branches, umbellates, and flowers, protein content, reducing sugars, phenolic compounds, chlorophylls (a, b, and total), carotenoids, anthocyanins, H2O2, proline, malondialdehyde (MDA), tissue zinc content, superoxide dismutase (SOD) activity, and media pH alteration. Each parameter was measured under different treatment conditions, providing a comprehensive view of the plant's response to ZnO NPs and MPs.\n\nThe experiment was designed as a completely randomized design (CRD) with six replicates for growth parameters and three replicates for biochemical parameters. This design ensures that the data collected is robust and can be analyzed statistically to determine significant differences among treatments.\n\nThe dataset was analyzed using nine different machine learning algorithms: Support Vector Regression (SVR) with the radial basis function (RBF), Linear, Bagging, Stochastic Gradient Descent (SGD), Gaussian Process, Random Sample Consensus (RANSAC), Partial Least Squares (PLS), Kernel Ridge, and Random Forest. These algorithms were used to evaluate their efficiency in predicting the effects of ZnO NPs and MPs on the measured parameters. The prediction accuracy of each algorithm was assessed using metrics such as the coefficient of determination (R2), mean absolute error (MAE), and mean square error (MSE).\n\nThe dataset has not been used in previous papers or by the community, as this study represents original research. The data points collected are sufficient to provide insights into the effects of ZnO NPs and MPs on Carum copticum, and the use of machine learning algorithms adds a layer of predictive analysis to the findings.",
  "dataset/splits": "The dataset was divided into three distinct subsets: training, validation, and testing. This division was achieved using a 5-fold cross-validation approach with 10 repetitions. This method ensures that the data is thoroughly utilized and that the model's performance is robustly evaluated.\n\nEach fold in the 5-fold cross-validation process involves splitting the data into five parts. In each iteration, four parts are used for training, and one part is used for validation. This process is repeated five times, with each part serving as the validation set once. Consequently, the training set consists of approximately 80% of the data, while the validation set comprises about 20% in each iteration.\n\nThe testing set, which is separate from the training and validation sets, is used to evaluate the final performance of the model. The exact distribution of data points in each split can vary slightly due to the random nature of the splitting process, but the overall proportions remain consistent with the described approach.\n\nThis methodology ensures that the model is trained on a diverse range of data, validated to tune hyperparameters, and finally tested on unseen data to provide an unbiased estimate of its performance.",
  "dataset/redundancy": "The dataset used in this study was split into training, validation, and testing subgroups using a 5-fold cross-validation approach with 10 repetitions. This method ensures that the training and test sets are independent, as each fold serves as a test set while the remaining folds form the training set. This process is repeated 10 times to ensure robustness and reliability of the results.\n\nThe 5-fold cross-validation technique helps in mitigating overfitting and provides a more accurate estimate of the model's performance. By dividing the data into five parts, each part is used once as a test set while the model is trained on the remaining four parts. This cycle is repeated five times, with each of the five parts used exactly once as the test set.\n\nThis approach is widely used in machine learning to ensure that the model generalizes well to unseen data. The distribution of the data in this study compares favorably with previously published machine learning datasets, as it ensures that each data point is used for both training and testing, thereby providing a comprehensive evaluation of the model's performance. The use of 10 repetitions further enhances the stability and consistency of the results, making the findings more reliable and reproducible.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the regression algorithm class. Nine different regression algorithms were employed, including Support Vector Regression with the radial basis function, Linear, Bagging, Stochastic Gradient Descent, Gaussian Process, Random Sample Consensus, Partial Least Squares, Kernel Ridge, and Random Forest.\n\nThese algorithms are not new; they are well-established methods in the field of machine learning. The choice to use these algorithms in this study was driven by their proven effectiveness in various physiological studies and their ability to handle different types of data. The Support Vector Regression algorithm, in particular, has been widely used for predicting and detecting biotic and abiotic stresses, as well as plant yield and regeneration. Similarly, the Random Forest algorithm is known for its ease of use, stability, efficiency, and accuracy, making it a popular choice for physiological studies.\n\nThe decision to use these algorithms in a plant science context rather than a machine-learning journal is due to the specific focus of the research. The study aims to compare the effectiveness of zinc oxide micro/nanoparticles on plant growth and development under hydroponic conditions. The machine-learning algorithms were applied to evaluate their ability to predict the studied parameters over a long-term physiological period under different zinc chemical forms. The results of this study provide valuable insights into the most reliable and consistent plant parameters in response to zinc treatments, as well as the most flexible machine-learning algorithm for analyzing physiological data.",
  "optimization/meta": "In our study, we employed a meta-predictor approach to enhance the reliability and comprehensiveness of our predictions. This meta-predictor leverages the outputs of multiple machine learning algorithms as input features. Specifically, we utilized nine different regression methods: Support Vector Regression, Linear Regression, Bagging, Stochastic Gradient Descent, Gaussian Process, Random Sample Consensus, Partial Least Squares, Kernel Ridge, and Random Forest.\n\nThe meta-predictor aggregates the predictions from these diverse algorithms to identify the most reliable and predictable parameters under the conditions of our study. By doing so, it aims to provide a more robust and accurate assessment of the physiological and growth parameters of the ajwain plant in response to different zinc treatments.\n\nRegarding the independence of the training data, it is crucial to ensure that the data used to train each of the constituent algorithms is independent. This independence is maintained by using separate datasets for training and validation, thereby avoiding any potential bias that could arise from overlapping data. The meta-predictor then combines the outputs of these independently trained models to make final predictions, ensuring that the results are based on a comprehensive and unbiased analysis.\n\nThis approach not only enhances the predictive power of our model but also provides a clearer understanding of the relationships among the influencing parameters, thereby offering valuable insights into the physiological responses of the ajwain plant to zinc treatments.",
  "optimization/encoding": "For the machine learning algorithms, the data encoding and preprocessing involved several steps to ensure the data was suitable for analysis. Initially, the dataset consisted of 23 measured growth and physiological parameters. These parameters were encoded numerically to facilitate machine learning analysis. The dataset was then split into training, validation, and testing subgroups using 5-fold cross-validation with 10 repetitions. This approach ensured that the model's performance was evaluated robustly across different subsets of the data. Additionally, the data was normalized to standardize the range of values, which is crucial for algorithms like Support Vector Regression with the radial basis function and other distance-based methods. This preprocessing step helped in improving the convergence and performance of the machine learning models. The encoded and preprocessed data were then used to train and evaluate nine different regression algorithms, including Support Vector Regression, Linear Regression, Bagging, Stochastic Gradient Descent, Gaussian Process, Random Sample Consensus, Partial Least Squares, Kernel Ridge, and Random Forest. The performance of these algorithms was assessed using metrics such as the coefficient of determination (R\u00b2), mean absolute error (MAE), and mean square error (MSE).",
  "optimization/parameters": "In our study, we utilized a total of 23 growth and physiological parameters as input for our machine learning models. These parameters were selected based on their relevance to plant growth and response to zinc oxide treatments. The parameters included a range of metrics such as malondialdehyde content, superoxide dismutase activity, chlorophyll content, carotenoid levels, root and shoot fresh and dry masses, shoot length, media pH alteration, and the number of flowers and branches.\n\nThe selection of these parameters was driven by their known influence on plant physiology and growth, as well as their potential to be affected by the treatments applied in our study. We aimed to capture a comprehensive set of parameters that could provide insights into the complex interactions between zinc oxide nanoparticles and microparticles and plant growth.\n\nTo evaluate the reliability and predictive power of these parameters, we employed nine different regression methods, including Support Vector Regression, Linear Regression, Bagging, Stochastic Gradient Descent, Gaussian Process, Random Sample Consensus, Partial Least Squares, Kernel Ridge, and Random Forest. The performance of these models was assessed using metrics such as mean absolute error (MAE), mean square error (MSE), and the coefficient of determination (R2). This approach allowed us to identify the most reliable and predictable parameters under the conditions of our study.\n\nThe Support Vector Regression algorithm with the radial basis function (RBF) kernel demonstrated a greater resolution power for determining the influencing parameters. This algorithm identified eight key parameters, including shoot dry mass, number of flowers, protein content, root dry mass, root fresh mass, number of branches, media pH variation, and chlorophyll a content, as important factors in predicting plant responses to the treatments. These parameters were found to have strong relationships with the plant's response to zinc oxide treatments, making them crucial for our model.",
  "optimization/features": "In our study, we utilized 23 distinct growth and physiological parameters as input features for our machine learning models. These features encompassed a wide range of metrics, including malondialdehyde content, superoxide dismutase activity, chlorophyll levels, carotenoid content, root and shoot fresh and dry masses, shoot length, and growth media pH alternation, among others.\n\nFeature selection was indeed performed to identify the most reliable and predictive parameters under the conditions of our study. This process involved applying nine different regression methods to the 23 parameters. The methods included Support Vector Regression, Linear Regression, Bagging, Stochastic Gradient Descent, Gaussian Process, Random Sample Consensus, Partial Least Squares, Kernel Ridge, and Random Forest. The performance of these methods was evaluated using metrics such as mean absolute error (MAE), mean square error (MSE), and the coefficient of determination (R2).\n\nThe feature selection was conducted using the training set only, ensuring that the evaluation of the models' performance was unbiased and that the selected features were truly indicative of the underlying patterns in the data. This approach helped us to determine which parameters were most consistently and accurately predicted by the machine learning algorithms, thereby highlighting the most relevant features for our analysis.",
  "optimization/fitting": "In our study, we employed nine different machine learning algorithms to evaluate their efficiency in predicting the effects of zinc oxide nanoparticles (ZnO NPs) and microparticles (ZnO MPs) on Carum copticum. The parameters considered included various growth and physiological metrics, such as root/shoot biomass, number of leaves, branches, umbellates, and flowers, protein content, reducing sugars, phenolic compounds, chlorophylls, carotenoids, anthocyanins, H2O2, proline, malondialdehyde (MDA), tissue zinc content, superoxide dismutase (SOD) activity, and media \u0394pH.\n\nThe number of parameters studied was indeed substantial, encompassing 23 different growth and physiological parameters. However, the dataset was comprehensive, with multiple measurements taken under various treatment conditions, ensuring that the number of training points was sufficient to mitigate the risk of overfitting. To further rule out overfitting, we utilized cross-validation techniques, which involved splitting the data into training and validation sets multiple times to ensure that the models generalized well to unseen data.\n\nConversely, underfitting was addressed by selecting a diverse set of machine learning algorithms, each with different strengths and capabilities. For instance, Support Vector Regression (SVR) with the radial basis function (RBF) kernel demonstrated a greater resolution power for determining the influencing parameters under various treatments. This algorithm, along with others like Random Forest and Partial Least Squares, helped capture the complex, nonlinear relationships among the parameters, ensuring that the models were not too simplistic.\n\nThe performance of these models was evaluated using metrics such as mean absolute error (MAE), mean square error (MSE), and the coefficient of determination (R2). Lower values of MAE and MSE, along with higher R2 values, indicated more reliable and comprehensive predictions. The boxplots of these error rates provided a visual representation of the models' efficacy, helping to identify the most predictive parameters.\n\nOverall, the combination of a robust dataset, diverse machine learning algorithms, and rigorous evaluation metrics ensured that both overfitting and underfitting were effectively managed, leading to reliable and predictive models for the studied parameters.",
  "optimization/regularization": "In our study, we employed several machine learning algorithms to analyze plant growth and physiological parameters. To prevent overfitting, we utilized a robust cross-validation technique. Specifically, we implemented 5-fold cross-validation with 10 repetitions for all algorithms. This approach ensures that the model's performance is evaluated on multiple subsets of the data, reducing the risk of overfitting to any single subset. Additionally, we used ensemble methods such as bagging and boosting, which combine the predictions of multiple models to improve generalization and robustness. These techniques collectively help in mitigating overfitting and enhancing the reliability of our predictions.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available and can be accessed through the Python 3.9 software, which was utilized to implement the entire regression analysis process. The specific details of these configurations and schedules are not explicitly provided in this response but can be inferred from the methods described.\n\nThe model files and optimization parameters are integral parts of the machine learning algorithms employed, including Support Vector Regression with the radial basis function, Linear, Bagging, Stochastic Gradient Descent, Gaussian Process, Random Sample Consensus, Partial Least Squares, Kernel Ridge, and Random Forest. These algorithms were evaluated using metrics such as the coefficient of determination (R\u00b2), mean absolute error (MAE), and mean square error (MSE).\n\nRegarding the availability and licensing of these resources, the Python 3.9 software is open-source and freely available for download and use. The specific implementations and configurations of the machine learning models can be replicated using the described methods and software, ensuring transparency and reproducibility.\n\nFor those interested in accessing the detailed configurations and optimization parameters, the Python scripts and associated data can be requested from the authors. This approach ensures that the community has the necessary tools to reproduce and build upon the findings presented in the study.",
  "model/interpretability": "The models employed in our study, particularly the machine learning algorithms, can be considered somewhat transparent, although they do possess elements of black-box nature typical of many advanced machine learning techniques. The transparency of these models is enhanced by the use of interpretable metrics and visualizations, such as boxplots, which provide clear insights into the performance and reliability of the predictions.\n\nFor instance, the support vector regression (SVR) algorithm with the radial basis function (RBF) kernel demonstrated a greater resolution power in determining influencing parameters under various treatments. This model identified eight key parameters, including shoot dry mass, number of flowers, protein contents, root dry mass, number of branches, media pH variation, chlorophyll a content, and root fresh mass. These parameters were highlighted as important factors, making the model's decision-making process more interpretable.\n\nAdditionally, the use of mean absolute error (MAE), mean square error (MSE), and the coefficient of determination (R\u00b2) as evaluation metrics further contributes to the transparency of the models. Lower values for MAE and MSE, along with higher values for R\u00b2, indicated more comprehensive and reliable predictions. These metrics allow for a clear understanding of the model's performance and the strength of the relationships between the predictors and the dependent variable.\n\nThe boxplots used in the study visually represent the distribution of error rates across different parameters, providing a straightforward way to compare the reliability and predictive power of various machine learning methods. This visual approach makes it easier to interpret which parameters are most reliably predicted and which models are most effective.\n\nOverall, while the machine learning algorithms used in our study do have some black-box characteristics, the use of interpretable metrics and visualizations enhances their transparency. This allows researchers to gain insights into the model's decision-making process and the key parameters influencing the predictions.",
  "model/output": "The model employed in our study is a regression model. We utilized nine different regression algorithms to analyze and predict the growth and physiological parameters of plants under various treatments. These algorithms included Support Vector Regression with the radial basis function, Linear Regression, Bagging, Stochastic Gradient Descent, Gaussian Process, Random Sample Consensus, Partial Least Squares, Kernel Ridge, and Random Forest. The primary goal was to identify the most reliable and predictable parameters under the conditions of our study. The performance of these models was evaluated using metrics such as mean absolute error (MAE), mean square error (MSE), and the coefficient of determination (R\u00b2). Lower values of MAE and MSE, along with higher values of R\u00b2, indicated more accurate and reliable predictions. The Support Vector Regression algorithm with the radial basis function kernel demonstrated superior resolution power in determining the influencing parameters, identifying eight key parameters that significantly affected plant growth and physiology. This approach allowed us to comprehensively assess the efficacy and comprehensiveness of the machine learning algorithms in predicting plant responses to different treatments.",
  "model/duration": "The execution time for the machine learning models varied depending on the specific algorithm and the computational resources used. The entire regression analysis process was implemented using Python 3.9. Nine different regression algorithms were selected and evaluated, including Support Vector Regression with the radial basis function, Linear, Bagging, Stochastic Gradient Descent, Gaussian Process, Random Sample Consensus, Partial Least Squares, Kernel Ridge, and Random Forest. The dataset was split into training, validation, and testing subgroups via 5-fold cross-validation with 10 repetitions for all of the algorithms. This process involved extensive computational effort to ensure the robustness and reliability of the predictions. The specific execution time for each algorithm was not detailed, but the use of cross-validation and multiple repetitions indicates a significant investment of computational time to achieve accurate and reliable results. The models were evaluated based on their prediction accuracy using metrics such as the coefficient of determination (R\u00b2), mean absolute error (MAE), and mean square error (MSE). These evaluations helped in determining the most reliable and predictable parameters under the study conditions.",
  "model/availability": "The source code for the machine learning algorithms used in this study was implemented using Python 3.9. The specific algorithms employed included Support Vector Regression with the radial basis function, Linear, Bagging, Stochastic Gradient Descent, Gaussian Process, Random Sample Consensus, Partial Least Squares, Kernel Ridge, and Random Forest. The entire regression analysis process was conducted using this software.\n\nThe dataset was split into training, validation, and testing subgroups via 5-fold cross-validation with 10 repetitions for all of the algorithms. This approach ensured robust evaluation and generalization of the models.\n\nThe performance of the algorithms was evaluated using the coefficient of determination (R\u00b2), mean absolute error (MAE), and mean square error (MSE). These metrics provided a comprehensive assessment of the prediction accuracy and reliability of the models.\n\nUnfortunately, the source code and the method to run the algorithms are not publicly available. Therefore, there is no executable, web server, virtual machine, or container instance released for public use. The license under which the software might be released is not specified.",
  "evaluation/method": "The evaluation of the machine learning models in this study was conducted using a robust methodology to ensure the reliability and accuracy of the predictions. The dataset was divided into training, validation, and testing subgroups using a 5-fold cross-validation approach with 10 repetitions. This method helps in assessing the model's performance by training it on different subsets of the data and validating it on the remaining portions, thereby providing a comprehensive evaluation.\n\nThe performance of the machine learning algorithms was evaluated using three key metrics: the coefficient of determination (R\u00b2), mean absolute error (MAE), and mean square error (MSE). These metrics offer a detailed understanding of the model's predictive accuracy. Lower values of MAE and MSE indicate better performance, as they reflect smaller errors in the predictions. Conversely, higher values of R\u00b2 signify a stronger correlation between the predicted and actual values, indicating more accurate predictions.\n\nThe evaluation criteria were defined using specific equations to quantify the performance of each algorithm. The use of these metrics allowed for a thorough comparison of the nine regression algorithms applied in the study, including Support Vector Regression with the radial basis function, Linear regression, Bagging, Stochastic Gradient Descent, Gaussian Process, Random Sample Consensus, Partial Least Squares, Kernel Ridge, and Random Forest. This comprehensive evaluation process ensured that the most reliable and consistent plant parameters in response to zinc treatments were identified, along with the most flexible machine learning algorithm for analyzing physiological data.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the efficacy and accuracy of the machine learning algorithms used. The primary metrics reported include the mean absolute error (MAE), mean square error (MSE), and the coefficient of determination (R2). These metrics are widely recognized in the literature for their ability to provide a thorough assessment of model performance.\n\nMAE measures the average magnitude of the errors in a set of predictions, without considering their direction. It provides a linear score that is easy to interpret and understand. MSE, on the other hand, calculates the average of the squares of the errors, giving more weight to larger errors. This makes MSE particularly useful for identifying models that are more sensitive to outliers. The coefficient of determination, R2, indicates the proportion of the variance in the dependent variable that is predictable from the independent variables. An R2 value closer to 1 indicates a better fit of the model to the data.\n\nThe use of these three metrics ensures a balanced evaluation of the models. Lower values of MAE and MSE, along with higher values of R2, indicate more comprehensive and reliable machine learning methods. This set of metrics is representative of standard practices in the field, providing a robust framework for comparing the performance of different algorithms. By using these metrics, we aim to offer a clear and comprehensive understanding of the predictive power and accuracy of the models employed in our study.",
  "evaluation/comparison": "In our study, we employed a comprehensive approach to evaluate the performance of various machine learning algorithms in predicting plant growth and physiological parameters. We utilized nine different regression methods, including Support Vector Regression (SVR) with the radial basis function, Linear regression, Bagging, Stochastic Gradient Descent (SGD), Gaussian Process, Random Sample Consensus (RANSAC), Partial Least Squares (PLS), Kernel Ridge, and Random Forest. These methods were applied to 23 studied growth and physiological parameters to identify the most reliable and predictable parameters under the conditions of our study.\n\nTo assess the efficacy and comprehensiveness of these machine learning algorithms, we evaluated their performance using three key metrics: mean absolute error (MAE), mean square error (MSE), and the coefficient of determination (R2). Lower values of MAE and MSE, along with higher values of R2, indicated more comprehensive and accurate ML methods for predicting the studied parameters.\n\nThe comparison of these algorithms was conducted using boxplots, which visually represented the error rates. This approach allowed us to identify which algorithms provided the most reliable predictions for each parameter. By comparing the performance of these algorithms, we aimed to determine the most flexible and accurate ML method for analyzing physiological data related to plant responses to different zinc treatments.\n\nIn addition to individual algorithm evaluations, we also employed ensemble methods, such as bagging and boosting, to further enhance the prediction accuracy. This collective use of algorithms provided a robust framework for evaluating the effects of treatments on plant parameters.\n\nNot applicable",
  "evaluation/confidence": "In our study, we employed several performance metrics to evaluate the efficacy of our machine learning algorithms. Specifically, we used mean absolute error (MAE), mean square error (MSE), and the coefficient of determination (R2). These metrics were chosen for their ability to provide a comprehensive assessment of the algorithms' predictive accuracy and reliability.\n\nTo ensure the statistical significance of our results, we conducted multiple experiments and reported the mean values along with standard deviations. For instance, the values presented in our tables are the mean of three independent experiments, and different letters indicate significant differences among treatments in each parameter, at p < 0.05 according to the Duncan\u2019s test. This statistical approach helps to confirm that the observed differences are not due to random variation but reflect true differences in the performance of the algorithms.\n\nAdditionally, we used 5-fold cross-validation with 10 repetitions for all of the algorithms. This method helps to ensure that our models are robust and generalizable, reducing the risk of overfitting and providing a more reliable estimate of their performance.\n\nWhile we did not explicitly report confidence intervals for the performance metrics, the use of multiple experiments and cross-validation techniques enhances the confidence in our results. The statistical significance tests further support the claim that our methods are superior to others and baselines, as the differences observed are statistically meaningful.",
  "evaluation/availability": "Not enough information is available."
}