{
  "publication/title": "Using Machine Learning to Predict Cognitive Impairment Among Middle-Aged and Older Chinese: A Longitudinal Study.",
  "publication/authors": "Liu H, Zhang X, Liu H, Chong ST",
  "publication/journal": "International journal of public health",
  "publication/year": "2023",
  "publication/pmid": "36798738",
  "publication/pmcid": "PMC9926933",
  "publication/doi": "10.3389/ijph.2023.1605322",
  "publication/tags": "- Longitudinal study\n- Machine learning\n- Random forest\n- Middle-aged and older Chinese\n- Cognitive impairment\n- Dementia\n- Predictive modeling\n- Cognitive function\n- Risk factors\n- Public health",
  "dataset/provenance": "The dataset used in this study was obtained from the China Health and Retirement Longitudinal Study (CHARLS), conducted between 2011 and 2015. CHARLS is a nationally representative longitudinal survey that covers 450 villages or communities across 150 counties/districts in China, with 52.67% of the areas being rural and 47.33% urban.\n\nThe initial baseline survey in 2011 included 19,817 respondents. Subsequent surveys in 2013 and 2015 involved 18,605 and 21,095 respondents, respectively. By merging the data from 2011, 2013, and 2015 based on ID and household ID matching, a total of 4,043 participants were identified as having participated in all three surveys. After excluding missed follow-up data, interviewees answered by others, and variables with more than 20% missing data, the final sample size for analysis was 2,326 participants.\n\nThe data utilized in this study included basic information, health status and functioning, physical examination and blood-based biomarkers from 2011 to 2015, and cognitive module data from 2013. This comprehensive dataset allowed for a detailed analysis of cognitive impairment predictors among middle-aged and elderly individuals.",
  "dataset/splits": "The dataset was divided into two primary splits: a training set and a test set. The training set comprised 70% of the total data, amounting to 1,628 data points. The test set consisted of the remaining 30%, totaling 698 data points. This division was done to evaluate the performance of the models on unseen data.\n\nAdditionally, a 10-fold cross-validation method was employed to validate the models. This technique involves splitting the data into 10 subsets, where the model is trained on 9 subsets and tested on the remaining 1 subset. This process is repeated 10 times, with each subset serving as the test set once. This approach ensures that every data point is used for both training and testing, providing a robust evaluation of the model's performance.",
  "dataset/redundancy": "The dataset used in this study was obtained from the China Health and Retirement Longitudinal Study (CHARLS) spanning from 2011 to 2015. The initial dataset included 19,817 respondents in 2011, 18,605 in 2013, and 21,095 in 2015. After merging the data from 2011, 2013, and 2015 based on ID and household ID matching, 4,043 participants who participated in all three surveys were identified. To ensure data quality, respondents with missed follow-up data, those whose interviews were answered by others, and variables missing more than 20% of patient data were excluded. This rigorous filtering process resulted in a final sample size of 2,326 participants.\n\nThe dataset was then split into a training set and a test set. The training set comprised 70% of the data, totaling 1,628 samples, while the test set included the remaining 30%, amounting to 698 samples. This split ensured that the training and test sets were independent, which is crucial for evaluating the model's performance on unseen data.\n\nTo handle missing values, the method of nearest neighbor imputation was employed. This technique helps to maintain the integrity of the dataset by filling in missing values based on the nearest available data points, thereby reducing potential biases that could arise from incomplete data.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets in terms of its comprehensive coverage of demographic, health status, functioning, emotional status, and lifestyle and behavior variables. This rich set of predictors allows for a robust analysis and enhances the model's ability to identify key risk factors for cognitive impairment. The use of longitudinal data further strengthens the study by providing a temporal dimension to the analysis, which is essential for understanding the progression of cognitive impairment over time.",
  "dataset/availability": "The data used in this study were obtained from the China Health and Retirement Longitudinal Study (CHARLS), which is a nationally representative longitudinal survey. The CHARLS data is publicly available and can be accessed through the Peking University Open Research Data Platform. The data is licensed under terms that allow for academic and research use, ensuring that the data can be utilized by other researchers for similar studies.\n\nThe CHARLS survey covers a wide range of variables, including demographic information, health status, functioning, cognitive function, and lifestyle behaviors. The specific data used in this study included baseline data from 2011, follow-up data from 2013 and 2015, and cognitive function assessments from 2013. The final sample size for the analysis was 2,326 participants who had complete data across all three survey waves.\n\nTo ensure the integrity and reproducibility of the data splits, the study followed a rigorous data preprocessing protocol. This included merging data from different years based on unique identifiers, excluding participants with missing follow-up data or variables with more than 20% missing data, and ensuring that only participants who completed all three surveys were included in the final analysis. The data preprocessing steps are detailed in the supplementary materials, providing transparency and allowing other researchers to replicate the data splits used in the study.\n\nThe CHARLS data is freely available to the public, and the terms of use are clearly outlined on the Peking University Open Research Data Platform. Researchers are required to adhere to these terms, which typically include proper citation of the data source and compliance with ethical guidelines for data use. This ensures that the data is used responsibly and that the contributions of the original data collectors are acknowledged.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the random forest algorithm. This is a well-established ensemble learning method known for its robustness and ability to handle complex, non-linear relationships in data.\n\nThe random forest algorithm is not new; it has been widely used and studied in the machine learning community for many years. It was chosen for this study due to its proven effectiveness in predictive modeling and its ability to handle a large number of input variables without overfitting.\n\nThe reason it was not published in a machine-learning journal is that the focus of our research was on applying this algorithm to a specific problem in public health\u2014predicting cognitive impairment among middle-aged and older Chinese individuals. Our primary goal was to demonstrate the practical application of machine learning in this domain, rather than to introduce a new algorithm. The study was published in the International Journal of Public Health, which aligns with our objective of contributing to the field of public health and primary care.",
  "optimization/meta": "The model employed in this study does not use data from other machine-learning algorithms as input. Instead, it relies on a variety of baseline predictors, including demographic information, health status, functioning, emotional status, and lifestyle behaviors. These predictors were collected from a large representative sample of middle-aged and elderly individuals who participated in a longitudinal survey.\n\nThe primary machine-learning method used is the random forest algorithm. This algorithm was chosen for its ability to handle multiple explanatory variables without overfitting, and for its high accuracy in predicting cognitive impairment compared to other methods. The random forest model was trained and validated using a dataset divided into training and test sets, with missing values imputed using the nearest neighbor method. The model's performance was evaluated using the area under the receiver operating characteristic curve (AUC), with values indicating good to very good performance.\n\nThe study also compared the random forest model with traditional logistic regression. While logistic regression is a standard method for binary classification, it is limited by assumptions of normality and linear relationships. In contrast, the random forest model can evaluate non-linear and complex relationships between variables, making it more suitable for predicting cognitive impairment.\n\nThe training data for the random forest model was independent, as it was divided into training and test sets to ensure that the model's performance could be validated on unseen data. This approach helps to prevent overfitting and ensures that the model's predictions are generalizable to new data. The random forest model's parameters were carefully set to optimize performance and prevent overfitting, including the maximum depth of the forest, the maximum number of leaves, and the learning rate. The model's training was stopped when the AUC value no longer increased, further ensuring that the model was not overfitted to the training data.",
  "optimization/encoding": "The data used in this study was pre-processed and encoded to facilitate the application of machine learning algorithms. Initially, the dataset consisted of 2,326 records, which were split into a training set comprising 70% of the data (1,628 records) and a test set with the remaining 30% (698 records). Missing values within the dataset were handled using the nearest neighbor imputation method to ensure completeness.\n\nFor the random forest model, specific parameters were set to optimize performance and prevent overfitting. The maximum depth of the forest was limited to 6, and the maximum number of leaves was set to 90. The learning rate was configured at 0.001, and the training evaluation index involved 100 iterations of the Area Under the Curve (AUC) training. To avoid overfitting, the training process was halted if the AUC value did not improve after 5 consecutive iterations.\n\nIn the logistic regression model, the input data underwent standardization to accelerate the gradient descent process in finding the optimal solution. The regularization parameter was set to \"L2,\" and the number of cross-validation folds was set to 10. The loss function was optimized using the second derivative matrix, and the regularization coefficient was divided into 20 equal parts ranging from -2 to 2. The iteration termination criterion was defined by an error range of 0.01.\n\nDescriptive statistics were used to express the results, with continuous variables presented as the mean and standard deviation, and categorical variables as percentages. The AUC value from the Receiver Operator Characteristic (ROC) curve was used to evaluate model performance, with thresholds set for interpreting the AUC values: >0.9 was considered very good, 0.8\u20130.90 good, 0.7\u20130.8 fair, and <0.7 poor.",
  "optimization/parameters": "In our study, we employed two primary models: a random forest model and a logistic regression model. For the random forest model, several parameters were set to optimize performance and prevent overfitting. The maximum depth of the forest was set to 6, and the maximum number of leaves was set to 90. The learning rate was set to 0.001, and the training evaluation index was based on 100 iterations of the Area Under the Curve (AUC) training. Training was stopped if the AUC value did not increase for more than 5 consecutive iterations to prevent overfitting. Additionally, a 10-fold cross-validation method was used to validate the model.\n\nFor the logistic regression model, the input data were standardized to accelerate the gradient descent process in finding the optimal solution. The regularization parameter selected was \"L2,\" and the number of cross-validation folds was set to 10. The loss function was optimized using the second derivative matrix of the loss function. The regularization coefficient was set to 20 equal parts ranging from -2 to 2, and the error range for the iteration termination criterion was set to 0.01.\n\nThe specific number of parameters (p) used in the models was not explicitly stated, but the models were optimized using the aforementioned parameters and techniques to ensure robust performance and generalization to new data.",
  "optimization/features": "In our study, we utilized a total of 44 baseline features as input for our models. These features were selected based on previous research indicating their relevance to cognitive classification and baseline demographics. The selection process involved considering various aspects such as demographic variables, health status, functioning, emotional status, and lifestyle behaviors.\n\nFeature selection was indeed performed to ensure that only the most relevant predictors were included in the dataset. This process was conducted using the training set exclusively, adhering to best practices to prevent data leakage and maintain the integrity of the model evaluation. By focusing on the training set, we aimed to create a robust model that could generalize well to unseen data.",
  "optimization/fitting": "In our study, we employed two primary models: random forest and logistic regression, each with specific parameters and techniques to address potential overfitting and underfitting issues.\n\nFor the random forest model, we set the maximum depth of the forest to 6 and the maximum number of leaves to 90. These parameters were chosen to alleviate overfitting. Additionally, we used a learning rate of 0.001 and monitored the training evaluation index over 100 iterations of AUC training. The training process was stopped if the AUC value did not increase for more than 5 consecutive iterations, ensuring that the model did not overfit the training data. To further validate the model, we utilized the 10-fold cross-validation method, which helps in assessing the model's performance and generalizability.\n\nIn the logistic regression model, we standardized the input data to speed up the gradient descent and find the optimal solution. The regularization parameter was set to \"L2,\" which helps in preventing overfitting by penalizing large coefficients. We performed 10-fold cross-validation to ensure the model's robustness. The regularization coefficient was set to 20 equal parts ranging from -2 to 2, and the error range of the iteration termination criterion was set to 0.01. These settings helped in balancing the model complexity and preventing both overfitting and underfitting.\n\nOverall, our approach involved careful selection of model parameters, regularization techniques, and cross-validation methods to ensure that the models were neither overfitting nor underfitting the data.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting. For the random forest model, we set specific parameters to control the complexity of the trees. The maximum depth of the forest was limited to 6, and the maximum number of leaves was set to 90. Additionally, the learning rate was set to 0.001, and the training evaluation index was monitored over 100 iterations of AUC training. To further prevent overfitting, the training process was stopped if the AUC value did not increase for more than 5 consecutive iterations.\n\nFor the logistic regression model, we used L2 regularization, also known as ridge regularization, to penalize large coefficients and prevent the model from becoming too complex. The regularization coefficient was set to 20 equal parts ranging from -2 to 2. This approach helped to ensure that the model generalized well to unseen data by avoiding overfitting to the training set.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in this study are reported in detail. For the random forest model, the maximum depth of the forest was set to 6, and the maximum number of leaves was set to 90 to mitigate overfitting. The learning rate was set to 0.001, and the training evaluation index was 100 iterations of AUC training. The training process was stopped when the number of iterations exceeded 5 times without an increase in the AUC value to prevent overfitting. Additionally, 10-fold cross-validation was used to validate the model.\n\nFor the logistic regression model, the input data were standardized to speed up the gradient descent process. The regularization parameter selected was \"L2,\" with 10 folds of cross-validation. The loss function was optimized using the second derivative matrix, and the regularization coefficient was set to 20 equal parts ranging from -2 to 2. The error range for the iteration termination criterion was set to 0.01.\n\nThe specific details of these configurations and schedules are provided within the publication, ensuring reproducibility. However, model files and optimization parameters are not explicitly mentioned as being available for download or further use. The publication does not specify the availability of these files or the license under which they might be shared.",
  "model/interpretability": "The models employed in this study, specifically the random forest and logistic regression models, offer varying degrees of interpretability. The random forest model is often considered a black-box model due to its complexity and the ensemble nature of multiple decision trees. However, it provides insights into feature importance, which can be used to understand the relative significance of different predictors in the model. For instance, in the cognitive impairment prediction models, variables such as education, BMI, and depression were identified as key predictors, indicating their importance in the decision-making process of the random forest model.\n\nOn the other hand, the logistic regression model is more transparent and interpretable. It provides coefficients for each predictor, which can be directly interpreted as the log-odds of the outcome variable changing with a one-unit change in the predictor, holding other variables constant. This transparency allows for a clear understanding of how each variable contributes to the model's predictions. For example, if the coefficient for BMI in the logistic regression model is positive, it indicates that higher BMI is associated with an increased likelihood of cognitive impairment.\n\nIn summary, while the random forest model offers insights through feature importance, the logistic regression model provides more straightforward interpretability through its coefficients. Both models contribute to understanding the predictors of cognitive impairment, with the random forest model excelling in capturing complex relationships and the logistic regression model offering clear, interpretable results.",
  "model/output": "The model employed in this study is primarily a classification model. Both random forest and logistic regression were used to predict cognitive function, which is a categorical outcome. The random forest model, in particular, was utilized to classify cognitive impairment in middle-aged and elderly individuals. The performance of these models was evaluated using the Area Under the Curve (AUC) of the Receiver Operator Characteristic (ROC) curve, a common metric for classification tasks. The AUC values indicated the effectiveness of the models in distinguishing between different cognitive states. For instance, the best-performing model achieved an AUC of 0.81, which is considered good. Additionally, the models were validated using 10-fold cross-validation, further confirming their classification capabilities. The predictor variables, such as education, triglycerides, age, and BMI, were ranked by importance to enhance the classification accuracy. Overall, the focus was on classifying individuals based on their cognitive function, making it a classification model.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the machine learning models employed in this study involved several rigorous steps to ensure the robustness and generalizability of the results. The dataset, consisting of 2,326 participants, was divided into a training set (70%, N = 1,628) and a test set (30%, N = 698). This split allowed for the training of the models on a substantial portion of the data while reserving a separate set for unbiased evaluation.\n\nTo handle missing values, the method of nearest neighbor imputation was utilized. This technique helps to maintain the integrity of the dataset by filling in missing values based on the closest available data points.\n\nFor the random forest model, specific parameters were set to optimize performance and prevent overfitting. The maximum depth of the forest was set to 6, and the maximum number of leaves was set to 90. The learning rate was set to 0.001, and the training evaluation index was based on 100 iterations of the Area Under the Curve (AUC) training. To further prevent overfitting, the training was stopped if the AUC value did not increase for more than 5 consecutive iterations. Additionally, 10-fold cross-validation was used to validate the model, ensuring that it performed well across different subsets of the data.\n\nIn the logistic regression model, input data were standardized to accelerate the gradient descent process in finding the optimal solution. The regularization parameter was set to \"L2,\" and the number of cross-validation folds was set to 10. The loss function was optimized using the second derivative matrix, and the regularization coefficient was set to 20 equal parts ranging from -2 to 2. The iteration termination criterion was set to an error range of 0.01.\n\nThe performance of the models was evaluated using the AUC value in the Receiver Operator Characteristic (ROC) curve. The AUC values were interpreted as follows: an AUC greater than 0.9 was considered very good, 0.8\u20130.90 was considered good, 0.7\u20130.8 was regarded as fair, and less than 0.7 was regarded as poor. The results indicated that the random forest model outperformed the logistic regression model, with the best model achieving an AUC of 0.81. This suggests that the random forest model was more effective in predicting cognitive impairment in middle-aged and elderly people.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our machine learning models in predicting cognitive impairment. The primary metric used was the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve. The AUC provides a comprehensive measure of the model's ability to distinguish between different cognitive function outcomes. We categorized the AUC values to assess model performance: an AUC greater than 0.9 was considered very good, between 0.8 and 0.9 was considered good, between 0.7 and 0.8 was regarded as fair, and less than 0.7 was considered poor.\n\nFor the random forest models, we reported AUC values for different follow-up periods. Specifically, the AUC for the 2-year follow-up model was 0.81, indicating good performance. The 4-year follow-up model had an AUC of 0.79, which is also considered good. Additionally, the cross-sectional verification data model had an AUC of 0.80, further supporting the model's reliability.\n\nWe also compared the performance of the random forest model with logistic regression. The results showed that the random forest model consistently outperformed logistic regression, highlighting its superiority in handling complex, non-linear relationships in the data.\n\nThese performance metrics are widely recognized in the literature and are representative of standard practices in evaluating machine learning models for predictive tasks. The use of AUC, in particular, is a robust method for assessing model performance across various thresholds, providing a clear indication of the model's discriminative power.",
  "evaluation/comparison": "In our study, we compared the performance of machine learning methods, specifically random forest, with traditional statistical methods, such as logistic regression, to predict cognitive impairment in middle-aged and elderly individuals. This comparison was crucial to highlight the advantages of machine learning techniques over conventional methods.\n\nRandom forest demonstrated superior performance compared to logistic regression. The random forest models achieved higher AUC values, indicating better predictive accuracy. For instance, the best-performing random forest model had an AUC of 0.81, which is considered good. In contrast, logistic regression, while previously considered a standard method for binary classification, is limited by assumptions of normality and linear relationships. These limitations make it less effective in evaluating the non-linear and complex relationships between variables, which are often present in cognitive impairment data.\n\nThe random forest method's robustness and high predictive performance were evident in our study. It effectively handled the complexity of the data, including multiple explanatory variables, without overfitting. This was achieved by setting parameters such as the maximum depth of the forest, the maximum number of leaves, and the learning rate to optimize the model's performance.\n\nAdditionally, we employed 10-fold cross-validation to validate the model, ensuring that the results were reliable and generalizable. This rigorous validation process further confirmed the superiority of the random forest method over logistic regression.\n\nIn summary, our comparison showed that machine learning methods, particularly random forest, offer significant advantages over traditional statistical methods like logistic regression in predicting cognitive impairment. The random forest's ability to manage complex data and provide accurate predictions makes it a valuable tool in this field.",
  "evaluation/confidence": "Evaluation Confidence\n\nThe study employed several statistical methods to ensure the confidence of the evaluation results. The random forest and logistic regression models were used to predict cognitive function, with the data divided into training and test sets. The random forest model parameters were carefully set to prevent overfitting, including a maximum depth of 6, a maximum number of leaves of 90, and a learning rate of 0.001. The training evaluation index was set to 100 iterations of AUC training, and the training was stopped if the AUC value did not increase for five consecutive iterations.\n\nFor the logistic regression model, input data were standardized to speed up the gradient descent process. The regularization parameter was set to \"L2,\" with 10-fold cross-validation used to validate the model. The regularization coefficient was set to 20 equal parts from -2 to 2, and the error range of the iteration termination criterion was 0.01.\n\nThe performance of the models was evaluated using the area under the receiver operating characteristic curve (AUC). The AUC values indicated the performance of the models, with AUC > 0.9 considered very good, 0.8\u20130.90 considered good, 0.7\u20130.8 regarded as fair, and <0.7 considered poor. The random forest model achieved an AUC of 0.81 for the 2-year follow-up, 0.79 for the 4-year follow-up, and 0.80 for another model, demonstrating good to fair performance.\n\nStatistical significance was not explicitly mentioned for the performance metrics, but the use of cross-validation and the stopping criteria for the random forest model suggest a rigorous approach to evaluating model performance. The comparison between random forest and logistic regression showed that random forest performed better, indicating the superiority of the machine learning method over the traditional logistic regression approach.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study utilized data from the China Health and Retirement Longitudinal Study (CHARLS), which is a nationally representative longitudinal survey. While CHARLS data is publicly accessible, the specific preprocessed datasets and models used in this study are not released. The data preprocessing process involved merging datasets from 2011, 2013, and 2015, excluding missed follow-up data, and handling variables with missing values. The final sample size was 2,326 participants. The study protocol was approved by the Peking University Biomedical Ethics Committee, and the data was analyzed using SPSS version 26.0 and Python version 3.8. The models employed included random forest and logistic regression, with parameters set to prevent overfitting and ensure robust performance. The evaluation metrics, such as the area under the receiver operating characteristic curve (AUC), are reported in the study, but the raw evaluation files themselves are not provided."
}