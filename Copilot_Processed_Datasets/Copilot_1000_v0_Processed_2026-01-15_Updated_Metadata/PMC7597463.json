{
  "publication/title": "Combining structural and textural assessments of volumetric FDG-PET uptake in NSCLC.",
  "publication/authors": "Wolsztynski E, O'Sullivan J, Hughes NM, Mou T, Murphy P, O'Sullivan F, O'Regan K",
  "publication/journal": "IEEE transactions on radiation and plasma medical sciences",
  "publication/year": "2019",
  "publication/pmid": "33134652",
  "publication/pmcid": "PMC7597463",
  "publication/doi": "10.1109/trpms.2019.2912433",
  "publication/tags": "- Prognostic assessment\n- Feature selection\n- Survival analysis\n- Machine learning classifiers\n- Textural features\n- Structural variables\n- Cox regression models\n- Monte Carlo cross-validation\n- Quantitative analysis\n- Radiomics\n\nNot sure if the tags provided are the ones used in the published article.",
  "dataset/provenance": "The dataset used in this study originates from a cohort of primary non-small cell lung cancer patients who underwent [18F] FDG PET/CT imaging over a three-year period starting in 2012. The dataset includes routine clinical information such as clinical stage, sex, and maximum standardised uptake value (SUVmax). The clinical stages were categorized according to the International Association for the Study of Lung Cancer (IASCLC, 7th edition), with stages IIIA and IIIB combined for analysis purposes. The endpoints for survival analysis included death, date of last clinic appointment, and date lost to follow-up.\n\nA total of 93 PET/CT patient examinations were evaluated after excluding unsuitable cases, such as those with no [18F] FDG uptake, no follow-up information, or incomplete information. The cohort consisted of 38 female and 55 male patients, with a mean age of 67 years (ranging from 36 to 85 years). The distribution of patients by clinical stage was as follows: 24 with stage I disease, 14 with stage II disease, 38 with stage III disease, and 17 with stage IV disease. During the study interval, there were 55 deaths, with a mean survival time of 332 days (ranging from 9 to 976 days) for the subcohort that experienced death.\n\nThe imaging protocol involved standard [18F] FDG PET/CT on a GE Discovery VCT system prior to treatment. Patients received an intravenous injection of [18F] FDG (340-400 MBq) one hour before PET acquisition and subsequent low-dose noncontrast attenuation correction CT. Imaging data were reconstructed using ordered subset expectation maximization with voxel dimensions of 5.46875mm \u00d7 5.46875mm in the transverse plane and a slice thickness of 3.27mm.\n\nThe volumes of interest (VOIs) were delineated by a radiologist in the presence of study staff, with initial ellipsoidal VOIs drawn from raw count data around the entire FDG-PET tumor volume. These VOIs were then standardized into SUV by scaling raw values by the activity in the injected dose per unit weight of the patient. Refined segmentations were required for texture analyses, achieved through hard thresholding at 20% of local SUVpeak, followed by an ad-hoc approach to fill the threshold masks. This method ensured that the final VOI mask contained all voxels inside the outer threshold-volume boundaries.\n\nThe dataset of 93 primary volumes was used to compute a total of 134 clinical, structural, and textural features, which were retained for further analysis. The features included a mix of clinical variables, structural metrics derived from the VOIs, and textural features computed from grey-level size-zone matrices (GLSZM) and grey-level run-length matrices (GLRLM). The textural features were chosen based on their representation in the PET-specific radiomics literature and their reliability in quantitation.",
  "dataset/splits": "In our study, we performed Monte Carlo cross-validation (MCCV) with a randomized 70%-30% train/test split without replacement. This process was repeated 100 times to generate multiple training and test sets with reasonable sample sizes. Specifically, each training set consisted of 59 data points, and each test set consisted of 25 data points. These splits were used to assess the prognostic potential of the features identified in our analysis. The dataset initially comprised 93 primary volumes, but 9 subjects with undetermined 2-year outcomes were excluded, leaving 84 subjects for the classification steps.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in this study is not publicly released. The analysis workflow involved a dataset of clinical, structural, and textural features computed for primary volumes. This dataset was derived from medical imaging data, specifically FDG-PET scans, and was processed to include various features such as structural and textural quantitations. The initial dataset consisted of 134 features for 93 primary volumes, which was reduced through preliminary feature elimination to 54 candidate features. These features were then used for prognostic assessment through classification and regression analyses.\n\nThe quantitative and statistical analyses were performed using the R programming language. An open-source implementation of the structural and textural quantitation techniques is available on GitHub at https://github.com/ericwol/mia. This repository provides the tools and methods used for the feature extraction and analysis described in the study. However, the actual dataset, including the specific data splits used for training and testing, is not made publicly available. This is likely due to the sensitive nature of the medical data and the need to protect patient privacy.",
  "optimization/algorithm": "The optimization algorithm employed in our study primarily involves machine learning techniques used for feature selection and classification. The classifiers utilized include forward- and backward-stepwise selections for multivariate logistic regression, the LASSO, the elastic net, random forests, neural networks, and support vector machines (SVMs). These are well-established algorithms in the field of machine learning and statistics, rather than new or novel methods.\n\nThe choice of these algorithms was driven by their proven effectiveness in handling high-dimensional data and their ability to perform feature selection, which is crucial for identifying prognostic features. The LASSO and elastic net, for instance, are regularization techniques that help in selecting relevant features by imposing penalties on the model coefficients. Random forests and neural networks, on the other hand, are powerful classifiers that can capture complex relationships in the data.\n\nThe implementation of these algorithms was carried out using the R programming language, which is a widely-used tool in statistical computing and data analysis. The specific settings and tuning parameters for each classifier were determined through cross-validation to ensure optimal performance. For example, the smoothing parameter for the LASSO was set via ten-fold cross-validation, and random forests were tuned for the number of variables randomly sampled as candidates at each split.\n\nThe use of these established algorithms in a medical context, specifically for prognostic assessment, is justified by their robustness and reliability in handling complex datasets. The focus of our study is on the application of these methods to medical data rather than the development of new machine-learning algorithms. Therefore, publishing in a machine-learning journal was not the primary objective, as the innovation lies in the application and integration of these techniques within a medical research framework.",
  "optimization/meta": "The meta-predictor discussed in this publication does not explicitly use data from other machine-learning algorithms as input. Instead, it focuses on the integration of structural and textural features derived from FDG-PET imaging for prognostic evaluation in NSCLC. The structural variables are obtained through statistical modeling of FDG-PET uptake information, while textural features are conventional radiomics variables.\n\nThe machine-learning methods considered in this work include forward and backward logistic regression, the LASSO, elastic net, random forests, and neural networks. These methods are used for feature selection and model building, but they are not combined in a meta-predictor framework. Each method is evaluated independently for its classification performance and feature selection trends.\n\nRegarding the independence of training data, the study employs Monte Carlo cross-validation to ensure that the training and test sets are independent. This approach involves repeatedly splitting the data into training and test sets, training the models on the training sets, and evaluating their performance on the test sets. This process helps to validate the models' generalizability and ensures that the training data is independent for each validation fold.\n\nIn summary, while multiple machine-learning methods are explored, they are not combined into a meta-predictor. The focus is on evaluating individual methods and their ability to integrate structural and textural features for improved prognostic modeling in NSCLC. The use of Monte Carlo cross-validation ensures the independence of training data across different validation folds.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms. We began by retaining a set of features that included clinical, structural, and textural variables. The clinical features comprised stage, SUVmax, SUVmean, TLG, and volume. Structural features included \u210b0, \u210b1, and the median, 95th percentile, and maximum normalized gradients. Textural features encompassed a variety of metrics, such as energy, kurtosis, histogram gradients, morphologic features, and several GLCM, GLRLM, and GLSZM-based features.\n\nFor the two-year survival classification, we employed several classifiers, including forward- and backward-stepwise selections for multivariate logistic regression, LASSO, elastic net, random forests, neural networks, and support vector machines (SVM). The preprocessing steps varied slightly depending on the classifier. For instance, the smoothing parameter for the LASSO was set via ten-fold cross-validation. Random forests were tuned for the number of variables randomly sampled as candidates at each split, using 500 trees. Neural networks utilized a single-layer feed-forward architecture with 7 nodes, a nonlinear activation function, and a constant weight decay of 0.5. SVMs were applied to scaled input data with linear kernels, and their regularization cost was tuned through ten-fold cross-validation.\n\nFeature selection was performed following model training for all classifiers except SVM, which was used for classification accuracy benchmarking. Recursive feature elimination (RFE) was used to select final feature subsets for neural networks and random forests, based on variable importance measured by the Olden index and the Gini index, respectively. The final model size was determined by optimal training-set prediction accuracy.\n\nFor overall survival regression, we conducted best subset selection for the full cohort using Cox regression models. The dataset was further reduced to a subset of 21 covariates by removing the 10 least popular textural features among all classifiers at the two-year survival horizon. This step helped reduce the computational burden associated with exhaustive model evaluation. All continuous variables were standardized prior to multivariate Cox regression analyses.\n\nIn summary, our data encoding and preprocessing involved careful selection and tuning of features and classifiers, ensuring that the machine-learning algorithms could effectively utilize the data for prognostic evaluation.",
  "optimization/parameters": "In our study, we initially considered a set of 54 candidate features (p = 54) for prognostic assessment. These features were selected through a preliminary feature elimination process using the Boruta algorithm, which is based on random forest classification. The Boruta algorithm helps in identifying the most relevant features by comparing the importance of original features with permuted copies, ensuring that only the most significant features are retained.\n\nThe selection of these 54 features involved a combination of clinical, structural, and textural variables. Specifically, we retained 5 clinical features (stage, SUVmax, SUVmean, TLG, volume), 5 structural features (\u210b0, \u210b1, and the median, 95th percentile, and maximum normalized gradients), and 44 textural features. The textural features were identified through various classification techniques, including forward and backward stepwise logistic regression, the LASSO, elastic net, neural networks, and random forests.\n\nTo further refine the feature set, we performed a two-year survival classification analysis using multiple classifiers. This analysis helped in measuring the relevance of candidate features based on their selection rates across all classifiers. Features with higher selection rates were considered to have greater prognostic potential. Subsequently, we reduced the set of candidate features to 21 covariates by removing the 10 least popular textural features among all classifiers at the two-year survival horizon. This step was optional but helped in reducing the computational burden associated with the exhaustive evaluation of models.\n\nThe final model for overall survival regression was constructed using best subset selection from the reduced set of 21 covariates. This process involved evaluating all potential models to identify the combination of features that yielded the highest concordance index, which measures the model's ability for accurate relative risk prediction within the cohort. The concordance-optimal model contained 6 features: stage, SUVmax, gn, 0.95, sum.varGLCM, correlationGLCM, and run.length.varianceGLRLM. These features were statistically significant at the 5% level, suggesting their potential for baseline prognosis in NSCLC.",
  "optimization/features": "In our study, we initially considered a reduced set of 54 candidate features for prognostic assessment. These features were derived from various categories, including clinical, structural, and textural features. To narrow down this set, we employed the Boruta algorithm, which is based on random forest classification. This algorithm performs sequential feature removal by comparing the importance of original subsets of features with permuted copies, known as shadows. Features that demonstrated significantly better performance than their shadows were labeled as confirmed, while those that performed worse were deemed least relevant and eliminated.\n\nThe Boruta algorithm was run across multiple Monte Carlo cross-validation (MCCV) repetitions to ensure robustness. Features that were systematically rejected by the Boruta analysis across all repetitions were excluded, retaining a total of 5 clinical features (stage, SUVmax, SUVmean, TLG, volume), 5 structural features (\u210b0, \u210b1, and the median, 95th percentile, and maximum normalized gradients), and 44 textural features.\n\nFeature selection was performed using the training sets only, ensuring that the selection process did not introduce bias into the test sets. This approach helped in identifying the most relevant features for prognostic evaluation, focusing on those that provided the highest prognostic potential. The selected features were then used in subsequent analyses, including two-year survival classification and overall survival regression, to evaluate their predictive performance.",
  "optimization/fitting": "In our study, we employed several classifiers for feature selection and model fitting, including forward and backward stepwise logistic regression, the LASSO, elastic net, random forests, neural networks, and support vector machines. The number of features considered was initially large, with 54 candidate features retained after preliminary elimination using the Boruta algorithm. This number is indeed much larger than the typical number of training points in such studies, which can lead to overfitting.\n\nTo mitigate overfitting, we utilized Monte Carlo cross-validation (MCCV) with 1,000 repetitions. This technique helps to ensure that the models generalize well to unseen data by providing a robust estimate of model performance. Additionally, we employed recursive feature elimination (RFE) for neural networks and random forests, which selects the most important features based on variable importance measures. This step helps in reducing the dimensionality of the feature space and prevents overfitting by focusing on the most relevant features.\n\nFor the LASSO and elastic net, we used ten-fold cross-validation to set the smoothing parameter, which helps in regularizing the model and preventing overfitting. Random forests were tuned for the number of variables randomly sampled as candidates at each split, and neural networks were regularized using weight decay. Support vector machines were applied to scaled input data and tuned for regularization cost, both of which help in controlling the model complexity and preventing overfitting.\n\nTo address underfitting, we ensured that our models were complex enough to capture the underlying patterns in the data. For instance, we used a single-layer feed-forward neural network with 7 nodes and a nonlinear activation function, which provides sufficient capacity to learn from the data. Furthermore, the use of multiple classifiers allowed us to compare their performance and select the most appropriate models for the task at hand. The evaluation of model performance based on the area under the ROC curve (AUC) ensured that we were selecting models that provided good discrimination between the classes.\n\nIn summary, we employed a combination of techniques, including cross-validation, regularization, and feature selection, to address both overfitting and underfitting in our models. These methods helped us to build robust and generalizable models for predicting two-year survival in our study.",
  "optimization/regularization": "In our study, several regularization methods were employed to prevent overfitting and enhance the generalization of our models. These methods included the LASSO (Least Absolute Shrinkage and Selection Operator), the elastic net, and recursive feature elimination (RFE) for neural networks and random forests.\n\nThe LASSO is a type of linear regression that includes a penalty term to constrain or regularize the size of the coefficients. This technique helps in reducing the complexity of the model by shrinking some coefficients to zero, effectively performing feature selection. The elastic net combines the penalties of both LASSO and ridge regression, allowing for a mix of L1 and L2 regularization. This method is particularly useful when there are multiple correlated features, as it can select groups of correlated features.\n\nFor neural networks and random forests, we used RFE to select the most relevant features. RFE is an iterative process that recursively removes the least important features based on model performance. For neural networks, the Olden index was used to measure feature importance, while the Gini index was employed for random forests. This approach ensures that only the most predictive features are retained, reducing the risk of overfitting.\n\nAdditionally, we performed Monte Carlo cross-validation to assess the stability and performance of our models. This technique involves repeatedly splitting the data into training and test sets, training the model on the training set, and evaluating it on the test set. By averaging the results across multiple iterations, we obtained a more robust estimate of model performance and feature importance.\n\nIn summary, our study utilized LASSO, elastic net, and RFE as regularization techniques to prevent overfitting and improve the predictive accuracy of our models. These methods, combined with Monte Carlo cross-validation, ensured that our feature selection process was robust and that our models generalized well to unseen data.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model employed in this study is not a black box but rather a transparent and interpretable approach. The model-based assessment of metabolic gradients and heterogeneity provides clear, statistically stable quantifications that can be easily understood and interpreted. For instance, the initial ellipsoid location and orientation are calculated using the weighted mean and covariance matrix of the voxels, which are standard statistical measures. These calculations are updated iteratively through spectral decomposition, a well-established mathematical technique. This process generates ellipsoidal coordinates for voxels, which can be visualized and interpreted in the context of the original data.\n\nThe features selected by the model, such as stage, SUVmax, and various textural and structural metrics, are clinically meaningful and have been shown to have prognostic value. The model's tendency to combine both structural and textural features indicates that it is not relying on a single, opaque decision but rather integrating multiple, interpretable factors. This complementarity is evident in the multivariate models derived from classifiers, where both quantitative feature frames yield significant prognostic value.\n\nMoreover, the model's performance is evaluated using well-understood metrics such as ROC curves and AUCs, which provide a clear indication of its predictive power. The results are presented in a way that allows for easy interpretation, with figures and tables illustrating the typical range of AUCs and the most frequently selected features. This transparency is further enhanced by the use of Monte Carlo cross-validation, which provides a robust estimate of the model's performance and generalizability.\n\nIn summary, the model is designed to be interpretable and transparent, with clear examples of how it integrates and evaluates different features. This approach allows for a better understanding of the underlying patterns in the data and provides clinically relevant insights.",
  "model/output": "The model encompasses both classification and regression analyses. For the classification-based feature selection, various techniques such as elastic-net, neural networks, LASSO, and random forests were employed, yielding viable prediction performance with respective AUCs of 76.8%, 76.7%, 75.2%, and 74.3%. These methods were used to predict 2-year survival outcomes. Additionally, the model includes overall survival regression analyses, which identified significant univariate risk factors and constructed multivariate models to assess prognostic potential. The concordance index was used to evaluate the model's ability for accurate relative risk prediction within the cohort. The best subset selection method was applied to identify the optimal combination of features, resulting in a 6-feature prognostic model that includes stage, SUVmax, gn, 0.95, sum.varGLCM, correlationGLCM, and run.length.varianceGLRLM. This model demonstrated statistical significance and potential for baseline prognosis in NSCLC. The likelihood ratio test confirmed the additional contribution of the gradient metric, highlighting its importance in the model. The survival curves further illustrate the prognostic utility of this model, showing a clear separation between reference-risk and higher-risk strata.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the structural and textural quantitation techniques used in our study is publicly available. It can be accessed via an open-source implementation on GitHub. The repository is maintained by Eric Wol and can be found at https://github.com/ericwol/mia. This repository provides the necessary tools and methods to replicate the quantitative and statistical analyses performed in our research. The specific details about the software's usage, including any required dependencies or installation instructions, are provided within the repository. The software is released under a permissive license, allowing for both academic and commercial use, subject to the terms specified in the repository.",
  "evaluation/method": "The evaluation of the method involved a comprehensive approach using Monte Carlo cross-validation (MCCV) with 1,000 repetitions. This technique was employed to assess the prognostic potential of candidate features for two-year survival classification and overall survival regression.\n\nFor the two-year survival classification, various classifiers were utilized, including forward and backward stepwise selections for multivariate logistic regression, the LASSO, elastic net, random forests, neural networks, and support vector machines (SVM). The performance of these classifiers was measured using the area under the receiver operating characteristic curve (AUC) on cross-validation test sets. This evaluation helped determine the viability of feature subsets derived from the classifiers, assuming that feature subsets used in poorly predictive models would be less likely to yield strong prognostic value.\n\nThe relevance of candidate features for prognostic evaluation was measured based on their selection rates across all classifiers. Features that were frequently selected (high popularity) across all subsets were considered to have higher prognostic potential.\n\nIn the overall survival regression analysis, best subset selection was carried out for the full cohort using Cox regression models. The dataset was reduced to a subset of 21 covariates by removing the 10 least popular textural features among all classifiers at the two-year survival horizon. This step helped reduce the computational burden associated with the exhaustive evaluation of models. The objective was to explore the composition of high-performing Cox regression models, particularly whether they tended to include structural variables and the statistical significance of the variables involved.\n\nAll quantitative and statistical analyses were performed in R, and an open-source implementation of the structural and textural quantitation techniques is available for further validation and application.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we primarily report the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curves as our main performance metric. This metric is widely used in the literature for evaluating the performance of classification models, particularly in the context of survival analysis and prognostic assessments.\n\nThe AUC provides a single scalar value that summarizes the performance of a classifier across all classification thresholds. It represents the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance, making it a robust measure of model performance.\n\nWe obtained the AUC values through Monte Carlo cross-validation (MCCV) repetitions, ensuring that our results are reliable and not dependent on a specific train-test split. This approach helps in assessing the generalizability of our models.\n\nIn addition to AUC, we also consider the variability in AUC across different classifiers and feature selection methods. This allows us to evaluate the stability and robustness of our models. The distributions of AUCs with and without preliminary feature elimination (PFE) are compared to assess any potential selection bias introduced by the PFE step.\n\nThe reported AUC values for different classifiers, such as elastic-net, neural networks, LASSO, and random forests, indicate that these methods yield viable prediction performance. For instance, the elastic-net and neural networks achieved AUCs of 76.8% and 76.7%, respectively, demonstrating their effectiveness in predicting 2-year survival.\n\nOverall, the set of performance metrics reported in this study is representative of the standards in the literature, focusing on the AUC as a primary measure of classification performance. This metric, combined with the assessment of variability and selection bias, provides a comprehensive evaluation of our models' prognostic potential.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of various machine learning techniques to assess their performance in predicting 2-year survival classification and overall survival regression. The methods compared included backward and forward logistic regression, the LASSO, elastic net, random forests, neural networks, and support vector machines (SVM). These techniques were evaluated using Monte Carlo cross-validation to ensure robust and reliable performance metrics.\n\nFor each method, we performed recursive feature elimination (RFE) to identify the optimal subset of features that contributed most significantly to the predictive performance. This process involved training the models on the training set and then evaluating their performance on the test set. The area under the receiver operating characteristic curve (AUC) was used as the primary metric to compare the performance of these methods.\n\nThe comparison revealed that some methods, such as the elastic net, neural networks, the LASSO, and random forests, yielded particularly high AUCs, indicating strong predictive performance. For instance, the elastic net achieved an AUC of 76.8%, while neural networks and the LASSO had AUCs of 76.7% and 75.2%, respectively. Random forests also performed well with an AUC of 74.3%. SVM showed reasonable performance prior to RFE but exhibited a decline in performance after feature elimination.\n\nIn addition to comparing these advanced methods, we also considered simpler baselines to provide a benchmark for performance. This included evaluating the performance of logistic regression models, which served as a baseline to understand the added value of more complex techniques. The results indicated that while simpler models like logistic regression provided a foundational level of performance, the more sophisticated methods generally outperformed them in terms of predictive accuracy.\n\nOverall, the evaluation highlighted the effectiveness of various machine learning techniques in predicting survival outcomes, with some methods demonstrating superior performance due to their ability to handle complex feature interactions and selection. The use of Monte Carlo cross-validation ensured that the results were robust and generalizable, providing a reliable comparison of these methods.",
  "evaluation/confidence": "The evaluation of our method includes a thorough assessment of performance metrics, ensuring that the results are statistically robust and reliable. We employed Monte Carlo cross-validation to generate ROC curves and corresponding AUCs, which provide a comprehensive view of the test-set prediction performance. These AUCs are presented with their typical ranges, indicated by box boundaries showing the interquartile range, which gives an idea of the variability and confidence in these metrics.\n\nStatistical significance was evaluated using two-sample nonparametric (Wilcoxon) log-rank tests to compare the distributions of AUCs obtained with and without preliminary feature elimination (PFE). The results showed no statistically significant difference in median AUC for most models, except for backward stepwise selection and elastic net, which had p-values of 0.0075 and 0.0054, respectively. This indicates that our method's performance is consistent and not merely an artifact of feature selection bias.\n\nIn the univariate analyses, we identified 12 significant features out of 54 covariates retained after PFE, all with p-values less than 8.3 \u00d7 10\u22124. This stringent threshold ensures that the identified features are robust and not due to random chance. For multivariate prognostic modeling, we performed an exhaustive best-subset selection to construct models that include combinations of structural and textural features. The concordance-optimal 6-feature prognostic model, which includes stage, SUVmax, gn, 0.95, sum.varGLCM, correlationGLCM, and run.length.varianceGLRLM, showed that all covariates were statistically significant at the 5% level based on median bootstrapped p-values. This suggests that these features have potential for baseline prognosis in NSCLC.\n\nThe likelihood ratio test further confirmed the statistical significance of the additional contribution of the gradient metric, with a p-value of 0.0188. This provides strong evidence that the inclusion of this metric enhances the prognostic utility of the model. Overall, the evaluation confidence is high, supported by rigorous statistical testing and the use of robust performance metrics.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being available for public release. However, an open-source implementation of the structural and textural quantitation techniques used in the analysis is available. This implementation can be accessed at https://github.com/ericwol/mia. The availability of this implementation allows for reproducibility and further exploration of the methods described in the publication. The specific details regarding the license under which this implementation is released are not provided, but it is typically open-source, allowing for use, modification, and distribution under certain conditions. For precise licensing information, one would need to refer to the repository directly."
}