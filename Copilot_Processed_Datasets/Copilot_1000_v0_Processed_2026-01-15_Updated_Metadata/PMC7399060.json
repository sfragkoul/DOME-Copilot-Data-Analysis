{
  "publication/title": "Weakly Supervised Learning of 3D Deep Network for Neuron Reconstruction.",
  "publication/authors": "Huang Q, Chen Y, Liu S, Xu C, Cao T, Xu Y, Wang X, Rao G, Li A, Zeng S, Quan T",
  "publication/journal": "Frontiers in neuroanatomy",
  "publication/year": "2020",
  "publication/pmid": "32848636",
  "publication/pmcid": "PMC7399060",
  "publication/doi": "10.3389/fnana.2020.00038",
  "publication/tags": "- Neuron reconstruction\n- Weakly supervised deep learning\n- Precise\n- Generalization\n- Automatic\n- 3D deep convolutional neural networks\n- Neurite segmentation\n- Neuronal images\n- Deep learning\n- Image processing",
  "dataset/provenance": "The datasets used in our study were sourced from various 3D optical neuronal datasets, including the fMOST datasets, BigNeuron, and Diadem Challenge datasets. The fMOST datasets provided brain-scale long-projection neuron images at a terabyte scale, with a voxel size of 0.2 \u00d7 0.2 \u00d7 1 \u00b5m. For training, we used 35 image stacks of 300 \u00d7 300 \u00d7 300 volume from different brain regions. The testing set included 15 challenging image stacks of the same volume, five image stacks of 1000 \u00d7 1000 \u00d7 300, and a large image of approximately 140 Gigabytes with a volume of 9620 \u00d7 3780 \u00d7 2100.\n\nThe BigNeuron and Diadem datasets comprised neuronal image stacks from different organizations, varying in types, scales, and sizes. These datasets were used to validate the generalization and accuracy of our proposed method using transfer learning. A larger neuronal image from these datasets, with a volume of 2111 \u00d7 3403 \u00d7 291 and a voxel size of 0.18 \u00d7 0.18 \u00d7 0.5 \u00b5m, was used for fine-tuning. Additionally, six image stacks with voxel sizes ranging from 0.31\u20131, 0.31\u20131, and 0.54\u20133.4 \u00b5m were used for evaluation.\n\nThe Diadem and BigNeuron communities provided the public datasets used in our study. These datasets have been utilized in previous research and by the community for neuronal image analysis and reconstruction tasks.",
  "dataset/splits": "The dataset used in our study consists of 3D optical neuronal images, which were split into training and testing sets. For the training set, we selected 35 image stacks of 300 \u00d7 300 \u00d7 300 volume from different brain regions. These images included neuronal images with different neurite appearances, such as neurites with high, low, or inhomogeneous intensities, and structures like straight or twisted neurites with varied radius.\n\nFor the testing set, we used 15 challenging image stacks of volume 300 \u00d7 300 \u00d7 300, five image stacks of volume 1000 \u00d7 1000 \u00d7 300, and a large image of approximately 140 Gigabytes with a volume of 9620 \u00d7 3780 \u00d7 2100. Additionally, we evaluated our method on public datasets like BigNeuron and Diadem, which comprised neuronal image stacks from different organizations of various types, scales, and sizes. For these public datasets, we used a larger neuronal image of volume 2111 \u00d7 3403 \u00d7 291 and six image stacks with voxel sizes ranging from 0.31\u20131, 0.31\u20131, and 0.54\u20133.4 \u00b5m for evaluation.",
  "dataset/redundancy": "The datasets used in our study were split into training and testing sets to evaluate the performance of our proposed method. For the fMOST datasets, we selected image stacks from brain-scale mouse neuronal images with a voxel size of 0.2 \u00d70.2 \u00d71 \u00b5m. The training set included 35 image stacks of 300 \u00d7300 \u00d7300 volume from different brain regions. The testing set included 15 challenging image stacks of volume 300 \u00d7300 \u00d7300, five image stacks of volume 1000\u00d71000 \u00d7300, and a large image of volume 9620 \u00d73780 \u00d72100.\n\nThe BigNeuron and Diadem datasets comprised neuronal image stacks from different organizations of various types, scales, and sizes. We validated the generalization and accuracy of the proposed method on these public datasets using transfer learning. In our weakly supervised training process, manual tracing results of the public datasets were not used. A larger neuronal image from the datasets of volume 2111 \u00d73403 \u00d7291 and voxel size of 0.18 \u00d70.18 \u00d70.5 \u00b5m was used for fine-tuning, and six image stacks with voxel sizes of 0.31\u20131, 0.31\u20131, and 0.54\u20133.4 \u00b5m were used for evaluation.\n\nThe training and test sets were independent, ensuring that the model's performance was evaluated on unseen data. This independence was enforced by selecting image stacks from different brain regions and ensuring that the testing set included more challenging and larger volumes than the training set. The distribution of the datasets used in our study is comparable to previously published machine learning datasets in the field of neuronal imaging, which often involve large-scale, high-resolution images with varied intensities and structures. The use of transfer learning further ensured that our method could generalize well to different types of datasets, addressing the challenges faced by current methods for precise tracing on diverse neuronal datasets.",
  "dataset/availability": "The datasets generated for this study are not publicly available. They are available on request to the corresponding author. This approach ensures that the data is shared responsibly and that any use complies with the necessary permissions and ethical considerations. The datasets include various 3D optical neuronal images, such as those from the fMOST datasets, BigNeuron, and Diadem Challenge datasets. These datasets were used to evaluate the proposed method and several best available tracing algorithms. The fMOST datasets, for instance, include brain-scale long-projection neuron images at terabyte scale, while the BigNeuron and Diadem datasets comprise neuronal image stacks from different organizations of various types, scales, and sizes. The datasets were used to validate the generalization and accuracy of the proposed method using transfer learning. The datasets were carefully selected to include neuronal images with different neurite appearances and structures, ensuring a comprehensive evaluation of the method's performance.",
  "optimization/algorithm": "The optimization algorithm employed in our work is based on a convolutional neural network (CNN) architecture, specifically a 3D residual CNN named VoxResNet. This class of algorithms is well-established in the field of deep learning and has been widely used for various image processing tasks, including segmentation.\n\nThe machine-learning algorithm used is not entirely new; it builds upon existing CNN architectures and techniques. However, the specific application and modifications made to the algorithm for neurite segmentation in 3D optical images with low signal-to-noise ratio (SNR) are novel. The algorithm incorporates several key innovations:\n\n1. **Weakly Supervised Learning**: The method uses initial training labels generated automatically through tracing methods and neurite shape characteristics, rather than relying on manual annotations. This approach addresses the scarcity of manually annotated training samples, which is a common limitation in deep learning applications.\n\n2. **Iterative Refinement**: The training process involves iteratively updating the training labels by mining more weak neurites from the CNN-predicted probability map. This iterative refinement helps improve the accuracy of the model by progressively enhancing the training labels.\n\n3. **Hybrid Loss Function**: To mitigate the influence of class imbalance and preserve region continuity, a hybrid loss function combining weighted cross-entropy loss and dice loss is used. This loss function helps the model to better handle the unbalanced foreground and background classes in the training data.\n\nThe reason this work was published in a neuroanatomy journal rather than a machine-learning journal is that the primary focus is on the application of deep learning techniques to solve a specific problem in neuroanatomy\u2014namely, the automatic reconstruction of neurons from 3D optical images. The innovations lie in the application and adaptation of existing machine-learning algorithms to this particular domain, rather than in the development of entirely new machine-learning algorithms. The contributions are more aligned with the goals and audience of a neuroanatomy journal, which emphasizes the practical application of advanced techniques to biological and medical problems.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It is a standalone weakly supervised deep learning method designed for automatic neuron reconstruction in various 3D optical images with low signal-to-noise ratio (SNR). The method utilizes a 3D residual convolutional neural network (CNN) to iteratively refine training labels by mining weak neurites, thereby improving the accuracy of neuron tracing.\n\nThe training process involves an initial automatic tracing method to generate initial training labels, which are then iteratively updated. This iterative process helps in refining the 3D CNN model based on the tubularity and continuity of neurites. The model does not rely on data from other machine-learning algorithms as input; instead, it uses automatically constructed training labels and an iterative refinement process.\n\nThe method is evaluated using various datasets, including fMOST, BigNeuron, and Diadem datasets, to demonstrate its generalization across different types of neuronal images. Transfer learning is applied to adapt the model to new datasets without the need for manual annotation, ensuring that the training data remains independent and automatically generated.",
  "optimization/encoding": "In our study, we utilized three-dimensional patches of 120 \u00d7 120 \u00d7 120 or 64 \u00d7 64 \u00d7 64 volumes as training samples, selected randomly from the training images. This selection considered the input image size, computational cost, and segmentation accuracy. Patches containing fewer than a specified threshold of positive pseudo-labels were removed to eliminate blank patches. The patches were then normalized to have zero mean and unit variance. To enhance the robustness of the network, we performed data augmentation techniques such as random rotation, flipping, contrast and brightness adjustment, and Gaussian blur. These preprocessing steps ensured that the data was well-prepared for the machine-learning algorithm, facilitating better training and generalization.",
  "optimization/parameters": "In our model, the number of parameters is determined by the architecture of the VoxResNet, which includes 21 convolutional layers and four deconvolutional layers. The convolutional layers have varying numbers of channels, starting with 32 channels in the first two layers and increasing to 64 channels in subsequent layers. The kernel size for these convolutional layers is 3 \u00d7 3 \u00d7 3. Additionally, the network incorporates residual modules and batch normalization layers, which also contribute to the total number of parameters.\n\nThe selection of these parameters was guided by the need to balance computational efficiency and segmentation accuracy. The initial learning rate was set to 0.01 and decreased by half every four epochs. The batch size was set to 3, the momentum to 0.9, and the weight decay to 0.0005. These hyperparameters were chosen based on empirical observations and previous studies to ensure effective training and generalization of the model. The maximum number of training epochs was set to 100, allowing the model sufficient time to converge.\n\nData augmentation techniques, such as random rotation, flipping, contrast and brightness adjustment, and Gaussian blur, were applied to enhance the robustness of the network. These techniques help the model generalize better to unseen data by exposing it to a variety of transformed inputs during training. The use of stochastic gradient descent optimization further aids in efficient learning and convergence.",
  "optimization/features": "The input features for the proposed method are three-dimensional patches of neuronal images. These patches are randomly chosen from the training images and have sizes of either 120 \u00d7 120 \u00d7 120 or 64 \u00d7 64 \u00d7 64 volumes. The selection of these patch sizes considers the input image size, computational cost, and segmentation accuracy.\n\nFeature selection is implicitly performed by removing patches that contain few voxels of positive pseudo-labels. Specifically, patches with fewer than a certain threshold of positive pseudo-labels are eliminated to avoid blank patches. This threshold is defined as \u03b11V, where V is the total number of voxels in a patch, and \u03b11 is a ratio set to 0.001.\n\nThe feature selection process is conducted using the training set only, ensuring that the model generalizes well to unseen data. This approach helps in focusing the training on relevant and informative patches, thereby improving the overall performance of the neurite segmentation.",
  "optimization/fitting": "In our study, we employed a weakly supervised deep learning approach for neurite segmentation and tracing, which inherently involves a large number of parameters due to the complexity of the neural networks used. The number of parameters in our convolutional neural networks (CNNs) was indeed much larger than the number of training points, especially when using a limited number of manually annotated samples.\n\nTo address the risk of overfitting, we implemented several strategies. Firstly, we utilized an iterative mining process for training label refinement. This process allowed the network to progressively improve its performance by refining the training labels, which helped in reducing overfitting to the initial small set of annotated samples. Secondly, we evaluated the model's performance using various metrics such as neuron distances, recall, and precision, which provided a comprehensive assessment of the model's generalization capability. Additionally, we compared our method with a general deep learning (GDL) method that used manually annotated labels, demonstrating that our weakly supervised approach could achieve comparable performance without extensive manual annotation.\n\nTo ensure that our model was not underfitting, we conducted experiments with different quantities of manual annotations. We observed that as the number of training samples increased, the model's performance improved, indicating that the network was capable of learning from the data. Furthermore, we evaluated the model on diverse datasets, including the fMOST, BigNeuron, and Diadem datasets, which have varied appearances and volumes. The consistent performance across these datasets suggested that our model was generalizing well and not underfitting.\n\nIn summary, by using an iterative mining process, evaluating with multiple metrics, and testing on diverse datasets, we mitigated the risks of both overfitting and underfitting in our weakly supervised deep learning method for neurite segmentation and tracing.",
  "optimization/regularization": "In our work, we employed several techniques to prevent overfitting and improve the robustness of our model. One of the key methods used was batch normalization. This technique involves normalizing the inputs of each layer for each mini-batch, which helps to stabilize and accelerate the training process. By reducing the internal covariate shift, batch normalization allows for higher learning rates and acts as a form of regularization, thereby mitigating overfitting.\n\nAdditionally, we utilized data augmentation techniques to enhance the diversity of our training data. This included random rotations, flips, contrast and brightness adjustments, and Gaussian blur. These augmentations help the model generalize better by exposing it to a wider variety of input variations, making it less likely to overfit to the specific characteristics of the training data.\n\nWe also implemented a hybrid loss function that combines weighted cross-entropy loss and dice loss. This hybrid approach helps to address class imbalance, which is a common issue in neurite segmentation due to the small fraction of foreground (neurites) compared to the background. By balancing the contributions of both loss functions, we ensure that the model pays adequate attention to both classes, further reducing the risk of overfitting.\n\nLastly, we employed stochastic gradient descent (SGD) optimization with a decaying learning rate. The learning rate was initially set to 0.01 and decreased by half every four epochs. This gradual reduction helps the model to converge more smoothly and avoids the sharp fluctuations that can lead to overfitting. The use of momentum in SGD also aids in accelerating convergence and navigating local minima, contributing to a more robust training process.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule are reported in the publication. The initial learning rate is set to 0.01 and is decreased by half every four epochs. The batch size is 3, the momentum is 0.9, and the weight decay is 0.0005. The maximum number of training epochs is set to 100. Stochastic gradient descent optimization is applied. The model architecture, including the sizes of convolutional and deconvolutional layers, is detailed in the publication.\n\nThe model files and optimization parameters are not explicitly provided in the publication. However, the method was implemented using the PyTorch library, and the code is available under the Creative Commons Attribution License (CC BY). This license permits use, distribution, or reproduction in other forums, provided the original authors and the copyright owner are credited, and the original publication in this journal is cited.\n\nThe publication also mentions the use of a hybrid loss function combining weighted cross-entropy loss and dice loss to mitigate class imbalance and preserve region continuity. The specific formulas for these loss functions are provided, along with the values of the parameters used in these formulas.\n\nNot applicable",
  "model/interpretability": "The model employed in our study is a 3D deep voxelwise residual network (VoxResNet) designed for neurite segmentation. This network architecture is not a black box; it is designed to be interpretable through several key features.\n\nThe VoxResNet architecture includes 21 convolutional layers and four deconvolutional layers, which are structured to extract features from different receptive field sizes. The use of convolutional layers with a kernel size of 3 \u00d7 3 \u00d7 3 allows for the capture of local patterns in the 3D images, making it easier to understand how specific features are detected.\n\nOne of the primary reasons for the interpretability of this model is the integration of deep residual learning. This concept facilitates the training process by allowing the network to learn residual functions, which are easier to optimize. The residual modules help in maintaining the gradient flow through the network, ensuring that deeper layers can learn more complex features without degrading the performance of shallower layers.\n\nThe network also employs batch normalization and rectified linear unit (ReLU) activation functions, which help in stabilizing and accelerating the training process. Batch normalization normalizes the inputs of each layer, making the learning process more consistent and interpretable.\n\nAdditionally, the use of deconvolutional layers helps in reconstructing the segmented neurites from the feature maps, providing a clear understanding of how the network translates the extracted features back into the original image space.\n\nThe architecture of the network is illustrated in a detailed figure, showing the sizes of the convolutional and deconvolutional layers, as well as the structure of the residual modules. This visual representation aids in understanding the flow of data through the network and how different layers contribute to the final segmentation output.\n\nOverall, the VoxResNet architecture is designed to be transparent, with clear examples of how each component contributes to the segmentation process. The use of residual learning, batch normalization, and detailed visual representations of the network architecture all contribute to the interpretability of the model.",
  "model/output": "The model is a classification model designed for neurite segmentation. It employs a 3D deep voxelwise residual network, specifically VoxResNet, which is tailored for image segmentation tasks. The network architecture includes convolutional and deconvolutional layers, with a softmax layer at the end to normalize the output probabilities between 0 and 1. This setup allows the model to classify each voxel in the input image as either part of a neurite (foreground) or not (background).\n\nThe model uses a hybrid loss function that combines weighted cross-entropy loss and dice loss to handle class imbalance and ensure region continuity. The cross-entropy loss helps in classifying the voxels correctly, while the dice loss focuses on the overlap between the predicted and actual segmentation, which is crucial for maintaining the structural integrity of the neurites.\n\nDuring training, the model processes 3D patches of varying sizes, with data augmentation techniques applied to enhance robustness. The training process involves stochastic gradient descent optimization, with parameters like learning rate, batch size, momentum, and weight decay carefully tuned to achieve optimal performance. The final output of the model is a probability map indicating the likelihood of each voxel belonging to a neurite, which is then used for further refinement and segmentation.",
  "model/duration": "The model was implemented using C++ and Python 3.6 with the PyTorch library. The segmentation network was trained and evaluated on a computer equipped with an Intel i7-6850K CPU, 64 GB of RAM, and two NVIDIA 1080Ti GPUs. The training process involved stochastic gradient descent optimization with an initial learning rate of 0.01, which was halved every four epochs. The batch size was set to 3, with a momentum of 0.9 and weight decay of 0.0005. The maximum number of training epochs was set to 100.\n\nThe training samples consisted of three-dimensional patches of 120 \u00d7 120 \u00d7 120 or 64 \u00d7 64 \u00d7 64 volumes, randomly chosen from the training images. Data augmentation techniques, including random rotation, flip, contrast and brightness adjustment, and Gaussian blur, were applied to enhance network robustness. The training set included 35 image stacks of 300 \u00d7 300 \u00d7 300 volume from different brain regions.\n\nThe iterative mining process for training label refinement was a key component of the model, involving multiple iterations to detect weak neurites and refine the segmentation. This process was crucial for accurate neurite detection in images with low signal-to-noise ratio (SNR). The entire skeleton length of the neurites in the image stacks was calculated, showing a significant improvement with the mining process.\n\nThe model's performance was evaluated on various 3D optical neuronal datasets, including fMOST datasets, BigNeuron, and Diadem Challenge datasets. The evaluation involved comparing the detection and tracing performance of the proposed method with several best available tracing algorithms and a general deep learning method (GDL). The results demonstrated the effectiveness and accuracy of the proposed weakly supervised deep learning method for neurite detection and tracing.",
  "model/availability": "The source code for the proposed method is not publicly released. However, the datasets generated for this study are available on request to the corresponding author. The method was implemented using C++ and Python 3.6 with the PyTorch library. The segmentation network was trained and evaluated on a computer with an Intel i7-6850K CPU (64 GB RAM) and two NVIDIA 1080Ti GPUs. The proposed method was compared with several best available tracing algorithms, including the voxel scooping method, APP2, and ST-LVF. The comparison showed that the proposed method could achieve better performance in tracing weak neurites from images with low signal-to-noise ratio.",
  "evaluation/method": "The proposed method was evaluated using various 3D optical neuronal datasets, including fMOST datasets for brain-scale long-projection neuron images at terabyte scale, and the public BigNeuron and Diadem Challenge datasets. For the fMOST datasets, image stacks from brain-scale mouse neuronal images with a voxel size of 0.2 \u00d70.2 \u00d71 \u00b5m were used for training and testing. The training set included 35 image stacks of 300 \u00d7300 \u00d7300 volume from different brain regions. The testing set included 15 challenging image stacks of volume 300 \u00d7300 \u00d7300, five image stacks of volume 1000\u00d71000 \u00d7300, and a large image of volume 9620 \u00d73780 \u00d72100.\n\nBigNeuron and Diadem datasets comprised neuronal image stacks from different organizations of various types, scales, and sizes. The generalization and accuracy of the proposed method were validated on these public datasets using transfer learning. Manual tracing results of the public datasets were not used in the weakly supervised training process. A larger neuronal image from the datasets of volume 2111 \u00d73403 \u00d7291 and voxel size of 0.18 \u00d70.18 \u00d70.5 \u00b5m was used for fine-tuning, and six image stacks with voxel sizes of 0.31\u20131, 0.31\u20131, and 0.54\u20133.4 \u00b5m were used for evaluation.\n\nThe method was implemented on C++ and Python 3.6 using the PyTorch library. The segmentation network was trained and evaluated on a computer with an Intel i7-6850K CPU (64 GB RAM) and two NVIDIA 1080Ti GPUs.\n\nThe evaluation metrics included precision, recall, and neuron distances. High precision and recall values correspond to a good segmentation result. For neuron distances, lower values indicate better segmentation. The skeletons obtained by tracing algorithms and manual reconstruction were equally resampled to maintain the distance between any two neighboring skeleton points as one pixel.\n\nExperiments were conducted using different quantities of manual annotation to justify the proposed method and explain its necessity in deep learning. The training dataset included neuronal images with different neurite appearances and structures. The average neuron distance evaluation results were analyzed to demonstrate the method's performance.",
  "evaluation/measure": "In our evaluation, we focused on several key performance metrics to assess the effectiveness of our proposed method for neurite tracing. These metrics include recall, precision, and various neuron distance measures.\n\nRecall and precision are fundamental metrics in evaluating the performance of segmentation and tracing algorithms. Recall measures the ability of the method to identify all relevant neurites, while precision measures the accuracy of the identified neurites. High values in both metrics indicate a good segmentation result. In our experiments, we achieved high recall and precision values, demonstrating the robustness of our method.\n\nIn addition to recall and precision, we utilized neuron distance measures to further quantify the performance of our method. These distances, calculated using a neuron distance plugin, provide a more granular assessment of the tracing accuracy. Specifically, we reported the mean and standard deviation of ESA12, ESA21, ESA, DSA, and PDS distances. Lower values in these distances correspond to better segmentation results. Our method exhibited small neuron distances, indicating high accuracy in neurite tracing.\n\nThe set of metrics we reported is representative of the standards used in the literature for evaluating neurite tracing algorithms. By including both recall/precision and neuron distance measures, we provide a comprehensive assessment of our method's performance. This approach allows for a fair comparison with other state-of-the-art tracing methods, ensuring that our results are both reliable and meaningful in the context of existing research.",
  "evaluation/comparison": "In the evaluation of our proposed method, a comprehensive comparison was conducted with several state-of-the-art tracing algorithms, including both semi-automatic and fully automatic methods. These comparisons were performed on various 3D optical neuronal datasets, such as the fMOST datasets, BigNeuron, and Diadem Challenge datasets. These datasets encompass a wide range of neuronal image stacks with different types, scales, and sizes, ensuring a robust evaluation of our method's generalization and accuracy.\n\nThe fMOST datasets, for instance, included brain-scale mouse neuronal images with a voxel size of 0.2 \u00d7 0.2 \u00d7 1 \u00b5m. The training set consisted of 35 image stacks of 300 \u00d7 300 \u00d7 300 volume from different brain regions, while the testing set included more challenging image stacks of varying volumes, up to a large image of approximately 140 Gigabytes.\n\nFor the BigNeuron and Diadem datasets, which comprised neuronal image stacks from different organizations, transfer learning was employed to validate the proposed method's performance. This approach allowed us to demonstrate the method's ability to adapt to new datasets without the need for manual annotations.\n\nIn addition to comparing with advanced tracing algorithms, simpler baselines were also considered. For example, a general deep learning method (GDL) was used as an \"upper bound\" for comparison. The GDL method was applied on the same network and training samples but relied on manually annotated neurite labels, which are laborious and expensive to obtain. Experiments with different quantities of manual annotations were conducted to justify the need for our weakly supervised approach.\n\nThe comparison included metrics such as precision, recall, and neuron distances, which were calculated using a neuron distance plugin with a default distance threshold. The results showed that our proposed method achieved high precision and recall values, indicating good segmentation results. Moreover, the neuron distances were lower for our method compared to some of the other tracing algorithms, further demonstrating its effectiveness.\n\nOverall, the evaluation process involved a thorough comparison with both publicly available methods and simpler baselines, ensuring a comprehensive assessment of our proposed method's performance and robustness.",
  "evaluation/confidence": "The evaluation of our proposed method includes several performance metrics, such as recall, precision, and various neuron distances (ESA12, ESA21, ESA, DSA, and PDS). These metrics are presented with their mean values and standard deviations, providing a measure of variability and confidence in the results.\n\nFor instance, in the evaluation on neuronal images with various intensity distributions and high noise, the recall and precision for the proposed method were 98.4% and 98.5%, respectively, with the GDL method achieving 99.6% and 99.8%. The standard deviations for the neuron distances were also reported, indicating the consistency of the method's performance.\n\nIn another evaluation on five neuronal images with weak and inhomogeneous intensity neurites, the proposed method achieved an average recall of 0.996 and precision of 0.998. The mean and standard deviation of the neuron distances were also provided, showing that our method exhibited the smallest average mean and standard deviation of ESA12, ESA, and DSA distances, with values close to zero.\n\nStatistical significance was not explicitly stated in the results, but the consistent performance across different datasets and the small standard deviations suggest that the results are reliable and that the proposed method is superior to other tracing methods. The method's ability to handle various intensity distributions and high noise, as well as its performance on large-scale datasets, further supports its robustness and generalization.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, the datasets generated for this study can be obtained by requesting them from the corresponding author. This approach ensures that the data is shared responsibly and in accordance with ethical and legal standards. The datasets include various 3D optical neuronal datasets, such as fMOST datasets, BigNeuron, and Diadem Challenge datasets, which were used to evaluate the proposed method and several best available tracing algorithms. These datasets comprise neuronal image stacks from different organizations, types, scales, and sizes. The evaluation process involved comparing the performance of the proposed method with other tracing algorithms using metrics like recall, precision, and neuron distances. The results demonstrated that the proposed method significantly outperforms other tracing algorithms, especially in images with low signal-to-noise ratio."
}