{
  "publication/title": "Improving Outcome Predictions for Patients Receiving Mechanical Circulatory Support by Optimizing Imputation of Missing Values.",
  "publication/authors": "Jaeger BC, Cantor R, Sthanam V, Xie R, Kirklin JK, Rudraraju R",
  "publication/journal": "Circulation. Cardiovascular quality and outcomes",
  "publication/year": "2021",
  "publication/pmid": "34517728",
  "publication/pmcid": "PMC8455450",
  "publication/doi": "10.1161/circoutcomes.120.007071",
  "publication/tags": "- Missing data\n- Imputation\n- Survival analysis\n- Machine learning\n- Predictive modeling\n- Bayesian analysis\n- Cox proportional hazards\n- Boosting models\n- Monte-Carlo cross-validation\n- Risk prediction",
  "dataset/provenance": "The dataset used in this study is sourced from the INTERMACS registry, a North American observational registry for patients receiving mechanical circulatory support (MCS) devices. This registry began as a collaborative effort involving the National Heart, Lung, and Blood Institute, the US Food and Drug Administration, the Centers for Medicaid and Medicare Services, industry partners, and individual hospitals. The primary goal of INTERMACS is to improve outcomes for patients receiving MCS devices. In 2018, INTERMACS became an official database of the Society of Thoracic Surgeons.\n\nThe analysis included a contemporary cohort of 14,738 patients who received continuous flow left ventricular assist devices (LVADs) between 2012 and 2017. This dataset is publicly available through the National Heart, Lung, and Blood Institute. The data used in this study is de-identified and was obtained with approval from the University of Alabama Institutional Review Board and individual sites.\n\nThe INTERMACS registry collects a wide range of data, including pre-implant patient characteristics, medical status, and laboratory values. It also gathers follow-up data at regularly scheduled visits and during adverse events such as re-hospitalization. For this analysis, all pre-implant variables were considered as potential predictors. The dataset has been utilized in various studies and by the community to enhance the understanding and prediction of outcomes for patients with MCS devices.",
  "dataset/splits": "In our study, we employed Monte-Carlo cross-validation to internally validate the prognostic value of each missing data strategy. This method involved creating 200 replicates, where in each replicate, the data was split into two parts: one for model training and the other for testing. Specifically, 50% of the available data was used for training, and the remaining 50% was used for testing.\n\nThe distribution of data points in each split was balanced, with each split containing approximately half of the total dataset. This approach ensured that the model's performance was evaluated on a diverse set of data points, enhancing the robustness of our findings.\n\nAdditionally, we induced artificial missingness in the data to simulate different scenarios. The missingness was introduced based on patient age, with younger and older patients more likely to have missing data compared to those between 40 and 65 years of age. This step was crucial for assessing the performance of various imputation strategies under different conditions.\n\nThe splits were used to evaluate the performance of different modeling algorithms, which were combinations of imputation strategies and modeling strategies. For each replicate, the performance metrics, such as the scaled Brier score and C-index, were computed to assess the models' predictive accuracy and discrimination. This comprehensive validation process provided a thorough evaluation of the models' performance and the effectiveness of the imputation strategies.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in this study is publicly available through the National Heart, Lung, and Blood Institute. The specific dataset is the INTERMACS registry, which is a North American observational registry for patients receiving mechanical circulatory support (MCS) devices. The data can be accessed via the BioLINCC (Biologic Specimen and Data Repository Information Coordinating Center) website at https://biolincc.nhlbi.nih.gov/studies/intermacs/.\n\nThe dataset includes a contemporary cohort of 14,738 patients who received continuous flow left ventricular assist devices (LVAD) from 2012 to 2017. This is a secondary analysis of de-identified data, ensuring patient privacy and compliance with ethical standards. The primary data collection was approved through the University of Alabama Institutional Review Board and at individual sites.\n\nThe data splits used in the analysis were enforced through a rigorous internal validation process using Monte-Carlo cross-validation (MCCV). This involved 200 replicates where 50% of the available data were used for model training and testing in each replicate. This method ensures that the model's performance is robust and generalizable.\n\nThe dataset is made available under the terms and conditions set by the National Heart, Lung, and Blood Institute, which typically include guidelines for ethical use, data security, and proper citation. Researchers interested in accessing the data must adhere to these guidelines to ensure responsible and compliant use of the information.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages a combination of imputation strategies and modeling techniques, collectively referred to as modeling algorithms. These algorithms are designed to handle missing data effectively and improve the performance of predictive models.\n\nThe machine-learning algorithm class used primarily involves decision trees and ensemble methods. Specifically, we utilized techniques such as boosting, random forests, and survival ensembles. These methods are well-established in the field of machine learning and are known for their robustness in handling complex datasets with missing values.\n\nThe algorithms used are not entirely new but are applied in a novel context. The combination of different imputation strategies with these modeling techniques is what sets our approach apart. This integration allows for a more comprehensive handling of missing data, which is crucial for improving the accuracy and reliability of predictive models in clinical settings.\n\nThe decision to use these established algorithms in a new context was driven by practical considerations. The goal was to develop a robust framework for predicting outcomes in the presence of missing data, which is a common challenge in clinical research. By leveraging well-known algorithms and applying them in a novel way, we aimed to provide a practical solution that can be easily implemented and validated in real-world scenarios.\n\nThe algorithms were not published in a machine-learning journal because the focus of our work is on their application in a specific domain\u2014clinical research and survival analysis. The primary objective was to demonstrate the effectiveness of these algorithms in handling missing data in clinical datasets, rather than introducing a new machine-learning algorithm. The results and methodologies are tailored to address the unique challenges and requirements of clinical research, making them more relevant to journals and audiences in the medical and statistical fields.",
  "optimization/meta": "In our study, we employed a meta-predictor approach to assess the prognostic value of various missing data strategies. This meta-predictor leverages the outputs of multiple modeling algorithms as input features. Specifically, we combined different imputation strategies with various modeling techniques, such as proportional hazards (PH) models and gradient boosted decision trees.\n\nThe modeling algorithms considered in our meta-predictor include imputation methods like mean/mode imputation, hot deck, k-nearest neighbors (KNN), predictive mean matching (PMM), random forests, and Bayesian regression. These imputation strategies were applied to handle missing data in the predictor variables before fitting the PH models or gradient boosted decision trees.\n\nTo ensure the independence of training data, we utilized Monte-Carlo cross-validation (MCCV) with 200 replicates. In each replicate, 50% of the available data were used for model training and the remaining 50% for testing. This resampling technique helps to validate the performance of the meta-predictor and ensures that the training data is independent across different folds.\n\nThe meta-predictor aggregates the predictions from these diverse modeling algorithms to provide a robust risk prediction model. By combining the strengths of multiple imputation and modeling strategies, we aim to improve the prognostic accuracy and generalizability of our risk prediction models.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the machine-learning algorithms could effectively handle the data. We began by addressing missing values, as they are prevalent in clinical datasets. Various imputation strategies were employed, including single and multiple imputation methods. For single imputation, we used techniques like imputing to the mean for numeric variables and the mode for nominal variables. We also explored more sophisticated methods such as k-nearest neighbors (KNN) and random forests, which can capture complex patterns in the data. Multiple imputation involved creating several imputed datasets and then pooling the results to generate a single set of predictions. This approach helps to account for the uncertainty introduced by the imputation process.\n\nAfter handling missing data, we selected predictor variables using a boosting model that quantified variable importance based on the total gain attributed to each predictor while growing decision trees. This step ensured that only the most relevant variables were included in the modeling process, reducing dimensionality and improving model performance.\n\nFor numeric variables, we standardized the data to have a mean of zero and a standard deviation of one. This preprocessing step is essential for algorithms that are sensitive to the scale of the input features, such as those based on gradient descent. For nominal variables, we used one-hot encoding to convert categorical data into a format that could be easily understood by the machine-learning algorithms.\n\nWe also induced artificial missingness in the data to simulate different scenarios of missing data patterns. This was done by introducing additional missing values based on patient age, with younger and older patients more likely to have missing data. This step helped us to evaluate the robustness of our imputation strategies under various missing data conditions.\n\nIn summary, our data encoding and preprocessing pipeline involved handling missing data through imputation, selecting relevant predictor variables, standardizing numeric data, and encoding nominal variables. These steps were essential to prepare the data for the machine-learning algorithms and to ensure that the models could accurately predict patient outcomes.",
  "optimization/parameters": "In our study, we initially considered 50 predictor variables for imputation and subsequent model development. These variables were selected using a boosting model that quantified variable importance based on the total gain attributed to using the predictor while growing decision trees. This approach allowed us to focus on the most relevant predictors for our risk prediction models. The selection of these 50 variables was crucial as it ensured that only the most informative predictors were included in the modeling process, thereby enhancing the prognostic accuracy of our downstream risk prediction models.",
  "optimization/features": "In our analysis, we initially considered a large set of predictor variables. However, to streamline the modeling process and enhance predictive performance, we performed feature selection. This selection was conducted using a boosting model that quantified variable importance based on the total gain attributed to each predictor while growing decision trees. Specifically, we selected 50 predictor variables that demonstrated significant importance in the boosting model.\n\nThe feature selection process was performed using the training data only, ensuring that the testing data remained independent and unbiased. This approach helped in identifying the most relevant features that contributed to the predictive accuracy of our models. By focusing on these selected features, we aimed to improve the prognostic value of our risk prediction models.",
  "optimization/fitting": "In our study, we employed a robust internal validation strategy using Monte-Carlo Cross-Validation (MCCV) to ensure that our models were neither overfitting nor underfitting. The MCCV process involved 200 replicates, where in each replicate, 50% of the available data were used for model training and the remaining 50% for testing. This approach helped to mitigate the risk of overfitting by ensuring that the model's performance was evaluated on unseen data in each replicate.\n\nTo address the potential issue of having a large number of parameters relative to the number of training points, we implemented several strategies. First, we used a boosting model to select the top 50 predictor variables based on their importance, quantified as the fractional contribution of each predictor to the model. This step reduced the dimensionality of the data and focused on the most relevant features, thereby preventing the model from becoming too complex.\n\nAdditionally, we employed different imputation strategies to handle missing data, which further helped in managing the parameter-to-data ratio. For instance, imputation methods like K-nearest neighbors and random forests can effectively handle high-dimensional data by leveraging the structure within the data itself.\n\nTo rule out underfitting, we compared the performance of various modeling algorithms, including Cox Proportional Hazards (PH) models and boosting models. The scaled Brier score and C-index were used to evaluate the models' predictive accuracy and discrimination, respectively. By comparing these metrics across different imputation and modeling strategies, we ensured that our models were not too simplistic to capture the underlying patterns in the data.\n\nFurthermore, we conducted a Bayesian analysis of model performance to assess the relative improvement of different imputation strategies over imputation to the mean. This analysis provided a probabilistic framework to evaluate the models' performance, ensuring that we did not underfit the data by using overly simplistic models.\n\nIn summary, our use of MCCV, variable selection through boosting, and comprehensive evaluation metrics helped us to balance the complexity of our models, ruling out both overfitting and underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was Monte-Carlo Cross-Validation (MCCV), which involved 200 replicates. This resampling technique helped to assess the prognostic value of each missing data strategy by internally validating a total of 23 modeling algorithms. Each replicate used 50% of the available data for model training and testing, providing a rigorous evaluation of model performance.\n\nAdditionally, we utilized boosting models to quantify variable importance. This approach helped in selecting 50 predictor variables by measuring the fractional contribution of each predictor to the model based on the total gain attributed to using the predictor while growing decision trees. This step ensured that only the most relevant variables were considered for imputation and subsequent model development, reducing the risk of overfitting.\n\nFurthermore, we applied different imputation strategies, including random forests, Bayesian regression, predictive mean matching (PMM), hot deck, multiple imputation by chained equations (MIA), and k-nearest neighbors (KNN). These strategies were used to handle missing data in both the training and testing sets, ensuring that the models were not overly reliant on any single imputation method. The use of multiple imputation strategies also helped to mitigate the risk of overfitting by providing a more comprehensive and stable estimation of missing values.\n\nIn summary, our study incorporated several regularization techniques, including Monte-Carlo Cross-Validation, variable importance quantification through boosting, and the use of multiple imputation strategies. These methods collectively helped to prevent overfitting and enhance the generalizability of our risk prediction models.",
  "optimization/config": "The configuration details, including hyper-parameter settings, optimization schedules, model files, and optimization parameters, are not explicitly reported in the publication. However, the computational details and the code used for the analyses are available. The R code utilized for the statistical analyses and manuscript creation is accessible on GitHub under the repository named \"INTERMACS-missing-data.\" This repository likely contains the necessary scripts and configurations used in the study, which can be reviewed and replicated by other researchers. The specific license under which this code is shared is not mentioned, but it is common for such repositories to be open-source, allowing for public access and use. For precise details on the license, one would need to refer directly to the GitHub repository.",
  "model/interpretability": "The models employed in our study include both transparent and black-box components. The Cox proportional hazards (PH) model is relatively transparent, as it provides a clear interpretation of the relationship between predictors and the outcome. In this model, the effect of a unit increase in a predictor is multiplicative with respect to a baseline hazard function, making it straightforward to understand how changes in predictors influence the risk of the event.\n\nOn the other hand, gradient boosted decision trees, referred to as 'boosting,' are more of a black-box model. Boosting grows a sequence of decision trees, each attempting to correct the errors of the previous ones. While this method can capture complex relationships in the data, it is less interpretable because the final model is an ensemble of many trees, making it difficult to trace the exact contributions of individual predictors.\n\nIn summary, while the Cox PH model offers transparency and interpretability, the boosting model, though powerful, is more of a black-box model. This dual approach allows us to balance between interpretability and predictive performance.",
  "model/output": "The model discussed in this publication is primarily focused on risk prediction for time-to-event outcomes, which is a type of regression problem, specifically survival analysis. Two main modeling strategies were applied after imputing missing values: the Cox proportional hazards (PH) model and gradient boosted decision trees, referred to as 'boosting'.\n\nThe Cox PH model is a well-established method for analyzing right-censored time-to-event data. It assesses the effect of predictors on the hazard function, which represents the risk of an event occurring at a specific time. The boosting approach, on the other hand, involves growing a sequence of decision trees to correct errors from previous trees, aiming to improve predictive accuracy.\n\nThe evaluation of these models was conducted using metrics such as the Brier score and the scaled Brier score, which measure the prognostic value by assessing both discrimination and calibration of predicted risk values. The Brier score is analogous to mean-squared error for continuous outcomes, while the scaled Brier score is similar to the R2 statistic, providing a standardized measure of model performance.\n\nAdditionally, the discrimination of the models was evaluated using a time-dependent concordance (C-) index, which measures the probability that the model correctly identifies which of two observations is at higher risk. Calibration was assessed through calibration slope plots, which evaluate the absolute accuracy of the predicted event rates.\n\nInternal validation was performed using Monte-Carlo cross-validation, a resampling technique that involves training and testing the models on different subsets of the data. This process was repeated 200 times to ensure robust evaluation of the models' prognostic value. The models were developed to predict short-term risk, specifically at 6 months after receiving mechanical circulatory support (MCS), focusing on immediate risk assessment.",
  "model/duration": "The computational process for our study was conducted using a high-performance computing cluster at the University of Alabama at Birmingham. This cluster facilitated the execution of 200 Monte-Carlo cross-validation runs, which were essential for internally validating the prognostic value of various modeling algorithms. The use of such a robust computational resource was crucial given the complexity and scale of the analyses performed.\n\nThe specific software and programming languages utilized included SAS software (version 9.4) and Python (version 3.8.2) for creating analytic data. Additionally, Base R (version 4.0.3) was employed in conjunction with several open-source R packages, such as drake, tidyverse, naniar, table.glue, mice, and miceRanger, among others. These tools were instrumental in conducting the statistical analyses and generating the results presented in the manuscript.\n\nThe R code developed for this study is publicly available on GitHub, providing transparency and reproducibility for the methods and findings. The high-performance computing environment, combined with the efficient use of statistical software and programming languages, ensured that the model runs were completed in a timely manner, allowing for thorough validation and analysis of the data.",
  "model/availability": "The source code for the analyses conducted in this study is publicly available on GitHub. The repository, titled \"INTERMACS-missing-data,\" can be accessed at https://github.com/bcjaeger/INTERMACS-missing-data. This repository contains the R code used for statistical analyses and the creation of the manuscript. The code is released under the DOI: 10.5281/zenodo.4247449, which ensures that it is archived and citable.\n\nThe computational environment utilized for these analyses includes SAS software (version 9.4) and Python (version 3.8.2). Additionally, Base R (version 4.0.3) was employed in conjunction with several open-source R packages, such as drake, tidyverse, naniar, table.glue, mice, and miceRanger, among others. These tools were essential for conducting the statistical analyses and generating the results presented in the study.\n\nThe analyses were performed using the American Heart Association Precision Medicine Platform, which provides a robust framework for handling and analyzing large datasets. Furthermore, the high-performance computing resources of Cheaha, part of the UABgrid, were utilized to ensure efficient and scalable processing of the data. The documentation for Cheaha can be found at https://docs.uabgrid.uab.edu/wiki/cheaha.\n\nIn summary, the source code and computational details are openly available, facilitating reproducibility and further research. The use of well-documented software and high-performance computing resources ensures the reliability and scalability of the analyses conducted in this study.",
  "evaluation/method": "The evaluation method employed in our study involved a comprehensive internal validation process using Monte-Carlo cross-validation (MCCV). This technique is a robust resampling method designed to assess the prognostic value of various missing data strategies. We conducted 200 replicates of MCCV, ensuring a thorough evaluation of our models.\n\nIn each replicate, 50% of the available data was used for model training and testing. Predictor variables with less than 50% missing values were considered for imputation and subsequent model development. Artificial missingness was induced at three levels: 0%, 15%, and 30%, with younger and older patients more likely to have missing data compared to those aged between 40 and 65.\n\nPrior to imputation, 50 predictor variables were selected using a boosting model that quantified variable importance based on the total gain attributed to each predictor while growing decision trees. Imputation was performed separately in the training and testing sets for each imputation strategy. This approach ensured that the models were evaluated under realistic conditions, mimicking the challenges of dealing with missing data in real-world scenarios.\n\nAfter imputation, Cox proportional hazards (PH) and boosting models were applied to each imputed dataset. Model predictions for death and transplant were computed at 6 months following mechanical circulatory support (MCS) surgery. To determine the effectiveness of different imputation methods, we applied Bayesian hierarchical models to analyze differences in scaled Brier scores. This strategy provided a flexible framework for hypothesis testing and accounted for correlations within each replicate of MCCV.\n\nThe scaled Brier score, analogous to the R2 statistic, was used to evaluate model performance. A Kaplan-Meier estimate based on the training data served as the naive prediction. The discrimination of the risk prediction models was measured using a time-dependent concordance (C-) index, which accounts for covariate-dependent censoring. Calibration error was estimated by averaging the squared distance between expected and observed event rates according to a calibration plot.\n\nIn summary, our evaluation method involved a rigorous internal validation process using MCCV, comprehensive imputation strategies, and advanced statistical techniques to assess model performance. This approach ensured that our findings are reliable and generalizable to other studies dealing with missing data in risk prediction models.",
  "evaluation/measure": "In our study, we primarily assessed the prognostic value of risk prediction models using the Brier score, which evaluates both discrimination and calibration of predicted risk values. The Brier score is analogous to the mean-squared error for continuous outcomes, and it measures the mean squared difference between observed and predicted event statuses. To make this metric more interpretable, we scaled it relative to a naive model, resulting in a scaled Brier score. This scaled version is akin to the R\u00b2 statistic, where a value of 1.00 indicates a perfect model and 0.00 indicates a worthless model. For ease of interpretation, we multiplied the scaled Brier score values by 100.\n\nIn addition to the Brier score, we evaluated the discrimination of our models using the time-dependent concordance (C-) index. This index measures the probability that the model correctly identifies which of two observations is at higher risk for the event of interest. A C-index of 0.50 indicates worthless discrimination, while a C-index of 1.00 indicates perfect discrimination. Similar to the scaled Brier score, we multiplied C-index values by 100 for easier interpretation.\n\nTo assess the absolute accuracy of our risk prediction models, we used calibration slope plots and estimated calibration error. Calibration error is calculated by averaging the squared distance between expected and observed event rates according to a calibration plot. This metric provides a measure of how well the predicted probabilities align with the actual outcomes.\n\nThese performance metrics are widely recognized and used in the literature for evaluating risk prediction models, particularly in the context of survival analysis and time-to-event data. The Brier score, scaled Brier score, and C-index are standard measures that allow for a comprehensive assessment of model performance, covering both discrimination and calibration aspects. The inclusion of calibration error further ensures that our models are not only discriminative but also accurate in their probability estimates. This set of metrics is representative of best practices in the field and provides a robust evaluation of our models' prognostic value.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of various imputation methods to handle missing data in our datasets. We evaluated multiple imputation strategies, including random forests, Bayesian regression, predictive mean matching, hot deck, multiple imputation by chained equations (MIA), and k-nearest neighbors (KNN), against the simpler baseline of imputation to the mean.\n\nTo ensure fair comparisons, we employed a rigorous experimental design. Each imputation procedure was applied to the training set using models fitted to the training set and then to the testing set using models fitted to the testing set. This approach helped us to assess the performance of each imputation method independently.\n\nWe used Monte-Carlo cross-validation to internally validate the prognostic value of each missing data strategy. This resampling technique involved 200 replicates, where 50% of the data were used for model training and testing in each replicate. We induced artificial missingness at different levels (0%, 15%, and 30%) to simulate various scenarios of data incompleteness.\n\nAfter imputation, we applied Cox proportional hazards (PH) and boosting models to each imputed dataset separately. Model predictions for death and transplant were computed at 6 months following mechanical circulatory support (MCS) surgery. We then analyzed the differences in scaled Brier scores using Bayesian hierarchical models to determine whether any of the imputation methods improved upon imputation to the mean.\n\nOur analysis showed that multiple imputation with random forests had the highest posterior probability of improving the scaled Brier score and the C-index compared to imputation to the mean. However, there was also moderate to strong evidence that MIA and other multiple imputation strategies improved the prognostic accuracy of downstream models.\n\nIn summary, our comparison included both sophisticated imputation methods and simpler baselines, providing a thorough evaluation of their performance in handling missing data. This approach allowed us to identify the most effective strategies for improving model performance in the presence of missing data.",
  "evaluation/confidence": "To evaluate the confidence in our performance metrics, we employed Bayesian hierarchical models to analyze differences in scaled Brier scores. This approach provides a flexible framework for hypothesis testing and accounts for correlations within each replicate of Monte-Carlo cross-validation. Specifically, we estimated the posterior probability that an imputation strategy would improve the scaled Brier score of a downstream model relative to imputation to the mean. For instance, multiple imputation with random forests showed a posterior probability of improvement greater than 0.999, indicating strong evidence of its superiority.\n\nAdditionally, we assessed the discrimination and calibration of our models using time-dependent concordance (C-) indices and calibration error, respectively. The C-index values were multiplied by 100 for ease of interpretation, and we observed that boosting models generally obtained higher median C-indices than proportional hazards (PH) models for predicting 6-month mortality risk. Similarly, multiple imputation strategies consistently provided higher median C-indices compared to single imputation methods.\n\nCalibration error was estimated by averaging the squared distance between expected and observed event rates. We found that almost all imputation strategies provided lower median calibration error compared to imputation to the mean, especially when additional missing data were induced. For example, when 30% additional data were amputed, boosting models using multiple imputation achieved lower calibration error than other strategies.\n\nIn summary, our evaluation metrics include confidence intervals and statistical significance tests, providing robust evidence for the superiority of certain imputation and modeling strategies. The use of Bayesian hierarchical models and resampling techniques ensures that our results are reliable and generalizable.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The analyses were conducted using proprietary software and specific datasets that are not released to the public. The computational details involved the use of SAS software (version 9.4) and Python (version 3.8.2), along with various R packages and tools. The R code used for the analyses is available on GitHub, which can be accessed at the provided link. However, the actual raw evaluation files, including the specific datasets and results, remain confidential and are not part of the public release."
}