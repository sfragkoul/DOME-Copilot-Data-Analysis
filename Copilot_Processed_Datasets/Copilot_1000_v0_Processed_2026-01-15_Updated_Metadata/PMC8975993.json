{
  "publication/title": "Multi-modality machine learning predicting Parkinson's disease.",
  "publication/authors": "Makarious MB, Leonard HL, Vitale D, Iwaki H, Sargent L, Dadu A, Violich I, Hutchins E, Saffo D, Bandres-Ciga S, Kim JJ, Song Y, Maleknia M, Bookman M, Nojopranoto W, Campbell RH, Hashemi SH, Botia JA, Carter JF, Craig DW, Van Keuren-Jensen K, Morris HR, Hardy JA, Blauwendraat C, Singleton AB, Faghri F, Nalls MA",
  "publication/journal": "NPJ Parkinson's disease",
  "publication/year": "2022",
  "publication/pmid": "35365675",
  "publication/pmcid": "PMC8975993",
  "publication/doi": "10.1038/s41531-022-00288-w",
  "publication/tags": "- Parkinson's Disease\n- Genomics\n- Multi-omic Analysis\n- Machine Learning\n- Transfer Learning\n- Genetic Variants\n- Mendelian Randomization\n- Network Communities\n- Performance Metrics\n- Supplementary Data",
  "dataset/provenance": "The dataset used in this study was obtained from the Accelerating Medicines Partnership in Parkinson\u2019s disease (AMP PD) initiative and the Global Parkinson\u2019s Genetics Program (GP2) initiative. Specifically, clinical, demographic, and genome-wide DNA and RNA sequencing data were collected at baseline visits from the Parkinson\u2019s Progression Marker Initiative (PPMI) and the Parkinson\u2019s Disease Biomarkers Program (PDBP). These datasets include cases with Parkinson\u2019s disease (PD) and control subjects unaffected by neurological diseases.\n\nThe PPMI cohort was chosen as the training dataset due to its recruitment design, which includes unmedicated individuals within one year of diagnosis. This design helps ensure that the data is as close to the time of diagnosis as possible. The PDBP dataset was used for validation purposes.\n\nThe study prioritized features that were available for at least 80% of the training and validation cohorts on the AMP PD platform. This approach ensures a robust and comprehensive dataset for analysis. Additionally, the study focused on data that met the modeling inclusion criteria from previous efforts and genomic data that could be passively collected.\n\nThe datasets include a variety of data types, such as clinical information, demographic details, and genomic data, which were used to develop and validate machine learning models for Parkinson\u2019s disease prediction. The use of open-source automated machine learning software and public controlled-access datasets enhances the transparency and reproducibility of the findings.\n\nFor more detailed information on the datasets and their acquisition, readers can refer to the supplementary notes and the acknowledgments section of the publication. The AMP PD Knowledge Platform provides up-to-date information on the study and can be accessed at https://www.amp-pd.org.",
  "dataset/splits": "The dataset was split into two primary parts: a training set and a testing set. The training set comprised 70% of the data, while the testing set contained the remaining 30%. This split was performed randomly on the PPMI dataset. The training set was used to train various machine learning algorithms, while the testing set was used to validate their performance. Additionally, a fivefold cross-validation was conducted on the entire PPMI cohort during the hyperparameter tuning phase to ensure the stability and robustness of the models. This cross-validation process involved dividing the data into five subsets, training the model on four of them, and validating it on the remaining one, repeating this process five times with different subsets. The models were also validated on an external dataset, PDBP, to assess their generalizability.",
  "dataset/redundancy": "The datasets were split into training and testing sets using a 70:30 ratio. This split was applied to the PPMI dataset, ensuring that the training and testing sets were independent. To enforce this independence, data preprocessing steps, including feature selection, adjustment, and normalization, were performed before the split. This approach helped to minimize the risk of data leakage, where information from the testing set might inadvertently influence the training process.\n\nThe distribution of the datasets was designed to be comparable to previously published machine learning datasets in the field. By focusing on baseline data, the PPMI dataset, which includes newly diagnosed and drug-naive patients, was made more similar to the PDBP dataset, which also includes some later-stage Parkinson's disease patients. This alignment aimed to enhance the generalizability of the models across different datasets.\n\nTo further ensure the independence and robustness of the models, hyperparameter tuning was performed using fivefold cross-validation within the entire PPMI cohort. This process helped to improve performance and reduce bias, ensuring that the models could generalize well to unseen data. Additionally, the models were validated on an external dataset, PDBP, to assess their performance and generalizability in a different cohort. This validation step is crucial for confirming that the models are not overfitted to the training data and can perform well in real-world applications.",
  "dataset/availability": "All data and statistical programming code used in this project for the analyses and results generation are accessible to all authors and the public. The online version of the publication contains supplementary material, which includes additional details and resources. The link to the GitHub repository provides access to figures, code, tables, supplementary figures and tables, trained and tuned models, and an example of how to run these models for transfer learning. This repository also includes links and references to the main software used, as well as an interactive web application for further investigation.\n\nThe article is licensed under a Creative Commons Attribution 4.0 International License. This license permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source, a link to the Creative Commons license is provided, and any changes made are indicated. The images or other third-party material in this article are included in the article\u2019s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons license and the intended use is not permitted by statutory regulation or exceeds the permitted use, permission must be obtained directly from the copyright holder. To view a copy of this license, visit the provided link.\n\nThe data and resources are made available to ensure transparency and reproducibility of the research. The GitHub repository serves as the primary platform for accessing the data splits, code, and models used in the study. The licensing terms ensure that users can freely access and utilize the materials while giving proper credit to the original authors. This approach promotes open science and facilitates further research and collaboration in the field.",
  "optimization/algorithm": "The machine-learning algorithms used in this study were not new, but rather well-established methods that have proven successful in various domains. The algorithms included logistic regression, random forests, adaptive boosting, gradient boosting, gradient boosting machines, stochastic gradient descent, support vector machines, multi-layer perceptron neural networks, k-nearest neighbors, linear discriminant analysis, quadratic discriminant analysis, bagging, and extreme gradient boosting. These algorithms were chosen for their success in other domains, their execution in Python\u2019s scikit-learn package, and their ability to export probability-based predictions, which facilitated the training, testing, and interpretation of the models.\n\nThe decision to use these established algorithms was driven by the need for robust and interpretable models that could handle the complexities of the data. The algorithms were selected to provide a comprehensive comparison and to ensure that the best-performing model was identified. The focus was on leveraging these algorithms to build accurate peri-diagnostic models for predicting disease risk, rather than developing new algorithms. This approach allowed for a systematic and thorough evaluation of different machine-learning methods, ensuring that the final model was both accurate and reliable.",
  "optimization/meta": "The model leverages a meta-predictor approach, which integrates outputs from multiple machine-learning algorithms to enhance predictive performance. This meta-predictor is designed to combine the strengths of various algorithms, including gradient boosting, stochastic gradient descent, support vector machines, multi-layer perceptron neural networks, k-nearest neighbors, linear discriminant analysis, quadratic discriminant analysis, bagging, and extreme gradient boosting. These algorithms collectively provide a robust framework for handling complex datasets and improving predictive accuracy.\n\nThe meta-predictor approach ensures that the training data remains independent by employing a rigorous feature selection process. This process involves using the extremely randomized trees classifier algorithm to remove redundant features, thereby reducing the risk of overfitting and maintaining the independence of the training data. The feature selection is carried out separately for genetic and transcriptomic data, acknowledging their structural differences and ensuring that each modality is analyzed appropriately.\n\nThe model's performance is evaluated using a combination of metrics, including AUC, balanced accuracy, sensitivity, and specificity. This multi-faceted evaluation helps in addressing case-control imbalances and ensures that the model generalizes well to unseen data. The final best-performing model is selected based on these metrics, and further optimization is performed using Youden\u2019s J calculation to better account for class imbalances. This optimization is applied post hoc, ensuring that the model's performance is enhanced without compromising the independence of the training data.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, data preprocessing, also known as data munging, included feature selection, adjustment, and normalization. For DNA sequencing, ten principal components were calculated based on a random set of 10,000 variants sampled after linkage disequilibrium pruning, ensuring that variants with r\u00b2 < 0.1 were kept. This process excluded regions containing large tracts of linkage disequilibrium. For RNA sequencing data, read counts per sample for all protein-coding genes were used to generate another set of ten principal components. All genetic features, represented as minor allele dosages, were adjusted for these DNA sequence-derived principal components using linear regression to remove the effects of quantifiable European population substructure. A similar adjustment was done for RNA sequencing data using RNA sequencing-derived principal components. This adjustment helped to statistically account for latent population substructure and experimental covariates at the feature level, increasing generalizability across heterogeneous datasets.\n\nAfter adjustment, all continuous features were Z-transformed to have a mean of 0 and a standard deviation of 1, ensuring that all features were on the same numeric scale. Feature selection was then carried out using the extremely randomized trees classifier algorithm on combined data modalities. This step aimed to remove redundant feature contributions that could overfit the model, optimizing the information content from the features and limiting artificial inflation in predictive accuracy. The implementation of decision trees for feature selection helped to generate the most parsimonious feature set for modeling, reducing the potential for overfitting. Feature selection was run on combined data modalities to remove potentially redundant feature contributions that could artificially inflate model accuracy. Export estimates for features most likely to contribute to the final model in order of importance were generated by the extremely randomized trees classifier for each of the combined models. By removing redundant features, the potential for overfitting was limited, making the models more conservative. Additionally, if a variant provided redundant model information, such as being in strong linkage with a PRS variant, it would be removed from the potential feature list.",
  "optimization/parameters": "In our study, we utilized a comprehensive approach to select the optimal parameters for our machine learning models. We began by considering a range of p-value thresholds for feature inclusion, specifically [1E-2, 1E-3, 1E-4, 1E-5, 1E-6, 1E-7, and 1E-8], for both genetic and transcriptomic data. This range was chosen based on the most recent published genome-wide association studies (GWAS) and differential expression analyses.\n\nWe conducted an exhaustive search across all possible combinations of these p-value thresholds, resulting in 49 different models. Each model underwent a rigorous feature selection process using the extremely randomized trees classifier algorithm. This algorithm helped us to remove redundant features that could potentially overfit the model, ensuring that we retained only the most informative features.\n\nFor each of the 49 models, we evaluated performance metrics such as the area under the curve (AUC), balanced accuracy, sensitivity, and specificity. The final best-performing model was selected based on a combination of these metrics to account for case-control imbalance. This approach allowed us to identify the optimal p-value thresholds that maximized model performance while minimizing the risk of overfitting.\n\nAdditionally, we performed hyperparameter tuning using fivefold cross-validation to further optimize the performance of the selected model. This process involved iteratively testing different hyperparameters, such as the number of estimators in the AdaBoostClassifier, to find the configuration that yielded the best performance metrics.\n\nIn summary, the selection of p-value thresholds and other parameters was driven by a systematic and data-driven approach, ensuring that our models were robust and generalizable.",
  "optimization/features": "In our study, we utilized a comprehensive set of input features to build our predictive models. The final model included 51 single nucleotide polymorphisms (SNPs) and 418 protein-coding transcripts, along with additional features such as demographics, age, family history, olfactory function, and polygenic risk scores.\n\nFeature selection was indeed performed to ensure that only the most relevant features were included in the model. This process was carried out using the extremely randomized trees classifier algorithm, which helped to remove redundant feature contributions that could potentially overfit the model. The feature selection was done using the training set only, ensuring that the model's performance on the testing set was not artificially inflated.\n\nThe goal of feature selection was to optimize the information content from the features and to limit artificial inflation in predictive accuracy that might be introduced by including a large number of features before filtering. By removing redundant features, we aimed to create a more parsimonious model that would generalize better to new, unseen data.",
  "optimization/fitting": "The fitting method employed in this study involved a rigorous process to ensure that both overfitting and underfitting were minimized. The dataset was split into a 70:30 training:testing ratio, which provided a substantial amount of data for training while reserving a significant portion for testing. This split helped in evaluating the model's performance on unseen data, thereby reducing the risk of overfitting.\n\nTo further mitigate overfitting, feature selection was performed using the extremely randomized trees classifier algorithm on combined data modalities. This step was crucial in removing redundant features that could artificially inflate the model's accuracy. By focusing on the most informative features, the model became more parsimonious and less likely to overfit the training data. Additionally, the use of cross-validation, specifically fivefold cross-validation, ensured that the model's performance was consistent across different subsets of the data. This technique helped in estimating the model's generalizability to unseen data.\n\nThe number of parameters was managed carefully to avoid overfitting. The feature selection process ensured that only the most relevant features were included, thereby keeping the number of parameters in check relative to the number of training points. This approach helped in maintaining a balance where the model complexity was appropriate for the amount of data available.\n\nTo address underfitting, a variety of well-performing machine learning algorithms were competed to identify the best model. These algorithms were chosen for their success in other domains and their ability to handle complex data structures. The final model was selected based on a combination of metrics, including AUC, balanced accuracy, sensitivity, and specificity. This multi-metric approach ensured that the model was not only accurate but also robust and generalizable.\n\nPost hoc optimization was also employed to address class imbalance. By using Youden\u2019s J calculation to optimize the probability threshold for case classification, the model's performance was further enhanced. This step ensured that the model could handle imbalanced datasets effectively, thereby reducing the risk of underfitting.\n\nIn summary, the fitting method involved a careful balance of feature selection, algorithm competition, and cross-validation to minimize both overfitting and underfitting. The use of multiple performance metrics and post hoc optimization ensured that the final model was robust, accurate, and generalizable.",
  "optimization/regularization": "In our study, several regularization methods were employed to prevent overfitting and ensure the robustness of our models. Initially, we performed feature selection using the extremely randomized trees classifier algorithm on combined data modalities. This step was crucial in removing redundant feature contributions that could artificially inflate model accuracy and lead to overfitting. By focusing on the most informative features, we optimized the information content and limited the potential for overfitting.\n\nAdditionally, we used correlation-based pruning to further reduce redundancy among features. This involved removing features that had high correlation with others, as including such features could introduce artificial inflation in predictive accuracy. The mean correlation between the top 5% of features ranked in the Shapley analysis was kept below 5%, with a maximum of 36%, ensuring that the selected features were diverse and contributed uniquely to the model.\n\nWe also implemented hyperparameter tuning with fivefold cross-validation. This process helped in selecting the best hyperparameters for our algorithms, ensuring that the models performed well on unseen data and were not overfitted to the training data. The stability of the results over iterations suggested minimal bias, further confirming the effectiveness of our regularization techniques.\n\nMoreover, we addressed class imbalance by using Youden\u2019s J calculation to optimize the probability threshold for case classification. This post hoc optimization was done after fitting the tuned model to the external validation cohort, ensuring that the model performed well even with imbalanced datasets.\n\nOverall, these regularization methods\u2014feature selection, correlation-based pruning, hyperparameter tuning, and threshold optimization\u2014were integral to building robust and generalizable models.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are indeed available. All relevant data, including code, tables, and models, can be accessed through our online repository. This repository is hosted on GitHub and includes a comprehensive set of resources for the community to utilize.\n\nThe repository contains figures and tables referenced in the manuscript, as well as additional supplementary figures and tables. It also provides links and references to the main software used, GenoML. The trained and tuned models, along with their associated performance metrics, are available for deployment and use by the scientific community. This allows researchers to replicate our findings and apply the models to their own datasets.\n\nAdditionally, the repository includes an example of how to run these trained and tuned models for transfer learning. This is particularly useful for researchers who wish to adapt the models to different datasets or specific research questions. The repository is open-access, and all materials are licensed under a Creative Commons Attribution 4.0 International License. This license permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source. This ensures that the community can freely access and build upon our work, fostering further research and collaboration.",
  "model/interpretability": "The model employed in this study is not a black box. To enhance transparency and interpretability, the Shapley additive explanations (SHAP) approach was utilized. SHAP values provide a game theory-based approximation of a feature's impact on the model, considering the relative bidirectional change in that feature compared to all other features. This method creates accurate explanations for each observation in the dataset, allowing for a deeper understanding of the model's decision-making process.\n\nThe SHAP package was used to calculate and visualize these values, which are presented in the manuscript's figures and an interactive website. This visualization helps to identify the most influential features in the model. For instance, the University of Pennsylvania Smell Identification Test (UPSIT) score was found to account for roughly half of the decision-making process. In some cases, an unexpectedly high UPSIT score led to misclassification of individuals as healthy controls. This demonstrates how specific features can significantly influence the model's predictions.\n\nAdditionally, a surrogate XGBoost model was trained on 70% of the data and tested on the remaining 30% to evaluate the model's contributing features. This approach further aids in understanding which features are most important in the classification process.\n\nAn interactive website has been developed to allow researchers to investigate the top features of the model and how these features influence the classification of individual samples. This tool provides a granular level of insight into the interplay between clinical and omic data, making the model more transparent and interpretable.",
  "model/output": "The model developed is a classification model designed to identify individuals at high risk of Parkinson's disease (PD) at the point of diagnosis. It utilizes a multimodal approach, incorporating various data types such as genetic, transcriptomic, and clinical information. The primary output of the model is the probability of an individual being a case (having PD) or a control (not having PD). This probability is determined using an optimized threshold based on Youden\u2019s J calculation, which helps to account for case-control imbalance and improve balanced accuracy.\n\nThe model employs several machine learning algorithms, including AdaBoostClassifier, GradientBoostingClassifier, stochastic gradient descent (SGDClassifier), support vector machines (SVC), multi-layer perceptron neural networks (MLPClassifier), k-nearest neighbors (KNeighborsClassifier), linear discriminant analysis (LinearDiscriminantAnalysis), quadratic discriminant analysis (QuadraticDiscriminantAnalysis), bagging (BaggingClassifier), and extreme gradient boosting (XGBClassifier). These algorithms were selected for their ability to handle nonlinear associations and accommodate the complexity of the data.\n\nFeature selection was performed using the extremely randomized trees classifier algorithm (extraTrees) to remove redundant features and prevent overfitting. The final model was chosen based on a combination of metrics, including AUC, balanced accuracy, sensitivity, and specificity, to best account for case-control imbalance.\n\nThe model's performance was evaluated using metrics such as AUC, accuracy, balanced accuracy, log loss, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). The model demonstrated improved performance when validated in the PDBP dataset, with an AUC of 83.84% before tuning and 85.03% after tuning. The multimodal approach yielded better results across all performance metrics compared to models focusing on a single data modality.\n\nAn interactive web-based application was developed to allow researchers to investigate the driving factors in the best model. This application provides decision plots and allows users to explore variations such as transcriptomics-only models or models without clinico-demographic features. The application also includes Shapley additive explanations (SHAP) to evaluate each feature\u2019s influence in the model, enhancing understanding by creating accurate explanations for each observation in the dataset.",
  "model/duration": "To address the execution time of our models, it's important to note that we employed a systematic approach to optimize both performance and efficiency. We initially surveyed and triaged a list of top-performing machine learning algorithms from the 2020 Kaggle executive summary, focusing on those that were fundamentally different to ensure a diverse set of methods. This step was crucial in reducing runtime and computational overhead.\n\nWe selected twelve well-performing algorithms that are known for their success in various domains and their ability to execute efficiently in Python\u2019s scikit-learn package. These algorithms included logistic regression, random forests, adaptive boosting, gradient boosting, and others. By limiting our selection to these twelve algorithms, we significantly reduced the computational burden while still ensuring a comprehensive evaluation of different machine learning techniques.\n\nThe data preprocessing steps, including feature selection, adjustment, and normalization, were performed using the extremely randomized trees classifier algorithm. This step was essential for removing redundant features and optimizing the information content from the features, which in turn helped in limiting overfitting and reducing the potential for artificial inflation in predictive accuracy.\n\nThe model training and hyperparameter tuning were conducted using a 70:30 training-testing split in the PPMI dataset, with thorough hyperparameter tuning performed using fivefold cross-validation. This process ensured that the models were robust and generalizable, even though it added to the overall execution time.\n\nThe final models were validated on an external dataset, PDBP, to assess their performance in a real-world setting. This validation step was crucial for ensuring that the models could generalize well to new, unseen data.\n\nIn summary, while the exact execution time varied depending on the specific algorithm and the complexity of the data, our approach was designed to balance thoroughness and efficiency. The use of automated machine learning software and the selection of a diverse yet manageable set of algorithms helped in optimizing the execution time without compromising on the quality of the models.",
  "model/availability": "The source code used for the analyses and results generation in this project is publicly accessible. It is available in an online repository, specifically on GitHub. The repository includes the code used for the manuscript, as well as additional supplementary figures and tables. This allows all authors and the public to access and utilize the data and statistical programming code.\n\nThe repository also provides trained and tuned models along with their associated performance metrics. This enables the community to deploy and use these models. Additionally, an example of how to run these trained and tuned models for transfer learning is included. For those interested in further investigation, an interactive web application is available. This application allows users to explore the contribution of different features at validation to an individual sample\u2019s classification, showcasing the interplay between clinical and omic data on a more granular level. The repository is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source.",
  "evaluation/method": "The evaluation method employed in this study was comprehensive and multi-faceted, ensuring the robustness and generalizability of the models developed. Initially, the dataset was split into 70% training and 30% testing subsets. This split allowed for the training of various machine learning algorithms on the training set, while the testing set was used for validation. A total of 12 well-performing machine learning algorithms were evaluated to identify which could maximize the Area Under the Curve (AUC) across the two classes (cases and controls).\n\nTo further validate the models, a thorough hyperparameter tuning process was conducted using fivefold cross-validation. This technique involved training and testing the models on different splits of the dataset to estimate performance on unseen data. The results from the cross-validation were stable over iterations, suggesting minimal bias. Additionally, Z-transformations were applied to the entire dataset prior to splitting, ensuring consistency in the data preprocessing steps.\n\nThe best-performing algorithm was then selected based on a combination of metrics, including AUC, balanced accuracy, sensitivity, and specificity. This approach aimed to account for case-control imbalance and optimize the model's performance. Post hoc optimization was performed to address class imbalance by re-fitting the model to the withheld samples using an optimized threshold for case probability based on Youden\u2019s J calculation. This step was repeated after fitting the tuned model to the external validation cohort, ensuring that the probability threshold for discerning case status was specific to the validation dataset.\n\nExternal validation was conducted by fitting the trained and tuned models from the PPMI dataset to the PDBP dataset. This step was crucial for assessing the generalizability and performance of the models in a de novo dataset. The models were exported to enable external validation and transfer learning, allowing researchers to deploy and use the models in their own datasets.\n\nAn interactive web application was developed to facilitate further investigation into the contribution of different features at validation to an individual sample\u2019s classification. This tool provides a granular level of insight into the interplay between clinical and omic data, enhancing the interpretability of the models.\n\nIn summary, the evaluation method involved a rigorous process of algorithm competition, feature selection, hyperparameter tuning, cross-validation, and external validation. These steps ensured that the models were robust, generalizable, and capable of addressing class imbalance, ultimately providing reliable and interpretable results.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our models. The primary metric we prioritized was the area under the curve (AUC), which provides an aggregate measure of a classifier's performance across all potential probability thresholds. This metric is particularly valuable as it is less affected by class imbalance, making it a robust indicator of model performance.\n\nIn addition to AUC, we reported several secondary metrics to gain a more nuanced understanding of our models' performance. These included accuracy, which measures the rate of correct predictions, and balanced accuracy, which is the mean accuracy weighted across cases and control samples. Balanced accuracy is especially useful for dealing with imbalanced datasets, ensuring that the performance on both classes is taken into account.\n\nWe also considered sensitivity (true positive rate) and specificity (true negative rate), which relate to the proportion of true positive cases and true controls identified, respectively. These metrics are crucial for understanding how well the model distinguishes between cases and controls.\n\nTo address class imbalance, we optimized the probability threshold using Youden\u2019s J statistic. This approach helps in improving metrics like balanced accuracy and related measures. The final best-performing model was selected based on a combination of AUC, balanced accuracy, sensitivity, and specificity to account for case-control imbalance effectively.\n\nOur approach to reporting these metrics is representative of current best practices in the literature, ensuring that our evaluation is thorough and transparent. By focusing on these key metrics, we aim to provide a clear and comprehensive assessment of our models' performance, making our findings reliable and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we compared our multimodal machine learning approach to several simpler baselines and publicly available methods to evaluate its performance. We utilized data from the Accelerating Medicines Program \u2014 Parkinson\u2019s Disease (AMP PD) program, which includes datasets from the Parkinson\u2019s Progression Marker Initiative (PPMI) and the Parkinson\u2019s Disease Biomarker Program (PDBP).\n\nWe began by comparing our multimodal model to single-modality models, such as those based solely on clinico-demographic data, genetics, or transcriptomics. The multimodal approach consistently outperformed these single-modality models across various performance metrics, including the area under the curve (AUC), balanced accuracy, sensitivity, and specificity. This comparison demonstrated the superiority of integrating multiple data modalities over relying on a single type of data.\n\nAdditionally, we compared our model to previous efforts, including those by Nalls and colleagues. While their UPSIT-only model showed strong classification performance, our multimodal model provided more informative and balanced results. The UPSIT model, although having a high AUC, has limitations because a decline in smell identification is not specific to Parkinson\u2019s Disease (PD) and can be influenced by general neurodegeneration, aging, and environmental factors. Our multimodal approach leveraged the diversity of data types to enhance overall sensitivity and specificity.\n\nWe also conducted a thorough hyperparameter tuning and cross-validation process to improve performance and reduce bias. This involved using a 70:30 training-testing split in the PPMI dataset and performing fivefold cross-validation. The results from the cross-validation were stable over iterations, suggesting minimal bias. Furthermore, we validated our models by fitting the trained and tuned models from PPMI to the external validation dataset, PDBP, ensuring the robustness of our approach.\n\nIn summary, our multimodal machine learning model was compared to simpler baselines and publicly available methods, demonstrating superior performance in predicting PD diagnosis. The integration of multiple data modalities provided a more comprehensive and accurate predictive model, addressing the limitations of single-modality approaches.",
  "evaluation/confidence": "The evaluation of our model's performance was conducted using several metrics, with a primary focus on the area under the curve (AUC). The AUC metric was chosen for its robustness in summarizing classifier performance across all potential probability thresholds, making it less susceptible to class imbalance issues. We considered an AUC of greater than 80% to indicate a robust and \"very good to excellent\" diagnostic performance.\n\nTo ensure the reliability of our results, we employed a combination of metrics, including balanced accuracy, sensitivity, and specificity. These metrics were used to account for the case-control imbalance in our dataset. The final best-performing model was selected based on a balanced consideration of these metrics, rather than relying solely on AUC.\n\nStatistical significance was assessed through p-values derived from the largest meta-analysis of 17 datasets from Parkinson's Disease Genome-Wide Association Studies (PD GWAS) available from European ancestry samples. We filtered our data using these p-values prior to training, ensuring that only the most statistically significant features were included. For genetic data, we focused on a model with a maximum p-value of 1E-5, and for transcriptomic data, a maximum p-value of 1E-2.\n\nPost hoc optimization was performed to address class imbalance further. This involved refitting the model to withheld samples using an optimized threshold for case probability based on Youden\u2019s J calculation. This process was repeated after fitting the tuned model to an external validation cohort, ensuring that the probability threshold for discerning case status was specific to the validation dataset.\n\nConfidence intervals for the performance metrics were not explicitly mentioned, but the use of multiple metrics and the rigorous filtering process based on p-values provide a strong foundation for the statistical significance of our results. The model's performance was validated on an external dataset (PDBP), demonstrating its generalizability and robustness.\n\nIn summary, the evaluation of our model's performance was thorough and statistically rigorous, ensuring that the claims of superiority over other methods and baselines are well-supported.",
  "evaluation/availability": "All data and statistical programming code used in this project for the analyses and results generation are accessible to all authors and the public. This includes the raw evaluation files, which can be found in an online repository. The repository also contains figures, code, tables, and models used in the study, allowing for reproducibility and further investigation. The article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source. This license allows for the reuse of the data and code, facilitating transparency and collaboration in research."
}