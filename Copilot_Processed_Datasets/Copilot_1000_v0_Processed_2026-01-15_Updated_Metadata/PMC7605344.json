{
  "publication/title": "Risk prediction models to guide antibiotic prescribing: a study on adult patients with uncomplicated upper respiratory tract infections in an emergency department.",
  "publication/authors": "Wong JG, Aung AH, Lian W, Lye DC, Ooi CK, Chow A",
  "publication/journal": "Antimicrobial resistance and infection control",
  "publication/year": "2020",
  "publication/pmid": "33138859",
  "publication/pmcid": "PMC7605344",
  "publication/doi": "10.1186/s13756-020-00825-3",
  "publication/tags": "- Antimicrobial Resistance\n- Infection Control\n- Predictive Modeling\n- Logistic Regression\n- LASSO Regression\n- Classification and Regression Trees (CART)\n- Respiratory Infections\n- Clinical Decision Support\n- Statistical Analysis\n- Patient Subgroups",
  "dataset/provenance": "The dataset used in our study was sourced from the electronic medical records of a tertiary hospital. The study included a total of 715 participants who presented with upper respiratory tract infections (URTI). The data points extracted from these records encompassed a wide range of demographic, clinical, and laboratory variables. These variables included age, gender, ethnicity, visit date, pre-existing comorbidities, respiratory symptoms, full blood count, kidney/liver panels, and biochemistry tests. Additionally, epidemiologic data such as smoking status, influenza vaccination history, travel history, and prior medical consultations and antibiotic consumption were obtained through an interviewer-administered questionnaire.\n\nThe dataset was carefully curated to ensure that it represented real-life clinical scenarios. Variables with more than 10% of data missing were excluded from the analysis. Categorical variables with missing data were recoded as zero, assuming that the presence of any clinical covariates would have been recorded. Continuous variables were imputed according to their group medians.\n\nThis dataset has not been used in previous publications by our team or the broader community. It is unique to this study and was specifically collected to address the research questions related to antibiotic prescribing decisions for patients with URTI. The comprehensive assessment of medical records was performed by two clinically trained individuals to ensure data accuracy and consistency. This meticulous approach allowed us to develop robust predictive models that can aid physicians in making informed decisions at the point of care.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a validation set. The training set consisted of 70% of the participants' data, while the validation set comprised the remaining 30%. This split was chosen to ensure that the models were trained on a substantial amount of data while also having a sufficient portion for validation to assess their performance accurately. The distribution of data points in each split was designed to reflect the overall characteristics of the study population, maintaining the representativeness of the original dataset in both the training and validation sets.",
  "dataset/redundancy": "The dataset used in our study consisted of 715 patients with uncomplicated upper respiratory tract infections (URTIs) recruited from Singapore\u2019s busiest emergency department. To ensure robust model development and validation, the dataset was split into two independent sets: a training set and a validation set. The training set comprised 70% of the data, while the remaining 30% constituted the validation set. This split was designed to maintain the independence of the training and test sets, ensuring that the models were evaluated on data they had not seen during training.\n\nTo enforce the independence of the training and validation sets, we used a random sampling technique. This method ensures that each patient's data is included in only one of the sets, preventing any overlap that could bias the model's performance evaluation. The distribution of the dataset was carefully considered to reflect the diversity of the patient population, including variations in age, gender, and clinical presentations. This approach aligns with best practices in machine learning, where the goal is to create models that generalize well to new, unseen data.\n\nComparing our dataset to previously published machine learning datasets in similar contexts, our approach of using a 70-30 split is consistent with standard practices. This split ratio is commonly used to balance the need for a sufficiently large training set to capture the underlying patterns in the data while reserving a substantial portion for validation to assess the model's performance accurately. The independence of the training and validation sets is crucial for obtaining reliable estimates of model performance and ensuring that the models can be effectively deployed in real-world settings.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm used in our study falls under the category of supervised learning techniques, specifically focusing on classification tasks. We employed three distinct methods: logistic regression, LASSO (Least Absolute Shrinkage and Selection Operator) regression, and classification and regression trees (CART).\n\nThese algorithms are well-established and widely used in the field of machine learning and statistics. Logistic regression is a fundamental method for binary classification problems, providing a probabilistic approach to predicting outcomes. LASSO regression is an extension of logistic regression that includes a regularization term to handle multicollinearity and feature selection, thereby improving model interpretability and performance. CART, on the other hand, is a decision tree-based method that recursively partitions the data to create a tree structure, making it robust to outliers and capable of capturing complex interactions between variables.\n\nNone of these algorithms are new; they have been extensively studied and applied in various domains. The choice of these methods was driven by their practicality and ease of implementation in a clinical setting. Logistic regression and LASSO regression are straightforward to interpret and implement, making them suitable for guiding clinical decisions. CART provides a visual representation of the decision-making process, which can be particularly useful for healthcare professionals.\n\nThe algorithms were developed using established packages in R, such as glmnet for LASSO regression and rpart for CART. These packages are well-documented and widely used in the statistical community, ensuring reliability and reproducibility of our results. The decision to use these specific methods was based on their proven effectiveness in similar predictive modeling tasks and their compatibility with the data and objectives of our study.",
  "optimization/meta": "The models developed in this study do not use data from other machine-learning algorithms as input. Instead, they are standalone predictive models. The study employed three distinct methods for predictive modeling: logistic regression, LASSO regression, and classification and regression trees (CART). Each of these methods was trained and validated independently using the same dataset, which was split into a training set (70% of the data) and a validation set (30% of the data).\n\nThe training data for each model was derived from a cohort of 715 patients with uncomplicated upper respiratory tract infections (URTIs) recruited from a busy emergency department. The dataset included demographic, clinical, and laboratory data extracted from hospital electronic medical records. The independence of the training data is ensured by the random split of the dataset into training and validation sets, which helps in assessing the generalizability of the models.\n\nThe logistic regression model was built using univariate analysis followed by stepwise elimination of variables. The LASSO regression model was used to address issues of overfitting and multicollinearity by shrinking the coefficients of less relevant features to zero. The CART model utilized a decision tree approach to identify important interactions and patient subgroups.\n\nIn summary, the models are not meta-predictors but rather individual predictive models trained on the same dataset. The training data is independent, as ensured by the random split of the dataset into training and validation sets.",
  "optimization/encoding": "For the machine-learning algorithms, data encoding and preprocessing were crucial steps to ensure the models' effectiveness. Demographic, clinical, and laboratory data were extracted from hospital electronic medical records. These included age, gender, ethnicity, visit date, pre-existing comorbidities, respiratory symptoms, full blood count, kidney/liver panels, and biochemistry tests. Additionally, epidemiologic data such as smoking status, influenza vaccination history, travel history, and prior medical consultations and antibiotic consumption were obtained through an interviewer-administered questionnaire.\n\nTo handle missing data, variables with more than 10% of data missing were excluded from the analysis. For categorical variables with missing data, a value of 0 was assigned, assuming that the presence of any clinical covariates would have been recorded. Continuous variables were imputed using their group medians to maintain data integrity.\n\nThe Charlson Comorbidity Index was calculated based on the participants' comorbid status to quantify the burden of comorbidities. This index helped in standardizing the health status of the participants, which is essential for accurate predictive modeling.\n\nThe data was then split into a training set (70% of participants) and a validation set (30% of participants). This split ensured that the models were trained on a substantial amount of data while still having a robust validation set to assess their performance.\n\nFor the logistic regression model, univariate analysis was performed on all candidate variables. Demographic factors, clinically relevant variables, and significant variables from the univariate models were fitted into the final multivariable model via stepwise elimination using a cutoff of p < 0.1.\n\nIn the LASSO regression model, a minimum optimal shrinking parameter (\u03bb) was selected through 10-fold cross-validation of the training dataset. This process helped in shrinking the coefficients of less relevant features to zero, reducing overfitting and variance without substantially increasing bias.\n\nFor the classification and regression trees (CART) model, a maximum tree depth of 5 and a minimum of 10 subjects in a node were set before attempting a split. This configuration helped in preventing overfitting and ensured that the model was robust to outliers and multicollinearity. The final tree size was decided by finding the number of splits that produced the smallest cross-validation error.\n\nThe analyses were performed using R 4.0.2 and STATA 13.0 at a 5% significance level. The LASSO and CART models were developed using the glmnet, rpart, and rattle packages in R. These tools provided the necessary functionality to implement the models effectively.",
  "optimization/parameters": "In our study, we utilized 50 candidate variables for the initial univariate analysis. These variables encompassed a wide range of demographic, clinical, and laboratory data, including age, gender, ethnicity, pre-existing comorbidities, respiratory symptoms, full blood count, kidney/liver panels, and biochemistry tests. Additionally, epidemiologic data such as smoking status, influenza vaccination history, travel history, and prior medical consultations were considered.\n\nThe selection of these parameters was guided by their relevance to the clinical context of uncomplicated upper respiratory tract infections (URTIs) and their potential impact on antibiotic prescribing decisions. The final set of significant predictors was determined through a stepwise elimination process in the logistic regression model, where variables with a p-value less than 0.1 were retained. This approach ensured that only the most relevant variables were included in the final models, thereby enhancing their predictive accuracy and practical applicability.\n\nFor the LASSO regression, a minimum optimal shrinking parameter (\u03bb) of 0.03971531 was selected through 10-fold cross-validation of the training dataset. This process helped in identifying the most important predictors by shrinking the coefficients of less relevant features to zero, thereby reducing overfitting and improving the model's generalizability.\n\nIn the classification and regression trees (CART) model, the choice of the final tree size was decided by finding the number of splits that produced the smallest cross-validation error. This method ensured that the model was neither too complex nor too simple, striking a balance between bias and variance. The maximum tree depth was set to 5, and a minimum of 10 subjects were required in a node before a split was attempted, further preventing overfitting.\n\nOverall, the selection of parameters was driven by a combination of clinical relevance, statistical significance, and model performance metrics, ensuring that the final models were robust and applicable in real-world settings.",
  "optimization/features": "In our study, we initially considered 50 candidate variables as potential input features for our predictive models. These variables encompassed a wide range of demographic, clinical, and laboratory data, including age, gender, ethnicity, pre-existing comorbidities, respiratory symptoms, full blood count, kidney/liver panels, and biochemistry tests. Additionally, epidemiologic data such as smoking history, influenza vaccination status, travel history, and prior medical consultations were included.\n\nFeature selection was performed to identify the most relevant predictors. For the logistic regression model, univariate analysis was conducted on all 50 candidate variables. Demographic factors, clinically relevant variables, and significant variables from the univariate models were then fitted into the final multivariable model via stepwise elimination using a cutoff of p < 0.1. This process ensured that only the most significant features were retained.\n\nFor the LASSO regression model, feature selection was integrated into the modeling process. LASSO regression shrinks the coefficients of less relevant or collinear features to zero, effectively performing feature selection by reducing the number of variables in the model. The optimal shrinking parameter (\u03bb) was determined through 10-fold cross-validation of the training dataset.\n\nIn the case of the classification and regression trees (CART) model, feature selection is inherently part of the tree-building process. The Gini index is used to iteratively split branches based on purity, which helps in identifying the most important interactions and features. The final tree size was decided by finding the number of splits that produced the smallest cross-validation error, ensuring that only the most predictive features were included.\n\nAll feature selection processes were performed using the training set only, which consisted of 70% of the participants. This approach helped to prevent overfitting and ensured that the models could generalize well to the validation set.",
  "optimization/fitting": "In our study, we employed three different predictive modeling methods: logistic regression, LASSO regression, and classification and regression trees (CART). Each method has its own approach to handling the potential issues of overfitting and underfitting.\n\nFor logistic regression, we performed univariate analysis on all candidate variables and then used stepwise elimination to fit the final multivariable model. This approach helps in selecting the most relevant variables, thereby reducing the risk of overfitting. By setting a cutoff of p < 0.1, we ensured that only significant variables were included in the model, which helps in avoiding underfitting.\n\nLASSO regression was used to address the problem of overfitting, especially in datasets with a large number of variables. LASSO performs variable selection and regularization to enhance the prediction accuracy and interpretability of the statistical model it produces. It shrinks the coefficients of less relevant features to zero, effectively performing feature selection and reducing the model complexity. We selected an optimal shrinking parameter (\u03bb) through 10-fold cross-validation, which helps in finding the right balance between bias and variance, thus preventing overfitting.\n\nCART, or classification and regression trees, is robust to outliers and can handle multicollinearity and skewed covariates effectively. To prevent overfitting, we set a maximum tree depth of 5 and required a minimum of 10 subjects in a node before attempting a split. The final tree size was chosen based on the number of splits that produced the smallest cross-validation error. This method ensures that the model is not too complex and generalizes well to new data, thus avoiding overfitting.\n\nIn summary, we used different techniques tailored to each modeling method to address the issues of overfitting and underfitting. Logistic regression relied on stepwise elimination and significance cutoffs, LASSO used regularization and cross-validation, and CART employed tree depth and node size constraints to ensure robust and generalizable models.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting in our predictive models. One of the key methods used was LASSO (Least Absolute Shrinkage and Selection Operator) regression. LASSO is particularly useful for datasets with a large number of variables, as it helps to shrink the coefficients of less relevant features or those exhibiting collinearity to zero. This process reduces the complexity of the model, thereby minimizing overfitting while maintaining a low bias.\n\nAdditionally, we utilized classification and regression trees (CART). CART is robust to outliers and can handle multicollinearity and skewed covariates effectively. To further prevent overfitting, we set a maximum tree depth of 5 and required a minimum of 10 subjects in a node before attempting a split. The final tree size was determined by identifying the number of splits that produced the smallest cross-validation error, ensuring that the model generalizes well to unseen data.\n\nFor the logistic regression model, we performed univariate analysis on all candidate variables and then used stepwise elimination to fit the final multivariable model. This approach helps in selecting the most relevant variables, reducing the risk of overfitting.\n\nOverall, these techniques\u2014LASSO regression, CART with controlled tree depth and node size, and stepwise elimination in logistic regression\u2014were integral to our strategy for building robust and generalizable predictive models.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in this study offer varying degrees of interpretability. The logistic regression model is highly transparent, providing clear insights into the relationship between predictors and the outcome. It allows for straightforward interpretation of coefficients, indicating the direction and magnitude of the effect of each predictor on the likelihood of antibiotic prescription. For instance, higher temperatures and pulse rates were consistently identified as important predictors across all models, suggesting a direct relationship with the need for antibiotics.\n\nThe LASSO regression model also offers interpretability by shrinking less relevant coefficients to zero, effectively performing feature selection. This results in a more parsimonious model where the remaining coefficients can be interpreted similarly to those in logistic regression. The LASSO model included predictors such as Indian ethnicity, fever, giddiness, and cancer status, providing a clear set of factors influencing antibiotic prescription decisions.\n\nIn contrast, the Classification and Regression Trees (CART) model, while powerful in handling complex interactions and non-linear relationships, is less transparent. CART models use a series of decision rules to split the data, which can be visualized through a decision tree. This tree structure allows for the identification of patient subgroups that are more predictive of the outcome. However, the interpretability of CART models can be challenging due to the hierarchical nature of the splits and the potential for multiple paths leading to the same outcome. Despite this, CART models are robust to outliers and can handle multicollinearity effectively, making them a valuable tool in predictive modeling.\n\nOverall, while logistic regression and LASSO regression offer more straightforward interpretability, CART models provide a different perspective by highlighting important interactions and subgroup effects. The combination of these models offers a comprehensive approach to understanding and predicting antibiotic prescription decisions.",
  "model/output": "The models developed in this study are classification models. They were designed to predict whether antibiotics were needed for patients with uncomplicated upper respiratory tract infections (URTIs). Specifically, three different types of classification models were employed: logistic regression, LASSO regression, and classification and regression trees (CART). These models were used to identify key predictors and to classify patients into those who require antibiotics and those who do not. The performance of these models was evaluated using metrics such as the area under the receiver operating characteristic curve (AUC), sensitivity, specificity, positive predictive value, and negative predictive value. The highest AUC was observed in the logistic regression model, followed by the LASSO model and then the CART model. The models were validated on a separate dataset to ensure their generalizability and reliability in clinical settings.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method involved several steps to ensure the robustness and generalizability of the predictive models. Initially, the dataset was split into a training set comprising 70% of the participants and a validation set with the remaining 30%. This split allowed for the development and tuning of the models on the training set while evaluating their performance on unseen data from the validation set.\n\nFor the logistic regression model, univariate analysis was conducted on all candidate variables. Significant variables and clinically relevant factors were then included in a multivariable model using stepwise elimination with a cutoff of p < 0.1. The performance of the logistic regression model was assessed using the area under the receiver operating characteristic curve (AUC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) on the validation set.\n\nThe LASSO regression model was evaluated using a 10-fold cross-validation on the training dataset to select the optimal shrinking parameter (\u03bb). This process helped in reducing overfitting and identifying the most relevant features. The performance metrics for the LASSO model were similarly calculated on the validation set.\n\nThe classification and regression trees (CART) model was built with a maximum tree depth of 5 and a minimum of 10 subjects in a node before attempting a split. The final tree size was determined by finding the number of splits that produced the smallest cross-validation error. The CART model's performance was also evaluated using the same metrics on the validation set.\n\nThe optimal cutoffs for each model were decided by selecting the predicted probability that achieved the highest sensitivity with a specificity of at least 0.4. This approach ensured that the models were balanced in terms of sensitivity and specificity, providing a reliable tool for clinical decision-making.\n\nIn summary, the evaluation method involved a rigorous process of model development, tuning, and validation using a split dataset and cross-validation techniques. The performance of the models was assessed using standard metrics, ensuring that they could be reliably deployed in clinical settings.",
  "evaluation/measure": "In our study, we evaluated the performance of three prediction models\u2014logistic regression, LASSO regression, and classification and regression trees (CART)\u2014using several key metrics. The primary metric reported was the area under the receiver operating characteristic curve (AUC), which provides an aggregate measure of performance across all classification thresholds. The AUC values for the logistic, LASSO, and CART models were 0.72, 0.70, and 0.67, respectively, indicating modest but comparable performance among the models.\n\nIn addition to AUC, we reported sensitivity and specificity at optimal cutoff points. Sensitivity, or the true positive rate, measures the proportion of actual positives correctly identified by the model. Specificity, or the true negative rate, measures the proportion of actual negatives correctly identified. For the logistic model, sensitivity and specificity were 0.72 and 0.65, respectively. The LASSO model had similar sensitivity at 0.72 but slightly lower specificity at 0.62. The CART model exhibited the highest sensitivity at 0.77 but the lowest specificity at 0.49.\n\nWe also calculated the positive predictive value (PPV) and negative predictive value (NPV) for each model. PPV indicates the probability that a positive test result is a true positive, while NPV indicates the probability that a negative test result is a true negative. The logistic model had a PPV of 0.78 and an NPV of 0.56. The LASSO model showed a PPV of 0.77 and an NPV of 0.55. The CART model had the lowest PPV at 0.62 and the highest NPV at 0.66.\n\nThese metrics are representative of those commonly reported in the literature for similar prediction models. They provide a comprehensive view of model performance, balancing the trade-offs between sensitivity and specificity. The choice of metrics ensures that our evaluation is robust and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we employed three distinct predictive modeling methods\u2014logistic regression, LASSO regression, and classification and regression trees (CART)\u2014to develop algorithms for guiding antibiotic prescribing decisions in the emergency department. These methods were chosen for their ease of implementation and deployment in clinical settings.\n\nTo ensure the robustness of our models, we compared their performance using several metrics, including the area under the receiver operating characteristic curve (AUC), sensitivity, specificity, positive predictive value, and negative predictive value. The AUC values for the logistic, LASSO, and CART models were 0.72, 0.70, and 0.67, respectively, indicating that all models performed similarly in terms of discriminative ability.\n\nWe also evaluated the models at different probability cutoffs to assess their sensitivity and specificity. At a cutoff of 0.6, the logistic model achieved a sensitivity of 0.72 and a specificity of 0.65, while the LASSO model had a sensitivity of 0.72 and a specificity of 0.62. The CART model, although having the highest sensitivity of 0.77, was the least specific at 0.49.\n\nIn addition to these comparisons, we explored more complex classification methods, such as random forests and deeper classification trees. However, these advanced techniques did not show significant improvement in performance, with AUC values around 0.7. This aligns with findings from a recent systematic review, which suggested that machine learning techniques do not necessarily outperform traditional regression models in all scenarios.\n\nOur approach also included a pragmatic consideration of clinical utility. The models were designed to be easily integrated into existing clinical workflows, either through an Excel sheet or the hospital's electronic system, without requiring complex programming. This ensures that the models can be readily adopted by physicians in their decision-making processes.\n\nIn summary, our comparison of different predictive modeling methods revealed that all three approaches\u2014logistic regression, LASSO regression, and CART\u2014provided similarly modest performance. The choice of model can be tailored to the specific needs of the clinical setting, with considerations for ease of use and integration into existing systems.",
  "evaluation/confidence": "The performance metrics for the models presented in this study include confidence intervals. For instance, the area under the receiver operating characteristic curve (AUC) for the logistic model is reported as 0.72 with a 95% confidence interval (CI) of 0.65 to 0.79. Similarly, the sensitivity and specificity for the logistic model on the validation set are provided with their respective 95% CIs. This approach ensures that the reported performance metrics are accompanied by measures of uncertainty, allowing for a more nuanced interpretation of the results.\n\nThe statistical significance of the models' performance is also considered. The models were evaluated using a validation set, which is a standard practice to assess their generalizability. The use of cross-validation, particularly 10-fold cross-validation for the LASSO model, further enhances the reliability of the results. The optimal cutoffs for the models were determined by achieving the highest sensitivity with a specificity of at least 0.4, which provides a balanced approach to evaluating model performance.\n\nWhile the performance of the three models\u2014logistic regression, LASSO regression, and classification and regression trees (CART)\u2014is reported to be similarly modest, the study acknowledges that more complex models like classification trees and random forests did not show significant improvement. This suggests that the simpler models are sufficient for the task at hand, and their performance is statistically sound within the context of the data and methods used.\n\nIn summary, the performance metrics are robustly reported with confidence intervals, and the results are statistically significant. The models' performance is evaluated rigorously, ensuring that the claims about their effectiveness are supported by reliable statistical evidence.",
  "evaluation/availability": "Not enough information is available."
}