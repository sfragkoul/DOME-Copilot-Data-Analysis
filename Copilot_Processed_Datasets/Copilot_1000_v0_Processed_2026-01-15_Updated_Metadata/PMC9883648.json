{
  "publication/title": "[Development and validation of an automatic diagnostic tool for lumbar stability based on deep learning].",
  "publication/authors": "Hu H, Wang X, Yang H, Zhang J, Li K, Zeng J",
  "publication/journal": "Zhongguo xiu fu chong jian wai ke za zhi = Zhongguo xiufu chongjian waike zazhi = Chinese journal of reparative and reconstructive surgery",
  "publication/year": "2023",
  "publication/pmid": "36708120",
  "publication/pmcid": "PMC9883648",
  "publication/doi": "10.7507/1002-1892.202209058",
  "publication/tags": "- Artificial Intelligence\n- Medical Imaging\n- Spine Stability\n- Deep Learning\n- Neural Networks\n- Cobb Angle\n- Vertebral Slippage\n- Diagnostic Tools\n- Radiology\n- Clinical Diagnosis",
  "dataset/provenance": "The dataset used in this study was sourced from patients who underwent lateral anterior fusion surgery at the L4 and L5 levels due to lumbar spine diseases at the Department of Orthopedics, West China Hospital, Sichuan University, between October 2013 and March 2020. The dataset consists of preoperative lumbar spine X-ray images in the extension and flexion positions, totaling 306 images from 153 patients. These images were in DICOM format and were anonymized using a custom tool to remove sensitive information such as patient names, hospital numbers, ID numbers, and contact information.\n\nThe images were manually annotated by three orthopedic surgeons using the AI-Tutor annotation tool developed by West China Hospital. Each image had five key points annotated: the posterior-inferior corner (A point), anterior-inferior corner (B point) of L4, and the posterior-superior corner (C point), anterior-superior corner (D point), and posterior-inferior corner (E point) of L5. The annotated images were then converted to PNG format, and the coordinate information was saved in text files.\n\nThe dataset was randomly split into three subsets: a training set with 156 images, a validation set with 50 images, and a test set with 100 images, following a 3:1:2 ratio. This division ensured that the model could be trained, validated, and tested on distinct sets of data, enhancing the robustness and generalizability of the results.",
  "dataset/splits": "The dataset was divided into three splits: training, validation, and test sets. The distribution of data points in each split was as follows: 156 data points for training, 50 for validation, and 100 for testing. The ratio used for this split was 3:1:2.",
  "dataset/redundancy": "The dataset used in this study consisted of 306 lumbar X-ray films. These images were randomly divided into three sets: training, validation, and test sets, in a ratio of 3:1:2. Specifically, 156 images were allocated to the training set, 50 to the validation set, and 100 to the test set. This division ensured that the training and test sets were independent, which is crucial for evaluating the model's performance on unseen data.\n\nTo enforce the independence of the datasets, a random splitting method was employed. This approach helps to mitigate any potential bias that could arise from non-random selection processes. The random split ensures that each subset of the data is representative of the overall dataset, providing a robust evaluation of the model's generalization capabilities.\n\nComparing this dataset distribution to previously published machine learning datasets in medical imaging, the approach aligns with common practices. Many studies in this field use similar ratios for splitting datasets to ensure a balanced evaluation of model performance. The use of a validation set is also standard, as it allows for hyperparameter tuning and model selection without directly influencing the final performance metrics on the test set.\n\nThe random splitting method is a well-established technique in machine learning to ensure that the training and test sets are independent. This method helps to prevent data leakage, where information from the test set might inadvertently influence the training process. By maintaining the independence of the datasets, the study ensures that the results are reliable and generalizable to new, unseen data.",
  "dataset/availability": "The data used in this study is not publicly available. The dataset consists of X-ray images from patients who underwent lumbar fusion surgery at a specific hospital. Due to the sensitive nature of medical data, including patient information and radiological images, the dataset was not released in a public forum. Instead, the data was carefully managed and used internally for the research purposes described.\n\nThe dataset was divided into training, validation, and test sets in a 3:1:2 ratio, respectively. This split was enforced randomly to ensure that the model's performance could be evaluated on unseen data. The images were annotated by medical professionals using a specialized tool, and the annotations were used to train and evaluate the neural network model.\n\nThe study adhered to strict ethical guidelines and regulations regarding the use of patient data. All sensitive information was removed from the images using a custom-built de-identification tool. This process ensured that patient privacy was protected throughout the research process. The dataset was used solely for the purpose of developing and evaluating the Swin-PGNet model, and no external parties had access to the raw data.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages a two-stage process for key point localization in lumbar spine X-ray images. The first stage involves a coarse localization to identify the regions of interest, specifically the L4 and L5 vertebrae. This is achieved by processing the images through a neural network that outputs a heatmap indicating the probable locations of these vertebrae. The second stage focuses on refining the localization by zooming into the areas identified in the first stage, providing a more precise determination of the key points.\n\nThe machine-learning algorithm class used is a convolutional neural network (CNN) combined with a self-attention mechanism. Specifically, we utilize a variant of the Swin Transformer, integrated with a convolutional neural network, forming what we refer to as Swin-PGNet. This hybrid approach allows the model to capture both local and global features effectively, which is crucial for accurate key point localization in medical images.\n\nThe algorithm is not entirely new but represents an innovative application of existing techniques tailored to the specific challenges of medical imaging. The integration of the Swin Transformer with convolutional layers is designed to enhance the model's ability to handle the large-scale and high-resolution nature of lumbar spine X-ray images. This combination leverages the strengths of both transformer and convolutional architectures, making it well-suited for the task at hand.\n\nThe reason this algorithm was not published in a machine-learning journal is that the primary focus of our work is on its application in the medical field, specifically in the context of lumbar spine analysis. The innovation lies in the adaptation and optimization of existing machine-learning techniques for a specialized medical task, rather than the development of a entirely new algorithm. Our study aims to demonstrate the practical utility of these advanced machine-learning methods in improving diagnostic accuracy and efficiency in clinical settings.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It is a standalone neural network architecture designed for specific tasks related to medical imaging, particularly the analysis of lumbar spine X-ray images. The model, referred to as Swin-PGNet, operates in two stages: a coarse localization stage and a fine localization stage. It does not incorporate data from other machine-learning algorithms as input. Instead, it processes X-ray images directly to predict key points, Cobb angles, and vertebral slip distances.\n\nThe training process involves a combination of convolutional neural networks and self-attention mechanisms, but these components are integrated within a single model rather than being separate algorithms whose outputs are combined. The model is trained end-to-end, meaning that it learns from the raw input data without relying on intermediate predictions from other models.\n\nRegarding the independence of training data, the model is trained on a dataset of lumbar spine X-ray images. The images are preprocessed to ensure consistency, including resizing and normalization. Data augmentation techniques are applied to increase the diversity of the training set, which helps the model generalize better. The training data consists of images from different patients in both flexion and extension positions, ensuring a varied and independent dataset. This approach helps in avoiding overfitting and ensures that the model can perform well on unseen data.",
  "optimization/encoding": "The data encoding process for the machine-learning algorithm involved a two-stage approach to handle the large size of the X-ray images. Initially, the X-ray images were resized to have a pixel spacing of 0.1 mm/pixel. These images were then padded with zero pixels to achieve a 1:1 aspect ratio and subsequently resized to 256x256 pixels for input into the model. This preprocessing step ensured that the images were standardized in size and aspect ratio, which is crucial for consistent input into the neural network.\n\nIn the first stage of the algorithm, the goal was to perform a coarse localization of the key points, specifically identifying the L4 and L5 regions. The images were processed to predict the coordinates of these regions, and then a 512x512 pixel region centered around these predicted coordinates was extracted. This region was further resized to 256x256 pixels for the second stage of the algorithm.\n\nFor the second stage, the training dataset was created by randomly selecting a point within a 50-pixel radius of the true key point coordinates. A 512x512 pixel region centered around this point was then extracted and resized to 256x256 pixels. This approach ensured that the model could focus on the relevant areas of the image for precise key point localization.\n\nThe images used in both stages were single-channel grayscale images, which simplified the input data for the neural network. The network structure itself was designed to handle these preprocessed images effectively, utilizing a combination of convolutional and self-attention mechanisms to extract and fuse features from different depths of the image.\n\nData augmentation techniques were employed to enhance the robustness of the model. These techniques included random translations in the x and y directions by up to 10 pixels, random scaling of pixel intensities between 0.5 and 1.0, random scaling of the image size between 95% and 105% of the original, random rotations of up to 3 degrees, and elastic transformations. These augmentations helped to make the model more generalizable by exposing it to a variety of image variations during training.\n\nThe network was trained using a random gradient descent optimizer with an initial learning rate of 0.001 and a batch size of 4. The learning rate was reduced by a factor of 10 at the 4th, 6th, and 8th epochs to fine-tune the model's performance. The training process involved 20 epochs, and the model that performed best on the validation set was selected as the final model. The output channels responsible for predicting the heatmap were processed through a sigmoid function, and cross-entropy loss was used to define the loss for this part of the network. For training the offset regression head, a Huber robust loss was employed to penalize the differences between the predicted and true offsets.",
  "optimization/parameters": "The model employs a two-stage approach for keypoint localization in lumbar spine X-ray images. In the first stage, the input images are preprocessed to have a pixel spacing of 0.1 mm/pixel, padded to a 1:1 aspect ratio, and resized to 256\u00d7256 pixels. This stage focuses on coarse localization of the L4 and L5 vertebrae.\n\nFor the second stage, a 512\u00d7512 pixel region centered around the predicted coordinates from the first stage is extracted and resized to 256\u00d7256 pixels. This stage refines the keypoint localization within the identified region.\n\nThe network architecture includes Swin Transformer blocks (STB) and PGFB modules, which integrate features from convolutional layers and self-attention mechanisms. The specific dimensions and channels at various stages of the network are carefully designed to balance computational efficiency and feature representation.\n\nThe training process utilizes a random gradient descent optimizer with an initial learning rate of 0.001 and a batch size of 4. The learning rate is reduced by a factor of 10 at the 4th, 6th, and 8th epochs. Data augmentation techniques, implemented using the imgaug library, include random translations, intensity scaling, scaling, rotations, and elastic transformations.\n\nThe loss function combines a heatmap loss and an offset regression loss, with weights of 4 and 1, respectively. This combination ensures a balanced focus on both keypoint localization and offset prediction.\n\nThe model is evaluated on a test set of 100 lumbar spine X-ray images, comparing the predicted keypoints, Cobb angles, and vertebral slip distances against manual annotations by medical experts. The evaluation metrics include mean errors for keypoint localization, Cobb angle measurement, and vertebral slip distance, as well as the accuracy of diagnosing lumbar instability and spondylolisthesis.\n\nThe network's parameters are selected to optimize performance on the given task, with a focus on achieving high accuracy in keypoint localization and clinical measurements. The architecture and training regimen are designed to leverage the strengths of both convolutional neural networks and transformer-based models, ensuring robust and precise predictions.",
  "optimization/features": "The input features for the model are derived from X-ray images of the lumbar spine, specifically focusing on the L4 and L5 vertebrae. The images are preprocessed to have a pixel spacing of 0.1 mm/pixel and are resized to 256x256 pixels. The input to the network is a single-channel grayscale image.\n\nFeature selection in the traditional sense is not applicable here, as the features are the pixel values of the X-ray images themselves. However, the model does involve a form of feature extraction and fusion. The images are processed through a convolutional neural network (ResNet34) and a Swin Transformer (ST) module. The convolutional network has layers with channel dimensions set to [32, 32, 64, 128, 256], and the ST module processes the images through multiple stages with increasing channel dimensions.\n\nThe fusion of features from different layers is performed using a Patch Gating Fusion Block (PGFB) module. This module takes the features from the convolutional network and the ST module, concatenates them in the channel dimension, and then processes them further. The specific dimensions of the features at different stages of the network are well-defined, ensuring that the features are appropriately fused and processed.\n\nThe training process involves two stages: coarse localization and fine localization. In the first stage, the model predicts the approximate locations of the L4 and L5 vertebrae. In the second stage, the model focuses on a smaller region around the predicted locations to refine the predictions. This two-stage approach helps in accurately identifying the key points on the X-ray images.\n\nThe model is trained using a combination of cross-entropy loss for the heatmap and Huber robust loss for the offset maps. The total loss function is a weighted sum of these two losses, with weights set to balance their contributions. The training process includes data augmentation techniques such as random translation, scaling, rotation, and elastic transformation to improve the model's robustness.\n\nIn summary, the input features are the pixel values of the preprocessed X-ray images, and the model employs a sophisticated feature extraction and fusion mechanism to accurately predict the key points on the lumbar spine.",
  "optimization/fitting": "The fitting method employed in this study involves a two-stage training process designed to effectively handle the complexity of the data while mitigating issues of overfitting and underfitting.\n\nThe model, Swin-PGNet, utilizes a combination of ST (Swin Transformer) and ResNet34 architectures, which together provide a robust feature extraction capability. The use of these architectures ensures that the model has a sufficient number of parameters to capture the intricate details of the X-ray images, thus avoiding underfitting. The ResNet34, with its deep convolutional layers, and the ST module, with its windowed multi-head self-attention mechanism, work in tandem to encode both local and global features of the images.\n\nTo address the potential issue of overfitting, several strategies were implemented. First, data augmentation techniques were applied using the imgaug library. These techniques included random translations, scaling of pixel intensities, slight resizing, rotations, and elastic transformations. These augmentations help in creating a more diverse training set, which in turn helps the model generalize better to unseen data.\n\nAdditionally, the training process involved a learning rate schedule where the learning rate was reduced by a factor of 10 at specific epochs (4, 6, and 8). This gradual reduction helps in fine-tuning the model parameters more effectively, preventing the model from converging too quickly to a suboptimal solution.\n\nThe model was trained for 20 epochs, and the best-performing model on the validation set was selected as the final model. This approach ensures that the model is not overfitted to the training data but rather generalizes well to the validation data.\n\nFurthermore, the use of a Huber robust loss function for the offset regression head helps in making the model more robust to outliers, thereby improving its generalization capability.\n\nIn summary, the fitting method carefully balances the complexity of the model with regularization techniques and a well-designed training strategy to avoid both overfitting and underfitting.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and enhance the generalization of our model. One of the key methods used was data augmentation, which involved applying various transformations to the training images. These transformations included random translations in the x and y directions by up to 10 pixels, random scaling of pixel intensities between 0.5 and 1.0, random resizing of the images to maintain the original aspect ratio within 0.95 to 1.05, and random rotations of up to 3 degrees in either direction. Additionally, elastic transformations were applied to each image to further diversify the training dataset.\n\nAnother important regularization technique was the use of a learning rate schedule. We started with an initial learning rate of 0.001 and reduced it by a factor of 10 at specific epochs, namely the 4th, 6th, and 8th epochs. This gradual reduction helped in fine-tuning the model parameters more effectively as training progressed.\n\nWe also utilized a combination of loss functions to balance the training process. The total loss function was a weighted sum of two components: a heatmap loss and an offset loss. The heatmap loss was defined using a sigmoid function and cross-entropy loss, which helped in accurately predicting the presence of key points. The offset loss, on the other hand, used the Huber robust loss to penalize the differences between predicted and actual offsets, ensuring precise localization of the key points.\n\nFurthermore, the architecture of our model, Swin-PGNet, incorporated skip connections and residual blocks, which are known to aid in mitigating overfitting by allowing gradients to flow more easily through the network during backpropagation. The use of pre-trained weights from the ImageNet dataset also provided a robust starting point for our model, leveraging features learned from a large and diverse dataset.\n\nIn summary, our approach to regularization involved a combination of data augmentation, learning rate scheduling, a balanced loss function, and architectural choices that collectively contributed to the prevention of overfitting and improved the model's performance on unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule are reported in detail. The initial learning rate is set to 0.001, and the learning rate is reduced by a factor of 10 at the 4th, 6th, and 8th epochs. The training batch size is 4. The optimization process uses the stochastic gradient descent optimizer. The total number of training epochs is 20.\n\nThe data augmentation techniques applied include random translations in the x and y directions by up to 10 pixels, random scaling of pixel intensities between 0.5 and 1.0, random scaling of the image size between 95% and 105% of the original dimensions, random rotations of up to 3 degrees, and elastic transformations. The model is initialized with pre-trained weights from the ImageNet dataset.\n\nThe final model is selected based on the best performance on the validation set. The loss functions used include a cross-entropy loss for the heatmap output and a Huber robust loss for the offset regression head. The total loss function is a weighted sum of these two losses, with weights of 4 and 1, respectively.\n\nThe specific model files and optimization parameters are not explicitly mentioned in the provided details. However, the implementation details and the training process are thoroughly described, allowing for reproducibility. The methods and techniques used are standard and widely recognized in the field, ensuring that the configurations can be easily adapted and implemented by other researchers.",
  "model/interpretability": "The model discussed in this publication, Swin-PGNet, incorporates several design choices that enhance its interpretability, making it more transparent than typical black-box models. This transparency is achieved through a combination of architectural innovations and methodological approaches.\n\nOne key aspect of Swin-PGNet's transparency is its use of the Swin Transformer (ST) module, which leverages window multi-head self-attention (W-MSA) and shifted window multi-head self-attention (SW-MSA). These mechanisms allow the model to focus on local and global relationships within the image, providing a clearer understanding of how different parts of the image contribute to the final prediction. The ST module's ability to capture both local and global context helps in interpreting the model's decisions, as it can highlight which regions of the image are most influential.\n\nAdditionally, the model employs a Progressive Fusion Block (PGFB) that integrates features from different depths, enhancing the network's ability to learn complex relationships. This fusion process allows for a more granular understanding of how various layers of the network contribute to the final output, making it easier to trace back the model's decisions to specific features in the input image.\n\nThe use of a two-stage process\u2014first extracting the region of interest and then refining the key point locations\u2014also contributes to the model's interpretability. This approach ensures that the model's focus is directed towards the most relevant parts of the image, making it easier to understand which areas are critical for the final prediction.\n\nFurthermore, the model's evaluation metrics, such as key point localization error, Cobb angle measurement error, and vertebral slip distance measurement error, provide clear and quantifiable ways to assess the model's performance. These metrics offer insights into the model's strengths and weaknesses, allowing for a more nuanced understanding of its behavior.\n\nIn summary, Swin-PGNet's architecture and evaluation methods contribute to its transparency, making it a more interpretable model compared to traditional black-box approaches. The use of ST modules, PGFB, and a two-stage process all play crucial roles in enhancing the model's interpretability, providing clear examples of how the model arrives at its predictions.",
  "model/output": "The model, Swin-PGNet, is primarily designed for regression tasks rather than classification. It focuses on predicting continuous values related to spinal measurements, such as the Cobb angle and vertebral slip distance. These measurements are crucial for assessing spinal stability and diagnosing conditions like spinal instability and spondylolisthesis.\n\nThe model's output includes several key metrics:\n\n* Key point localization error, which measures the average error in identifying specific points on the vertebrae.\n* Cobb angle measurement error, which quantifies the difference between the predicted and actual angles between vertebrae.\n* Vertebral slip distance measurement error, which assesses the discrepancy between the predicted and actual distances of vertebral slippage.\n\nAdditionally, the model provides diagnostic accuracy for spinal instability and spondylolisthesis based on the predicted Cobb angles and vertebral slip distances. The model's performance is evaluated using metrics such as average error and intraclass correlation coefficients (ICC), which indicate the consistency of the model's predictions with those of human experts.\n\nThe final output of the model is a comprehensive assessment of spinal stability, including the prediction of key points, Cobb angles, and vertebral slip distances, along with diagnostic accuracy for spinal conditions. This information is valuable for clinicians in diagnosing and treating spinal disorders.",
  "model/duration": "The model was trained for a total of 20 epochs. The training process utilized a batch size of 4 and employed a learning rate schedule where the learning rate was reduced by a factor of 10 at the 4th, 6th, and 8th epochs. The optimization was performed using the stochastic gradient descent (SGD) algorithm with an initial learning rate of 0.001.\n\nThe training dataset underwent various data augmentation techniques, including random translations in the x and y directions by up to 10 pixels, random scaling of pixel intensities between 0.5 and 1.0, random scaling of the image size between 95% and 105% of the original dimensions, random rotations of up to 3 degrees, and elastic transformations. These augmentations were implemented using the Python library imgaug.\n\nThe model's architecture incorporated pre-trained weights from the ImageNet dataset for both the ST module and the convolutional neural network components. This pre-training helped in leveraging learned features from a large and diverse dataset, potentially accelerating the convergence during training.\n\nThe evaluation of the model's performance was conducted on a test set consisting of 100 lumbar spine X-ray images, which included both flexion and extension views from 50 patients. The evaluation metrics focused on key aspects such as the average error in key point localization, the average error in measuring the Cobb angle between L4 and L5 vertebrae, and the average error in measuring the slip distance of the L4 vertebra. Additionally, the model's accuracy in diagnosing lumbar instability and lumbar spondylolisthesis was assessed.\n\nThe results indicated that the model achieved an average key point localization error of 1.407 \u00b1 0.939 mm, which was comparable to the inter-observer variability among medical professionals. The model's performance in measuring the Cobb angle and slip distance also showed high consistency with the annotations provided by the medical experts, demonstrating its potential for clinical application in assessing lumbar spine stability and diagnosing related conditions.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the proposed method, Swin-PGNet, was conducted using a comprehensive approach that involved multiple metrics and comparisons with human experts. The evaluation dataset consisted of 100 lumbar X-ray images, including both flexion and extension views, from 50 patients. These images were used to assess the performance of Swin-PGNet in key point localization, Cobb angle measurement, and lumbar sliding distance measurement.\n\nThe evaluation metrics included the mean error for key point localization, Cobb angle measurement, and lumbar sliding distance measurement. Additionally, the intra-class correlation coefficient (ICC) was used to assess the consistency between Swin-PGNet and human annotations. The accuracy of diagnosing lumbar instability and spondylolisthesis was also evaluated.\n\nFor key point localization, the mean error of Swin-PGNet was found to be significantly lower than that of human experts, indicating superior performance. The Cobb angle measurement task showed that Swin-PGNet achieved a mean error of 2.062\u00b11.352\u00b0, which was lower than the inter-expert variability, demonstrating its reliability. The lumbar sliding distance measurement task revealed that Swin-PGNet performed slightly better than human experts, with a mean error of 1.656\u00b10.878 mm.\n\nThe ICC values further supported the consistency of Swin-PGNet's annotations with those of human experts. For Cobb angle measurement, the ICC between Swin-PGNet and human experts was 0.922, indicating excellent agreement. For lumbar sliding distance measurement, the ICC was 0.748, showing good agreement.\n\nIn terms of diagnostic accuracy, Swin-PGNet achieved an accuracy of 84.0% for lumbar instability diagnosis, which was higher than the 75.3% accuracy of human experts. For spondylolisthesis diagnosis, Swin-PGNet's accuracy was 71.3%, slightly higher than the 70.7% accuracy of human experts.\n\nOverall, the evaluation demonstrated that Swin-PGNet is a reliable and accurate tool for automatic diagnosis of lumbar instability and spondylolisthesis, with performance comparable to or better than human experts. The use of a two-stage method for key point localization helped to minimize system errors, contributing to the high accuracy and consistency of the results.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report several key metrics to evaluate the performance of our model, Swin-PGNet, in analyzing lumbar spine X-ray images. These metrics are designed to assess both the accuracy of key point localization and the clinical relevance of the measurements derived from these points.\n\nFirstly, we report the average error in key point localization, measured in millimeters. This metric indicates how accurately the model can identify specific points on the vertebrae, which is crucial for subsequent measurements. We compare this error to the average error between different human annotators to provide context for the model's performance.\n\nSecondly, we evaluate the model's performance in measuring the Cobb angle between the L4 and L5 vertebrae. The Cobb angle is a critical indicator of spinal deformity, and accurate measurement is essential for clinical diagnosis. We report the average error in Cobb angle measurement and compare it to the inter-annotator variability among human experts.\n\nThirdly, we assess the model's ability to measure vertebral slip distance, another important clinical indicator. We report the average error in this measurement and compare it to the variability among human annotators.\n\nIn addition to these error metrics, we also report the accuracy of the model in diagnosing lumbar instability and vertebral slip. These clinical diagnoses are based on thresholds applied to the measured Cobb angles and vertebral slip distances. We compare the model's diagnostic accuracy to that of human experts.\n\nFurthermore, we use Intraclass Correlation Coefficient (ICC) to evaluate the consistency of the model's measurements with those of human experts. This metric provides a measure of agreement between the model and human annotators, indicating the reliability of the model's outputs.\n\nThese performance metrics are representative of the state-of-the-art in medical image analysis, particularly in the context of spinal imaging. They provide a comprehensive evaluation of the model's technical accuracy and its clinical relevance, ensuring that the model's performance is both precise and meaningful in a real-world clinical setting.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of the Swin-PGNet model with medical professionals to assess its performance in key tasks related to lumbar spine analysis. We evaluated the model on a test set consisting of 100 lumbar spine X-ray images, including both flexion and extension views from 50 patients.\n\nThe comparison focused on several critical metrics:\n\n1. **Key Point Localization Error**: We measured the average error in locating key points on the vertebrae. The Swin-PGNet model demonstrated a significantly lower average error compared to the inter-observer variability among medical professionals. This indicates that the model's key point localization is more consistent and accurate.\n\n2. **Cobb Angle Measurement Error**: The Cobb angle, which measures the curvature of the spine, was evaluated for accuracy. The Swin-PGNet model achieved an average error of (2.062 \u00b1 1.352)\u00b0, which is considerably lower than the inter-observer error among medical professionals, reported at (3.580 \u00b1 2.338)\u00b0. This suggests that the model can reliably measure spinal curvature with high precision.\n\n3. **Vertebral Slippage Measurement Error**: For measuring vertebral slippage, the Swin-PGNet model showed an average error of (1.656 \u00b1 0.878) mm, slightly better than the average error reported by medical professionals, which was (1.884 \u00b1 1.612) mm. This indicates that the model performs comparably to human experts in this task.\n\n4. **Clinical Judgment**: The model's performance in diagnosing lumbar instability and vertebral slippage was also assessed. Swin-PGNet achieved an accuracy of 84.0% for diagnosing lumbar instability, outperforming the medical professionals' average accuracy of 75.3%. For diagnosing vertebral slippage, the model's accuracy was 71.3%, slightly higher than the 70.7% accuracy of medical professionals.\n\nIn summary, the Swin-PGNet model demonstrates superior or comparable performance to medical professionals in key tasks related to lumbar spine analysis. The model's consistency and accuracy in key point localization, Cobb angle measurement, and clinical judgment highlight its potential for assisting in diagnostic and treatment planning for spinal conditions.",
  "evaluation/confidence": "The evaluation of the Swin-PGNet model includes several performance metrics with associated confidence intervals, providing a clear indication of the reliability of the results. For instance, the intraclass correlation coefficient (ICC) for the Cobb angle measurement between the model and all physicians is reported as 0.922 with a 95% confidence interval of (0.891, 0.938), and for the vertebral slip distance, it is 0.748 with a 95% confidence interval of (0.726, 0.783). These intervals help in understanding the precision of the estimates.\n\nStatistical significance is also a key aspect of the evaluation. The p-values for the performance metrics are consistently reported, with many results showing P<0.001, indicating strong statistical significance. For example, the comparison of the model's performance against individual physicians and the combined physician group shows P<0.001 for various metrics, suggesting that the differences observed are unlikely to be due to chance.\n\nThe model's performance in key tasks such as key point localization, Cobb angle measurement, and vertebral slip distance measurement is not only quantitatively superior but also statistically significant. The model's average error in Cobb angle measurement is (2.062\u00b11.352)\u00b0, which is significantly lower than the inter-physician average error of (3.580\u00b12.338)\u00b0. Similarly, the model's accuracy in diagnosing lumbar instability (84.0%) is notably higher than that of physicians (75.3%), with the differences being statistically significant.\n\nIn summary, the evaluation provides robust confidence intervals and statistically significant results, reinforcing the claim that the Swin-PGNet model performs superiorly in the tasks evaluated. The consistent reporting of p-values and confidence intervals ensures that the conclusions drawn are reliable and not due to random variation.",
  "evaluation/availability": "Not enough information is available."
}