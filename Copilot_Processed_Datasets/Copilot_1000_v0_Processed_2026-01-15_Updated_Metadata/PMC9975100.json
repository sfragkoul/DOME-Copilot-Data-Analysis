{
  "publication/title": "Predicting colorectal cancer microsatellite instability with a self-attention-enabled convolutional neural network.",
  "publication/authors": "Chang X, Wang J, Zhang G, Yang M, Xi Y, Xi C, Chen G, Nie X, Meng B, Quan X",
  "publication/journal": "Cell reports. Medicine",
  "publication/year": "2023",
  "publication/pmid": "36720223",
  "publication/pmcid": "PMC9975100",
  "publication/doi": "10.1016/j.xcrm.2022.100914",
  "publication/tags": "- Colorectal Cancer\n- Microsatellite Instability\n- Convolutional Neural Network\n- Self-Attention Mechanism\n- Machine Learning\n- Medical Imaging\n- Histopathology\n- Deep Learning\n- Cancer Diagnosis\n- Image Analysis",
  "dataset/provenance": "The dataset used in this study is derived from the TongShu Microsatellite Instability Colorectal Cancer (TSMCC) cohort. It consists of 2078 deidentified patients. The data includes Hematoxylin and Eosin (H&E) stained whole-slide images (WSIs) and C0 WSIs. The study protocol was approved by the Medical Ethics Committee of each participating institution, and no patient consent was required due to the retrospective nature of the study.\n\nThe dataset has been utilized in the current research to develop and validate the WiseMSI model, which is designed for microsatellite instability classification. The data has not been previously published or used by the community in the same context as this study. All data reported in this paper will be shared by the lead contact upon request, ensuring transparency and reproducibility. The original codes have been deposited on GitHub and are publicly available. Any additional information required to reanalyze the data reported in this paper is available from the lead contact upon request.",
  "dataset/splits": "The dataset was split into three parts: training, validation, and testing. For the training set, 286 whole slide images (WSIs) were used. The validation set consisted of 40 WSIs, and the testing set included 81 WSIs. These WSIs were tessellated into patches at 512x512 pixels for further processing. The distribution of data points in each split was designed to ensure a comprehensive evaluation of the models. The training set was the largest, followed by the testing set, and then the validation set. This split allowed for robust training, validation, and testing phases in the development of the models.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data reported in this paper is available upon request from the lead contact. The data includes whole slide images (WSIs) of hematoxylin and eosin (H&E) stained tissue sections from colorectal cancer patients. The dataset consists of 1579 WSIs from the TongShu Microsatellite Instability Colorectal Cancer (TSMCC) cohort, which were obtained from multiple medical centers across China. Additionally, 609 WSIs from The Cancer Genome Atlas (TCGA) were used as an external test set.\n\nThe WSIs were tessellated into non-overlapping square patches of 512-pixel edge length and saved at a resolution of 0.5 micrometers per pixel. These patches were manually annotated by experienced pathologists as tumor patches or normal patches. The data splits used for training, validation, and testing of the neural networks are described in the methods section.\n\nThe data is not publicly available in a forum, but it can be accessed by requesting it from the lead contact. The specific license under which the data is shared is not mentioned, but it is implied that the data can be used for research purposes upon request. The enforcement of data sharing is managed by the lead contact, who will fulfill any requests for resources and codes. The data is shared in accordance with the ethical guidelines and regulations of the participating institutions.",
  "optimization/algorithm": "The optimization algorithm employed in our study primarily involves the use of convolutional neural networks (CNNs) and self-attention mechanisms, which are well-established classes of machine-learning algorithms. Specifically, we utilized ResNet18 for tumor patch detection and ResNet50 for feature extraction, both of which are variants of residual networks known for their effectiveness in image recognition tasks. Additionally, we integrated a self-attention model to enhance the feature representation and classification performance.\n\nThe algorithms used are not new; they are widely recognized and have been extensively studied and applied in various domains, particularly in computer vision and medical imaging. The choice of these algorithms was driven by their proven efficacy in handling complex image data and their ability to generalize well across different datasets.\n\nThe decision to publish in a medical journal rather than a machine-learning journal was influenced by the primary focus of our research. Our work is centered on the application of these machine-learning techniques to solve a specific medical problem\u2014microsatellite instability (MSI) classification in colorectal cancer (CRC). The medical implications and clinical relevance of our findings are of paramount importance, making a medical journal the most appropriate venue for dissemination. This allows us to reach a audience of clinicians, researchers, and practitioners who can directly benefit from and apply our results in a clinical setting.",
  "optimization/meta": "The model described in this publication is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it is a hybrid approach that combines self-attention mechanisms with convolutional neural networks (CNNs) to predict microsatellite instability (MSI) in colorectal cancer (CRC) patients.\n\nThe model, named WiseMSI, is designed to process whole slide images (WSIs) and focuses on tumor regions detected by INSIGHT, a tool used for tumor purity prediction. The model's architecture includes a feature extraction step followed by an MSI classification step. The feature extraction is performed using a pre-trained model, such as ImageNet or Moco V2, and the classification is done using a self-attention-based CNN.\n\nThe training data used for WiseMSI is collected from various medical centers and open databases to ensure heterogeneity and sufficient disparities, reflecting diverse patient features. The model's performance is evaluated on multiple cohorts, including TCGA-COAD and TCGA-READ, demonstrating consistent results across different datasets.\n\nThe independence of the training data is ensured by using datasets from different medical centers and open databases, which helps to reflect heterogeneous patient features and reduces the risk of overfitting to a specific cohort. This approach enhances the model's generalizability and reliability in predicting MSI status across diverse patient populations.",
  "optimization/encoding": "In our study, whole slide images (WSIs) from the TSMCC Cohort were tessellated into smaller patches, which were then normalized. These patches were randomly assigned to training, validation, and test sets in a 7:1:2 ratio. The training and validation sets were fed into a ResNet18 tumor patch detector, which was trained for a maximum of 20 iterations with a learning rate of 2e-5. Binary cross-entropy loss was calculated after each iteration, and training was terminated when the loss did not decrease for 5 consecutive iterations, with a minimum of 10 training iterations.\n\nThe trained ResNet18 model, named INSIGHT, was used to classify patches as tumor or non-tumor using a SoftMax output layer. Patches with a probability greater than 0.5 were classified as tumor patches. These tumor patches were then processed by a pre-trained ResNet50, which encoded each patch into a 1024-dimensional feature vector. This resulted in a feature matrix of size N x 1024, where N is the number of tumor patches.\n\nThe feature matrix was then input into a Self-Attention model, which was trained on the training set and validated on the validation set. Standard cross-entropy loss was used, and training was stopped when the loss did not decrease for 40 consecutive iterations, with a minimum of 100 training iterations. The Self-Attention model used the Rectified Linear Unit (ReLU) as its activation function. The 1024-dimensional feature vector was reduced to a 512-dimensional vector through two linear transformations. The precision, recall, and area under the curve (AUC) of the Self-Attention model were evaluated using the test set.",
  "optimization/parameters": "In our study, the input parameters for the models varied depending on the specific architecture used. For the ResNet18 tumor patch detector, the model was trained with a learning rate of 2e-5 and a maximum of 20 iterations. Training was terminated when the binary cross-entropy loss did not decrease for 5 consecutive iterations, with a minimum of 10 training iterations required.\n\nFor the Self-Attention model, the feature vectors were initially 1024-dimensional, derived from the ResNet50 pre-trained on ImageNet. These vectors were then reduced to 512-dimensional through two linear transformations using the ReLU activation function. The training process for the Self-Attention model involved calculating the standard cross-entropy loss and updating the minimal loss after each iteration. Training was stopped when the cross-entropy loss did not decrease for 40 uninterrupted iterations, with at least 100 training iterations completed.\n\nThe selection of these parameters was based on empirical observations and standard practices in the field of deep learning. The learning rate and number of iterations were chosen to balance between model convergence and computational efficiency. The dimensionality reduction in the Self-Attention model was designed to optimize the model's performance while managing computational resources effectively.",
  "optimization/features": "In our study, each tissue patch was encoded using a 1024-dimensional feature vector. This resulted in a feature matrix with dimensions N x 1024, where N represents the number of tumor patches. Therefore, the number of features (f) used as input is 1024.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, we utilized a pre-trained ResNet50 model, which was initially trained on the ImageNet dataset, to extract relevant features from the tumor patches. This approach leverages the model's learned representations to focus on the most informative features for our specific task.\n\nThe feature extraction process was conducted using the training set, validation set, and test set. The patches from these sets were fed into the pre-trained ResNet50 model to obtain the 1024-dimensional feature vectors. This ensures that the feature extraction process is consistent across all datasets and that the model's learned representations are applied uniformly.",
  "optimization/fitting": "The fitting method employed in our study involved training a ResNet18 tumor patch detector with a maximum of 20 iterations and a learning rate of 2e-5. Binary cross-entropy loss was calculated after each iteration, and the model's weights were updated to minimize this loss. Training was terminated when the binary cross-entropy loss did not decrease for 5 consecutive iterations, provided that at least 10 training iterations had been completed. This approach helped to prevent overfitting by ensuring that the model did not continue to train indefinitely, which could lead to memorization of the training data rather than generalization to new data.\n\nTo further mitigate overfitting, we used a validation set to optimize the network architecture and preserve the model weights only when the minimal loss was achieved. This validation process ensured that the model's performance was evaluated on unseen data, helping to identify and prevent overfitting.\n\nThe number of parameters in our model was indeed larger than the number of training points, which is a common scenario in deep learning. However, the use of a validation set and early stopping based on the validation loss helped to rule out overfitting. Additionally, the model's performance was evaluated on a separate test set, which provided an unbiased estimate of the model's generalization capability.\n\nUnderfitting was ruled out by ensuring that the model had sufficient capacity to learn the underlying patterns in the data. The use of a deep neural network with multiple layers and a large number of parameters allowed the model to capture complex relationships in the data. Furthermore, the training process was continued until the validation loss stopped decreasing, which ensured that the model had enough iterations to learn from the training data.\n\nIn summary, our fitting method involved a combination of early stopping, validation set optimization, and a deep neural network architecture to balance the trade-off between overfitting and underfitting. This approach allowed us to develop a robust tumor patch detector model with good generalization capability.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our models. One of the primary methods used was early stopping during the training of both the ResNet18 tumor patch detector and the Self-Attention model. For the ResNet18 model, training was terminated when there was no further decrease in binary cross-entropy loss over 5 consecutive iterations, provided that at least 10 training iterations had been completed. This approach helped in stopping the training process before the model began to overfit to the training data.\n\nSimilarly, for the Self-Attention model, training was halted when there was no reduction in cross-entropy loss over 40 uninterrupted iterations, with a minimum of 100 training iterations. This ensured that the model generalized well to the validation set and did not memorize the training data.\n\nAdditionally, the use of a validation set to optimize the network architecture and preserve model weights when minimal loss was achieved further aided in preventing overfitting. The validation set provided an unbiased evaluation of the model's performance during training, allowing us to select the best-performing model based on its generalization ability.\n\nThe division of the dataset into training, validation, and test sets in a 7:1:2 ratio also contributed to the prevention of overfitting. This split ensured that the model was trained on a sufficiently large dataset while being evaluated on separate validation and test sets, which helped in assessing the model's performance on unseen data.\n\nFurthermore, the use of pre-trained models, such as ResNet50 pre-trained on ImageNet, provided a robust starting point for feature extraction. This transfer learning approach leveraged the knowledge gained from a large and diverse dataset, reducing the risk of overfitting to the specific task at hand.\n\nIn summary, early stopping, the use of a validation set, appropriate dataset splitting, and transfer learning were key techniques employed to prevent overfitting and enhance the generalization performance of our models.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are available. The original codes have been deposited on GitHub and are publicly available. This includes the configurations used for training the ResNet18 tumor patch detector and the Self-Attention model. The training process involved specific learning rates, loss functions, and termination criteria, all of which are detailed in the code.\n\nThe optimization parameters, such as the learning rate and the number of iterations, are explicitly mentioned in the training procedures. For instance, the ResNet18 tumor patch detector was trained with a learning rate of 2e-5 and a maximum of 20 iterations, with training termination criteria based on the binary cross-entropy loss. Similarly, the Self-Attention model was trained with a standard cross-entropy loss and specific iteration criteria.\n\nThe code and additional information required to reanalyze the data reported in this paper are available from the lead contact upon request. This ensures that all necessary details for reproducing the experiments and optimizing the models are accessible to the research community. The lead contact can be reached at quanxueping@tongshugene.com for any further information and requests for resources and codes.",
  "model/interpretability": "The WiseMSI model, while leveraging advanced machine learning techniques, incorporates elements that enhance its interpretability. Unlike traditional black-box models, WiseMSI provides insights into its decision-making process through attention score maps. These maps highlight regions within whole slide images (WSIs) that the model focuses on when making predictions. For instance, in the case of colorectal cancer (CRC) patients, the attention score maps visually indicate tumor regions with high and low attention scores, differentiating between microsatellite stable (MSS) and microsatellite instability-high (MSI-H) patients. This visual representation allows pathologists and researchers to understand which areas of the slide are most influential in the model's classification, thereby increasing transparency. Additionally, the model's workflow includes steps that can be traced and understood, such as tumor tile detection and feature extraction, which further contribute to its interpretability. This approach ensures that the model's predictions are not only accurate but also explainable, making it a valuable tool in clinical settings.",
  "model/output": "The model in question is designed for classification, specifically for the task of MSI (Microsatellite Instability) prediction. It categorizes samples into different classes based on their MSI status. The performance of the model is evaluated using metrics such as specificity, sensitivity, and the area under the curve (AUC), which are typical for classification tasks. The model's output provides probabilities or class labels indicating the presence or absence of MSI.\n\nSeveral models were compared, including EfficientNet, ViT, MIL, AttMIL, VarMIL, and WiseMSI. Each of these models was assessed for its ability to classify MSI status accurately. The WiseMSI model, in particular, was evaluated in two versions: Moco V2 and ImageNet. The performance statistics, including specificity, sensitivity, and AUC, were reported for each model, demonstrating their effectiveness in classifying MSI status.\n\nThe running time for each model was also compared, with WiseMSI and MIL models taking significantly longer than others like ViT and EfficientNet. This information is crucial for understanding the practical implications of using each model in a real-world setting.\n\nIn summary, the model's output is focused on classifying MSI status, with various models evaluated for their performance and efficiency. The results indicate that while some models offer high accuracy, they may come with longer processing times. This trade-off between accuracy and speed is an important consideration in the selection and implementation of these models.",
  "model/duration": "The execution time for the models varied significantly. The WiseMSI model, which includes steps for tumor tile detection, feature extraction, and MSI classification, had an average running time of approximately 807.8 seconds per whole slide image (WSI). This time is slightly shorter than that of MIL and AttMIL models when run on an Intel I9 10900K with a 3,090 GPU. It is important to note that the execution time for WiseMSI could be reduced to less than 10 minutes by utilizing a more powerful GPU server, making it acceptable for clinical applications.\n\nIn contrast, classical weakly supervised approaches, such as ViT and EfficientNet, required much less time to provide MSI predictions. Specifically, ViT took around 3.3 seconds per WSI, and EfficientNet took about 4.0 seconds. The shorter execution time for WiseMSI, despite having an extra step, is attributed to the smaller number of tumor tiles that are passed onto the feature extraction process. This efficiency is a result of the model's design, which focuses on processing only the relevant tumor regions, thereby optimizing the overall running time.",
  "model/availability": "The source code for the models and algorithms used in this study is publicly available. The original codes have been deposited on GitHub and can be accessed at https://github.com/woshihang01/WiseMSI. This repository contains the necessary code to replicate the analysis and models presented in the paper.\n\nIn addition to the source code, all data reported in the paper will be shared upon request. Any further information and requests for resources and codes should be directed to the lead contact. This ensures that researchers have access to both the data and the tools needed to reproduce the findings and build upon the work.\n\nThe software and algorithms utilized in this study include a variety of tools and frameworks, such as staintools, ImageNet, MoCo v2, ResNet, PyTorch, EfficientNet, MIL, AttMIL, ViT, and VarMIL. These tools are either publicly available or can be accessed through their respective repositories. For instance, staintools can be found at https://github.com/Peter554/StainTools, and PyTorch is available at https://pytorch.org/.\n\nThe study also makes use of commercial assays and deposited data from sources like the National Cancer Institute's TCGA portal, which can be accessed at https://portal.gdc.cancer.gov. These resources are critical for the reproducibility and validation of the models and methods described in the paper.\n\nFor those interested in running the algorithms, the source code and associated tools provide a comprehensive framework. The lead contact can provide additional support and information as needed. This approach ensures transparency and facilitates further research in the field.",
  "evaluation/method": "The evaluation of the method involved several rigorous steps to ensure its robustness and accuracy. For performance evaluation, ten-fold cross-validation was employed. This process involved randomly dividing the input whole slide images (WSIs) into 10 subsets. The training and testing process was repeated 10 times, each time using 8 different subsets for training and validation, and 2 different subsets for internal testing. This approach helped in assessing the model's performance across various data splits.\n\nAdditionally, the Spearman correlation coefficient was used to evaluate the performance, particularly in the context of tumor detection among patches from colorectal cancer (CRC) WSIs. Slides with a discrepancy of more than 20% in tumor purity estimations between the pathologist and the model were further inspected by an independent experienced pathologist. This step ensured that the model's predictions were reliable and accurate.\n\nThe 95% confidence interval (CI) for the mean specificity, sensitivity, and area under the curve (AUC) value were calculated to provide a statistical measure of the model's performance. Statistical analysis was conducted using IBM SPSS Statistics 22.\n\nFor survival analysis, disease-free survival (DFS) was calculated as the duration from the date of surgery to the date of recurrence, second cancer, or death from any cause, whichever occurred earlier. The data cutoff date was July 31, 2021. Kaplan-Meier survival analysis was used to determine differences in the survival distribution among different groups.",
  "evaluation/measure": "In our evaluation, we reported several key performance metrics to comprehensively assess the effectiveness of our models. Specifically, we focused on specificity, sensitivity, and the area under the receiver operating characteristic curve (AUC).\n\nSpecificity measures the ability of the model to correctly identify true negatives, which is crucial for ensuring that non-diseased cases are accurately classified. Sensitivity, on the other hand, evaluates the model's capability to correctly identify true positives, which is essential for detecting diseased cases. Both metrics are presented with their 95% confidence intervals (CI) to provide a range within which the true performance is likely to fall.\n\nThe AUC is a widely used metric that summarizes the overall performance of the model across all classification thresholds. It provides a single value that represents the model's ability to distinguish between positive and negative classes, with higher values indicating better performance.\n\nThese metrics are reported at the whole slide image (WSI) level, ensuring that the evaluation is relevant to real-world clinical applications. For most models, the performance is calculated based on 5-fold cross-validation, which helps to ensure that the results are robust and not dependent on a particular split of the data. For WiseMSI, we used 10-fold cross-validation to further validate its performance.\n\nThe set of metrics we reported is representative of the standards in the literature, providing a clear and comprehensive view of the models' performance. This allows for easy comparison with other studies and ensures that our evaluation is rigorous and reliable.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we conducted a thorough evaluation of WiseMSI against several established methods to benchmark its performance. We selected two classical weakly supervised methods, EfficientNet and ViT, which have shown top performance in previous benchmark analyses. Additionally, we included two multiple instance learning (MIL) methods, MIL and AttMIL, known for their strong performance in similar tasks. We also compared WiseMSI with the VarMIL model, which is part of the DeepSMILE approach. These comparisons were performed on the TSMCC cohort, using a 7:1:2 ratio for training, validation, and testing sets, respectively. The training-validation-testing process was repeated for five iterations for each method to ensure robustness.\n\nFurthermore, we benchmarked the Moco V2 feature extractor, which was self-trained using WSIs from the TCGA-COAD + READ cohorts and our in-house TSMCC data, against the ImageNet pre-trained feature extractor model. This comparison aimed to assess whether replacing the feature extraction model with the in-house trained Moco V2 model would significantly improve performance. The results showed that while the Moco V2 version of WiseMSI had a slightly lower AUC value of 0.937 compared to the ImageNet version's 0.954, the difference was not substantial.\n\nIn addition to these comparisons, we evaluated the running time of WiseMSI and other models using 1,579 WSIs for validation. WiseMSI demonstrated an average running time of 807.8 seconds per WSI, which is slightly shorter than MIL and AttMIL but longer than EfficientNet and ViT. However, using a more powerful GPU server could reduce the running time to less than 10 minutes, making it acceptable for clinical applications.\n\nOverall, these comparisons highlight that WiseMSI outperforms both classical weakly supervised and MIL approaches in terms of sensitivity and specificity, making it a robust tool for MSI classification.",
  "evaluation/confidence": "The performance metrics presented in this study include confidence intervals, specifically 95% confidence intervals (CI), for specificity, sensitivity, and the area under the curve (AUC). These intervals provide a range within which the true performance metrics are expected to lie, giving an indication of the reliability and precision of the estimates.\n\nStatistical significance is assessed through cross-validation techniques. For the models Ef\ufb01cientNet, ViT, MIL, and AttMIL, a 5-fold cross-validation was conducted. For WiseMSI, a 10-fold cross-validation was performed. This rigorous validation process helps ensure that the results are not due to random chance and that the models generalize well to unseen data.\n\nThe AUC values, which measure the overall ability of the models to discriminate between classes, show that WiseMSI outperforms the other models. The WiseMSI (ImageNet version) achieved an AUC of 0.954, which is higher than the AUCs of the other models, indicating superior performance. The confidence intervals for these AUC values do not overlap significantly with those of the other models, suggesting that the differences in performance are statistically significant.\n\nAdditionally, the sensitivity and specificity of WiseMSI are compared against other models. WiseMSI demonstrates higher sensitivity while maintaining high specificity, which is crucial for clinical applications where both false positives and false negatives need to be minimized.\n\nIn summary, the performance metrics are robust, with confidence intervals providing a measure of uncertainty. The use of cross-validation ensures that the results are statistically significant, supporting the claim that WiseMSI is superior to the other models and baselines evaluated in this study.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being available. However, all data reported in the paper can be shared upon request to the lead contact. This includes any additional information required to reanalyze the data reported in the paper. The original codes used for the evaluation have been deposited on GitHub and are publicly available. The specific repository can be found at https://github.com/woshihang01/WiseMSI. The license under which the codes are released is not specified, but they are open to the public for access."
}