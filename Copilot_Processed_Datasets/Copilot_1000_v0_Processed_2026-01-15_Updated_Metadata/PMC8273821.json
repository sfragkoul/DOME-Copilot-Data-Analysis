{
  "publication/title": "Application of deep learning to predict advanced neoplasia using big clinical data in colorectal cancer screening of asymptomatic adults.",
  "publication/authors": "Yang HJ, Cho CW, Jang J, Kim SS, Ahn KS, Park SK, Park DI",
  "publication/journal": "The Korean journal of internal medicine",
  "publication/year": "2021",
  "publication/pmid": "33092313",
  "publication/pmcid": "PMC8273821",
  "publication/doi": "10.3904/kjim.2020.020",
  "publication/tags": "- Machine Learning\n- Deep Neural Networks\n- Logistic Regression\n- Support Vector Machines\n- Random Forests\n- XGBoost\n- Area Under the Curve\n- Receiver Operating Characteristic\n- Advanced Neoplasia\n- Predictive Modeling",
  "dataset/provenance": "The dataset used in this study was sourced from a screening of 121,794 individuals. After applying exclusion criteria, 70,336 individuals were included in the analysis. These individuals were divided into a development group consisting of 56,269 individuals and a validation group of 14,067 individuals.\n\nThe demographic characteristics of the study population revealed a mean age of 41.6 years with a standard deviation of 8.3 years. The majority of the participants, 69.4%, were male. A condition referred to as ACRN was detected in 1.4% of the participants. Among those aged 50 years or older, which constituted 15.1% of the total population, 3.9% had ACRN.\n\nThe dataset was carefully curated to ensure that there were no significant differences between the demographics and clinical characteristics of the development and validation groups. While there were statistically significant differences in serum glucose levels and high-sensitivity C-reactive protein (hsCRP) levels due to the large sample size, these differences were deemed to have little clinical significance. Specifically, the mean serum glucose levels were 93.5 mg/dL in the development group and 93.9 mg/dL in the validation group, with median hsCRP levels of 0.1 mg/L in both groups.\n\nThe dataset has not been used in previous papers or by the community.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "The dataset used in this study consisted of individuals who underwent screening colonoscopy at Kangbuk Samsung Hospital from 2003 to 2012. Initially, there were 121,794 potential participants. Various exclusions were applied, including those with previous colorectal exams, history of colorectal cancer or other malignancies, inflammatory bowel disease, colorectal surgery, incomplete colonoscopy, and missing clinical data. After these exclusions, 70,336 participants were included in the study.\n\nThe dataset was split into two main groups: the development group and the validation group. The development group consisted of 56,269 participants, while the validation group had 14,067 participants. This split ensured that the training and test sets were independent, allowing for an unbiased evaluation of the models' performance.\n\nTo enforce the independence of the training and test sets, a clear separation was maintained between the development and validation groups. The development group was used to train and tune the models, while the validation group was used to evaluate the models' performance. This approach helped to prevent data leakage and ensured that the models were tested on unseen data.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets in the context of colorectal cancer screening. The dataset includes a comprehensive set of clinical and laboratory variables, which were carefully selected and standardized for feature scaling. This ensures that the models developed are robust and generalizable to other similar datasets. The use of a large and well-defined dataset enhances the reliability and validity of the study's findings.",
  "dataset/availability": "The data used in this study is not publicly available. The dataset was developed specifically for the research and was not released in a public forum. The data splits used for training, validation, and testing were carefully designed to ensure robust model evaluation but were not made available externally. This decision was made to maintain the integrity of the dataset and to prevent potential biases that could arise from public access. The dataset was used under strict confidentiality agreements, and access was limited to the research team to ensure compliance with ethical guidelines and data protection regulations.",
  "optimization/algorithm": "The optimization algorithm employed in our study is Adam, a widely-used class of machine-learning algorithms known for its efficiency in training deep learning models. Adam, which stands for Adaptive Moment Estimation, is not a new algorithm. It was introduced by Diederik P. Kingma and Jimmy Ba in 2014 and has since become a staple in the field of deep learning due to its robustness and adaptability.\n\nThe reason Adam was not published in a machine-learning journal in the context of this study is straightforward: it is an established algorithm that has already been extensively documented and validated in the literature. Our focus was on applying this well-known optimization technique to the specific problem of colorectal cancer screening using deep learning models. The innovation lies in the application and the comparative analysis with other conventional machine learning methods, rather than in the optimization algorithm itself.\n\nAdam was chosen for its ability to combine the advantages of two other extensions of stochastic gradient descent. Specifically, it computes adaptive learning rates for each parameter, which helps in achieving faster convergence and better performance. The parameters used in our implementation were a learning rate of 0.001, \u03b21 of 0.9, and \u03b22 of 0.999. These settings were selected based on empirical evidence and common practices in the field to ensure optimal performance of the deep neural network models.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The deep neural network (DNN) model was developed independently using a set of clinical and laboratory variables. The study compared the performance of the DNN model with conventional machine learning methods such as support vector machine (SVM), random forest (RF), and extreme gradient boosting (XGBoost). However, these comparisons were made to evaluate the DNN's performance relative to other methods, not as part of a meta-predictor framework.\n\nThe training data for the DNN model was derived from a large cohort of participants who underwent screening colonoscopy. The data was split into development and validation groups to ensure independent training and testing. This approach helps to validate the model's performance and generalizability. The DNN model was trained using specific hyperparameters, including the Adam optimization algorithm with a learning rate of 0.001, \u03b21 of 0.9, and \u03b22 of 0.999. Various configurations of hidden layers and nodes were tested to optimize the model's performance. The area under the receiver operating characteristic curve (AUC) was used as a primary metric to evaluate the model's predictive accuracy.",
  "optimization/encoding": "For the machine-learning algorithms employed in our study, data encoding and preprocessing were crucial steps to ensure optimal model performance. Initially, we standardized all continuous variables to facilitate feature scaling. This process involved transforming the data to have a mean of zero and a standard deviation of one, which is essential for algorithms that are sensitive to the scale of input features.\n\nIn our logistic regression (LR) models, we manually selected nine key variables from a pool of 26 clinical and laboratory variables. These variables included age, sex, smoking status, family history of colorectal cancer (CRC), body mass index (BMI), serum levels of fasting glucose, low-density lipoprotein-cholesterol (LDL-C), and carcinoembryonic antigen (CEA). For ad-hoc analyses, we also fitted an LR model that included all 26 variables to explore the impact of a larger set of covariates.\n\nFor conventional machine learning methods, such as support vector machine (SVM), random forest (RF), and extreme gradient boosting (XGBoost), we used the same 26 variables without additional encoding, as these algorithms can handle both continuous and categorical data natively.\n\nIn the development of deep neural networks (DNNs), we utilized a feedforward neural network structure. For the initial DNN model, we used the same nine variables as in the LR model to assess whether deep learning could outperform logistic regression with the same information. The main DNN model, however, was developed using all 26 variables to determine if deep learning could overcome the limitations of LR when dealing with a large number of covariates. The continuous variables were standardized for feature scaling, ensuring that the DNN could effectively learn from the data.\n\nHyperparameter tuning for the DNNs was conducted using 5-fold cross-validation. This process helped in selecting the optimal architecture and parameters for the models. The DNN with nine variables was configured with two hidden layers, each containing 26 nodes. The DNN with 26 variables had two hidden layers with 10 nodes each. The Adam optimization algorithm was employed with a learning rate of 0.001, \u03b21 of 0.9, and \u03b22 of 0.999 to enhance the training efficiency and convergence of the models.",
  "optimization/parameters": "In our study, we developed deep neural network (DNN) models using two different sets of input parameters. The first model utilized nine clinical and laboratory variables, while the second model incorporated a more extensive set of 26 variables. The selection of these parameters was driven by a combination of manual selection and conventional logistic regression processes. For the model with nine variables, a manual selection approach was employed to identify the most relevant features. In contrast, the model with 26 variables included a broader range of clinical and laboratory data, aiming to capture a more comprehensive picture of the participants' health status. This approach allowed us to compare the performance of models with different levels of input complexity, providing insights into the trade-offs between model simplicity and predictive accuracy. The optimization process involved using the Adam algorithm with specific learning rates and beta parameters to fine-tune the models and achieve optimal performance.",
  "optimization/features": "In the optimization process of our deep learning model, we utilized a total of 26 clinical and laboratory variables as input features. These features were selected based on their relevance to the prediction of advanced colorectal neoplasia (ACRN).\n\nInitially, a logistic regression (LR) model was fitted using a subset of nine manually selected variables from the 26 available features. This subset included age, sex, smoking status, family history of colorectal cancer (CRC), body mass index (BMI), serum levels of fasting glucose, low-density lipoprotein-cholesterol (LDL-C), and carcinoembryonic antigen (CEA). This manual selection was performed to compare the performance of the LR model with that of the deep neural network (DNN) using the same information.\n\nFor the main DNN model, all 26 variables were used as input nodes. This approach aimed to determine whether deep learning could overcome the limitations of the LR model, particularly when dealing with a large number of covariates. The use of all 26 variables allowed the DNN to potentially capture more complex relationships within the data.\n\nFeature selection was performed manually for the initial LR model, ensuring that the selected features were based on domain knowledge and their known relevance to CRC screening. This selection process was conducted using the training set only, adhering to best practices in machine learning to prevent data leakage and maintain the integrity of the validation process. The DNN models were then developed and optimized using these selected features, as well as all 26 features, to evaluate their predictive performance comprehensively.",
  "optimization/fitting": "In our study, we developed deep neural network (DNN) models using a feedforward neural network structure. The models were designed with varying numbers of hidden layers and nodes to explore different architectures. For the DNN with nine variables, we initially set the model to have two hidden layers, each with 26 nodes. For the DNN with 26 variables, the model was configured with two hidden layers, each containing 10 nodes. This configuration was determined through hyperparameter tuning using 5-fold cross-validation, ensuring that the models were optimized for performance.\n\nTo address the potential issue of overfitting, given the number of parameters in our models, we employed several strategies. Firstly, we used a 5-fold cross-validation approach, which helps in assessing the model's performance on different subsets of the data and reduces the risk of overfitting to a specific training set. Secondly, we standardized all continuous variables for feature scaling, which is crucial for the stable and efficient training of neural networks. Additionally, we utilized the Adam optimization algorithm with a learning rate of 0.001, \u03b21 of 0.9, and \u03b22 of 0.999. This adaptive learning rate method helps in converging the model more effectively and can mitigate overfitting by adjusting the learning rate during training.\n\nTo rule out underfitting, we ensured that our models had sufficient complexity by experimenting with different architectures and hyperparameters. The use of multiple hidden layers and nodes allowed the models to capture complex patterns in the data. Furthermore, the cross-validation process helped in identifying models that were too simple and thus underfitting the data. By comparing the performance across different folds, we could select models that generalized well to unseen data, indicating that they were neither overfitting nor underfitting.\n\nIn summary, our approach to fitting the DNN models involved careful consideration of model complexity, regularization techniques, and thorough validation to ensure that the models neither overfit nor underfit the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was cross-validation. Specifically, we conducted 5-fold cross-validation to tune the hyperparameters of our deep neural network models. This process involved dividing the dataset into five subsets, training the model on four of these subsets, and validating it on the remaining subset. This procedure was repeated five times, with each subset serving as the validation set once. By doing so, we were able to obtain a more reliable estimate of the model's performance and reduce the risk of overfitting.\n\nAdditionally, we standardized all continuous variables for feature scaling. This technique helps in ensuring that the model does not become biased towards features with larger scales, thereby improving the overall generalization of the model.\n\nFurthermore, we utilized the Adam optimization algorithm with specific parameters, including a learning rate of 0.001, \u03b21 of 0.9, and \u03b22 of 0.999. The Adam optimizer is known for its adaptive learning rate properties, which help in converging the model more efficiently and reducing the likelihood of overfitting.\n\nThe deep neural networks were designed with a varying number of hidden layers and nodes, and we experimented with different architectures to find the optimal configuration. For instance, the main deep neural network model with 26 variables was set to have two hidden layers with 10 nodes each. This architecture was chosen based on the results of the cross-validation process, which indicated that it provided a good balance between model complexity and performance.\n\nIn summary, our approach to preventing overfitting included the use of cross-validation, feature scaling, and an appropriate optimization algorithm. These techniques collectively contributed to the development of robust and generalizable models for predicting advanced colorectal neoplasms.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules for the deep neural network models discussed in our publication are thoroughly detailed. Specifically, we have provided a comprehensive comparison of model performances with varying hyper-parameters, including the number of hidden layers and nodes at each layer. These details are presented in a supplementary table, which includes the area under the receiver operating characteristic curve (AUC) for different cross-validation sets.\n\nThe models utilized the sigmoid function, Xavier initializer, and Adam optimizer, with each model trained for 1,000 epochs on the same dataset. This consistency ensures reproducibility and allows other researchers to replicate our findings.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly provided in the publication. However, the detailed descriptions of the configurations and optimization processes should enable researchers to implement similar models. For those interested in accessing the specific datasets or code, additional information can be obtained by contacting the corresponding author.\n\nThe publication adheres to standard academic practices, ensuring that the methods and results are transparent and reproducible. While the exact model files may not be directly downloadable, the provided information is sufficient for researchers to build and optimize their own models based on the parameters and schedules outlined.",
  "model/interpretability": "The deep learning model used in our study is primarily a black-box model, meaning that it is challenging to interpret how it makes predictions. This lack of transparency is a significant issue in clinical settings, where understanding the reasoning behind a model's predictions is crucial for trust and acceptance.\n\nTo address this, we conducted a reverse-engineering process. We compared subjects with advanced colorectal neoplasia (ACRN) detected by both the deep neural network (DNN) and logistic regression (LR) models, those detected only by the LR model, and those detected only by the DNN model. This comparison was based on clinical characteristics such as age, sex, body mass index (BMI), triglycerides, white blood cell count, and ferritin.\n\nThe results showed substantial differences among the three groups, suggesting that the DNN model may reduce the impact of conventional risk factors, particularly sex and BMI. This implies that the DNN model might be capturing more nuanced patterns in the data that are not immediately apparent through traditional risk factors.\n\nHowever, despite these efforts, the exact mechanisms by which the DNN model arrives at its predictions remain largely opaque. This black-box nature is a limitation that needs to be considered when applying such models in clinical practice. Further research is needed to develop more interpretable models or to enhance the interpretability of existing models to ensure they can be effectively integrated into clinical decision-making processes.",
  "model/output": "The model discussed in this publication is a classification model. It is designed to predict the presence of advanced colorectal neoplasia (ACRN) in colorectal cancer screening. The model's performance is evaluated using metrics such as the area under the receiver operating characteristic curve (AUC), sensitivity, and specificity. These metrics are typical for classification tasks, where the goal is to distinguish between two classes: individuals with ACRN and those without.\n\nThe model's output is a probability score indicating the likelihood of an individual having ACRN. This probability can be thresholded to make a binary classification decision. The model's performance is compared with other conventional machine learning methods and logistic regression models, further emphasizing its role as a classification model.\n\nThe deep neural network (DNN) model demonstrated high sensitivity and specificity, making it effective for identifying individuals at high risk of ACRN. The model's ability to reduce the impact of conventional risk factors, such as sex and body mass index (BMI), suggests that it can provide more accurate predictions by considering a broader range of variables.\n\nThe model's performance was evaluated using cross-validation, and the results showed that the DNN model outperformed logistic regression models with both nine and 26 variables. Additionally, the DNN model exhibited a significantly better prediction performance than the logistic regression model in the validation group.\n\nThe model's output is crucial for clinical interpretations, as it helps specify why a particular individual was categorized as having a high risk of ACRN. However, the black-box nature of the DNN model poses a challenge in understanding its internal workings. To address this, the model was reverse-engineered by comparing subjects with ACRN detected by both the DNN and logistic regression models, as well as those detected by only one of the models. This analysis revealed substantial differences in clinical characteristics among the groups, providing insights into the model's decision-making process.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the deep learning models involved a comprehensive approach to ensure robustness and generalizability. A 5-fold cross-validation was conducted to tune the hyperparameters of the deep neural networks (DNNs). This process involved dividing the dataset into five subsets, where the model was trained on four subsets and validated on the remaining one, repeating this process five times with different subsets as the validation set. This method helps in assessing the model's performance and reducing overfitting.\n\nIn addition to cross-validation, the study population was split into a development group and a validation group. The development group consisted of 56,269 participants, while the validation group included 14,067 participants. This split allowed for the evaluation of the models on an independent dataset, providing a more reliable assessment of their performance.\n\nThe performance of the models was primarily evaluated using the area under the receiver operating characteristic curve (AUC). This metric provides a single scalar value that represents the ability of the model to distinguish between classes. The AUC was calculated for each fold in the cross-validation process and averaged to provide a comprehensive measure of the model's performance.\n\nThe deep learning models were developed using a feedforward neural network structure. The main DNN model was developed using all 26 variables in the dataset as input nodes. For hyperparameter tuning, various configurations of hidden layers and nodes were tested. The final configuration for the DNN with nine variables included two hidden layers with 26 nodes each, while the DNN with 26 variables had two hidden layers with 10 nodes each.\n\nThe optimization algorithm used for training the DNNs was Adam, with a learning rate of 0.001, \u03b21 of 0.9, and \u03b22 of 0.999. This algorithm was chosen for its efficiency and effectiveness in training deep learning models.\n\nIn summary, the evaluation method involved a rigorous process of cross-validation, independent dataset validation, and performance assessment using the AUC metric. This approach ensured that the models were thoroughly evaluated and validated for their predictive performance.",
  "evaluation/measure": "In the evaluation of our models, we primarily focused on the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve as our key performance metric. This metric was chosen due to its robustness in evaluating the performance of classification models, especially when dealing with imbalanced datasets. The AUC provides a single scalar value that represents the ability of the model to distinguish between classes, with higher values indicating better performance.\n\nWe reported the AUC for multiple cross-validation sets to ensure the stability and generalizability of our results. Specifically, we provided AUC values for five different cross-validation sets, allowing for a comprehensive assessment of model performance across various data splits. Additionally, we calculated the average AUC across these sets to provide a summary measure of model performance.\n\nOur choice of metrics is aligned with common practices in the literature, where AUC is widely used for evaluating classification models. This metric is particularly useful in medical and diagnostic contexts, where the trade-off between sensitivity and specificity is crucial. By focusing on AUC, we aimed to provide a clear and interpretable measure of our models' discriminative power.\n\nIn addition to AUC, we also compared the performance of different machine learning methods, including conventional techniques and deep neural networks (DNNs). This comparison was facilitated by reporting the AUC along with confidence intervals and p-values, which helped in assessing the statistical significance of the differences observed between models. This approach ensured that our evaluation was rigorous and that our conclusions were supported by robust statistical evidence.\n\nOverall, the set of metrics reported in our study is representative of standard practices in the field and provides a comprehensive evaluation of model performance. The use of AUC, along with cross-validation and statistical comparisons, ensures that our results are reliable and comparable to those reported in the literature.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the performance of our deep neural network (DNN) model against various conventional machine learning methods and simpler baselines to ensure a comprehensive assessment.\n\nWe compared our DNN model with logistic regression (LR), support vector machine (SVM), random forest (RF), and extreme gradient boosting (XGBoost). The logistic regression model was evaluated with both nine and twenty-six variables. The DNN model demonstrated superior performance compared to the logistic regression model with nine variables, achieving a higher area under the receiver operating characteristic curve (AUC). When compared to the logistic regression model with twenty-six variables, the DNN model showed significantly better prediction performance in the validation group.\n\nThe SVM and RF models exhibited lower prediction performance, with AUC values significantly below that of the DNN model. The XGBoost model, however, showed a prediction performance comparable to the DNN model, although it was not significantly better than the logistic regression model.\n\nAdditionally, we compared the DNN model's performance with that of the fecal immunochemical test (FIT) and the combined FIT and clinical score. The DNN model showed lower specificity at the same sensitivity levels, indicating that while it is effective, there are areas for potential improvement in specificity.\n\nOverall, the comparison with these publicly available methods and simpler baselines provided a robust evaluation of our DNN model's effectiveness in predicting advanced neoplasia.",
  "evaluation/confidence": "The evaluation of our models includes performance metrics with confidence intervals, providing a range within which the true performance is likely to fall. This is crucial for understanding the reliability of our results. For instance, the area under the receiver operating characteristic curve (AUC) for our logistic regression model with 26 variables is reported with a 95% confidence interval, indicating the precision of our estimate.\n\nStatistical significance is also considered to determine if our methods are superior to others and baselines. P-values are provided for each model, allowing us to assess whether the observed differences in performance are likely due to chance. A p-value less than 0.05 is typically considered statistically significant. For example, the support vector machine (SVM) and random forest (RF) models show p-values less than 0.001, indicating strong evidence that their performance differs from the reference model. In contrast, the extreme gradient boosting (XGBoost) and deep neural network (DNN) models have p-values of 0.064 and 0.036, respectively, suggesting that the DNN model's performance is statistically significant, while the XGBoost model's performance is borderline significant.\n\nAdditionally, the comparison of different deep neural network architectures with varying hyperparameters includes average AUC values across multiple cross-validation sets. This approach helps to ensure that the reported performance is robust and not dependent on a specific random split of the data. The use of cross-validation further enhances the confidence in our results by providing a more comprehensive evaluation of model performance.",
  "evaluation/availability": "The raw evaluation files for the deep neural network models discussed in our publication are not publicly available. The evaluation process involved cross-validation on specific datasets, and the results are presented in the supplementary tables within the paper. These tables provide detailed performance metrics, such as the area under the receiver operating characteristic curve (AUC), for various model configurations. The datasets used for development and evaluation were proprietary and specific to the study, and thus, they have not been released publicly. Access to the raw evaluation files would require direct communication with the authors or relevant institutions, subject to any applicable data sharing agreements and ethical considerations."
}