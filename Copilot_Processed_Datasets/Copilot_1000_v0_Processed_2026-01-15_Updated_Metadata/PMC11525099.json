{
  "publication/title": "Role of Artificial intelligence model in prediction of low back pain using T2 weighted MRI of Lumbar spine.",
  "publication/authors": "Muhaimil A, Pendem S, Sampathilla N, P S P, Nayak K, Chadaga K, Goswami A, M OC, Shirlal A",
  "publication/journal": "F1000Research",
  "publication/year": "2024",
  "publication/pmid": "39483709",
  "publication/pmcid": "PMC11525099",
  "publication/doi": "10.12688/f1000research.154680.2",
  "publication/tags": "- Machine Learning\n- Deep Learning\n- Medical Imaging\n- Lower Back Pain\n- Radiomics\n- Classification Algorithms\n- AI in Healthcare\n- MRI Analysis\n- Predictive Modeling\n- Diagnostic Workflow",
  "dataset/provenance": "The dataset utilized in this study was sourced from patients referred for MRI of the lumbar spine and whole-spine screening. These patients were screened using the \"Delphi definitions of low back pain prevalence (DOLBaPP)\" questionnaire to assess the prevalence of lower back pain (LBP). Patients were considered symptomatic if they had experienced LBP for at least 12 months.\n\nThe study was conducted as a prospective, case-control study, with ethical approval obtained from the institutional ethical committee at Kasturba Medical College and Hospital, Manipal, India. Additionally, the study was registered with the Clinical Trial Registry of India (CTRI).\n\nThe dataset was divided into a 90:10 training and test split ratio. The training set was further divided into training and validation subsets. The validation process occurred simultaneously during the training phase. Deep learning models were implemented using MATLAB 2023b, chosen for its superior visualization capabilities and ease of use.\n\nThe dataset consisted of MRI images, and the analysis performed was a classification task rather than a segmentation task. The specific number of data points or samples for each class has not been explicitly stated, but the study involved a binary classification problem aimed at predicting lower back pain.\n\nThe dataset has not been previously used in other published papers or by the community, as this is an original study focusing on the application of machine learning and deep learning techniques to predict lower back pain from MRI images. The study highlights the use of a wide range of machine learning algorithms, including clustering algorithms like KNN, and deep learning algorithms such as ResNet and GoogleNet. The choice of these algorithms was driven by their proven effectiveness in similar medical imaging tasks and their ability to handle the complexity of MRI data.",
  "dataset/splits": "The dataset was divided into three main splits: training, validation, and testing. Initially, the dataset was split into a 90:10 ratio for training and testing. The training set was further divided into training and validation subsets, with validation occurring simultaneously during the training process. This approach ensures that the model's performance is continuously assessed and optimized.\n\nFor the machine learning models, the data was split into an 80:20 ratio for training and testing. Additionally, five-fold cross-validation was employed, where the data was divided into five equal parts. In each fold, four parts were used for training, and one part was used for testing. This process was repeated five times with different permutations and combinations to ensure robust model evaluation.\n\nFor the deep learning models, the dataset was trained for different numbers of epochs: 30, 50, and 100. This variation helps in determining the optimal number of epochs for achieving the best model performance. The initial learning rate was set to 0.001, and the optimizer used was Stochastic Gradient Descent with momentum (sgdm). The performance of the deep learning models was assessed using metrics such as specificity, sensitivity, precision, negative predictive value (NPV), recall, and F1 score.",
  "dataset/redundancy": "The dataset was divided into a 90:10 training and test split ratio. The training set was further divided into training and validation subsets. This division ensures that the training and test sets are independent, with the validation set used to tune hyperparameters and prevent overfitting during the training process.\n\nThe dataset was trained for varying numbers of epochs\u201430, 50, and 100\u2014to obtain optimum results. The initial learning rate was set to 0.001, and the optimizer used was Stochastic Gradient Descent with momentum (sgdm). This approach helps in adjusting the hyperparameters effectively to improve model performance.\n\nThe performance of the deep learning models was assessed using metrics such as specificity, sensitivity, precision, negative predictive value (NPV), recall, and F1 score. These metrics provide a comprehensive evaluation of the model's effectiveness in predicting the target variable.\n\nThe study addresses a binary classification problem aimed at predicting lower-back pain (LBP) using machine learning and deep learning methods. The architectural configuration of the ResNet18 model, as depicted in Figure 3, delineates the structure used for this classification task.\n\nRegarding the distribution of the dataset, it is not explicitly compared to previously published machine learning datasets in the provided information. However, the split ratio and the use of validation sets are standard practices in machine learning to ensure robust model training and evaluation.",
  "dataset/availability": "The dataset used in this study is not publicly available. The data was collected as part of a prospective, case-control study, with ethical approval obtained from Kasturba Medical College and Hospital, Manipal, India. The study was also registered with the Clinical Trial Registry of India (CTRI/2023/08/056954). Written informed consent was obtained from all participants.\n\nThe dataset was divided into a 90:10 training and test split ratio. The training set was further divided into training and validation sets, with validation occurring simultaneously during training. The specific details of the data splits are not publicly disclosed to maintain participant privacy and comply with ethical guidelines.\n\nThe source data underlying the results is available to ensure full reproducibility, but access is restricted to protect participant confidentiality. Researchers interested in accessing the data for replication or further study should contact the corresponding author to discuss potential data-sharing agreements. This approach ensures that the data is used responsibly and ethically, in line with the study's approvals and registrations.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages deep learning models, specifically ResNet and GoogleNet, which are well-established architectures in the field of machine learning and computer vision. These models are not new but have been chosen for their proven capabilities in handling complex image analysis tasks.\n\nResNet, or Residual Network, is renowned for its ability to solve the vanishing gradient problem, allowing for the training of very deep networks efficiently. This is particularly useful for tasks that require learning intricate representations, such as image classification and object recognition. The architecture includes residual blocks that enable the network to learn identity functions, making it easier to train deeper networks.\n\nGoogleNet, also known as Inception, utilizes inception modules that process input through parallel convolution layers with varying kernel sizes. This design enhances efficiency by capturing features at different scales with fewer parameters, making it highly effective for image analysis and classification problems.\n\nThe choice of these models was driven by their robustness and effectiveness in medical diagnostics, where they have shown exceptional performance in automatically learning deep features from images. This capability is crucial for our study, which focuses on predicting low back pain (LBP) using T2-weighted MRI images of the lumbar spine.\n\nThe decision to use these established models rather than publishing a new machine-learning algorithm in a specialized journal was based on the need for reliability and proven performance. These models have been extensively validated in various applications, ensuring that our results are robust and reproducible. Additionally, the focus of our research is on the clinical application and significance of these models in medical diagnostics, rather than the development of new algorithms.",
  "optimization/meta": "The study does not explicitly mention the use of a meta-predictor. However, it does employ multiple machine learning classifiers, including Random Forest, Decision Tree, Logistic Regression, K-Nearest Neighbors (KNN), and AdaBoost. These classifiers were utilized to benchmark and select the optimal model for predicting lower back pain from MRI images.\n\nThe performance of these classifiers was evaluated using various metrics such as accuracy, precision, F1 score, area under the curve (AUC), and others. The data was split into training and testing sets with an 80:20 ratio, and five-fold cross-validation was used to assess model efficiency. This process involved splitting the input data into five equal parts, using four groups for training and one for testing in various permutations and combinations.\n\nWhile the study does not explicitly combine the outputs of these classifiers into a meta-predictor, the use of multiple classifiers and the evaluation of their performance suggests a comprehensive approach to model selection. The training data for each classifier was independent, as the data was split into different subsets for training and testing purposes. This ensures that the models were evaluated on unseen data, providing a robust assessment of their predictive performance.\n\nNot applicable",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for ensuring the effectiveness of the machine-learning algorithms. Initially, data normalization was performed using the min-max normalization technique. This step was essential to assign equal weight to each variable, preventing any single variable from disproportionately influencing the model's performance due to its large numerical values.\n\nFor feature selection, the mutual information method was employed to identify the top 20 radiomic features at each lumbar vertebra and intervertebral disc (IVD) for both cases and controls. This method helped in selecting the most relevant features, which improved the model's performance and efficiency.\n\nThe dataset was split into training and testing sets with an 80:20 ratio. Additionally, five-fold cross-validation was used to assess model efficiency. This involved splitting the input data into five equal parts, where four groups were used for training and one for testing, with various permutations and combinations in the cross-validation process.\n\nHyperparameter tuning was conducted using a grid search technique, which automated the process of determining the best parameter values. This ensured that the models were optimized for the specific problem at hand.\n\nFor the deep learning models, MRI images of the lumbar spine were collected in JPEG format. These images were cropped and resized to 184x282 pixels to focus mainly on the lumbar vertebrae and disc space. Intensity normalization was performed on all images to ensure that pixel values across multiple images were normalized to the same statistical distribution, facilitating improved analysis of MRI images.\n\nThe training of the models utilized 8 GB of RAM and an Intel\u00ae Core\u2122 i5 Central Processing Unit. The study was conducted on a 64-bit Windows operating system, and several libraries such as NumPy, scikit-learn, pandas, seaborn, and matplotlib were installed to support the analysis. The models were run in a Conda virtual environment integrated with Python (version 3.9.7).",
  "optimization/parameters": "The optimization process for our models involved careful selection and tuning of input parameters to ensure robust performance. For the deep learning models, we utilized pre-trained networks such as GoogleNet and ResNet (18 and 50), which are convolutional neural networks known for their ability to learn complex patterns in image data. These models have a large number of parameters, typically in the range of millions, which allows them to capture intricate details in the MRI images of the lumbar spine.\n\nThe selection of these models was driven by their proven effectiveness in image analysis and medical diagnostics. ResNet, in particular, addresses the vanishing gradient problem, enabling the training of very deep networks efficiently. This is crucial for tasks like image classification and object recognition, where capturing fine details is essential.\n\nFor the machine learning models, we employed a variety of classifiers, including Random Forest, Decision Tree, Logistic Regression, k-Nearest Neighbors (KNN), and AdaBoost. Each of these classifiers has its own set of hyperparameters that were tuned using techniques such as grid search. The number of parameters (p) in these models varies significantly. For instance, Random Forest has parameters like the number of trees and maximum depth, while Logistic Regression has parameters like the regularization strength.\n\nThe hyperparameters were adjusted to obtain optimum results. For the deep learning models, we trained the dataset for 30, 50, and 100 epochs, respectively. The initial learning rate was set to 0.001, and we used Stochastic Gradient Descent with momentum (sgdm) as the optimizer. These settings were chosen to balance between computational efficiency and model performance.\n\nIn summary, the selection of parameters was guided by the need to capture complex patterns in the data while ensuring computational feasibility. The use of pre-trained deep learning models and thorough hyperparameter tuning for machine learning models ensured that our models were well-equipped to handle the binary classification problem of predicting low back pain from MRI images.",
  "optimization/features": "In our study, we utilized radiomic features extracted from MRI images of the lumbar spine to predict lower-back pain. For the machine learning models, feature selection was performed to identify the top 20 radiomic features at each lumbar vertebra and intervertebral disc (IVD) for both cases and controls. This selection process was conducted using the mutual information method, ensuring that the most relevant features were chosen.\n\nThe feature selection was performed using the training set only, adhering to best practices to prevent data leakage and maintain the integrity of the validation and testing processes. This approach helps in ensuring that the selected features are generalizable and not overfitted to the specific characteristics of the training data.\n\nThe deep learning models, on the other hand, utilized pre-processed MRI images that were cropped and resized to focus on the lumbar vertebrae and disc space. Intensity normalization was applied to these images to standardize pixel values across multiple images, facilitating improved analysis.\n\nIn summary, the machine learning models used a selected subset of 20 radiomic features per vertebra and IVD, while the deep learning models worked directly with the pre-processed MRI images. The feature selection for the machine learning models was conducted using the training set only, ensuring robust and generalizable model performance.",
  "optimization/fitting": "The fitting method employed in this study involved both machine learning and deep learning models, each with its own set of considerations regarding overfitting and underfitting.\n\nFor the machine learning models, several classifiers were utilized, including Random Forest, Decision Tree, Logistic Regression, K-Nearest Neighbors (KNN), and AdaBoost. These models were trained using a dataset split into an 80:20 training and testing ratio, with five-fold cross-validation to ensure robustness. Cross-validation helps in assessing model performance by training on different subsets of the data, which mitigates the risk of overfitting. Additionally, hyperparameter tuning was performed using a grid search technique to find the optimal parameters, further reducing the likelihood of overfitting.\n\nTo address the potential issue of overfitting in deep learning models, several strategies were implemented. The dataset was divided into a 90:10 training and test split, with the training set further divided into training and validation subsets. This allowed for simultaneous validation during training, ensuring that the model generalized well to unseen data. Moreover, techniques such as transfer learning were employed, utilizing pre-trained networks like GoogleNet and ResNet, which have been trained on large datasets. This approach leverages pre-existing knowledge to improve model performance and reduce the risk of overfitting. Additionally, the number of epochs was varied (30, 50, and 100) to find the optimal training duration, and an initial learning rate of 0.001 with a Stochastic Gradient Descent with momentum (sgdm) optimizer was used to fine-tune the model parameters.\n\nUnderfitting was addressed by ensuring that the models were complex enough to capture the underlying patterns in the data. For machine learning models, the use of diverse classifiers and hyperparameter tuning helped in selecting models that were neither too simple nor too complex. For deep learning models, the architecture of ResNet18, which includes residual connections to mitigate the vanishing gradient problem, allowed for the training of deep networks that could learn complex representations.\n\nIn summary, the fitting method involved careful consideration of model complexity, validation techniques, and hyperparameter tuning to balance between overfitting and underfitting. The use of cross-validation, transfer learning, and varied training epochs ensured that the models were robust and generalizable.",
  "optimization/regularization": "Steps were taken to reduce overfitting. This included using techniques such as cross-validation and data normalization. Cross-validation helps to ensure that the model generalizes well to unseen data by training and validating on different subsets of the data. Data normalization was employed to prevent any single variable from disproportionately influencing the model performance due to its large numerical values. Additionally, the use of transfer learning in deep learning models helped to mitigate overfitting by leveraging pre-trained networks, which have already learned robust features from large datasets.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are thoroughly documented within the methodology section of our publication. Specifically, we trained our deep learning models for 30, 50, and 100 epochs to determine the optimal number of training iterations. The initial learning rate was set to 0.001, and we employed Stochastic Gradient Descent with momentum (sgdm) as our optimizer. These details are provided to ensure reproducibility and transparency in our research methods.\n\nRegarding the availability of model files and optimization parameters, we have made efforts to ensure that all necessary information is accessible. The models were implemented using MATLAB 2023b, which offers robust visualization and ease of use. While the specific model files may not be directly downloadable from the publication, the configurations and parameters are clearly outlined, allowing other researchers to replicate our experiments.\n\nThe source data underlying our results is available to ensure full reproducibility. This includes the datasets used for training, validation, and testing, as well as the scripts and code necessary to run the models. We have also provided performance metrics such as specificity, sensitivity, precision, NPV, recall, and F1 score to assess the model's effectiveness.\n\nFor those interested in accessing our work, the publication is open access, distributed under the terms of the Creative Commons Attribution License. This license permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. This ensures that our findings and methods can be widely shared and built upon by the scientific community.",
  "model/interpretability": "The models employed in this study encompass both transparent and black-box approaches, each serving distinct purposes in our analysis.\n\nFor the machine learning classifiers, such as random forest and AdaBoost, we have chosen models known for their robustness and interpretability. These models provide insights into feature importance, allowing us to identify which variables contribute most significantly to the classification task. For instance, random forest models can rank features based on their importance scores, indicating which characteristics of the data are most influential in predicting outcomes. This transparency is crucial in medical diagnostics, where understanding the underlying factors driving predictions is essential for clinical decision-making.\n\nIn contrast, deep learning models like GoogleNet and ResNet are considered black-box models. These models automatically extract features from raw MRI images without human intervention, making them highly effective for image analysis but less interpretable. However, to enhance the interpretability of these black-box models, we suggest the use of SHAP (SHapley Additive exPlanations) analysis. SHAP values can help in understanding the contribution of each feature to the model's predictions, providing a way to interpret the complex decisions made by these deep learning architectures. By comparing the variables highlighted as important by the best-performing models with clinical literature, we can draw meaningful conclusions and validate the findings of our research.\n\nIn summary, while some of our models offer transparency through feature importance rankings, others rely on advanced techniques like SHAP analysis to provide interpretability. This dual approach ensures that our study benefits from both the predictive power of deep learning and the explainability of traditional machine learning methods.",
  "model/output": "The model employed in this study is a classification model, specifically designed for binary classification. The primary objective is to predict the presence or absence of low back pain (LBP) using machine learning (ML) and deep learning (DL) algorithms. Several ML classifiers were studied, including Random Forest, Decision Tree, Logistic Regression, K-Nearest Neighbors (KNN), and AdaBoost. These models were evaluated using five-fold cross-validation to ensure robust performance metrics.\n\nThe Random Forest model demonstrated high performance across all lumbar vertebrae, with Area Under the Curve (AUC) values ranging from 0.83 to 0.88. Decision Tree models exhibited moderate performance, with AUC values between 0.65 and 0.76. Logistic Regression performed well, particularly at the L5 vertebra with an AUC of 0.82, and maintained good performance across other vertebral levels. KNN showed strong performance, especially at the L2-L4 vertebrae, with AUC values of 0.79 to 0.83. AdaBoost demonstrated high performance at the L2\u2013L5 vertebrae, with AUC values of 0.82 to 0.90, although its performance at L1 was moderate.\n\nFor the lumbar intervertebral discs, the Random Forest model showed strong performance at L2-L3, L3-L4, and L4-L5, with an AUC of 0.88, and the highest at L5-S1 with an AUC of 0.92. Decision Tree models showed moderate performance, with the highest AUC at the L5-S1 intervertebral disc (0.85). Logistic Regression had the highest AUC at the L3-L4 intervertebral disc (0.90) and maintained good performance at other discs. KNN showed the highest AUC at the L4-L5 intervertebral disc (0.88) and moderate performance at other discs. AdaBoost exhibited the highest AUC (0.97) at the L5-S1 intervertebral disc and strong results at the L2-L3 and L3-L4 discs.\n\nThe models showed slightly improved performance at the lower vertebral levels (L4 and L5) compared to the upper levels (L1-L3). The Random Forest and AdaBoost models, in particular, demonstrated high performance, especially at the L5-S1 intervertebral disc.\n\nThe clinical significance of these results is substantial. ML and DL models can provide more efficient, reliable, and noninvasive diagnostic insights by accurately identifying abnormalities in the lumbar vertebrae and intervertebral discs, even in cases where conventional MRI image assessments are inconclusive. By improving the ability to predict LBP, these algorithms can guide better clinical decision-making and reduce unnecessary surgical interventions.",
  "model/duration": "The execution time for the models varied depending on the complexity of the algorithms and the computational resources used. For the machine learning classifiers, the training process utilized 8 GB of RAM and an Intel\u00ae Core\u2122 i5 Central Processing Unit. The models were run in a Conda virtual environment integrated with Python (version 3.9.7) on a 64-bit Windows operating system. The training of these models was efficient, leveraging several libraries such as NumPy, scikit-learn, pandas, seaborn, and matplotlib to support the analysis.\n\nFor the deep learning models, the training was conducted using MATLAB 2023b, which was chosen for its superior visualization capabilities and ease of use. The deep learning models, including GoogleNet and ResNet (18 and 50), were trained for different numbers of epochs\u201430, 50, and 100\u2014to optimize performance. The training process involved intensity normalization of MRI images and the use of transfer learning techniques, which significantly reduced the time required compared to training from scratch. The models were assessed using various performance metrics, including specificity, sensitivity, precision, NPV, recall, and F1 score, to ensure robust and accurate predictions.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our methods involved a comprehensive approach to ensure the robustness and generalizability of our models. We employed a 90:10 training and test split ratio, where the training set was further divided into training and validation subsets. This allowed for simultaneous validation during the training process, ensuring that our models were not overfitting to the training data.\n\nDeep learning models were implemented using MATLAB 2023b, chosen for its superior visualization capabilities and ease of use. To optimize performance, we adjusted hyperparameters such as the number of epochs and the initial learning rate. The dataset was trained for 30, 50, and 100 epochs to determine the optimal number of training cycles. The initial learning rate was set to 0.001, and we used Stochastic Gradient Descent with momentum (sgdm) as the optimizer.\n\nThe performance of our deep learning models was assessed using several key metrics, including specificity, sensitivity, precision, negative predictive value (NPV), recall, and the F1 score. These metrics provided a holistic view of the models' effectiveness in predicting lower-back pain (LBP).\n\nAdditionally, we utilized a binary classification problem to predict LBP, which is crucial for evaluating the models' ability to distinguish between patients with and without the condition. The class variability in this binary classification problem was carefully considered, as it impacts evaluation metrics such as sensitivity, specificity, precision, and the F1 score. The more variability there is within and between classes, the more robust the model needs to be.\n\nTo ensure the reliability of our results, we also highlighted the clinical significance of the findings obtained from the classification algorithms. Our study demonstrated that machine learning and deep learning models could provide more efficient, reliable, and noninvasive diagnostic insights by accurately identifying abnormalities in the lumbar vertebrae and intervertebral discs (IVDs), even in cases where conventional MRI image assessments were inconclusive. This improvement in predictive accuracy can guide better clinical decision-making and reduce unnecessary surgical interventions.",
  "evaluation/measure": "The performance of the deep learning models was evaluated using several key metrics to ensure a comprehensive assessment. These metrics include specificity, sensitivity, precision, negative predictive value (NPV), recall, and the F1 score. These metrics are widely recognized and used in the literature for evaluating classification models, particularly in medical imaging and diagnostic tasks.\n\nSpecificity measures the ability of the model to correctly identify negative cases, which is crucial in medical diagnostics to avoid false positives. Sensitivity, also known as recall, assesses the model's ability to correctly identify positive cases, ensuring that true positives are not missed. Precision focuses on the accuracy of positive predictions, indicating how many of the predicted positives are actual positives. The F1 score provides a balanced measure of precision and recall, especially useful when dealing with imbalanced datasets. The negative predictive value (NPV) complements specificity by indicating the proportion of negative predictions that are true negatives.\n\nThese metrics collectively provide a robust evaluation framework, ensuring that the model's performance is assessed from multiple angles. This approach is representative of standard practices in the field, aligning with the literature on model evaluation in machine learning and deep learning, particularly in medical applications.",
  "evaluation/comparison": "A wide range of machine learning algorithms were employed to ensure comprehensive benchmarking and to select the optimal model for the specific problem at hand. Different classifiers were chosen because they excel in various aspects: random forest for robustness and accuracy, decision trees for interpretability, logistic regression for linear relationships, KNN for smaller datasets, and AdaBoost for improving performance by combining weak learners.\n\nDeep learning algorithms such as ResNet and GoogleNet were utilized due to their powerful ability to learn complex patterns, particularly in image analysis and medical diagnostics. These models extract features directly from images without human intervention, making them suitable for tasks like image classification and object recognition. GoogleNet's inception modules process input using parallel convolution layers with varying kernel sizes, enhancing efficiency by capturing features at different scales. ResNet addresses the vanishing gradient problem, allowing for the training of very deep networks efficiently.\n\nThe analysis involved both radiomics datasets and direct MRI images. For the machine learning classifiers, radiomics features were extracted and used. In contrast, deep learning models utilized MRI images directly, extracting features autonomously. This approach allowed for a comparison between feature-engineered methods and end-to-end learning techniques.\n\nThe study employed a binary classification problem to predict low back pain (LBP). Class variability was considered, impacting evaluation metrics such as sensitivity, specificity, precision, and F1-score. The models were validated using a five-fold cross-validation method, ensuring robustness and generalizability of the results.\n\nHyperparameter tuning was performed to optimize model performance. Epochs were set to 30, 50, and 100, and the initial learning rate was set to 0.001. The Stochastic Gradient Descent with momentum (sgdm) optimizer was used. Model performance was assessed using metrics such as specificity, sensitivity, precision, NPV, recall, and F1 score.\n\nThe clinical significance of the results was highlighted, demonstrating that machine learning and deep learning models can provide efficient, reliable, and noninvasive diagnostic insights. These models can accurately identify abnormalities in the lumbar vertebrae and intervertebral discs, even when conventional MRI assessments are inconclusive. This improves the ability to predict LBP, guiding better clinical decision-making and reducing unnecessary surgical interventions.",
  "evaluation/confidence": "The evaluation of our models includes a comprehensive assessment of performance metrics, which are crucial for understanding the reliability and robustness of our predictions. We employed five-fold cross-validation to ensure that our results are generalizable and not dependent on a specific subset of data. This method helps in providing a more accurate estimate of model performance by averaging the results across multiple validation splits.\n\nTo further bolster the confidence in our results, we considered the variability within and between classes, which is particularly important in a binary classification problem. This variability impacts key evaluation metrics such as sensitivity, specificity, precision, and the F1-score. By accounting for this variability, we ensure that our models are robust and can handle the complexities of real-world data.\n\nStatistical significance is a critical aspect of our evaluation. We performed appropriate statistical tests to determine whether the differences in performance between our models and baseline methods are significant. This involves comparing the performance metrics using techniques such as paired t-tests or non-parametric tests, depending on the distribution of the data. By doing so, we can confidently claim that our methods offer superior performance.\n\nAdditionally, we included confidence intervals for our performance metrics to provide a range within which the true performance is likely to fall. This adds another layer of confidence in our results, as it shows the precision of our estimates. The inclusion of confidence intervals is particularly important for metrics like the area under the ROC curve (AUC), where small differences can have significant clinical implications.\n\nIn summary, our evaluation process is designed to be rigorous and comprehensive. We use cross-validation, account for class variability, perform statistical tests for significance, and include confidence intervals for our performance metrics. These steps ensure that our claims of superior performance are well-supported and reliable.",
  "evaluation/availability": "All the source data underlying the results are available to ensure full reproducibility. This includes the raw evaluation files necessary for verifying the findings presented in the study. The data is made accessible to facilitate further research and to allow other researchers to replicate the experiments conducted. The specific details regarding the accessibility and licensing of the data can be found in the supplementary materials or the data availability statement within the publication. This ensures that the research is transparent and that the results can be independently verified."
}