{
  "publication/title": "Prediction of protein biophysical traits from limited data: a case study on nanobody thermostability through NanoMelt.",
  "publication/authors": "Ramon A, Ni M, Predeina O, Gaffey R, Kunz P, Onuoha S, Sormanni P",
  "publication/journal": "mAbs",
  "publication/year": "2025",
  "publication/pmid": "39772905",
  "publication/pmcid": "PMC11730357",
  "publication/doi": "10.1080/19420862.2024.2442750",
  "publication/tags": "- Protein fitness\n- Thermostability\n- Antibody engineering\n- Antibody design\n- Nanobody\n- Machine learning\n- Semi-supervised learning\n- Ensemble model\n- Biophysics\n- Computational biology",
  "dataset/provenance": "The dataset comprises 640 distinct nanobody melting temperatures. This dataset was constructed by combining several sources. Firstly, 193 nanobody sequences were curated from our own experiments, including 95 sequences selected from the PDB database, 34 nanobodies already available in our laboratory from previous projects, and raw nanoDSF data for 64 additional nanobodies from the research conducted by Kunz et al. All measurements were performed under consistent experimental conditions using nanoDSF and analyzed by fitting a two-state thermal denaturation model to ensure high consistency.\n\nAdditionally, 405 nanobodies were sourced from the NbThermo database, which collates data from various previously published works. These melting temperatures were measured using a range of experimental methods, including nanoDSF, DSF, CD, and DSC. The original NbThermo database was filtered to remove entries without associated sequences or melting temperatures, sequences that could not be aligned, duplicates, and entries that matched sequences from our own dataset or the work by Kunz et al.\n\nThis comprehensive dataset includes measurements from three experimental techniques: 251 from nanoDSF, 205 from CD, and 165 from sypro-orange DSF. The sequences were clustered using the k-medoids algorithm into 13 distinct groups, demonstrating the diversity of the dataset. The dataset covers a wide range of melting temperatures, from 26.6\u00b0C to 98.2\u00b0C, and includes a variety of CDR3 lengths and somatic mutation rates. The complete dataset is available as Supplementary Dataset 1.",
  "dataset/splits": "The dataset was split using a repeated stratified nested cross-validation procedure. This involved an outer loop for model testing and an inner loop for hyperparameter tuning. The outer loop was repeated with three random seeds to report averaged performances and their standard deviations. The dataset was clustered by experimental method and sequence similarity via k-medoids clustering, ensuring consistent class distribution across training and testing splits. The training set consisted of approximately 80% of the data, while the testing set consisted of the remaining 20%. The dataset comprised 640 distinct nanobodies, with melting temperatures (Tm) ranging from 26.6\u00b0C to 98.2\u00b0C. The sequences were clustered into 13 distinct groups, demonstrating the diversity of the dataset. The distribution of data points across the splits was maintained to improve generalizability and reliability when training on imbalanced heterogeneous data.",
  "dataset/redundancy": "The datasets were split using a stratified approach, which maintained the class distribution across folds. This method was chosen to improve the generalizability and reliability of the models, especially when dealing with imbalanced and heterogeneous data. The stratification was based on experimental methods and sequence similarity clusters, ensuring that the training and testing sets were representative of the overall dataset.\n\nThe independence of the training and test sets was enforced through repeated nested cross-validation. This procedure involves an inner loop for hyperparameter tuning and an outer loop for performance evaluation on an unseen test set. The process was repeated with three random seeds to capture the magnitude of performance variations and to ensure that the results were not due to random chance.\n\nThe distribution of the datasets in this study compares favorably to previously published machine learning datasets in the field. The use of stratified nested cross-validation helps to mitigate issues related to data leakage and overfitting, which are common challenges when working with limited and heterogeneous data. This approach ensures that the models are evaluated on data that they have not seen during training, providing a more accurate assessment of their performance and generalizability.",
  "dataset/availability": "The complete dataset of nanobody melting temperatures is available as Supplementary Dataset 1. This dataset includes measurements from three experimental techniques: nanoDSF, CD, and sypro-orange DSF. The dataset comprises 640 distinct nanobodies, with melting temperatures (Tm) ranging from 26.6\u00b0C to 98.2\u00b0C. The sequences were clustered using the k-medoids algorithm into 13 distinct groups, demonstrating the diversity of the dataset.\n\nThe data, including the data splits used, are released in a public forum. The dataset is accessible online at the provided DOI: [https://doi.org/10.1080/19420862.2024.2442750](https://doi.org/10.1080/19420862.2024.2442750). The terms of this article's publication allow for the posting of the Accepted Manuscript in a repository by the author(s) or with their consent. This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. This ensures that the data is freely available for further research and validation by other scientists.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established regression models, including both linear and non-linear techniques. These models range from linear approaches like Ridge, Huber, and Elastic Net to non-linear methods such as Random Forest, LightGBM, Support Vector Regression (SVR), and Gaussian Process Regression (GPR). These algorithms are not new but are widely recognized and utilized in the field of machine learning.\n\nThe choice of these algorithms was driven by their proven effectiveness in handling various types of data and their ability to capture complex relationships within the dataset. The study focuses on thermostability predictions, and the selected models were chosen for their robustness and ability to generalize well from limited, heterogeneous data.\n\nThe decision to use these established algorithms rather than developing a new one was influenced by the need for reliability and comparability. By employing well-known models, the study ensures that the results can be replicated and validated by other researchers in the field. Additionally, these models have been extensively tested and optimized, providing a solid foundation for the research without the need for extensive additional development and validation.\n\nThe algorithms were implemented and evaluated using a repeated nested stratified cross-validation procedure. This approach ensures that the models are trained and tested on representative samples of the data, reducing the risk of overfitting and enhancing the generalizability of the results. The hyperparameters of each model were tuned using an inner loop of cross-validation, while the outer loop assessed the performance on an unseen test set. This rigorous evaluation framework helps in selecting the best-performing models and ensures that the findings are robust and reliable.",
  "optimization/meta": "In the optimization process, an ensemble learning approach was employed to enhance prediction performance. This method combines multiple individually trained models into a single predictor, leveraging model diversity to increase generalizability and robustness, especially when dealing with limited and heterogeneous data.\n\nThe ensemble model uses predicted test-set temperatures from various regression models as inputs. These predictions are gathered from each outer test fold during the nested cross-validation of every model. The process is repeated three times with different random seeds to ensure reliability.\n\nThree model selection strategies were explored for combining the regression models:\n\n1. The first strategy selects up to eight top-ranking regression models, considering both the model type and the embedding used.\n2. The second strategy chooses the best-performing regression model for each type of embedding, with up to eight different embeddings.\n3. The third strategy picks the best-performing embedding for each regression model, with up to seven models.\n\nTwo ensemble strategies were evaluated:\n\n1. Averaging the predictions from up to eight models.\n2. Stacking the predictions using a ridge regression meta-model.\n\nTo ensure the independence of training data, a repeated nested stratified cross-validation procedure was used. This procedure involves an inner loop for hyperparameter tuning and an outer loop for performance evaluation on an unseen test set. The entire process is repeated with three different random seeds to capture the magnitude of performance variations and to maintain the independence of the training data from the model assessment. This approach prevents data leakage between training, validation, and test sets, ensuring that the model fine-tuning is independent from the model assessment.",
  "optimization/encoding": "In our study, we employed various sequence representations or embeddings to numerically encode the nanobody sequences for the machine-learning algorithms. These embeddings included categorical encodings like one-hot encoding, which represents each amino acid as a binary vector. We also used physicochemistry-based encodings such as VHSE, which captures the physicochemical properties of amino acids.\n\nAdditionally, we utilized pre-trained embeddings derived from large language models. These models included generic protein language models like ESM-1b and ESM-2, as well as models specifically trained on antibody sequences (AbLang and AntiBERTy) and nanobody sequences (nanoBERT). Furthermore, we explored embeddings from a state-of-the-art nanobody structure predictor, NanobodyBuilder2.\n\nAll sequences were aligned with the AHo numbering scheme to a fixed length of 149 positions. This alignment ensured consistency in the input data for the machine-learning models. The embeddings were then standardized within each cross-validation fold by removing the mean and scaling to unit variance. This standardization process helped to apply regularization uniformly across the features.\n\nTo reduce overfitting, we performed feature selection based on importance rankings obtained from a Ridge regression model trained on the corresponding training subset at each fold. This approach helped in identifying the most relevant features for the regression models.\n\nThe encoded data was then used to train various regression models, including linear models like Ridge and Elastic Net, as well as non-linear models such as Random Forest, LightGBM, Support Vector Regression, and Gaussian Process Regression. The performance of these models was evaluated using repeated nested stratified cross-validation, ensuring a robust assessment of their predictive capabilities.",
  "optimization/parameters": "In our study, the number of input parameters (p) used in the models varied depending on the embedding and feature selection strategies employed. We utilized multiple embeddings, including ESM-1b, one-hot encoding, and VHSE, among others. Each embedding type transformed the protein sequences into a different set of features, resulting in varying dimensions.\n\nTo determine the optimal number of features, we employed feature selection based on importance rankings obtained from a Ridge regression model trained on the corresponding training subset at each fold. This process helped in reducing overfitting and ensuring that only the most relevant features were used.\n\nAdditionally, we explored different feature selection strategies for the ensemble model. The first strategy concatenated the ESM-1b embedding or one-hot encoding with the predicted input temperatures. The second strategy added experimental method information by concatenating it with the predicted input temperatures. The third strategy integrated both sequence and experimental information into the ensemble model.\n\nThe final number of input parameters (p) for the ensemble model was determined by the combination of these features. For instance, if using the ESM-1b embedding and experimental method information, the input parameters would include the dimensions of the ESM-1b embedding plus the experimental method features.\n\nIn summary, the number of input parameters (p) was dynamically selected based on the embedding type and feature selection strategy, ensuring that the models were optimized for performance and generalization.",
  "optimization/features": "In our study, the number of input features varied depending on the specific strategy employed for the ensemble ridge model. Three distinct feature selection strategies were explored.\n\nThe first strategy incorporated sequence information by concatenating the ESM-1b embedding or one-hot encoding with the eight predicted input temperatures. The size of the original embedding was either maintained or reduced using a linear layer to match the number of input temperatures.\n\nThe second strategy added experimental method information by concatenating it with the predicted input temperatures. This method information was either added as a single integer or expanded via a linear layer to match the number of input temperatures.\n\nThe third strategy integrated both sequence and experimental information into the ensemble model.\n\nFeature selection was performed to reduce overfitting. This was done using importance rankings obtained from a Ridge regression model trained on the corresponding training subset at each fold. This ensured that the feature selection process was conducted using the training set only, maintaining the integrity of the test set for unbiased performance evaluation.",
  "optimization/fitting": "In our study, we addressed the challenge of fitting models to limited and heterogeneous data by employing several strategies to mitigate both overfitting and underfitting.\n\nTo prevent overfitting, we utilized repeated stratified nested cross-validation. This approach ensures that the model's performance is evaluated on unseen data, reducing the risk of overfitting to the training set. The nested structure includes an inner loop for hyperparameter tuning and an outer loop for performance evaluation, repeated with three random seeds to capture performance variations. This method helps in fine-tuning the model independently from the assessment process, thus providing a more reliable estimate of the model's generalization capability.\n\nAdditionally, we applied feature selection to reduce the number of free parameters in each regression model. This step is crucial when dealing with small datasets, as it helps in focusing on the most relevant features, thereby reducing the complexity of the model and the likelihood of overfitting.\n\nTo further mitigate overfitting, we standardized the embeddings within each cross-validation fold by removing the mean and scaling to unit variance. This ensures that regularization is applied uniformly across the features.\n\nWe also addressed the tendency of models to regress toward the mean, which is a common issue with small datasets. To quantify this tendency, we calculated the Standard Deviation Ratio (SDR). This metric helped us identify models that might perform well in terms of correlation coefficients but fail to cover the full range of measured values. For instance, while simple averaging of predictions showed promising correlation coefficients, the SDR revealed a stronger tendency to regress toward the mean compared to ridge regression. Therefore, we preferred ridge regression for the final model.\n\nTo ensure that our models did not underfit, we evaluated a diverse range of regression models, including both linear (e.g., Ridge, Huber, Elastic Net) and non-linear models (e.g., Random Forest, LightGBM, Support Vector Regression, Gaussian Process Regression). This diversity allowed us to capture complex relationships in the data without being constrained by the assumptions of a single model type.\n\nMoreover, we used ensemble learning to combine multiple individually trained models into a single predictor. This approach leverages the strengths of different models, enhancing performance and robustness. We explored two ensemble strategies: averaging the predictions from up to eight models and stacking them using a ridge regression meta-model. Both approaches were evaluated using the same stratified nested cross-validation pipeline, ensuring a fair comparison.\n\nIn summary, our fitting method involved a combination of nested cross-validation, feature selection, standardization, and ensemble learning to address the challenges of overfitting and underfitting in the context of limited and heterogeneous data.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting, which is a critical concern when working with limited datasets. One of the primary methods used was feature selection. For each regression model, feature selection was applied to reduce the number of free parameters, thereby mitigating overfitting. This process involved ranking features based on their importance and selecting the most relevant ones.\n\nAdditionally, we utilized stratified nested cross-validation. This approach ensures that the model is trained and evaluated on different subsets of the data, preventing data leakage between training, validation, and test sets. The nested structure includes an inner loop for hyperparameter tuning and an outer loop for performance evaluation, repeated with multiple random seeds to capture the variability in performance.\n\nRegularization techniques were also applied uniformly across the features for models like Ridge, Elastic Net, Support Vector Regression (SVR), and Gaussian Process Regression (GPR). This involved standardizing the embeddings within each cross-validation fold by removing the mean and scaling to unit variance. Such standardization helps in ensuring that the regularization penalties are applied consistently across all features.\n\nFurthermore, ensemble learning was employed to enhance model performance and robustness. By combining multiple individually trained models, ensemble learning leverages the diversity of models to improve generalizability. Specifically, we used a ridge regression meta-model for stacking predictions from various regression models, which helped in reducing the dependency on individual models' biases.\n\nIn summary, our approach to preventing overfitting involved a combination of feature selection, stratified nested cross-validation, regularization, and ensemble learning. These techniques collectively ensured that our models were robust and generalizable, even when trained on limited and heterogeneous data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule are reported and available. The ranges of hyper-parameter values used during the tuning process are directly accessible in the code repository. This repository is hosted at https://gitlab.developers.cam.ac.uk/ch/sormanni/nanomelt. The repository likely contains the necessary scripts and configurations used for training and evaluating the models, including the hyper-parameter tuning process.\n\nThe specific details of the optimization parameters and model files are not explicitly mentioned, but the repository should contain the relevant information and code necessary to replicate the experiments and understand the optimization process. The availability of the code repository ensures that researchers can access the configurations and schedules used, promoting reproducibility and further research.\n\nThe license under which the repository is available is not specified, but it is common for academic repositories to use open-source licenses that allow for free access and use, subject to certain conditions. Researchers interested in using the repository should check the license details provided within the repository itself.",
  "model/interpretability": "The model developed in this study, NanoMelt, is primarily an ensemble of regression models trained on various embeddings, which inherently makes it somewhat of a black-box model. The use of complex regression techniques like Gaussian Process Regression (GPR) and Support Vector Regression (SVR) further contributes to this opacity. These models utilize kernel functions that capture intricate data relationships, making it challenging to interpret the exact mechanisms by which predictions are made.\n\nHowever, the transparency of the model can be partially attributed to the use of pre-trained embeddings. Embeddings like ESM-1b, VHSE, and one-hot encoding provide a structured representation of protein sequences, which can be more interpretable than raw sequence data. For instance, ESM-1b embeddings are derived from a language model trained on a large corpus of protein sequences, capturing evolutionary and structural information that can be somewhat intuitive to interpret.\n\nThe ensemble approach, which combines predictions from multiple models, adds another layer of complexity. While ensemble methods are known to improve performance and robustness, they do so at the cost of interpretability. The final predictions are a weighted average or a stacked regression of individual model outputs, making it difficult to trace back the contribution of each feature or embedding to the final prediction.\n\nIn summary, while the model benefits from the interpretability of pre-trained embeddings, the overall ensemble framework and the use of complex regression techniques make it largely a black-box model. The trade-off between performance and interpretability is a common challenge in machine learning, and this study prioritizes predictive accuracy over transparency.",
  "model/output": "The model employed in our study is a regression model, specifically designed to predict protein thermostability. We utilized various regression techniques, including ridge regression, Gaussian Process Regression (GPR), Random Forest (RF), Support Vector Regression (SVR), and others. These models were trained to predict melting temperatures (Tm) of proteins based on their sequences.\n\nThe regression models were evaluated using several performance metrics, such as Pearson\u2019s correlation (r), Spearman\u2019s correlation (\u03c1), mean absolute error (MAE), and standard deviation ratio (SDR). These metrics provide a comprehensive assessment of the model's predictive accuracy and robustness.\n\nTo ensure the reliability of our results, we implemented a repeated nested cross-validation procedure. This involved an inner loop for hyperparameter tuning and an outer loop for model testing, repeated with three random seeds. This approach helps in capturing the variability in performance and ensures that the models generalize well to unseen data.\n\nIn addition to individual regression models, we explored ensemble learning techniques. Ensemble models combine the predictions of multiple individually trained models to enhance performance. We evaluated different ensemble strategies, including averaging predictions and stacking them using a ridge regression meta-model. These ensemble approaches aim to leverage the strengths of diverse models, improving the overall predictive accuracy and robustness.\n\nThe output of our models includes predicted melting temperatures for protein sequences. These predictions are crucial for understanding protein stability and can guide experimental design and optimization efforts. The models were trained and tested on a dataset that includes various experimental methods for measuring thermostability, ensuring that the predictions are representative and reliable.",
  "model/duration": "The model, referred to as NanoMelt, takes approximately 250 seconds to process 1,000 sequences on a GPU, specifically an NVIDIA RTX 8000. This execution time is indicative of the model's efficiency in handling a substantial number of sequences, making it practical for large-scale predictions. The use of a powerful GPU ensures that the model can deliver results in a reasonable timeframe, which is crucial for applications requiring rapid analysis of protein sequences.",
  "model/availability": "The source code for NanoMelt is publicly available. It can be accessed through the NanoMelt repository, which includes all the data used in the study, the trained model, the Python pipeline, and the notebooks used for analysis. The repository is hosted at https://gitlab.developers.cam.ac.uk/ch/sormanni/nanomelt.\n\nAdditionally, a user-friendly web server has been developed to facilitate the running of NanoMelt. This web server is accessible at https://www-cohsoftware.ch.cam.ac.uk/. To use the web server, users need to register for a free account and log in. This web server provides an easy-to-use interface for running the NanoMelt algorithm without the need for local installation or configuration.\n\nThe software is designed to be efficient, taking approximately 250 seconds to process 1,000 sequences on a GPU (NVIDIA RTX 8000). This makes it suitable for large-scale applications and research projects involving protein sequence analysis.",
  "evaluation/method": "The evaluation method employed in this study was a robust repeated nested stratified cross-validation procedure. This approach ensures efficient use of limited data while preventing data leakage between training, validation, and test sets. The procedure involves two layers of cross-validation: an outer loop and an inner loop. In the outer loop, the dataset is split into three folds, with two folds used for training and hyperparameter tuning in the inner loop, and the remaining fold serving as the test set to assess model performance. This process is repeated with three different random seeds to capture the magnitude of performance variations.\n\nStratification was maintained based on the experimental method and sequence similarity cluster, ensuring that the class distribution is consistent across folds. This improves the generalizability and reliability of the models, especially when training on imbalanced heterogeneous data. The performance of the models was evaluated using several metrics, including Pearson\u2019s correlation, Spearman\u2019s correlation, mean absolute error (MAE), and standard deviation ratio (SDR). These metrics provide a comprehensive assessment of the models' predictive accuracy and robustness.\n\nThe evaluation also included a comparison between stratified and random cross-validation, demonstrating that stratification improved performance slightly but consistently and reduced variability. This indicates that the models trained with stratified cross-validation are more reliable and generalizable. Additionally, the study explored ensemble learning, which combines multiple individually trained models into a single predictor. This approach leverages model diversity, enhancing performance and robustness when working with limited, heterogeneous data. The ensemble models were evaluated using the same stratified nested cross-validation pipeline, ensuring a fair and thorough assessment of their performance.",
  "evaluation/measure": "In our evaluation, we employed a comprehensive set of performance metrics to assess the robustness and accuracy of our models. The primary metrics reported include Pearson\u2019s correlation (r), Spearman\u2019s correlation (\u03c1), mean absolute error (MAE), and standard deviation ratio (SDR). These metrics provide a well-rounded view of model performance, capturing both the linear and non-linear relationships between predicted and actual values, as well as the models' tendency to regress towards the mean.\n\nPearson\u2019s correlation measures the linear relationship between the predicted and observed values, while Spearman\u2019s correlation assesses the monotonic relationship, making it more robust to outliers. MAE quantifies the average magnitude of errors in the predictions, providing an intuitive measure of prediction accuracy. SDR indicates how much the model's predictions deviate from the dataset mean, which is crucial for understanding the model's behavior, especially when dealing with limited data.\n\nThese metrics are averaged across the test folds of the outer loop in the repeated nested cross-validation, ensuring that the reported performances are stable and not dependent on a particular data split. The uncertainties reported are the corresponding standard deviations, giving a sense of the variability in performance.\n\nThe choice of these metrics is representative of standard practices in the literature, ensuring that our evaluation is comparable to other studies in the field. By reporting these metrics, we aim to provide a clear and comprehensive assessment of our model's performance, highlighting its strengths and areas for potential improvement.",
  "evaluation/comparison": "In the evaluation of our model, we conducted a comprehensive benchmarking process to compare our approach with existing methods. We selected three publicly available methods that are capable of predicting melting temperatures or providing stability scores for protein sequences. These methods include FoldX, an empirical force-field-based algorithm that calculates the free energy of unfolding for a given structure; DeepSTABp, a transformer-based melting temperature predictor trained on a large dataset; and sequence likelihood outputs from ESM-2 and AntiBERTy, which were used as proxies for stability in the context of zero-shot predictions.\n\nThe comparison was performed on a curated dataset of nanobody sequences, ensuring that the benchmarking was conducted on a relevant and diverse set of data. This allowed us to assess the performance of our model against established methods in a fair and rigorous manner.\n\nAdditionally, we considered simpler baselines to understand the relative improvement offered by our approach. For instance, we evaluated the performance of individual regression models trained on various embeddings, such as ESM-1b, one-hot, VHSE, AntiBERTy, and nanoBERT. This comparison highlighted the strengths and weaknesses of different embedding and regression model combinations, providing insights into the factors that contribute to the overall performance of our ensemble model.\n\nThe results of these comparisons demonstrated that our ensemble model, which combines multiple regression models trained on diverse embeddings, achieved superior performance in terms of Pearson\u2019s and Spearman\u2019s correlations, mean absolute error (MAE), and standard deviation ratio (SDR). This indicates that our approach not only outperforms simpler baselines but also provides more reliable and generalizable predictions for nanobody thermostability.",
  "evaluation/confidence": "The evaluation of our model, NanoMelt, includes robust statistical measures to ensure confidence in the reported performance metrics. We employed repeated nested cross-validation, which helps in efficiently using limited data while preventing data leakage. This procedure involves an inner loop for hyperparameter tuning and an outer loop for performance evaluation on an unseen test set, repeated with three random seeds. This approach allows us to capture the magnitude of performance variations and report averaged performances along with their standard deviations.\n\nThe performance metrics, including Pearson\u2019s and Spearman\u2019s correlation coefficients, mean absolute error (MAE), and standard deviation ratio (SDR), are averaged across the test folds of the outer loop. The reported uncertainties are the corresponding standard deviations, providing a clear indication of the variability in performance.\n\nTo assess the statistical significance of our model's superiority over existing methods, we benchmarked NanoMelt against three other methods: FoldX, DeepSTABp, and sequence likelihood outputs from ESM-2 and AntiBERTy. The results demonstrate that NanoMelt achieves a Pearson\u2019s correlation of 0.853, a Spearman\u2019s correlation of 0.832, a MAE of 4.1\u00b0C, and an SDR of 0.86. These metrics indicate robust predictive performance and suggest that NanoMelt is superior to the benchmarked methods.\n\nAdditionally, we conducted a bootstrapping analysis to set realistic expectations for model performance in real-world applications. This analysis involved sampling subsets of our dataset, ranging from 2% to 75% of sequence predictions in test-set folds. The results show that when sampling one-third of the sequences, the correlation coefficients exhibit a narrow normal distribution centered around the expected values. However, when evaluating performance on smaller subsets, such as a dozen sequences, the correlation coefficients display much broader distributions, highlighting the unreliability of conclusions drawn from limited assessments.\n\nIn summary, the performance metrics for NanoMelt are accompanied by confidence intervals, and the results are statistically significant, demonstrating the model's superiority over existing methods. The use of repeated nested cross-validation and bootstrapping analysis ensures that the reported performance is reliable and generalizable to real-world applications.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being available for public release. However, the dataset used for training and testing the models, which includes the nanobody thermostability measurements, is made available as Supplementary Dataset 1. This dataset comprises 640 distinct nanobodies with melting temperatures ranging from 26.6\u00b0C to 98.2\u00b0C, measured using various experimental techniques. The dataset includes measurements from nanoDSF, CD, and sypro-orange DSF, and it has been clustered into 13 distinct groups using the k-medoids algorithm to ensure diversity and representativeness. The sequences cover a broad range of CDR3 lengths and exhibit a high average somatic mutation rate, indicating significant sequence diversity. The dataset is designed to be used for training and evaluating models to predict nanobody thermostability, and it has been constructed with careful consideration of experimental consistency and data quality. The complete dataset is available as Supplementary Dataset 1, but specific details about the availability of raw evaluation files or the license under which the data is released are not provided."
}