{
  "publication/title": "Machine Learning Methods for Identifying Atrial Fibrillation Cases and Their Predictors in Patients With Hypertrophic Cardiomyopathy: The HCM-AF-Risk Model.",
  "publication/authors": "Bhattacharya M, Lu DY, Ventoulis I, Greenland GV, Yalcin H, Guan Y, Marine JE, Olgin JE, Zimmerman SL, Abraham TP, Abraham MR, Shatkay H",
  "publication/journal": "CJC open",
  "publication/year": "2021",
  "publication/pmid": "34169259",
  "publication/pmcid": "PMC8209373",
  "publication/doi": "10.1016/j.cjco.2021.01.016",
  "publication/tags": "- Hypertrophic Cardiomyopathy\n- Atrial Fibrillation\n- Machine Learning\n- Data Imbalance\n- Logistic Regression\n- Na\u00efve Bayes Classifiers\n- Clinical Variables\n- Feature Selection\n- Polychoric Correlation\n- Cardiovascular Risk Prediction",
  "dataset/provenance": "The dataset used in our study consists of patient records, each represented by an 18-dimensional vector. This vector includes various clinical variables that were identified as most informative for predicting atrial fibrillation (AF). The dataset comprises 831 patients, each denoted as \\( p_i \\) where \\( 1 \\leq i \\leq 831 \\). Each patient's record is mapped to a vector \\( V_i = < p1_i, ..., p18_i > \\), with each dimension corresponding to a specific clinical value.\n\nThe dataset was derived from a larger set of 153 variables, which were preprocessed to retain 93 variables. These variables include a mix of continuous and nominal types, covering a wide range of clinical measurements such as biometrics, echocardiographic parameters, cardiac magnetic resonance (CMR) findings, exercise test results, and demographic information.\n\nThe dataset addresses a significant challenge of data imbalance, with a higher number of non-AF cases compared to AF cases, resulting in a ratio of approximately 3:1. To mitigate this imbalance, we employed a combination of under-sampling and over-sampling techniques. This approach was chosen after evaluating various methods, including simple oversampling, simple under-sampling, Adaptive Synthetic Sampling Approach (ADASYN), and meta-classification. Our combined method using Synthetic Minority Over-sampling Technique (SMOTE) proved to be superior in handling the imbalance and improving classifier performance.\n\nThe dataset has been used to train and test an ensemble classifier comprising logistic regression and na\u00efve Bayes, as well as individual classifiers like decision trees and random forests. The performance of these classifiers was evaluated using five-fold cross-validation, ensuring robust and reliable results. The dataset and the methods applied have not been previously used in the same manner by the community, representing a novel approach to addressing data imbalance in AF prediction.",
  "dataset/splits": "In our study, we employed a five-fold cross-validation scheme to train and test our ensemble classifier. The dataset was partitioned into five equal subsets. In each iteration of the cross-validation, four of these subsets (comprising 80% of the data) were used for training, while the remaining subset (20% of the data) was reserved for testing. This process was repeated five times, with each subset being used as the test set exactly once.\n\nTo address the data imbalance, we applied a combination of under-sampling and over-sampling techniques specifically to the training data. Initially, the ratio of non-AF to AF records was 3:1. We performed random under-sampling of the non-AF class to adjust this ratio to 2:1. Subsequently, we used the Synthetic Minority Over-sampling Technique (SMOTE) to generate synthetic minority instances, balancing the training set to have an equal number of AF and non-AF records.\n\nThe performance of the classifiers was evaluated on the imbalanced test set, which consisted of the 20% of data that was left out in each iteration. We conducted 10 complete sets of five-fold cross-validation experiments, resulting in a total of 50 runs. The reported performance metrics are averaged over these 50 runs, ensuring a robust evaluation of our model's effectiveness.",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in our study are well-established and widely recognized in the field. Specifically, we employed Logistic Regression, Decision Tree, Random Forest, and Na\u00efve Bayes classifiers. These are standard classification methods that have been extensively used and validated in various domains, including medical research.\n\nThe optimization method used for Logistic Regression is the Newton-Conjugate Gradient (newton-cg) algorithm, which is a well-known optimization technique for minimizing the cost function in logistic regression models. This method is chosen for its efficiency and effectiveness in handling large datasets and complex models.\n\nThese algorithms are not new; they have been thoroughly studied and applied in numerous research papers and practical applications. The choice to use these established methods was driven by their robustness and proven performance in similar contexts. While these algorithms are fundamental in machine learning, our application focuses on their use in a specific medical context\u2014predicting the likelihood of atrial fibrillation (AF) in patients with hypertrophic cardiomyopathy (HCM). This context is what sets our work apart, rather than the novelty of the algorithms themselves.\n\nGiven the focus of our study on medical data and the specific clinical application, publishing in a machine-learning journal was not the primary goal. Instead, we aimed to contribute to the medical literature by demonstrating the effectiveness of these machine-learning techniques in a clinical setting. This approach allows us to highlight the practical benefits of machine learning in healthcare, particularly in addressing data imbalance and improving diagnostic accuracy.",
  "optimization/meta": "The model employs a meta-predictor approach, utilizing data from other machine-learning algorithms as input. Specifically, the ensemble classifier combines the predicted probabilities from logistic regression and na\u00efve Bayes classifiers. This combination allows each classifier to influence the final classification decision differently, with weights determined through grid search.\n\nThe ensemble classifier is composed of logistic regression and na\u00efve Bayes. Logistic regression uses a hypothesis function and a cost function to model the probability of a record belonging to the AF class. Na\u00efve Bayes, on the other hand, applies Bayes' theorem with the assumption of independence between features.\n\nTo ensure the independence of training data, a five-fold cross-validation scheme is employed. The dataset is partitioned into five equal subsets, with four subsets (80%) used for training and one subset (20%) left out for testing. This process is repeated five times, each time with a different subset left out for testing. The combined over- and under-sampling approach is applied to the 80% of the dataset used for training, ensuring a balanced training set. The performance of the trained model is then evaluated on the imbalanced left-out test set.\n\nThe training and testing procedures are independent of each other, with the classifier assigning a probability to each record in the test set indicating its likelihood of belonging to the AF or non-AF class. This meta-predictor approach leverages the strengths of multiple classifiers to improve the overall performance and robustness of the model.",
  "optimization/encoding": "The data encoding process involved selecting and preprocessing a set of variables to represent each patient's record. Initially, a comprehensive set of 153 variables was considered. Through a rigorous preprocessing step, this set was refined to 93 variables that remained relevant for the analysis. These variables encompassed a mix of continuous and nominal types, covering various aspects such as biometrics, cardiac magnetic resonance (CMR) measurements, echocardiogram (ECHO) parameters, demographics, and exercise test results.\n\nTo address the challenge of data imbalance, where the number of non-AF cases significantly outnumbered the AF cases, a combination of oversampling and undersampling techniques was employed. This approach ensured that the training dataset was balanced, thereby improving the classifier's performance. Specifically, synthetic minority over-sampling technique (SMOTE) was used to generate synthetic samples for the minority class, while random undersampling was applied to reduce the majority class.\n\nThe final dataset consisted of 18 highly informative variables, which were identified through feature selection methods. These variables were used to represent each patient as an 18-dimensional vector. The classifier then calculated the probability of each patient belonging to the AF class versus the non-AF class based on these vectors. This probabilistic approach allowed for a nuanced assessment of each patient's risk for AF.\n\nThe machine learning classifiers, including logistic regression and na\u00efve Bayes, were trained using these encoded vectors. The performance of these classifiers was evaluated using a 5-fold cross-validation scheme, which involved partitioning the dataset into five equal subsets. Four subsets were used for training, while the remaining subset was used for testing. This process was repeated five times, with each subset serving as the test set once. The combination of oversampling, undersampling, and the ensemble of classifiers effectively addressed the data imbalance and improved the model's ability to distinguish between AF and non-AF cases.",
  "optimization/parameters": "In our study, we utilized a set of 18 variables that were identified as the most informative for predicting atrial fibrillation (AF). These variables were selected through a rigorous feature-selection process that employed univariate feature selection using information gain and a two-sample t-test for unequal variance. Additionally, polychoric correlation was used to determine the association between the selected features and the outcome variable (AF/no-AF).\n\nThe selection of these 18 parameters was crucial for ensuring that our model was both efficient and effective. By focusing on the most predictive features, we were able to reduce the dimensionality of our data, which helped to mitigate the risk of overfitting and improved the overall performance of our classifier. The chosen features encompass a range of clinical and demographic variables, including continuous measures such as left atrial diameter and heart rate at peak stress, as well as nominal variables like the presence of septal myectomy and diuretic treatment.\n\nThis approach allowed us to build a robust model that could accurately classify patients into AF and No-AF categories. The selected features were used to represent patients in our dataset, and this reduced set of features was then utilized to train our ensemble classifier, which combined logistic regression and na\u00efve Bayes methods along with oversampling and undersampling techniques to address data imbalance. The performance of our model, as evaluated by metrics such as sensitivity, specificity, and the area under the receiver operating characteristic (ROC) curve, demonstrated the effectiveness of our feature selection process.",
  "optimization/features": "In the optimization process, the input features were carefully selected to ensure the model's robustness and generalizability. Initially, a comprehensive set of 93 clinical variables was considered after removing irrelevant and directly indicative variables of adverse outcomes. To mitigate overfitting and enhance the model's performance, feature selection was performed using univariate methods. For nominal attributes, the Information Gain criterion was employed to identify highly predictive features. For continuous features, a two-sample t-test under unequal variance was used to determine statistically significant distributional differences between AF and No-AF cases. This iterative process resulted in a reduced feature set containing 18 clinical variables that were deemed informative and predictive of atrial fibrillation (AF) in hypertrophic cardiomyopathy (HCM) patients. The feature selection was conducted using the training set only, ensuring that the model's evaluation on the test set remained unbiased. This approach helped in guarding against overfitting and improved the model's ability to generalize to new, unseen data.",
  "optimization/fitting": "In our study, we addressed the challenge of high-dimensional data and potential overfitting through several key strategies. Initially, we started with a feature set consisting of 93 clinical variables. To mitigate the risk of overfitting, we employed a feature selection process to identify the most informative variables for distinguishing between AF and No-AF cases. This reduced the dimensionality of our data, ensuring that the classifiers were trained on relevant features.\n\nTo handle data imbalance, we used a combination of under-sampling and over-sampling techniques. Specifically, we performed random under-sampling of the majority class (No-AF) and over-sampled the minority class (AF) using the Synthetic Minority Over-sampling Technique (SMOTE). This approach helped in creating a balanced training set, which is crucial for training robust classifiers.\n\nWe utilized an ensemble classifier comprising logistic regression and naive Bayes, both of which demonstrated strong standalone performance. The ensemble method helps in balancing out the individual weaknesses of the base classifiers, thereby improving the overall model performance. We employed grid search to tune the hyperparameters of all classifiers, ensuring optimal performance.\n\nTo evaluate the performance of our classifiers, we used a 5-fold cross-validation scheme. This involved partitioning the dataset into five equal subsets, with four subsets used for training and one subset for testing in each iteration. This process was repeated five times, each time with a different subset left out for testing. We ran 10 complete sets of 5-fold cross-validation experiments, resulting in a total of 50 runs. The reported performance metrics, including specificity, sensitivity, and the area under the ROC curve, are averaged over these 50 runs. This rigorous validation approach helped in ensuring that our model generalizes well to unseen data and does not overfit to the training set.\n\nAdditionally, we restricted the number of synthetic minority instances generated during the over-sampling process to avoid overfitting. This careful balancing of the training set, along with the use of cross-validation, ensured that our model was neither overfitted nor underfitted. The ensemble approach further enhanced the model's robustness by combining the strengths of multiple classifiers.",
  "optimization/regularization": "To prevent overfitting, several techniques were employed. Firstly, a combination of under-sampling and over-sampling was used to balance the training set, which helps in reducing the model's tendency to overfit the majority class. Specifically, random under-sampling was applied to the majority class, and synthetic minority over-sampling technique (SMOTE) was used to generate synthetic instances for the minority class. This approach ensures that the model is trained on a balanced dataset, thereby mitigating the risk of overfitting to the majority class.\n\nAdditionally, a 5-fold cross-validation scheme was implemented. This involves partitioning the dataset into five equal subsets, where four subsets are used for training and one subset is used for testing. This process is repeated five times, each time with a different subset left out for testing. This method ensures that the model is trained and tested on different portions of the data, providing a more robust evaluation of its performance and helping to prevent overfitting.\n\nFurthermore, the ensemble classifier, which combines logistic regression and na\u00efve Bayes, was used. Ensemble methods are known for their ability to reduce overfitting by combining the strengths of multiple models. The ensemble classifier assigns a class probability to a record by calculating the weighted average of the predicted probabilities from each base classifier, thereby balancing out individual weaknesses and improving generalization.\n\nThe use of grid search for hyperparameter tuning also contributes to preventing overfitting. By systematically searching through a specified subset of hyperparameters, the optimal settings for each classifier are identified, ensuring that the models are not overly complex and are better generalized to unseen data.",
  "optimization/config": "In our study, we have provided detailed information regarding the hyper-parameter configurations for the base-classifiers employed. Specifically, for logistic regression, we used a learning rate of 0.01 and the Newton-CG optimization method. For decision trees and random forests, pruning was applied. The random forest consisted of 200 trees. For na\u00efve Bayes, no hyper-parameters were identified via grid search.\n\nThe optimization schedule involved a 5-fold cross-validation scheme. The dataset was partitioned into five equal subsets, with four subsets used for training and one subset left out for testing. This procedure was repeated five times, each time with a different subset left out for testing. To address data imbalance, we applied a combination of under- and oversampling to the training set, ensuring a balanced dataset for training the classifiers.\n\nModel files and specific optimization parameters are not explicitly detailed in this summary, as the focus was on the methodological approach and the performance evaluation metrics such as specificity, sensitivity, and the area under the ROC curve. The performance of the classifiers was evaluated on an imbalanced test set, and the results were averaged over 50 runs of 5-fold cross-validation experiments.\n\nRegarding availability and licensing, the methods and configurations described are standard practices in machine learning and are widely documented in the literature. The specific implementations and datasets used in our study would typically be subject to institutional or collaborative agreements, and their availability would depend on those terms. For detailed access to model files or specific optimization parameters, interested parties would need to contact the authors or relevant institutions directly.",
  "model/interpretability": "The model introduced, known as the HCM-AF-Risk Model, is designed to be transparent and interpretable, unlike many other machine learning models in the clinical domain that act as \"black boxes.\" This model represents each patient as a collection of interpretable clinical values, making it clear how the classification decision is made.\n\nThe classification decision is based on the probability that a given patient has atrial fibrillation (AF) or a history of AF. This probability is calculated using an ensemble of logistic regression and na\u00efve Bayes classifiers, which are both well-understood and interpretable models. The use of these classifiers allows for a clear understanding of how each clinical variable contributes to the final prediction.\n\nFor example, the model identifies 18 highly informative variables that are positively or negatively correlated with AF in hypertrophic cardiomyopathy (HCM) patients. These variables include clinical measurements such as left atrial diameter, heart rate at peak stress, age, and exercise metabolic equivalents, among others. The model's transparency allows clinicians to understand which specific factors are associated with an increased or decreased risk of AF, enabling more personalized and interpretable patient care.\n\nAdditionally, the model addresses data imbalance through a combination of oversampling and undersampling strategies, ensuring that the classification is not biased towards the more frequent class. This approach further enhances the model's interpretability by providing a balanced view of the data.\n\nIn summary, the HCM-AF-Risk Model is not a black box. It is designed to be transparent and interpretable, providing clear insights into the clinical factors that contribute to the risk of AF in HCM patients. This transparency is crucial for building trust in the model and for its practical application in clinical settings.",
  "model/output": "The model employed in our study is a classification model. It operates by taking a vector of values representing a patient's record and assigns a probability indicating the likelihood of the patient belonging to the atrial fibrillation (AF) class versus the non-AF class. This probability is calculated for each 18-dimensional vector representing individual patients. The higher the probability value, the more likely the patient is to have AF.\n\nTo address the data imbalance in our cohort, where the number of non-AF cases is significantly higher than AF cases, we utilized a combination of under- and oversampling techniques. This approach, along with an ensemble of logistic regression and na\u00efve Bayes classifiers, helps in effectively separating AF records from non-AF records.\n\nWe evaluated the performance of our model using several standard measures, including specificity, sensitivity (recall), and the area under the receiver operating characteristics (ROC) curve. These metrics provide a comprehensive assessment of the model's ability to correctly identify AF cases and non-AF cases.\n\nThe model's performance was further validated using a 5-fold cross-validation scheme. This involved partitioning the dataset into five equal subsets, with four subsets used for training and one subset left out for testing. This process was repeated five times, each time with a different subset left out for testing. This method ensures that the model is trained and tested on different portions of the data, providing a robust evaluation of its performance.\n\nThe model's output is a probability that indicates the likelihood of a patient having AF based on their clinical data. This probability can be used to assess the risk of AF in individual patients, aiding in early detection and intervention.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved a rigorous process to ensure its robustness and reliability. We employed a five-fold cross-validation scheme, where the dataset was partitioned into five equal subsets. In each iteration of the cross-validation, four subsets (80% of the data) were used for training, while the remaining subset (20%) was used for testing. This process was repeated five times, each time with a different subset left out for testing. To address data imbalance, we applied a combination of under-sampling and over-sampling techniques to the training set, ensuring a balanced dataset for training the classifiers.\n\nWe trained and tested four simple classifiers\u2014logistic regression, na\u00efve Bayes, decision tree, and random forest\u2014and an ensemble classifier that combined logistic regression and na\u00efve Bayes. The ensemble classifier used a weighted average of the predicted probabilities from the individual classifiers, with weights determined through grid search. The final probability of a record being associated with atrial fibrillation (AF) was calculated, and a record was assigned the AF class label if this probability exceeded 0.5.\n\nThe performance of the classifiers was evaluated using several standard measures: specificity, sensitivity (recall), and the area under the receiver operating characteristics (ROC) curve. Specificity was defined as the ratio of true negatives to the sum of true negatives and false positives, while sensitivity was the ratio of true positives to the sum of true positives and false negatives. The ROC curve plotted the true positive rate against the false positive rate, and the area under this curve (C-index) provided a comprehensive measure of classifier performance.\n\nTo ensure the reliability of our results, we conducted 10 complete sets of five-fold cross-validation experiments, resulting in a total of 50 runs. The performance metrics reported are averages over these 50 runs. This extensive evaluation process helped us to guard against overfitting and to obtain a reliable assessment of the classifiers' performance on imbalanced test sets.",
  "evaluation/measure": "In the evaluation of our HCM-AF-Risk Model, we utilized several standard performance metrics to comprehensively assess its effectiveness. These metrics include specificity, sensitivity (also known as recall), and the area under the receiver operating characteristics (ROC) curve, often referred to as the C-index.\n\nSpecificity measures the proportion of true negatives correctly identified by the model, which is crucial for understanding how well the model avoids false positives. It is calculated as the ratio of true negatives to the sum of true negatives and false positives.\n\nSensitivity, on the other hand, evaluates the model's ability to correctly identify true positives. It is determined by the ratio of true positives to the sum of true positives and false negatives. This metric is essential for assessing the model's performance in detecting actual cases of atrial fibrillation (AF).\n\nThe ROC curve provides a visual representation of the model's performance across different threshold levels. It plots the true positive rate (sensitivity) against the false positive rate (1-specificity). The area under the ROC curve (AUC), or C-index, summarizes the overall performance of the model, with higher values indicating better discriminative ability.\n\nThese performance metrics are widely recognized and used in the literature, making our evaluation approach representative and comparable to other studies in the field. By reporting specificity, sensitivity, and the AUC, we ensure a thorough assessment of our model's diagnostic accuracy, reliability, and overall effectiveness in identifying AF cases in patients with hypertrophic cardiomyopathy.",
  "evaluation/comparison": "In our evaluation, we compared the performance of our HCM-AF-Risk Model against several simpler baseline classifiers to assess its effectiveness. These baseline classifiers included Logistic Regression, Na\u00efve Bayes, Decision Tree, and Random Forest. Each of these classifiers was trained and tested using five-fold cross-validation, ensuring robust performance evaluation. We found that our combined approach of under-sampling and over-sampling significantly improved the performance metrics, particularly sensitivity and the area under the ROC curve (AUC), compared to the individual baseline classifiers.\n\nAdditionally, we compared our model's performance with previously published models focused on atrial fibrillation (AF) prediction in the general population, such as those from the Framingham Heart Study (FHS), Atherosclerosis Risk in Communities (ARIC), and Cohorts for Heart and Aging Research in Genomic Epidemiology Atrial Fibrillation (CHARGE-AF) studies. However, since the datasets used in these studies are not publicly available, we could not directly train or test our model on them. Instead, we compared the performance levels reported by these studies in terms of the C-index, demonstrating that our HCM-AF-Risk Model achieved significantly higher performance across all evaluation metrics, including specificity, sensitivity, and AUC.\n\nNot applicable",
  "evaluation/confidence": "The performance metrics reported in our study include confidence intervals, specifically standard deviations, which provide a measure of the variability and reliability of the results. These metrics include sensitivity, specificity, and the area under the receiver operating characteristic curve (AUC or C-index).\n\nOur method demonstrates statistically significant superiority over other approaches and baselines. For instance, the difference in performance between our feature set and other sets is highly statistically significant with a P-value of less than 0.001. Additionally, when comparing our HCM-AF-Risk Model to previous AF models, such as those from the FHS, ARIC, and CHARGE-AF studies, our model shows significantly higher performance across all evaluation metrics, again with a P-value of less than 0.001. This statistical significance underscores the robustness and effectiveness of our approach in identifying atrial fibrillation (AF) cases in patients with hypertrophic cardiomyopathy.",
  "evaluation/availability": "Not enough information is available."
}