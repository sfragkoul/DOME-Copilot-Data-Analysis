{
  "publication/title": "Machine learning-driven simplification of the hypomania checklist-32 for adolescent: a feature selection approach.",
  "publication/authors": "Shen G, Chen H, Ye X, Xue X, Tang S",
  "publication/journal": "International journal of bipolar disorders",
  "publication/year": "2024",
  "publication/pmid": "39692968",
  "publication/pmcid": "PMC11655905",
  "publication/doi": "10.1186/s40345-024-00365-4",
  "publication/tags": "- Feature selection\n- Random forest\n- Gradient boosting machine\n- Scale reduction\n- Manic symptoms\n- Adolescent psychiatry\n- Bipolar disorder\n- Diagnostic tools\n- Machine learning\n- Psychiatric assessment",
  "dataset/provenance": "The dataset used in this study was sourced from the electronic medical records of a single clinical population at the Seventh People\u2019s Hospital. The final sample consisted of 2850 participants, with a mean age of 15.50 years. The gender distribution was 68.81% female and 31.19% male. The dataset included demographic information such as gender and age, which were transformed into dummy variables for analysis. Gender was coded as 0 for male participants and 1 for female participants, while age was coded as 0 for participants aged 16 years and below, and 1 for those aged 17\u201318.\n\nThe dataset was not readily available due to the inclusion of student-specific information. Requests to access the datasets should be directed to shenguanghuipsy@163.com or yexinwu@163.com.\n\nThe dataset was used to assess manic symptoms using the Hypomania Checklist-32 (HCL-32), a self-report questionnaire designed to screen for hypomanic symptoms. The HCL-32 consists of 32 yes/no items that inquire about various aspects of mood, behavior, and cognitive functioning typically associated with hypomanic episodes. The dataset was analyzed using machine learning techniques, specifically random forest (RF) and gradient boosting machine (GBM) algorithms, to perform feature selection and identify the most predictive items from the original 32-item HCL scale. The study aimed to assess the diagnostic utility of a compressed version of the HCL-32 by employing these machine learning techniques.",
  "dataset/splits": "The dataset was split into two parts using a hold-out validation approach. This approach involved dividing the data into a training set and a testing set. The training set consisted of 70% of the total dataset, while the testing set comprised the remaining 30%. This split was designed to minimize overfitting and ensure robust model performance. The total sample size was 2850 participants. Therefore, approximately 1995 data points were used for training, and around 855 data points were reserved for testing.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The datasets used in this study are not publicly available due to the inclusion of student-specific information. To access the datasets, interested parties should direct their requests to the specified email addresses. This approach ensures that the data remains confidential and protected, adhering to ethical standards and regulations. The enforcement of this policy is managed through controlled access, where requests are reviewed and granted on a case-by-case basis to maintain the privacy and security of the participants' information.",
  "optimization/algorithm": "The optimization algorithm used in this study falls under the category of machine learning algorithms designed for feature selection. Specifically, two well-established algorithms were employed: Random Forest (RF) and Gradient Boosting Machine (GBM). These algorithms are widely recognized for their effectiveness in handling high-dimensional data and capturing complex interactions between features.\n\nThe use of these algorithms is not novel in the context of machine learning, as they have been extensively studied and applied in various fields, including psychiatry. The choice of these algorithms was driven by their proven ability to identify important features in datasets, which is crucial for developing a shortened version of the Hypomania Checklist-32 (HCL-32) tailored for adolescents.\n\nThe decision to use RF and GBM was motivated by their complementary strengths. RF is particularly adept at handling high-dimensional data and capturing non-linear relationships, while GBM excels at identifying the most influential predictors through its iterative, gradient-based approach. The consistency in feature importance rankings between these two algorithms lends credibility to the findings, suggesting that the selected items are truly the most informative for identifying manic symptoms in adolescents.\n\nThe algorithms were implemented using the \"ranger\" and \"gbm\" packages in R, which are standard tools in the machine learning community. The focus of this study was on applying these algorithms to a specific clinical problem rather than developing new machine learning techniques. Therefore, the publication in a psychiatric journal is appropriate, as the primary contribution lies in the application of existing machine learning methods to improve diagnostic tools in psychiatry.",
  "optimization/meta": "The model employed in this study does not function as a meta-predictor. Instead, it utilizes individual machine learning algorithms to identify key features for predicting manic symptoms. Specifically, two primary algorithms were used: Random Forests (RF) and Gradient Boosting Machines (GBM). These algorithms were independently applied to the dataset to determine the most important features.\n\nThe Random Forests algorithm is particularly adept at handling high-dimensional data and capturing complex interactions between features. On the other hand, Gradient Boosting Machines excel at identifying the most influential predictors through an iterative, gradient-based approach. The consistency in feature importance rankings between these two algorithms lends credibility to the findings, suggesting that the selected items are indeed the most informative for identifying manic symptoms in adolescents.\n\nThe training data for these algorithms was derived from hospital electronic medical records, ensuring that the data used was independent and not influenced by other machine-learning algorithms. This approach allows for a robust and reliable feature selection process, addressing concerns about the potential instability of feature selection in psychiatric research.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for ensuring the effectiveness of the machine-learning algorithms used in our study. Initially, the data was extracted from hospital electronic medical records, which included clinical diagnoses made by qualified psychiatrists according to DSM-V criteria. This data was then cleaned and standardized to address any inconsistencies or missing values.\n\nFor the machine-learning algorithms, specifically the random forest and gradient boosting machine (GBM) models, the data was encoded using a binary classification scheme. Each participant's responses to the HCL-32 items were converted into numerical values, where the presence of manic symptoms was coded as 1 and the absence as 0. This binary encoding facilitated the algorithms' ability to identify patterns and relationships within the data.\n\nFeature extraction was performed using random forest and GBM algorithms, with the area under the curve (AUC) serving as the primary performance metric. The importance of each feature was ranked based on its contribution to the predictive accuracy of the models. This process involved computing AUC values for models with an increasing number of features, from 1 to 32, to determine the optimal number of features for maximal predictive accuracy. The final model was selected at the point where the AUC stabilized, indicating that additional features did not significantly enhance performance.\n\nThe demographic characteristics of the participants, including gender and age, were also considered in the preprocessing stage. Chi-square tests were conducted to assess the association between these demographic variables and the presence of manic symptoms. The results showed no significant association between gender and manic symptoms, nor between age group and manic symptoms. This information was used to ensure that the models were not biased by these demographic factors.\n\nIn summary, the data encoding and preprocessing involved cleaning and standardizing the data, converting responses into binary numerical values, and using feature extraction methods to identify the most important predictors of manic symptoms. These steps were essential for optimizing the performance of the machine-learning algorithms and ensuring the reliability of the findings.",
  "optimization/parameters": "In our study, we utilized a sequential feature selection method to determine the optimal number of parameters (p) for our model. We started with a single feature and incrementally added more features up to a total of 32, which corresponds to the original number of items in the Hypomania Checklist-32 (HCL-32) scale. The area under the receiver operating characteristic curve (AUC) was computed for each model iteration to evaluate classification performance.\n\nThe AUC values indicated that the model's predictive accuracy improved as more features were included, up to a point. Specifically, the AUC began to stabilize after incorporating 8 features, reaching a value of 0.97. Beyond this point, additional features did not significantly enhance the model's performance. Therefore, we selected 8 features as the optimal number for our final model. These 8 features were identified as the most important for predicting manic symptoms based on their consistent ranking in importance across different machine learning algorithms, including random forest (RF) and gradient boosting machines (GBM).",
  "optimization/features": "The optimization process involved a sequential feature selection method to identify the most predictive features for manic symptoms. Initially, a range of features (from 1 to 32) were evaluated to determine the optimal number for maximal predictive accuracy. The area under the receiver operating characteristic curve (AUC) was used as the primary metric for evaluation.\n\nThe selection process revealed that the AUC began to stabilize after including 8 features, indicating that additional features did not significantly enhance the model\u2019s predictive performance. Therefore, 8 features were selected as the optimal number for the final model. These 8 features were identified as the most important for predicting manic symptoms.\n\nThe feature selection was performed using the training set only, ensuring that the model's performance was evaluated on unseen data. This approach helps to prevent overfitting and ensures that the selected features are generalizable to new data. The final model was then evaluated using receiver operating characteristic (ROC) curves and confusion matrices to assess its discriminative power and classification performance.\n\nThe selected features include items such as increased energy, sociability, risk-taking, creativity, flirtatiousness, talkativeness, humor, and quarrelsomeness. These items were chosen based on their importance in predicting manic symptoms, as determined by the machine learning algorithms used in the study. The consistency in feature importance rankings between different algorithms, such as random forests and gradient boosting machines, lends credibility to the selected items and suggests that they are truly the most informative for identifying manic symptoms in adolescents.",
  "optimization/fitting": "The fitting method employed in this study utilized machine learning algorithms, specifically Random Forest (RF) and Gradient Boosting Machine (GBM), to identify the most predictive items from the original 32-item Hypomania Checklist (HCL-32). The dataset consisted of 2850 participants, which is a substantial number of training points relative to the number of features being selected.\n\nTo address the potential issue of overfitting, a hold-out validation approach was used. The dataset was split into training (70%) and testing (30%) subsets. This approach helps to ensure that the model generalizes well to unseen data, as it is evaluated on a separate test set that was not used during the training process. Additionally, the importance of each item was ranked based on the mean decrease in accuracy (MDA) for RF and the Gain metric for GBM, which are robust measures of feature importance that help in selecting the most relevant features.\n\nThe sequential feature selection method was applied, with AUC values computed for models including an increasing number of features (from 1 to 32). The final model was selected based on the point where the AUC stabilized, indicating that additional features did not significantly enhance performance. This method ensures that the model is not overfitted to the training data, as it stops adding features once the performance gain diminishes.\n\nTo rule out underfitting, the study ensured that the selected features were clinically relevant and captured the core symptoms of mania in adolescents. The reduced 8-item scale demonstrated strong predictive accuracy, with an AUC of 0.91, confirming its potential utility in adolescent psychiatry. The selected items, such as \"I feel more energetic\" and \"I take more risks,\" are known to be characteristic of adolescent mania, ensuring that the model captures the essential aspects of the condition.\n\nIn summary, the fitting method effectively balanced the risk of overfitting and underfitting by using a hold-out validation approach, robust feature importance metrics, and a sequential feature selection process. The resulting model is both concise and clinically relevant, making it a practical tool for diagnosing manic symptoms in adolescents.",
  "optimization/regularization": "In our study, we employed machine learning algorithms to optimize the diagnostic utility of the HCL-32 scale. To prevent overfitting, we utilized two robust algorithms: Random Forests (RF) and Gradient Boosting Machines (GBM). These algorithms inherently include regularization techniques that help mitigate overfitting.\n\nRandom Forests operate by constructing multiple decision trees during training and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees. This ensemble method reduces the risk of overfitting by averaging the results, which smooths out the noise and focuses on the underlying patterns in the data.\n\nGradient Boosting Machines, on the other hand, build trees sequentially, where each new tree corrects the errors of the previous ones. This iterative process includes a regularization parameter that controls the complexity of the trees, preventing them from becoming too complex and overfitting the training data.\n\nAdditionally, we performed a sequential feature selection method, where we evaluated the model's performance with an increasing number of features. The area under the receiver operating characteristic curve (AUC) was used as the primary metric. We identified the optimal number of features where the AUC stabilized, indicating that additional features did not significantly enhance the model's performance. This approach ensured that our model was not overfitting to the training data by including unnecessary features.\n\nFurthermore, we conducted cross-validation to assess the model's generalizability. This technique involves splitting the data into multiple subsets, training the model on some subsets, and validating it on the remaining subsets. This process helps to ensure that the model performs well on unseen data, further reducing the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in this study are not explicitly detailed in the publication. The focus of the paper is primarily on the methodology and results of using machine learning techniques to simplify the Hypomania Checklist-32 (HCL-32) for adolescents. The study employed random forest (RF) and gradient boosting machine (GBM) algorithms for feature selection and evaluated model performance using the area under the curve (AUC).\n\nThe datasets used in this research are not readily available due to the inclusion of student-specific information. Requests to access the datasets should be directed to the specified email addresses. The study adheres to ethical standards and has been approved by the relevant institutional committees.\n\nThe supplementary materials, which may include additional details on the model configurations and optimization parameters, are available online. However, specific information about the hyper-parameter configurations and optimization schedule is not provided in the main text of the publication. For more detailed information, readers are encouraged to refer to the supplementary materials or contact the authors directly.",
  "model/interpretability": "The models employed in this study, specifically random forest (RF) and gradient boosting machines (GBM), are generally considered to be more interpretable compared to many other machine learning algorithms. Both RF and GBM provide insights into feature importance, which allows for a transparent understanding of which factors are most influential in predicting manic symptoms.\n\nRandom forests operate by constructing multiple decision trees during training and outputting the class that is the mode of the classes of the individual trees. This ensemble approach helps in reducing overfitting and improves the model's generalization ability. One of the key advantages of RF is its ability to rank features by importance, indicating which variables contribute most to the prediction. This feature importance ranking is derived from the average reduction in impurity (such as Gini impurity or entropy) across all trees in the forest. For instance, items like \"I feel more energetic and more active\" and \"I am more sociable\" were consistently identified as crucial features across both algorithms, highlighting their significance in predicting manic symptoms.\n\nGradient boosting machines, on the other hand, build trees sequentially, with each new tree aiming to correct the errors of the previous ones. GBM also provides feature importance scores, which are calculated based on the number of times a feature is used in the trees and the improvement it brings to the model. This iterative process allows GBM to identify the most influential predictors, making it a powerful tool for feature selection. The consistency in feature importance rankings between RF and GBM lends credibility to our findings, suggesting that the selected items are truly the most informative for identifying manic symptoms in adolescents.\n\nIn summary, the use of RF and GBM in this study ensures that the models are not black boxes. Instead, they offer clear insights into the most important features, making the prediction process transparent and interpretable. This transparency is crucial for clinical applications, as it allows clinicians to understand the underlying factors contributing to the predictions, thereby enhancing the trustworthiness and practical utility of the models.",
  "model/output": "The model developed in this study is a classification model. It was designed to predict the presence of manic symptoms based on specific features derived from the HCL-32 scale. The primary metric used to evaluate the model's performance was the area under the receiver operating characteristic curve (AUC), which is commonly used in classification tasks. The model's performance was assessed using metrics such as sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and accuracy, all of which are relevant to classification problems. Additionally, the model's output was visualized using confusion matrices at various cutoff points, further indicating its classification nature. The final model selected 8 key features that best predicted manic symptoms, and these features were used to calculate a total score, which was then subjected to ROC analysis to determine the optimal cutoff value for classification.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to assess the diagnostic utility of a compressed version of the Hypomania Checklist-32 (HCL-32) using machine learning techniques. The primary metric used for evaluation was the area under the receiver operating characteristic curve (AUC), which was utilized to gauge the classification performance of the selected features.\n\nA sequential feature selection method was applied, where AUC values were computed for models including an increasing number of features, ranging from 1 to 32. This process aimed to identify the optimal number of features that provided maximal predictive accuracy. The final model was selected at the point where the AUC stabilized, indicating that additional features did not significantly enhance the model's performance.\n\nReceiver operating characteristic (ROC) curves were generated to assess the discriminative power of the compressed HCL-32 scale in predicting manic symptoms. Various metrics, including AUC, sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and accuracy, were calculated for the compressed HCL-32 scale. Confusion matrices were also used to visualize classification performance at different cutoff points.\n\nThe study utilized random forest (RF) and gradient boosting machine (GBM) algorithms for feature selection, with a hold-out validation approach. This involved using 70% of the dataset for training and reserving 30% for testing to minimize overfitting and ensure robust model performance. The importance of each item was ranked based on the mean decrease in accuracy (MDA) for RF and the Gain metric for GBM.\n\nThe evaluation process also included detailed ROC analyses and confusion matrix evaluations at specific cutoff points, such as 3 and 4, to determine practical application in clinical settings. The results indicated that the optimal cutoff value was 3.50, with an AUC of 0.96, demonstrating excellent discriminative ability. However, for clinical use, integer cutoff values were considered, leading to further analysis at cut points 3 and 4.\n\nAll machine learning analyses were performed using the \"ranger\" and \"gbm\" packages in R, while ROC analyses were conducted using the \"pROC\" package. This rigorous evaluation method ensured that the compressed HCL-32 scale was thoroughly assessed for its diagnostic utility in predicting manic symptoms.",
  "evaluation/measure": "In our study, we employed several key performance metrics to evaluate the effectiveness of the compressed HCL-32 scale in predicting manic symptoms. The primary metric used was the area under the receiver operating characteristic curve (AUC), which provided a comprehensive measure of the model's discriminative power. An AUC of 0.96 indicated excellent performance, demonstrating the scale's strong ability to distinguish between participants with and without manic symptoms.\n\nIn addition to AUC, we calculated sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and accuracy. Sensitivity, or the true positive rate, measured the proportion of actual positives correctly identified by the model. Specificity, or the true negative rate, assessed the proportion of actual negatives correctly identified. PPV indicated the probability that participants with a positive test result truly had manic symptoms, while NPV represented the probability that participants with a negative test result did not have manic symptoms. Accuracy provided an overall measure of the model's correctness, balancing both sensitivity and specificity.\n\nWe also utilized confusion matrices to visualize the classification performance at various cutoff points. These matrices helped in understanding the trade-offs between sensitivity and specificity at different thresholds, providing a clear picture of the model's performance across different decision boundaries.\n\nThe set of metrics reported is representative of standard practices in the literature for evaluating diagnostic tools, particularly in the field of psychiatry. AUC is widely recognized as a robust metric for assessing the performance of binary classifiers, and the additional metrics of sensitivity, specificity, PPV, NPV, and accuracy offer a comprehensive view of the model's diagnostic capabilities. This approach ensures that our findings are comparable to other studies in the field and provides a thorough evaluation of the compressed HCL-32 scale's effectiveness.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, we focused on evaluating the performance of our machine learning algorithms, specifically Random Forests (RF) and Gradient Boosting Machines (GBM), within the context of our specific dataset and research questions.\n\nWe did, however, compare the performance of these two algorithms against each other. Both RF and GBM are robust methods for handling high-dimensional data and capturing complex interactions between features. RF is particularly adept at managing high-dimensional data, while GBM excels at identifying the most influential predictors through its iterative, gradient-based approach. The consistency in feature importance rankings between these two algorithms lends credibility to our findings and suggests that the selected items are truly the most informative for identifying manic symptoms in adolescents.\n\nRegarding simpler baselines, our approach inherently serves as a comparison to traditional item reduction methods. Traditional methods often rely on factor analysis or expert judgment, which may overlook complex non-linear relationships between symptoms. In contrast, our machine learning algorithms can identify such patterns, allowing for more precise and clinically relevant item selection.\n\nThe use of multiple algorithms and the comparison of their results provide a robust evaluation of our feature selection process. This multi-algorithm approach addresses concerns about the potential instability of feature selection in psychiatric research. However, future studies should replicate these findings using other feature selection methods to ensure the reliability of the item selection process.",
  "evaluation/confidence": "The evaluation of our model's performance was conducted using several key metrics, including the area under the receiver operating characteristic curve (AUC), sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and accuracy. These metrics were calculated for the compressed HCL-32 scale to assess its discriminative power in predicting manic symptoms.\n\nThe AUC, which serves as the primary metric, was used to evaluate the classification performance of the selected features. A sequential feature selection method was applied, with AUC values computed for models including an increasing number of features (from 1 to 32) to identify the optimal number of features for maximal predictive accuracy. The final model was selected based on the point where the AUC stabilized, indicating that additional features did not significantly enhance performance.\n\nThe results indicated that the optimal cutoff value was 3.50, with an AUC of 0.96, suggesting excellent discriminative ability. However, for practical application in clinical settings, integer cutoff values are necessary. Therefore, further ROC analysis and confusion matrix evaluations were conducted at cut points 3 and 4. These analyses provided insights into the model's performance at different threshold levels, ensuring that the selected cutoff points are clinically relevant and practical.\n\nThe statistical significance of our findings was assessed through various tests, including chi-square tests, which showed no significant association between gender and manic symptoms, as well as between age group and manic symptoms. These tests help to validate the robustness of our model and ensure that the observed results are not due to random chance.\n\nConfidence intervals for the performance metrics were not explicitly mentioned, but the stability of the AUC values across different numbers of features and the consistency in feature importance rankings between the random forest and gradient boosting machine algorithms lend credibility to our findings. The use of multiple algorithms and the iterative, gradient-based approach of the gradient boosting machine further support the reliability of the selected features.\n\nIn summary, the evaluation of our model's performance was thorough and statistically sound, with a focus on key metrics and the stability of results across different feature combinations and algorithms. The findings suggest that the compressed HCL-32 scale is a reliable and effective tool for predicting manic symptoms in adolescents.",
  "evaluation/availability": "The datasets used in this study are not readily available due to the inclusion of student-specific information. However, requests to access the datasets can be directed to shenguanghuipsy@163.com or yexinwu@163.com. This approach ensures that the data remains confidential while allowing for potential collaboration and verification of the study's findings."
}