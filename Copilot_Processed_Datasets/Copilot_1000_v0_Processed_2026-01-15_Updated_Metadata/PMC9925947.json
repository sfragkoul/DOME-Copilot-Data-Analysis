{
  "publication/title": "The applications of machine learning to predict the forming of chemically stable amorphous solid dispersions prepared by hot-melt extrusion.",
  "publication/authors": "Jiang J, Lu A, Ma X, Ouyang D, Williams RO 3rd",
  "publication/journal": "International journal of pharmaceutics: X",
  "publication/year": "2023",
  "publication/pmid": "36798832",
  "publication/pmcid": "PMC9925947",
  "publication/doi": "10.1016/j.ijpx.2023.100164",
  "publication/tags": "- Machine Learning\n- Amorphization\n- Chemical Stability\n- Hot-Melt Extrusion\n- Pharmaceuticals\n- Drug Formulation\n- Predictive Modeling\n- Molecular Representation\n- Data Mining\n- Gradient Boosting Models",
  "dataset/provenance": "The dataset used in this study was collected from published literature. It consists of 760 formulation data points for amorphization modeling and 495 data points for chemical stability modeling. The dataset includes information on various APIs and excipients, with itraconazole, felodipine, and nifedipine being among the most widely used drugs. For excipients, polyvinylpyrrolidone (PVPVA64), hypromellose acetate succinate (HPMCAS), and polyvinyl caprolactam-polyvinyl acetate-polyethylene glycol graft copolymer (Soluplus) were the most popular. The dataset also includes processing parameters such as drug loading, screw diameter, screw speed, barrel temperature, extruder configuration, and feed rate. This dataset has been used to train and evaluate machine learning models for predicting amorphization and chemical stability.",
  "dataset/splits": "The dataset was split into two primary subsets: a training subset and a testing subset. This split was done using an 80%:20% ratio. For the amorphization modeling task, 608 data points were allocated to the training subset, while 152 data points were reserved for the testing subset. Similarly, for the chemical stability modeling task, 396 data points were used for training, and 99 data points were used for testing.\n\nIn addition to the training and testing splits, a five-fold cross-validation technique was employed. This method involves dividing the data into five parts, or folds, and then training and validating the model five times, each time using a different fold as the validation set and the remaining four folds as the training set. This approach helps to ensure the model's generalization ability and to prevent overfitting.\n\nThe distribution of data points in each split reflects the imbalance observed in the original dataset. For amorphization, approximately 16.3% of the formulations were labeled as \"crystalline,\" and 83.7% were labeled as \"amorphous.\" For chemical stability, about 17.6% of the formulations were classified as \"chemically unstable,\" while 82.4% were classified as \"chemically stable.\" These distributions were maintained in both the training and testing subsets to ensure that the model's performance could be accurately evaluated across different categories.",
  "dataset/redundancy": "The datasets were split using an 80%: 20% ratio for both amorphization and chemical stability modeling tasks. This means that for amorphization, 608 data points were used for training and 152 for testing. Similarly, for chemical stability, 396 data points were used for training and 99 for testing. This split ensures that the training and test sets are independent, which is crucial for unbiased model evaluation.\n\nTo enforce the independence of the training and test sets, a five-fold cross-validation was performed. This technique helps in assessing the model's generalization ability and prevents overfitting by ensuring that the model is evaluated on different subsets of the data.\n\nThe distribution of the datasets shows an imbalance in the target categories. For amorphization, 16.3% of the formulations were crystalline, and 83.7% were amorphous. For chemical stability, 17.6% of the formulations were chemically unstable, and 82.4% were chemically stable. This imbalance is a common challenge in machine learning datasets and requires techniques such as class weights, upsampling, downsampling, and evaluation metrics like the F1 score and AUC to handle effectively.\n\nThe datasets used in this study are comparable to previously published machine learning datasets in terms of size and imbalance. The techniques employed to handle these challenges, such as cross-validation and the use of appropriate evaluation metrics, are standard practices in the field. This ensures that the models developed are robust and generalizable to new, unseen data.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the ensemble learning class, specifically gradient boosting and random forest methods. The algorithms employed include XGBoost, LightGBM, and Random Forest (RF). These are well-established algorithms in the field of machine learning and have been widely used for various predictive modeling tasks.\n\nThe algorithms used are not new; they have been extensively studied and applied in numerous research and industrial applications. The choice of these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to provide robust predictions. XGBoost and LightGBM, in particular, are known for their efficiency and performance in tasks involving large and high-dimensional data, making them suitable for the amorphization and chemical stability prediction problems addressed in this study.\n\nThe decision to use these algorithms in a pharmaceutical context, rather than publishing them in a machine-learning journal, is due to the specific application and the focus of the research. The study aims to demonstrate the practical utility of these algorithms in predicting amorphization and chemical stability in pharmaceutical formulations. The algorithms were selected based on their ability to handle the unique challenges presented by the dataset, such as imbalanced data and high dimensionality. The results highlight the effectiveness of these algorithms in pharmaceutical research, contributing to the field's knowledge and potential applications in drug development.",
  "optimization/meta": "The models discussed in this publication do not operate as meta-predictors. Instead, they are individual machine learning models trained on specific datasets. The machine learning algorithms used include 2D-XGBoost, 2D-LightGBM, ECFP-XGBoost, ECFP-LightGBM, 2D-RF, 2D-SVM, ECFP-RF, and ECFP-SVM. These models were evaluated using metrics such as accuracy (ACC), F1-score (F1), and the area under the receiver operating characteristic curve (AUC).\n\nEach model was trained and tested independently, with no indication that the output of one model was used as input for another. The training, cross-validation, and testing subsets were clearly defined, ensuring that the data used for training was independent of the data used for testing. This independence is crucial for evaluating the generalization ability and performance of each model.\n\nThe models were implemented using Scikit-Learn 1.0.1 and Python 3.9.7, with additional libraries such as Pandas, NumPy, Matplotlib, and Seaborn used for data processing and visualization. The evaluation metrics were calculated based on the confusion matrix results, providing a comprehensive assessment of each model's predictive performance.\n\nIn summary, the models discussed are standalone machine learning algorithms, each trained and evaluated independently. There is no use of meta-predictors or stacking of models in this study. The focus is on comparing the performance of different individual models to identify the best-performing algorithm for the given tasks.",
  "optimization/encoding": "To prepare the dataset for machine learning modeling, several preprocessing steps were undertaken. Initially, missing values were addressed by median substitution rather than removing entire data points, ensuring the best use of the dataset and obtaining a robust trained model.\n\nFor converting API and excipient molecules into computer-readable formats, multiple molecular representation methods were employed. These included extended-connectivity fingerprints (ECFP) and 2D molecular descriptors. Molecular descriptors, which encompass the physical and chemical properties of APIs and excipients, were generated using computational tools. Published literature has shown that ECFP-based models generally outperform those based on 2D molecular descriptors. Therefore, both methods were used for representing APIs, while excipients were represented using 2D molecular descriptors. Specifically, excipients were described by 208 2D molecular descriptors in RDKit, and APIs were described by either 208 2D molecular descriptors or ECFP fingerprints with a length of 1024 and a radius of 3.\n\nThe final dataset included formulation compositions such as drug loading and excipient ratios, drug and excipient properties via molecular descriptors, processing parameters like barrel temperature, screw speed, and feed rate, and extruder configuration details including screw diameter and L/D. This comprehensive dataset was then used for machine learning modeling.\n\nTo handle imbalanced data, techniques such as class weights, upsampling, downsampling, and evaluation metrics like the F1 score, receiver operating characteristic, and accuracy were considered. The dataset was split into training and testing subsets using an 80%:20% ratio, with five-fold cross-validation performed to ensure model generalization and prevent overfitting. Missing values, predominantly in extruder configuration and processing parameters, were fixed before modeling.",
  "optimization/parameters": "In our study, the number of input parameters (p) used in the models varied depending on the molecular representation method of the active pharmaceutical ingredient (API). For models using 2D-descriptors, a set of specific 2D parameters were utilized. In contrast, models employing Extended-Connectivity Fingerprints (ECFP) used a different set of parameters derived from the API's substructures.\n\nThe selection of these parameters was guided by their relevance to the processes being modeled, specifically amorphization and chemical stability during hot-melt extrusion (HME). Critical processing parameters included barrel temperature, feed rate, screw speed, and screw diameter. Additionally, material attributes such as the loading of the first dominant excipient, drug loading, and properties of the excipients and drugs represented by 2D descriptors and ECFP were considered.\n\nThe importance of these parameters was further validated through feature importance analysis using SHapley Additive exPlanations (SHAP) and information gain (IG). These analyses helped identify the most influential features contributing to the model's predictions. For instance, barrel temperature and the ratio of the first dominant excipient were found to be among the top important features in both amorphization and chemical stability models.\n\nIn summary, the selection of input parameters was driven by their known significance in the HME process and confirmed through rigorous feature importance analysis. This approach ensured that the models were trained on the most relevant and impactful parameters, enhancing their predictive performance.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "In our study, we employed multiple machine learning algorithms, including XGBoost, LightGBM, Random Forest (RF), and Support Vector Machine (SVM), to predict amorphization and chemical stability. The dataset used for training these models consisted of 760 formulations for amorphization and 495 for chemical stability. Given the complexity of the molecular representations and the high dimensionality of the features, particularly when using extended-connectivity fingerprints (ECFP) for APIs, the number of parameters in our models was indeed larger than the number of training points.\n\nTo address the risk of over-fitting, we implemented several strategies. Firstly, we used an 80%:20% train-test split to ensure that the model's performance was evaluated on unseen data. Additionally, we performed five-fold cross-validation to further validate the model's generalization ability. This technique helps in assessing how the model performs on different subsets of the data, reducing the likelihood of over-fitting to the training set.\n\nMoreover, we fine-tuned the hyperparameters of our models using both grid search and random search methods. This process involved systematically exploring different combinations of hyperparameters to find the optimal settings that minimized over-fitting while maximizing model performance. The hyperparameters for each algorithm were carefully selected and listed in a table, ensuring that the models were not overly complex.\n\nTo rule out under-fitting, we ensured that our models were sufficiently complex to capture the underlying patterns in the data. The use of advanced algorithms like XGBoost and LightGBM, which are known for their robustness and ability to handle high-dimensional data, helped in this regard. These algorithms employ gradient boosting techniques, which iteratively add weak classifiers to minimize the loss, leading to a robust classifier through gradient optimization.\n\nFurthermore, we addressed the issue of imbalanced data by using class weights and evaluating the models using metrics such as the F1 score, receiver operating characteristic (ROC) area under the curve (AUC), and accuracy. This approach ensured that the models were not biased towards the majority class and could effectively predict both amorphization and chemical stability.\n\nIn summary, by employing cross-validation, hyperparameter tuning, and advanced machine learning algorithms, we mitigated the risks of both over-fitting and under-fitting, ensuring that our models were robust and generalizable.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was gradient boosting, specifically with XGBoost and LightGBM. These algorithms inherently help in preventing overfitting by giving more importance to misclassified categories and minimizing loss through the addition of weak classifiers using gradient descent. This process leads to the creation of a robust classifier that generalizes well to new data.\n\nAdditionally, we utilized five-fold cross-validation. This technique involves splitting the data into five subsets, training the model on four of them, and validating it on the remaining one. This process is repeated five times, with each subset serving as the validation set once. Cross-validation helps in assessing the model's performance more reliably and in reducing the risk of overfitting by ensuring that the model is evaluated on different portions of the data.\n\nWe also addressed the issue of imbalanced data, which can lead to overfitting. Techniques such as class weights, upsampling, and downsampling were considered to handle the imbalance. Furthermore, we used evaluation metrics like the F1 score and AUC, which are more informative for imbalanced datasets compared to accuracy alone.\n\nHyperparameter tuning was performed using grid search and random search methods. This process helps in finding the optimal set of hyperparameters that minimize overfitting and improve the model's generalization ability. The hyperparameters of XGBoost, LightGBM, Random Forest, and SVM were finely tuned to achieve the best performance.\n\nIn summary, our approach to preventing overfitting involved the use of gradient boosting algorithms, cross-validation, handling imbalanced data, and thorough hyperparameter tuning. These methods collectively contributed to the development of robust and generalizable models.",
  "optimization/config": "The hyperparameter configurations for the machine learning models used in this study are reported in detail. These configurations are provided in a table format, specifying the settings for different algorithms such as XGBoost, LightGBM, Random Forest (RF), and Support Vector Machine (SVM). The table includes the hyperparameters for both amorphization and chemical stability models, with different molecular representation methods (2D-descriptors and ECFP).\n\nThe optimization schedule involved both grid search and random search methods to fine-tune the hyperparameters of the models. This process ensured that the models were optimized for the best performance.\n\nRegarding the availability of model files and optimization parameters, the specific files and parameters used in the study are not explicitly mentioned as being publicly available. However, the implementation of the machine learning models was conducted using Scikit-Learn 1.0.1, and the programming language used was Python 3.9.7. Additional open-source packages such as Pandas 1.3.4 and NumPy 1.23.0 were used for data processing, while Matplotlib 3.5.0 and Seaborn 0.11.2 were used for visualization.\n\nThe study does not provide information on the licensing terms for the datasets or the code used. Therefore, it is not clear whether the datasets, model files, or optimization parameters are available under specific licenses.",
  "model/interpretability": "The models employed in this study are not entirely black-box, as efforts were made to enhance their interpretability. To achieve this, two main techniques were utilized: SHapley Additive exPlanations (SHAP) and information gain (IG).\n\nSHAP was used to provide local interpretability, which means it explains individual predictions made by the models. This technique helps to understand how each feature contributes to a specific prediction. For instance, in the ECFP-LightGBM model, SHAP analysis revealed that features such as barrel temperature, excipient loading, and screw speed had positive correlations with the model's output, indicating that higher values of these features increased the likelihood of amorphization. Conversely, features like drug loading and certain drug substructures had negative correlations, suggesting that higher values of these features decreased the likelihood of amorphization.\n\nIn addition to SHAP, information gain (IG) was employed to assess global feature importance. IG measures the reduction in entropy or uncertainty by including a feature in the model. This method helps to identify the most critical features that influence the model's predictions across the entire dataset. For example, the top 12 most important substructures of the active pharmaceutical ingredient (API) were analyzed based on their IG values. This analysis provided insights into which structural features of the API were most relevant to the model's predictions.\n\nFurthermore, the study highlighted the importance of specific processing parameters and material attributes in the models' predictions. For instance, barrel temperature and excipient loading were identified as key features in both SHAP and IG analyses, underscoring their significance in the amorphization process.\n\nBy using SHAP for local explanations and IG for global feature importance, the models' decision-making processes were made more transparent. This approach allows for a better understanding of how different features influence the models' predictions, moving away from treating the models as black-box systems.",
  "model/output": "The models developed in this study are classification models. They are designed to predict two specific outcomes related to pharmaceutical processes: amorphization and chemical stability. For amorphization, the models predict whether a crystalline drug will become amorphous during the hot melt extrusion (HME) process. For chemical stability, the models predict the stability of the drug formulations. The performance of these models is evaluated using metrics such as accuracy (ACC), F1 score (F1), and the area under the receiver operating characteristic curve (AUC), which are typical for classification tasks.\n\nThe models utilize various machine learning algorithms, including XGBoost, LightGBM, Random Forest (RF), and Support Vector Machine (SVM), with different molecular representations of the active pharmaceutical ingredient (API), such as 2D descriptors and extended-connectivity fingerprints (ECFP). The best-performing models for amorphization and chemical stability are ECFP-LightGBM and ECFP-XGBoost, respectively. These models demonstrate high accuracy, F1 scores, and AUC values, indicating their effectiveness in classifying the outcomes of interest.\n\nThe interpretability of the models is enhanced using SHapley Additive exPlanations (SHAP) for local explanations and information gain (IG) for global feature importance analysis. This allows for a better understanding of the factors contributing to the model's predictions, such as the importance of excipient ratios and barrel temperature in the amorphization process. The top 12 important substructures of the API are also analyzed based on IG values, providing insights into the structural features that influence the model's output.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the machine learning models involved several rigorous methods to ensure their predictive performance and generalization ability. We employed an 80%:20% train-test split for both amorphization and chemical stability modeling tasks. This split allowed us to use the majority of the data for training the models and a separate portion for unbiased evaluation. Additionally, five-fold cross-validation was performed to further validate the models' performance and prevent overfitting.\n\nFor assessing the models' predictive performance, we utilized multiple evaluation metrics, including accuracy (ACC), F1 score (F1), and the area under the receiver operating characteristic curve (AUC). These metrics were calculated based on the confusion matrix results, providing a comprehensive view of the models' effectiveness. The F1 score and AUC were particularly important given the imbalanced nature of the datasets, ensuring that the models could handle both majority and minority classes effectively.\n\nIn the cross-validation sets, models such as 2D-XGBoost, 2D-LightGBM, ECFP-XGBoost, and ECFP-LightGBM demonstrated similar high performance with ACC, F1, and AUC values exceeding 0.91, approximately 0.95, and approximately 0.92, respectively. However, 2D-SVM showed relatively poorer performance with the lowest metrics in the cross-validation set.\n\nIn the testing subsets, LightGBM-based models, particularly ECFP-LightGBM, outperformed others, achieving the highest ACC, F1, and AUC values. This model was selected for further study due to its excellent generalization ability and lower variance when fed new formulation data.\n\nFor the chemical stability prediction, ECFP-XGBoost exhibited the highest ACC, F1, and AUC in the testing subsets, making it the chosen model for further analysis. Other models like SVM and LightGBM showed good ACC and F1 values but relatively lower AUC, indicating their limitations in handling imbalanced datasets.\n\nOverall, the evaluation methods ensured that the selected models were robust, reliable, and capable of generalizing well to new data, making them suitable for predicting amorphization and chemical stability in pharmaceutical formulations.",
  "evaluation/measure": "In our study, we employed multiple performance metrics to thoroughly evaluate the predictive capabilities of our machine learning models. The primary metrics reported are accuracy (ACC), F1 score (F1), and the area under the receiver operating characteristic curve (AUC). These metrics were chosen for their robustness and widespread use in the literature, ensuring that our evaluation is both comprehensive and comparable to other studies in the field.\n\nAccuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. It provides a general sense of how often the model is correct. The F1 score, on the other hand, is the harmonic mean of precision and recall, offering a balance between these two metrics. It is particularly useful when dealing with imbalanced datasets, as it considers both false positives and false negatives. The AUC, derived from the ROC curve, evaluates the model's ability to distinguish between classes across all possible classification thresholds. A higher AUC indicates better model performance, especially in scenarios with imbalanced data.\n\nThese metrics were calculated using the confusion matrix, which provides a detailed breakdown of true positives, true negatives, false positives, and false negatives. This approach allows for a nuanced understanding of model performance beyond simple accuracy measures.\n\nIn addition to these standard metrics, we also utilized SHapley Additive exPlanations (SHAP) for local interpretability and information gain (IG) for global feature importance analysis. SHAP helps in understanding the contribution of each feature to individual predictions, while IG measures the reduction in entropy by transforming the dataset, highlighting the most influential features.\n\nThe choice of these metrics is representative of current best practices in machine learning evaluation. They provide a holistic view of model performance, covering aspects of classification accuracy, balance between precision and recall, and the model's discriminative power. This set of metrics ensures that our models are not only accurate but also reliable and interpretable, aligning with the standards set by the scientific community.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of various machine learning models to evaluate their performance in predicting amorphization and chemical stability. We employed multiple algorithms, including XGBoost, LightGBM, Random Forest (RF), and Support Vector Machine (SVM), each with two different molecular representation methods: 2D descriptors and Extended-Connectivity Fingerprints (ECFP). This approach allowed us to assess the effectiveness of different modeling techniques and molecular representations.\n\nFor amorphization prediction, we found that models based on LightGBM, particularly ECFP-LightGBM, demonstrated superior performance. This model achieved the highest accuracy, F1 score, and AUC in both cross-validation and testing subsets, indicating its robustness and generalization ability. In contrast, the 2D-SVM model showed relatively poor performance across all metrics.\n\nIn the context of chemical stability prediction, ECFP-XGBoost emerged as the top-performing model. It exhibited the highest accuracy, F1 score, and AUC in the testing subset, outperforming other models, including those based on RF and SVM. The RF-based models, in particular, struggled with imbalanced datasets and sparse data, leading to lower performance metrics.\n\nOur comparison also highlighted the general superiority of ECFP-based models over 2D-based models, especially in the testing set. This finding aligns with previous research that has shown ECFP-based models to be more effective in capturing relevant molecular features.\n\nIn summary, our evaluation involved a thorough comparison of different machine learning algorithms and molecular representation methods. The results underscored the strengths of LightGBM and XGBoost models, particularly when combined with ECFP representations, in predicting amorphization and chemical stability. This comprehensive analysis provides valuable insights into the optimal modeling approaches for these predictive tasks.",
  "evaluation/confidence": "The evaluation of our models focused on several key metrics, including accuracy (ACC), F1 score (F1), and the area under the receiver operating characteristic curve (AUC). These metrics were chosen to provide a comprehensive assessment of the models' predictive performance. However, it is important to note that confidence intervals for these performance metrics were not explicitly reported in our study.\n\nStatistical significance was not directly addressed in the context of comparing our models to baselines or other methods. While the models demonstrated strong performance, particularly the ECFP-LightGBM for amorphization prediction and ECFP-XGBoost for degradation prediction, the statistical significance of these results in comparison to other models or baselines was not explicitly evaluated. This means that while the models showed high metrics values, the claim of superiority over other methods or baselines lacks statistical backing.\n\nIn future work, it would be beneficial to include confidence intervals for the performance metrics and conduct statistical tests to determine the significance of the differences observed between the models. This would provide a more robust evaluation of the models' performance and their superiority over other methods.",
  "evaluation/availability": "Not enough information is available."
}