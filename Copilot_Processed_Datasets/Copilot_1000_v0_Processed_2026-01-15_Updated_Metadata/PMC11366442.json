{
  "publication/title": "LDCNN: A new arrhythmia detection technique with ECG signals using a linear deep convolutional neural network.",
  "publication/authors": "Bayani A, Kargar M",
  "publication/journal": "Physiological reports",
  "publication/year": "2024",
  "publication/pmid": "39218586",
  "publication/pmcid": "PMC11366442",
  "publication/doi": "10.14814/phy2.16182",
  "publication/tags": "- Arrhythmia detection\n- ECG signal analysis\n- Deep learning\n- Convolutional neural networks\n- Cardiac disease diagnosis\n- Machine learning\n- MIT-BIH Arrhythmia dataset\n- PTB Diagnostic ECG dataset\n- Model evaluation metrics\n- Performance comparison",
  "dataset/provenance": "In our study, we utilized two widely recognized benchmark datasets for evaluating our proposed method: the PTB Diagnostic ECG dataset and the MIT-BIH Arrhythmia dataset.\n\nThe PTB Diagnostic ECG dataset is accessible through the DOI: https://doi.org/10.13026/C28C71. This dataset includes 549 records from 290 individuals, with each record containing 15 simultaneously measured signals, comprising 12 leads. The dataset is categorized into two main classes: normal and abnormal beats. The abnormal beats encompass various conditions such as cardiomyopathy, heart failure, myocarditis, myocardial infarction, bundle branch block, dysrhythmia, valvular heart disease, and myocardial hypertrophy. Additionally, the annotated PTB dataset is available on Kaggle at https://www.kaggle.com/datasets/shayanfazeli/heartbeat.\n\nThe MIT-BIH Arrhythmia dataset is a standard dataset for evaluating arrhythmia diseases. It consists of 48 dual-channel ECG recordings, each lasting 30 minutes, collected from 47 patients over four years. The dataset includes five categories: normal beat, ventricular premature contraction, supraventricular premature beat, combined ventricular and normal beat, and unclassifiable beat. For our study, we selected a subset of these arrhythmias based on their clinical relevance and prevalence in real-world scenarios. This subset focuses on representative arrhythmias to ensure a robust evaluation of our proposed methodology.\n\nBoth datasets have been extensively used in previous research and by the community for the development and validation of arrhythmia detection and classification algorithms. The PTB Diagnostic ECG dataset and the MIT-BIH Arrhythmia dataset provide a comprehensive and diverse set of ECG signals, enabling a thorough assessment of our proposed linear deep convolutional neural network (LDCNN) model.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a testing set. The testing set comprises 20% of the total data, while the training set contains the remaining 80%. This split is crucial for evaluating the model's performance and ensuring its effectiveness on new, unseen data. The specific distribution of data points within each class before and after resampling can be visualized in relevant figures, which illustrate the balance achieved through resampling techniques. This approach helps in mitigating issues related to class imbalance, thereby enhancing the model's robustness and accuracy.",
  "dataset/redundancy": "The datasets used in our study were carefully split to ensure independent training and testing sets. After applying resampling techniques to balance the class distribution, the data was divided into two sets: a training set and a testing set. The testing set was allocated 20% of the total data, while the remaining 80% was used for training. This split is crucial for evaluating the model's performance and ensuring that it generalizes well to new, unseen data.\n\nTo enforce the independence of the training and test sets, we ensured that there was no overlap between the samples used in each set. This was achieved through a systematic data segmentation strategy, which is a significant step in preparing datasets for machine learning models. By maintaining independent sets, we could accurately assess the model's ability to perform on new data, thereby validating its robustness and reliability.\n\nThe distribution of the datasets used in our study compares favorably to previously published machine learning datasets in the field of ECG signal analysis. The resampling technique employed helped to address the common issue of uneven class distribution, which is prevalent in many ECG datasets. This approach ensures that the model is trained on a balanced dataset, leading to improved performance and accuracy. The results demonstrate that our method achieves high accuracy rates on both the MIT-BIH Arrhythmia and PTB Diagnostic ECG datasets, indicating its effectiveness in handling class-specific irregularities and enhancing the overall accuracy of ECG signal analysis.",
  "dataset/availability": "The datasets utilized in this study are publicly accessible. Dataset 1, referred to as the MIT-BIH Arrhythmia dataset, is a standard dataset widely used for evaluating arrhythmia detection methods. It includes 48 dual-channel ECG recordings, each lasting 30 minutes, collected from 47 patients over four years. This dataset comprises five categories of heartbeats: normal beat, ventricular premature contraction, supraventricular premature beat, combined ventricular and normal beat, and unclassifiable beat. For our study, a subset of these arrhythmias was selected based on clinical relevance and prevalence.\n\nDataset 2, known as the PTB Diagnostic ECG dataset, is also publicly available. This dataset can be accessed through a specific DOI, ensuring that researchers can easily obtain it for further studies. Additionally, the annotated PTB dataset is available on Kaggle, providing another accessible platform for data retrieval.\n\nThe data splits used for training and testing were created by separating the datasets into two sets: 80% for training and 20% for testing. This split was enforced to evaluate the performance of the models and ensure their generalizability to new data. The training set was used to train the models, while the testing set was used to assess their accuracy and robustness.\n\nThe code, including deep learning and machine learning models, preprocessing scripts, and architectures, is available on GitHub. This repository provides the necessary tools and scripts to replicate the study's findings and further explore the methodologies employed. The availability of the datasets and code ensures transparency and reproducibility in research.",
  "optimization/algorithm": "The optimization algorithm employed in our study is based on a deep convolutional neural network (DCNN) classifier, specifically enhanced by the BaROA optimization algorithm. This approach is not entirely new, as it builds upon existing DCNN architectures but integrates a novel optimization technique.\n\nThe BaROA algorithm combines elements of Bat Algorithm (BA) and Ray Optimization Algorithm (ROA) to optimize the weights of the DCNN. This integration aims to achieve a globally optimal solution, improving the accuracy and efficiency of feature extraction and classification.\n\nThe reason this algorithm was not published in a machine-learning journal is that the primary focus of our research is on its application in arrhythmia detection using ECG signals. The innovation lies in the specific use case and the integration of the optimization algorithm within the context of cardiac disease diagnosis, rather than the algorithm itself. This application-driven approach is more aligned with biomedical engineering and healthcare research, which is the focus of our publication.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data encoding was a crucial preprocessing step to ensure that our machine learning algorithms could effectively process the input data. We employed the One-Hot Encoding method to convert categorical data into a numerical format. This technique is particularly useful when dealing with categorical variables that do not have a natural numeric representation.\n\nFor instance, in the MIT-BIH Arrhythmia dataset, we had a variable called \"arrhythmia\" with values such as \"Normal beat,\" \"Right bundle branch block beat,\" \"Left bundle branch block beat,\" \"Atrial premature beat,\" and \"Premature ventricular contraction.\" Each of these categories was converted into a separate binary column. For a given sample, a value of 1 was assigned to the column corresponding to the sample's category, while all other columns were set to 0. This approach ensures that the categorical data is transformed into a format that machine learning algorithms can easily interpret and process.\n\nThis encoding method is essential because machine learning algorithms typically require numerical inputs. By converting categorical data into binary vectors, we enable the algorithms to handle and learn from the data more effectively, ultimately improving the performance and accuracy of our models.",
  "optimization/parameters": "In our proposed model, the number of parameters (p) varies depending on the dataset and the specific architecture used. For the PTB Diagnostic ECG dataset, the model consists of eight convolutional layers, each followed by a pooling layer, and additional dense layers. For the MIT-BIH Arrhythmia dataset, the model includes five convolutional layers, each paired with a pooling layer, along with dense layers.\n\nThe selection of these parameters was guided by extensive experimentation and validation. We configured the model with diverse hyperparameters to optimize performance. For instance, the PTB model utilizes convolutional layers with varying filter sizes and activation functions, such as ReLU, to capture intricate patterns in the ECG signals. Similarly, the MIT-BIH model employs convolutional layers with different kernel sizes and activation functions to extract relevant features from the input data.\n\nThe choice of parameters was also influenced by the need to balance model complexity and computational efficiency. We ensured that the model could generalize well to unseen data by incorporating dropout layers to prevent overfitting. The dropout rates were carefully selected to maintain a good trade-off between training and testing performance.\n\nIn summary, the number of parameters in our model is determined by the specific architecture tailored to each dataset, and the selection process involved rigorous testing and validation to achieve optimal performance.",
  "optimization/features": "The input features for our model consist of a one-dimensional vector representing ECG heartbeat signals. The input shape is 1 \u00d7 187, indicating that the input feature vector has a length of 187.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the model leverages the power of convolutional layers to automatically extract relevant features from the raw ECG signals. This approach allows the model to identify important patterns and features directly from the input data, making it robust and adaptable to the complexities of ECG signals.\n\nThe feature extraction process is integrated into the training pipeline, ensuring that the model learns and selects the most relevant features directly from the training data. This method avoids the need for manual feature selection and relies on the model's ability to generalize from the training set to the test set.",
  "optimization/fitting": "The fitting method employed in our study involved training a linear deep convolutional neural network (LDCNN) on two datasets: the PTB Diagnostic ECG dataset and the MIT-BIH Arrhythmia dataset. The model was implemented in Python and trained on both Google Colab and a local machine with an AMD Ryzen 5 5500U processor.\n\nRegarding the number of parameters, the LDCNN architecture is designed with multiple convolutional and pooling layers, which indeed results in a large number of parameters. However, the number of training points in both datasets is substantial, which helps mitigate the risk of overfitting. To further address overfitting, we utilized dropout layers with varying rates, which randomly set a fraction of input units to zero during training. This technique helps prevent the model from becoming too reliant on specific features and improves generalization.\n\nAdditionally, we monitored the model's performance on a separate test set during training. The accuracy and loss trends for both the training and test sets were closely observed over multiple epochs. For the PTB dataset, the model was trained for 30 epochs, and for the MIT-BIH dataset, it was trained for 80 epochs. The consistent performance on the test set, without significant degradation, indicates that overfitting was effectively managed.\n\nTo rule out underfitting, we ensured that the model had sufficient capacity to learn the underlying patterns in the data. The architecture of the LDCNN, with its multiple convolutional layers and fully connected layers, provides the necessary complexity to capture intricate features in the ECG signals. Furthermore, the use of appropriate activation functions, such as ReLU, and the optimization of hyperparameters contributed to the model's ability to learn effectively from the data.\n\nThe results demonstrate that our proposed method achieves high accuracy rates of 99.24% on the PTB dataset and 99.38% on the MIT-BIH dataset, indicating that the model is neither overfitting nor underfitting. The evaluation metrics, including F1 scores, recall, and precision, further support the robustness of our model's performance.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and enhance the generalization of our model. One of the key methods used was dropout. Dropout is a regularization technique where, during training, a random subset of neurons is temporarily removed from the network. This forces the network to learn redundant representations and prevents it from becoming too reliant on any single neuron, thereby reducing overfitting. In our architecture, dropout layers were strategically placed after certain convolutional and pooling layers. For instance, dropout rates of 0.1 were applied after the first, second, and third dropout layers, and a dropout rate of 0.2 was used after the fourth dropout layer. This helped in regularizing the model and improving its performance on unseen data.\n\nAdditionally, we utilized data augmentation techniques to increase the diversity of our training dataset. By applying transformations such as noise addition and signal shifting, we created variations of the original ECG signals. This approach helped the model to generalize better and reduced the risk of overfitting to the specific patterns present in the training data.\n\nFurthermore, we monitored the model's performance on a separate validation set during training. This allowed us to tune hyperparameters and stop training when the performance on the validation set started to degrade, indicating the onset of overfitting. By carefully managing the training process and employing these regularization techniques, we were able to achieve robust and reliable performance on both the PTB Diagnostic ECG and MIT-BIH Arrhythmia datasets.",
  "optimization/config": "The hyper-parameter configurations for our proposed LDCNN model are thoroughly detailed for both the MIT-BIH Arrhythmia and PTB Diagnostic ECG datasets. These configurations include specific parameters such as the number of filters, kernel sizes, activation functions, input and output shapes, dropout rates, and pooling sizes. This information is presented in tables within the publication, providing a clear and comprehensive overview of the model's architecture and settings.\n\nThe optimization schedule and parameters used during the training process are also discussed. This includes the number of epochs, the trends in accuracy and loss over these epochs, and the evaluation metrics used to assess the model's performance. The results demonstrate the model's ability to achieve high accuracy rates on both datasets without overfitting.\n\nIn addition to the hyper-parameter configurations and optimization details, the code used in this study is available on GitHub. This repository includes the deep learning and machine learning models, preprocessing scripts, and architectures implemented in Python. The availability of the code allows for reproducibility and further exploration by other researchers.\n\nThe datasets used in this study are accessible through provided DOIs, ensuring that others can replicate the experiments and validate the results. The PTB Diagnostic ECG dataset is also available on Kaggle, making it easily accessible for further research and development.\n\nThe publication adheres to open science practices by providing detailed information on the model configurations, optimization parameters, and making the code and datasets publicly available. This transparency supports the reproducibility of the research and encourages further advancements in the field.",
  "model/interpretability": "The model we proposed, the Linear Deep Convolutional Neural Network (LDCNN), is primarily a black-box model. This means that while it excels in accurately classifying cardiac diseases from ECG signals, the internal decision-making process is not easily interpretable. The LDCNN processes ECG waveforms through multiple convolutional and pooling layers, extracting complex features that are not straightforward to interpret.\n\nHowever, the model's design allows it to capture and process nuanced waveform features associated with various cardiac conditions. This capability enables the identification of novel markers of disease within ECG waveforms, providing insights into unique patterns that may not be immediately apparent. While the model itself does not provide transparent explanations for its predictions, the patterns it identifies can be analyzed further to understand the underlying cardiac conditions better.\n\nThe LDCNN's strength lies in its ability to be employed across different datasets, offering a versatile solution for cardiac disease diagnosis. Its adaptability and high performance make it a valuable tool for medical professionals, even if the internal workings are not fully transparent. The model's outputs can be used to inform clinical decisions, and further research can focus on interpreting the features it extracts to enhance our understanding of cardiac diseases.",
  "model/output": "The model is a classification model. It is designed to differentiate between disease types based on subtle patterns identified in ECG signals. The output layer uses a sigmoid activation function, which is typically used for binary classification tasks. However, the model's architecture and the use of one-hot encoding for more than two arrhythmia classes suggest that it can handle multi-class classification as well. The model's performance is evaluated using metrics such as accuracy, precision, recall, and F1 score, which are commonly used for classification tasks. The final output of the model is a set of predicted arrhythmia labels for the test set, along with calculated performance metrics.",
  "model/duration": "The model was implemented in Python and tested on both Google Colab and a local machine equipped with an AMD Ryzen 5 5500U processor with Radeon Graphics, running at 2.10 GHz and 8 GB of RAM. The execution time varied depending on the dataset and the number of epochs. For the PTB dataset, the model was trained and tested over 30 epochs, while for the MIT-BIH Arrhythmia dataset, the process extended to 80 epochs. The specific execution time for each epoch was not detailed, but the overall performance was evaluated based on accuracy and loss trends observed during these epochs. The model demonstrated high accuracy rates of 99.24% for the PTB dataset and 99.38% for the MIT-BIH Arrhythmia dataset, indicating efficient training and testing phases.",
  "model/availability": "The source code for the models and algorithms presented in this study is publicly available. This includes deep learning and machine learning models, as well as preprocessing scripts and architectures implemented in Python. The code can be accessed via a GitHub repository. This repository provides a comprehensive set of tools and scripts necessary to replicate the experiments and results discussed in the publication. The availability of this code ensures transparency and reproducibility, allowing other researchers to build upon the work and apply the methods to their own datasets. The repository is open-source, facilitating collaboration and further development in the field of cardiac disease diagnosis using ECG signals.",
  "evaluation/method": "The evaluation of our proposed method, the Linear Deep Convolutional Neural Network (LDCNN), was conducted using two well-known datasets: the MIT-BIH Arrhythmia dataset and the PTB Diagnostic ECG dataset. The model was implemented in Python and tested on both Google Colab and a local machine with an AMD Ryzen 5 5500U processor and 8 GB RAM.\n\nFor the PTB dataset, the model was trained and tested over 30 epochs, with accuracy and loss trends monitored for both the training and test sets. Similarly, for the MIT-BIH Arrhythmia dataset, the model underwent training and testing over 80 epochs, with similar monitoring of accuracy and loss. These evaluations helped assess the model's performance improvement during training and its ability to maintain performance on the test set.\n\nThe evaluation metrics used included accuracy, precision, recall, and F1 score. Accuracy measured the overall correctness of the model, precision gauged the reliability of positive predictions, recall assessed the model's ability to detect positive instances, and the F1 score balanced precision and recall to provide a single value for overall performance evaluation.\n\nIn addition to these metrics, the model's performance was compared against various machine learning techniques and contemporary methods. Tables summarizing the results showed that our proposed LDCNN outperformed traditional methods and other modern techniques in terms of accuracy, precision, recall, and F1 score on both datasets. This comprehensive evaluation demonstrated the robustness and effectiveness of our LDCNN in arrhythmia detection and cardiac disease diagnosis.",
  "evaluation/measure": "In our evaluation, we employed a comprehensive set of performance metrics to assess the efficacy of our proposed model. The primary metrics reported include accuracy, precision, recall, and F1 score. These metrics are widely recognized and used in the literature for evaluating classification models, particularly in the context of medical diagnostics.\n\nAccuracy measures the overall correctness of the model by calculating the proportion of correctly identified samples out of the total samples. It provides a general sense of the model's performance but can be misleading if the dataset is imbalanced.\n\nPrecision focuses on the reliability of positive predictions, indicating the proportion of true positives among all samples classified as positive. This metric is crucial when the cost of false positives is high.\n\nRecall, also known as sensitivity, assesses the model's ability to detect positive instances by calculating the proportion of true positives among all actual positive samples. It is particularly important when the cost of false negatives is high.\n\nThe F1 score is a harmonic mean of precision and recall, providing a single value that balances both metrics. It is especially useful when there is an uneven class distribution.\n\nThese metrics collectively offer a robust evaluation framework, ensuring that our model's performance is assessed from multiple angles. The choice of these metrics aligns with standard practices in the field, making our results comparable to other studies in the literature. This comprehensive approach allows us to demonstrate the strengths and reliability of our proposed model in detecting arrhythmias.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our proposed Linear Deep Convolutional Neural Network (LDCNN) model against both publicly available methods and simpler baselines on benchmark datasets. For the publicly available methods, we utilized two widely recognized datasets: the MIT-BIH Arrhythmia dataset and the PTB Diagnostic ECG dataset. These datasets are standard benchmarks in the field of cardiac arrhythmia detection, ensuring that our comparisons are both relevant and rigorous.\n\nWe evaluated our model against a range of contemporary techniques, including various machine learning algorithms and other deep learning approaches. For instance, we compared our LDCNN with methods such as Gaussian Naive Bayes, Logistic Regression, Decision Trees, Linear Support Vector Machine, Support Vector Machine, K-Nearest Neighbors, and Random Forest. These comparisons were performed using multiple evaluation metrics, including accuracy, precision, recall, and F1 score, to provide a comprehensive assessment of our model's performance.\n\nAdditionally, we benchmarked our LDCNN against other state-of-the-art techniques reported in recent literature. This included methods proposed by researchers such as Atal & Singh, Sharma et al., Farag, Kumar et al., and Fradi et al. The results consistently showed that our LDCNN outperformed these methods across all evaluated metrics, demonstrating its superior diagnostic effectiveness.\n\nFurthermore, we implemented and evaluated simpler baselines to ensure that our model's performance was not merely due to the complexity of the architecture but rather due to its inherent strengths. These baselines included traditional machine learning algorithms, which provided a foundational comparison point. The results indicated that our LDCNN not only matched but significantly surpassed the performance of these simpler models, highlighting its robustness and efficiency in arrhythmia detection.\n\nIn summary, our evaluation involved a comprehensive comparison with both publicly available methods and simpler baselines on benchmark datasets, ensuring that our findings are reliable and reproducible. The consistent superior performance of our LDCNN across various metrics and datasets underscores its potential for practical applications in cardiac disease diagnosis.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being available for public access. However, the code used for the evaluation, including deep learning and machine learning models, preprocessing scripts, and architectures, is available on a GitHub repository. This repository can be accessed at the following link: https://github.com/aliebayani/Linear-Deep-Convolutional-Neural-Network-LDCNN.git. The availability of the code allows for reproducibility of the evaluation process and further analysis by other researchers. The specific details regarding the license under which the code is released are not provided, but it is common for such repositories to be open-source, allowing for wide accessibility and collaboration."
}