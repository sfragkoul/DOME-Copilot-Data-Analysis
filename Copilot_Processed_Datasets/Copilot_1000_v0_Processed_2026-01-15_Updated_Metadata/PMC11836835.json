{
  "publication/title": "Predicting diabetes self-management education engagement: machine learning algorithms and models.",
  "publication/authors": "Jiang X, Lv G, Li M, Yuan J, Lu ZK",
  "publication/journal": "BMJ open diabetes research & care",
  "publication/year": "2025",
  "publication/pmid": "39965870",
  "publication/pmcid": "PMC11836835",
  "publication/doi": "10.1136/bmjdrc-2024-004632",
  "publication/tags": "- Epidemiology/Health services research\n- Data handling\n- Machine learning\n- Diabetes self-management education\n- Predictive modeling\n- Health disparities\n- Random forest\n- Extreme gradient boosting\n- Recursive Feature Elimination\n- Synthetic Minority Over-sampling Technique\n- Diabetes management\n- Health equity\n- National Institute on Aging Health Disparities Research Framework\n- American Diabetes Association\n- Medicare Current Beneficiary Survey",
  "dataset/provenance": "The dataset used in this study is sourced from the Medicare Current Beneficiary Survey (MCBS). This survey includes data on US Medicare beneficiaries with diabetes from the years 2017 to 2019. The MCBS data is nationally representative, providing a comprehensive overview of the Medicare population.\n\nThe study included 37.94% of participants who received diabetes self-management education (DSME) after applying specific inclusion and exclusion criteria. A total of 95 variables were utilized in the analysis, covering various domains such as biological, behavioral, sociocultural, and environmental factors. These variables were selected based on the National Institute on Aging Health Disparities Research Framework and the DSME Consensus Report by the American Diabetes Association (ADA).\n\nThe dataset has been employed in previous research to investigate DSME participation among the older population and to identify factors associated with DSME engagement. The use of this dataset allows for a thorough evaluation of the predictors of DSME engagement, considering the diverse needs and backgrounds of the Medicare beneficiaries.",
  "dataset/splits": "In our study, we employed a simple random sampling method with a fixed ratio to create two primary data splits: a training set and a validation set. Specifically, 70% of the samples were randomly assigned to the training set, while the remaining 30% were allocated to the validation set. This approach ensured a balanced distribution of data points between the two splits, facilitating robust model training and evaluation. Additionally, we conducted a sensitivity analysis by excluding individuals with missing values for any variables, which further validated the robustness of our findings. This analysis confirmed that the performance of our machine learning models remained consistent, even when dealing with incomplete data.",
  "dataset/redundancy": "The datasets were split using a simple random sampling method with a fixed ratio. Specifically, 70% of the samples were randomly assigned to the training set, while the remaining 30% were allocated to the validation set. This approach ensures that the training and validation datasets are independent, as the random sampling process prevents any overlap between the two sets.\n\nTo address potential issues with class imbalance, the Synthetic Minority Over-sampling Technique (SMOTE) was employed. SMOTE operates in the feature space and calculates the Euclidean distance between data points based on their nearest neighbors. By artificially synthesizing new samples for the minority class, SMOTE helps to alleviate the overfitting problem that can occur when randomly replicating existing samples.\n\nThe distribution of the datasets in this study compares favorably to previously published machine learning datasets. The use of SMOTE ensures that the models are trained on a balanced dataset, which is crucial for achieving robust and generalizable performance. Additionally, the random sampling method with a fixed ratio ensures that the training and validation sets are representative of the overall population, enhancing the reliability of the models' predictions.\n\nThe study also conducted a sensitivity analysis by excluding individuals with missing values for any variables. This analysis confirmed the robustness of the study's findings, as all machine learning models showed similar performance in predicting DSME engagement, except for the Support Vector Machine (SVM) model. This further validates the independence and representativeness of the training and validation datasets.",
  "dataset/availability": "The data used in this study were obtained from the Medicare Current Beneficiary Survey (MCBS), which is a nationally representative survey linked to claims data. The specific dataset used includes information on Medicare beneficiaries with type 2 diabetes from 2017 to 2019. This dataset is not publicly released in its entirety due to the sensitive nature of the information it contains, which includes personal health data.\n\nHowever, the methods and variables used in the analysis are thoroughly described in the publication, allowing for reproducibility. The study employed a simple random sampling method with a fixed ratio to balance the training and validation datasets, with 70% of the samples assigned to the training set and 30% to the validation set. This approach ensures that the data splits are clearly defined and can be replicated by other researchers.\n\nThe study also utilized synthetic minority over-sampling technique (SMOTE) to address class imbalances and Recursive Feature Elimination (RFE) for variable selection. These techniques are well-documented in the literature, and their application in this study is detailed in the methods section.\n\nWhile the raw data cannot be publicly shared due to privacy concerns, the analytical methods and the specific variables used are transparent and reproducible. Researchers interested in replicating the study can follow the described procedures using similar datasets, ensuring the integrity and reliability of the findings.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the supervised learning class. Specifically, five different models were employed: support vector machines (SVM), random forest (RF), logistic regression (LR), extreme gradient boosting (XGB), and least absolute shrinkage and selection operator (LASSO) regression.\n\nThese algorithms are well-established and widely used in the field of machine learning, and they are not new. They were chosen for their ability to handle classification tasks and their varying strengths in dealing with different types of data and potential issues like overfitting and multicollinearity.\n\nThe study focuses on applying these algorithms to predict diabetes self-management education (DSME) engagement, rather than introducing a novel machine-learning algorithm. Therefore, it is published in a health services research journal rather than a machine-learning journal. The emphasis is on the application of these models to a specific healthcare problem, leveraging their established capabilities to provide insights into DSME participation among Medicare beneficiaries.",
  "optimization/meta": "Not applicable. The study does not employ a meta-predictor. Instead, it utilizes five distinct machine learning models: support vector machines (SVM), random forest (RF), logistic regression (LR), extreme gradient boosting (XGB), and least absolute shrinkage and selection operator (LASSO) regression. Each of these models is trained and evaluated independently to predict diabetes self-management education (DSME) engagement. The random forest model demonstrated the best performance, achieving an area under the curve (AUC) of 94% and an accuracy of 85%. The study does not combine the outputs of these models or use the predictions of one model as input for another.",
  "optimization/encoding": "In our study, we employed several techniques to handle and pre-process the data before applying the machine-learning algorithms. Missing values were treated as a distinct category, ensuring that participants with incomplete data were retained in the analysis. This approach helped to maintain the sample size and avoid potential biases that could arise from excluding participants with missing data.\n\nTo balance the training and validation datasets, we used a simple random sampling method with a fixed ratio. Specifically, 70% of the samples were randomly assigned to the training set, while the remaining 30% were allocated to the validation set. This split ensured that the models were trained on a representative subset of the data and validated on an independent set, enhancing the generalizability of the results.\n\nGiven that our dataset had unbalanced class data, we adopted the Synthetic Minority Over-sampling Technique (SMOTE). SMOTE operates in the feature space and calculates the Euclidean distance between data points based on their nearest neighbors. By artificially synthesizing new samples for the minority class, SMOTE effectively alleviates the overfitting problem that can occur when randomly replicating existing samples. This technique helped to balance the class distribution and improve the performance of the machine-learning models.\n\nAdditionally, we conducted a sensitivity analysis by excluding individuals with missing values for any variables. This analysis confirmed the robustness of our study, as all machine-learning models showed similar performance in predicting DSME engagement, except for the Support Vector Machine (SVM).\n\nFor feature selection, we employed Recursive Feature Elimination (RFE). RFE is a greedy algorithm that uses feature ranking techniques. It starts with a complete set of features and then iteratively eliminates the least relevant features, one by one, based on a specific feature ranking criterion. This process continues until the desired number of important features is selected, helping to identify the most relevant variables for the analysis.\n\nIn summary, our data encoding and pre-processing steps included handling missing values, balancing the datasets, addressing class imbalances with SMOTE, conducting sensitivity analyses, and using RFE for feature selection. These techniques ensured that the data was appropriately prepared for the machine-learning algorithms, enhancing the accuracy and reliability of our models.",
  "optimization/parameters": "In our study, we utilized 95 variables as input parameters for our machine learning models. These variables were selected based on the NIA Health Disparities Research Framework and the DSME Consensus Report by the ADA. They encompassed four domains: biological, behavioral, sociocultural, and environmental.\n\nTo identify the key variables among these 95, we employed Recursive Feature Elimination (RFE). RFE is a greedy algorithm that iteratively eliminates the least relevant features based on a specific ranking criterion. This process continued until the desired number of important features was selected. Through this method, we identified 74 key variables that significantly contributed to the prediction of DSME engagement. These key variables included 36 from the biological domain, 16 from the behavioral domain, 10 from the sociocultural domain, and 12 from the environmental domain.\n\nThe selection of these key variables was crucial for enhancing the precision of our models. For instance, the Random Forest (RF) model demonstrated the best performance, achieving an accuracy of 85% and an AUC of 94%. The model could reach an accuracy of over 70% with the first 9 variables and over 80% with the first 18 variables, highlighting the effectiveness of the selected features in predicting DSME engagement.",
  "optimization/features": "In our study, we utilized a comprehensive set of 95 variables as input features. These variables were carefully selected based on the National Institute on Aging Health Disparities Research Framework and the DSME Consensus Report by the American Diabetes Association. The variables span across four domains: biological, behavioral, sociocultural, and environmental.\n\nTo ensure that our models were efficient and focused on the most relevant predictors, we performed feature selection using Recursive Feature Elimination (RFE). This method is a greedy algorithm that iteratively removes the least important features based on a specific ranking criterion. The process continues until the desired number of key variables is identified. This approach helps in identifying the most relevant variables for the analysis.\n\nIt is crucial to note that the feature selection process was conducted using only the training set. This ensures that the validation set remains independent and unbiased, allowing for a true evaluation of the model's performance on unseen data. By doing so, we maintain the integrity of our results and avoid overfitting, which is essential for the robustness of our predictive models.",
  "optimization/fitting": "In our study, we employed several techniques to address potential overfitting and underfitting issues. We started with a dataset containing 95 variables, which is indeed a large number of parameters relative to the number of training points. To mitigate overfitting, we utilized the Synthetic Minority Over-sampling Technique (SMOTE). This method artificially synthesizes new samples for the minority class, helping to balance the dataset and reduce the risk of overfitting that can occur with random replication of existing samples.\n\nAdditionally, we used Recursive Feature Elimination (RFE) to select key variables. RFE is a greedy algorithm that iteratively eliminates the least relevant features, ensuring that only the most important variables are retained. This process helps to simplify the model and reduce the risk of overfitting by focusing on the most relevant predictors.\n\nTo further ensure the robustness of our models, we conducted a sensitivity analysis by excluding individuals with missing values for any variables. This analysis confirmed that our results were consistent and reliable, even when dealing with incomplete data.\n\nWe also employed a simple random sampling method with a fixed ratio to balance the training and validation datasets. Specifically, 70% of the samples were assigned to the training set, while the remaining 30% were allocated to the validation set. This approach helps to ensure that the model generalizes well to new, unseen data, thereby reducing the risk of both overfitting and underfitting.\n\nMoreover, we evaluated the performance of our models using various metrics, including accuracy, sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and F1 score. These metrics provided a comprehensive assessment of model performance, helping us to identify and address any potential issues related to overfitting or underfitting.\n\nIn summary, our study employed a combination of techniques, including SMOTE, RFE, sensitivity analysis, and balanced sampling, to address potential overfitting and underfitting issues. These methods ensured that our models were robust, reliable, and capable of generalizing well to new data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method used was the Synthetic Minority Over-sampling Technique (SMOTE). SMOTE operates in the feature space and calculates the Euclidean distance between data points based on their nearest neighbors. By artificially synthesizing new samples for the minority class, SMOTE effectively alleviates the overfitting problem that can occur when randomly replicating existing samples.\n\nAdditionally, we utilized Recursive Feature Elimination (RFE) as part of our variable selection process. RFE is a greedy algorithm that starts with a complete set of features and iteratively eliminates the least relevant features based on a specific feature ranking criterion. This process continues until the desired number of important features is selected, helping to identify the most relevant variables for the analysis and reducing the risk of overfitting.\n\nFurthermore, we employed regularization techniques within our machine learning models. For instance, the Extreme Gradient Boosting (XGB) model incorporates a regularization parameter that addresses overfitting by penalizing complex models. This enhances the model\u2019s speed and efficiency while preventing it from becoming too complex and overfitting the training data.\n\nThese methods collectively ensured that our models were robust and generalizable, minimizing the risk of overfitting and enhancing their predictive performance.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model employed in this study, specifically the Random Forest (RF), is not entirely a black-box model. RF is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees. This structure allows for a degree of interpretability.\n\nOne of the key strengths of RF is its ability to provide feature importance scores. These scores indicate the relative importance of each variable in making predictions. In our study, RFE (Recursive Feature Elimination) was used in conjunction with RF to select key variables associated with DSME engagement. This process identified 74 out of 95 variables as key, including variables from biological, behavioral, sociocultural, and environmental domains. The top 10 important variables for DSME prediction, such as knowing about managing diabetes, census, testing own blood for sugar/glucose, income, and education level, were clearly identified. This transparency allows stakeholders to understand which factors are most influential in predicting DSME engagement.\n\nAdditionally, the decision trees within the RF can be individually examined to understand the decision-making process. While interpreting each tree can be complex, the overall importance scores and the structure of the trees provide insights into how the model makes predictions. This level of interpretability is crucial for healthcare applications, where understanding the underlying factors driving predictions is essential for trust and practical implementation.",
  "model/output": "The model employed in this study is a classification model. The primary objective was to predict whether Medicare beneficiaries with type 2 diabetes participated in diabetes self-management education (DSME). The models used, including support vector machines (SVM), random forest (RF), logistic regression (LR), extreme gradient boosting (XGB), and least absolute shrinkage and selection operator (LASSO) regression, were all applied to classify participants into two categories: those who participated in DSME and those who did not.\n\nThe evaluation metrics used, such as accuracy, sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and F1 score, further confirm that the models were designed for classification tasks. These metrics are commonly used to assess the performance of classification models in predicting binary outcomes.\n\nThe random forest model, in particular, demonstrated the best performance with an accuracy of 85%, sensitivity of 84%, specificity of 87%, PPV of 86%, NPV of 85%, and F1 score of 85%. This indicates that the model was effective in correctly classifying participants based on their DSME engagement status.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the machine learning models in this study involved several key steps and metrics to ensure robust and reliable results. Initially, the dataset was split using a simple random sampling method with a fixed ratio, allocating 70% of the samples to the training set and the remaining 30% to the validation set. This approach helped in balancing the training and validation datasets, ensuring that the models were trained and tested on representative samples.\n\nTo address the issue of unbalanced class data, the Synthetic Minority Over-sampling Technique (SMOTE) was employed. SMOTE operates in the feature space, calculating the Euclidean distance between data points based on their nearest neighbors. By artificially synthesizing new samples for the minority class, SMOTE effectively alleviates the overfitting problem that can occur when randomly replicating existing samples.\n\nThe performance of the machine learning models was assessed using various evaluation metrics, including accuracy, sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and F1 score. These metrics provided a comprehensive view of the models' predictive performance. Additionally, the receiver operating characteristic (ROC) curve and the area under the curve (AUC) were used to evaluate the models' ability to distinguish between the classes.\n\nFive different machine learning models were used in this study: support vector machines (SVM), random forest (RF), logistic regression (LR), extreme gradient boosting (XGB), and least absolute shrinkage and selection operator (LASSO) regression. Each model was evaluated based on its prediction performance, accuracy, strengths, and limitations for predicting diabetes self-management education (DSME) engagement.\n\nThe random forest model demonstrated the best performance, achieving an accuracy of 85%, sensitivity of 84%, specificity of 87%, PPV of 86%, NPV of 85%, and F1 score of 85%. The extreme gradient boosting model also performed well, with an accuracy of 82%, sensitivity of 78%, specificity of 86%, PPV of 85%, NPV of 80%, and F1 score of 82%. The AUC for the random forest and extreme gradient boosting models were 94% and 91%, respectively, indicating strong predictive performance.\n\nTo further validate the robustness of the results, a sensitivity analysis was conducted by excluding individuals with missing values for any variables. This analysis confirmed that the findings were reliable despite the presence of missing data. Additionally, subgroup analyses were performed to investigate the distinct predictors for individuals from different racial/ethnic groups, including non-Hispanic Whites, non-Hispanic Blacks, Hispanics, and other racial/ethnic groups. This approach ensured that the models were applicable to diverse populations.\n\nIn summary, the evaluation method involved a combination of data splitting, SMOTE for handling unbalanced data, and a comprehensive set of evaluation metrics. The random forest model emerged as the most accurate and reliable for predicting DSME engagement, with strong performance across various metrics.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our machine learning models in predicting diabetes self-management education (DSME) engagement. The metrics used include the receiver operating characteristic (ROC) curve, area under the curve (AUC), accuracy, sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and the F1 score.\n\nThe ROC curve and AUC provide a visual and quantitative measure of the model's ability to distinguish between classes, with higher AUC values indicating better performance. Accuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, also known as recall, assesses the model's ability to identify true positives, while specificity measures the model's ability to identify true negatives. PPV and NPV evaluate the probability that positive and negative results are true positives and true negatives, respectively. The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both concerns.\n\nThese metrics are widely recognized and used in the literature for evaluating classification models, ensuring that our evaluation is representative and comparable to other studies in the field. By reporting these metrics, we aim to provide a thorough assessment of our models' performance, highlighting their strengths and areas for potential improvement.",
  "evaluation/comparison": "In our study, we did not perform a comparison to publicly available methods on benchmark datasets. However, we did compare the performance of five different machine learning models, including support vector machines (SVM), random forest (RF), logistic regression (LR), extreme gradient boosting (XGB), and least absolute shrinkage and selection operator (LASSO) regression. These models were evaluated based on various metrics such as accuracy, sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and F1 score.\n\nThe comparison to simpler baselines was implicitly addressed by including logistic regression, which is considered one of the simplest machine learning models. This allowed us to assess how more complex models like RF and XGB performed in comparison to a more straightforward approach. The results indicated that RF and XGB had better performance, with RF achieving an accuracy of 85% and an area under the curve (AUC) of 94%, while XGB had an accuracy of 82% and an AUC of 91%. This comparison helped us identify the strengths and limitations of each model in predicting diabetes self-management education (DSME) engagement.",
  "evaluation/confidence": "The study employed several evaluation metrics to assess the performance of the machine learning models, including accuracy, sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and F1 score. These metrics were used to compare the models' predictive capabilities for diabetes self-management education (DSME) engagement.\n\nThe results indicated that the random forest (RF) model demonstrated the best performance, achieving an accuracy of 85%, sensitivity of 84%, specificity of 87%, PPV of 86%, NPV of 85%, and F1 score of 85%. The area under the curve (AUC) for the RF model was 94%, which is notably high and suggests strong discriminative ability. Similarly, the extreme gradient boosting (XGB) model also performed well, with an AUC of 91% and an accuracy of 82%.\n\nStatistical significance was considered with a p-value threshold of less than 0.05. This threshold was applied to determine the significance of differences in characteristics among patients receiving or not receiving DSME, ensuring that the findings were robust and not due to random chance.\n\nThe study also conducted a sensitivity analysis by excluding individuals with missing values for any variables. This analysis confirmed the robustness of the results, as all machine learning models showed similar performance in predicting DSME engagement, except for the support vector machine (SVM) model. The RF model, in particular, maintained an AUC of 94%, accuracy of 85%, sensitivity of 85%, specificity of 85%, PPV of 85%, NPV of 85%, and F1 score of 85% even after excluding individuals with missing data.\n\nThese findings provide a high level of confidence in the performance metrics and the overall robustness of the models. The consistent performance across different analyses and the statistical significance of the results support the claim that the RF model is superior in predicting DSME engagement compared to other models evaluated in the study.",
  "evaluation/availability": "Not enough information is available."
}