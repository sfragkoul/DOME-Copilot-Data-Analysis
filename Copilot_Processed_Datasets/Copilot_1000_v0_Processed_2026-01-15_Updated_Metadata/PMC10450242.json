{
  "publication/title": "The Role of Levodopa Challenge in Predicting the Outcome of Subthalamic Deep Brain Stimulation.",
  "publication/authors": "Wolke R, Becktepe JS, Paschen S, Helmers AK, K\u00fcbler-Weller D, Youn J, Brinker D, Bergman H, K\u00fchn AA, Fasano A, Deuschl G",
  "publication/journal": "Movement disorders clinical practice",
  "publication/year": "2023",
  "publication/pmid": "37635781",
  "publication/pmcid": "PMC10450242",
  "publication/doi": "10.1002/mdc3.13825",
  "publication/tags": "- Parkinson's disease\n- Deep brain stimulation\n- Levodopa\n- UPDRS III\n- Predictive modeling\n- Statistical analysis\n- Machine learning\n- Clinical data\n- Multicenter study\n- Subthalamic nucleus",
  "dataset/provenance": "The dataset used in this study was sourced from multiple centers, with significant differences found in the age of implantation and UPDRS III scores, including sub-scores, between these centers. To ensure a comprehensive analysis, the datasets from these centers were merged. This merging allowed for the coverage of a wide range of phenotypes, which was crucial for the predictive model training.\n\nThe study included a relatively large number of cases, specifically 429, which is substantial for this type of analysis. However, it was noted that this number might still be insufficient to fully capture the heterogeneity among patients suffering from idiopathic Parkinson's disease (iPD).\n\nThe data used in this study was gathered as UPDRS III. While the findings of this analysis are based on UPDRS III, it is acknowledged that further study is needed to determine if these findings hold true for MDS-UPDRS III, given the high correlation between the two (R = 0.96).\n\nThe dataset included clinical data such as age of implantation, UPDRS III scores, and various sub-scores. These data points were essential for the correlation analyses and the development of predictive models. The study also utilized data visualization techniques, such as Sankey diagrams, to explore relationships within the dataset.\n\nThe dataset has not been previously used in other papers by the community, as it is a new analysis focusing on the value of the Levodopa challenge for prediction in the context of Deep Brain Stimulation (DBS) outcomes. The code used for statistical analysis is available upon reasonable request, but the data itself cannot be shared due to privacy regulations.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in this study is not publicly available due to privacy regulations. However, the code used for statistical analysis is available upon reasonable request. This ensures that other researchers can replicate the analysis while protecting patient confidentiality. The study was conducted following the Declaration of Helsinki and was approved by the ethics committee of the Kiel Medical Faculty, ensuring that all data handling procedures were in compliance with ethical standards. The decision to keep the data private was made to safeguard the anonymity of the patients involved, as consent from individual patients to use anonymized data for further research purposes was obtained at the time of inclusion in the databases.",
  "optimization/algorithm": "The machine-learning algorithms used in our study were primarily from the classes of regression and classification models. For regression tasks, we employed linear models, Xgradient boosting tree models, and support vector machines with polynomial kernels. For classification tasks, we utilized logistic regression models. These algorithms are well-established and widely used in the field of machine learning and statistics.\n\nThe algorithms used are not new; they are standard techniques that have been extensively studied and applied in various domains. The choice of these algorithms was driven by their robustness and the need for interpretability in our medical context. The linear model, in particular, was favored for its simplicity and ease of understanding, which is crucial when communicating results to clinicians and patients.\n\nThe decision to use these established algorithms rather than novel ones was based on the specific requirements of our study. Our primary goal was to predict the outcomes of deep brain stimulation (DBS) based on preoperative Levodopa challenge results. The focus was on the clinical applicability and interpretability of the models rather than on developing new machine-learning techniques. Therefore, publishing in a machine-learning journal was not a priority, as our contributions lie more in the clinical and statistical validation of these models in a medical context.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The methods used in the study include polynomial kernels, grid search for hyperparameter tuning, and the SMOTE algorithm for addressing class imbalances. The study employed various regression models, such as linear models, Xgradient boosting tree models, and support vector machines with polynomial kernels, to predict stimulation improvement. These models were evaluated using metrics like R\u00b2 for regression tasks and ROC-AUC for classification tasks. The data was normalized, centered, and scaled before model training. The study also utilized the \"caret\" and \"caret ensemble\" R packages for statistical analysis and model building. The code used for statistical analysis is available upon reasonable request, ensuring transparency and reproducibility.",
  "optimization/encoding": "The data was normalized and centered before model fitting. For hyperparameter tuning, a default grid search was employed. Additionally, the data was centered and scaled before model training. To address class imbalances, the Synthetic Minority Over-sampling Technique (SMOTE) algorithm was applied. This was done using 10-fold-10-times-cross-validation to estimate the predictive power of the model on unseen data. For regression tasks, the R\u00b2 measure was used to evaluate model performance. In classification tasks, sensitivity, specificity, and the area under the curve (AUC) of the corresponding receiver operating curves (ROC) were reported. The ROC-AUC can range from 0 to 1, with values greater than 0.5 indicating the model's ability to discriminate between two classes. Statistical analysis and model building were conducted using the R \"base\" library and the \"caret\" and \"caret ensemble\" R packages. For data visualization, \"ggplot2\" was utilized. The code for these processes will be made available upon reasonable request. This protocol adhered to the Declaration of Helsinki and was approved by the ethics committee of the Kiel Medical Faculty.",
  "optimization/parameters": "In our study, we utilized three primary input parameters for our predictive models. These parameters were selected based on their significant correlation with postoperative stimulation improvement. The parameters included:\n\n1. Age at implantation: This variable was chosen due to its potential influence on the outcome of the surgical procedure.\n2. Preoperative Levodopa improvement: This parameter was selected because it showed a significant correlation with absolute stimulation improvement.\n3. Preoperative UPDRS III score in the medication-off state: This variable was included as it demonstrated a strong correlation with both Levodopa and stimulation improvement.\n\nThe selection of these parameters was informed by correlation analyses and multivariate linear regression modeling. The model's performance was evaluated using cross-validation techniques, ensuring that the chosen parameters provided a robust prediction of stimulation improvement.",
  "optimization/features": "The input features used in our models included various clinical metrics and patient characteristics. Specifically, we utilized the rest and action tremor scores of the most affected hand, the rigidity score, the akinesia score, and the PIGD sub-score, all measured during preoperative med-off and med-on conditions. Additionally, the age at implantation was included as a feature. These features were selected based on their clinical relevance and potential to influence the outcomes of the stimulation improvement.\n\nFeature selection was not explicitly performed as a separate step. Instead, we relied on domain knowledge to choose the most relevant features for our models. This approach ensured that the features used were clinically meaningful and likely to contribute to the predictive power of our models.\n\nTo maintain the integrity of our analysis, all feature selection and preprocessing steps were conducted using only the training data during the cross-validation process. This practice helps to prevent data leakage and ensures that the models' performance is a true reflection of their ability to generalize to unseen data.",
  "optimization/fitting": "The fitting method employed in this study involved several steps to ensure robust model performance and to address potential issues of overfitting and underfitting.\n\nTo mitigate overfitting, particularly given the complexity of the models and the potential for a large number of parameters relative to the training points, several techniques were utilized. Data normalization and centering were performed before model fitting to ensure that the data was on a comparable scale, which can help in stabilizing the training process. Additionally, hyperparameter tuning was conducted using the default grid search of the \"caret\" R package, which systematically explores different combinations of hyperparameters to find the optimal settings that generalize well to unseen data.\n\nTo further address class imbalances, the Synthetic Minority Over-sampling Technique (SMOTE) was applied. This method generates synthetic samples for the minority class, helping to balance the dataset and improve the model's ability to learn from underrepresented classes. The predictive power of the model was estimated using 10-fold-10-times cross-validation, which provides a more reliable estimate of model performance by averaging results over multiple train-test splits.\n\nFor regression tasks, the R\u00b2 measure was used to evaluate model performance, ensuring that the models captured a significant portion of the variance in the target variable. For classification tasks, sensitivity, specificity, and the area under the curve (AUC) of the receiver operating characteristic (ROC) curves were reported. The ROC-AUC metric, ranging from 0 to 1, indicates the model's discriminating power between two classes, with values greater than 0.5 suggesting useful predictive capability.\n\nStatistical analysis and model building were carried out using the R \"base\" library and the \"caret\" and \"caret ensemble\" R packages. For data visualization, \"ggplot2\" was used to create informative plots that aided in understanding the relationships within the data.\n\nIn summary, the fitting method included data preprocessing, hyperparameter tuning, and the use of cross-validation to ensure that the models were neither overfitting nor underfitting. The evaluation metrics provided a comprehensive assessment of model performance, ensuring that the results were reliable and generalizable.",
  "optimization/regularization": "Regularization techniques were employed to prevent overfitting in our models. Specifically, we used the Synthetic Minority Over-sampling Technique (SMOTE) to address class imbalances, which is crucial for ensuring that our models do not become biased towards the majority class. This technique helps in generating synthetic samples for the minority class, thereby balancing the dataset and improving the model's ability to generalize to unseen data.\n\nAdditionally, we utilized cross-validation, specifically 10-fold-10-times cross-validation, to estimate the predictive power of our models. This method involves dividing the data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. By averaging the results, we obtain a more robust estimate of the model's performance and reduce the risk of overfitting.\n\nFor hyperparameter tuning, we relied on the default grid search provided by the \"caret\" R package. This systematic approach helps in finding the optimal hyperparameters that minimize overfitting and enhance the model's performance on unseen data. By combining these techniques, we aimed to build models that are not only accurate but also generalizable to new, unseen datasets.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are available upon reasonable request. The code, including the specifics of the hyper-parameter tuning process, is also accessible under this condition. We utilized the default grid search of the \u201ccaret\u201d R package for hyperparameter tuning, ensuring reproducibility. The data was normalized, centered, and scaled before model training, and the SMOTE algorithm was applied to address class imbalances. For regression tasks, the R\u00b2 measure was used to evaluate model performance, while for classification tasks, sensitivity, specificity, and the area under the curve (AUC) of the receiver operating characteristic (ROC) curves were reported. The statistical analysis and model building were conducted using the R \u201cbase\u201d library and the \u201ccaret\u201d and \u201ccaret ensemble\u201d R packages. For data visualization, we employed \u201cggplot2\u201d. The code and detailed configurations will be provided to interested parties who request it, facilitating further research and validation of our methods.",
  "model/interpretability": "The models employed in our study range from transparent to somewhat interpretable, depending on the specific algorithm used. The linear model, for instance, is highly transparent. It provides clear coefficients for each input feature, allowing us to understand the direct impact of each variable on the outcome. For example, the preoperative UPDRS III score in the med-off state, the improvement in UPDRS III during the Levodopa challenge, and the age at implantation are all directly interpretable in terms of their contribution to the stimulation outcome.\n\nIn contrast, models like the gradient boosting tree (xgbTree) and support vector machine with polynomial kernel (svmPoly) are more complex and less transparent. These models can capture intricate patterns in the data but at the cost of interpretability. While they can provide feature importance scores, these do not offer the same level of clarity as the coefficients in a linear model.\n\nTo enhance interpretability, we also utilized Shapley-analysis, which helps in understanding the contribution of each feature to the model's predictions. This method is particularly useful for excluding multi-collinearities and provides a more nuanced view of variable importance. Both the linear model and Shapley-analysis ranked the factors in a similar order, with the preoperative UPDRS III in the med-off state being the strongest predictor, followed by the improvement in UPDRS III during the Levodopa challenge, and lastly, the age at implantation.\n\nOverall, while some of our models are black-box in nature, we have taken steps to ensure that the key predictors and their impacts are clear and interpretable. This balance allows us to leverage the predictive power of advanced algorithms while maintaining transparency in our findings.",
  "model/output": "The models developed in this study encompass both regression and classification types. For predicting the absolute and relative stimulation improvement as continuous variables, regression models were employed. These included a linear model, a gradient boosting tree model, and a support vector machine with a polynomial kernel. The linear model was ultimately chosen for its simplicity and interpretability, achieving an R\u00b2 value of 0.41 for absolute stimulation improvement and 0.14 for relative stimulation improvement.\n\nIn contrast, for dichotomizing outcomes into favorable and unfavorable categories, classification models were used. Specifically, logistic regression models were applied to predict whether patients would achieve more than 33% postoperative stimulation improvement or a minimal clinically relevant improvement of 5 UPDRS III points. The performance of these models was evaluated using metrics such as the area under the receiver operating characteristic curve (ROC-AUC) and specificity. For instance, the logistic regression model predicting a 5-point improvement in UPDRS III scores showed a median ROC-AUC of 0.72.\n\nAdditionally, attempts were made to classify dichotomized outcomes for rest and action tremor, as well as akinesia, rigidity, and PIGD sub-scores. However, these models did not reach clinically applicable levels of ROC-AUC and specificity. The challenges included class imbalances and the limited number of subjects with sufficient PIGD score reduction, which hindered the training of robust predictive models for these specific outcomes.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code used for the analysis and model building in this study will be made available upon reasonable request. This includes the scripts and implementations for data preprocessing, model training, and evaluation. The code is designed to be reproducible and can be used by other researchers to validate or build upon the findings presented.\n\nThe statistical analysis and model building were carried out using the R programming language, specifically utilizing the \"base\" library and the \"caret\" and \"caret ensemble\" packages. For data visualization, the \"ggplot2\" package was employed. These packages are widely used in the scientific community and are well-documented, ensuring that the methods and results can be easily replicated.\n\nThe decision to provide the code upon request is to ensure that it is used appropriately and to facilitate collaboration and further research in the field. The code will be shared under a license that allows for academic use and further development, promoting transparency and reproducibility in scientific research.",
  "evaluation/method": "The evaluation of the models involved several rigorous statistical methods to ensure the robustness and generalizability of the results. For hyperparameter tuning, a default grid search from the \"caret\" R package was employed. This approach systematically worked through multiple combinations of hyperparameter values to determine the optimal settings for the models.\n\nTo address class imbalances in the data, the Synthetic Minority Over-sampling Technique (SMOTE) was applied. This technique helps to balance the dataset by generating synthetic samples for the minority class, which is crucial for improving the model's performance on underrepresented classes.\n\nFor evaluating the predictive power of the models, 10-fold-10-times cross-validation was used. This method involves dividing the data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. The entire procedure is then repeated 10 times to ensure the stability and reliability of the results.\n\nFor regression tasks, the R-squared (R\u00b2) measure was used to evaluate the models' performance. This metric indicates the proportion of the variance in the dependent variable that is predictable from the independent variables.\n\nFor classification tasks, the sensitivity, specificity, and area under the curve (AUC) of the receiver operating characteristic (ROC) curves were reported. The ROC-AUC provides a single scalar value that summarizes the performance of the model across all classification thresholds. A ROC-AUC value greater than 0.5 indicates that the model has discriminative power between the two classes.\n\nStatistical analysis and model building were carried out using the R \"base\" library and the \"caret\" and \"caret ensemble\" R packages. For data visualization, the \"ggplot2\" package was utilized to create informative and visually appealing plots.\n\nThe code used for the analysis will be made available upon reasonable request, ensuring transparency and reproducibility of the results. The study was conducted following the Declaration of Helsinki and was approved by the ethics committee of the Kiel Medical Faculty, ensuring ethical standards were met throughout the research process.",
  "evaluation/measure": "For the evaluation of our models, we primarily reported the area under the curve (AUC) of the receiver operating characteristic (ROC) curves, which is a standard metric for assessing the performance of classification models. The ROC-AUC values ranged from 0.66 to 0.78, depending on the specific outcome being predicted. For instance, the ROC-AUC was 0.72 for predicting a 5-point improvement in UPDRS III scores and 0.78 for predicting a 5-point improvement. These values indicate the discriminating power between two classes, with values greater than 0.5 suggesting better than random performance.\n\nIn addition to ROC-AUC, we also reported sensitivity and specificity, which are crucial for understanding the true positive and true negative rates of our models, respectively. For example, the median specificity for predicting a 5-point improvement was 0.50.\n\nFor regression tasks, we used the R\u00b2 measure to evaluate model performance. The R\u00b2 values indicated the proportion of variance in the dependent variable that is predictable from the independent variables. For predicting absolute stimulation improvement, the linear model achieved an R\u00b2 of 0.41, while for relative stimulation improvement, the R\u00b2 was 0.14.\n\nWe also addressed class imbalances using techniques like Synthetic Minority Over-sampling Technique (SMOTE) and applied 10-fold-10-times cross-validation to ensure the robustness of our results. This approach is representative of current practices in the literature, ensuring that our performance metrics are both reliable and comparable to other studies in the field.\n\nOverall, the reported metrics provide a comprehensive view of our models' performance, covering both classification and regression tasks. The use of standard metrics like ROC-AUC, sensitivity, specificity, and R\u00b2 ensures that our results are interpretable and comparable to other research in the domain.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. However, we did compare different statistical models and machine learning algorithms to evaluate their performance in predicting outcomes. Specifically, we used regression models, including a linear model and support vector machines with polynomial kernels, to predict stimulation improvement as a continuous variable. For classification tasks, we employed logistic regression models to dichotomize outcomes into favorable and unfavorable categories.\n\nWe also compared the performance of these models to simpler baselines. For instance, we found that the linear model and support vector machines performed comparably in predicting absolute stimulation improvement, with the linear model being simpler and more interpretable. This comparison allowed us to opt for the linear model due to its simplicity and understandability.\n\nAdditionally, we used cross-validation techniques, such as 10-fold-10-times cross-validation, to estimate the predictive power of our models on unseen data. This approach helped us to ensure that our models were robust and generalizable. We also applied the SMOTE algorithm to address class imbalances, which is a common technique in machine learning to handle datasets with unequal class distributions.\n\nIn summary, while we did not compare our methods to publicly available benchmarks, we conducted thorough comparisons between different statistical and machine learning models, as well as simpler baselines, to evaluate and validate our approach.",
  "evaluation/confidence": "The performance metrics presented in our study include confidence intervals, which are crucial for understanding the uncertainty and variability of our predictions. For instance, the ROC-AUC values for predicting improvement as a dichotomized variable are provided with interquartile ranges (IQR25\u201375), indicating the spread of the results across different cross-validation folds. This approach ensures that the reported metrics are robust and not overly optimistic.\n\nStatistical significance is also a key consideration in our analysis. We employed 10-times repeated 10-fold cross-validation to estimate the predictive power of our models on unseen data. This rigorous validation method helps to ensure that our results are not due to chance and that the models generalize well to new, independent datasets.\n\nMoreover, we used the SMOTE algorithm to address class imbalances, which is a common issue in medical datasets. By applying this technique, we aimed to create a more balanced dataset, thereby improving the reliability of our performance metrics.\n\nIn summary, the confidence intervals and statistical significance of our performance metrics provide a solid foundation for claiming that our methods are reliable and potentially superior to simpler baselines. However, it is important to note that while our models show promising results, there are inherent limitations in predicting individual patient outcomes due to the complexity and heterogeneity of the disease.",
  "evaluation/availability": "The raw evaluation files, specifically the data used in our study, are not publicly available due to privacy regulations. However, the code used for statistical analysis is available upon reasonable request. This ensures that other researchers can replicate our findings while maintaining the confidentiality of patient information. The data itself cannot be shared to protect the privacy of the individuals involved in the study."
}