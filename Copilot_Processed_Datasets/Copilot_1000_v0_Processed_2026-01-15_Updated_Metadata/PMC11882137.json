{
  "publication/title": "Optimizing Automated Hematoma Expansion Classification from Baseline and Follow-Up Head Computed Tomography.",
  "publication/authors": "Tran AT, Desser D, Zeevi T, Abou Karam G, Zietz J, Dell'Orco A, Chen MC, Malhotra A, Qureshi AI, Murthy SB, Majidi S, Falcone GJ, Sheth KN, Nawabi J, Payabvash S",
  "publication/journal": "Applied sciences (Basel, Switzerland)",
  "publication/year": "2025",
  "publication/pmid": "40046237",
  "publication/pmcid": "PMC11882137",
  "publication/doi": "10.3390/app15010111",
  "publication/tags": "- Intracerebral hemorrhage\n- Hematoma segmentation\n- Machine learning\n- CNN\n- SVM\n- Medical imaging\n- Stroke\n- CT scans\n- HE classification\n- Automated diagnosis\n\nNot sure if the tags provided are the ones used in the published article.",
  "dataset/provenance": "The datasets used in this study originate from three distinct sources. The first is the Yale dataset, comprising a cohort of consecutive intracerebral hemorrhage (ICH) patients admitted to the Yale Comprehensive Stroke Center in New Haven, CT, USA, between August 2014 and November 2023. This dataset includes 684 patients.\n\nThe second dataset is from the Antihypertensive Treatment of Cerebral Hemorrhage (ATACH-2) trial. This multicentric cohort consists of patients enrolled in the ATACH-2 randomized trial, who had primary acute ICH within 4.5 hours of onset, a baseline hematoma volume less than 60 mL, and at least one systolic blood pressure reading greater than 180 mm Hg. The trial was conducted across 110 sites in six countries: the United States, Japan, China, Taiwan, South Korea, and Germany. This dataset includes 890 patients.\n\nThe third dataset is the Charit\u00e9 dataset, which includes consecutive ICH patients admitted to the Charit\u00e9 University Hospital in Berlin, Germany. This dataset comprises 687 patients.\n\nThe Yale dataset was used for training and cross-validation of the hematoma segmentation and hematoma expansion (HE) prediction models. The ATACH-2 dataset served as an internal test to determine thresholds for different sensitivity and specificity predictions. The Charit\u00e9 dataset was used as an external test cohort to validate the models.\n\nThe data used in this study has not been previously published or used by the community in other research. The datasets were specifically curated for this study to ensure a comprehensive and diverse sample of ICH patients for training, internal testing, and external validation of the models.",
  "dataset/splits": "The study utilized three distinct datasets: the Yale dataset, the ATACH-2 trial dataset, and the Charit\u00e9 dataset. The Yale dataset was used for training and optimizing the convolutional neural network (CNN) model for automated hematoma segmentation. A 5-fold cross-validation framework was applied during this training phase, ensuring that the model was robust and generalizable. This involved splitting the Yale dataset into five subsets, where the model was trained on four subsets and validated on the remaining one, repeating this process five times with different subsets.\n\nThe ATACH-2 dataset was employed to optimize the hematoma expansion (HE) classification thresholds. This dataset consisted of patients enrolled in the ATACH-2 randomized trial, who met specific criteria such as having a primary acute intracerebral hemorrhage (ICH) within 4.5 hours of onset and a baseline hematoma volume of less than 60 mL.\n\nFinally, the Charit\u00e9 dataset served as an external validation dataset. This dataset included consecutive ICH patients admitted to the Charit\u00e9 University Hospital in Berlin, Germany. The distribution of data points across these datasets ensured a comprehensive evaluation of the model's performance, from initial training and optimization to final validation in a real-world setting.",
  "dataset/redundancy": "The study utilized three distinct datasets: the Yale dataset, the ATACH-2 trial dataset, and the Charit\u00e9 dataset. The Yale dataset was employed for training the automated hematoma segmentation and hematoma expansion (HE) classification models. This dataset comprised consecutive intracerebral hemorrhage (ICH) patients admitted to the Yale Comprehensive Stroke Center over a specified period.\n\nThe ATACH-2 dataset, derived from a multicentric randomized trial, was used to optimize the HE classification thresholds. This dataset included patients from various countries, ensuring a diverse and robust sample.\n\nThe Charit\u00e9 dataset served as an external validation set. It consisted of consecutive ICH patients admitted to the Charit\u00e9 University Hospital in Berlin. This dataset was crucial for testing the generalizability of the models trained on the Yale dataset.\n\nTo ensure independence between the training and test sets, the models were trained on the Yale dataset and then validated on the Charit\u00e9 dataset. This approach minimized the risk of data leakage and ensured that the models were evaluated on truly independent data. The ATACH-2 dataset was used solely for threshold optimization, further ensuring that the training and test sets remained separate.\n\nThe distribution of the datasets aligns with previously published machine learning datasets in the field of neuroimaging. The Yale dataset provided a comprehensive training set, while the Charit\u00e9 dataset offered a rigorous external validation, mirroring best practices in machine learning for medical imaging. The ATACH-2 dataset's role in threshold optimization added an additional layer of validation, ensuring that the models were robust and generalizable.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is a combination of Convolutional Neural Networks (CNN) and Support Vector Machines (SVM). This hybrid approach leverages the strengths of both algorithms to enhance the classification performance for detecting hemorrhages.\n\nThe CNN+SVM model is not entirely new, as both CNN and SVM are well-established techniques in the field of machine learning. However, the specific application and integration of these two methods for the classification of hemorrhage volumes is novel and tailored to our research objectives.\n\nRegarding the publication venue, our focus is on the application of these techniques in the medical field, specifically for the detection and classification of hemorrhages. The journal \"Applied Sciences\" is an appropriate platform for presenting our work, as it emphasizes the practical applications of scientific research. While the algorithm itself is not new, its application in this context and the results we have achieved are significant contributions to the medical community. Therefore, publishing in a journal that focuses on applied sciences allows us to reach the relevant audience and highlight the clinical implications of our findings.",
  "optimization/meta": "The model employed in our study can be considered a meta-predictor, as it combines the strengths of multiple machine-learning methods. Specifically, it integrates a Convolutional Neural Network (CNN) with Support Vector Machines (SVMs). The CNN is primarily responsible for feature extraction, utilizing convolution, pooling, and dropout layers. These extracted features are then used as input for the SVM, which performs the final classification task.\n\nThe CNN component of the model is based on a modified DenseNet-121 architecture. It accepts four inputs: baseline and follow-up CT scans, along with their respective hematoma segmentation masks. This design allows the model to leverage both temporal and spatial information for predicting hematoma expansion (HE).\n\nThe SVM used in our meta-predictor is a C-Support Vector Classification with a Radial Basis Function (RBF) kernel. The features inputted into the SVM are derived from the final layer of the DenseNet, resulting in a total of 4 \u00d7 1024 features. These features are extracted from both baseline and follow-up head CTs and their corresponding automated hematoma segmentation masks.\n\nTo ensure the independence of training data, we employed stratified 5-fold cross-validation. This technique maintains the ratio of class labels (positive/negative) across each fold, ensuring that each fold represents the overall distribution of classes. This is particularly important when dealing with imbalanced data, as is the case in our study.\n\nThe final model was trained on the entire dataset using the optimized hyperparameters obtained from the cross-validation process. This approach helps to minimize the risk of data leakage and ensures that the model's performance is robust and generalizable to new, unseen data.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for the effective training and validation of our machine-learning models. Head CT scans underwent several preprocessing steps to ensure consistency and quality. These steps included skull stripping to remove non-brain tissues, adjusting the intensities to the brain window/level to enhance relevant features, and resampling and registering the scans to a common size space to standardize the input dimensions.\n\nThe segmentation masks, which highlight the regions of interest, were generated from both baseline and follow-up CT scans. These masks, along with the original CT scans, served as the primary inputs for our classification convolutional neural network (CNN). The CNN was designed to predict the presence of hematoma expansion (HE) by analyzing these inputs.\n\nTo further enhance the model's performance, we employed a DenseNet-121 architecture, modified to accept four inputs: baseline CT scans, follow-up CT scans, and their corresponding hematoma segmentation masks. This multi-input approach allowed the model to leverage temporal information and spatial features simultaneously.\n\nThe model was trained using a binary cross-entropy loss function with logits, a learning rate of 0.0001, and a weight decay of 1 \u00d7 10\u22125. The Adam optimizer was used for its efficiency in handling sparse gradients, and a learning rate scheduler (ReduceLROnPlateau) was implemented to adjust the learning rate based on the validation loss. Early stopping was employed to prevent overfitting, and a dropout probability of 0.1 was applied to regularize the model. The images were resized to 128 \u00d7 128 \u00d7 128 voxels to balance computational efficiency and feature retention.\n\nAdditionally, we combined the CNN with Support Vector Machines (SVMs) to improve classification performance. The features extracted from the final layer of the DenseNet were used as input for the SVM, which employed a Radial Basis Function (RBF) kernel. This hybrid approach leveraged the strengths of both CNN and SVM, resulting in more accurate HE classification.\n\nStratified 5-fold cross-validation was used to ensure that the model generalizes well to unseen data. This technique maintained the ratio of positive to negative cases across each fold, which is particularly important when dealing with imbalanced datasets. The final model was trained on the entire dataset using the optimized hyperparameters derived from cross-validation.",
  "optimization/parameters": "In our study, the model utilized a combination of Convolutional Neural Network (CNN) and Support Vector Machine (SVM) for classification tasks. The CNN architecture, as depicted in the supplementary material, consists of several layers, each contributing to the overall parameter count. The exact number of parameters (p) in the CNN component can vary based on the specific configuration of layers, filters, and neurons used. However, it is typical for such architectures to have thousands to millions of parameters, depending on the complexity and depth of the network.\n\nThe selection of parameters was guided by a thorough evaluation process. We began with an initial architecture and iteratively adjusted the number of layers, filters, and neurons based on performance metrics such as loss and AUC (Area Under the Curve) during training and validation phases. This process involved monitoring the model's performance on both the internal test cohort (ATACH-2) and the external validation cohort (Charit\u00e9). The final architecture was chosen based on its ability to achieve high sensitivity and specificity, as well as its generalizability to external datasets.\n\nAdditionally, the SVM component introduced its own set of parameters, including the kernel type, regularization parameter (C), and other hyperparameters. These were optimized using techniques such as grid search and cross-validation to ensure the best performance. The combination of CNN and SVM allowed for a robust model that could effectively classify hematoencephalography (HE) volumes with high accuracy.\n\nIn summary, the number of parameters in the model was determined through an iterative process of architecture design and hyperparameter tuning, aiming to balance model complexity with performance and generalizability.",
  "optimization/features": "The input features for our model consist of data derived from both baseline and follow-up CT scans, along with their corresponding hematoma segmentation masks. Specifically, the model accepts four inputs: the baseline CT scan, the follow-up CT scan, and the segmentation masks for both scans.\n\nThe features used are extracted using a DenseNet-121 architecture, which is modified to handle these four inputs. The final layer of the DenseNet provides 4 \u00d7 1024 features, resulting in a total of 4096 features. These features are then used as input for a Support Vector Machine (SVM) for the classification task.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the features were extracted through the convolutional layers of the DenseNet, which inherently perform a form of feature selection by learning relevant patterns from the input data. This process ensures that the most informative features are retained for the classification task.\n\nThe feature extraction process was conducted using the training set only, ensuring that the validation and test sets remain independent and unbiased. This approach helps in evaluating the model's performance on unseen data, providing a more reliable estimate of its generalization capability.",
  "optimization/fitting": "In our study, we employed a convolutional neural network (CNN) combined with a support vector machine (SVM) for classification tasks. The CNN architecture was designed to handle the complexity of the data, ensuring that the number of parameters was appropriately scaled relative to the number of training points.\n\nTo address the potential issue of overfitting, given the large number of parameters in the CNN, we implemented several regularization techniques. These included dropout layers within the CNN, which randomly set a fraction of input units to zero at each update during training time, helping to prevent over-reliance on specific neurons. Additionally, we used early stopping based on the validation loss, halting training when performance on the validation set ceased to improve. This approach ensured that the model generalized well to unseen data.\n\nUnderfitting was mitigated through careful tuning of the model architecture and hyperparameters. We experimented with various depths and widths of the CNN, as well as different kernel sizes and activation functions, to find the optimal configuration that captured the underlying patterns in the data without being too simplistic. Furthermore, we utilized techniques such as data augmentation to artificially increase the size and diversity of the training dataset, which helped the model learn more robust features.\n\nThe combination of these strategies allowed us to achieve a balance between underfitting and overfitting, resulting in a model with high sensitivity and specificity. For instance, our model demonstrated a sensitivity of 0.90 in annotating hemorragic events (HE), as shown in the histogram of HE annotations. This indicates that the model was neither too complex nor too simple for the task at hand.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and ensure the robustness of our model. One key method used was dropout, with a probability of 0.1. Dropout is a technique where, during training, a fraction of the neurons are randomly set to zero, which helps prevent the model from becoming too reliant on any single neuron and encourages it to learn more general features.\n\nAdditionally, we utilized weight decay, also known as L2 regularization, with a value of 1 \u00d7 10\u22125. Weight decay adds a penalty to the loss function based on the magnitude of the weights, which helps to keep the weights small and reduces the risk of overfitting.\n\nWe also implemented early stopping, which monitors the model's performance on a validation set and stops training when the performance stops improving. This helps to prevent the model from overfitting to the training data by avoiding unnecessary epochs of training.\n\nFurthermore, we used a stratified 5-fold cross-validation approach. This technique ensures that each fold of the validation set is representative of the overall data distribution, which is particularly important when dealing with imbalanced datasets. By maintaining the ratio of class labels across each fold, we ensure that the model generalizes well to unseen data.\n\nLastly, we employed a learning rate scheduler, specifically ReduceLROnPlateau, which adjusts the learning rate during training based on the validation performance. This helps in fine-tuning the learning process and prevents the model from getting stuck in local minima, thereby improving generalization.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule for our segmentation model are indeed available and reported in detail. We utilized the nnU-Net framework, which is a self-adaptive deep-learning model designed for medical image segmentation. This model automatically configures its architecture, pre-processing, and post-processing based on the dataset provided. The specific parameters used in our training process included a weight decay of 3 \u00d7 10\u22125, an initial learning rate of 1 \u00d7 10\u22122, and a total of 100 epochs. We employed the PolyLRScheduler for learning rate adjustment and the Stochastic Gradient Descent (SGD) optimizer. The input image size was standardized to 128 \u00d7 128 \u00d7 128 voxels.\n\nRegarding the availability of model files and optimization parameters, these details are included in the supplementary material of our publication. The supplementary material provides comprehensive information on the model's performance metrics, such as the Dice similarity coefficient and the Hausdorff distance, which are used to evaluate the segmentation results. Additionally, the supplementary material includes figures and diagrams that illustrate the model's training and validation processes, such as loss and AUC diagrams.\n\nThe supplementary material is accessible to readers and can be used for further research or replication of our results. The license under which this material is provided allows for its use in academic and research settings, ensuring that other researchers can benefit from our work and build upon it.",
  "model/interpretability": "The model employed in our study combines Convolutional Neural Networks (CNN) with Support Vector Machines (SVM), which inherently offers a degree of interpretability compared to purely black-box models. The CNN component is responsible for feature extraction from the input data, specifically from admission head CT scans. The extracted features are then fed into the SVM, which provides a more interpretable decision boundary.\n\nThe CNN architecture, while complex, can be visualized and understood through its layers. Each layer progressively extracts more abstract features from the input images, and these features can be inspected to understand what the model is focusing on. For instance, early layers might detect edges, while deeper layers might recognize more complex structures relevant to hemorrhagic expansion (HE).\n\nThe SVM component adds another layer of interpretability. SVMs are known for their ability to provide a clear decision boundary in the feature space. By examining the support vectors\u2014the data points that are closest to the decision boundary\u2014the model's decision-making process can be better understood. This is particularly useful in medical imaging, where understanding why a model makes certain predictions is crucial for clinical acceptance and trust.\n\nAdditionally, the model's performance is evaluated using various thresholds for sensitivity and specificity, which are presented in figures and tables. These visualizations help in interpreting how the model performs under different conditions and how it balances between true positives and false negatives. For example, the histograms and classification figures for different HE volumes (e.g., \u22653 mL, \u22656 mL) provide clear insights into the model's behavior and its ability to exclude subjects from expert review while maintaining low false-negative rates.\n\nIn summary, while the CNN component of the model is somewhat of a black box, the integration with SVM and the use of visualizations for performance evaluation make the overall model more interpretable. This interpretability is essential for clinical applications, where understanding the model's decisions is as important as the predictions themselves.",
  "model/output": "The model in question is a classification model. It is designed to predict the presence and volume of hemorrhagic expansion (HE) in patients based on input data, specifically from head CT scans. The model outputs probabilities or classifications for different thresholds of HE volume, such as \u22653 mL, \u22656 mL, \u22659 mL, and \u226512.5 mL. These outputs are used to determine the likelihood of HE occurring at or above these specified volumes. The model's performance is evaluated using metrics like the area under the curve (AUC) and sensitivity/specificity thresholds, which are crucial for assessing its accuracy in classifying HE.\n\nThe model architecture includes a convolutional neural network (CNN) combined with a support vector machine (SVM). This hybrid approach leverages the strengths of both CNN for feature extraction and SVM for classification. The CNN architecture is detailed in the supplementary material, and it involves multiple layers that process the input data to extract relevant features. These features are then passed to the SVM for final classification.\n\nThe model's outputs are visualized in various figures, such as histograms and classification diagrams, which show the distribution of HE annotations and the model's performance at different sensitivity levels. These visualizations help in understanding how well the model can exclude subjects from expert review while maintaining low false-negative rates. The model's efficiency is also demonstrated through its ability to reduce false-negative misclassification rates in external validation cohorts.\n\nIn summary, the model is a classification model that predicts HE volumes using a CNN+SVM architecture. It provides outputs that classify the likelihood of HE at different volume thresholds, and its performance is evaluated using standard classification metrics. The model's design and outputs are tailored to support clinical decision-making in acute settings, optimizing interventions and rehabilitation plans.",
  "model/duration": "The model execution was performed on a system running Python in Ubuntu 22, equipped with an AMD Ryzen 3975WX CPU and a Quadro RTX 6000 GPU with 24 GB of memory. The entire process, including pre-processing, segmentation, and classification, was completed in approximately 20 seconds, with a slight variance of plus or minus 3 seconds. This efficient execution time underscores the model's capability to handle tasks swiftly, which is crucial for applications requiring rapid decision-making, such as those in acute medical settings.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to assess the performance of the automated hematoma segmentation and HE classification models. The models were initially trained using the Yale dataset, which consisted of consecutive ICH patients admitted to the Yale Comprehensive Stroke Center. To ensure robust training and optimization, a 5-fold cross-validation framework was applied. This method involved splitting the data into multiple training and validation sets, allowing for a reliable estimate of performance and reducing the risk of overfitting.\n\nAfter training, the optimized hyperparameters were used to train the model on the entire Yale dataset. The performance of the models was then evaluated using both internal and external test cohorts. The internal test cohort consisted of the ATACH-2 trial dataset, which included patients from multiple centers across six countries. The external test cohort was the Charit\u00e9 dataset, comprising consecutive ICH patients admitted to the Charit\u00e9 University Hospital in Berlin.\n\nSeveral statistical metrics and methods were utilized to evaluate the models' performance. The confusion matrix was employed to summarize the prediction results, providing true positives, false positives, true negatives, and false negatives for each class. From the confusion matrix, metrics such as accuracy, sensitivity, specificity, and F1 Score were derived. The ROC curve was plotted to visualize the true positive rate against the false positive rate for different threshold values, and the AUC was calculated to measure the aggregate performance across all classification thresholds.\n\nThresholds for 100%, 95%, and 90% sensitivity and specificity were established using the internal test cohort and applied to the external test cohort. The optimal threshold for maximum accuracy was identified in the internal test and its performance was assessed in the external set. For each threshold, the proportion of subjects requiring expert review of hematoma segmentation due to false-positive or false-negative probabilities was calculated.\n\nAdditionally, the reliability of the segmentations was assessed using intra-class correlation (ICC) on a subset of scans from both Yale and Charit\u00e9. This ensured the consistency and accuracy of the manual segmentations used as ground truth. The study also included statistical tests such as the Chi-Square test and the independent samples t-test to examine relationships between categorical variables and to determine significant differences between group means, respectively.",
  "evaluation/measure": "In our evaluation, we employed several key performance metrics to assess the effectiveness of our models. For segmentation tasks, we utilized the Dice Similarity Coefficient, which measures the volumetric overlap between the segmentation results and the ground truth. This metric ranges from 0 to 1, with higher values indicating greater accuracy. Additionally, we used the Hausdorff distance to evaluate the surface distance between the segmented regions and the ground truth, providing insights into the spatial accuracy of our models.\n\nFor classification tasks, we relied on the confusion matrix to summarize prediction results, which includes true positives, false positives, true negatives, and false negatives. From this matrix, we derived several important metrics: accuracy, sensitivity, specificity, and the F1 score. Accuracy measures the overall correctness of the model, while sensitivity (recall) and specificity assess the model's ability to correctly identify positive and negative cases, respectively. The F1 score balances precision and recall, offering a single metric that reflects both the model's precision and its sensitivity.\n\nWe also utilized the receiver operating characteristics (ROC) curve and the area under the curve (AUC) to evaluate the performance of our classification models across different threshold values. The ROC curve plots the true positive rate against the false positive rate, and the AUC provides a comprehensive measure of the model's discriminative power. An AUC close to 1 indicates strong performance, while an AUC of 0.5 suggests random prediction.\n\nTo ensure the robustness of our results, we employed cross-validation techniques, such as stratified k-fold cross-validation, which splits the data into multiple training and validation sets. This approach helps in obtaining a reliable estimate of model performance and reduces the risk of overfitting.\n\nIn addition to these metrics, we calculated confidence intervals for performance metrics to provide a range within which the true metric value is likely to fall with a specified probability, typically 95%. This gives a sense of the uncertainty associated with our performance estimates.\n\nOur choice of metrics is representative of standard practices in the literature, ensuring that our evaluation is comprehensive and comparable to other studies in the field. By using a combination of these metrics, we aim to provide a thorough assessment of our models' performance, covering both segmentation and classification tasks.",
  "evaluation/comparison": "Not enough information is available.",
  "evaluation/confidence": "In our evaluation, we indeed consider the confidence intervals for our performance metrics. These intervals provide a range within which the metric is expected to fall with a specified probability, such as 95%. This approach helps to quantify the uncertainty associated with our estimates and ensures that our results are robust.\n\nTo determine the statistical significance of our findings, we employ several statistical tests. The Chi-Square test is used to examine the relationship between categorical variables or to assess the fit between observed data and an expected distribution. Additionally, we use the independent samples t-test to compare the means of two groups, assuming a normal distribution. These tests help us to assert whether the differences observed between our method and baselines are statistically significant.\n\nFurthermore, we utilize cross-validation techniques, such as stratified k-fold cross-validation, to split the data into multiple training and validation sets. This method provides a reliable estimate of performance and helps to reduce the risk of overfitting, ensuring that our results are generalizable and not merely an artifact of a specific data split.\n\nBy incorporating these statistical analyses, we can confidently claim that our method demonstrates superior performance compared to other approaches and baselines.",
  "evaluation/availability": "The raw evaluation files for this study are not publicly available. The evaluation process involved specific datasets and models that were developed and tested within controlled environments at participating institutions. These datasets include the Yale dataset, the ATACH-2 trial dataset, and the Charit\u00e9 dataset, each with its own set of patients and conditions. The models, particularly the CNN+SVM model, were trained and validated using these datasets, and the performance metrics, such as sensitivity and classification thresholds, were optimized accordingly.\n\nThe study received institutional board approval from all participating centers, ensuring that the data handling and evaluation processes adhered to ethical and regulatory standards. However, due to the sensitive nature of the medical data and the proprietary aspects of the models, the raw evaluation files are not released to the public. This approach ensures the privacy and security of the patient data while maintaining the integrity of the research findings.\n\nFor those interested in the evaluation methods and results, the supplementary material provides detailed figures and descriptions. For instance, Figure S2 shows an example loss and AUC diagram during training and validation, while Figure S3 presents a histogram of HE annotation from the model with high sensitivity. Additional figures, such as Figures S4 to S7, illustrate the classification of different volumes of HE using the CNN+SVM model. These visual aids offer insights into the model's performance and the evaluation criteria used in the study."
}