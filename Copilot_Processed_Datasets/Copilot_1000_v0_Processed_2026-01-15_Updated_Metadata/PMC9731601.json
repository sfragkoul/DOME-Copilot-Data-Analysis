{
  "publication/title": "Application of back propagation neural network model optimized by particle swarm algorithm in predicting the risk of hypertension.",
  "publication/authors": "Yan Y, Chen R, Yang Z, Ma Y, Huang J, Luo L, Liu H, Xu J, Chen W, Ding Y, Kong D, Zhang Q, Yu H",
  "publication/journal": "Journal of clinical hypertension (Greenwich, Conn.)",
  "publication/year": "2022",
  "publication/pmid": "36380516",
  "publication/pmcid": "PMC9731601",
  "publication/doi": "10.1111/jch.14597",
  "publication/tags": "- BP neural network\n- PSO-BP neural network\n- Hypertension\n- Risk factors\n- Prediction performance\n- Machine learning\n- Logistic regression\n- Mean impact value\n- Chronic disease\n- Guangdong region\n- Neural network optimization\n- Particle swarm optimization\n- Disease prediction\n- Model training\n- Sensitivity and specificity",
  "dataset/provenance": "The dataset used in this study originates from the monitoring data of chronic disease risk factors in the Guangdong region, collected between 2016 and 2018. The research subjects were randomly sampled using a cluster sampling method. After thorough data cleaning, a total of 3012 individuals were included in the study. These subjects comprise a cross-sectional sample, encompassing both the normal population and individuals diagnosed with hypertension.\n\nThe diagnosis of hypertension was based on the 1999 World Health Organization (WHO) - International Society of Hypertension (ISH) criteria, which define hypertension as a systolic blood pressure of 140 mmHg or higher and/or a diastolic blood pressure of 90 mmHg or higher.\n\nThe dataset includes 17 independent variables, which were preliminarily screened from the monitoring data. These variables encompass a range of factors such as gender, age, educational status, occupation, smoking habits, drinking habits, sleep duration, body mass index (BMI), heart rate, blood glucose levels, hemoglobin levels, cholesterol levels, triglycerides, high-density lipoprotein cholesterol, low-density lipoprotein cholesterol, daily salt intake, and daily oil intake. The dependent variable in this study is the high blood pressure status.\n\nThe data underwent normalization using Min-Max normalization to ensure consistency and comparability. Abnormal values were eliminated, and missing values were filled using the mode for categorical variables and the mean for continuous variables. The dataset was then divided into a training set and a test set, with 80% of the samples allocated to the training set and 20% to the test set. This division was done to train the classification model and evaluate its performance, respectively. In the training set, there were 316 cases of hypertension and 2094 cases of non-hypertension, while the test set contained 56 cases of hypertension and 546 cases of non-hypertension.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a test set. The training set comprised 80% of the samples, while the test set comprised the remaining 20%. Specifically, the training set included 316 hypertension cases and 2094 non-hypertension cases. The test set included 56 hypertension cases and 546 non-hypertension cases. Additionally, the training set was further divided into three subsets for the neural network training process: a training set, a validation set, and a test set. The training set was used to determine the model parameters, the validation set was used for the final optimization of the model, and the test set was used to evaluate the generalization ability of the trained model.",
  "dataset/redundancy": "The dataset used in this study was derived from monitoring data of chronic disease risk factors in the Guangdong region from 2016 to 2018. The research subjects were randomly sampled by cluster, resulting in a final sample size of 3012 individuals after data cleaning. These subjects included both the normal population and hypertensive patients, with hypertension diagnosed using the 1999 World Health Organization (WHO) - International Society of Hypertension (ISH) criteria.\n\nThe dataset was split into training and test sets to evaluate the model's performance. Specifically, 80% of the samples were randomly selected as the training set, while the remaining 20% were used as the test set. This split ensured that the training and test sets were independent, which is crucial for assessing the model's generalization ability.\n\nIn the training set, there were 316 hypertension cases and 2094 non-hypertension cases. In the test set, there were 56 hypertension cases and 546 non-hypertension cases. This distribution reflects the prevalence of hypertension in the dataset and ensures that the model is trained and tested on representative samples.\n\nThe data preprocessing steps included sorting, eliminating abnormal values, and filling missing values with the mode for categorical variables or the mean for continuous variables. Additionally, the data were normalized using Min-Max normalization to ensure that all features contributed equally to the model's learning process.\n\nThe training set was further divided into three subsets: the training set, validation set, and test set. The training set was used to determine the model parameters, the validation set was used for the final optimization of the model, and the test set was used to evaluate the model's generalization ability. This approach helps in preventing overfitting and ensures that the model performs well on unseen data.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets in the medical field. The use of a large, randomly sampled population and the careful splitting of the dataset into independent training and test sets ensure that the results are robust and generalizable. The preprocessing steps, including normalization and handling of missing values, further enhance the quality of the dataset and the reliability of the model's predictions.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is artificial neural networks, specifically backpropagation (BP) neural networks. These are multilayer feedforward neural networks trained using the error backpropagation algorithm. They are widely recognized and used in various fields due to their advantages in complex model fitting and distribution approximation.\n\nThe optimization algorithm employed is the particle swarm optimization (PSO) algorithm. PSO is a method for optimizing continuous nonlinear functions by modifying adjustable parameters to minimize the error between predicted and expected outputs. It is particularly noted for its ability to improve the performance of recognition tasks and optimize the structure of deep neural networks without adjusting redundant parameters.\n\nThe combination of PSO and BP neural networks, referred to as the PSO-BP neural network, aims to overcome the limitations of each individual component. The PSO algorithm enhances the global search ability, which helps in finding the optimal solution more effectively. This hybrid approach has been shown to improve the prediction performance and fitting ability of the model, making it suitable for complex tasks such as predicting the risk of hypertension.\n\nThis study did not introduce a new machine-learning algorithm. Instead, it leveraged established methods\u2014BP neural networks and PSO\u2014to create a more effective hybrid model. The focus was on applying these algorithms to the specific problem of hypertension risk prediction, rather than developing a novel algorithm. Therefore, publishing in a specialized machine-learning journal was not necessary, as the innovation lies in the application and optimization of existing techniques for a particular medical problem.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on monitoring data of chronic disease risk factors to construct and optimize neural network models for predicting hypertension.\n\nThe primary models discussed are the BP neural network and the PSO-BP neural network. The BP neural network is a traditional backpropagation neural network, while the PSO-BP neural network is an optimized version that uses the particle swarm optimization (PSO) algorithm to enhance its performance. The PSO algorithm helps in fine-tuning the neural network parameters to achieve better predictive accuracy and efficiency.\n\nThe training data for these models is derived from a dataset of chronic disease risk factors collected from the Guangdong region between 2016 and 2018. The dataset includes 3012 individuals, randomly sampled by cluster, and comprises both normal population and hypertensive patients. The data underwent cleaning and preprocessing to ensure its quality and relevance for model training.\n\nThe models were evaluated using performance metrics such as accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (AUC). The PSO-BP neural network demonstrated superior performance compared to the traditional BP neural network and a logistic regression model, indicating its effectiveness in predicting hypertension.\n\nIn summary, the models described are standalone neural networks optimized using the PSO algorithm, and they do not rely on the outputs of other machine-learning algorithms as input. The training data is independent and specifically collected for this study.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithm. We began by consulting literature and combining it with monitoring data from subjects, which included on-site questionnaire surveys and physical examinations. This process helped us preliminarily screen out 17 independent variables, such as gender, age, educational status, occupation, smoking habits, drinking habits, sleep duration, body mass index (BMI), heart rate, blood glucose levels, hemoglobin levels, cholesterol levels, triglycerides, high-density lipoprotein cholesterol, low-density lipoprotein cholesterol, daily salt intake, and daily oil intake. The dependent variable was high blood pressure status.\n\nTo handle the data effectively, we sorted it and eliminated any abnormal values. Missing values were filled using the mode for categorical variables and the mean for continuous variables. This approach ensured that our dataset was complete and ready for further processing.\n\nNormalization was performed using Min-Max normalization to scale the data, which is essential for improving the convergence speed and stability of the neural network. This step ensured that all variables contributed equally to the model's performance.\n\nFor model construction, we divided the dataset into a training set and a test set in a 4:1 ratio. The training set was further split into three subsets: the training set, validation set, and test set. The training set was used to determine model parameters, the validation set for final optimization, and the test set to evaluate the model's generalization ability. This division helped in preventing overfitting and ensured that the model's performance was accurately assessed.",
  "optimization/parameters": "In our study, we utilized 17 variables as input parameters for the model. These variables were initially screened and included in the input vector of the backpropagation neural network. The selection of these parameters was based on their potential relevance to hypertension, as identified through preliminary research and statistical analysis. The variables encompassed a range of factors, including lipid profiles, lifestyle habits, demographic information, and physiological measurements. This comprehensive set of parameters aimed to capture the multifaceted nature of hypertension risk, ensuring that the model could effectively identify and predict the disease based on a broad spectrum of influencing factors.",
  "optimization/features": "In our study, we initially considered 17 variables as input features for our neural network models. These features were selected based on a thorough review of relevant literature and the available monitoring data. The variables included factors such as low-density lipoprotein cholesterol, cholesterol, triglycerides, daily oil intake, daily salt intake, smoking, age, BMI, drinking, sleep duration, heart rate, high-density lipoprotein cholesterol, and hemoglobin, among others.\n\nFeature selection was indeed performed to ensure that only the most relevant variables were included in the models. This process was crucial for preventing overfitting and enhancing the models' predictive performance. The feature selection was conducted using the training set only, ensuring that the test set remained independent and unbiased. This approach helped in identifying the key risk factors associated with hypertension, such as cholesterol levels, dietary habits, and lifestyle factors. By focusing on these significant variables, we aimed to improve the accuracy and reliability of our neural network models.",
  "optimization/fitting": "In our study, we constructed and optimized neural network models to predict hypertension risk. The number of parameters in our models was not excessively large compared to the number of training points, which helped mitigate the risk of overfitting. To further ensure that our models did not overfit, we employed several strategies.\n\nFirstly, we divided our dataset into training and testing sets in a 4:1 ratio. This separation allowed us to evaluate the model's performance on unseen data, providing a more reliable assessment of its generalization ability.\n\nSecondly, we monitored the model's performance on both the training and validation sets during the training process. By observing the mean square error curves, we ensured that the model's error on the validation set did not increase significantly while the training error decreased. This approach helped us identify and prevent overfitting.\n\nAdditionally, we compared the correlation coefficients of the training, validation, and test sets. The similar correlation coefficients across these sets indicated that the model was not overfitting to the training data.\n\nTo address underfitting, we ensured that our models had sufficient complexity to capture the underlying patterns in the data. We experimented with different architectures and hyperparameters, selecting those that provided the best performance on the validation set. Furthermore, we used a particle swarm optimization (PSO) algorithm to optimize the backpropagation (BP) neural network, which improved the model's fitting ability and prediction performance.\n\nIn summary, by carefully managing the model complexity, using appropriate validation techniques, and optimizing the neural network with PSO, we effectively addressed both overfitting and underfitting concerns in our study.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was the division of the dataset into training and testing sets in a 4:1 ratio. This approach helped in evaluating the model's performance on unseen data, thereby reducing the risk of overfitting.\n\nAdditionally, we monitored the model's performance on both the training and validation sets during the training process. For the BP neural network, the model prediction errors of the validation set and the test set gradually tended toward stability, indicating that the model was generalizing well to new data. Similarly, for the PSO-BP neural network, the model prediction errors of the validation set and the test set also showed stability, with the training set error decreasing, suggesting that the model was not overfitting.\n\nFurthermore, we used the mean influence value (MIV) algorithm to screen out the risk factors related to hypertension. This algorithm helped in excluding variables with interference in the model, thereby improving the analysis effect and preventing overfitting.\n\nIn summary, by carefully dividing the dataset, monitoring model performance, and using the MIV algorithm for feature selection, we effectively prevented overfitting in our models.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models discussed in this publication, specifically the BP neural network and the PSO-BP neural network, are generally considered black-box models. This means that while they can provide accurate predictions, the internal workings and the specific reasoning behind these predictions are not easily interpretable. These neural networks operate by learning complex patterns from the data through multiple layers of processing units, making it difficult to trace back how a particular input leads to a specific output.\n\nHowever, some level of interpretability can be achieved through feature importance analysis. For instance, the Mean Increase in Variance (MIV) was used to rank risk factors for hypertension. Factors with positive MIV values were found to be positively correlated with hypertension, such as low-density lipoprotein cholesterol, cholesterol, triglycerides, daily oil intake, daily salt intake, smoking, age, BMI, and drinking. Conversely, factors with negative MIV values were negatively correlated with hypertension, including sleep duration, heart rate, high-density lipoprotein cholesterol, and hemoglobin. This approach helps in understanding which factors are most influential in the model's predictions, even if the exact mechanisms within the neural network remain opaque.\n\nIn summary, while the neural network models themselves are black-box in nature, techniques like MIV analysis provide a way to gain insights into the key factors driving the model's decisions. This partial interpretability is crucial for practical applications, especially in medical fields where understanding the underlying factors is as important as making accurate predictions.",
  "model/output": "The model is primarily a classification model designed to predict the risk of hypertension. It categorizes individuals into two groups: those with hypertension and those without. The model's performance is evaluated using metrics such as accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (AUC). These metrics indicate how well the model can distinguish between the two classes.\n\nThe model uses a backpropagation neural network (BP neural network) optimized by a particle swarm optimization (PSO) algorithm. This optimization aims to improve the model's predictive performance by fine-tuning its parameters. The PSO-BP neural network was compared with a standard BP neural network and a logistic regression model to assess its effectiveness.\n\nIn the evaluation, the PSO-BP neural network demonstrated superior performance with an AUC of 0.85815, indicating good predictive accuracy. The model correctly predicted 76.56% of positive cases (hypertension) and 86.27% of negative cases (non-hypertension) in the test set, resulting in an overall accuracy of 85.38%. This suggests that the model is effective in classifying individuals based on their risk of hypertension.\n\nThe risk factors identified by the model include both positive and negative correlates of hypertension. Positive correlates, which increase the likelihood of hypertension, include cholesterol, low-density lipoprotein cholesterol, daily oil intake, triglycerides, daily salt intake, smoking, age, body mass index (BMI), and drinking. Negative correlates, which decrease the likelihood of hypertension, include sleep duration, high-density lipoprotein cholesterol, and hemoglobin.\n\nThe model's output provides valuable insights into the risk factors associated with hypertension, which can be used for preventive measures and early intervention. The comparison with other models highlights the advantages of using the PSO-BP neural network for predicting hypertension risk.",
  "model/duration": "The execution time for the models varied significantly. The BP neural network required approximately 10 seconds to achieve its best performance, iterating only 4 times. In contrast, the PSO-BP neural network needed to iterate the maximum number of times initially set, which took around 500 seconds to reach its optimal performance. This difference in execution time highlights the trade-off between the simplicity and speed of the BP neural network and the enhanced performance but increased computational demand of the PSO-BP neural network. The longer training time for the PSO-BP neural network is due to the additional optimization steps required to achieve better predictive accuracy.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the prediction models involved several key metrics to assess both the fitting ability and the prediction performance. The root mean square error and the coefficient of determination were used to evaluate the fitting ability of the models. For assessing the prediction performance, metrics such as accuracy, sensitivity, specificity, and the area under the receiver operating characteristic (ROC) curve (AUC) were employed.\n\nThe BP neural network and the PSO-BP neural network were trained and evaluated using a validation set and a test set. The training process involved monitoring the model prediction errors, which gradually stabilized as the number of iterations increased. The BP neural network showed a minimum mean square error of approximately 0.11156, while the PSO-BP neural network achieved a lower minimum mean square error of 0.085778, indicating better performance.\n\nThe AUC values were crucial in evaluating the models' predictive performance. The BP neural network had an AUC of 0.76103, indicating a general level of effectiveness. In contrast, the PSO-BP neural network demonstrated a superior AUC of 0.85815, suggesting a good predictive performance. The logistic regression model, however, had an AUC of 0.48749, which was below 0.5, indicating that its predictive performance was worse than random guessing.\n\nThe accuracy, sensitivity, and specificity were also calculated for the test sets. The BP neural network achieved an overall accuracy of 80.56%, with a sensitivity of 13.82% and a specificity of 97.70%. The PSO-BP neural network performed better with an overall accuracy of 85.38%, a sensitivity of 43.90%, and a specificity of 96.66%. The logistic regression model had the lowest performance metrics, with an accuracy of 55.81%, a sensitivity of 58.54%, and a specificity of 55.11%.\n\nIn summary, the evaluation methods included a comprehensive analysis of fitting ability and prediction performance using multiple metrics. The PSO-BP neural network consistently outperformed the other models, demonstrating its robustness and reliability in predicting hypertension.",
  "evaluation/measure": "In the evaluation of our models, we reported several key performance metrics to assess their effectiveness in predicting hypertension. These metrics include accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (AUC).\n\nAccuracy measures the overall correctness of the model's predictions, indicating the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, also known as the true positive rate, evaluates the model's ability to correctly identify positive cases (hypertension). Specificity, or the true negative rate, assesses the model's capability to correctly identify negative cases (non-hypertension). The AUC provides a single scalar value that summarizes the performance of the model across all classification thresholds, with higher values indicating better performance.\n\nThe reported metrics are representative of standard practices in the literature for evaluating predictive models, particularly in the context of medical diagnostics. Accuracy, sensitivity, and specificity are commonly used to evaluate the performance of classification models, while the AUC is a widely accepted metric for assessing the overall effectiveness of a model's predictions. These metrics collectively provide a comprehensive view of the model's performance, ensuring that both the true positive and true negative rates are considered, along with the model's ability to discriminate between positive and negative cases.\n\nIn our study, we compared three models: the BP neural network, the PSO-BP neural network, and logistic regression. The BP neural network achieved an accuracy of 80.56%, sensitivity of 13.82%, specificity of 97.70%, and an AUC of 0.76103. The PSO-BP neural network demonstrated superior performance with an accuracy of 85.38%, sensitivity of 43.90%, specificity of 96.66%, and an AUC of 0.85815. In contrast, the logistic regression model had an accuracy of 55.81%, sensitivity of 58.54%, specificity of 55.11%, and an AUC of 0.48749, indicating poor predictive performance.\n\nThese metrics highlight the strengths and weaknesses of each model, with the PSO-BP neural network showing the best overall performance. The reported metrics are consistent with those used in similar studies, ensuring that our evaluation is both rigorous and comparable to existing research in the field.",
  "evaluation/comparison": "In the evaluation of our models, we conducted a comprehensive comparison with both traditional and simpler baseline methods to assess the performance of our proposed PSO-BP neural network. We compared our model against a standard BP neural network and a logistic regression model. The logistic regression model served as a simpler baseline, providing a straightforward comparison to highlight the advantages of neural network approaches.\n\nFor the BP neural network, we evaluated its performance using metrics such as accuracy, sensitivity, specificity, and the area under the curve (AUC). The BP neural network achieved an accuracy of 80.56%, a sensitivity of 13.82%, a specificity of 97.70%, and an AUC of 0.76103. These metrics indicated a general performance level, but there was room for improvement.\n\nThe logistic regression model, which is a traditional statistical method, had an AUC of 0.48749, indicating that its predictive performance was worse than random guessing. This underscored the limitations of traditional models in handling complex, nonlinear relationships in the data.\n\nIn contrast, the PSO-BP neural network demonstrated superior performance. It achieved an accuracy of 85.38%, a sensitivity of 43.90%, a specificity of 96.66%, and an AUC of 0.85815. These results indicated that the PSO-BP neural network had a better ability to capture the underlying patterns in the data, leading to more accurate predictions.\n\nAdditionally, we compared the root mean square error (RMSE) and the coefficient of determination (R\u00b2) between the BP neural network and the PSO-BP neural network. The PSO-BP neural network had a lower RMSE of 0.09 compared to 0.34 for the BP neural network, and a higher R\u00b2 value of 0.29 compared to 0.16. These metrics further confirmed the superior fitting ability and stability of the PSO-BP neural network.\n\nOverall, the comparison with publicly available methods and simpler baselines demonstrated that the PSO-BP neural network offers significant improvements in prediction performance, making it a more reliable tool for hypertension risk assessment.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}