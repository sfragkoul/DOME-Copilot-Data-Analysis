{
  "publication/title": "Cross-modal contrastive learning for unified placenta analysis using photographs.",
  "publication/authors": "Pan Y, Mehta M, Goldstein JA, Ngonzi J, Bebell LM, Roberts DJ, Carreon CK, Gallagher K, Walker RE, Gernand AD, Wang JZ",
  "publication/journal": "Patterns (New York, N.Y.)",
  "publication/year": "2024",
  "publication/pmid": "39776848",
  "publication/pmcid": "PMC11701861",
  "publication/doi": "10.1016/j.patter.2024.101097",
  "publication/tags": "- Placenta\n- Artificial Intelligence\n- Machine Learning\n- Medical Imaging\n- Data Analysis\n- Biomedical Engineering\n- Sepsis\n- Meconium\n- Image Artifacts\n- Clinical Outcomes",
  "dataset/provenance": "The dataset used in this study was primarily collected from the pathology department at Northwestern Memorial Hospital (NMH) in Chicago, IL, USA. The collection period spanned from January 1, 2010, to December 31, 2022. The photographs were taken using a dedicated pathology specimen photography system, specifically the Macropath system from Milestone Medical, which includes an integrated, fixed camera and built-in lighting to minimize technical variability. Pathologists generated the pathology reports based on histological findings and widely adopted definitions.\n\nThe refined dataset from NMH comprises 31,763 fetal-side placenta images, each accompanied by a pathology report. From this dataset, 2,811 image-report pairs from the year 2017 were selected for internal validation, while the remaining images were used for pre-training. Additionally, 166 cases where the neonate was diagnosed with sepsis and 1,837 potential negative cases were identified from the internal validation set.\n\nFor external validation, the dataset was collected at the Mbarara Regional Referral Hospital (MRRH) in Mbarara, Uganda, between December 1, 2019, and November 30, 2023. This collection was part of the Placentas, Antibodies, and Child Outcomes study and utilized a Fuji\ufb01lm FinePix XP130 digital camera. The pathology reports for this dataset were generated using the same method as described for the primary dataset. A total of 353 placenta and pathology report pairs were obtained from MRRH for external validation.\n\nThe AI-based placental assessment and examination (AI-PLAX) algorithm was used to mask the background of each image in the NMH dataset. Similarly, the automatic and interactive segment anything model (AI-SAM) algorithm was employed to mask the background of each image in the MRRH dataset. For the internal validation set, images were manually checked to ensure the placenta was complete and its visibility was unobscured. Each image was labeled according to the diversity of the training dataset.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in this study are not publicly released. They are owned by individual sites and shared with Penn State through established data use agreements. The source code for the project is available on GitHub and has been archived on Zenodo. Any additional information required to reanalyze the data reported in this paper is available from the lead contact upon request. This ensures that the data are accessible for verification and further research while maintaining the necessary privacy and security protocols.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages advanced machine learning techniques tailored for placental analysis. Specifically, we utilized a model referred to as PlacentaCLIP+, which integrates both internal and external representations to enhance its capabilities. This approach synchronizes these representations, allowing for more robust and generalizable performance across various scenarios.\n\nThe machine-learning algorithm class used is a type of deep learning model, which benefits from pre-trained language models to extract external knowledge without relying on human intervention. This method improves the model's generalizability, particularly in situations where expert knowledge is limited or hard to obtain.\n\nThe algorithm is not entirely new but represents a significant advancement over previous methods. It builds upon existing techniques by incorporating novel data integration and evaluation protocols. The decision to publish this work in a broader scientific journal rather than a specialized machine-learning journal was driven by the interdisciplinary nature of the research. The focus is on the application of machine learning in medical imaging and placental analysis, which aligns more closely with the scope of journals that cover biomedical engineering and related fields.\n\nThe integration of these advanced techniques allows for a more comprehensive and robust evaluation of placental features, contributing to the field's understanding and potential clinical applications.",
  "optimization/meta": "The model does not operate as a traditional meta-predictor that combines predictions from multiple machine-learning algorithms. Instead, it integrates various modules and techniques to enhance placenta analysis. These modules include pre-aligned encoders, cross-modal distillation, and cross-modal retrieval, each contributing to different aspects of performance and robustness.\n\nThe pre-aligned encoders help in synchronizing internal and external representations, while cross-modal distillation enhances the performance of the image encoder. Cross-modal retrieval improves the model's robustness. The use of additional data, denoted as PDR+, further augments the model's capabilities.\n\nThe training data for these modules is designed to be independent, ensuring that the model's performance is not biased by overlapping datasets. This independence is crucial for the generalizability of the approach, particularly in scenarios where expert knowledge is scarce or difficult to obtain.\n\nThe model's performance is evaluated using a robustness evaluation protocol and a cross-national dataset, which includes a variety of placental features and conditions. This comprehensive evaluation ensures that the model can handle diverse and challenging real-world scenarios.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithm. For image data, we employed two algorithms for background masking: AI-PLAX for the primary dataset and AI-SAM for the external validation dataset. AI-SAM was preferred due to its robustness to domain shifts and capability for interactive modifications. All images were resized to 512x384 pixels to preserve content, and during pre-training, we applied random augmentations including brightness, contrast, saturation, hue adjustments, and rotations up to 180 degrees. For fine-tuning and validation, no augmentations were used.\n\nPathology reports underwent a simple preprocessing procedure. They were split by anomalies and stored as sets, with irrelevant text removed using keyword matching. During model training, bootstrap sampling was performed from these sets, and sampled items were concatenated into complete sentences.\n\nThe image encoder used was ResNet50, and the text encoder was a transformer model. Additionally, a BERT model trained in a self-supervised manner on the MEDLINE/PubMed corpus was used for cross-modal distillation. This approach ensured that both visual and textual data were effectively encoded and aligned, facilitating meaningful relationships between placental features and pathology reports.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The fitting method employed in our study involved a comprehensive approach to ensure both the robustness and generalizability of our model. The model, PlacentaCLIP+, was trained on a substantial dataset comprising 28,952 image-text pairs, which significantly expanded the training scope compared to the initial model, PlacentaCLIP. This extensive dataset helped mitigate the risk of overfitting by providing a diverse range of examples that the model could learn from.\n\nTo further address the potential for overfitting, we implemented several strategies. Firstly, we introduced additional regularization techniques within the model architecture. These techniques helped in preventing the model from becoming too complex and thus reduced the likelihood of overfitting. Secondly, we ensured that the training dataset was diverse, which is crucial for training models that can generalize well to unseen data. Additionally, we used a validation set to monitor the model's performance during training, allowing us to adjust hyperparameters and stop training when performance on the validation set began to degrade.\n\nOn the other hand, underfitting was addressed by carefully designing the model architecture and ensuring that it had sufficient capacity to learn the underlying patterns in the data. The use of a cross-modal retrieval module during the pre-training phase aided in capturing rich feature representations, which were essential for the downstream classification tasks. Furthermore, the model's performance was evaluated on multiple metrics, including AUC, mAP, and the Brier score, across different tasks and datasets. This thorough evaluation process helped in identifying any signs of underfitting and ensured that the model was adequately capturing the necessary information from the data.\n\nIn summary, the fitting method involved a balance between model complexity and regularization, coupled with a diverse and extensive training dataset. These measures collectively helped in ruling out both overfitting and underfitting, ensuring that the model performed robustly across various scenarios.",
  "optimization/regularization": "In our study, we employed several techniques to enhance the robustness of our model and prevent overfitting. One of the key methods involved increasing the diversity of the training dataset. A more diverse dataset helps the model generalize better to unseen data, reducing the risk of overfitting to specific patterns in the training data.\n\nAdditionally, we introduced regularization into the model architecture. Regularization techniques, such as adding dropout layers or using weight decay, help to prevent the model from becoming too complex and overfitting the training data. These methods encourage the model to learn more generalizable features, thereby improving its performance on validation and test datasets.\n\nWe also conducted a comprehensive robustness evaluation protocol. This protocol involved introducing various image artifacts to observe the performance drop, which helped us understand the model's sensitivity to different types of corruption. This evaluation provided insights into the model's robustness and guided us in developing application guidelines to mitigate the impact of common image artifacts.\n\nFurthermore, we assessed the model's performance across different demographic groups to identify any potential biases. This analysis ensured that our model performed consistently across various subgroups, reducing the risk of overfitting to specific demographic patterns.\n\nOverall, these techniques collectively contributed to enhancing the model's robustness and generalizability, ensuring that it performs well in real-world scenarios.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model's interpretability is a key aspect of its design, aiming to provide transparency and insights into its decision-making processes. Unlike black-box models, our approach incorporates mechanisms that allow for a clearer understanding of how predictions are made.\n\nOne of the primary ways we achieve interpretability is through the visualization of attention weights. These weights illustrate how different parts of the input image are related to specific text queries, offering a window into the model's internal workings. By examining these attention weights, users can see which features of the image are most influential in the model's predictions, thereby demystifying the decision-making process.\n\nAdditionally, the model's robustness evaluation provides further insights into its interpretability. By assessing how various image artifacts affect performance, we can identify which types of corruptions are most detrimental. For instance, it was observed that lossy image compression techniques like JPEG significantly degrade performance, while factors such as shadows and contrast changes have a relatively minor impact. This information not only guides users on best practices for image capture but also highlights the model's sensitivity to different types of image distortions.\n\nThe cross-modal retrieval module also plays a crucial role in interpretability. By aiding the pre-training process, this module helps in aligning image and text features more effectively. The visualization of attention weights from text queries to image features demonstrates the changes in the feature space, providing a clearer picture of how the model integrates multimodal information.\n\nOverall, the model's design emphasizes transparency, making it possible to understand and trust its predictions. The use of attention weights, robustness evaluations, and cross-modal retrieval mechanisms collectively contribute to a more interpretable and user-friendly model.",
  "model/output": "The model is designed for classification tasks. Specifically, it addresses two primary tasks: pre-training and downstream classification. In the pre-training phase, the objective is to learn a function using another function. For the downstream tasks, the model focuses on identifying specific placental features such as meconium, FIR (fetal inflammatory response), and MIR (maternal inflammatory response). The model's performance is evaluated using linear classification methods, with a focus on achieving high accuracy in these classification tasks.\n\nThe model's robustness was assessed by introducing various image artifacts and observing the performance drop. This evaluation helped in understanding how different types of image corruptions affect the model's ability to classify placental features accurately. The results indicated that while individual artifacts consistently impact performance, the additional application of artifacts rarely affects performance significantly. The model demonstrated the highest robustness in the sepsis classification task, with only a small percentage of combined variables showing a statistically significant effect on performance. Conversely, the model exhibited the least robustness in the meconium classification task, where a higher percentage of combined variables showed a statistically significant effect.\n\nThe analysis also included an evaluation of the model's performance across different demographic groups. It was observed that the 'Unknown' group had a significantly higher positive sample rate for sepsis compared to the 'White' group. This disparity in class distribution contributed to the performance difference, as the model performs best on the sepsis classification task.\n\nIn summary, the model is a classification model designed to identify specific placental features. Its performance and robustness were evaluated under various conditions, providing insights into its strengths and areas for improvement.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for our placenta analysis model is publicly available on GitHub. It can be accessed at the following URL: https://github.com/ymp5078/PlacentaCLIP. Additionally, the source code has been archived on Zenodo for long-term preservation and accessibility. This ensures that researchers and developers can easily access, review, and build upon our work. The code is released under a permissive license, allowing for broad use and modification, which facilitates collaboration and further development in the field of placental assessment.",
  "evaluation/method": "The evaluation of our method involved a comprehensive protocol designed to assess both performance and robustness. We utilized a cross-national dataset to validate our approach, ensuring that it generalizes well across different populations. The performance was measured using the average AUC score for each set of modules on the original primary dataset. To evaluate robustness, we employed a corrupted primary dataset, which allowed us to assess how well our model handles real-world variations and artifacts.\n\nOur evaluation included several key components. First, we conducted pairwise t-tests to compare the AUC between different demographic groups, ensuring that our model performs consistently across diverse populations. Additionally, we compared the performance of various module configurations, including the use of pre-aligned encoders, cross-modal distillation, and cross-modal retrieval. These comparisons helped us understand the contributions of each component to the overall performance and robustness of the model.\n\nWe also introduced a dynamic and adaptive evaluation protocol, inspired by concepts similar to Autoaugment, to generate meaningful combinations of aspects for a more thorough assessment. This protocol allowed us to explore the model's capabilities under different conditions and identify areas for improvement.\n\nFurthermore, we evaluated the model's performance on an external validation dataset comprising images of varying quality. This dataset provided valuable insights into real-world model performance, although it lacked sufficient diversity across demographic groups and devices. We also noted that our study focused exclusively on the fetal side of the placenta, without incorporating additional clinical data or considering the maternal side. Future work could address these limitations by expanding the diversity of the training dataset and introducing additional regularization into the model architecture.",
  "evaluation/measure": "In our evaluation, we primarily focused on the Area Under the Curve (AUC) score to measure performance. This metric was chosen for its robustness in evaluating the discriminative power of our models across various tasks. We reported the average AUC score for each set of modules on both the original primary dataset and a corrupted version of it, the latter being used for robustness evaluation.\n\nAdditionally, we utilized the mean Average Precision (mAP) and the Brier score to provide a more comprehensive assessment of model performance. The mAP metric was particularly useful for evaluating the precision of our models at different recall levels, while the Brier score offered insights into the calibration of predicted probabilities.\n\nThese metrics are widely recognized and used in the literature, ensuring that our evaluation is representative and comparable to other studies in the field. The AUC score, in particular, is a standard metric for binary classification tasks, making it an appropriate choice for our evaluation. The inclusion of mAP and the Brier score further enhances the robustness of our performance assessment, providing a holistic view of our models' capabilities.",
  "evaluation/comparison": "In our evaluation, we did not directly compare our approach to publicly available methods on benchmark datasets. Instead, we focused on advancing placenta analysis by introducing new data, models, and evaluation techniques. Our method synchronizes both internal and external representations, which sets it apart from previous approaches. We also extracted external knowledge from pre-trained language models without relying on expert input, enhancing our approach's generalizability, especially in scenarios where expert knowledge is scarce.\n\nWe conducted a detailed comparison of different modules within our framework. Specifically, we examined the impact of pre-aligned encoders, cross-modal distillation, and cross-modal retrieval on model performance and robustness. These comparisons were made using the same image encoder to ensure fairness. The performance was measured using the average AUC score on the original primary dataset, while robustness was assessed using the average AUC score on a corrupted primary dataset through our robustness evaluation protocol.\n\nAdditionally, we performed pairwise t-tests to compare the AUC between different sets of modules. This analysis helped us understand how various components of our model contribute to its overall performance and robustness. For instance, cross-modal retrieval improved robustness, while cross-modal distillation enhanced the performance of the image encoder. These findings provide insights into the strengths and weaknesses of different model configurations.\n\nWe also evaluated the model's performance across different demographic groups using pairwise t-tests. This analysis helped us identify any disparities in model performance and highlighted areas where further improvement is needed. However, it is important to note that our external validation dataset lacked sufficient diversity across demographic groups and devices, which is a limitation of our study.\n\nIn summary, while we did not perform a direct comparison with publicly available methods on benchmark datasets, our evaluation provides a comprehensive analysis of the internal workings of our model. This includes comparisons with simpler baselines and an examination of the model's robustness and performance across different scenarios.",
  "evaluation/confidence": "In our evaluation, we have conducted a thorough analysis to ensure the confidence and statistical significance of our results. We have included confidence intervals for our performance metrics, which are crucial for understanding the variability and reliability of our model's performance.\n\nStatistical significance is a key aspect of our evaluation. We have performed an Analysis of Variance (ANOVA) to assess the impact of various factors on model performance. This analysis helps us identify which variables have a statistically significant effect, typically with a P-value less than 0.05. For instance, in the sepsis classification task, we found that individual artifacts such as blood, glare, shadow, defocus blur, motion blur, zoom blur, contrast, brightness, saturation, and JPEG compression all have a significant impact on performance. Additionally, certain combined effects, like the interaction between glare and defocus blur, also show statistical significance.\n\nWe have also compared the performance across different demographic groups using pairwise t-tests. The corrected p-values from these tests indicate the statistical significance of the differences observed between groups. For example, the comparison between the 'Unknown' and 'White' groups for sepsis classification shows a statistically significant difference, highlighting the impact of class distribution on model performance.\n\nOverall, our evaluation includes robust statistical methods to ensure that our claims about the superiority of our method are well-supported. The use of confidence intervals and statistical tests provides a solid foundation for interpreting our results and comparing them with baselines and other methods.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, the source code used for the evaluation is accessible on GitHub and has been archived on Zenodo. Any additional information required to reanalyze the data reported in the paper is available from the lead contact upon request. This approach ensures that other researchers can replicate and build upon the work while maintaining data privacy and security."
}