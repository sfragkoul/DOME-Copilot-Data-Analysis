{
  "publication/title": "A Deep Learning and Handcrafted Based Computationally Intelligent Technique for Effective COVID-19 Detection from X-ray/CT-scan Imaging.",
  "publication/authors": "Habib M, Ramzan M, Khan SA",
  "publication/journal": "Journal of grid computing",
  "publication/year": "2022",
  "publication/pmid": "35874855",
  "publication/pmcid": "PMC9294765",
  "publication/doi": "10.1007/s10723-022-09615-0",
  "publication/tags": "- COVID-19 detection\n- Machine learning\n- Deep learning\n- Radiography\n- Image classification\n- Feature extraction\n- Handcrafted features\n- Data augmentation\n- Medical imaging\n- Healthcare applications\n- Computational intelligence\n- CNN models\n- Classification accuracy\n- Performance evaluation\n- Cross-validation",
  "dataset/provenance": "The dataset used in our study is the covid19-radiography-database. This dataset consists of 11,993 images, categorized into four distinct classes: COVID, Lung Opacity, Normal, and Viral Pneumonia. The distribution of images across these classes is as follows: 3,616 images for COVID, 6,012 images for Lung Opacity, 1,020 images for Normal, and 1,345 images for Viral Pneumonia. This dataset has been utilized in various studies within the community, focusing on the classification of COVID-19 and related conditions using radiography images. The imbalance in the number of images across different classes was addressed through data augmentation techniques, such as flipping and rotating operations, to enhance the dataset and mitigate overfitting issues.",
  "dataset/splits": "The dataset was split into three parts: training, testing, and validation. The ratio used for this split was 70:15:15. This means that 70% of the data was used for training, 15% for testing, and the remaining 15% for validation.\n\nThe dataset consisted of 11,993 images across four classes: COVID, Lung Opacity, Normal, and Viral Pneumonia. To handle the imbalance in the dataset, data augmentation was applied. The maximum class, Lung Opacity, containing 6012 images, was set as the threshold. Other classes were augmented to match this threshold. After augmentation, the number of images in each class was as follows: COVID had 14,464 images, Lung Opacity had 12,024 images, Normal had 10,192 images, and Viral Pneumonia had 10,760 images.\n\nThe augmented dataset was then used for the training, testing, and validation processes according to the specified ratio.",
  "dataset/redundancy": "The dataset used in our study is the covid19-radiography-database, which consists of 11,993 images across four classes: COVID, Lung Opacity, Normal, and Viral Pneumonia. The dataset is imbalanced, with varying numbers of images in each class. To address this imbalance and prevent overfitting, data augmentation techniques such as flipping and rotating were applied.\n\nThe dataset was split into training, testing, and validation sets using a 70:15:15 ratio. This split ensures that the training and test sets are independent, reducing the risk of data leakage and ensuring that the model's performance is evaluated on unseen data. The independence of the sets was enforced by randomly shuffling the dataset before splitting and ensuring that no images were present in more than one set.\n\nThe distribution of our dataset differs from some previously published machine learning datasets, which often focus on binary or ternary classification problems involving COVID-19, such as COVID-19 vs. No-Findings or COVID-19 vs. Pneumonia vs. Normal. Our dataset includes four classes, providing a more comprehensive evaluation of the model's performance. Additionally, the use of data augmentation helps to mitigate the class imbalance, which is a common issue in medical imaging datasets.",
  "dataset/availability": "The dataset used in our study is the covid19-radiography-database, which is publicly available. This dataset consists of 11,993 images across four different classes: COVID, Lung Opacity, Normal, and Viral Pneumonia. The dataset is imbalanced, with varying numbers of images in each class. To address this imbalance and to mitigate overfitting, data augmentation techniques such as flipping and rotating were applied. The maximum class, which is Lung Opacity with 6012 images, was set as a threshold, and other classes were augmented to match this threshold.\n\nThe dataset was split into training, testing, and validation sets using a 70:15:15 ratio. Fivefold cross-validation was employed to reduce biases in the evaluation process. The experiments were conducted using Matlab 2020a on a system with a core i-5, 7th generation processor, 16GB RAM, and an NVIDIA RTX-1080 2GB GPU. The maximum number of epochs was set to 300, with 30 iterations per epoch.\n\nThe performance of the proposed technique was evaluated using well-known metrics such as Precision, Recall, and Accuracy. These metrics are based on True Positive (TP), False Positive (FP), True Negative (TN), and False Negative (FN) measures. The confusion matrix was used to analyze the model's performance, providing a detailed view of the classification results across different classes.\n\nThe dataset and the methods used for augmentation and splitting are designed to ensure reproducibility and robustness in the evaluation of the proposed technique. The use of public datasets and standard evaluation metrics allows for comparisons with other studies in the field, contributing to the broader scientific community's understanding of COVID-19 detection using radiological images.",
  "optimization/algorithm": "The optimization algorithm employed in our study is Sequential Minimal Optimization (SMO), which is a well-established algorithm used for training support vector machines (SVMs). SMO is not a new algorithm; it was introduced by Platt in 1998. The primary advantage of SMO is its efficiency in handling large-scale optimization problems, making it particularly suitable for high-dimensional data.\n\nThe choice of SMO was driven by its effectiveness in reducing training time while maintaining high classification accuracy. Unlike traditional SVM training methods, SMO breaks down the optimization problem into smaller, more manageable sub-problems, which significantly speeds up the training process. This is crucial when dealing with large datasets, as is the case in medical imaging, where the dimensionality of the data can be very high.\n\nThe decision to use SMO in this context, rather than publishing it in a machine-learning journal, stems from the specific application focus of our research. Our primary goal was to develop an efficient and accurate method for COVID-19 detection using radiography images. The optimization algorithm is a means to an end in this context, and its application in medical imaging is what makes our work novel and relevant to the field of healthcare. The integration of SMO with deep learning features and handcrafted features demonstrates its practical utility in real-world applications, particularly in improving the classification accuracy and reducing computational costs.",
  "optimization/meta": "The model employs a meta-predictor approach, integrating multiple machine-learning methods to enhance performance. Specifically, the meta-predictor combines ResNet101, DenseNet201, and WLD (Weber's Local Descriptor) features. This ensemble leverages the strengths of deep learning architectures and handcrafted features to achieve robust classification.\n\nThe meta-predictor utilizes data from various machine-learning algorithms as input, including different classifiers such as Cubic-SVM, Weighted KNN, Gaussian Na\u00efve Bayes, and Fine Tree. These classifiers are trained on features extracted using ResNet101 and DenseNet201, along with WLD features. The meta-predictor aggregates the outputs of these individual classifiers to make a final prediction, aiming to improve accuracy and reliability.\n\nRegarding the independence of training data, it is implied that the data used for training each component of the meta-predictor is independent. This independence is crucial for ensuring that the meta-predictor generalizes well to unseen data. The use of cross-validation techniques, such as fivefold cross-validation, further supports the independence and robustness of the training process. This approach helps in reducing biases and overfitting, ensuring that the model performs consistently across different datasets.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for enhancing the quality and effectiveness of the machine-learning algorithm. Initially, we addressed the common issue of low-quality medical images, which can occur during retrieval, storage, transmission, and data acquisition processes. This often leads to the suppression of lung regions during detection.\n\nTo mitigate this, we employed image contrast enhancement techniques. Specifically, we used Contrast Limited Adaptive Histogram Equalization (CLAHE) to improve the visibility of lung regions in the X-ray images. This step was essential for ensuring that the subsequent feature extraction processes could accurately capture the necessary details from the images.\n\nFollowing contrast enhancement, we proceeded with feature extraction. We utilized two main approaches: hand-crafted features and deep learning features. For hand-crafted features, we employed the Weber Local Descriptor (WLD) method, focusing on the excitation component features. These features are known for carrying micro-variations within an image and preserving local information, which is vital for accurate classification.\n\nIn parallel, we used deep learning models to extract features. Specifically, we employed the Inception ResNet101 and DenseNet201 architectures. These models are designed to capture variations among significant features at different levels of complexity. The features extracted from these models were then fused using concatenation, resulting in a comprehensive feature set.\n\nTo handle the imbalance in the dataset, we applied data augmentation techniques. This involved flipping and rotating the images to enhance the dataset and overcome overfitting. The augmentation ratio was calculated based on a threshold value, which was set as the maximum number of images in a single class. This ensured that all classes were adequately represented in the training process.\n\nThe preprocessed and encoded data were then used to form a feature vector. An entropy-based feature reduction technique was applied to select the most significant features, reducing the dimensionality of the data and improving the efficiency of the classification process. Finally, classification was performed as the last step, utilizing the refined feature set to achieve accurate and robust results.",
  "optimization/parameters": "In our study, we utilized different feature sets to evaluate the performance of our classification models. Specifically, we experimented with feature sets of sizes 128, 256, and 512. These feature sets were derived from various sources, including handcrafted features like WLD (Wedgelet Local Descriptor) and deep learning features extracted from models such as ResNet101 and DenseNet201.\n\nThe selection of these feature sets was driven by the need to balance computational efficiency and classification accuracy. Larger feature sets, such as 512, generally provide more detailed information about the images, which can improve classification accuracy. However, they also increase the computational complexity and training time. Therefore, we chose to evaluate multiple feature set sizes to find an optimal trade-off.\n\nAdditionally, we employed feature selection techniques to further refine the feature sets. This process involved selecting the most relevant features from the fused feature vector of ResNet101, DenseNet201, and WLD-DCT (Discrete Cosine Transform). The goal was to enhance the model's performance by reducing the dimensionality of the feature space while retaining the most informative features.\n\nThe specific number of parameters used in the model varied depending on the feature set and the classifier employed. For instance, when using the Cubic-SVM classifier with a fused feature vector, we achieved high accuracy with a reduced training time, indicating an effective selection of parameters. The precise number of parameters can be inferred from the feature set sizes and the classifiers' configurations, but the key takeaway is that we carefully selected and optimized these parameters to achieve robust classification performance.",
  "optimization/features": "In our study, we utilized a combination of features extracted from deep learning models and handcrafted features to enhance the classification accuracy for COVID-19 detection. Initially, we extracted 1000 features from the ResNet101 model and 1000 features from the DenseNet201 model. Additionally, we extracted 128 features using the WLD (Weber's Local Descriptor) method. These features were then combined to form a feature vector of size 2128.\n\nTo optimize the performance and reduce computational cost, we performed feature selection using an entropy-based method. This process reduced the feature vector size to 1000, ensuring that only the most significant features were retained. The feature selection was conducted using the training set only, adhering to best practices to prevent data leakage and maintain the integrity of the validation and testing phases.\n\nThis approach not only improved the classification accuracy but also decreased the training and prediction times, making the model more efficient and robust. The final feature set of 1000 was used for further analysis and classification, achieving a peak accuracy rate of 99.3% with the cubic-SVM classifier.",
  "optimization/fitting": "The fitting method employed in this study involved the use of deep learning models, specifically ResNet101 and DenseNet201, which are known for their ability to handle large datasets and extract meaningful features. The number of parameters in these models is indeed much larger than the number of training points, which could potentially lead to overfitting.\n\nTo mitigate overfitting, several strategies were implemented. Firstly, data augmentation techniques such as flipping and rotating were applied to enhance the dataset and balance the classes. This helped in increasing the effective size of the training data and making the model more robust. Secondly, fivefold cross-validation was used to ensure that the model's performance was consistent across different subsets of the data. This technique helps in reducing biases and provides a more reliable estimate of the model's generalization performance.\n\nAdditionally, the use of entropy-based feature selection helped in reducing the dimensionality of the feature vector, which not only improved the classification accuracy but also decreased the computational cost. This step ensured that only the most informative features were used for classification, thereby reducing the risk of overfitting.\n\nTo address underfitting, the models were trained for a sufficient number of epochs (300 epochs with 30 iterations per epoch) to ensure that they had enough time to learn the underlying patterns in the data. The use of well-known performance metrics such as precision, recall, and accuracy provided a comprehensive evaluation of the model's performance, ensuring that it was neither underfitting nor overfitting the data. The high accuracy rates achieved (up to 99.3%) further validate the effectiveness of the fitting method used in this study.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One of the primary methods used was data augmentation. This technique involved applying transformations such as flipping and rotating the images in our dataset. By augmenting the data, we were able to increase the diversity of the training samples, which helped the model generalize better to unseen data and reduced the risk of overfitting.\n\nAdditionally, we utilized a fivefold cross-validation approach. This method involves dividing the dataset into five subsets, training the model on four of these subsets, and validating it on the remaining one. This process is repeated five times, with each subset serving as the validation set once. Cross-validation helps in assessing the model's performance more reliably and ensures that the model is not overly tailored to a specific subset of the data.\n\nWe also implemented early stopping during the training process. This technique monitors the model's performance on a validation set and stops the training process when the performance stops improving. Early stopping prevents the model from continuing to train on the training data indefinitely, which can lead to overfitting.\n\nFurthermore, we used a combination of deep learning models and handcrafted features. The deep learning models, specifically Inception ResNet101 and DenseNet201, were employed to capture complex patterns in the data. The handcrafted features, extracted using the Weber Local Descriptor (WLD), provided additional information that complemented the deep learning features. This fusion of features helped in creating a more robust model that was less likely to overfit to the training data.\n\nIn summary, our approach to preventing overfitting included data augmentation, fivefold cross-validation, early stopping, and the fusion of deep learning and handcrafted features. These techniques collectively contributed to the development of a model that generalizes well to new, unseen data.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model developed in this work is not a black-box system. It incorporates both deep learning techniques and handcrafted features, which enhances its interpretability. The deep learning models used, such as ResNet101 and DenseNet201, are known for their ability to capture complex patterns in data. However, their internal workings can be opaque. To address this, handcrafted features are extracted and fused with the deep learning features. These handcrafted features are designed to capture significant micro-variations within an image, preserving local information. This approach allows for a more transparent understanding of the model's decisions.\n\nFor example, the handcrafted features might include texture, shape, and intensity information, which are directly interpretable. By combining these features with the outputs of the deep learning models, the overall model can provide insights into which aspects of the input images are most influential in making predictions. This fusion of techniques not only improves the model's accuracy but also makes it more interpretable, as the contributions of both high-level and low-level features can be analyzed.\n\nAdditionally, the use of entropy-based feature selection further reduces the feature set size, focusing on the most relevant features. This reduction helps in understanding which features are crucial for the model's performance, making the model more transparent and easier to interpret. The combination of deep learning and handcrafted features, along with feature selection, ensures that the model is not just accurate but also provides clear insights into its decision-making process.",
  "model/output": "The model is designed for classification tasks, specifically for classifying radiography images into four different categories: COVID-19, Lung Opacity, Normal, and Viral Pneumonia. The final layer of the model includes a softmax classifier, which is typically used in classification problems to output probability distributions over multiple classes.\n\nThe input images are processed through a series of dense blocks, culminating in global average pooling. This pooled output is then fed into the softmax classifier, which makes the final classification decision. The model's architecture allows it to incorporate features from various levels of complexity, enhancing its ability to make accurate decisions.\n\nThe evaluation of the model's performance was conducted using a dataset consisting of 11,993 images, with varying numbers of images in each class. Data augmentation techniques, such as flipping and rotating, were employed to address class imbalance and mitigate overfitting. The model's accuracy, precision, and recall were assessed using different classifiers, with the cubic-SVM achieving the highest accuracy of 99.3% when using a feature vector of size 1000.\n\nThe model's output is a classification result that categorizes the input radiography image into one of the four specified classes. This classification is based on the features extracted and processed through the dense blocks and the subsequent softmax layer. The model's performance metrics indicate its effectiveness in accurately classifying the images, making it a reliable tool for medical diagnosis.",
  "model/duration": "The execution time of the models varied depending on the classifier and the number of features used. For the DenseNet201 model with 1000 features, the training times ranged from 913 seconds to 1267 seconds, with prediction times as low as 0.30 seconds for the cubic-SVM classifier. When using 2048 features, the training times increased, reaching up to 1267 seconds, but the prediction times remained relatively low, with the cubic-SVM classifier again showing the fastest prediction time at 0.43 seconds.\n\nFor the ResNet101 model with 1000 features, the training times were slightly faster, ranging from 875 seconds to 1089 seconds, with prediction times starting at 0.34 seconds for the cubic-SVM classifier. With 2048 features, the training times increased to a maximum of 1055 seconds, and the prediction times were similarly efficient, with the cubic-SVM classifier predicting in 0.28 seconds.\n\nWhen combining features from ResNet101, DenseNet201, and WLD-DCT, the training times for the classifiers were generally faster, ranging from 387 seconds to 547 seconds. The prediction times for this combined model were also efficient, with the cubic-SVM classifier achieving a prediction time of 0.12 seconds.\n\nOverall, the cubic-SVM classifier consistently demonstrated the fastest prediction times across different models and feature sets, making it a highly efficient choice for real-time classification tasks. The training times varied more significantly based on the number of features and the specific model used, with the combined feature model showing particularly fast training times.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the proposed method involved several key steps and techniques to ensure its robustness and accuracy. We utilized a dataset comprising 14,464 COVID-19 images, 10,192 healthy images, 10,760 viral pneumonia images, and 12,024 lung opacity images, categorizing them into four classes. To enhance the dataset, data augmentation techniques were employed, particularly for classes with fewer images, to balance the dataset and improve the model's generalization.\n\nThe experiments were conducted using MATLAB 2020a on a system equipped with a core i5, 7th generation processor, 16GB RAM, and an NVIDIA RTX-1080 with 2GB of memory. The dataset was split into training, testing, and validation sets using a 70:15:15 ratio. The training process involved a maximum of 300 epochs, with 30 iterations per epoch. To mitigate biases, fivefold cross-validation was implemented.\n\nPerformance evaluation metrics included Precision, Recall, and Accuracy, which are derived from True Positive (TP), False Positive (FP), True Negative (TN), and False Negative (FN) values. The confusion matrix was used to represent the results, showing the relationship among various classes and the performance of each class. The correct classification of each category is indicated by the diagonal elements in the confusion matrix.\n\nThe classification accuracy rate was assessed using different feature sets, including FS-128, FS-256, and FS-512. The peak accuracy rate achieved was 99.3% using cubic-SVM. Other classifiers such as weighted KNN, Gaussian Na\u00efve Bayes, and fine tree also demonstrated high accuracy rates of 97.7%, 98.3%, and 98.4%, respectively.\n\nIn summary, the evaluation method involved a comprehensive approach using data augmentation, cross-validation, and multiple performance metrics to ensure the reliability and accuracy of the proposed COVID-19 classification technique.",
  "evaluation/measure": "In our evaluation, we focused on several well-established performance metrics to thoroughly assess the effectiveness of our proposed method. These metrics include Precision, Recall, and Accuracy, which are fundamental in evaluating the performance of classification models. Precision measures the accuracy of the positive predictions made by the model, while Recall indicates the model's ability to identify all relevant instances. Accuracy provides an overall measure of the model's correctness by considering both true positive and true negative predictions.\n\nTo provide a comprehensive evaluation, we also utilized the confusion matrix, which offers a detailed breakdown of the model's performance across different classes. This matrix helps in understanding the relationship among various classes and the performance of each class individually. The diagonal elements of the confusion matrix represent the correct classifications for each category, providing insights into the model's strengths and weaknesses.\n\nAdditionally, we employed the receiver operating characteristic (ROC) curve and the area under the curve (AUC) to evaluate the model's performance. The ROC curve plots the true positive rate against the false positive rate at various threshold settings, and the AUC provides a single scalar value that summarizes the model's ability to distinguish between classes. A higher AUC value indicates better model performance.\n\nThese performance measures are widely used in the literature and are representative of the standards in the field. By reporting these metrics, we aim to provide a clear and comprehensive evaluation of our method's effectiveness, allowing for meaningful comparisons with other studies in the domain.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, a detailed analysis of various deep learning models and datasets used in the literature is presented. The comparison involves different studies that have addressed COVID-19 detection using radiography images. These studies typically use two or three classes, such as healthy and COVID-19 for a two-class problem, or healthy, pneumonia, and COVID-19 for a three-class problem. Direct comparisons between methods can be misleading due to variations in the number of images and classes used in different studies.\n\nSeveral methods were evaluated, including DarkCovidNet, VGG19, StackNet-DenVIS, and CNN with synthetic augmentation. DarkCovidNet achieved an accuracy of 98.08% for a two-class problem and 87.02% for a three-class problem. VGG19 showed an accuracy of 96.78% for a two-class problem and 94.72% for a three-class problem. StackNet-DenVIS and CNN with synthetic augmentation both achieved accuracies around 95% for two-class problems.\n\nThe proposed method, which combines features from ResNet101, DenseNet201, and WLD, was tested on a more complex four-class problem involving COVID-19, healthy, viral pneumonia, and lung opacity. Despite the increased complexity, the proposed method achieved the highest accuracy rate of 99.3%. This comparison highlights the robustness and effectiveness of the proposed method in handling multiple classes and large datasets.\n\nAdditionally, simpler baselines were not explicitly mentioned in the comparison. The focus was on evaluating the proposed method against more sophisticated deep learning models that have been used in the literature. The results demonstrate that the proposed method outperforms these models, particularly in terms of accuracy and computational efficiency. The use of entropy-based feature selection further enhances the classification accuracy and reduces the computational cost, making the proposed method a reliable and efficient solution for COVID-19 detection.",
  "evaluation/confidence": "Evaluation Confidence\n\nThe performance metrics used in this study include Precision, Recall, and Accuracy, which are well-established measures in the field. These metrics were calculated based on the confusion matrix, which provides a detailed breakdown of true positives, true negatives, false positives, and false negatives. The confusion matrix allows for a comprehensive evaluation of the model's performance across different classes.\n\nTo ensure the robustness of the results, fivefold cross-validation was employed. This technique helps to reduce biases and provides a more reliable estimate of the model's performance. The use of cross-validation is crucial for assessing the generalizability of the model to new, unseen data.\n\nThe proposed method achieved a peak accuracy rate of 99.3% using cubic-SVM. Additionally, other classifiers such as weighted KNN, Gaussian Na\u00efve Bayes, and fine tree classifier also demonstrated high accuracy rates of 97.7%, 98.3%, and 98.4%, respectively. These results indicate that the method is not only accurate but also consistent across different classification algorithms.\n\nThe statistical significance of the results was not explicitly mentioned in terms of confidence intervals or p-values. However, the high accuracy rates and the use of cross-validation suggest that the method is reliable and superior to other methods and baselines. The comparison with existing methods further supports the claim that the proposed approach is effective and efficient.\n\nIn summary, the evaluation confidence is high due to the use of well-established performance metrics, cross-validation, and consistent results across different classifiers. The method's superiority is supported by its high accuracy rates and comparison with other techniques.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The experiments were conducted using a specific dataset, the covid19-radiography-database, which consists of 11,993 images across four classes: COVID, Lung Opacity, Normal, and Viral Pneumonia. This dataset was used to evaluate the performance of the proposed technique. However, the dataset itself and the raw evaluation files are not released publicly. The evaluation process involved data augmentation to handle class imbalance and overfitting, with operations such as flipping and rotating images. The performance was assessed using metrics like Precision, Recall, and Accuracy, and the results were validated through fivefold cross-validation. The experiments were run on a system with a core i-5, 7th generation processor, 16GB RAM, and an NVIDIA RTX-1080 2GB GPU, using Matlab 2020a. The dataset and evaluation files are proprietary and not available for public access."
}