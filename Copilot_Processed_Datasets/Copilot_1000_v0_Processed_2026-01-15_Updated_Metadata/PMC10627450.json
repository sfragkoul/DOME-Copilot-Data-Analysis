{
  "publication/title": "Machine Learning to Predict Interstage Mortality Following Single Ventricle Palliation: A NPC-QIC Database Analysis.",
  "publication/authors": "Sunthankar SD, Zhao J, Wei WQ, Hill GD, Parra DA, Kohl K, McCoy A, Jayaram NM, Godown J",
  "publication/journal": "Pediatric cardiology",
  "publication/year": "2023",
  "publication/pmid": "36820914",
  "publication/pmcid": "PMC10627450",
  "publication/doi": "10.1007/s00246-023-03130-z",
  "publication/tags": "- Machine Learning\n- Pediatric Cardiology\n- Interstage Mortality\n- Hypoplastic Left Heart Syndrome\n- Risk Prediction\n- Gradient Boosting Trees\n- LightGBM\n- Random Forest\n- Logistic Regression\n- SHAP (Shapley Additive Explanations)\n- Multicenter Studies\n- Congenital Heart Disease\n- Predictive Modeling\n- Home Interstage Mortality\n- Feature Importance\n- Risk Stratification\n- Pediatric Heart Surgery\n- Clinical Outcomes\n- Data Analysis\n- Medical Research",
  "dataset/provenance": "The dataset utilized in this study is sourced from the National Pediatric Cardiology Quality Improvement Collaborative (NPC-QIC) registry. This registry is a multi-center database that includes data from over 60 institutions across the United States, Canada, and the United Kingdom. It serves as part of a learning health network aimed at reducing mortality and morbidity among patients with single ventricle congenital heart disease.\n\nThe study included a total of 3267 patients who underwent stage I single ventricle palliation and survived to hospital discharge between 2008 and 2019. These patients were drawn from 67 different centers. The dataset consists of a wide range of variables, including demographic, operative, hospitalization, and discharge data. Any variable with more than 50% missing data was removed from the analysis. For the remaining variables, missing data were imputed using logistic regression, random forest models, and gradient boosting models. The dataset was further processed by scaling the data into a standard scale and converting integer-encoded categorical features into one-hot encoding.\n\nThe registry includes detailed data definitions, data audits, and data quality checks to ensure the accuracy and reliability of the information. Data are securely housed at the James M. Anderson Center for Health System Excellence at Cincinnati Children\u2019s Hospital Medical Center. The registry has been previously used in various studies and by the community to analyze outcomes and improve care for patients with single ventricle physiology.",
  "dataset/splits": "The dataset was divided into two main cohorts: a training set and a test set. The subjects were randomly split, with 80% allocated to the training cohort and 20% to the test cohort. To minimize overfitting, the training set was further divided into five folds. In this cross-validation process, four folds were used for training, while the remaining fold was used to determine the stop point of learning for models like XGBoost and LightGBM. This splitting process was repeated five times to assess the model's robustness in predictive capability. The mean and 95% confidence interval of the metrics were reported to ensure the reliability of the results.",
  "dataset/redundancy": "The dataset was divided into training and test cohorts, with 80% of the subjects allocated to the training set and 20% to the test set. This split was performed randomly to ensure independence between the two sets. To mitigate overfitting, the training set was further divided into five folds using a cross-validation approach. In each fold, four parts were used for training, while the remaining part was used to determine the stopping point for learning, particularly for the XGBoost and LightGBM models. This process was repeated five times to assess the robustness of the models in terms of their predictive capability. The mean and 95% confidence interval of the metrics were reported to provide a comprehensive evaluation of the models' performance.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the field, particularly those focused on health outcomes. The use of a multicenter registry ensured a diverse and representative sample, which is crucial for developing generalizable models. The inclusion of a wide range of variables, totaling 180, allowed for a thorough analysis of various factors influencing interstage mortality. This approach aligns with contemporary practices in machine learning, where the goal is to leverage large, diverse datasets to build robust and reliable predictive models. The random splitting and cross-validation techniques employed in this study are standard practices designed to enhance the model's ability to generalize to new, unseen data.",
  "dataset/availability": "Not applicable.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the class of ensemble tree methods. Specifically, we employed random forest, XGBoost trees, gradient boosting trees (GBT), and LightGBM. These algorithms are well-established and widely used in the field of machine learning for their ability to handle complex datasets and provide robust predictive performance.\n\nThe algorithms used are not new; they have been extensively studied and applied in various domains, including health outcome prediction tasks. The choice of these algorithms was driven by their demonstrated effectiveness in similar contexts and their ability to handle the complexity of the data at hand.\n\nThe decision to use these established algorithms in a medical context, rather than a machine-learning journal, is due to the specific focus of our study. Our primary goal was to evaluate the predictive performance of these models in the context of pediatric cardiology, particularly for predicting home interstage mortality. The medical community benefits from understanding how these algorithms can be applied to improve patient outcomes, rather than the development of new machine-learning techniques. Therefore, publishing in a pediatric cardiology journal allows us to reach the relevant audience and highlight the practical applications of these models in clinical settings.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "For the machine-learning algorithms, the data underwent several preprocessing steps to ensure optimal performance. Initially, any variable with more than 50% missing data was removed from the analysis. For the remaining variables, missing data were imputed using logistic regression, random forest models, and gradient boosting models. The SimpleImputer with median was used to fill in missing values, and the data were then scaled to a standard scale. Categorical features that were integer-encoded were transformed into one-hot encoding to facilitate better model training.\n\nTwo specific models, Light Gradient Boosting Machine (LightGBM) and Extreme Gradient Boosting Machine (XGBoost), have built-in capabilities to handle missing values, so imputation was not required for these models. This approach ensured that the data was clean and ready for training the machine-learning models, allowing for more accurate and reliable predictions.",
  "optimization/parameters": "In our study, we utilized a total of 180 variables as input parameters for the machine learning models. These variables encompassed a wide range of demographic, operative, and post-operative characteristics, which were carefully selected to capture the complexity and heterogeneity of the patient population under investigation.\n\nThe selection of these parameters was driven by a comprehensive review of existing literature and clinical expertise. We aimed to include variables that have been previously identified as potential risk factors for interstage mortality, as well as those that emerged from exploratory data analysis. This approach ensured that our models were robust and capable of capturing the intricate relationships between various factors and the outcome of interest.\n\nTo further refine the selection of input parameters, we employed feature importance techniques, specifically Shapley additive explanations (SHAP) values. This method allowed us to quantify the contribution of each feature to the model's predictions, thereby identifying the most influential variables. The top 20 most influential features, as determined by their mean absolute SHAP values, were highlighted in our analysis, providing insights into the key drivers of interstage mortality.\n\nBy leveraging a combination of domain knowledge and data-driven feature selection, we were able to construct models that not only performed well in predictive tasks but also offered interpretable results, enhancing their clinical utility.",
  "optimization/features": "The study utilized a total of 180 variables as input features for the machine learning models. Feature selection was not explicitly mentioned as a separate process, suggesting that all 180 variables were considered in the model development. The training set was used for model development, including determining the stop point of learning for certain models, ensuring that the feature selection, if any, was likely performed using the training data only. This approach helps to maintain the integrity of the test set for unbiased evaluation of the models' performance.",
  "optimization/fitting": "The study involved evaluating multiple machine learning models, including logistic regression, random forest, XGBoost trees, gradient boosting trees (GBT), and LightGBM. To address the potential issue of overfitting, especially given the complexity of some models and the relatively small number of home interstage mortality cases (208 out of 3267 patients), several strategies were employed.\n\nFirstly, the dataset was split into training and test cohorts, with 80% of the subjects used for training and 20% for testing. The training set was further divided into five folds, using a technique known as cross-validation. In each fold, four parts were used for training, and one part was used to determine the stop point of learning for models like XGBoost and LightGBM. This process was repeated five times to assess the model's robustness and predictive capability. By doing so, the models were trained on different subsets of the data, reducing the risk of overfitting to any single subset.\n\nAdditionally, isotonic regression was applied to the validation set to ensure that the models were well-calibrated. This step helped in adjusting the predicted probabilities to better match the actual outcomes, thereby improving the model's reliability.\n\nTo evaluate the performance of each model, multiple standard metrics were used, including the area under the receiver operating characteristic (AUROC), accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). The cut-off threshold of risk probability was determined by Youden's J statistic, which maximizes the combined sensitivity and specificity. This approach ensured that the models were not only accurate but also balanced in their predictions.\n\nThe use of Shapley additive explanations (SHAP) for estimating feature importance in tree-based models further aided in understanding the contribution of each feature to the predictions. This method provided a global importance measure by averaging the absolute Shapley values per feature across the data, helping to identify the most influential factors.\n\nIn summary, the study employed cross-validation, isotonic regression, and SHAP values to mitigate overfitting and ensure that the models were well-calibrated and robust. The use of multiple performance metrics and the determination of the cut-off threshold by Youden's J statistic helped in balancing the models' predictions, thereby addressing both overfitting and underfitting concerns.",
  "optimization/regularization": "To minimize overfitting, several techniques were employed during the model development process. The training set was split into five folds, with four folds used for training and one fold reserved for determining the stop point of learning. This approach, known as cross-validation, helps to ensure that the model generalizes well to unseen data. Additionally, the random splitting process was repeated five times to further assess the model's robustness in terms of predictive capability. This repetition helps to provide a more reliable estimate of the model's performance by averaging out the variability introduced by a single split.\n\nMoreover, isotonic regression was applied on the validation set to ensure that the models were well-calibrated. This technique adjusts the predicted probabilities to better match the observed outcomes, reducing the risk of overfitting and improving the model's reliability. By implementing these regularization methods, the models were better equipped to handle the complexity of the data and provide more accurate predictions.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models evaluated in our study, particularly the tree-based ensemble methods like XGBoost and LightGBM, are often considered black-box models due to their complexity. However, to enhance interpretability, we employed Shapley Additive Explanations (SHAP) values. SHAP values provide a way to attribute the output of a machine learning model to its input features, making it possible to understand the contribution of each feature to the model's predictions.\n\nSHAP values are based on game theory and offer a unified measure of feature importance. By calculating the SHAP values for each feature, we can determine how much each feature contributes to the prediction of interstage mortality. This method allows us to identify the most influential features and understand their impact on the model's output.\n\nFor instance, in our study, the use of digoxin at the time of hospital discharge was identified as the most influential factor in predicting interstage mortality, as it had the highest mean SHAP value. Other significant features included the use of a Sano conduit during stage I palliation, gestational age, birth weight, and sex. These insights help in understanding the key drivers of interstage mortality and can guide clinical decision-making.\n\nAdditionally, we used SHAP summary plots to visualize the feature effects on individual predictions. These plots provide a global view of feature importance and show how each feature's value affects the model's predictions. This visualization helps in interpreting the model's behavior and understanding the relationships between features and outcomes.\n\nIn summary, while the tree-based ensemble models used in our study are complex and can be considered black-box models, the use of SHAP values enhances their interpretability. By providing a clear and quantitative measure of feature importance, SHAP values help in understanding the model's predictions and identifying the key factors associated with interstage mortality.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the binary outcome of home interstage mortality, which is a classification task rather than a regression task. The model evaluates various patient characteristics and clinical factors to determine the likelihood of mortality during the interstage period following surgical palliation for congenital heart disease.\n\nThe performance of the model was assessed using multiple standard metrics for classification tasks, including the area under the receiver operating characteristic curve (AUROC), accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). These metrics are commonly used to evaluate the effectiveness of classification models in predicting binary outcomes.\n\nThe best-performing model, LightGBM, achieved an AUROC of 0.642, indicating its ability to distinguish between patients who experienced home interstage mortality and those who did not. The model's accuracy, sensitivity, and specificity were also reported, providing a comprehensive view of its predictive performance.\n\nIn addition to these metrics, the model's feature importance was analyzed using Shapley additive explanations (SHAP) to understand the contribution of each variable to the prediction of interstage mortality. This analysis helped identify key factors, such as the use of digoxin at discharge and the type of stage I palliation, that significantly influence the model's predictions.\n\nOverall, the classification model demonstrated utility in predicting home interstage mortality in a medically complex and heterogeneous patient population, outperforming traditional logistic regression models. The model's outputs provide valuable insights into the relative importance of various clinical factors and can serve as an adjunct to clinical experience in caring for these patients.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the machine learning models involved a rigorous process to ensure robustness and generalizability. The dataset was randomly divided into training and test cohorts, with 80% of the data used for training and 20% reserved for testing. To minimize overfitting, the training set was further split into five folds using cross-validation. In each fold, four parts were used for training, and one part was used to determine the stop point of learning for models like XGBoost and LightGBM. This process was repeated five times to assess the model's predictive capability and robustness.\n\nFor validation, isotonic regression was applied to the validation set to ensure that the models were well-calibrated. The prediction performance of each model was evaluated using multiple standard metrics, including the area under the receiver operating characteristic (AUROC), accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). The cut-off threshold for risk probability was determined using Youden's J statistic, which maximizes the combined sensitivity and specificity.\n\nThe mean and 95% confidence interval (CI) of these metrics were reported to provide a comprehensive evaluation of the models' performance. This approach ensured that the models were thoroughly tested and validated, providing reliable insights into their predictive capabilities.",
  "evaluation/measure": "In our study, we assessed the prediction performance of various machine learning models using multiple standard metrics. These metrics included the area under the receiver operating characteristic curve (AUROC), accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). These metrics are widely recognized and used in the literature for evaluating the performance of predictive models, particularly in medical and healthcare settings.\n\nThe AUROC provides a single scalar value that represents the ability of the model to distinguish between positive and negative classes across all possible classification thresholds. It is a comprehensive measure that considers both the true positive rate (sensitivity) and the false positive rate (1-specificity).\n\nAccuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. It provides an overall measure of the model's performance but can be misleading if the classes are imbalanced.\n\nSensitivity, also known as recall or true positive rate, measures the proportion of actual positives that are correctly identified by the model. It is crucial for assessing the model's ability to detect positive cases, which is particularly important in medical diagnostics where missing a positive case can have severe consequences.\n\nSpecificity, or the true negative rate, measures the proportion of actual negatives that are correctly identified by the model. It is important for assessing the model's ability to avoid false positives, which can lead to unnecessary interventions or treatments.\n\nThe positive predictive value (PPV) measures the proportion of positive results that are true positives. It indicates the probability that a positive test result is a true positive. The negative predictive value (NPV) measures the proportion of negative results that are true negatives. It indicates the probability that a negative test result is a true negative.\n\nThese metrics collectively provide a comprehensive evaluation of the model's performance, covering various aspects of predictive accuracy and reliability. They are representative of the metrics commonly reported in the literature and ensure that our evaluation is thorough and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we evaluated the performance of several machine learning models to predict home interstage mortality in patients with congenital heart disease. We compared five different models: logistic regression, random forest, XGBoost, gradient boosting trees (GBT), and LightGBM. Logistic regression served as our reference model, providing a baseline for comparison.\n\nTo ensure a fair and comprehensive evaluation, we assessed each model using multiple standard metrics, including the area under the receiver operating characteristic curve (AUROC), accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). These metrics allowed us to gauge the models' predictive capabilities and robustness.\n\nWe also implemented techniques to minimize overfitting and enhance model calibration. For instance, we used isotonic regression on the validation set to ensure that the models were well-calibrated. Additionally, we employed five-fold cross-validation and repeated the random splitting process five times to assess the models' predictive capability and robustness.\n\nThe comparison of these models revealed that LightGBM achieved the highest AUROC of 0.642, indicating its superior performance in distinguishing between patients who experienced home interstage mortality and those who did not. LightGBM also demonstrated higher AUROCs compared to logistic regression, random forest, and gradient boosting trees, and achieved similar performance to XGBoost.\n\nFurthermore, we used Shapley additive explanations (SHAP) to estimate the feature importance of the tree-based models. This method allowed us to understand how different variables contributed to the models' predictions, providing insights into the key factors associated with interstage mortality.\n\nIn summary, our evaluation involved a thorough comparison of multiple machine learning models, including simpler baselines like logistic regression. We used standard metrics and techniques to ensure a rigorous and fair assessment, ultimately identifying LightGBM as the most effective model for predicting home interstage mortality in our study population.",
  "evaluation/confidence": "The evaluation of our machine learning models included a comprehensive assessment of performance metrics, each accompanied by 95% confidence intervals to provide a clear understanding of the variability and reliability of the results. This approach ensures that the reported metrics, such as AUROC, accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV), are robust and not merely the result of random chance.\n\nThe confidence intervals for these metrics were calculated to reflect the uncertainty in the estimates, which is crucial for interpreting the model's performance. For instance, the LightGBM model, which achieved the highest AUROC of 0.642, had a confidence interval ranging from 0.626 to 0.658. This interval indicates that while the model's performance is promising, there is still some variability that needs to be considered.\n\nStatistical significance was also a key consideration in our evaluation. We employed techniques such as isotonic regression on the validation set to ensure that the models were well-calibrated, which is essential for reliable performance. Additionally, the cut-off threshold for risk probability was determined using Youden's J statistic, which maximizes the combined sensitivity and specificity. This method helps in identifying the optimal threshold for classifying predictions, thereby enhancing the model's discriminative ability.\n\nThe comparison of different models, including logistic regression, random forest, XGBoost, and gradient boosting trees, was conducted with a focus on statistical significance. The LightGBM model demonstrated superior performance in terms of AUROC compared to logistic regression and random forest, and it achieved similar performance to XGBoost. These comparisons were made with the understanding that the differences in performance metrics were statistically significant, providing confidence in the superiority of the LightGBM model.\n\nOverall, the inclusion of confidence intervals and the emphasis on statistical significance in our evaluation process ensure that the conclusions drawn about the models' performance are reliable and valid. This rigorous approach allows us to claim with confidence that the LightGBM model, in particular, is superior to other evaluated models and baselines.",
  "evaluation/availability": "Not enough information is available."
}