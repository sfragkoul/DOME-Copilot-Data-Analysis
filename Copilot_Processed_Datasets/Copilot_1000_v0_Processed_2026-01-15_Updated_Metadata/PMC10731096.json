{
  "publication/title": "Classification of Fracture Risk in Fallers Using Dual-Energy X-Ray Absorptiometry (DXA) Images and Deep Learning-Based Feature Extraction.",
  "publication/authors": "Senanayake D, Seneviratne S, Imani M, Harijanto C, Sales M, Lee P, Duque G, Ackland DC",
  "publication/journal": "JBMR plus",
  "publication/year": "2023",
  "publication/pmid": "38130762",
  "publication/pmcid": "PMC10731096",
  "publication/doi": "10.1002/jbm4.10828",
  "publication/tags": "- Artificial Neural Networks\n- Fracture Risk Classification\n- DXA Images\n- Clinical Data Integration\n- Vision Transformers\n- Self-Supervised Learning\n- Class Imbalance Mitigation\n- Medical Image Analysis\n- Osteoporosis\n- Predictive Modeling",
  "dataset/provenance": "The dataset used in this study was sourced from community-dwelling older adults who presented to a Falls and Fracture Clinic in Melbourne, Australia, between October 2016 and January 2022. The study included data from 526 individuals. Among these, 196 individuals had experienced at least one fall, 282 individuals had experienced two or more falls, and 48 age-matched controls had no fall history. The cohort consisted of individuals aged 65 years and older who were able to mobilize independently or with the aid of a gait aid, had no severe cognitive deficits, and had at least one risk factor for falls or fractures. The dataset included information on fracture history, with 111 patients having no fracture history, 329 patients having one fracture, and 92 patients having two or more fractures. This dataset was used to evaluate fracture risk using DXA images and patient clinical information.",
  "dataset/splits": "The dataset was divided into three main splits: a train-validate set, a validation set, and a holdout test set. Initially, the complete dataset was split into two random groups: a train-validate set comprising 90% of the images and a holdout test set with the remaining 10% of the images. The train-validate set was further divided into a validation set and a training set by randomly sampling images such that 10% of the full image set was allocated to the validation set and 90% to the training set. This train-validate split process was repeated nine times, resulting in a total of ten different splits for cross-validation. Each iteration involved training a new model with the training set and selecting the best model iteration based on the validation test. Once trained, each model was tested on the held-out test set. This approach ensured that the model's performance was validated across multiple random splits of the data, providing a robust assessment of its generalization capabilities.",
  "dataset/redundancy": "The dataset used in this study was split into two main groups: a train-validate set and a holdout test set. The train-validate set comprised 90% of the images, while the holdout test set contained the remaining 10%. This initial split ensured that the test set was independent of the training process.\n\nThe train-validate set was further divided into validation and training sets. This was done by randomly sampling images such that 10% of the full image set was allocated to the validation set, and 90% were used for training. This process was repeated nine times, creating nine different train-validate splits. Each iteration involved training a new model with the training set and selecting the best model iteration based on performance with the validation set. This approach, known as k-fold cross-validation, helps to ensure that the model's performance is robust and not dependent on a particular split of the data.\n\nOnce trained, each of the models was tested on the held-out test set, which had not been used during the training or validation phases. This ensured that the test set remained independent and provided an unbiased evaluation of the model's performance.\n\nThe distribution of the dataset in this study is designed to mitigate issues commonly encountered in machine learning, such as overfitting and data redundancy. By using a holdout test set and performing k-fold cross-validation, the study aims to achieve a more reliable and generalizable model. This approach is consistent with best practices in machine learning, where maintaining the independence of the test set is crucial for evaluating the model's true performance.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are Convolutional Neural Networks (ConvNets) and small Vision Transformer networks (ViT-S). These are well-established classes of algorithms in the field of deep learning.\n\nThe specific ConvNet architectures employed are VGG-16 and ResNet-50, both of which are widely used and have been extensively studied in the literature. The ViT-S model, while relatively newer, has also gained significant attention and has been shown to be effective in various image recognition tasks.\n\nThe algorithms used are not new, but rather established methods that have been adapted and applied to the specific problem of fracture risk classification using DXA images and clinical data. The choice of these algorithms was driven by their proven effectiveness in image recognition tasks and their suitability for the data at hand.\n\nThe study focuses on the application of these algorithms to a specific medical problem, rather than the development of new machine-learning algorithms. Therefore, it is published in a medical journal rather than a machine-learning journal. The emphasis is on the clinical relevance and the improvement in fracture risk prediction, rather than the novelty of the machine-learning algorithms themselves.",
  "optimization/meta": "The model employed in this study can be considered a meta-predictor, as it integrates outputs from multiple machine-learning algorithms to enhance fracture risk classification. Specifically, the model combines features extracted from DXA images using various neural network architectures with clinical data processed through a multi-layer perceptron (MLP).\n\nThe neural network architectures used for image feature extraction include VGG-16 and ResNet-50, both pretrained using self-supervised learning strategies such as Momentum Contrastive Learning (MoCo) and Distillation with No Labels (DINO). Additionally, a small vision transformer network (ViT-S) was employed, which uses a multihead self-attention mechanism to extract features from images.\n\nThe clinical data, which includes variables such as vitamin D levels, calcium levels, height, and the ratio of appendicular lean mass to body mass index (ALM/BMI), is processed through a separate MLP. This MLP is trained to learn the optimal feature combinations from the clinical data.\n\nThe features extracted from the images and the clinical data are then concatenated and passed through a second MLP, which acts as the classification head. This integration allows the model to leverage both image-based and clinical information for fracture risk classification.\n\nRegarding the independence of training data, the study ensures that the data used for pretraining the feature extractors is independent of the data used for fine-tuning the classification head. The pretrained feature extractors are initially trained on the full set of available DXA images without label information specifying fracture history. Subsequently, the classification head is fine-tuned using the fracture risk labels associated with only the hip images. This approach helps to mitigate overfitting and ensures that the model generalizes well to new data.",
  "optimization/encoding": "For the machine-learning algorithm, data encoding and preprocessing involved several key steps. Clinical data, including vitamin D levels, calcium levels, height, and the ratio of appendicular lean mass to body mass index, were integrated into the model. These variables were selected due to their known association with fracture likelihood. Bone mineral density (BMD) and T-values for the hip and femoral neck were excluded, as relevant bone structural information was derived directly from DXA images using neural networks.\n\nThe clinical data were fed into a multilayer perceptron (MLP) model with one hidden layer. This MLP was trained to learn the optimal feature combinations from the clinical data. The output of this MLP was then concatenated with image latent features obtained through a feature extraction backbone. This concatenated data was passed through a second MLP, which acted as a classification head. Although the image feature extraction backend was frozen to retain the latent features learned through pretraining, both MLP models were trained through back-propagation.\n\nTwo types of artificial neural network models were employed for fracture risk classification: Convolutional Neural Networks (ConvNets) and small vision transformer networks (ViT-S). ConvNets utilized the pixel geometry of images to extract features by sliding a learnable template along the two axes of the images. ViT-S, on the other hand, used a more general feature extractor called multihead self-attention, building on sequence data processing methods commonly used in natural language processing (NLP). ViT-S models adopted a linear transformation instead of deep convolutional layers and leveraged the two-dimensional sequential nature of images to extract image features.\n\nTo improve model classification accuracy given the relatively low data sample sizes, two different self-supervised pretraining approaches were employed: Momentum Contrastive Learning (MoCo) and Distillation with No Labels (DINO). These pretraining strategies involved using a large unlabeled dataset for self-supervised feature-extractor learning. The pretrained feature-extractor used the full set of available DXA images from all modalities without label information specifying fracture history. Once the feature-extractor was initially pretrained with self-supervised learning, a classifier head made up of an MLP network was used to complete the classification model. Fine-tuning of the MLP classification head was performed using the fracture risk labels associated with only the hip images.\n\nThe dataset was split into a train-validate set comprising 90% of the images and a holdout test set with the remaining 10% of the images. The train-validate set was further split into validation and training sets by randomly sampling images such that 10% of the full image set was in the validation set and 90% were in the training set. This train-validate split process was repeated nine times, each time training a new model with the training set and selecting the best model iteration with the validation test. Once trained, each of the models was tested on the held-out test set.",
  "optimization/parameters": "In our study, we employed several different neural network architectures, each with its own set of parameters. For the convolutional neural networks (ConvNets), we used two widely adopted architectures: VGG-16 and ResNet-50. The VGG-16 model has approximately 138 million parameters, while the ResNet-50 model has around 25.6 million parameters. These architectures were chosen for their proven effectiveness in medical image analysis tasks.\n\nFor the vision transformer models (ViT-S), we utilized a smaller variant of the Vision Transformer, which has fewer parameters compared to the larger ViT models. The exact number of parameters in the ViT-S model can vary, but it is generally designed to be more efficient and suitable for smaller datasets.\n\nThe selection of these models and their parameters was based on a combination of factors, including their performance in similar tasks, the size of our dataset, and computational constraints. The VGG-16 model was chosen as a baseline due to its widespread use and generally good performance in medical imaging tasks. The ResNet-50 model was selected for its balance between performance and computational efficiency. The ViT-S model was chosen for its potential to capture complex features in the data, especially when combined with pretraining strategies.\n\nIn addition to the image-based models, we also integrated clinical data using a Multilayer Perceptron (MLP) network. The MLP was designed to learn the optimal feature combinations from the clinical data, which included variables such as vitamin D levels, calcium levels, height, and the ratio of appendicular lean mass to body mass index. The architecture of the MLP was kept relatively simple to avoid overfitting, given the smaller size of the clinical dataset compared to the image data.\n\nOverall, the choice of models and their parameters was driven by the need to balance performance, computational efficiency, and the ability to handle the specific characteristics of our dataset. The use of pretraining strategies, such as Momentum Contrastive Learning (MoCo) and Distillation with No Labels (DINO), further enhanced the performance of these models by mitigating overfitting and improving feature extraction from the data.",
  "optimization/features": "In our study, we utilized a combination of imaging and clinical data as input features for our models. The clinical data included variables such as vitamin D levels, calcium levels, height, and the ratio of appendicular lean mass to body mass index (ALM/BMI). These variables were selected based on their known association with fracture likelihood.\n\nFeature selection was not explicitly performed in the traditional sense, as we included variables that are clinically relevant and have been previously associated with fracture risk. The selection of these variables was informed by existing literature and clinical expertise, rather than a data-driven feature selection process.\n\nThe imaging data consisted of DXA scans, from which relevant bone structural information was derived directly using neural networks. Traditional DXA-derived information such as bone mineral density (BMD) and T-values were not used as input features, as the neural networks were designed to extract more nuanced structural information from the images.\n\nThe integration of clinical data and imaging data was achieved by passing the clinical data through a trainable multilayer perceptron (MLP) to learn the optimal feature combinations. The output of this MLP was then concatenated with the image latent features obtained through the feature extraction backbone. This combined set of features was passed through a second MLP, which acted as the classification head.\n\nIn summary, the input features consisted of a combination of clinically relevant variables and image-derived structural information. The selection of these features was based on clinical relevance and existing literature, rather than a data-driven feature selection process. The features were integrated using a combination of MLPs and neural network-based feature extraction from DXA images.",
  "optimization/fitting": "In our study, we employed several strategies to address potential overfitting and underfitting issues. The models used, particularly the Vision Transformer models (ViT-S), have a large number of parameters, which could lead to overfitting, especially with smaller datasets. To mitigate this, we utilized pretraining techniques such as Distillation with No Labels (DINO) and Momentum Contrastive Learning (MoCo). These self-supervised learning methods allowed the models to learn useful image features from a large set of unlabeled DXA images, reducing the risk of overfitting to the limited labeled data.\n\nAdditionally, we implemented class-specific reweighting to handle class imbalances, which further helped in mitigating overfitting. The stability and generalization of the models were demonstrated through k-fold cross-validation, which showed high mean values and relatively low standard deviations in performance metrics like F1 score and ROC-AUC. This indicates that the models were not overfitting to the training data.\n\nTo ensure that the models were not underfitting, we compared multiple architectures and pretraining strategies. The ViT-S models, despite their complexity, showed better performance compared to convolutional neural networks (ConvNets) like VGG-16 and ResNet-50. The integration of clinical data also improved model performance, suggesting that the models were capable of learning from the data without being too simplistic.\n\nOverall, the combination of pretraining, class reweighting, and cross-validation helped in balancing the models' capacity to generalize from the training data without overfitting or underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting, which is a common issue in artificial neural networks, especially when dealing with imbalanced label distributions.\n\nOne effective method we used was pretraining. We utilized self-supervised pretraining approaches, specifically Momentum Contrastive Learning (MoCo) and Distillation with No Labels (DINO). These methods allowed our models to learn useful features from a large unlabeled dataset of DXA images before fine-tuning on the labeled data. This pretraining step helped the models generalize better and reduced overfitting.\n\nAdditionally, we implemented class-specific reweighting to address class imbalances. This technique adjusts the loss function to give more weight to underrepresented classes, helping the model to learn from them more effectively. By mitigating class imbalances, we were able to improve the model's performance and reduce trivial predictions.\n\nWe also conducted k-fold cross-validation on the highest-performing model to ensure its stability and generalizability. This process involved splitting the data into multiple folds and training the model on different subsets, which helped to validate the model's performance and reduce overfitting.\n\nFurthermore, we integrated clinical information into the deep-learning pipeline. This integration improved model performance by providing additional relevant data, which helped the model to make more accurate predictions and reduced overfitting to the image data alone.\n\nOverall, these regularization techniques\u2014pretraining, class-specific reweighting, cross-validation, and integration of clinical data\u2014were crucial in preventing overfitting and enhancing the robustness of our models.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are available in the supplementary material. Specifically, the standard hyperparameters recommended in the software libraries for Momentum Contrastive Learning (MoCo) and Distillation with No Labels (DINO) are detailed there. The supplementary material also includes further information on the model architectures and training processes, such as the use of VGG-16 and ResNet-50 for Convolutional Neural Networks (ConvNets) and the Small Vision Transformer networks (ViT-S).\n\nModel files and optimization parameters are not explicitly mentioned as being available for download. However, the methods and configurations used are thoroughly described, allowing for replication of the experiments. The study emphasizes the use of self-supervised pretraining approaches with large unlabeled datasets, followed by fine-tuning with fracture risk labels associated with hip images. This approach ensures that the models can be trained and validated using the described procedures.\n\nThe integration of clinical data, including subject vitamin-D levels, calcium levels, height, and the ratio of certain measurements, is also evaluated. The effectiveness of combining DXA images with clinical data is demonstrated, showing improved model performance. The k-fold cross-validation results for the ViT-S+ DINO model, which combines DXA images and clinical data, are provided, indicating a mean F1 score of 62.6% and a mean ROC-AUC score of 74.2%.\n\nThe study highlights the importance of class rebalancing in highly unbalanced datasets, as demonstrated by the improvement in ROC-AUC scores with class reweighting strategies. The use of transformers, despite their general bias, is shown to be effective with pretraining, even with smaller datasets. The stability and performance of the models are validated through k-fold cross-validation, ensuring the reliability of the results.",
  "model/interpretability": "The model employed in our study is not entirely a black box, as we have implemented techniques to enhance its interpretability. To gain insights into the model's decision-making process, we utilized gradient-based class activation maps (CAMs). These visualizations highlight the regions within the DXA scans that the model considers most relevant for fracture risk classification.\n\nIn a preliminary exploratory study, we assessed EigenGradCAM and EigenCAM. These tools showed that the femoral neck had high relevance in classifying moderate fracture risk. This finding aligns with clinical knowledge, as the hip is a frequent fracture site in older adults. The isolated hip DXA images, having comparatively higher resolution than whole-body DXA images, are strongly associated with fracture risk.\n\nThe CAM results also indicated that for moderate fracture risk prediction, the inferred regions of interest generally resided within the bony anatomy visible on the DXA images. This suggests that the model focuses on areas where signs of previous fractures, such as fracture lines and the presence of metal implants, might be visible. The model's performance is likely improved by this focused attention on relevant anatomical features.\n\nHowever, it is important to note that fracture risk may be influenced by more nuanced features that are not captured in DXA images alone, such as the mechanical properties of bones, muscle architecture, and appendicular lean mass. Future work could involve extracting both foreground and background segmentations through self-supervised learning to incorporate more comprehensive information into the classification tasks. Additionally, using larger, higher-resolution image datasets or alternative imaging modalities could further improve classification accuracy and interpretability.",
  "model/output": "The model employed in this study is designed for classification rather than regression. Specifically, it focuses on categorizing fracture risk into three distinct groups: low risk, moderate risk, and high risk. This classification is achieved through the use of artificial neural networks, including Convolutional Neural Networks (ConvNets) and small Vision Transformer networks (ViT-S). The models utilize various architectures such as VGG-16, ResNet-50, and ViT-S, which are pretrained using self-supervised learning strategies like Momentum Contrastive Learning (MoCo) and Distillation with No Labels (DINO). The output of these models provides predictions for the fracture risk category, which are then evaluated using metrics such as the F1 Score and the area under the receiver-operating characteristic curve (ROC-AUC) score. The integration of both DXA images and clinical data further enhances the model's ability to accurately classify fracture risk, demonstrating the utility of combining different data sources and formats in this classification task.",
  "model/duration": "To reduce neural network training time, a 10-fold cross-validation was performed using only the highest-performing model. The complete dataset was split into two random groups: a train-validate set comprising 90% of the images and a holdout test set with the remaining 10%. The train-validate set was further divided into validation and training sets by randomly sampling images, ensuring that 10% of the full image set was in the validation set and 90% were in the training set. This train-validate split process was repeated nine times, each time training a new model with the training set and selecting the best model iteration based on the validation test. Once trained, each model was tested on the held-out test set.\n\nThe specific execution time for the model to run is not detailed, but the process involved multiple training iterations and validation steps to ensure robust performance evaluation. The use of a holdout test set and repeated training-validation splits indicates a thorough approach to model validation, aiming to balance computational efficiency with comprehensive performance assessment.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to ensure the robustness and generalizability of the models. A 10-fold cross-validation was conducted using the highest-performing model to reduce neural network training time. The complete dataset was split into two random groups: a train-validate set comprising 90% of the images and a holdout test set with the remaining 10% of the images. The train-validate set was further divided into validation and training sets by randomly sampling images, ensuring that 10% of the full image set was in the validation set and 90% were in the training set. This train-validate split process was repeated nine times, each time training a new model with the training set and selecting the best model iteration based on the validation test. Once trained, each model was tested on the held-out test set.\n\nThe performance of the models was evaluated using two primary metrics: the F1 Score with microaveraging (F1 Micro) and the area under the receiver-operating characteristic curve (ROC-AUC) score. These metrics provided a comprehensive assessment of the models' ability to classify fracture-risk groups accurately. The results reported were based on the predictions provided by the model with the highest validation accuracy when the hold-out test image set was used as input.\n\nAdditionally, the study compared the performance of different models, including VGG-16 pretrained on ImageNet, VGG-16 and ResNet-50 pretrained using the MoCo strategy, and a Vision Transformer model pretrained with the DINO strategy. The integration of clinical data, such as vitamin D levels, calcium levels, height, and the ratio of appendicular lean mass to body mass index, was also evaluated. This integration involved passing the clinical data through a trainable Multilayer Perceptron (MLP) to learn optimal feature combinations, which were then concatenated with image latent features obtained through the feature extraction backbone. The concatenated features were passed through a second MLP, acting as a classification head. Both MLP models were trained through back-propagation, ensuring that the network depth was increased for clinical data without losing information from the extracted image features.",
  "evaluation/measure": "In our study, we employed two primary performance metrics to evaluate the models' ability to classify fracture risk from DXA images: the F1 Score with micro-averaging (F1-Micro) and the area under the receiver-operating characteristic curve (ROC-AUC). These metrics were chosen for their robustness in handling imbalanced datasets, which is a common challenge in medical imaging tasks.\n\nThe F1-Micro score is particularly useful because it considers both precision and recall, providing a balanced measure of a model's performance. It is calculated as the weighted average of the F1 scores for each class, making it suitable for multi-class classification problems like ours. The ROC-AUC, on the other hand, measures the model's ability to distinguish between classes across all possible classification thresholds. A higher ROC-AUC indicates better model performance.\n\nThese metrics are widely used in the literature for evaluating classification models, especially in medical imaging. They provide a comprehensive view of model performance, considering both the overall accuracy and the ability to handle class imbalances. Additionally, we reported class-specific F1 scores and ROC-AUC values to provide insights into the model's performance for each fracture risk category (low, moderate, and high risk). This detailed reporting allows for a nuanced understanding of the models' strengths and weaknesses.\n\nIn summary, the reported metrics are representative of the standards in the field and provide a thorough evaluation of the models' performance in classifying fracture risk from DXA images.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of various neural network models to evaluate their performance in classifying fracture risk using DXA images. We benchmarked our proposed neural network workflows against several established models. The baseline model used was VGG-16, pretrained on the Imagenet natural image dataset, which is commonly used in medical image analysis. This model served as a reference point for comparing the performance of other models.\n\nWe also evaluated the VGG-16 model pretrained on DXA images using the MoCo strategy, as well as the Resnet-50 model with MoCo pretraining. These models were chosen because they represent widely used architectures in the field of computer vision and medical imaging. Additionally, we included a Vision Transformer model pretrained with the DINO strategy, which is an emerging technique that has shown promise in extracting image features.\n\nTo assess the impact of clinical data, we compared models that used only imaging data with those that integrated both DXA images and clinical information. This allowed us to determine whether the addition of clinical data improved model performance. The clinical data included variables such as vitamin D levels, calcium levels, height, and the ratio of appendicular lean mass to body mass index, which are known to be associated with fracture likelihood.\n\nFor each model, we measured the accuracy of fracture-risk group predictions using the F1 Score with microaveraging and the area under the receiver-operating characteristic curve (ROC-AUC) score. These metrics provided a comprehensive evaluation of the models' performance, taking into account both the precision and recall of the predictions.\n\nTo validate the model performance and sensitivity to random sampling, we conducted a 10-fold cross-validation on the highest-performing model. This involved splitting the dataset into training, validation, and test sets, and repeating the training process multiple times to ensure robustness. The results demonstrated the stability and reliability of the models, with high mean values and relatively low standard deviations in the cross-validation metrics.\n\nIn summary, our study compared multiple neural network models and demonstrated the effectiveness of integrating clinical data with imaging data for fracture risk classification. The use of pretraining strategies and the inclusion of clinical variables significantly improved model performance, highlighting the importance of a multi-modal approach in medical image analysis.",
  "evaluation/confidence": "The evaluation of our models included a thorough assessment of performance metrics, with a focus on confidence and statistical significance. For the highest-performing model, we conducted a 10-fold cross-validation to validate the model performance and sensitivity to random sampling. This process involved splitting the dataset into a train-validate set (90% of the images) and a holdout test set (10% of the images). The train-validate set was further divided into validation and training sets, with the training process repeated nine times to ensure robustness.\n\nThe cross-validation results for the Vision Transformer model with DINO pretraining (ViT-S+ DINO) demonstrated a mean F1 score of 62.6% with a standard deviation of 3.2%, and a mean ROC-AUC score of 74.2% with a standard deviation of 3.0%. These standard deviations provide a measure of the confidence intervals around the mean performance metrics, indicating the variability and reliability of the results.\n\nStatistical significance was assessed through the comparison of different model configurations. For instance, the ResNet + MoCo scenario showed a classification performance improvement with the class re-weighting strategy, underscoring the importance of class rebalancing in highly unbalanced datasets. The integration of clinical data with DXA images also showed significant improvements in performance metrics, particularly in the F1 score, without a significant drop in ROC-AUC scores.\n\nOverall, the evaluation process included rigorous statistical methods to ensure that the performance metrics are reliable and that the claimed superiority of certain models over others and baselines is statistically significant. The use of cross-validation and the reporting of standard deviations for performance metrics provide a clear indication of the confidence in the results.",
  "evaluation/availability": "Not enough information is available."
}