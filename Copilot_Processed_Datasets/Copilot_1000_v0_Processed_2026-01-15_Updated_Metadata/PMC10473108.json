{
  "publication/title": "What Patients Say: Large-Scale Analyses of Replies to the Parkinson's Disease Patient Report of Problems (PD-PROP).",
  "publication/authors": "Marras C, Arbatti L, Hosamath A, Amara A, Anderson KE, Chahine LM, Eberly S, Kinel D, Mantri S, Mathur S, Oakes D, Purks JL, Standaert DG, Tanner CM, Weintraub D, Shoulson I",
  "publication/journal": "Journal of Parkinson's disease",
  "publication/year": "2023",
  "publication/pmid": "37334615",
  "publication/pmcid": "PMC10473108",
  "publication/doi": "10.3233/jpd-225083",
  "publication/tags": "- Parkinson's disease\n- Patient-reported outcomes\n- Natural language processing\n- Machine learning\n- Multi-label text classification\n- Symptom curation\n- Deep learning\n- Human-in-the-loop\n- Data validation\n- Algorithm development",
  "dataset/provenance": "The dataset used in this study originates from the Fox Insight Study, an online, observational, longitudinal clinical study sponsored by the Michael J. Fox Foundation for Parkinson\u2019s Research. This study collects anonymous participant-reported outcomes on a large scale, including approximately 53,000 individuals with and without Parkinson\u2019s disease (PD).\n\nThe specific data utilized in this research comes from the Parkinson\u2019s Disease Patient Reports of Problems (PD-PROP), a module within Fox Insight. The PD-PROP consists of open-ended questions that allow participants to report and rank up to five PD-related bothersome problems and their functional consequences in their own words. This approach captures a fuller range of patient experiences without the constraints of pre-specified responses.\n\nThe dataset includes approximately 168,260 verbatim responses. These responses were curated by a human team, with approximately 3,500 responses initially curated. Subsequently, around 1,500 responses were used in the validation phase. The median age of respondents was 67 years, with 55% being male and a median of 3 years since PD diagnosis.\n\nThe data has been used previously in an initial curation of the PD-PROP, and this study represents an expansion of that process. The curated datasets derived from the verbatim responses are publicly available in FoxDEN upon signing a data use agreement. This availability allows for further research and community use, ensuring that the findings can be built upon and validated by other researchers.",
  "dataset/splits": "The dataset was divided into three splits: training, testing, and validation. Initially, a 90-10 train-test split was performed, resulting in approximately 16,000 samples for testing. The test data was further divided into test and validation sets in a 1:1 ratio, yielding around 8,000 samples for testing and 8,000 samples for validation. This approach ensured sufficient data for both training and evaluation while addressing potential overfitting and class imbalance issues.",
  "dataset/redundancy": "The dataset was split using a 90-10 train-test split to ensure sufficient data for both training and testing. This resulted in approximately 16,000 samples for testing. To prevent overfitting and address class imbalance issues, the test data was further divided into test and validation sets in a 1:1 ratio, yielding around 8,000 samples for testing and 8,000 for validation. This splitting was done using Sklearn\u2019s train-test-split function.\n\nThe training and test sets are independent. This independence was enforced by ensuring that the data used for training the model was not used in the testing phase. The validation set, derived from the test set, also maintained this independence, providing an unbiased evaluation of the model's performance.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in terms of size and diversity. The dataset comprises approximately 168,260 verbatims, which were classified into multiple symptoms, making it a robust multi-label text classification problem. The use of a deep learning neural network model trained on this data further ensures that the model can generalize well to new, unseen data. The careful splitting and validation processes ensure that the model's performance metrics, such as accuracy, proportion of false positives, and proportion of false negatives, are reliable and representative of its true capabilities.",
  "dataset/availability": "The data supporting the findings of this study are not publicly available due to privacy restrictions. However, the curated datasets derived from the verbatim responses are publicly available. These datasets can be accessed in FoxDEN at https://foxden.michaeljfox.org. To access these datasets, users must sign a data use agreement. This agreement ensures that the data is used responsibly and in accordance with the specified terms, thereby enforcing the proper use of the information.",
  "optimization/algorithm": "The machine-learning algorithm used in our study is a supervised deep learning neural network model implemented with Keras-TensorFlow. This model comprises two hidden layers and one output layer, designed to handle the multi-label text classification problem presented by the verbatims.\n\nThe approach we employed is novel in the context of healthcare research, particularly for analyzing patient-reported data. While the individual components of our method, such as deep learning and natural language processing, are well-established, their combined application in this specific manner is unique. This novelty is why the focus of our publication is on the healthcare application rather than the machine-learning algorithm itself.\n\nThe decision to publish in a healthcare-focused journal rather than a machine-learning journal is driven by the primary contribution of our work. Our study highlights the integration of human expert curation with machine learning to enhance the clinical meaningfulness of symptom classification. This combined approach addresses the limitations of traditional patient-reported outcome measures and demonstrates the value of free-text reporting in capturing a broader range of patient experiences. The machine-learning aspects serve as a tool to scale this approach to large datasets, making it practical for longitudinal studies and broader applications in healthcare research.",
  "optimization/meta": "The model described does not function as a meta-predictor. It is a supervised deep learning neural network model built using Keras-TensorFlow. This model is trained on data that has undergone natural language processing (NLP) classification and curation phases. The dataset used for training consists of verbatims, which are text entries from participants reporting their symptoms. These verbatims are classified into multiple symptoms, making it a multi-label text classification problem.\n\nThe model comprises two hidden layers and one output layer. It was trained using a 90-10 train-test split, with the test data further divided into test and validation sets in a 1:1 ratio. This split ensures that the data is not overfitted and avoids class imbalance problems. The model was compiled using binary cross-entropy loss.\n\nThe training data for this model comes from a curated dataset of verbatims, which were reviewed and classified by human curators. The curators used their clinical knowledge and personal experience to identify symptoms and adjust symptom names. The algorithm was further expanded using synonym generation through Unified Medical Language System (UMLS) ontologies and NLP techniques such as word vectorization. This process resulted in a finalized symptom term table that served as the input for the machine learning model.\n\nThe model's performance was evaluated based on concordance with the curators' classifications. The accuracy, proportion of false positives, and proportion of false negatives were calculated to assess the model's performance. Instances of discordance between the model and the curators were discussed to optimize the algorithm post-validation. The reasons for discordance informed modifications of terms and phrases to better identify positives and negatives.\n\nIn summary, the model does not use data from other machine-learning algorithms as input. It is a standalone deep learning model trained on a curated dataset of verbatims. The training data is independent, as it is derived from human curation and NLP techniques applied to the verbatims.",
  "optimization/encoding": "The data underwent several preprocessing steps before being fed into the machine-learning algorithm. Initially, the verbatims were cleaned to correct spelling errors using a Java-based package. This approach was preferred over a Python-based autocorrect module because it better handled specific jargon related to the symptoms being studied. Additionally, an \"ignore-terms\" library was built to prevent incorrect autocorrections of certain words.\n\nA database was created using Neo4j, which included participant user IDs and their verbatims. These verbatims were conjunctions of problems and consequences reported by participants. External files containing terms and phrases provided by curators, along with their synonyms, were used to perform phrase query extraction on the database. This process resulted in a master dataset that associated each verbatim with its corresponding symptom binning.\n\nThe dataset was then fine-tuned through manual inspection, and the algorithm was optimized accordingly. This refined dataset was used for data validation and the generation of a scalable machine-learning model. The model utilized a Keras-TensorFlow supervised deep learning neural network, comprising two hidden layers and one output layer. The data was split into training and testing sets using a 90-10 ratio, with the test data further divided into test and validation sets in a 1:1 ratio to ensure sufficient data for both training and testing while avoiding overfitting and class imbalance issues. The model was compiled using binary cross-entropy loss.",
  "optimization/parameters": "The model utilized in this study was a supervised deep learning neural network built using Keras-TensorFlow. It comprised two hidden layers and one output layer. The specific number of parameters (p) in the model was not explicitly stated, but the architecture suggests a relatively simple structure.\n\nThe selection of the model architecture, including the number of layers and neurons, was likely guided by the complexity of the multi-label classification problem at hand. The dataset contained approximately 168,260 verbatims with 10,519 unique multi-label combinations, indicating a high level of complexity. The choice of a 90-10 train-test split and further division of the test data into test and validation sets in a 1:1 ratio ensured sufficient data for training and validation while mitigating overfitting and class imbalance issues.\n\nThe model was compiled using binary cross-entropy loss, which is appropriate for multi-label classification tasks. The training process involved 50 epochs, and the performance was evaluated using metrics such as accuracy, proportion of false positives, and proportion of false negatives. The final model achieved high concordance with human curators, with an F1 score indicating strong performance in both precision and recall.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The fitting method employed a Keras-TensorFlow supervised deep learning neural network model, which comprised two hidden layers and one output layer. The dataset consisted of approximately 168,260 verbatims, with a 90-10 train-test split, resulting in around 16,000 samples for testing. To ensure the model was not overfitted and to avoid class imbalance issues, the test data were further split into test and validation sets in a 1:1 ratio, providing approximately 8,000 test and 8,000 validation samples.\n\nTo rule out overfitting, several strategies were implemented. First, the dataset was split into training, testing, and validation sets, which helped in evaluating the model's performance on unseen data. Additionally, the use of a validation set allowed for monitoring the model's performance during training and adjusting hyperparameters to prevent overfitting. The model was compiled using binary cross-entropy loss, which is suitable for multi-label classification problems and helps in managing the complexity of the model.\n\nTo address the potential issue of underfitting, the model was trained for an optimal number of epochs, specifically 50 epochs. This ensured that the model had sufficient time to learn the patterns in the data without becoming too simplistic. Furthermore, the use of a deep learning model with two hidden layers provided the necessary capacity to capture the complexities in the data, reducing the risk of underfitting.\n\nThe model's performance was evaluated using metrics such as accuracy, proportion of false positives, and proportion of false negatives. These metrics provided a comprehensive assessment of the model's ability to correctly classify verbatims, ensuring that it neither overfitted nor underfitted the data. The final model achieved high accuracy, with an F1 score calculated as (2 \u00d7 precision \u00d7 recall) / (precision + recall), indicating a balanced performance between precision and recall.",
  "optimization/regularization": "To prevent overfitting, the test data was further split into test and validation sets in a 1:1 ratio. This resulted in approximately 8,000 samples for testing and 8,000 samples for validation. This approach ensured that the model's performance was evaluated on unseen data, helping to identify and mitigate overfitting. Additionally, the use of a validation set allowed for the tuning of hyperparameters and the monitoring of the model's performance during training, further aiding in the prevention of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are not explicitly detailed in the publication. However, the model architecture is described as a supervised deep learning neural network with two hidden layers and one output layer, trained using a Keras-TensorFlow framework. The model was compiled using binary cross-entropy loss.\n\nThe dataset used for training and testing was split using a 90-10 train-test ratio, with the test data further divided into test and validation sets in a 1:1 ratio. This resulted in approximately 16,000 samples for testing and around 8,000 samples each for testing and validation. The splitting was performed using Sklearn\u2019s train-test-split function.\n\nThe optimization process involved ensuring that the data was not overfitted and addressing any class imbalance issues. The model's performance was evaluated based on accuracy, the proportion of false positives, and the proportion of false negatives. The final expert consensus classification for a verbatim was designated as the determination provided by at least two-thirds of the curators in a group, considered the 'gold standard' classification.\n\nThe curated datasets derived from the verbatim responses are publicly available in FoxDEN at https://foxden.michaeljfox.org upon signing a data use agreement. However, the specific model files and optimization parameters are not publicly available due to privacy restrictions and the proprietary nature of the tools used.\n\nFor those interested in replicating or building upon our work, the availability of the curated datasets in FoxDEN provides a valuable resource. The datasets can be accessed by signing a data use agreement, which ensures that the data is used responsibly and in accordance with privacy regulations. This approach allows for transparency and reproducibility while protecting the privacy of the participants.",
  "model/interpretability": "The model employed in our study is not a black-box system. It leverages a supervised deep learning neural network model built using Keras and TensorFlow, which inherently provides some level of interpretability. The model architecture is straightforward, consisting of two hidden layers and one output layer, making it relatively easy to understand and interpret.\n\nOne of the key aspects of interpretability in our model is the use of clear symptom boundaries and definitions. These boundaries were developed through a consensus process involving a team of curators, including clinicians, people with Parkinson's disease (PD), and experts in the field. The boundaries were refined iteratively to ensure they accurately reflect the self-reported experiences of individuals with PD. This process involved reviewing verbatims and establishing inclusion and exclusion criteria for each symptom, which were then used to train the model.\n\nAdditionally, the model's performance was validated through a rigorous curation process. Curators reviewed verbatims and provided classifications, which were then used to assess the model's accuracy. This human-in-the-loop approach ensured that the model's decisions could be understood and verified by experts. For instance, the model's accuracy was lowest for certain cognitive and depressive symptoms, but even in these cases, there were no false negatives, and the number of false positives did not exceed one for any symptom. This level of detail in the validation process adds to the model's interpretability.\n\nFurthermore, the terms and phrases used to identify symptoms were refined and expanded during the review process. This means that the model's decisions are based on specific, understandable language that can be traced back to the original verbatims. For example, terms and phrases for symptoms like internal tremor and executive abilities/working memory were carefully curated and used to train the model.\n\nIn summary, the model's architecture, the use of clear symptom boundaries, the human-in-the-loop validation process, and the refinement of terms and phrases all contribute to its interpretability. This ensures that the model's decisions are transparent and can be understood and verified by experts in the field.",
  "model/output": "The model developed is a classification model, specifically designed for multi-label text classification. It was trained using a supervised deep learning neural network with Keras-TensorFlow, comprising two hidden layers and one output layer. The model was compiled using binary cross-entropy loss, which is typical for classification tasks. The primary goal of the model is to classify verbatims into multiple symptoms, making it a multi-label text classification problem. The performance of the model was evaluated using metrics such as accuracy, proportion of false positives, and proportion of false negatives, further confirming its classification nature. The model's output provides classifications for verbatims, indicating whether they report specific symptoms or not.",
  "model/duration": "The execution time for the model is not explicitly detailed. However, the curation process, which is closely related to the model's development and validation, spanned approximately one year. This process involved regular meetings and independent work, suggesting a substantial time investment. The model itself was trained using a 90-10 train-test split, with further division of the test data into test and validation sets, indicating a thorough and time-consuming preparation phase. Additionally, the algorithm validation phase involved a large dataset of approximately 25,000 research participants, further implying a significant duration for data processing and analysis.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method involved a comprehensive process that combined human expertise with machine learning techniques. Initially, a dataset of approximately 168,260 verbatims was used. From this dataset, between 455 and 600 verbatims were provided to each curator. Each sample included 11 or 12 positives, predicted by the algorithm to include the symptom of interest, and 12 negatives, predicted not to include the symptom. The negatives were enriched with examples from closely related symptoms to challenge the curators' ability to distinguish similar symptoms.\n\nCurators classified the provided verbatims as reporting or not reporting the symptom, blind to the algorithm's classification. The final expert consensus classification for a verbatim was determined by the agreement of at least two-thirds of the curators in a group, considered the 'gold standard' classification. Algorithm performance metrics were calculated based on this concordance, including accuracy, proportion of false positives, and proportion of false negatives.\n\nInstances of discordance were discussed with curators at the discretion of the data science team to optimize the algorithm post-validation. Reasons for discordance informed modifications of terms and phrases to better identify positives and negatives.\n\nA Keras-TensorFlow supervised deep learning neural network model was trained on the data. The model comprised two hidden layers and one output layer. A 90-10 train-test split was used, with the test data further split into test and validation sets in a 1:1 ratio, providing approximately 8,000 test and 8,000 validation samples. The model was compiled using binary cross-entropy loss.\n\nThe evaluation also included assessing intra-rater reliability by presenting 68 verbatims twice to each curator, resulting in 204 pairs of duplicate presentations. Ratings agreed across 192 out of 204 presentations, indicating high reliability. Concordance between the machine and the curators was based on individual curator classification for a set of verbatims, with accuracy being lowest for several cognitive and depressive symptoms but reaching 100% for 58 out of 65 symptoms. For symptoms with less than 100% agreement, there were no false negatives, and the number of false positive responses did not exceed one for any symptom.",
  "evaluation/measure": "In the evaluation of our algorithm, several key performance metrics were reported to assess its effectiveness. The primary metrics included accuracy, precision, recall, and the F1 score. Accuracy, which measures the proportion of verbatims correctly classified, was reported at 95%. Precision, indicating the proportion of true positives among the predicted positives, was 97%. Recall, which represents the proportion of true positives among the actual positives, was 93%. The F1 score, a harmonic mean of precision and recall, was also 95%. These metrics provide a comprehensive view of the algorithm's performance, balancing both the correctness of positive predictions and the ability to identify all relevant instances.\n\nThe set of metrics reported is representative of standard practices in the literature for evaluating machine learning models, particularly in the context of text classification tasks. Accuracy provides an overall measure of the model's performance, while precision and recall offer insights into its specificity and sensitivity, respectively. The F1 score is particularly useful in scenarios where there is an imbalance between the classes, as it provides a single metric that balances both precision and recall. This combination of metrics ensures that the evaluation is thorough and aligned with established benchmarks in the field.",
  "evaluation/comparison": "Not applicable.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files, specifically the verbatim responses from participants, are not publicly available due to privacy restrictions. However, the curated datasets derived from these verbatim responses are accessible. These curated datasets can be found in FoxDEN. To access them, one must sign a data use agreement. This approach ensures that the sensitive information remains protected while still allowing researchers to utilize the valuable curated data for further analysis and studies."
}