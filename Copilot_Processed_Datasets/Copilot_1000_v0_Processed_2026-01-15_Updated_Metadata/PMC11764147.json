{
  "publication/title": "IL-6-Inducing Peptide Prediction Based on 3D Structure and Graph Neural Network.",
  "publication/authors": "Cao R, Li Q, Wei P, Ding Y, Bin Y, Zheng C",
  "publication/journal": "Biomolecules",
  "publication/year": "2025",
  "publication/pmid": "39858493",
  "publication/pmcid": "PMC11764147",
  "publication/doi": "10.3390/biom15010099",
  "publication/tags": "- IL-6-inducing peptides\n- Machine learning\n- Deep learning\n- Graph neural networks\n- Peptide prediction\n- Feature extraction\n- Dimensionality reduction\n- Cross-validation\n- Classification thresholds\n- Bioinformatics\n- Graph attention networks\n- Graph convolutional networks\n- Hierarchical clustering\n- Structural features\n- Immune response\n- Cytokine prediction\n- Model evaluation metrics\n- Data imbalance\n- Feature fusion\n- Pre-trained models",
  "dataset/provenance": "The dataset used in this study was initially compiled by Dhall et al. in 2020. It consists of experimentally validated peptides extracted from the Immune Epitope Database (IEDB). The positive dataset includes 365 IL-6-inducing peptides, which were tested in human or mouse hosts and are shorter than 25 amino acids. The negative dataset comprises peptides that induce other cytokines but not IL-6, ensuring they do not induce IL-6. This dataset has been rigorously validated and is widely used in research on IL-6-inducing peptide recognition. The dataset includes 365 IL-6-inducing peptides and 2991 non-IL-6-inducing peptides, providing a comprehensive benchmark for studies on IL-6-inducing peptide prediction tools. The imbalance between positive and negative samples is addressed through balancing techniques during training, such as weighting the samples to improve the model\u2019s ability to recognize the minority class.",
  "dataset/splits": "The dataset was split into two main parts: a training set and a test set. This split followed a standard 80:20 ratio, which is commonly used in similar studies. The training set contained 292 IL-6-inducing peptides and 2393 non-IL-6-inducing peptides. The test set comprised 73 IL-6-inducing peptides and 598 non-IL-6-inducing peptides. This split was designed to ensure a fair representation of the data in both sets, with comparable amino acid compositions and sequence length distributions. The fairness of the dataset split was confirmed through amino acid composition analysis and amino acid length analysis, which showed no significant biases in residue frequencies or sequence lengths between the training and testing sets.",
  "dataset/redundancy": "The dataset used in this study consists of 365 IL-6-inducing peptides and 2991 non-IL-6-inducing peptides. To ensure a fair evaluation, the dataset was split into training and testing sets using an 80:20 ratio, which is a standard protocol followed in several previous studies. This split resulted in a training set containing 292 IL-6-inducing peptides and 2393 non-IL-6-inducing peptides, while the test set comprised 73 IL-6-inducing peptides and 598 non-IL-6-inducing peptides.\n\nTo ensure the independence of the training and test sets, we conducted amino acid composition analysis and amino acid length analysis. The results showed that the amino acid compositions between the two sets were comparable, with no significant biases in residue frequencies. Similarly, the sequence length distributions were consistent, confirming the fairness of the dataset split. This careful splitting and analysis ensure that the training and test sets are independent and representative of the overall dataset.\n\nThe distribution of samples in our dataset is comparable to previously published machine learning datasets in the field. The imbalance between positive and negative samples is addressed through balancing techniques during training, such as weighting the positive and negative samples. This approach helps improve the model's ability to recognize the minority class, which is crucial for accurate prediction of IL-6-inducing peptides.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages the Adam optimizer, which is a widely-used class of stochastic gradient descent algorithms. Adam is known for its efficiency and effectiveness in handling sparse gradients on noisy problems, making it suitable for our model's training process.\n\nThe algorithm itself is not new; it has been extensively used and validated in various machine learning and deep learning applications. The choice of Adam was driven by its robustness and adaptability to different types of data and model architectures. While Adam is a well-established optimization algorithm, its application in our specific context\u2014particularly in the domain of biomolecular feature extraction and classification\u2014contributes to the novelty of our work.\n\nThe decision to use Adam in this study was based on its proven track record in similar tasks and its ability to handle the complexities of our dataset, which includes a substantial imbalance between positive and negative samples. The algorithm's adaptive learning rate and momentum properties help in achieving faster convergence and better generalization performance.\n\nThe reason Adam was not published in a machine-learning journal in this context is that it is a well-known and widely adopted optimization technique. Our focus was on applying established methods to a novel problem in biomolecular research, rather than developing a new optimization algorithm. The innovation lies in the integration of Adam with our specific model architecture and the unique challenges posed by our dataset.",
  "optimization/meta": "The model described in this publication is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it leverages a dual-channel graph network architecture to extract features from peptide sequences modeled as graphs. The primary components of this architecture include a graph attention network (GAT) and a graph convolutional network (GCN). The GAT is used to update node features, while the GCN focuses on updating node weights. These features are then fused to classify IL-6-inducing peptides effectively.\n\nThe model employs a pre-trained model, ESM-1b, to extract amino acid features as node attributes, in addition to conventional methods like one-hot encoding and position encoding. This approach ensures that the model captures both structural and sequential information, enhancing its predictive performance. The training data used in this study is independent and consists of experimentally validated IL-6-inducing peptides and non-IL-6-inducing peptides, ensuring that the model's performance is robust and reliable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure optimal performance of the machine-learning algorithm. We utilized several encoding methods to capture different aspects of the peptide sequences.\n\nFirstly, we employed ESM-1b, a pre-trained model known for its ability to capture deep features of amino acids, including their positions and relative importance in three-dimensional structures. This model was chosen due to its superior performance in capturing short-range dependencies and contextual information relevant to our dataset. The ESM-1b features were initially 1280-dimensional, but we reduced their dimensionality using Principal Component Analysis (PCA) to 30 dimensions. This reduction helped in maintaining the primary structure of the data while reducing the number of features, ensuring consistency and comparability across models.\n\nAdditionally, we used one-hot encoding to represent the diversity of amino acids and their influence on protein functionality. This encoding method provided categorical information about the nodes, which was vital for the model's performance. One-hot encoding resulted in improvements across various metrics, including AUC, MCC, SP, SN, and BACC.\n\nPosition encoding was introduced to compensate for the limitation of graph neural networks in capturing sequential positions when processing sequence data. By providing precise positional information for each node, position encoding enabled the model to better interpret node positions within peptide sequences. This encoding method significantly improved the model's performance, as evidenced by increases in AUC, MCC, SN, and BACC.\n\nTo integrate these features effectively, we conducted a series of ablation experiments. These experiments demonstrated that combining ESM-1b-extracted features with position encoding and one-hot encoding significantly enhanced the model's predictive performance. The feature fusion strategy improved the model's representation capability, allowing it to capture biological signals related to protein functionality.\n\nIn summary, the data encoding and preprocessing involved using ESM-1b for deep feature extraction, one-hot encoding for amino acid diversity, and position encoding for sequential information. These encoding methods were crucial in improving the model's ability to distinguish between negative and positive samples, ultimately leading to better predictive performance.",
  "optimization/parameters": "In the optimization of our model, several key parameters were selected based on extensive experimentation and evaluation. The primary parameters considered were the amino acid contact threshold, the number of Graph Attention Network (GAT) attention heads, and the embedding dimension of the hidden layers.\n\nThe amino acid contact threshold was chosen to balance the inclusion of relevant edge information without overwhelming the model with excessive data. Thresholds of 0.6, 0.7, 0.8, and 0.9 were tested, with a threshold of 0.8 ultimately selected as it provided the best performance by including essential edge information while avoiding unnecessary complexity.\n\nThe number of attention heads in the GAT layer was another critical parameter. We evaluated configurations with 4, 8, 12, and 16 attention heads. The model's performance improved with an increasing number of attention heads up to eight, beyond which the performance declined. Therefore, eight attention heads were chosen for the GAT layer.\n\nThe embedding dimension of the hidden layers was also optimized. Dimensions of 32, 64, 128, and 256 were tested, with an embedding dimension of 64 yielding the best results. This dimension provided a good balance between capturing sufficient feature information and maintaining computational efficiency.\n\nAdditionally, the selection of the pre-trained model involved comparing BERT, ProtT5, and ESM-1b. ESM-1b was chosen due to its superior performance in capturing the position and relative importance of amino acids in three-dimensional structures, which is crucial for our study.\n\nIn summary, the model parameters were carefully selected through a series of experiments and evaluations to ensure optimal performance. The chosen parameters include an amino acid contact threshold of 0.8, eight attention heads in the GAT layer, and an embedding dimension of 64 for the hidden layers. These selections were made to balance model complexity, training efficiency, and predictive power.",
  "optimization/features": "In the optimization process, the input features were carefully selected and processed to ensure optimal model performance. Initially, features were extracted using the ESM-1b model, resulting in 1280-dimensional feature vectors. To integrate these features effectively with other types, dimensionality reduction was performed. Several methods, including linear discriminant analysis (LDA), t-distributed stochastic neighbor embedding (t-SNE), and uniform manifold approximation and projection (UMAP), were compared. However, principal component analysis (PCA) was ultimately chosen for its ability to preserve variance and maintain the primary structure of the data.\n\nThe dimensionality of the ESM-1b features was reduced to 30 dimensions, as this configuration yielded the best and most stable performance during five-fold cross-validation. This reduction helped retain essential feature information while minimizing the impact on model performance. Additionally, position encoding and one-hot encoding were used to capture categorical information and amino acid positions within peptide sequences.\n\nFeature selection was performed using the training set only, ensuring that the evaluation metrics, such as AUC, were used to determine the optimal dimensionality and feature combinations. This approach allowed for a fair and consistent evaluation of performance differences between models. The final input features consisted of the reduced ESM-1b features, position encoding, and one-hot encoding, providing a comprehensive representation of the data for the model.",
  "optimization/fitting": "The model was constructed using PyTorch and its Geometric framework, with the ADAM optimizer employed. The training batch size was set to 256, and the initial learning rate was 0.001, reduced by 5% every 5 epochs over a total of 40 epochs. To address class imbalance, weights for positive and negative samples were set at a ratio of 10:1.\n\nTo mitigate overfitting, several strategies were implemented. First, a grid search strategy was used for hyperparameter optimization, selecting the best combinations based on the AUC from the validation set. Additionally, five-fold cross-validation was performed extensively across various aspects of the model, including the selection of the pre-trained model, dimensionality reduction methods, dimensions for ESM-1b features, GAT layers, and GCN layers. This cross-validation approach ensured that the model's performance was robust and not overly fitted to a specific subset of the data.\n\nThe dimensionality of the ESM-1b features was reduced to 30 dimensions using PCA, which helped in retaining essential feature information while reducing the risk of overfitting. The selection of three GAT layers and one GCN layer was also based on cross-validation results, ensuring that the model complexity was balanced with performance.\n\nTo avoid underfitting, the model's architecture was carefully designed to capture complex relationships in the data. The use of ESM-1b features, which consider deep features and the relative importance of amino acids, enhanced the model's ability to identify key positions for biological activity. The combination of ESM-1b features with position encoding and one-hot encoding further improved the model's representation capability, ensuring that it could learn from the data effectively.\n\nOverall, the model's performance was evaluated using metrics such as AUC, BACC, and SN, with the classification threshold set to 0.35 to optimize performance on imbalanced data. The extensive use of cross-validation and careful selection of hyperparameters ensured that the model was neither overfitted nor underfitted, providing reliable and generalizable results.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our model. One key strategy involved the use of a three-layer Graph Attention Network (GAT). While a deeper network might theoretically offer greater capacity, it was observed that increasing the number of layers beyond three did not yield further performance gains and could potentially lead to overfitting, especially given the limited volume of data available. Therefore, the three-layer GAT was selected to balance predictive power with training efficiency, capturing essential data characteristics without unnecessary complexity.\n\nAdditionally, we utilized a one-layer Graph Convolutional Network (GCN) as the second channel in our dual-channel feature extraction module. Experiments showed that increasing the number of GCN layers did not improve performance and could unnecessarily increase model complexity. Thus, a one-layer GCN was deemed sufficient to capture valuable information from the graph structure in our dataset.\n\nTo address class imbalance, which is crucial given the substantial disparity between positive and negative samples, we set the weights for positive and negative samples at a ratio of 10:1. This weighting scheme helps the model to focus more on the minority class, thereby improving its ability to distinguish between positive and negative samples.\n\nFurthermore, we employed a grid search strategy for hyperparameter optimization, selecting the best combinations based on the Area Under the Curve (AUC) from the validation set. This systematic approach ensures that the model parameters are finely tuned to achieve optimal performance.\n\nRegularization techniques such as dropout and early stopping were also implemented during the training process. Dropout helps to prevent overfitting by randomly setting a fraction of input units to zero at each update during training time, which prevents units from co-adapting too much. Early stopping monitors the model's performance on a validation set and halts training when performance stops improving, thus avoiding overfitting to the training data.\n\nOverall, these regularization methods and strategies were integral to maintaining the model's generalizability and preventing overfitting, ensuring that it performs well on unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule are reported in the publication. The model was constructed using PyTorch and its Geometric framework. The optimizer used was ADAM, with a training batch size of 256. The weights for positive and negative samples were set at a ratio of 10:1 to address class imbalance. The initial learning rate was 0.001, reduced by 5% every 5 epochs, and the model was trained for a total of 40 epochs. Hyperparameter optimization was conducted using a grid search strategy, with the best combinations selected based on the AUC from the validation set.\n\nThe specific pre-trained models evaluated included BERT, ProtT5, and ESM-1b. After dimensionality reduction, ESM-1b was selected due to its superior performance in capturing the position and relative importance of amino acids in three-dimensional structures. The classification threshold was set to 0.35 based on five-fold cross-validation results, which yielded the highest balanced accuracy (BACC).\n\nRegarding the model files and optimization parameters, the publication does not explicitly mention where these are available or under what license. However, it is common practice in scientific publications to make datasets and code available upon request or through repositories like GitHub or Zenodo. For detailed access to model files and optimization parameters, readers would typically contact the authors or check supplementary materials associated with the publication.",
  "model/interpretability": "The model employed in this study is not entirely a black box, as it incorporates several interpretable components and design choices that contribute to its transparency. The use of Graph Attention Networks (GAT) and Graph Convolutional Networks (GCN) allows for some level of interpretability by highlighting the importance of different nodes and edges in the graph structure. The attention mechanism in GAT, for instance, provides insights into which amino acids or features are most influential in the prediction process. This can be particularly useful for understanding the biological significance of certain amino acid positions within peptide sequences.\n\nAdditionally, the model integrates features extracted from ESM-1b, which are designed to capture deep biological signals related to protein functionality. The dimensionality reduction process using Principal Component Analysis (PCA) helps in retaining the most relevant information while making the features more interpretable. The combination of one-hot encoding and position encoding further enhances the model's ability to capture categorical and positional information, respectively, making it easier to trace back the model's decisions to specific input features.\n\nThe ablation experiments conducted on different node characteristics demonstrate the impact of various features on the model's performance. For example, the significant improvement in metrics like AUC and MCC when ESM-1b features are included indicates their crucial role in the model's predictive power. This kind of analysis provides a clear understanding of how different types of features contribute to the overall performance, adding to the model's interpretability.\n\nFurthermore, the selection of model parameters, such as the number of GAT layers and GCN layers, was based on systematic evaluations using five-fold cross-validation. This process ensures that the chosen architecture is not only optimal in terms of performance but also justifiable through empirical evidence. The decision to use a three-layer GAT and a one-layer GCN was driven by the need to balance model complexity and predictive accuracy, making the model's design more transparent.\n\nIn summary, while the model leverages complex neural network architectures, it includes several interpretable components and design choices that make it more transparent. The use of attention mechanisms, feature importance analysis, and systematic parameter selection all contribute to a better understanding of the model's inner workings and decision-making process.",
  "model/output": "The model is designed for classification, specifically to identify IL-6-inducing peptides. It employs a dual-channel feature extraction module that integrates Graph Attention Networks (GAT) and Graph Convolutional Networks (GCN) to process graph-based representations of peptide sequences. The output of this module is then passed through a classification module consisting of four fully connected layers and a sigmoid activation function. The final classification is determined by a threshold of 0.35, where probabilities exceeding this value are classified as positive (IL-6-inducing peptides), and those below are classified as negative. The model uses the BCEWithLogitsLoss function during training, which combines a sigmoid function and binary cross-entropy loss to handle the classification task effectively. This approach ensures that the model can accurately distinguish between positive and negative samples, even in the presence of class imbalance.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to ensure the robustness and reliability of the model's performance. Five-fold cross-validation was extensively used across various experiments to systematically evaluate different aspects of the model. This included the selection of pre-trained models, dimensionality reduction methods, dimensions for ESM-1b features, and the number of layers for Graph Attention Networks (GAT) and Graph Convolutional Networks (GCN).\n\nTo address the substantial imbalance between positive and negative samples in the dataset, a balanced accuracy (BACC) metric was utilized. This metric accounts for both positive and negative samples, providing a more accurate assessment of the model's performance. Additionally, other evaluation metrics such as sensitivity (SN), specificity (SP), Matthews Correlation Coefficient (MCC), and Area Under the ROC Curve (AUC) were used to offer a comprehensive evaluation.\n\nThe dataset was split into training and testing sets following an 80:20 ratio, which is a standard protocol in similar studies. This split ensured that the model's performance could be evaluated on unseen data, providing insights into its generalization capability. The training set consisted of 292 IL-6-inducing peptides and 2393 non-IL-6-inducing peptides, while the test set comprised 73 IL-6-inducing peptides and 598 non-IL-6-inducing peptides.\n\nAmino acid composition and sequence length analyses were conducted to confirm the fairness of the dataset split, showing comparable distributions between the training and testing sets. This ensured that the model's performance was not biased by differences in the dataset distributions.\n\nAblation experiments were also performed to evaluate the effectiveness of different feature extraction components. These experiments involved removing specific features, such as one-hot encoding, position encoding, and ESM-1b features, to determine their individual contributions to the model's performance. The results highlighted the distinct contributions and synergistic effects of various features, underscoring their importance in enhancing the model's predictive capability.\n\nIn summary, the evaluation method involved a rigorous combination of cross-validation, balanced metrics, and ablation experiments to ensure a thorough and unbiased assessment of the model's performance.",
  "evaluation/measure": "To ensure a comprehensive and objective evaluation of our model's performance, we have selected a set of widely recognized metrics that are commonly used in the literature for similar studies. These metrics provide a robust assessment of the model's capabilities and allow for meaningful comparisons with previous research.\n\nThe primary metrics we report include Balanced Accuracy (BACC), Sensitivity (SN), Specificity (SP), Matthews Correlation Coefficient (MCC), and Area Under the ROC Curve (AUC). Each of these metrics offers unique insights into different aspects of the model's performance.\n\nBalanced Accuracy (BACC) is particularly important because it accounts for both positive and negative samples, which is crucial in scenarios where the classes are imbalanced. This metric helps mitigate potential biases and provides a more accurate representation of the model's overall performance.\n\nSensitivity (SN), also known as the true positive rate, measures the model's ability to correctly identify positive samples. This is essential for applications where the cost of missing a positive case is high.\n\nSpecificity (SP), or the true negative rate, evaluates the model's effectiveness in correctly identifying negative samples. This metric is vital for ensuring that the model does not produce false positives, which can be equally detrimental in certain contexts.\n\nThe Matthews Correlation Coefficient (MCC) is a balanced measure that takes into account true and false positives and negatives. It provides a single value that summarizes the quality of the binary classifications, making it a valuable metric for overall performance assessment.\n\nThe Area Under the ROC Curve (AUC) is particularly valuable because it is independent of specific threshold settings. This metric provides a more robust assessment of the model's generalization performance, as it considers the trade-off between sensitivity and specificity across all possible classification thresholds.\n\nBy reporting these metrics, we aim to provide a thorough and representative evaluation of our model's performance, ensuring that it can be compared fairly with other studies in the field.",
  "evaluation/comparison": "To evaluate the performance of our proposed model, we conducted a comprehensive comparison with existing prediction tools on the testing set. The models compared include IL-6Pred, StackIL6, and MVIL6. The performance metrics for these models were derived from prior studies, with specific references cited for MVIL6.\n\nOur model, DGIL-6, demonstrated significant improvements across several key performance metrics. The Area Under the Curve (AUC) increased by 1.9% to 7.2%, Balanced Accuracy (BACC) increased by 0.4% to 9.5%, Matthews Correlation Coefficient (MCC) increased by 0.6% to 7.8%, Specificity (SP) increased by 0.5% to 7.2%, and Sensitivity (SN) increased significantly by 1.3% to 12.3%. These enhancements can be largely attributed to the use of the pre-trained ESM-1b model for extracting amino acid features, which provides the model with more comprehensive biological information, thereby enhancing prediction accuracy and reliability.\n\nThe DGIL-6 model achieves superior performance by integrating 3D structure-based graph representations with amino acid-based depth features. This integration allows the model to capture both global and local features within the graph, resulting in more accurate predictions of IL-6-inducing peptides. The comparison with simpler baselines and publicly available methods underscores the effectiveness of our approach in handling the complexities of peptide prediction tasks.",
  "evaluation/confidence": "Evaluation Confidence\n\nThe evaluation of the proposed model, DGIL-6, involved rigorous testing and comparison with existing methods. To ensure the reliability of the results, several key performance metrics were used, including Balanced Accuracy (BACC), Sensitivity (SN), Specificity (SP), Matthews Correlation Coefficient (MCC), and Area Under the ROC Curve (AUC). These metrics provide a comprehensive assessment of the model's performance, accounting for both positive and negative samples and mitigating potential biases caused by class imbalance.\n\nThe performance metrics were derived from experiments conducted on a testing set, and the results were compared with those of existing prediction tools such as IL-6Pred, StackIL6, and MVIL6. The comparative results showed that DGIL-6 outperformed these tools across several key metrics. For instance, DGIL-6 achieved the highest AUC, indicating superior generalization performance. Similarly, improvements in BACC, MCC, SP, and SN were observed, highlighting the model's enhanced predictive accuracy and reliability.\n\nTo further validate the findings, ablation experiments were conducted to evaluate the effectiveness of different components within the model. These experiments involved removing specific modules, such as the GAT and GCN, to assess their individual contributions. The results demonstrated that both the GAT and GCN modules played crucial roles in the model's performance, with their removal leading to significant decreases in key metrics. This underscores the complementary nature of these modules and their importance in capturing both local and global features within the graph.\n\nAdditionally, the dimensionality reduction of ESM-1b features was carefully analyzed to ensure optimal performance. Several dimensionality reduction methods were compared, and Principal Component Analysis (PCA) was selected as the most effective method. Experiments were conducted to determine the optimal number of dimensions, with the best and most stable performance achieved when the ESM-1b features were reduced to 30 dimensions. This dimensionality reduction helped retain essential feature information, leading to improved model performance.\n\nIn summary, the evaluation of DGIL-6 was conducted with a focus on statistical significance and reliability. The use of multiple performance metrics, comparative experiments with existing methods, and ablation studies provided a robust assessment of the model's capabilities. The results indicate that DGIL-6 is a superior method for predicting IL-6-inducing peptides, with statistically significant improvements in key performance metrics.",
  "evaluation/availability": "Not enough information is available."
}