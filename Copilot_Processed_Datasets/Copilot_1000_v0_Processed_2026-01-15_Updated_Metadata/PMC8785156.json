{
  "publication/title": "COVID-19 disease diagnosis with light-weight CNN using modified MFCC and enhanced GFCC from human respiratory sounds.",
  "publication/authors": "Kranthi Kumar L, Alphonse PJA",
  "publication/journal": "The European physical journal. Special topics",
  "publication/year": "2022",
  "publication/pmid": "35096278",
  "publication/pmcid": "PMC8785156",
  "publication/doi": "10.1140/epjs/s11734-022-00432-w",
  "publication/tags": "- COVID-19\n- Respiratory Sounds\n- Sound Feature Extraction\n- Machine Learning\n- Deep Learning\n- Convolutional Neural Networks\n- Audio Classification\n- Medical Diagnosis\n- Crowdsourced Data\n- Respiratory Disease Detection",
  "dataset/provenance": "The dataset used in this study is a respiratory COVID-19 sounds crowdsourced dataset. This dataset was collected with mutual agreement from Cambridge University for research purposes. It was authorized at Cambridge University in the department of computer science and technology by adhering to all ethical committee guidelines.\n\nThe dataset was collected using an android-based mobile application and a browser-based web application. The fundamental features of these applications are nearly identical. Users were asked to provide their past medical background, symptoms, and collect three respiratory audio sounds: a breath sound for 30 seconds, a cough sound for 3\u20135 intervals, and a sample voice of reading a single sentence. Additionally, users could enter further symptoms and sounds, providing a unique opportunity to investigate and discover the previous medical history of the user.\n\nThe data is extremely secure in the local data centers of the University of Cambridge. The acquired information is stored internally until it is linked with the Wi-Fi network. The data is removed from the database if the required data is received from the user\u2019s device. If a user requests that information be removed from the server, it will be updated.\n\nThe dataset consists of approximately 25,000 samples from the android-based application and 45,000 samples from the web-based application. Additionally, around 6,000 and 5,000 samples were collected from other sources. The total dataset consists of 1541 respiratory sound audio files, including breath, voice, and cough, collected from different users with a track size of 30 seconds.\n\nThe dataset has been used in previous studies and by the community. For instance, Brown et al. created an android-based mobile application and a browser-based web application to obtain respiratory COVID-19 sounds. Kun et al. implemented an intelligent technology on voice data by taking different considerations like the quality of sleep, anxiety, severity, and fatigue. The Cambridge University researchers have acquired data from the \u201cCOVID-19 sounds\u201d applications, and Mellon University research scientists have obtained data from the \u201cCOVID-19\u201d application. This preliminary study yielded 378 components and 256 audio files for feature analysis, according to the authors. These 256 audio sound features were gathered from approximately fifty COVID-19 infected patients.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The respiratory COVID-19 sounds crowdsourced dataset used in our study was collected with mutual agreement from Cambridge University for research purposes. The data was authorized at Cambridge University in the department of computer science and technology by adhering to all ethical committee guidelines. The dataset is extremely secure in the local data centers of the University of Cambridge. The acquired information is stored internally until it is linked with the Wi-Fi network. The data is removed from the database if the required data is received from the user\u2019s device. If a user requests that information be removed from the server, it will be updated.\n\nThe dataset consists of respiratory sound audio files collected from different users, including breath, voice, and cough sounds. The total dataset consists of 1541 respiratory sound audio files collected from different users with a track size of 30 seconds. The dataset includes various classes such as COVID-19 with cough, COVID-19 with breath, non-COVID-19 with cough, non-COVID-19 with breath, asthma with cough, asthma with breath, pertussis with cough, bronchitis with cough, COVID-19 with cough and breath, COVID-19 with cough, breath, and voice, healthy cough sounds, healthy breath sounds, and healthy speech sounds.\n\nThe dataset was collected using an android-based mobile application and a browser-based web application developed by Brown et al. The application collects respiratory audio sounds, including a breath sound for 30 seconds, a cough sound for 3\u20135 intervals, and a sample voice of reading a single sentence. Users are also asked to enter their symptoms and additional sounds to provide a unique opportunity to investigate and discover the previous medical history of the user.\n\nThe dataset is not publicly released in a forum. The data is securely stored and managed by Cambridge University, ensuring that all ethical guidelines are followed. Users have the option to request the removal of their information from the server, which is then updated accordingly. The dataset is used for research purposes and is not available for public download or access.",
  "optimization/algorithm": "The optimization algorithm employed in our study utilizes a convolutional neural network (CNN) architecture, specifically a light-weight CNN model. This class of machine-learning algorithms is well-established and widely used for various classification tasks, particularly in image and signal processing.\n\nThe CNN model developed is not entirely new but represents an innovative application tailored to respiratory sound classification. The model incorporates several key components to enhance its performance:\n\n* **Feature Extraction Techniques**: Two feature extraction techniques, Enhanced Gamma-tone Frequency Cepstral Coefficients (EGFCC) and Modified Mel-Frequency Cepstral Coefficients (MMFCC), are integrated into the CNN architecture. These techniques help in capturing relevant features from respiratory sound data, which are crucial for accurate classification.\n\n* **Architecture Design**: The CNN model is designed with five convolutional layers, each with different kernel sizes (1 \u00d7 12, 1 \u00d7 24, 1 \u00d7 36, 1 \u00d7 48, and 1 \u00d7 60). This multi-layered approach allows the model to learn hierarchical features from the input data. The use of batch normalization and ReLU activation functions helps in stabilizing and accelerating the training process, while dropout layers are employed to prevent overfitting.\n\n* **Optimization Parameters**: The model is trained using the Adam optimizer, which is known for its efficiency and adaptability in handling sparse gradients. Categorical cross-entropy is used as the loss function, and the model is trained for 100 epochs with a batch size of 32. These hyperparameters were chosen based on initial analysis results to achieve optimal performance.\n\nThe reason this model was not published in a machine-learning journal is that the focus of our work is on the application of machine learning to respiratory sound classification, particularly for identifying COVID-19 symptoms. The innovation lies in the specific application and the integration of feature extraction techniques with the CNN architecture to address a critical healthcare challenge. The model's performance was evaluated on a crowdsourced COVID-19 sounds dataset, demonstrating its effectiveness in classifying respiratory diseases, including COVID-19, asthma, bronchitis, and pertussis.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding process involved transforming respiratory sound signals into a format suitable for input into the convolutional neural network (CNN). The primary feature extraction technique used was the logarithmic scaled Mel-spectrum, generated using the 'Essentia' Python library. This spectrum was computed with 256 bands, covering a frequency range of 0\u201322 kHz at a frame rate of 25 ms. The input time duration for the respiratory sound data was fixed at 5 seconds, corresponding to 256 frames, resulting in a 256x256 dimensional input matrix.\n\nThe dataset consisted of various types of respiratory sounds, including cough, voice, and breath, collected from different users. These sounds were categorized into multiple classes, such as COVID-19 with cough, COVID-19 with breath, non-COVID-19 with cough, asthma with cough, pertussis with cough, bronchitis with cough, and healthy sounds. The dataset included 1541 audio files, each with a track size of 30 seconds.\n\nTo enhance the model's performance, data augmentation techniques were applied. The input data was pre-processed to extract time-frequency patches (T-FP) from the logarithmic Mel-spectrum of each respiratory sound excerpt. These patches were used as input to the CNN model, which was designed to learn the parameters mapping the input time-frequency patches to the estimated output classes.\n\nThe model architecture included convolutional layers with different kernel sizes (1x12, 1x24, 1x36, 1x48, and 1x60) to capture various features from the input data. Max-pooling layers were applied after every convolutional layer to reduce dimensionality and enhance training performance. The output of the convolutional layers was flattened and passed through fully connected layers, followed by a softmax activation function to classify the respiratory sounds into the predefined classes.\n\nThe model was trained using categorical cross-entropy as the loss function, with a batch size of 32 and the Adam optimizer. The training process involved 100 epochs, and dropout regularization was used to prevent overfitting. The model's performance was evaluated using accuracy and F1 score metrics, with the best results achieved using the modified Mel-frequency cepstral coefficients (MMFCC) feature extraction technique and a kernel size of 1x12.",
  "optimization/parameters": "In the optimization process of our model, we utilized a total of 122,112 trainable parameters. These parameters were selected based on an initial analysis of the model's performance. The hyper-parameter objective function was updated iteratively to improve the model's framework. The selection of these parameters was crucial for the model's ability to learn and classify respiratory disease symptoms accurately. The parameters were fine-tuned through various experiments, ensuring that the model achieved optimal performance. The choice of 122,112 trainable parameters was determined to balance the model's complexity and its ability to generalize from the training data.",
  "optimization/features": "The input features for the model consist of 477-dimensional vectors. These vectors are derived from respiratory sound data, specifically from the logarithmic scaled Mel-spectrum framework. The features are generated using the 'Essentia' Python library, which creates a log-scaled Mel-spectrum with 256 bands. This spectrum represents the recognizable respiratory sound frequency spectrogram range of 0\u201322 kHz at a frequency frame rate of 25 ms.\n\nThe model utilizes two feature extraction techniques: Enhanced Gamma-tone Frequency Cepstral Coefficients (EGFCC) and Modified Mel-Frequency Cepstral Coefficients (MMFCC). These techniques are applied to the respiratory sound data to extract relevant features for classification.\n\nFeature selection was performed to ensure that the most informative features are used for training and testing the model. This process involved evaluating different kernel sizes and feature types to determine the optimal configuration for accurate classification. The selection was done using the training set only, ensuring that the model's performance on the test set remains unbiased.\n\nThe input time duration for the respiratory sound data is fixed at 5 seconds, which corresponds to 256 frames. This duration is chosen to capture the essential characteristics of the respiratory sounds for accurate classification. The model is designed to learn parameters that map the input time-frequency patches to the estimated class labels, utilizing a light-weight convolutional neural network (CNN) architecture.",
  "optimization/fitting": "The model developed is a light-weight convolutional neural network (CNN) designed to classify respiratory sounds for diagnosing COVID-19 and other respiratory conditions. The input to the model is a 1 \u00d7 477 \u00d7 256 dimensional vector, and it is trained using a two-dimensional convolutional network.\n\nTo prevent overfitting, several techniques were employed. Batch normalization was used to normalize the output of each light-weight CNN layer, which helps in stabilizing and accelerating the training process. Additionally, the Rectified Linear Unit (ReLU) activation function was utilized to introduce sparsity in the network, reducing the number of parameters and the likelihood of overfitting. A dropout layer with a rate of 0.01 was also included to further mitigate overfitting by randomly setting a fraction of input units to zero during training.\n\nThe model consists of five convolutional layers with varying kernel sizes (1 \u00d7 12, 1 \u00d7 24, 1 \u00d7 36, 1 \u00d7 48, and 1 \u00d7 60), followed by a fully connected layer. The convolutional layers extract features from the input data, which are then flattened and passed through a dense layer before the final classification using the Softmax function. This architecture ensures that the model can capture both local and global features from the respiratory sound data.\n\nThe training process involved using categorical cross-entropy as the loss function, the Adam optimizer, a batch size of 32, and 100 epochs. The model has 122,112 trainable parameters and zero non-trainable parameters, indicating a complex but well-regularized network.\n\nTo ensure that the model does not underfit, extensive experimentation was conducted with different kernel sizes, feature channels, and network-layer configurations. The performance was evaluated using accuracy and the F1 score, which provides a balanced measure of precision and recall. The model's performance was compared with existing models on a benchmark dataset, showing improvements of 4% to 10% in accuracy.\n\nIn summary, the model's architecture and training procedures were carefully designed to balance complexity and generalization, ensuring that it neither overfits nor underfits the training data. The use of batch normalization, ReLU activation, and dropout layers, along with thorough experimentation, contributed to the model's robust performance in classifying respiratory sounds.",
  "optimization/regularization": "In our study, we implemented several regularization methods to prevent overfitting and enhance the generalization of our light-weight Convolutional Neural Network (CNN) model. One of the key techniques used was batch normalization. This method was applied to normalize the output of each CNN layer, which helped to stabilize and accelerate the training process. By reducing internal covariate shift, batch normalization allowed for higher learning rates and more consistent training dynamics.\n\nAdditionally, we employed the Rectified Linear Unit (ReLU) activation function. ReLU is known for its ability to introduce non-linearity into the model while mitigating the vanishing gradient problem, which is crucial for deep networks. The use of ReLU activation helped in reducing the number of parameters and connections within the network, thereby promoting sparsity and preventing overfitting.\n\nAnother important regularization technique we utilized was dropout. We incorporated a dropout layer with a dropout rate of 0.01. Dropout works by randomly setting a fraction of the input units to zero during training, which forces the network to learn redundant representations and prevents it from becoming too reliant on any single neuron. This technique significantly improves the model's ability to generalize to unseen data.\n\nFurthermore, we employed max-pooling layers after every convolutional layer. Max-pooling reduces the spatial dimensions of the feature maps, which not only decreases the computational complexity but also helps in making the model more robust to variations in the input data. By down-sampling the feature maps, max-pooling retains the most important information while discarding less relevant details, thereby enhancing the model's ability to generalize.\n\nIn summary, our model incorporated batch normalization, ReLU activation, dropout, and max-pooling as regularization techniques to prevent overfitting and improve the overall performance and generalization of the light-weight CNN. These methods collectively contributed to the robustness and efficiency of our model in classifying respiratory diseases based on sound data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. These include specific settings such as the use of categorical cross-entropy as the loss function, a batch size of 32, the Adam optimizer, three hidden layers with ReLU activation, a dropout rate of 0.01, max pooling, and the Softmax classifier. The model was trained for 100 epochs. The exact number of trainable parameters is also provided, totaling 122,112 with zero non-trainable parameters.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly mentioned as being publicly accessible. However, the methods and configurations are thoroughly described, allowing for replication of the experiments. The publication does not specify a particular license for the use of these configurations, but standard academic practices for sharing research methods apply.\n\nFor those interested in implementing or replicating the model, the detailed descriptions of the architecture, hyper-parameters, and training procedures should suffice. Any additional requests for specific model files or further details can be directed to the corresponding authors for potential sharing under appropriate academic collaboration agreements.",
  "model/interpretability": "The proposed model is not a blackbox. It employs a convolutional neural network (CNN) architecture that, while complex, is designed to be interpretable through its layers and activation functions. The model uses a softmax activation function in the output layer, which ensures that the output nodes have values between 0 and 1, and the sum of all output node values is always 1. This makes the classification probabilities for each class explicit and easy to interpret.\n\nThe model's architecture includes five convolutional layers with different kernel sizes, followed by a fully connected layer. Each convolutional layer applies filters to the input data, extracting features that are then passed to the next layer. The use of rectified linear units (ReLU) as the activation function in these layers helps in reducing parameter interconnections and mitigating overfitting issues, making the model's decision-making process more transparent.\n\nAdditionally, the model incorporates batch normalization to normalize the output of each convolutional layer, which aids in stabilizing and accelerating the training process. The use of max-pooling layers with stride one further simplifies the feature maps, making it easier to understand which features are being emphasized at each stage.\n\nThe model's input consists of 1 \u00d7 477 \u00d7 256 dimensional vectors, and it is trained using a two-dimensional convolutional network. The convolutional kernel dimensions are increased based on column transformation, allowing for a clear understanding of how different features are being extracted and combined.\n\nOverall, the model's design and the use of interpretable components like softmax and ReLU activation functions, along with batch normalization and max-pooling, contribute to its transparency. This makes it possible to trace the decision-making process from the input data through the various layers to the final classification output.",
  "model/output": "The model is a classification model. It uses a Softmax activation function in the output layer to classify respiratory sounds into different categories, such as COVID-19-positive symptoms, COVID-19-negative symptoms, asthma, bronchitis, pertussis, and healthy symptoms. The Softmax function ensures that the output nodes have values between 0 and 1, and the sum of all output node values is always 1, which is suitable for multi-class classification tasks.\n\nThe model's output is represented as \u02c6xc, which is the model prediction class. It is the result of the Softmax function applied to the input data, indicating the probability of each class. The loss function used is categorical cross-entropy, which is commonly used for classification problems.\n\nThe model's architecture includes convolutional layers, fully connected layers, and a Softmax classifier. The convolutional layers extract features from the input data, and the fully connected layers combine these features to make a final prediction. The Softmax classifier then outputs the probability of each class, allowing for the classification of respiratory sounds into the predefined categories.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the proposed light-weight CNN model involved several experiments and comparisons with existing methods. Three relative experiments were conducted to assess different aspects of the model. The first experiment compared different kernel convolution forms, the second experiment evaluated different feature channels, and the third experiment tested various network-layer numbers. The performance of the model was assessed using the accuracy rate and the F1 score, which is the mean average score of recall and precision. These metrics were calculated using true positive, false positive, false negative, and true negative values.\n\nThe model's performance was compared with several existing methods on COVID-19 respiratory sounds datasets. These comparisons included models like SVM with PCA, VGG-Net with Augmentation, and 1DCNN with DDAE. The proposed light-weight CNN model with Modified Mel-Frequency Cepstral Coefficients (MMFCC) demonstrated superior accuracy across multiple classes, achieving the highest accuracy in identifying COVID-19 symptoms and healthy respiratory sounds.\n\nAdditionally, the model was evaluated on its ability to distinguish between different respiratory diseases, including asthma, bronchitis, pertussis, and COVID-19, using various modalities such as voice, cough, and breath sounds. The model showed high accuracy in predicting COVID-19 disease and healthy symptoms, with around 92% and 93% accuracy, respectively. The model also performed well in diagnosing other respiratory conditions, with accuracies ranging from 89% to 90%.\n\nThe evaluation also included a comparison of the model's performance on different tasks and classes, as represented in various tables and figures. The proposed model consistently outperformed existing methods, demonstrating its effectiveness in detecting respiratory sound diseases without the need for X-ray or CT-scan images. The model's ability to prescreen COVID-19 and other respiratory disease symptoms before hospital visits was highlighted, emphasizing its potential for public health applications.",
  "evaluation/measure": "In our evaluation, we primarily focus on two key performance metrics to assess the efficiency of our method: the 'F1' score and the accuracy rate. The 'F1' score is particularly important in multi-classification studies as it indicates the specificity of a test. It is calculated as the mean average score of recall and precision, with 100 percent being the best and zero percent being the worst. The accuracy rate, on the other hand, provides a straightforward measure of the proportion of true results (both true positives and true negatives) among the total number of cases examined.\n\nThese metrics are widely recognized and used in the literature for evaluating classification models, making our set of metrics representative and comparable to other studies in the field. The 'F1' score is especially useful when dealing with imbalanced datasets, as it takes into account both the precision and recall, providing a more comprehensive evaluation of the model's performance. The accuracy rate, while simple, offers a clear and intuitive measure of the model's overall effectiveness.\n\nIn our experiments, we have calculated these metrics using specific equations that involve true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN). This approach ensures that our performance measures are robust and reliable, allowing for a fair comparison with other models and methods in the literature.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we have conducted a thorough evaluation of our proposed light-weight CNN model against various existing methods and simpler baselines on benchmark datasets. The comparison includes models such as SVM with PCA, VGG-Net with Augmentation, and 1DCNN with DDAE, among others. These comparisons were performed on the COVID-19 crowdsourced sounds dataset, which includes respiratory sounds like voice, cough, and breath.\n\nOur proposed model, utilizing Modified Mel-Frequency Cepstral Coefficients (MMFCC), demonstrated superior performance across multiple classes. For instance, it achieved an accuracy of 92.32% for detecting COVID-19 symptoms using voice, breath, and cough sounds. This performance was notably higher than other models, such as the SVM with PCA, which had accuracies ranging from 80% to 82% for different tasks.\n\nAdditionally, we compared our model with simpler baselines, such as MFCC with DCT and ResNet50 with MFCC. The results showed that our light-weight CNN model outperformed these baselines significantly. For example, the MFCC with DCT model had accuracies of 59% and 72% for different classes on a clinical sample dataset, whereas our model achieved accuracies above 88% for similar tasks.\n\nThe evaluation also included a detailed analysis of different kernel shapes and feature channels. We found that the model's performance improved when using a kernel size of 1 \u00d7 12 and MMFCC as the feature channel. This configuration yielded the highest accuracy and F1 score, indicating its effectiveness in classifying respiratory sounds.\n\nOverall, the comparisons with publicly available methods and simpler baselines on benchmark datasets validated the robustness and superiority of our proposed light-weight CNN model in detecting COVID-19 and other respiratory diseases from acoustic signals.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The data was collected through a collaborative effort with Cambridge University, specifically from their respiratory COVID-19 sounds crowdsourced dataset. This dataset includes respiratory sound audio files collected from various users, with each file being 30 seconds long. The data collection process involved an android-based mobile application and a browser-based web application, which were used to obtain respiratory sounds such as breath, voice, and cough.\n\nThe dataset is highly secure and is stored in the local data centers of the University of Cambridge. The information is kept internally until it is linked with the Wi-Fi network, and it is removed from the database once the required data is received from the user\u2019s device. If a user requests that their information be removed from the server, it will be updated accordingly.\n\nDue to the sensitive nature of the data and the ethical guidelines followed during its collection, the raw evaluation files are not publicly released. However, the results and analyses conducted using this dataset are presented in our publication, providing insights into the performance of the light-weight CNN model in identifying various respiratory conditions, including COVID-19."
}