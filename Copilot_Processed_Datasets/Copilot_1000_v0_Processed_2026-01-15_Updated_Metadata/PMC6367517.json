{
  "publication/title": "Using deep learning to identify translational research in genomic medicine beyond bench to bedside.",
  "publication/authors": "Hsu YY, Clyne M, Wei CH, Khoury MJ, Lu Z",
  "publication/journal": "Database : the journal of biological databases and curation",
  "publication/year": "2019",
  "publication/pmid": "30753477",
  "publication/pmcid": "PMC6367517",
  "publication/doi": "10.1093/database/baz010",
  "publication/tags": "- Deep Learning\n- Genomic Medicine\n- Translational Research\n- Machine Learning\n- Convolutional Neural Networks\n- Support Vector Machines\n- Biomedical Literature\n- Public Health Genomics\n- Article Classification\n- Automated Curation",
  "dataset/provenance": "The dataset used in our study consists of articles from the PHGKB, focusing on genomic translational research. The training data set comprises 2286 articles, with 1379 articles classified as T1 and 907 articles as T2\u2013T4. These articles were published between August 23, 2016, and October 14, 2016. The test set includes 400 articles, with 241 articles in T1 and 159 articles in T2\u2013T4, published after October 2016.\n\nOur dataset is derived from a combination of sources. We utilized PubMed\u2019s similar articles search to retrieve additional candidate articles based on a set of 283 previously identified translational research articles. This approach helps in expanding the dataset with relevant articles that can be further reviewed by human curators.\n\nThe articles in our dataset are ranked by their likelihood to be translational articles, based on the predicted scores of our CNN classifier. This ranking strategy aims to establish a prioritized article review process, enabling curators to focus on articles that are more likely to be relevant. The dataset includes articles that have been manually curated in the past, as well as new articles retrieved through our search methods. This combination of data sources ensures a comprehensive and diverse dataset for training and evaluating our machine learning models.",
  "dataset/splits": "The dataset used in our study consists of two main splits: a training set and a test set. The training set comprises 2286 articles, with 1379 articles classified as T1 and 907 articles classified as T2\u2013T4. These articles were published between August 23, 2016, and October 14, 2016. The test set includes 400 articles, with 241 articles in T1 and 159 articles in T2\u2013T4, published after October 2016.\n\nAdditionally, we employed 5-fold cross-validation on the training set to optimize our models and select the best parameters. This cross-validation process involves splitting the training data into five subsets, where the model is trained on four subsets and validated on the remaining one. This procedure is repeated five times, with each subset serving as the validation set once. This method ensures that the model's performance is evaluated on different portions of the data, providing a more robust assessment of its generalization capabilities.",
  "dataset/redundancy": "The dataset used in this study was split into training and test sets to evaluate the performance of machine learning models. The training data set consists of 2286 articles, including 1379 articles in T1 and 907 articles in T2\u2013T4, published from August 23, 2016, to October 14, 2016. The test set, on the other hand, consists of 400 articles, with 241 articles in T1 and 159 articles in T2\u2013T4, published after October 2016. This ensures that the training and test sets are independent, as the articles in the test set were published after the articles in the training set.\n\nThe distribution of articles in the training and test sets reflects the challenge of the classification task, where the number of translational articles (T2\u2013T4) is much smaller compared to basic research articles (T1). This imbalance is a common characteristic in many machine learning datasets, particularly in text classification tasks involving specialized domains like genomic research.\n\nTo further validate the models, 5-fold cross-validation was performed on the training set. This technique involves dividing the training data into five subsets, training the model on four subsets, and validating it on the remaining subset. This process is repeated five times, with each subset serving as the validation set once. The results of the cross-validation experiments were used to tune the model parameters and select the best-performing model.\n\nThe independent test set was used to evaluate the final performance of the models, providing an unbiased assessment of their generalization capability. The results on the test set showed that the convolutional neural network (CNN) classifier outperformed the support vector machine (SVM) classifier, achieving higher precision, recall, and F-measure scores. This demonstrates the effectiveness of the CNN model in handling the imbalanced dataset and capturing the relevant features for the classification task.",
  "dataset/availability": "Not applicable",
  "optimization/algorithm": "The machine-learning algorithm class used in this work is convolutional neural networks (CNNs). This approach is not entirely new, as CNNs have been previously applied in various natural language processing (NLP) tasks, including text classification. They have shown improved results in document triage for kinome curation, genomic variation, and protein-protein interactions.\n\nThe decision to use CNNs in this specific context was driven by their demonstrated effectiveness in capturing the characteristics of beyond bench-to-bedside phase articles. The CNN-based approach was chosen because it has the potential to greatly improve the current workflow of curating beyond bench-to-bedside articles in genomic translational research.\n\nThe reason this work was not published in a machine-learning journal is that the primary focus is on applying established machine-learning techniques to a specific problem in genomic medicine. The innovation lies in the application of these techniques to identify translational research in genomics beyond bench to bedside, rather than in the development of a new machine-learning algorithm. The goal is to utilize state-of-the-art machine learning approaches to enhance the efficiency of manual curation in this domain.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The model is based on two separate machine learning algorithms: Support Vector Machines (SVMs) and Convolutional Neural Networks (CNNs). These algorithms are used independently to classify genomic research translational phase articles. The SVM is used as a baseline method, while the CNN is the primary classifier. The CNN architecture includes convolutional layers, pooling layers, and fully connected layers, which are trained using article titles and abstracts with a bag-of-words model. The performance of these models is evaluated using precision, recall, and the F-measure. The CNN consistently outperforms the SVM in both cross-validation experiments on the training set and on the independent test set. The training data for the CNN is derived from articles published from August 23, 2016, to October 14, 2016, while the test set includes articles published after October 2016. This ensures that the training and test data are independent.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithms involved several steps. For the support vector machines (SVMs), the articles' titles and abstracts were encoded using the bag-of-words model. This model represents text data by counting the frequency of words, converting the text into numerical features that the SVM can process. Additionally, features such as the journal title were tested, but they did not improve the overall performance.\n\nFor the convolutional neural networks (CNNs), the text data was preprocessed and encoded using the Keras library with TensorFlow. The implementation followed a specific approach for text classification using Convolution1D, which is available in the Keras examples. This method involves converting text into sequences of integers based on a vocabulary, and then padding or truncating these sequences to ensure uniform input lengths for the CNN. The CNN architecture includes convolutional layers that detect patterns and extract salient features, pooling layers that down-sample the feature maps, and fully connected layers that transform and vote on important features to produce predicted class probabilities. The CNN parameters were empirically optimized based on the training data, with specific attention to the batch size and kernel size to achieve the best performance.",
  "optimization/parameters": "In our study, we focused on optimizing two key parameters for our Convolutional Neural Network (CNN) model: batch size and kernel size.\n\nThe batch size controls the number of samples that propagate through the CNN network during the training phase. We observed that the peak performance of our model appeared when the batch size was between 50 and 80. This range was determined through benchmarking with different combinations of batch sizes and analyzing their impact on the training loss curve and computation efficiency.\n\nThe kernel size at the convolutional layers defines the dimensions of the feature maps and affects how much neighbor information can be processed. Through our experiments, we found that the best value for the kernel size was 8. This value was selected based on its performance in capturing relevant features from the data.\n\nIn summary, our model uses two primary parameters: batch size and kernel size. The optimal values for these parameters were determined through empirical testing and analysis of their effects on model performance.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "In our study, we employed convolutional neural networks (CNNs) and support vector machines (SVMs) for the classification task. The CNNs, in particular, have a large number of parameters due to their architecture, which includes convolutional layers, pooling layers, and fully connected layers. To address the potential issue of overfitting, we utilized several strategies.\n\nFirstly, we performed 5-fold cross-validation on the training set to ensure that our model generalized well to unseen data. This technique helps in assessing the model's performance and stability across different subsets of the data.\n\nSecondly, we tuned hyperparameters such as batch size and kernel size. For the batch size, we observed that the optimal performance was achieved when it was between 50 and 80. This tuning process helped in balancing the trade-off between computational efficiency and model performance.\n\nFor the kernel size, we found that a value of 8 worked best for our data. The kernel size affects the dimensions of feature maps and how much neighbor information is processed, which is crucial for capturing relevant patterns in the text data.\n\nAdditionally, we used dropout layers and pooling layers in our CNN architecture to reduce the number of parameters and control overfitting. These layers help in down-sampling the feature maps and preventing the model from becoming too complex.\n\nTo further validate our model, we compared its performance with a baseline method using ROC curves. The CNN classifier significantly outperformed the baseline method, achieving an improvement of 29.6% and 28.6% over two weeks of data. This indicates that our model is not only effective but also robust against overfitting.\n\nIn summary, by employing cross-validation, hyperparameter tuning, and architectural modifications, we ensured that our CNN model generalized well to new data and did not overfit. The consistent performance across different datasets and the significant improvement over the baseline method further support the robustness of our approach.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting during the optimization of our Convolutional Neural Network (CNN) classifier. One key method involved tuning the batch size, which controls the number of samples propagating through the network during training. By experimenting with different batch sizes, we found that the optimal performance was achieved when the batch size was between 50 and 80. This tuning helped in stabilizing the training process and improving the generalization of the model.\n\nAdditionally, we utilized pooling layers in our CNN architecture. These layers down-sample the feature maps generated by the convolutional layers, reducing the number of parameters and thereby controlling overfitting. Pooling layers help in extracting the most salient features while discarding less important information, which enhances the model's ability to generalize to new data.\n\nWe also performed 5-fold cross-validation on the training set to select the best parameters for our models. This technique ensures that the model is evaluated on different subsets of the data, providing a more robust estimate of its performance and helping to prevent overfitting to a specific subset of the training data.\n\nFurthermore, we empirically optimized the CNN parameters based on the training data. This involved testing different combinations of parameters, such as kernel size and batch size, to find the configuration that yielded the best performance. The kernel size, in particular, was found to be crucial, with a size of 8 providing the best results for our data.\n\nIn summary, our approach to preventing overfitting included batch size tuning, the use of pooling layers, cross-validation, and empirical parameter optimization. These methods collectively contributed to the development of a robust and generalizable CNN classifier for the classification of genomic translational research articles.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in the discussion section and supplementary materials. Specifically, we detail the best values for kernel size and batch size, which were found to be 8 and between 50 and 80, respectively. These configurations were determined through benchmarking with different combinations of these parameters.\n\nThe optimization schedule and model files are not explicitly provided in the main text, but the implementation details and data preprocessing steps are available on a public GitHub repository. This repository contains the code for the Convolution1D used for text classification, which can be accessed at https://github.com/keras-team/keras/blob/master/examples/imdb_cnn.py. The supplementary materials also include additional tables that provide further insights into the optimization process.\n\nRegarding the availability and licensing of these resources, the code on GitHub is open-source and can be used under the terms of the Apache License 2.0. The supplementary data, including tables and additional information, are available at Database Online. This ensures that researchers can replicate our experiments and build upon our findings.\n\nNot sure if the model files are available for download, but the implementation details and preprocessing steps are publicly accessible, allowing for reproducibility.",
  "model/interpretability": "The model we developed is primarily based on Convolutional Neural Networks (CNNs), which are known for their ability to capture complex patterns in data. However, CNNs are often considered black-box models due to their intricate architecture and the difficulty in interpreting the internal representations they learn. This lack of transparency can be a challenge, especially in fields like genomics where interpretability is crucial for gaining insights and building trust.\n\nTo address this, we have considered several strategies to enhance the interpretability of our model. One approach is to use word features that the CNN model utilizes for classification. By identifying the important words involved in the machine decisions, human experts can analyze these features and suggest additional improvements. This can be facilitated by a user-friendly visualization tool for neural networks, which would allow experts to see which words or phrases are most influential in the model's predictions.\n\nAdditionally, we have explored the idea of interactive machine learning, where human experts can actively engage with the model. This interaction can help in understanding the model's decisions better and in refining the model based on expert feedback. An explainable system can also make manual curators more comfortable working with artificial intelligence algorithms, as it provides a clearer understanding of how the model arrives at its predictions.\n\nIn summary, while our CNN model is inherently a black-box, we are taking steps to make it more transparent and interpretable. By focusing on word features and incorporating interactive and explainable elements, we aim to bridge the gap between the model's predictions and human understanding.",
  "model/output": "The model is a classification model. Specifically, it is designed to classify genomic research articles into different translational phases. The task is formulated as a binary document classification problem, where the model predicts whether an article belongs to the translational phases T2\u2013T4 or not. The performance of the model is evaluated using metrics such as precision, recall, and the F-measure, which are commonly used in classification tasks. The model employs convolutional neural networks (CNNs) and support vector machines (SVMs) to achieve this classification. The CNN-based classifier, in particular, has shown superior performance in distinguishing relevant translational articles from the rest.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our method involved several steps to ensure its validity and utility. Initially, we performed 5-fold cross-validation experiments on the training dataset to tune and select the best-performing models. This process helped us to optimize the parameters for both the Convolutional Neural Networks (CNNs) and Support Vector Machines (SVMs).\n\nFollowing the cross-validation, we applied the best-tuned models to an independent test set consisting of 400 articles. This test set was not used during the training phase, providing an unbiased evaluation of the models' performance. The metrics used for evaluation included precision, recall, and the F-measure, which is the harmonic mean of precision and recall.\n\nTo assess the utility of our approach in a real-world scenario, we conducted additional evaluations over a two-week period in June 2017. During this time, we collected and analyzed new articles, comparing the performance of our CNN classifier against the baseline method of sorting articles by publication date. The results showed that our CNN classifier significantly outperformed the baseline method, achieving improvements of 29.6% and 28.6% in the two weeks, respectively.\n\nFurthermore, we analyzed the classification errors of our CNN model on the test set, identifying false positives and false negatives. This error analysis helped us understand the limitations of our model and areas for potential improvement. Overall, the evaluation demonstrated that our machine learning approach effectively reduces the workload of manual triage and curation, enhancing the efficiency of identifying translational research in genomic medicine.",
  "evaluation/measure": "In our evaluation, we focus on three key performance metrics to assess the validity of our models: precision, recall, and the F-measure. Precision is defined as the fraction of the number of relevant T2\u2013T4 articles divided by the total number of predicted articles in this category. Recall, on the other hand, is the fraction of the number of relevant T2\u2013T4 articles divided by the number of actual T2\u2013T4 articles in the gold-standard data set. The F-measure, which provides a single score that balances both precision and recall, is calculated as the harmonic mean of the two, using the formula: F-measure = 2\u00d7 [(recall \u00d7 precision)/(recall + precision)].\n\nThese metrics are widely used in the literature for evaluating classification tasks, particularly in text classification and information retrieval. They provide a comprehensive view of a model's performance by considering both the correctness of positive predictions (precision) and the ability to identify all relevant instances (recall). The F-measure is especially useful when there is an uneven class distribution, as it gives a single value that reflects the trade-off between precision and recall.\n\nWe report these metrics for both our training and test sets, ensuring that our models are evaluated on independent data. Additionally, we use cross-validation on the training set to tune our models and select the best parameters, further validating our approach. This set of metrics is representative of standard practices in the field and allows for a clear comparison with other studies.",
  "evaluation/comparison": "In our evaluation, we compared the performance of our convolutional neural network (CNN) classifier with a support vector machine (SVM) classifier. The SVM was included as a comparison algorithm due to its superior performance in previous studies. Both classifiers were applied to the same dataset, which consisted of articles classified into two translational phases: the initial bench-to-bedside phases (T1) and the beyond bench-to-bedside phases (T2\u2013T4).\n\nThe CNN classifier demonstrated superior performance compared to the SVM. In 5-fold cross-validation experiments on the training set, the CNN achieved a precision of 0.7681, a recall of 0.8785, and an F-measure of 0.8196. In contrast, the SVM had a precision of 0.7688, a recall of 0.7354, and an F-measure of 0.7517. Similarly, on the independent test set, the CNN outperformed the SVM with a precision of 0.7614, a recall of 0.8428, and an F-measure of 0.8000, compared to the SVM's precision of 0.7615, recall of 0.7232, and F-measure of 0.7419.\n\nAdditionally, we compared the utility of the CNN classifier in a real-world task of curating translational articles. The CNN classifier significantly outperformed the baseline method of sorting articles by publication date, achieving an improvement of 29.6% and 28.6% over two weeks. This indicates that the CNN classifier is not only more accurate but also more efficient in prioritizing articles for human review.\n\nIn summary, our comparison with the SVM classifier and the baseline method demonstrates that the CNN classifier is a more effective tool for identifying translational research in genomic medicine beyond bench to bedside.",
  "evaluation/confidence": "Evaluation Confidence\n\nThe evaluation of our methods involved a rigorous assessment of their validity and utility. For the validity assessment, we employed precision, recall, and the F-measure to evaluate the performance of our classifiers. The CNN-based classifier demonstrated superior performance with an F-measure of 0.8196 on the training set and 0.8000 on the test set, outperforming the SVM classifier consistently. These results indicate a high level of confidence in the CNN's ability to accurately classify translational articles.\n\nTo further validate our findings, we conducted additional evaluations using two weeks' worth of data. The receiver operating characteristic (ROC) curves showed significant improvements of 29.6% and 28.6% for the CNN classifier over the baseline date-sort method. This statistical significance suggests that the CNN classifier is indeed superior in ranking articles by their likelihood to be translational.\n\nMoreover, the error analysis provided insights into the classification errors, revealing that some errors were due to mis-curation in the gold standard or borderline cases. This analysis helps in understanding the limitations and areas for improvement in our model.\n\nOverall, the performance metrics and statistical significance of our results provide a strong confidence in the superiority of the CNN-based classifier for the task of curating translational articles in genomic research.",
  "evaluation/availability": "Not applicable"
}