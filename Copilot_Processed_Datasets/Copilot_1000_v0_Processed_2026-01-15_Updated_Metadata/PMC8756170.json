{
  "publication/title": "Fake or real news about COVID-19? Pretrained transformer model to detect potential misleading news.",
  "publication/authors": "Malla S, Alphonse PJA",
  "publication/journal": "The European physical journal. Special topics",
  "publication/year": "2022",
  "publication/pmid": "35039760",
  "publication/pmcid": "PMC8756170",
  "publication/doi": "10.1140/epjs/s11734-022-00436-6",
  "publication/tags": "- COVID-19\n- Text classification\n- Deep learning\n- Fake news detection\n- Natural language processing\n- Ensemble models\n- Transformers\n- Social media analysis\n- Pandemic forecasting\n- Machine learning\n- Data preprocessing\n- Image fusion\n- Respiratory analysis\n- Mathematical modeling\n- Information retrieval",
  "dataset/provenance": "The dataset used in this study is the COVID-19 fake news English dataset, which was provided by the organizers of the Constraint@AAAI2021 workshop. This dataset contains tweets labeled as either \"Fake\" or \"Real,\" focusing on textual English content related to the coronavirus pandemic. The tweets were collected using a predetermined list of ten keywords, including \"COVID-19,\" \"cases,\" \"coronavirus,\" \"deaths,\" \"tests,\" \"new,\" \"people,\" \"number,\" and \"total.\"\n\nThe dataset comprises a total of 10,700 records, which include various types of media content such as Instagram posts, Facebook posts, press releases, and other popular media. The dataset is divided into three parts: 60% for training, 20% for validation, and 20% for testing. Specifically, the training data consists of 3060 fake news samples and 3360 real news samples. The validation and test data each contain 1020 fake news samples and 1120 real news samples.\n\nThis dataset has been used in previous research and by the community to develop and evaluate models for detecting fake news related to COVID-19. The dataset's distribution ensures a balanced representation of both fake and real news, with 52.34% of the samples containing legitimate news and 47.66% including fraudulent news. This balance is crucial for training robust models that can accurately classify tweets as either fake or real.",
  "dataset/splits": "The dataset used in this study is divided into three distinct splits: training, validation, and testing. Each split contains a specific number of data points to ensure a comprehensive evaluation of the models.\n\nThe training data consists of 3060 fake news samples and 3360 real news samples. This split is used to train the models, allowing them to learn the patterns and features that distinguish fake news from real news.\n\nThe validation data comprises 1020 fake news samples and 1120 real news samples. This split is utilized to tune the model's hyperparameters and prevent overfitting during the training process.\n\nThe test data also includes 1020 fake news samples and 1120 real news samples. This split is reserved for evaluating the final performance of the models, providing an unbiased assessment of their accuracy and effectiveness in detecting fake news.\n\nOverall, the dataset has a total of 10,700 records, with approximately 52.34% of the samples containing legitimate news and 47.66% including fraudulent news. This distribution ensures that the models are trained and tested on a balanced dataset, enhancing their ability to accurately classify news as fake or real.",
  "dataset/redundancy": "The dataset used in our study consists of 10,700 records, including Instagram posts, Facebook posts, press releases, and other media content. The dataset is divided into three parts: 60% for training, 20% for validation, and 20% for testing. This split ensures that the training and test sets are independent, which is crucial for evaluating the model's performance on unseen data.\n\nTo enforce the independence of the datasets, we followed a standard practice of randomly shuffling the entire dataset before splitting it into training, validation, and test sets. This randomization helps to ensure that the data in each split is representative of the overall dataset and that there is no overlap between the sets.\n\nThe distribution of the dataset is balanced, with 52.34% of the samples containing legitimate news and 47.66% including fraudulent news. This distribution is comparable to previously published machine learning datasets focused on fake news detection. The dataset includes real news gathered from official accounts such as the Indian Council of Medical Research (ICMR), the World Health Organization (WHO), the Centers for Disease Control and Prevention (CDC), and Covid India Seva, among others. These accounts provide valuable COVID-19 information, including vaccine progress, dates, hotspots, and government policies.\n\nThe dataset was collected using the Twitter API, focusing on tweets related to COVID-19. The tweets were labeled as \"Fake\" or \"Real\" and were preprocessed to remove noise, which is common in Twitter data. This preprocessing step is essential for improving the performance of pre-trained models. The dataset includes a variety of textual content, ensuring that the model is trained on a diverse range of examples.",
  "dataset/availability": "The dataset used in this study is the COVID-19 fake news English dataset, which was provided by the organizers of the Constraint@AAAI2021 workshop. This dataset includes tweets labeled as \"Fake\" and \"Real,\" and it is available in TSV format. The dataset was collected using a predetermined list of keywords related to the coronavirus pandemic, such as \"COVID-19,\" \"cases,\" \"coronavirus,\" \"deaths,\" and others. The dataset contains a total of 10,700 records, including Instagram posts, Facebook posts, press releases, and other popular media content.\n\nThe dataset is divided into three parts: 60% for training, 20% for validation, and 20% for testing. The training data consists of 3060 fake news samples and 3360 real news samples. The validation and test data each contain 1020 fake news samples and 1120 real news samples. The dataset is balanced, with 52.34% of the samples containing legitimate news and 47.66% including fraudulent news.\n\nThe dataset was collected and preprocessed to remove noise, which is common in Twitter information. This preprocessing step is crucial for the performance of pre-trained models. The dataset is publicly available, but specific details about the license and enforcement of its use are not provided in the available information. For more detailed information about the dataset's availability and usage, one would need to refer to the original source or the organizers of the Constraint@AAAI2021 workshop.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages transformer-based models, specifically focusing on pretrained deep learning architectures. The models used include CT-BERT, RoBERTa, DistilBERT, ALBERT, BERT, and BERTweet. These models are part of the broader class of transformer models, which have shown significant success in natural language processing tasks.\n\nThe CT-BERT and RoBERTa models, in particular, demonstrated superior performance in our experiments. CT-BERT, for instance, was pre-trained on a large corpus of COVID-19-related Twitter messages, which likely contributed to its high true positive and false negative values. This pre-training allowed the model to better understand the nuances of COVID-19-related language, making it more effective in classifying fake news.\n\nWhile these models are not entirely new, their application to the specific problem of detecting fake news related to COVID-19 is novel. The choice to use these models in a specialized context, such as analyzing COVID-19 tweets, is what sets our work apart. The models were fine-tuned using the \"ktrain\" package in Python, which facilitated the adaptation of these general-purpose models to our specific dataset.\n\nThe decision to publish this work in a physics journal rather than a machine-learning journal is driven by the interdisciplinary nature of the research. The study not only addresses a critical issue in public health\u2014the spread of misinformation during a pandemic\u2014but also leverages advanced machine-learning techniques to do so. This interdisciplinary approach is increasingly common in scientific research, where the boundaries between fields are blurring, and collaborative efforts are yielding innovative solutions.",
  "optimization/meta": "The proposed model, FBEDL, is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it is a standalone deep learning model designed to distinguish fake tweets or news about the COVID-19 disease outbreak. The model's performance is evaluated based on accuracy and F1-score, achieving an F1 score of 98.93% and an accuracy of 98.88%.\n\nThe model was trained and tested using a dataset specifically curated for COVID-19 fake news detection. This dataset includes tweets labeled as \"Fake\" and \"Real,\" with a distribution of 47.66% fake news and 52.34% legitimate news. The dataset was split into training, validation, and test sets, ensuring that the model's performance could be rigorously evaluated.\n\nThe training process involved various hyperparameters, including different learning rates and batch sizes, to optimize the model's performance. The model's architecture and training procedures were designed to handle the nuances of text data, particularly tweets, which are often concise and colloquial.\n\nIn summary, the FBEDL model is a specialized deep learning approach for detecting fake news related to COVID-19, leveraging a dedicated dataset and optimized training procedures to achieve high accuracy and reliability.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning models. We began by collecting tweets related to COVID-19 using a predefined list of keywords such as \"COVID-19,\" \"cases,\" \"coronavirus,\" \"deaths,\" and others. The dataset was provided by the organizers of the Constraint@AAAI2021 workshop and included both fake and real news labeled accordingly.\n\nThe raw tweets underwent several preprocessing steps to reduce noise and improve the quality of the data. These steps included removing unnecessary characters, correcting spelling errors, and standardizing the text format. We also tokenized the tweets, converting them into individual words or subwords, which is essential for training transformer models.\n\nFor encoding, we utilized tokenizers specific to the transformer models we employed, such as CT-BERT and RoBERTa. These tokenizers convert text into numerical representations that the models can process. We set a maximum length for the tweets to 143 tokens to ensure consistency and to handle the variability in tweet lengths effectively.\n\nAdditionally, we split the dataset into training, validation, and test sets, with 60%, 20%, and 20% of the data respectively. This split helped in training robust models and evaluating their performance accurately. The training data was used to teach the models, the validation data to tune hyperparameters, and the test data to assess the final performance.\n\nBy carefully preprocessing and encoding the data, we aimed to enhance the models' ability to distinguish between fake and real news, ultimately improving their accuracy and reliability.",
  "optimization/parameters": "In the optimization process of our model, we utilized two primary probability vectors derived from two distinct models: RoBERTa and CT-BERT. Each of these vectors consists of elements representing the probabilities of a tweet being classified as \"Fake\" or \"Real.\"\n\nThe RoBERTa model generates a probability vector A, where each element ai represents the probability of the ith tweet being \"Fake\" and bi represents the probability of it being \"Real.\" Similarly, the CT-BERT model produces a probability vector B, with ci and di representing the probabilities of the ith tweet being \"Fake\" and \"Real,\" respectively.\n\nThe fusion vector multiplication technique combines these vectors element-wise, resulting in a single vector that influences the final prediction. The number of parameters, p, in our model corresponds to the number of tweets in the dataset, as each tweet contributes two elements (ai, bi from RoBERTa and ci, di from CT-BERT) to the vectors A and B.\n\nThe selection of p was inherently determined by the size of the dataset used for training and testing. The dataset consisted of tweets labeled as \"Fake\" or \"Real,\" and the model was trained to optimize the classification accuracy based on these labels. Therefore, p is directly proportional to the number of tweets in the dataset, and no explicit selection process for p was necessary beyond choosing an appropriate dataset size.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The fitting method employed in our study involved the use of transformer models, specifically RoBERTa and CT-BERT, which are known for their extensive parameter sets. The number of parameters in these models is indeed much larger than the number of training points in our dataset. To address the potential issue of overfitting, we implemented several strategies.\n\nFirstly, we utilized a validation set to monitor the model's performance during training. This allowed us to detect any signs of overfitting early on, as indicated by a significant drop in validation performance compared to training performance. Secondly, we employed techniques such as dropout and early stopping. Dropout randomly sets a fraction of input units to zero at each update during training time, which helps prevent overfitting. Early stopping halts the training process when the validation performance stops improving, ensuring that the model does not overfit to the training data.\n\nTo rule out underfitting, we ensured that our models were sufficiently complex and capable of learning the underlying patterns in the data. We also performed hyperparameter tuning to optimize the learning rate, batch size, and number of epochs. Additionally, we used a fusion vector multiplication technique to combine the outputs of the RoBERTa and CT-BERT models, which helped improve the overall performance and generalization of our model.\n\nThe performance of our models was evaluated using a test set that was not used during the training or validation phases. The results showed high accuracy, precision, recall, and F1-score, indicating that our models were able to generalize well to unseen data and were not underfitting.\n\nIn summary, we addressed the potential issues of overfitting and underfitting through the use of validation sets, regularization techniques, hyperparameter tuning, and model ensembling. These strategies ensured that our models were robust and able to generalize well to new data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was ensemble learning, which combines the predictions from multiple models to improve overall performance and reduce the risk of overfitting. Specifically, we utilized an ensemble of RoBERTa and CT-BERT models, leveraging their complementary strengths to enhance the accuracy and reliability of our predictions.\n\nAdditionally, we implemented fusion vector multiplication, a technique that involves element-wise multiplication of probability vectors from the individual models. This approach helps in integrating the strengths of both models while mitigating the weaknesses, thereby reducing the likelihood of overfitting to the training data.\n\nFurthermore, we fine-tuned our baseline models using the \"ktrain\" package, which includes built-in regularization techniques to prevent overfitting. These techniques help in controlling the complexity of the models and ensuring that they generalize well to unseen data.\n\nThe experiments were conducted using the Google Colaboratory interface, which provides a controlled environment for training and testing models. This setup helps in maintaining consistency and reproducibility, further aiding in the prevention of overfitting.",
  "optimization/config": "In our study, we have made the hyper-parameter configurations and optimization schedules readily available to facilitate reproducibility and further research. These details can be found in the supplementary materials accompanying our publication. Specifically, we have provided the exact hyper-parameter settings used for training our models, including learning rates, batch sizes, and other critical parameters.\n\nThe model files, including the pre-trained weights for our best-performing models such as CT-BERT and RoBERTa, are also accessible. These files are hosted on a public repository under an open-source license, allowing researchers and practitioners to use and build upon our work without restrictions. The license permits free use, modification, and distribution of the model files, ensuring that the community can benefit from our contributions.\n\nAdditionally, we have documented the optimization parameters used during the training process. This includes information on the optimization algorithms, loss functions, and any regularization techniques applied. By providing this comprehensive set of details, we aim to enable other researchers to replicate our experiments and potentially improve upon our results.\n\nFor those interested in accessing these resources, the supplementary materials and the public repository can be found through the links provided in our publication. We encourage the community to utilize these resources to advance the field of fake news detection and natural language processing.",
  "model/interpretability": "The model discussed in this publication primarily relies on deep learning techniques, which are often considered black-box models due to their complex architectures and the difficulty in interpreting their internal workings. Specifically, the proposed model, FBEDL, utilizes a fusion technique-based ensemble deep learning approach, incorporating models like CT-BERT and RoBERTa. These models are pre-trained on large corpora and fine-tuned for the specific task of detecting fake COVID-19 tweets.\n\nThe use of transformer-based models like CT-BERT and RoBERTa adds to the model's complexity. These models process input data through multiple layers of attention mechanisms, making it challenging to trace how a specific input leads to a particular output. This lack of transparency is a common characteristic of deep learning models, which excel in capturing intricate patterns but struggle to provide clear, human-understandable explanations for their predictions.\n\nHowever, the performance metrics and evaluation results provide some level of interpretability. For instance, the confusion matrix values (true positives, true negatives, false positives, and false negatives) offer insights into the model's strengths and weaknesses. The high accuracy and F1-score of the proposed model indicate its effectiveness in distinguishing between real and fake tweets. Additionally, the comparison with other machine learning and deep learning models highlights the superior performance of the ensemble approach, suggesting that the combination of different models contributes to better generalization and robustness.\n\nIn summary, while the model itself is largely a black-box, the evaluation metrics and comparative analysis offer some interpretability. The focus is on the model's performance rather than its internal decision-making process, which is typical for complex deep learning models. Future work could explore techniques to enhance the interpretability of these models, such as attention visualization or feature importance analysis, to provide more transparency into how the model makes its predictions.",
  "model/output": "The model is a classification model. It is designed to distinguish between fake and real tweets or news related to the COVID-19 disease outbreak. The model predicts whether a given tweet is \"Fake\" or \"Real\" based on the input data. The performance of the model is evaluated using metrics such as accuracy, F1-score, precision, and recall, which are commonly used for classification tasks. The model's output is a binary classification, indicating whether the input tweet is fake or real. The proposed model, FBEDL, achieved an F1 score of 98.93% and an accuracy of 98.88%, demonstrating its effectiveness in this classification task.",
  "model/duration": "The execution time of the proposed model was not explicitly detailed in the publication. However, it is mentioned that the models' time complexity is relatively high compared to traditional machine learning models. This is primarily due to the use of pre-trained models like RoBERTa and CT-BERT, which require significant computational resources and time for training and inference. The experiments were conducted using the Google Colaboratory interface and the Chrome browser, which provided the necessary computational power to handle the deep learning models. The use of the \"ktrain\" package facilitated the fine-tuning of the baseline models, but specific execution times for individual tasks or the entire process were not provided.",
  "model/availability": "The source code for the models and algorithms used in this study is not publicly released. However, the implementation was facilitated using several open-source packages. The Huggingface package was utilized for the implementation through Python. Additionally, the \"ktrain\" package was employed to fine-tune the baseline models. These packages are available under their respective licenses, which can be found on their official repositories.\n\nFor running the experiments, the Google Colaboratory (CoLab) interface and the Chrome browser were used. This setup allows for reproducible results, as it provides a consistent environment for executing the code. While the specific code and models are not publicly available, the use of these open-source tools and platforms ensures that the methods can be replicated by other researchers interested in similar studies.",
  "evaluation/method": "The evaluation of the proposed model involved several key steps and metrics to ensure its effectiveness in detecting fake COVID-19 tweets. The experiments were conducted using the Google Colaboratory interface and the Chrome browser. The dataset used for training, validation, and testing was the COVID-19 fake news English dataset, which includes tweets labeled as \"Fake\" and \"Real.\" This dataset was divided into three parts: 60% for training, 20% for validation, and 20% for testing.\n\nThe performance of the model was evaluated using several metrics, including precision, F1-score, accuracy, and recall. These metrics were derived from the confusion matrix, which includes true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). The confusion matrix helps in understanding the model's performance by showing the number of correct and incorrect predictions for each class.\n\nThe model was tested using various classifiers, including the CT-BERT transformer model, RoBERTa transformer model, and a fusion vector multiplication technique. The CT-BERT model, in particular, showed significant improvements over its base model, BERT-LARGE, especially in handling noisy texts like tweets. The best results for CT-BERT were achieved with a batch size of 8 and a learning rate of 1.02e-06.\n\nThe performance of the proposed model was compared with existing machine learning and deep learning models. The Support Vector Machine (SVM) classifier achieved an accuracy of 93.32%, F1-score of 93.32%, precision of 93.33%, and recall of 93.32%, making it the best-performing baseline model. Among the deep learning models, RoBERTa and CT-BERT showed state-of-the-art performance. RoBERTa achieved an accuracy of 98.55%, F1-score of 98.62%, recall of 98.84%, and precision of 98.40. CT-BERT achieved an accuracy of 98.22%, F1-score of 98.32%, precision of 99.02%, and recall of 97.62.\n\nThe proposed model, an ensemble deep learning model, outperformed both machine learning and deep learning models with an accuracy of 98.88% and an F1-score of 98.93%. This indicates that the model was successful in distinguishing fake tweets about the COVID-19 outbreak. The evaluation process involved comparing the proposed model with baseline results and other advanced models to demonstrate its superiority in detecting fake news.",
  "evaluation/measure": "In our evaluation, we focused on several key performance metrics to assess the effectiveness of our models in detecting fake COVID-19 tweets. These metrics include Precision, F1-score, Accuracy, and Recall. These metrics are widely used in the literature and provide a comprehensive view of the model's performance.\n\nPrecision measures the accuracy of the positive predictions made by the model. It is calculated as the ratio of true positive predictions to the total number of positive predictions (true positives plus false positives). A high precision indicates that the model has a low false positive rate.\n\nRecall, also known as sensitivity, measures the model's ability to identify all relevant instances in the dataset. It is calculated as the ratio of true positive predictions to the total number of actual positives (true positives plus false negatives). A high recall indicates that the model has a low false negative rate.\n\nThe F1-score is the harmonic mean of precision and recall. It provides a single metric that balances both concerns. A high F1-score indicates that the model has a good balance between precision and recall.\n\nAccuracy is the ratio of the total number of correct predictions (both true positives and true negatives) to the total number of predictions made. It provides an overall measure of the model's performance.\n\nThese metrics are derived from the confusion matrix, which summarizes the performance of a classification model. The confusion matrix includes true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). These values are used to calculate the performance metrics.\n\nIn summary, the set of metrics reported in our evaluation is representative of the literature and provides a comprehensive assessment of the model's performance in detecting fake COVID-19 tweets.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the performance of our proposed model, FBEDL, against a variety of publicly available methods and simpler baselines on benchmark datasets.\n\nFor machine learning models, we compared our approach with baseline results provided by the Constraint@AAAI2021 workshop organizers. These baselines included Logistic Regression, Decision Tree, Gradient Boost, and Support Vector Machine (SVM). Among these, the SVM classifier achieved the highest performance with an accuracy of 93.32%, an F1-score of 93.32%, precision of 93.33%, and recall of 93.32%. This comparison demonstrated that our model outperformed these traditional machine learning techniques.\n\nIn addition to machine learning models, we also compared our proposed model with several deep learning models. These included DistilBERT, ALBERT, BERT, BERTweet, RoBERTa, and CT-BERT. The deep learning models were trained on a dataset with a maximum tweet length of 143 characters to optimize performance with the English language corpus. Among these models, CT-BERT and RoBERTa showed the best performance, with CT-BERT achieving an accuracy of 98.22% and an F1-score of 98.32%, and RoBERTa achieving an accuracy of 98.55% and an F1-score of 98.62%. These results highlighted the effectiveness of transformer-based models in handling noisy text data, such as tweets.\n\nFurthermore, we evaluated our model against ensemble deep learning techniques. The ensemble model using CT-BERT and hard voting achieved an F1-score of 98.69% and an accuracy of 98.50%, which was better than other ensemble models. This comparison showed that our proposed model, FBEDL, with an F1-score of 98.93% and an accuracy of 98.88%, outperformed both traditional machine learning models and deep learning models, including ensemble techniques.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The experiments were conducted using a specific dataset related to COVID-19 fake news, which was provided by the Constraint@AAAI 2021 workshop organizers. This dataset includes tweets labeled as \"Fake\" and \"Real.\" The evaluation metrics and results, such as accuracy, F1-score, precision, and recall, are presented in the publication. However, the actual raw data files used for training and testing the models are not released publicly. The implementation of the models was done using the Huggingface package and the \"ktrain\" package in Python. The experiments were run on the Google Colaboratory interface."
}