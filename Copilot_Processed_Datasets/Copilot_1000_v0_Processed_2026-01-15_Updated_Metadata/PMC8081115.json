{
  "publication/title": "Association Between Coffee Intake and Incident Heart Failure Risk: A Machine Learning Analysis of the FHS, the ARIC Study, and the CHS.",
  "publication/authors": "Stevens LM, Linstead E, Hall JL, Kao DP",
  "publication/journal": "Circulation. Heart failure",
  "publication/year": "2021",
  "publication/pmid": "33557575",
  "publication/pmcid": "PMC8081115",
  "publication/doi": "10.1161/circheartfailure.119.006799",
  "publication/tags": "- Cardiovascular Disease\n- Risk Prediction\n- Machine Learning\n- Feature Selection\n- Electronic Health Records\n- Dietary Factors\n- Longitudinal Studies\n- Random Forest Analysis\n- Cox Proportional Hazards\n- Epidemiology",
  "dataset/provenance": "The dataset used in this study was obtained from the National Heart, Lung, and Blood Institutes Biologic Specimen and Data Repository Information Coordinating Center (BioLINCC). This dataset includes clinical data from three prospective longitudinal cohort studies: the Framingham Heart Study (FHS), the Cardiovascular Health Study (CHS), and the Atherosclerosis Risk in Communities (ARIC) Study. These studies were designed to investigate the incidence, survival rate, and determinants of cardiovascular disease (CVD).\n\nThe FHS included 5,209 participants aged 30 to 62, assessed every 2 years. For this analysis, 2,732 participants who attended FHS Exam 14 and had not yet had a CVD event were used. The CHS included 5,888 participants aged above 65, assessed annually for approximately 10 years. Of these, 3,704 participants without prior CVD and complete data for the dietary factors identified during feature selection were used in the analysis. The ARIC study enrolled 15,792 individuals aged 45\u201364 without prior CVD, with 4 exams conducted every 3 years and a 5th exam approximately 25 years after enrollment. A total of 14,925 participants from the ARIC study contained data for follow-up CVD events and were used for analysis.\n\nThese studies provide a robust foundation for exploring CVD risk factors due to their longitudinal nature, large participant numbers, and high event rates over decades of follow-up. The data from these studies have been used in various epidemiological and clinical research, contributing to the understanding of CVD risk factors and prevention strategies.",
  "dataset/splits": "The study utilized data from three prospective longitudinal cohort studies: the Framingham Heart Study (FHS), the Cardiovascular Health Study (CHS), and the Atherosclerosis Risk in Communities study (ARIC). These studies were designed to investigate the incidence, survival rate, and determinants of cardiovascular disease (CVD).\n\nFor the FHS, participants from Exam 14 who had not yet experienced a CVD event were included in the analysis, totaling 2,732 individuals. Exam 14, conducted between 1975 and 1978, served as the reference date for all time-to-event analyses.\n\nIn the CHS, 3,704 participants above the age of 65 without prior CVD and with complete data for the dietary factors identified during feature selection were used in the analysis.\n\nThe ARIC study enrolled 14,925 individuals aged 45\u201364 without prior CVD, who had data for follow-up CVD events. The study consisted of four exams conducted every three years and a fifth exam approximately 25 years after enrollment.\n\nThe baseline exams of CHS and ARIC were used for validation of the findings from FHS. Dietary factors significantly associated with outcomes of interest in FHS were harmonized with comparable variables in CHS and ARIC for validation purposes.",
  "dataset/redundancy": "The datasets used in this study were derived from three prospective longitudinal cohort studies: the Framingham Heart Study (FHS), the Cardiovascular Health Study (CHS), and the Atherosclerosis Risk in Communities (ARIC) study. These studies were designed to investigate the incidence, survival rate, and determinants of cardiovascular disease (CVD).\n\nFor the FHS, participants who attended Exam 14 and had not yet experienced a CVD event were included in the analysis. Exam 14 served as the reference date for all time-to-event analyses. The CHS included participants above the age of 65 who were assessed annually for approximately 10 years. For the ARIC study, individuals aged 45\u201364 without prior CVD were enrolled and followed through multiple exams conducted over several years.\n\nThe training and test sets were independent. The FHS dataset was used for initial feature selection and model development. Specifically, participants from FHS Exam 14 who had complete data for the identified dietary factors were included in the analysis. The CHS and ARIC datasets were used for validation purposes. Dietary factors significantly associated with outcomes of interest in the FHS were harmonized with comparable variables in CHS and ARIC to validate the findings.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the context of CVD research. The studies enrolled thousands of patients with relatively high event rates over decades of follow-up, providing a robust foundation for exploring CVD risk factors. The use of longitudinal data ensures that the temporal dynamics of CVD development are captured, which is crucial for accurate risk prediction. The independence of the training and test sets was enforced by using different cohorts for model development and validation, ensuring that the results are generalizable and not overfitted to a single dataset.",
  "dataset/availability": "The data collected for this study is not publicly available. However, requests to access the dataset from qualified researchers trained in human subject confidentiality protocols can be made. These protocols are completed through BioLINCC. This ensures that only researchers who have demonstrated the necessary training and commitment to confidentiality can access the data, thereby protecting the privacy and security of the participants.",
  "optimization/algorithm": "The optimization algorithm employed in this study falls under the class of ensemble learning methods, specifically utilizing random forests. This approach is well-established in the field of machine learning and is not a new algorithm. Random forests are a type of supervised learning algorithm used for both classification and regression tasks. They operate by constructing multiple decision trees during training and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n\nThe choice of random forests is advantageous due to their ability to handle large datasets with numerous variables, which is particularly relevant given the extensive patient characteristics collected in epidemiological studies like the Framingham Heart Study, the Atherosclerosis Risk in Communities Study, and the Cardiovascular Health Study. Random forests also provide a robust mechanism for feature selection, which is crucial for identifying potential risk factors for cardiovascular diseases.\n\nThe decision to use random forests in this context is driven by their effectiveness in managing high-dimensional data and their capacity to reduce overfitting, which is essential for reliable risk prediction models. The algorithm's stability and ability to control for potential confounding and collinearity make it a suitable choice for the hypothesis-free, data-driven approach taken in this analysis.\n\nThe algorithm was not published in a machine-learning journal because the focus of this study is on applying established machine learning techniques to identify novel risk factors for cardiovascular diseases. The primary contribution of this work lies in the application of these methods to a specific medical domain rather than the development of new machine-learning algorithms. The emphasis is on leveraging existing tools to advance understanding and prediction in cardiovascular health.",
  "optimization/meta": "The model employed in this study does not utilize data from other machine-learning algorithms as input. Instead, it relies on supervised machine learning techniques, specifically random forest analysis, to identify potential variables important for assessing the risk of incident coronary heart disease (CHD), stroke, and heart failure (HF).\n\nThe random forest method was chosen for its ability to handle large datasets and identify important features in a relatively unbiased manner. This approach allows for the assessment of a wide range of patient characteristics, reducing the likelihood of missing important predictors that might be overlooked using traditional hypothesis-driven methods.\n\nThe random forest analysis was conducted using 10-fold cross-validation with 5 repeats to ensure optimal feature selection. This process involved evaluating the importance metrics of various candidate variables across all outcomes. The top 20% of predictors based on these importance metrics were selected for further analysis.\n\nThe significance, magnitude, and direction of association between the candidate dietary factors and the outcomes of interest were assessed using multivariable Cox proportional hazards analysis. This step involved controlling for potential confounding and collinearity, ensuring that the identified variables were robust and meaningful.\n\nIn summary, the model does not function as a meta-predictor but rather as a standalone machine learning approach designed to identify novel risk factors for CHD, HF, and stroke. The training data used in the random forest analysis was independent, ensuring the reliability and validity of the findings.",
  "optimization/encoding": "For the machine-learning algorithm, the data underwent several preprocessing steps to ensure optimal performance. Initially, variables with more than 15% missing values were excluded to maintain data integrity. This filtering process resulted in a dataset comprising 204 variables, which included 16 dietary factors and 13 non-dietary lifestyle behaviors. Participants were included in the analysis only if they had complete data for all these variables, ensuring a robust dataset for feature selection.\n\nThe remaining variables were then subjected to random forest analysis to identify patient characteristics potentially important for predicting incident coronary heart disease (CHD), heart failure (HF), and stroke. To enhance the reliability of feature selection, 10-fold cross-validation with 5 repeats was employed. This method helped in identifying the top 20% of predictors based on importance metrics across all outcomes in the random forest model.\n\nThe significance, magnitude, and direction of association between candidate dietary factors and the outcomes of interest were assessed using multivariable Cox proportional hazards analysis. This approach controlled for potential confounding and collinearity, ensuring that the selected variables were robust predictors. Variables with importance scores in the top 20% were further evaluated for collinearity and compared with known risk factors to determine their inclusion as covariates in the Cox proportional hazards analysis.\n\nGiven the collinearity of many non-dietary risk factors with known risk factors, a risk score was used to account for the combined impact of known risk factors. This approach provided a comprehensive assessment of the probabilities associated with known risk factors while also considering those with weaker associations. The models for individual CHD, HF, and stroke outcomes were calibrated using the Framingham Heart Study (FHS) CVD risk score, ensuring consistency and reliability in the predictions.",
  "optimization/parameters": "In our study, we initially considered 204 variables for feature selection. Variables with more than 15% missing values were excluded, and only participants with complete data for the remaining variables were included in the analysis. This process resulted in 204 variables being used in the model.\n\nTo select the most important variables, we employed a random forest analysis with 10-fold cross-validation and 5 repeats. This method helped us identify the top 20% of predictors based on importance metrics across all outcomes. These top predictors were then considered for further analysis.\n\nThe decision to use the top 20% of variables was driven by the need to balance between capturing important predictors and avoiding overfitting. This approach allowed us to focus on the most relevant features while ensuring that our model remained robust and generalizable. The selected variables included a mix of known risk factors and potentially novel dietary and lifestyle factors, which were further evaluated for their significance and association with the outcomes of interest.",
  "optimization/features": "In the optimization process, we began with a comprehensive set of 204 potential features. These features were derived from various patient characteristics, including dietary factors and non-dietary lifestyle behaviors. To ensure robustness, variables with more than 15% missing values were excluded, and only participants with complete data for the remaining variables were included in the feature selection process.\n\nFeature selection was performed using a random forest analysis, which is a machine learning technique known for its ability to handle large datasets and identify important variables. The random forest model was trained using 10-fold cross-validation with 5 repeats to ensure the stability and generalizability of the selected features. This method allowed us to identify the top 20% of predictors based on importance metrics across all outcomes, which included coronary heart disease (CHD), heart failure (HF), and stroke.\n\nThe feature selection process was conducted using the training set only, ensuring that the validation set remained independent and unbiased. This approach helped us to control for potential confounding and collinearity, providing a more reliable set of features for further analysis. The selected features were then evaluated for their significance and association with the outcomes of interest using multivariable Cox proportional hazards analysis. This rigorous process ensured that the final set of features used in our models was both relevant and statistically significant.",
  "optimization/fitting": "The number of parameters in our analysis was indeed much larger than the number of training points. To address potential overfitting, we employed several strategies. Firstly, we used a random forest analysis for feature selection, which inherently reduces overfitting due to its ensemble nature and ability to handle high-dimensional data. We also utilized 10-fold cross-validation with 5 repeats to ensure the robustness of our feature selection process. This technique helps in assessing the model's performance on different subsets of the data, thereby reducing the risk of overfitting.\n\nTo further mitigate overfitting, we selected only the top 20% of predictors based on importance metrics across all outcomes in the random forest model. This step ensured that we focused on the most relevant features, reducing the complexity of the model. Additionally, we assessed the collinearity of the selected variables and compared them with known risk factors, ensuring that the model did not rely on redundant or highly correlated features.\n\nRegarding underfitting, we ensured that our model was complex enough to capture the underlying patterns in the data. The use of random forest analysis, which can model complex interactions between variables, helped in this regard. Moreover, the validation of our findings in independent datasets from the Atherosclerosis Risk in Communities (ARIC) Study and the Cardiovascular Health Study (CHS) provided further confidence that our model was not underfitting the data. The consistent associations between dietary factors and clinical outcomes across different studies suggested that our model was appropriately capturing the relevant risk factors.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the primary methods used was 10-fold cross-validation with 5 repeats. This technique helps to assess the model's performance and generalization ability by dividing the data into training and validation sets multiple times, thereby reducing the risk of overfitting to a specific subset of the data.\n\nAdditionally, we utilized random forest analysis for feature selection, which inherently provides a form of regularization by averaging the results of multiple decision trees. This method helps to reduce the variance and improve the stability of the model, making it less likely to overfit the training data.\n\nFor the evaluation of feature significance, we employed multivariable Cox proportional hazards analysis. This statistical method adjusts for potential confounding and collinearity, ensuring that the identified features are genuinely associated with the outcomes of interest rather than being artifacts of the data.\n\nFurthermore, we validated our findings using external datasets from the Cardiovascular Health Study (CHS) and the Atherosclerosis Risk in Communities (ARIC) Study. This validation step is crucial for confirming that the identified risk factors are generalizable to different populations and not merely specific to the Framingham Heart Study (FHS) cohort.\n\nIn summary, our approach included cross-validation, random forest analysis, multivariable Cox proportional hazards analysis, and external validation to prevent overfitting and ensure the reliability of our results.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the text. Specifically, we employed 10-fold cross-validation with 5 repeats for optimal feature selection. This approach is well-documented in the literature and is a standard method for ensuring robust model performance.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly provided in the publication. However, the methods and techniques used, such as random forest analysis and Cox proportional hazards analysis, are thoroughly described. The specific parameters and configurations can be inferred from the detailed descriptions of the analytical processes.\n\nFor those interested in replicating or building upon our work, the methods section provides comprehensive details on the analytical techniques and statistical approaches used. This includes the criteria for variable selection, the handling of missing data, and the statistical models employed. While the exact model files are not available, the transparency in our methodology allows for reproducibility.\n\nThe publication adheres to standard academic practices, ensuring that the methods and results are reproducible. The use of established statistical techniques and the detailed description of the analytical process support the integrity and reproducibility of our findings. For further details, readers are encouraged to refer to the supplementary materials and the cited literature.",
  "model/interpretability": "The model employed in this study is not a blackbox. It utilizes random forest decision trees, which are inherently interpretable. Random forests provide a way to assess the importance of each variable in predicting the outcomes. This is achieved by evaluating how much each variable contributes to the accuracy of the model. Variables with higher importance scores are considered more influential in the prediction process.\n\nFor instance, variables such as blood pressure measurements, cholesterol levels, and age were found to be among the top predictors. These variables are well-known risk factors for cardiovascular diseases, which aligns with established medical knowledge. This transparency allows for a clear understanding of which factors are driving the predictions, making the model's decisions interpretable and trustworthy.\n\nAdditionally, the use of multivariable Cox proportional hazards analysis further enhances the interpretability. This statistical method assesses the significance, magnitude, and direction of association between candidate dietary factors and the outcomes of interest. By examining the hazard ratios, one can understand the relative risk associated with each variable, providing a clear picture of how different factors influence the likelihood of developing cardiovascular diseases.\n\nThe model's transparency is also supported by the validation process, which involved harmonizing dietary factors and traditional risk factors across different studies. This ensures that the findings are consistent and reliable, further reinforcing the interpretability of the model.",
  "model/output": "The model employed in our study is primarily a regression model, specifically designed for time-to-event analysis. We utilized multivariable Cox proportional hazards analysis to assess the significance, magnitude, and direction of association between candidate dietary factors and outcomes of interest, such as incident coronary heart disease (CHD), heart failure (HF), and stroke. The Cox model is well-suited for survival analysis, which is crucial for understanding the time until an event occurs.\n\nTo identify important variables, we initially used random forest analysis, a classification technique, to rank variables based on their importance metrics across all outcomes. The top 20% of predictors from this analysis were then considered for further evaluation. This approach allowed us to handle a large number of variables and identify those most relevant to the outcomes of interest.\n\nThe final models for individual CHD, HF, and stroke outcomes were calibrated using the Framingham Heart Study (FHS) cardiovascular disease (CVD) risk score. This risk score provides a comprehensive assessment of known risk factors, accounting for both strong and weaker associations with the outcomes. By incorporating this risk score, we aimed to capture the combined impact of multiple risk factors rather than focusing on individual factors alone.\n\nIn summary, while the initial feature selection involved classification techniques, the primary analytical approach was regression-based, using Cox proportional hazards models to evaluate the associations between dietary factors and clinical outcomes.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not applicable.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to assess the significance, magnitude, and direction of association between candidate dietary factors and outcomes of interest. This was achieved through multivariable Cox proportional hazards analysis. The random forest methods used, along with the importance metrics calculated, inherently control for potential confounding and collinearity in machine learning experiments.\n\nTo ensure optimal feature selection, 10-fold cross-validation with 5 repeats was utilized. This process helped in identifying the top 20% predictors based on importance metrics across all outcomes in the random forest model. These top predictors were then evaluated for collinearity and compared with known risk factors to determine which variables to include as covariates in the Cox proportional hazards analysis.\n\nGiven that many non-dietary risk factors with high importance scores were collinearly related to known risk factors, and some known risk factors showed only modest associations with the outcomes, the Framingham Heart Study (FHS) CVD risk score was employed. This score provided good coverage of the probabilities for known risk factors while also accounting for known risks with weaker associations. Models for individual outcomes such as coronary heart disease (CHD), heart failure (HF), and stroke were calibrated according to the original FHS CVD risk score publication by D\u2019Agostino et al.\n\nThe use of a risk score in the multivariable analysis was crucial for accounting for collinearity and the combined impact of known risk factors, rather than considering each factor individually. A p-value of less than 0.05 was considered significant throughout the analysis.\n\nFor validation, baseline exams from the Cardiovascular Health Study (CHS) and the Atherosclerosis Risk in Communities (ARIC) study were used. Dietary factors significantly associated with outcomes of interest in the FHS were harmonized with comparable variables in CHS and ARIC. The associations between these dietary factors and clinical outcomes were then validated in these independent datasets. This multi-study validation approach ensured the robustness and generalizability of the findings.",
  "evaluation/measure": "In our evaluation, we employed several performance metrics to assess the effectiveness of our models. We primarily used the importance metrics derived from random forest analysis to identify the top predictors for incident coronary heart disease (CHD), heart failure (HF), and stroke. These importance metrics helped us select the top 20% of variables for further analysis.\n\nFor the multivariable Cox proportional hazards analysis, we assessed the significance, magnitude, and direction of association between candidate dietary factors and the outcomes of interest. We considered a p-value of less than 0.05 as statistically significant throughout our analysis.\n\nTo validate our findings, we used baseline exams from the Cardiovascular Health Study (CHS) and the Atherosclerosis Risk in Communities Study (ARIC). We harmonized dietary factors significantly associated with outcomes in the Framingham Heart Study (FHS) with comparable variables in CHS and ARIC. The associations between these dietary factors and clinical outcomes were then validated in these external cohorts.\n\nThe use of these metrics is consistent with established practices in the literature. The importance metrics from random forest analysis are widely used for feature selection in machine learning and bioinformatics. The Cox proportional hazards model is a standard approach for time-to-event analysis in medical research. Additionally, validating findings in external cohorts like CHS and ARIC is a robust method to ensure the generalizability of our results.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our approach focused on leveraging established statistical techniques and machine learning methods tailored to our specific research questions.\n\nWe utilized random forest analysis for feature selection, which is a robust method for identifying important predictors in complex datasets. This technique was chosen for its ability to handle high-dimensional data and its effectiveness in capturing non-linear relationships. The random forest method was implemented using 10-fold cross-validation with 5 repeats to ensure the stability and generalizability of our feature selection process.\n\nFor evaluating the significance and magnitude of associations between dietary factors and clinical outcomes, we employed multivariable Cox proportional hazards analysis. This method is well-suited for time-to-event data and allows for the adjustment of multiple covariates, providing a comprehensive assessment of risk factors.\n\nWhile we did not compare our methods to simpler baselines explicitly, the use of established statistical techniques like random forest and Cox proportional hazards analysis serves as a form of baseline comparison. These methods are widely accepted in the field of epidemiological research and provide a solid foundation for our analyses.\n\nIn summary, our study relied on well-established statistical and machine learning techniques to identify and evaluate important predictors of cardiovascular disease outcomes. Although we did not perform a direct comparison to publicly available methods or simpler baselines on benchmark datasets, our approach was grounded in robust methodologies that are commonly used in similar research contexts.",
  "evaluation/confidence": "In our evaluation, we employed several statistical methods to ensure the robustness and significance of our findings. We used multivariable Cox proportional hazards analysis to assess the significance, magnitude, and direction of association between candidate dietary factors and outcomes of interest. This approach allowed us to control for potential confounding and collinearity in our machine learning experiments.\n\nTo determine the importance of variables, we utilized random forest analysis, which inherently provides importance metrics for each variable. These metrics were calculated across all outcomes, and variables in the top 20% were considered for further analysis. The randomized nature of the random forest methods helps in controlling for potential confounding and collinearity.\n\nWe also performed 10-fold cross-validation with 5 repeats to ensure the stability and generalizability of our feature selection process. This rigorous validation technique helps in assessing the performance metrics more reliably.\n\nFor statistical significance, we considered a p-value of less than 0.05 throughout our analysis. This threshold ensures that our results are statistically significant, allowing us to claim that the identified variables and models are robust and superior to baselines.\n\nAdditionally, we validated our findings using baseline exams from the Cardiovascular Health Study (CHS) and the Atherosclerosis Risk in Communities Study (ARIC). This external validation step further strengthens the confidence in our results, as it demonstrates that the associations between dietary factors and clinical outcomes are consistent across different populations.\n\nIn summary, our evaluation process included multiple layers of statistical validation, ensuring that our performance metrics are reliable and that our results are statistically significant. This comprehensive approach allows us to confidently claim the superiority of our method over others and baselines.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, requests to access the dataset can be made by qualified researchers who have completed human subject confidentiality protocols through BioLINCC. This ensures that the data is used responsibly and ethically. The dataset includes detailed information from three prospective longitudinal cohort studies: the Framingham Heart Study (FHS), the Cardiovascular Health Study (CHS), and the Atherosclerosis Risk in Communities Study (ARIC). These studies were designed to investigate the incidence, survival rate, and determinants of cardiovascular disease (CVD). The studies are community-based and include multiple incident CVD endpoints with at least 10 years of follow-up from the baseline exam. The specific methodologies and response rates for each study are documented elsewhere. For further details or to request access to the dataset, researchers should follow the protocols established by BioLINCC."
}