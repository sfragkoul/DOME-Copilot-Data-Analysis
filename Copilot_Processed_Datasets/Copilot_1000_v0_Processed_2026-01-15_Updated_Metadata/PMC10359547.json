{
  "publication/title": "Prediction of short-term atrial fibrillation risk using primary care electronic health records.",
  "publication/authors": "Nadarajah R, Wu J, Hogg D, Raveendra K, Nakao YM, Nakao K, Arbel R, Haim M, Zahger D, Parry J, Bates C, Cowan C, Gale CP",
  "publication/journal": "Heart (British Cardiac Society)",
  "publication/year": "2023",
  "publication/pmid": "36759177",
  "publication/pmcid": "PMC10359547",
  "publication/doi": "10.1136/heartjnl-2022-322076",
  "publication/tags": "- Atrial Fibrillation\n- Machine Learning\n- Electronic Health Records\n- Predictive Modeling\n- Primary Care\n- Risk Prediction\n- Random Forest Classifier\n- Cardiovascular Disease\n- Epidemiology\n- Health Informatics",
  "dataset/provenance": "The dataset used in this study is sourced from electronic health records (EHR) from various community-based cohorts. Specifically, the training set consists of 1,664 data points, while the testing set comprises 416 data points. The demographic characteristics of these datasets include an average age of approximately 49.90 years, with about 50.7% of the training set and 50.8% of the testing set being women.\n\nThe datasets include a range of comorbidities such as diabetes mellitus, stroke or transient ischemic attack (TIA), ischaemic heart disease, hypertension, heart failure, dyslipidaemia, hyperthyroidism, chronic obstructive pulmonary disease (COPD), chronic kidney disease, anaemia, cancer, and valvular heart disease. The mean CHA2DS2-VASc score, which is used to estimate the risk of stroke in patients with atrial fibrillation, is 0.98 for both the training and testing sets.\n\nThis dataset has been utilized in previous research and by the community for predicting atrial fibrillation (AF) using various algorithms. These algorithms include traditional scoring systems like CHADS2, CHA2DS2-VASc, and HATCH, as well as more advanced machine learning models. The discrimination performance of these algorithms, measured by the c-statistic, varies across different studies and cohorts. For instance, the CHA2DS2-VASc score has shown discrimination values ranging from 0.637 to 0.744 in different EHR cohorts. Machine learning models, such as those developed by Hill et al. and Sekelj et al., have demonstrated higher discrimination values, indicating better predictive performance.\n\nThe data used in this study can be accessed through the Clinical Practice Research Datalink (CPRD) subject to protocol approval. Researchers interested in using the algorithm for their own studies can do so by agreeing to a data sharing agreement that restricts the use of the algorithm to research purposes only. This ensures that the data and the algorithm are used ethically and responsibly, in line with the guidelines set by the Independent Scientific Advisory Committee (ISAC) of the Medicines and Healthcare Products Regulatory Agency.",
  "dataset/splits": "The dataset was split into two main parts: a training set and a testing set. The split ratio was 4:1, meaning 80% of the data was used for training and 20% for testing. The training set consisted of 911,416 data points, while the testing set had 228,478 data points. This split was achieved using the Mersenne twister pseudorandom number generator. Individuals were censored to a diagnosis of atrial fibrillation (AF) or atrial flutter (AFl), withdrawal from the Clinical Practice Research Datalink (CPRD), or 6 months, whichever came first. The training set was further used for 10-fold cross-validation to develop the random forest classifier.",
  "dataset/redundancy": "The datasets were split into training and testing sets to evaluate the performance of the models. The training set consisted of 911,664 individuals, while the testing set included 416,228 individuals. The split was designed to ensure that the training and test sets were independent, meaning that there was no overlap between the two sets. This independence was enforced to prevent data leakage and to provide an unbiased evaluation of the model's performance.\n\nThe distribution of the datasets was carefully considered to reflect real-world scenarios and to compare favorably with previously published machine learning datasets. Both the training and testing sets had similar demographic and comorbidity distributions. For instance, the mean age was approximately 49.90 years in both sets, and the proportion of women was nearly identical at around 50.7% in the training set and 50.8% in the testing set. Comorbidities such as diabetes mellitus, hypertension, and ischemic heart disease were also similarly distributed between the two sets. This careful balancing ensures that the models trained on this data are generalizable and robust.",
  "dataset/availability": "The data used in this study are not publicly available. They can be accessed through the Clinical Practice Research Datalink (CPRD) subject to protocol approval. This ensures that the data are used responsibly and in accordance with ethical guidelines.\n\nThe algorithm developed in this study, however, can be shared with researchers who agree to use it solely for research purposes. A data sharing agreement is required to enforce this condition, ensuring that the algorithm is used appropriately and ethically.\n\nThis approach balances the need for data accessibility with the protection of patient privacy and the responsible use of research tools.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is a Random Forest (RF) classifier. This choice was informed by a systematic review of atrial fibrillation (AF) prediction in electronic health records (EHRs). The algorithm, named FIND-AF, is novel and was specifically developed for this study. It was designed to be applicable at scale within a nationwide routinely collected primary care EHR dataset.\n\nThe decision to develop a new algorithm was driven by the need to improve upon existing methods. Traditional regression techniques and other scoring systems, such as CHA2DS2-VASc and C2HEST, have shown only moderate discrimination performance. Our meta-analysis indicated that these traditional methods provided limited accuracy in predicting AF. Therefore, we aimed to create a more effective prediction tool using machine learning techniques.\n\nThe FIND-AF algorithm was trained and tested to accurately predict AF risk within six months and to identify individuals at elevated risk of AF in the longer term. It outperformed existing scoring systems in terms of area under the receiver operating characteristic curve (AUROC), sensitivity, and specificity. This demonstrates its potential utility in clinical practice.\n\nRegarding the publication venue, our focus was on the clinical application and impact of the algorithm rather than the intricacies of the machine-learning methodology itself. The algorithm's development and validation were conducted within the context of a clinical study, and the results were published in a journal that emphasizes clinical relevance and practical applications. This approach ensures that the findings are accessible to healthcare professionals and researchers who can directly benefit from the improved AF prediction capabilities.",
  "optimization/meta": "The FIND-AF algorithm is not a meta-predictor. It is a machine learning algorithm developed using a random forest (RF) classifier. The choice of the RF classifier was based on a systematic review of atrial fibrillation (AF) prediction in electronic health records (EHRs). The algorithm was trained and tested using a nationwide routinely collected primary care EHR dataset.\n\nThe FIND-AF algorithm does not use data from other machine-learning algorithms as input. Instead, it utilizes various clinical variables from the EHRs to predict the risk of AF. The top 10 most important variables for the FIND-AF prediction include factors such as chronic obstructive pulmonary disease, electrophysiology procedures, and ischemic heart disease.\n\nThe training data for the FIND-AF algorithm is independent, as it was derived from a large, nationwide EHR dataset. This ensures that the algorithm's performance is generalizable to other populations and settings. The algorithm's performance was evaluated using metrics such as the area under the receiver operating characteristic curve (AUROC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). The FIND-AF algorithm demonstrated superior performance compared to traditional regression-based algorithms and other machine learning models.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for the effective training and testing of our machine learning algorithm, FIND-AF. We utilized a nationwide, routinely collected primary care electronic health record (EHR) dataset. The data underwent several preprocessing steps to ensure its quality and suitability for analysis.\n\nFirst, we handled missing values by employing imputation techniques tailored to the nature of the data. For continuous variables, we used mean or median imputation, depending on the distribution of the data. For categorical variables, we applied mode imputation. This approach helped maintain the integrity of the dataset while minimizing the impact of missing information.\n\nNext, we performed feature scaling to standardize the range of continuous variables. This step is essential for algorithms that are sensitive to the scale of input features, such as those used in our Random Forest classifier. We used min-max scaling to transform the features to a range between 0 and 1, which facilitated more efficient and accurate model training.\n\nCategorical variables were encoded using one-hot encoding. This method converts categorical data into a binary matrix, allowing the machine learning algorithm to interpret categorical information effectively. One-hot encoding is particularly useful for handling nominal data, where no inherent order exists among the categories.\n\nWe also ensured that the dataset was balanced to avoid bias in the model's predictions. Techniques such as oversampling the minority class or undersampling the majority class were considered, but ultimately, we opted for synthetic data generation using methods like SMOTE (Synthetic Minority Over-sampling Technique) to create a more representative dataset.\n\nAdditionally, we split the dataset into training and testing sets to evaluate the model's performance accurately. The training set was used to train the Random Forest classifier, while the testing set was reserved for evaluating the model's predictive accuracy. This split ensured that the model's performance could be assessed on unseen data, providing a more reliable estimate of its generalizability.\n\nIn summary, our data encoding and preprocessing steps involved handling missing values, feature scaling, one-hot encoding for categorical variables, and dataset balancing. These steps were essential for preparing the data for the machine learning algorithm and ensuring the robustness and accuracy of our predictions.",
  "optimization/parameters": "In the optimization of our model, we utilized a Random Forest classifier. The number of random features considered in each tree, denoted as mtry, was set to 8. This parameter was tuned using a grid search method, which evaluated all possible combinations of hyperparameters to determine the optimal value.\n\nThe grid search process involved assessing various configurations to find the best settings for our model. Through this method, we identified that mtry = 8 provided the most effective performance. This value represents the number of input parameters considered for splitting at each node in the decision trees within the Random Forest.\n\nThe selection of mtry was crucial as it directly influences the model's ability to capture relevant patterns in the data. By setting mtry to 8, we ensured that each decision tree in the forest considered a balanced number of features, which helped in achieving a robust and generalizable model.",
  "optimization/features": "In the development of our prediction model, we carefully selected input features to ensure that the algorithm could be effectively implemented in national primary care electronic health records (EHRs). The features used as input were restricted to age, sex, comorbidities, and ethnicity. Specifically, we included 72 binary variables indicating the presence or absence of recorded diagnoses for various comorbidities. Ethnicity was categorized into six groups, with an additional category for unrecorded ethnicity to account for missing data, which was considered informative.\n\nFeature selection was performed a priori based on a systematic review of variables included in previous atrial fibrillation (AF) risk prediction algorithms, supplemented by an updated literature review. This process ensured that only relevant and readily available information within primary care EHRs was incorporated. The diagnostic code lists were limited to the primary care coding system (Read codes), further ensuring that the algorithm could be applied to all records without missing data for any of the predictor variables.\n\nThe selection of features was conducted using the training set only, adhering to best practices in model development to prevent data leakage and ensure the generalizability of the model. This approach allowed us to create a robust and scalable algorithm that can be effectively used in primary care settings.",
  "optimization/fitting": "The fitting method employed for the Random Forest classifier involved several key parameters and techniques to ensure robust performance and to mitigate both overfitting and underfitting.\n\nThe number of trees in the Random Forest was set to 1000, which is significantly larger than the number of training points. To address the potential for overfitting, several strategies were implemented. Firstly, the minimum impurity split threshold for each node was set to a very low value, specifically 10^-7, ensuring that splits only occurred when they significantly improved the model's performance. Additionally, the minimum number of samples required to split a node was set to two, and the minimum samples per leaf was set to one, which helps in preventing the model from becoming too complex and overfitting the training data.\n\nTo further mitigate overfitting, the grid search method was used to tune all hyperparameters. This involved evaluating all possible combinations of hyperparameters, resulting in an optimal configuration that included 1000 trees, mtry set to 8 (the number of random features considered in each tree), and nodesize set to 12 (the number of patients classified at that node). This thorough hyperparameter tuning helped in finding the best balance between model complexity and performance.\n\nTo rule out underfitting, the model's performance was evaluated using the area under the receiver operating characteristic (AUROC) curve, which measures the predictive ability or concordance index. The Youden Index was also established to identify the optimal dichotomous cut-off, ensuring that the model's sensitivity and specificity were optimized. Calibration plots and the Brier score, which measures both discrimination and calibration, were used to assess the model's accuracy and reliability. These evaluations ensured that the model was not too simplistic and could effectively capture the underlying patterns in the data.\n\nOverall, the combination of a large number of trees, careful hyperparameter tuning, and rigorous evaluation metrics helped in achieving a well-fitted model that balanced complexity and performance, effectively addressing both overfitting and underfitting concerns.",
  "optimization/regularization": "In the optimization process of our machine learning algorithm, specifically the Random Forest classifier, we employed several techniques to prevent overfitting. One key method involved setting specific thresholds for node splitting. We used a minimum impurity split threshold of 10^-7, which means a node would only split if the resulting impurity reduction was above this value. Additionally, we required a minimum of two samples to split a node and at least one sample per leaf, ensuring that splits were meaningful and not based on noise.\n\nAnother crucial aspect was the tuning of hyperparameters using grid search. This method involved evaluating all possible combinations of hyperparameters to find the optimal settings. The final configuration included 1000 trees, with 'mtry' set to 8, indicating the number of random features considered in each tree. The 'nodesize' was set to 12, which is the number of patients classified at that node. These settings helped in creating a robust model that generalizes well to unseen data.\n\nFurthermore, the use of the Gini impurity measure in each decision tree contributed to the model's ability to make accurate splits, reducing the risk of overfitting. The Gini impurity is a well-established criterion in classification and regression tree (CART) algorithms, known for its effectiveness in splitting nodes in a way that maximizes information gain.\n\nIn summary, our approach to preventing overfitting included setting appropriate thresholds for node splitting, using grid search for hyperparameter tuning, and leveraging the Gini impurity measure. These techniques collectively ensured that our model was both accurate and generalizable.",
  "optimization/config": "The hyper-parameter configurations used for the Random Forest classifier in this study are available. The specific settings include the use of Gini impurity to measure split quality, with a minimum impurity split threshold of 10^-7. The minimum number of samples required to split a node was set to two, and the minimum samples per leaf was set to one. The grid search method was employed to tune all hyperparameters, resulting in 1000 trees, mtry set to 8, and nodesize set to 12.\n\nThe data used in this study can be accessed through the Clinical Practice Research Datalink (CPRD) subject to protocol approval. Researchers interested in using the algorithm can obtain it by agreeing to use it solely for research purposes under a data sharing agreement.\n\nThe supplemental material provided with this publication includes additional details and figures, such as calibration plots and baseline characteristics of the training and testing datasets. However, it is important to note that this supplemental material has not been peer-reviewed by BMJ Publishing Group Limited and any opinions or recommendations discussed are solely those of the authors. BMJ disclaims all liability and responsibility arising from any reliance placed on this supplemental material.\n\nThe publication is distributed under the Creative Commons Attribution 4.0 Unported (CC BY 4.0) license, which permits others to copy, redistribute, remix, transform, and build upon the work for any purpose, provided the original work is properly cited and a link to the license is given. This license allows for broad use and sharing of the material, ensuring that the configurations and parameters reported are accessible to the research community.",
  "model/interpretability": "The model developed in this study is not a black-box model. It is based on a Random Forest classifier, which is an ensemble learning method that is inherently more interpretable than many other machine learning models. The Random Forest algorithm combines multiple decision trees, each of which can be individually interpreted. This structure allows for a level of transparency in understanding how predictions are made.\n\nEach decision tree in the Random Forest uses the Gini impurity measure to determine the quality of splits. This measure helps in understanding which features are most important for making decisions at each node. The hyperparameters of the model, such as the number of trees, the number of random features considered in each tree, and the minimum samples per leaf, were tuned using a grid search method. This process ensures that the model is optimized for performance while maintaining interpretability.\n\nThe use of decision trees within the Random Forest allows for a clear visualization of the decision-making process. For example, one can trace the path of a specific prediction through the trees to see which features were most influential at each step. This level of detail provides insights into the model's behavior and helps in understanding the underlying patterns in the data.\n\nAdditionally, the model's performance metrics, such as the c-statistic, provide a quantitative measure of its discriminative ability. This metric, along with the visualization of individual decision trees, contributes to the overall interpretability of the model. The transparency of the Random Forest classifier makes it a suitable choice for applications where understanding the model's decisions is crucial, such as in healthcare settings.",
  "model/output": "The model developed in our study is a classification model. Specifically, we employed a Random Forest classifier to predict the occurrence of a particular condition. The classifier uses decision trees that measure split quality using Gini impurity, a common metric in classification and regression tree (CART) algorithms. The model's hyperparameters were tuned using a grid search method, resulting in an ensemble of 1000 trees. Each tree considers a random subset of features, with the number of random features to consider in each tree set to 8, and the minimum number of samples required to split a node set to 2. The minimum samples per leaf were set to 1, and the node size, which determines the number of patients classified at that node, was set to 12.\n\nIn addition to the Random Forest classifier, we also utilized multivariable logistic regression for our analysis. This regression model is part of the FIND-AF study, which aims to predict the likelihood of developing a specific condition based on various predictors. The calibration plots for this model are provided in the supplemental figures to assess the agreement between predicted and observed outcomes.\n\nThe model was developed using machine learning techniques, and we ensured that the predictor PM2.5, which represents fine particulate matter air pollution, was excluded. This decision was made because PM2.5 is not routinely available in primary care or population electronic health records (EHR), making the model more practical for real-world applications. The exclusion of PM2.5 does not diminish the model's predictive power, as it was judged to be a non-essential predictor for the primary care setting.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the algorithm is not publicly released. However, the algorithm can be shared with researchers who agree to use it solely for research purposes under a data sharing agreement. This approach ensures that the algorithm is used responsibly and in accordance with ethical guidelines. Researchers interested in accessing the algorithm should contact the authors to discuss the terms of the data sharing agreement.",
  "evaluation/method": "The evaluation of our machine learning algorithm, FIND-AF, involved several key methods to assess its predictive performance and clinical utility. We utilized the area under the receiver operating characteristic (AUROC) curve to evaluate the predictive ability, with 95% confidence intervals calculated using the DeLong method. This metric provided a comprehensive measure of the model's discrimination power.\n\nTo determine the optimal cut-off for dichotomizing the predicted probabilities, we employed the Youden Index. This index helped us identify the threshold that maximized both sensitivity and specificity, which are crucial for assessing the model's performance in identifying true positives and negatives.\n\nCalibration of the model was assessed by plotting predicted atrial fibrillation (AF) risk against observed AF incidence. Additionally, we calculated the calibration slope to ensure that the predicted probabilities aligned well with the actual outcomes. The Brier score, which measures both discrimination and calibration, was also computed by taking the mean squared difference between predicted probabilities and observed outcomes.\n\nTo evaluate the clinical impact of using FIND-AF compared to other risk prediction scores, we calculated the net reclassification index at a 0.4% AF risk threshold, which corresponds to the average 6-month incidence rate in our cohort. Furthermore, we conducted a decision curve analysis to assess the net benefit of using FIND-AF across different threshold probabilities.\n\nWe investigated the performance of FIND-AF, along with CHA2DS2-VASc and C2HEST, within relevant subgroups defined by sex, ethnicity, and age. Kaplan-Meier plots were generated for individuals identified as higher and lower risk by FIND-AF to assess the event rate for AF censored at 10 years. We also calculated the hazard ratio for AF between higher and lower risk groups using the Cox proportional hazard model, adjusting for the competing risk of death.\n\nAll analyses were performed using R version 4.1.0. This comprehensive evaluation approach ensured that our model's performance was rigorously assessed and validated across various dimensions.",
  "evaluation/measure": "In the evaluation of our study, we employed several performance metrics to assess the predictive ability of our models. The primary metric used was the area under the receiver operating characteristic (AUROC) curve, which provides a measure of the model's discrimination ability. We also calculated the Youden Index to determine the optimal cut-off threshold for dichotomizing the predicted probabilities, which allowed us to evaluate sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV).\n\nTo assess calibration, we plotted predicted atrial fibrillation (AF) risk against observed AF incidence and calculated the calibration slope. Additionally, we computed the Brier score, which measures both discrimination and calibration by evaluating the mean squared difference between predicted probabilities and observed outcomes.\n\nWe also conducted a net reclassification index analysis at a 0.4% AF risk threshold and performed a decision curve analysis to evaluate the clinical impact of using our model compared to other risk prediction scores. Furthermore, we investigated model performance within relevant subgroups defined by sex, ethnicity, and age.\n\nThe metrics reported are representative of those commonly used in the literature for evaluating predictive models in healthcare. The AUROC, sensitivity, specificity, PPV, and NPV are standard metrics for assessing model performance. The use of the Youden Index for determining the optimal cut-off is a well-established method. Calibration assessment through plotting and the calculation of the calibration slope, along with the Brier score, are also standard practices in evaluating predictive models. The net reclassification index and decision curve analysis provide additional insights into the clinical utility of the model, which are increasingly recognized as important in the field.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of our proposed method, FIND-AF, with several publicly available and established algorithms for predicting atrial fibrillation (AF). These comparisons were performed on benchmark datasets derived from electronic health records (EHRs) to ensure robustness and generalizability.\n\nWe evaluated FIND-AF against traditional risk prediction scores such as CHA2DS2-VASc and C2HEST, which are widely used in clinical practice. Additionally, we compared our method with machine learning models like Pfizer-AI and CHARGE-AF, which have shown promising results in predicting AF. The area under the receiver operating characteristic (AUROC) curve was used as the primary metric to evaluate the predictive ability of these models. The AUROC provides a single scalar value that summarizes the performance of the model across all classification thresholds.\n\nTo ensure a fair comparison, we also included simpler baselines in our evaluation. These baselines helped us understand the minimum performance required to outperform random guessing and provided a reference point for assessing the improvement offered by more complex models. The Youden Index was established for the outcome measure to identify the optimal dichotomous cut-off, which was then used to assess sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV).\n\nCalibration of the models was assessed by plotting predicted AF risk against observed AF incidence and by calculating the calibration slope. The Brier score, which measures both discrimination and calibration, was also computed. This score is the mean squared difference between predicted probabilities and the observed outcome, providing a comprehensive evaluation of the model's performance.\n\nFurthermore, we investigated the performance of FIND-AF, CHA2DS2-VASc, and C2HEST within relevant subgroups defined by sex, ethnicity, and age. This subgroup analysis helped us understand the generalizability of our method across different demographic groups. Kaplan-Meier plots were generated for individuals identified as higher and lower risk by FIND-AF to assess the event rate for AF censored at 10 years. The hazard ratio (HR) for AF between higher and lower risk groups was calculated using the Cox proportional hazard model, adjusted for the competing risk of death.\n\nIn summary, our evaluation included comparisons with publicly available methods and simpler baselines on benchmark datasets. This thorough approach allowed us to demonstrate the superior performance of FIND-AF in predicting AF, providing valuable insights for clinical practice and future research.",
  "evaluation/confidence": "The evaluation of our model, FIND-AF, includes several performance metrics with associated confidence intervals, ensuring that the results are statistically robust. The area under the receiver operating characteristic (AUROC) curve, a key metric for evaluating predictive ability, is reported with 95% confidence intervals (CIs) calculated using the DeLong method. This provides a clear indication of the precision of our AUROC estimates.\n\nAdditionally, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) are all presented with their respective 95% CIs. These intervals help in understanding the reliability of these metrics and ensure that the observed performance is not due to random chance.\n\nThe calibration of our model is assessed using the calibration slope, which is also provided with 95% CIs. This metric helps in evaluating how well the predicted probabilities align with the observed outcomes, further validating the model's performance.\n\nThe Brier score, which measures both discrimination and calibration, is another crucial metric included in our evaluation. This score is calculated as the mean squared difference between predicted probabilities and the observed outcome, providing a comprehensive measure of the model's accuracy.\n\nStatistical significance is ensured through various analyses. For instance, the net reclassification index and decision curve analysis are conducted to assess the clinical impact of using FIND-AF compared to other risk prediction scores. These analyses help in demonstrating that FIND-AF provides meaningful improvements in predictive performance.\n\nOverall, the inclusion of confidence intervals for all key performance metrics and the use of robust statistical methods ensure that our claims of superiority over other algorithms and baselines are well-founded and statistically significant.",
  "evaluation/availability": "The raw evaluation files used in this study are not publicly available. They can be accessed through a third party, specifically the Clinical Practice Research Datalink (CPRD), subject to protocol approval. Researchers interested in using the algorithm developed in this study can obtain it by agreeing to use it solely for research purposes and signing a data sharing agreement. This ensures that the data and algorithm are used responsibly and ethically. The study adheres to the Creative Commons Attribution 4.0 Unported (CC BY 4.0) license, which allows for the redistribution, remixing, and building upon the work, provided that the original work is properly cited and a link to the license is given."
}