{
  "publication/title": "Development and comparison of 1-year survival models in patients with primary bone sarcomas: External validation of a Bayesian belief network model and creation and external validation of a new gradient boosting machine model.",
  "publication/authors": "Holm CE, Grazal CF, Raedkjaer M, Baad-Hansen T, Nandra R, Grimer R, Forsberg JA, Petersen MM, Skovlund Soerensen M",
  "publication/journal": "SAGE open medicine",
  "publication/year": "2022",
  "publication/pmid": "35154743",
  "publication/pmcid": "PMC8832594",
  "publication/doi": "10.1177/20503121221076387",
  "publication/tags": "- Machine Learning\n- Bone Sarcoma\n- Survival Prediction\n- Gradient Boosting Machines\n- Bayesian Belief Networks\n- External Validation\n- Prognostic Models\n- Medical Decision-Making\n- Retrospective Study\n- Clinical Use of Algorithms",
  "dataset/provenance": "The dataset used in this study originates from two primary sources. The training cohort consists of 3493 patients with newly diagnosed bone sarcomas, treated between 1970 and 2012 at the Royal Orthopedics Hospital in Birmingham, UK. This cohort was initially described by Nandra et al. and is drawn from their institutional prospectively maintained database.\n\nThe validation cohort comprises 771 patients newly diagnosed with bone sarcomas during the period from 2000 to 2016. These patients were obtained from the Danish Sarcoma Registry, a national database that has been prospectively maintained since January 1, 2009. Patients from the years 2000 to 2008 were later included in the registry through validation processes involving the Danish Cancer Registry and the Danish National Pathology Registry.\n\nThe Danish Sarcoma Registry includes data from the two tertiary referral centers for orthopedic oncology in Denmark. All patients in this cohort have a minimum of 1-year follow-up, ensured by the Danish Civil Registration System, which records the exact date of death for all individuals.\n\nThe training cohort used in this study is the same as the one used by Nandra et al. in their previous work, ensuring consistency in the data used for model development and validation. The validation cohort, while smaller, provides a modern and well-documented set of data points for assessing the generalizability of the models.",
  "dataset/splits": "The dataset was split into 10 unique test and train sets using randomization. Each split was designed to have balanced events per variable. The training set comprised 80% of the data, while the test set comprised 20%. Specifically, a gradient boosting machine (GBM) model was trained on a training set consisting of 2794 data points and subsequently tested on a corresponding test set of 699 cases. This process was repeated for each of the 10 splits to ensure robust validation and to mitigate overfitting. The distribution of data points in each split was maintained at 80% for training and 20% for testing, ensuring a consistent and balanced approach across all splits.",
  "dataset/redundancy": "The datasets were split into 10 unique test and train sets, ensuring balanced events per variable. Each set comprised 20% test data and 80% training data. A gradient boosting machine (GBM) model was trained on a training set of 2794 cases and tested on a corresponding test set of 699 cases. To ensure independence between the training and test sets, randomization was used during the splitting process. This method helps to prevent data leakage and ensures that the model's performance is evaluated on unseen data.\n\nThe distribution of the datasets was designed to mirror previously published machine learning datasets, with a focus on maintaining balanced events per variable. This approach is crucial for creating robust and generalizable models. The training cohort spanned from 1970 to 2012, while the validation cohort covered the period from 2000 to 2012. This temporal separation helps to assess the model's performance across different time periods and patient demographics.\n\nTo enforce the independence of the training and test sets, we adhered to strict protocols that prevented any overlap between the datasets. This was achieved through careful randomization and verification processes. The datasets were drawn from a prospectively maintained database, ensuring high-quality and reliable data. However, it is important to note that while the data were prospectively collected, the analysis itself is retrospective.\n\nThe distribution of baseline variables between the training and validation cohorts was compared, revealing significant differences in several characteristics. For instance, the median age differed between the cohorts, with the validation cohort being older. Additionally, there were notable differences in tumor size, grade, histology, and anatomic location. These differences highlight the importance of external validation in assessing the model's generalizability to diverse patient populations. Despite these differences, the cohorts were chosen for their limited missing data and no loss to follow-up, ensuring robust validation of the model.",
  "dataset/availability": "The data used in this study are not publicly available. The training cohort was originally described by Nandra et al. and consisted of 3493 patients with newly diagnosed bone sarcomas treated between 1970 and 2012 at the Royal Orthopedics Hospital, Birmingham, UK. This cohort was used as the training set for creating the GBM model in this study.\n\nThe validation cohort comprised 771 patients with newly diagnosed bone sarcomas from the Danish Sarcoma Registry, included during January 1, 2000\u2013June 22, 2016. This registry is a national database prospectively maintained since January 1, 2009, with patients from 2000 to 2008 included later through validation processes.\n\nThe data splits used for training and testing were created through randomization, resulting in 10 unique test and train sets with balanced events per variable. Each test set comprised 20% of the data, and each train set comprised 80% of the data. The GBM model was trained on a training set of 2794 cases and tested on a corresponding test set of 699 cases.\n\nThe study was approved by the Danish Data Protection Agency (no. P-2019-54) and the Danish Patient Safety Authority (no. 3-3013-2866/1), ensuring compliance with ethical and regulatory standards. However, due to privacy and regulatory constraints, the specific datasets used in this study are not released in a public forum.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is Gradient Boosting Machines (GBMs). This class of algorithms is well-established and has been widely used in various fields due to its ability to handle large, complex datasets and provide high predictive accuracy.\n\nThe GBM algorithm is not new; it has been extensively studied and applied in both academic and industrial settings. The choice to use GBM in this study was driven by its proven effectiveness in generating non-parametric regression or classification models, as well as its ability to handle missing data and complex interactions within the data.\n\nThe decision to use GBM in a medical context, rather than publishing it in a machine-learning journal, is rooted in the specific goals of the study. The primary focus was to validate and develop prediction models for survival in patients with bone sarcoma, leveraging the strengths of GBM to achieve accurate and reliable predictions. The study aimed to contribute to the medical field by demonstrating the potential of machine-learning techniques in clinical decision-making, rather than introducing a novel algorithm.\n\nThe GBM algorithm's customizability and transparency were particularly advantageous for this study. The ability to fine-tune parameters and ensure full transparency in the model's construction was crucial for validating the model's performance and generalizability. This approach aligns with the broader trend in the research community of moving towards open-source software, which allows for independent validation and continuous improvement.",
  "optimization/meta": "The model developed in our study is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it is a Gradient Boosting Machine (GBM) model, which is an ensemble learning technique that builds predictive models in the form of an ensemble of weak prediction models, typically decision trees. The GBM model was trained on a dataset comprising various features related to patients with bone sarcoma, such as diagnosis, tumor size, age, metastasis at diagnosis, and year of diagnosis.\n\nThe GBM model was created using decision trees as base-learners. The outcome variable was binary, and the Bernoulli loss function was chosen accordingly. Missing data were handled using the missForest imputation method. For feature selection, the Boruta algorithm was employed, which shuffles copies of all features and trains a random forest on the overall data to confirm or reject features based on their relative influence in the model.\n\nThe training data used for the GBM model was split into 10 unique test and train sets with balanced events per variable. Each test and train set comprised 20% and 80% of the data, respectively. The model was trained on a training set of 2794 cases and subsequently tested on a corresponding test set of 699 cases. This approach ensured that the training data was independent and that the model's performance could be evaluated on unseen data.\n\nThe final parameters selected for the GBM model included a shrinkage rate of 0.01, an interaction depth of 3, a bag fraction of 0.8, and a minimum number of observations in a node of 5. The optimum number of iterations with minimum loss was determined to be 536. These parameters were chosen through a hyper-tuning process to optimize the model's performance and prevent overfitting.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the data was suitable for training and testing the Gradient Boosting Machine (GBM) model. The dataset was split into 10 unique test and train sets using randomization, with each set comprising 20% test data and 80% training data, ensuring balanced events per variable. The training set consisted of 2794 samples, while the test set had 699 samples.\n\nFor correct comparison with a previous model by Nandra et al., the same variables were used, excluding alkaline phosphatase due to missing data. Tumor sites were subcoded into five location categories as previously described. Decision trees were selected as base-learners, and the Bernoulli loss function was chosen because the outcome variable was binary.\n\nMissing data were handled using the missForest imputation method. This approach is robust for handling missing data in complex datasets. Feature selection was performed using the Boruta algorithm, which shuffles copies of all features and trains a random forest on the overall data. This process helps in rejecting or confirming features and ranking them based on their relative influence in the model.\n\nThe preprocessing steps ensured that the data was clean and ready for the GBM model, which is known for its ability to handle large, non-parametric sample sizes with complex interactions and substantial missing or outlying data. The final parameters selected for the GBM model included shrinkage of 0.01, interaction depth of 3, bag fraction of 0.8, and a minimum of 5 observations per node. The optimum number of iterations with minimum loss was determined to be 536. These steps collectively prepared the data for effective training and testing of the GBM model.",
  "optimization/parameters": "In our study, we utilized a Gradient Boosting Machine (GBM) model, which is known for its ability to handle complex interactions and substantial missing data. The model was trained on a dataset with 11 candidate features, which were selected to match those used by Nandra et al. These features included age, sex, tumor size at diagnosis, location, grade, metastasis at diagnosis, pathologic fracture at diagnosis, diagnosis, tumor site, status 1-year after diagnosis, and year of diagnosis. Notably, alkaline phosphatase was excluded due to missing data.\n\nThe selection of these parameters was crucial for ensuring a fair comparison with the previous model by Nandra et al. We did not exclude or include any additional variables beyond these 11 features. Tumor sites were subcoded into five location categories, following the methodology described by Nandra et al.\n\nFor feature selection, we employed the Boruta algorithm, which is designed to identify all relevant features in a dataset. This algorithm works by shuffling copies of all features and training a random forest on the overall data. Features are then either rejected or confirmed based on their importance in the model. This process helps in ranking features by their relative influence, ensuring that only the most relevant features are included in the final model.\n\nThe GBM model parameters were carefully tuned to mitigate overfitting. The final parameters selected were shrinkage = 0.01, interaction depth = 3, bag fraction = 0.8, and n.minobsinnode = 5. The optimum number of iterations with minimum loss was determined to be 536. These parameters were chosen through a hyper-tuning process to balance model complexity and generalization performance.\n\nIn summary, the model used 11 parameters, which were selected to align with a previous study and validated through rigorous feature selection and hyperparameter tuning processes.",
  "optimization/features": "The input features used in the model were carefully selected to ensure consistency with a previous study by Nandra et al. This consistency was maintained to allow for a fair comparison between the models. The features included in the final analysis were age, sex, tumor size at diagnosis, location, grade, metastasis at diagnosis, pathologic fracture at diagnosis, diagnosis, tumor site, status 1-year after diagnosis, and year of diagnosis. This resulted in a total of 11 features used as input.\n\nFeature selection was performed using the Boruta algorithm, which is designed to identify and rank the most relevant features for the model. The Boruta algorithm works by shuffling copies of all features and training a random forest on the overall data. This process helps to either reject or confirm features based on their importance in the model. The confirmed features are then ranked according to their relative influence.\n\nTo ensure the integrity of the feature selection process, it was conducted using only the training set. This approach helps to prevent data leakage and ensures that the model's performance on the test set is a true reflection of its generalizability. By using the training set exclusively for feature selection, the risk of overfitting is minimized, and the model's ability to perform well on unseen data is enhanced.",
  "optimization/fitting": "The fitting method employed for the Gradient Boosting Machine (GBM) model was carefully designed to address potential issues of overfitting and underfitting. The model was trained on a dataset comprising 2794 samples, with a corresponding test set of 699 samples. The number of parameters in the model was managed through hyperparameter tuning, ensuring that the model's complexity was appropriate for the dataset size.\n\nTo mitigate overfitting, a 10-fold cross-validation was initially conducted. This process involved splitting the data into 10 unique test and train sets, with each set comprising 20% and 80% of the data, respectively. The GBM model was trained on the training set and subsequently tested on the corresponding test set. This approach helped in assessing the model's performance on different subsets of the data, reducing the risk of overfitting to any single subset.\n\nAdditionally, the model's hyperparameters were carefully selected to balance complexity and performance. Parameters such as shrinkage, interaction depth, bag fraction, and the minimum number of observations in a node were tuned to optimize the model's performance. The final parameters selected were shrinkage = 0.01, interaction depth = 3, bag fraction = 0.8, and n.minobsinnode = 5. The optimum number of iterations with minimum loss was determined to be 536. This hyperparameter tuning process was crucial in preventing overfitting by ensuring that the model did not become too complex relative to the number of training points.\n\nTo address underfitting, the model's capacity was sufficiently large to capture the underlying patterns in the data. The use of decision trees as base-learners allowed the model to handle complex interactions and non-parametric sample sizes. The Bernoulli loss function was chosen for the binary outcome variable, and missing data were imputed using missForest, ensuring that all available information was utilized. Feature selection was performed using the Boruta algorithm, which helped in identifying the most relevant features for the model, further enhancing its predictive power.\n\nIn summary, the fitting method for the GBM model involved a careful balance of model complexity and performance. Overfitting was ruled out through 10-fold cross-validation and hyperparameter tuning, while underfitting was addressed by ensuring the model had sufficient capacity to capture the data's underlying patterns. The resulting model demonstrated good accuracy and performance in predicting 1-year mortality in patients with newly diagnosed bone sarcoma.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting, a common risk in machine learning models, particularly with complex algorithms like Gradient Boosting Machines (GBM).\n\nFirstly, we utilized a 10-fold cross-validation approach. This involved splitting the data into 10 unique test and train sets, with each set comprising 20% and 80% of the data, respectively. This method ensures that the model is trained and tested on different subsets of the data, reducing the likelihood of overfitting to any single subset.\n\nSecondly, we carefully selected and tuned the hyperparameters of the GBM model. Hyperparameter tuning is crucial for balancing the model's complexity and its ability to generalize to new data. We chose parameters such as shrinkage, interaction depth, bag fraction, and the minimum number of observations in a node. The shrinkage parameter, for instance, controls the learning rate, which helps in preventing the model from fitting the noise in the data.\n\nAdditionally, we used the Boruta algorithm for feature selection. This algorithm helps in identifying and confirming the most relevant features, thereby reducing the dimensionality of the data and preventing the model from overfitting to irrelevant features.\n\nMoreover, we addressed missing data using the missForest imputation method. Proper handling of missing data is essential to avoid introducing bias or noise into the model, which can lead to overfitting.\n\nLastly, we monitored the number of iterations during the training process. While increasing the number of iterations can improve the model's performance on the training data, it can also lead to overfitting. We found that beyond a certain number of iterations (n = 536 in our case), the model's performance on the training data improved, but its generalizability decreased. Therefore, we selected the optimal number of iterations that minimized the loss and maximized the model's generalizability.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. The final parameters selected for our Gradient Boosting Machine (GBM) model were shrinkage = 0.01, interaction depth = 3, bag fraction = 0.8, and n.minobsinnode = 5. The optimum number of iterations with minimum loss was determined to be n = 536. These specifics are provided to ensure reproducibility and transparency.\n\nThe code used for the development and validation of the GBM model, including the hyper-parameter tuning process, is included as supplementary material. This allows other researchers to access and utilize the exact methods and configurations employed in our study.\n\nRegarding model files, while the specific files are not directly referenced in the text, the supplementary material and the detailed methodology provided should enable the recreation of the model. The use of publicly available software such as R Studio for development and validation further supports the accessibility of our work.\n\nThe optimization parameters and the process of feature selection using the Boruta algorithm are also described. The Boruta algorithm's role in training a random forest to confirm and rank features is explained, ensuring that the feature selection process is clear and reproducible.\n\nIn summary, the hyper-parameter configurations, optimization schedule, and relevant code are available and detailed within the supplementary material and the main text. This information is provided to facilitate reproducibility and to allow other researchers to build upon our findings.",
  "model/interpretability": "The models discussed in our study, specifically the Bayesian Belief Network (BBN) and Gradient Boosting Machine (GBM), are generally considered black-box models. This means that while they can provide accurate predictions, the internal workings and the specific reasons behind these predictions are not easily interpretable.\n\nThe BBN model, for instance, uses a probabilistic approach to represent the relationships between different variables. While it can show the dependencies between variables, the exact way it combines these dependencies to make a prediction is not straightforward to interpret. Similarly, the GBM model, which is an ensemble of decision trees, can be quite complex and opaque. Although decision trees themselves are interpretable, the combination of many trees in a boosting framework makes the overall model difficult to understand.\n\nHowever, some level of interpretability can be achieved through feature importance analysis. For the GBM model, we identified that features such as diagnosis, tumor size, and age were ranked highest in variable importance. This indicates that these features significantly influence the model's predictions. While this does not provide a full understanding of how the model makes decisions, it does give some insight into which factors are most critical.\n\nIn summary, while our models are powerful predictive tools, they lack transparency in terms of how they arrive at their predictions. Future work could focus on developing more interpretable models or techniques to better understand the decision-making processes of these black-box models.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict 1-year mortality in patients with newly diagnosed bone sarcoma. The outcome variable is binary, indicating whether a patient survives or does not survive within one year of diagnosis. This binary nature of the outcome makes the model a classification model rather than a regression model. The model's performance was evaluated using metrics such as the area under the receiver operating characteristic curve (AUC ROC) and the Brier score, which are commonly used for assessing classification models. The decision curve analysis (DCA) further supported the model's clinical usability by quantifying the net benefit across a range of threshold probabilities. The features used in the model, such as age, tumor size, and diagnosis, were selected to predict this binary outcome, reinforcing the classification nature of the model.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models developed in this study is available as supplementary material. This allows for independent validation and continuous development by the research community. The code is published to ensure transparency and to facilitate further refinement and customization of the functions. The specific software used for development and validation includes R and Python, which are open-source and widely used in the field of machine learning. The use of open-source software is advantageous as it is available at low or no cost and allows for community-driven improvements. The code is included as supplementary material, enabling other researchers to access and utilize it for their own studies or to validate the models in different populations. The supplementary material provides the necessary details to reproduce the models and apply them to new datasets.",
  "evaluation/method": "The evaluation of the models involved both internal and external validation processes. For internal validation, a 10-fold cross-validation was conducted on the training cohort. This process involved splitting the data into 10 unique test and train sets, with each set comprising 20% and 80% of the data, respectively. This method helps to mitigate overfitting and ensures that the model generalizes well to unseen data.\n\nFor external validation, the models were tested on a Danish population cohort. This cohort included patients from a prospectively maintained database, although the data were considered retrospective. The validation cohort consisted of 771 patients, with 113 (15%) dying within the 1-year follow-up period. The external validation set was chosen due to its completeness, with no loss to follow-up and limited missing data.\n\nThe performance of the models was evaluated using several metrics. Discrimination was assessed using the area under the receiver operating characteristic curve (AUC). An AUC greater than 0.7 was considered the lowest acceptable threshold for successful validation. The AUC measures the probability that a person who experienced the outcome (death) had a higher predicted probability than someone who did not. A value of 1 indicates perfect discrimination, while a value of 0.5 represents chance.\n\nOverall model performance was evaluated using the Brier score, which quantifies the agreement between predicted probabilities and observed outcomes. The Brier score ranges from 0 to 1, with 0 indicating perfect agreement and 1 indicating perfect disagreement. A score of 0.25 reflects a 50% incidence of the outcome, and scores above 0.25 are considered noninformative.\n\nThe Bayesian belief network (BBN) model was used \"as-is\" without prior refitting or optimization. Validation of the BBN model was performed using commercially available software. The gradient boosting machine (GBM) model was trained on a training set of 2,794 patients and tested on a corresponding test set of 699 patients. Decision trees were chosen as base-learners, and the Bernoulli loss function was used due to the binary nature of the outcome variable. Missing data were imputed using the missForest method.\n\nThe external validation results showed that the GBM model did not outperform the BBN model when validated in the Danish population cohort. The study highlights the importance of external validation of prediction models before clinical use and encourages further validation in non-Scandinavian populations.",
  "evaluation/measure": "In our evaluation, we employed several performance metrics to assess the models' predictive capabilities. For discrimination, we used the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve. The AUC provides a measure of how well the model can distinguish between patients who survive and those who do not, with values ranging from 0.5 (no discrimination) to 1 (perfect discrimination). We considered an AUC greater than 0.7 as the lowest acceptable threshold for successful validation.\n\nAdditionally, we used the Brier score to evaluate the overall performance of the models. The Brier score quantifies the accuracy of probabilistic predictions, with lower scores indicating better performance. It measures the mean squared difference between predicted probabilities and actual outcomes, with a score of 0 indicating perfect prediction and a score of 0.25 reflecting a 50% incidence of the outcome, which is considered noninformative.\n\nTo assess the clinical utility of the models, we performed Decision Curve Analysis (DCA). DCA evaluates the net benefit of using the model across a range of threshold probabilities, comparing it to the strategies of treating all patients or treating none. A model demonstrates clinical usefulness if it shows a net benefit across a range of thresholds, indicating that it outperforms these simple strategies.\n\nThese metrics are widely used in the literature for evaluating predictive models, particularly in survival analysis. The AUC and Brier score provide a comprehensive assessment of the model's discriminative ability and overall accuracy, while DCA offers insights into the model's clinical applicability. By using these metrics, we aimed to provide a thorough evaluation of our models' performance and their potential value in clinical settings.",
  "evaluation/comparison": "In our study, we did not perform a comparison to publicly available methods on benchmark datasets. Instead, our focus was on validating and comparing two specific models: the Bayesian belief network (BBN) model developed by Nandra et al. and a new gradient boosting machine (GBM) model that we created.\n\nFor the comparison, we used the same metrics for both models: discrimination by ROC analysis and AUC, and overall performance using the Brier score. This approach allowed us to directly compare the performance of the two models on the same validation set, which consisted of a Danish population cohort.\n\nWe did not use simpler baselines for comparison. The BBN model served as a baseline for our study, as it was the existing model that we aimed to validate and potentially improve upon. The GBM model was developed using the same training cohort as the BBN model, ensuring a fair comparison.\n\nThe comparison was conducted using external validation, which is a crucial step in assessing the generalizability and robustness of prediction models. By validating both models on an independent Danish population cohort, we could evaluate their performance in a real-world setting and determine whether the GBM model outperformed the BBN model.",
  "evaluation/confidence": "The evaluation of the models included several performance metrics, each accompanied by confidence intervals to indicate the reliability of the results. The area under the receiver operator characteristic curve (AUC-ROC) for the Bayesian belief network (BBN) model was reported as 68% with a 95% confidence interval (CI) of 62%-73%. For the gradient boosting machine (GBM) model, the AUC-ROC was 75% (95% CI: 70%-80%) during internal validation and 63% (95% CI: 57%-68%) during external validation. These intervals provide a range within which the true AUC-ROC is likely to fall, offering a measure of the precision of these estimates.\n\nThe Brier score, which assesses the overall performance of the models, was also reported with confidence intervals. For the BBN model, the Brier score was 0.12 (95% CI: 0.102\u20130.141), and for the GBM model, it was 0.09 (95% CI: 0.077\u20130.11) during internal validation and 0.14 (95% CI: 0.12\u20130.16) during external validation. These scores, along with their confidence intervals, help in understanding the models' accuracy in predicting survival probabilities.\n\nStatistical significance was assessed using nonparametric tests such as the Mann\u2013Whitney U-test for continuous variables and the chi-square test for categorical variables. These tests were used to compare baseline distributions between the training and validation cohorts, ensuring that any observed differences were not due to chance.\n\nDecision curve analysis (DCA) was employed to evaluate the clinical utility of the models. The GBM model demonstrated a positive net benefit for probability thresholds above 0.5, indicating its potential suitability for clinical use in specific scenarios. However, at threshold probabilities below 0.5, assuming that all patients are alive provided more benefit. This analysis helps in determining the range of threshold probabilities where the model's predictions are most useful.\n\nIn summary, the performance metrics included confidence intervals, and statistical tests were used to ensure the significance of the results. The models' clinical utility was assessed through DCA, providing insights into their practical applicability.",
  "evaluation/availability": "The raw evaluation files for our study are not publicly available. The evaluation process involved internal and external validation of our models using specific datasets, which were not released due to privacy and ethical considerations. The internal validation was conducted using a test set comprising 699 cases, while the external validation utilized a Danish cohort. The datasets used for these evaluations are part of the Danish Sarcoma Registry and other clinical databases, which are not openly accessible to the public. The validation process included metrics such as ROC analysis, AUC, and Brier score, which were calculated using commercially available software. The specific details and results of these evaluations are thoroughly described in the publication, but the raw data itself remains confidential."
}