{
  "publication/title": "Development of Machine Learning Models for Predicting the 1-Year Risk of Reoperation After Lower Limb Oncological Resection and Endoprosthetic Reconstruction Based on Data From the PARITY Trial.",
  "publication/authors": "Deng J, Moskalyk M, Shammas-Toma M, Aoude A, Ghert M, Bhatnagar S, Bozzo A",
  "publication/journal": "Journal of surgical oncology",
  "publication/year": "2024",
  "publication/pmid": "39257289",
  "publication/pmcid": "PMC11849712",
  "publication/doi": "10.1002/jso.27854",
  "publication/tags": "- Machine Learning\n- Cross-Validation\n- Model Performance\n- Classification Metrics\n- Calibration\n- AUC-ROC\n- Brier Score\n- Sensitivity\n- Specificity\n- Matthews Correlation Coefficient",
  "dataset/provenance": "The dataset used in this study originates from the PARITY trial, which was conducted across 48 clinical sites in 12 countries from January 1, 2013, to October 29, 2019. The trial included a total of 604 patients, with baseline and 1-year follow-up data from all participants being analyzed in the current study.\n\nThe outcome of interest for our predictive models was unplanned additional surgeries within 1 year following the original lower extremity resection/reconstruction surgery. This outcome was recorded as a secondary outcome of the PARITY trial.\n\nThe dataset has been previously described in detail, including the eligibility criteria and study protocol. The PARITY trial generated one of the largest and most comprehensive prospective datasets on malignant bone tumors and soft tissue sarcomas. The dataset includes a variety of features, such as patient demographics, tumor characteristics, and surgical details, which were used for predictive modeling.\n\nThe dataset has been used in previous publications, including univariate and multivariate analyses, to identify predictors of reoperation risk. The features selected for our machine learning models were based on availability and expert domain knowledge, with further dimension reduction using LASSO penalized logistic regression and Boruta. The final feature set included around 10 features, following the \"one in ten\" rule commonly used in predictive modeling.",
  "dataset/splits": "The dataset was split into two main cohorts: a derivation cohort and an internal validation cohort. The derivation cohort constituted 80% of the total data, while the internal validation cohort made up the remaining 20%. This split was stratified by outcome classification to ensure a balanced representation of both classes in each cohort.\n\nThe derivation cohort was further divided into different training and testing subsets during the cross-validation process for model tuning and evaluation. Specifically, fivefold cross-validation was employed, meaning the derivation cohort was split into five subsets. In each iteration of the cross-validation, one subset was used as the test set, and the remaining four subsets were used as the training set. This process was repeated five times, with each subset serving as the test set once.\n\nThe total dataset included 604 patients. Therefore, approximately 483 patients were in the derivation cohort, and around 121 patients were in the internal validation cohort. The distribution of data points in each fold of the cross-validation within the derivation cohort was roughly equal, with each fold containing about 97 patients. This approach ensured that each model was trained and tested on different portions of the data, providing a robust evaluation of its performance.",
  "dataset/redundancy": "The dataset was split into an 80% derivation cohort and a 20% internal validation cohort, stratified by outcome classification. This ensures that both subsets maintain the same proportion of patients who required additional operations and those who did not. The derivation cohort was further divided into different training and testing subsets during cross-validation for model tuning and evaluation. This approach helps in assessing the model's performance on unseen data and reduces the risk of overfitting.\n\nTo maintain the independence of the training and test sets, data preprocessing steps were applied after the cross-validation splits and after the derivation-validation split. This strategy prevents potential data leakage, where information from the test set might inadvertently influence the training process. By enforcing this separation, the model's performance metrics, such as the area under the receiver operating characteristic curve (AUC-ROC) and the Brier score, are more reliable and generalizable.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets in the medical field, particularly those dealing with rare conditions like soft tissue sarcomas and malignant bone tumors. The PARITY trial generated one of the largest and most comprehensive prospective datasets in this area, although the sample size was still lower than the ideal threshold for optimal predictions from modern machine learning algorithms and deep-learning neural networks. This limitation was addressed by employing robust feature selection methods and hyperparameter tuning techniques to maximize the model's performance within the constraints of the available data.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study is Bayesian Optimization. This method is not new and is widely recognized in the machine learning community for its efficiency in hyperparameter tuning. Bayesian Optimization starts with random parameter searches to gather initial data points. These data points are used to build a probabilistic model that predicts the performance of different hyperparameter settings. An acquisition function then identifies the most promising parameters for the next round of evaluations. The results are used to update the performance model, and this process is repeated until a pre-established performance budget is exhausted.\n\nBayesian Optimization is empirically considered superior to traditional random search techniques and grid-search methods. It is particularly effective for tuning machine learning algorithms because it can reliably identify hyperparameters from the top 5% of the most performant combinations with a relatively small number of iterations. In our study, we set the budget for Bayesian Optimization to at least 500 iterations per model to ensure thorough exploration of the hyperparameter space.\n\nThis algorithm was chosen for its proven effectiveness in optimizing complex models, making it a suitable choice for tuning the six commonly-used machine learning classification algorithms in our study: Penalized Logistic Regression, Support Vector Machine (SVM), Random Forest, Light Gradient Boosting Machines (LightGBM), eXtreme Gradient Boosting (XGBoost), and Neural Networks. The use of Bayesian Optimization allowed us to efficiently find the optimal hyperparameters for each model, leading to improved classification performance.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure compatibility with machine learning algorithms. We handled categorical data by encoding ordinal features, such as tumor grade and the amount of preoperative chemotherapy received, as ordinal integer values. For categorical features without clear ordinality, like ethnicity and tumor type, we used one-hot encoding to transform them into a format suitable for machine learning models.\n\nTo address missing data, we imputed values based on the specific characteristics of each feature. This step was essential to maintain the integrity of the dataset and prevent any potential data leakage. We also implemented techniques to correct for class imbalance, which is common in medical datasets. This involved using a combination of Synthetic Minority Oversampling Technique (SMOTE) and Tomek Links, as well as random undersampling. Additionally, we scaled each feature according to its variance and re-centered the data distribution around the mean. This scaling process helped to improve the performance of the machine learning algorithms by ensuring that all features contributed equally to the model.\n\nWe also employed principal component analysis (PCA) to address multicollinearity within the selected features. PCA helped to reduce the dimensionality of the data while retaining the most important information, which is beneficial for model training and performance. The preprocessing pipeline was designed to be applied after cross-validation splits and after the derivation-validation split to avoid any potential data leakage, ensuring that the models were trained and evaluated on independent datasets.",
  "optimization/parameters": "In our study, the number of input parameters, p, used in the model was determined through a combination of domain expertise and automated feature selection techniques. Initially, a set of features was chosen based on clinical relevance and expert domain knowledge. To address potential multicollinearity, principal component analysis was trialed as a hyperparameter in the data preprocessing pipeline. Additionally, we employed LASSO-based and Boruta-based feature selection methods to further refine the feature set. These techniques helped in identifying the most relevant features while mitigating the risk of overfitting and ensuring that the model generalizes well to new data. The final feature set, which included the most important parameters, was used for model training and evaluation. The specific features selected are detailed in the supporting information.",
  "optimization/features": "The input features for the models were selected through a rigorous process. Initially, candidate features were chosen based on their availability (with no more than 30% missing data) and expert domain knowledge. The dimension of the feature set was further reduced using two methods: Least Absolute Shrinkage and Selection Operator (LASSO) penalized logistic regression and Boruta. The final feature set included a union of features selected by these two methods. This approach aimed to include around 10 features, adhering to the \"one in ten\" rule commonly used in predictive modeling, which suggests that 10 events are needed for every predictor included in the model.\n\nThe final feature set consisted of approximately 10 features, which were used as input for the predictive models. Feature selection was performed using the training set only, ensuring that the selection process did not introduce bias from the validation or test sets. This method helped in identifying the most relevant features for predicting the outcome of interest, which was unplanned additional surgeries within one year following the original lower extremity resection/reconstruction surgery.",
  "optimization/fitting": "The fitting method involved tuning and calibrating six different machine learning algorithms: Penalized Logistic Regression, Support Vector Machine, Random Forest, Light Gradient Boosting Machines, eXtreme Gradient Boosting, and Neural Networks. Each algorithm was tuned to minimize cross-entropy loss across stratified fivefold cross-validation. This approach helps to ensure that the models generalize well to unseen data and do not overfit to the training set.\n\nFor neural networks, the maximum number of layers was capped at three to prevent overfitting. Additionally, dropout was used to further mitigate overfitting by randomly setting a fraction of input units to zero at each update during training time. Performance-based learning rate decay was also trialed to improve network performance and prevent overfitting.\n\nThe number of iterations for Bayesian Optimization was set to at least 500 per model, ensuring a thorough search of the hyperparameter space. This method is empirically considered superior to traditional random search techniques and grid-search methods, providing a more efficient way to find optimal hyperparameters.\n\nTo address potential underfitting, various resampling techniques were employed, including Synthetic Minority Oversampling Technique (SMOTE) combined with Tomek Links and random undersampling. These techniques help to balance the dataset and ensure that the models are exposed to a diverse range of examples, reducing the risk of underfitting.\n\nFeature scaling was performed to re-center the data distribution around the mean, which helps in improving the convergence of optimization algorithms and the overall performance of the models. This step is crucial for algorithms like Support Vector Machines and Neural Networks, which are sensitive to the scale of the input features.\n\nIn summary, the fitting method included robust techniques to prevent both overfitting and underfitting, ensuring that the models were well-tuned and generalized effectively to new data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting during the model training process. One of the key methods used was dropout, which is particularly relevant for neural networks. Dropout works by randomly setting a fraction of input units to zero at each update during training time, which helps to prevent overfitting by ensuring that the network does not become too reliant on any single neuron.\n\nAdditionally, we utilized regularization techniques such as penalized logistic regression, which incorporates a penalty term to the loss function to constrain or regularize the model parameters. This helps in reducing the complexity of the model and prevents it from fitting the noise in the training data.\n\nFor neural networks, we also implemented performance-based learning rate decay. This technique adjusts the learning rate during training based on the model's performance, which can help in fine-tuning the model and preventing overfitting by ensuring that the learning rate decreases as the model converges.\n\nThese regularization methods were integral to our approach, ensuring that our models generalized well to unseen data and did not overfit to the training set.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule are reported in the publication. The optimal hyper-parameter combinations identified during cross-validation for each model are detailed in the supplementary material, specifically in Supporting Information S1: Table S4. The optimization process involved Bayesian Optimization, which is described in detail within the text. This method was used to tune six commonly-used machine learning classification algorithms: Penalized Logistic Regression, Support Vector Machine (SVM), Random Forest, Light Gradient-Boosting Machines (LightGBM), eXtreme Gradient-Boosting (XGBoost), and Neural Networks. The Bayesian Optimization process started with random parameter searches to gather data points for building a probabilistic model that predicts the performance of different hyperparameter settings. An acquisition function then identified the most promising parameters for the next round of evaluations. This process was repeated until a pre-established performance budget of at least 500 iterations per model was exhausted.\n\nRegarding model files and optimization parameters, the specific files and detailed parameters are not explicitly provided in the main text or supplementary materials. However, the methods and tools used for data preprocessing, model fitting, tuning, and calibration are well-documented. These include scikit-learn, lightgbm, xgboost, tensorflow, keras, scikit-optimize, and ml-insights in Python, as well as arsenal and stddiff in R. The feature importance was assessed using eli5 in Python. The polynomial SVM model, which was designated as the best-performing model, was incorporated into an online re-operation risk calculator available at https://parity-ml.shinyapps.io/reop-estimator/. This calculator includes the re-trained polynomial SVM model pipeline and other ML algorithms for demonstration purposes.\n\nThe license under which these configurations, schedules, and tools are available is not specified in the provided information. However, the tools and libraries mentioned are commonly used in the machine learning community and are typically open-source, allowing for reproducibility and further development by other researchers.",
  "model/interpretability": "The models developed in this study, including the polynomial Support Vector Machine (SVM) and Random Forest, are generally considered to be more interpretable compared to other machine learning models like neural networks. These models provide insights into feature importance, which helps in understanding the factors contributing to the predictions.\n\nThe polynomial SVM model, which was designated as the best-performing model overall, allows for some level of interpretability through its feature weights. These weights indicate the importance of each feature in the model's decision-making process. For instance, total operative time was identified as the most important feature, suggesting that longer operative times are strongly associated with the risk of reoperation.\n\nRandom Forest models are also known for their interpretability. They provide feature importance scores based on the contribution of each feature to the predictive accuracy of the model. This makes it easier to understand which factors are most influential in predicting the risk of reoperation.\n\nIn contrast, models like neural networks are often considered black-box models due to their complex architecture and the difficulty in tracing the decision-making process. However, techniques such as permutation importance can be used to assess the importance of features in neural networks as well.\n\nOverall, the models used in this study strike a balance between performance and interpretability, providing valuable insights into the key factors influencing reoperation risk. This interpretability is crucial for clinical deployment, as it allows healthcare providers to understand and trust the model's predictions.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the risk of reoperations in patients undergoing oncological resection and endoprosthetic reconstruction of the lower limbs. The model categorizes patients into those who will require additional unplanned operations and those who will not. Various machine learning algorithms were employed, including Penalized Logistic Regression (Ridge), Support Vector Machine (Polynomial), Random Forest, LightGBM, XGBoost, and Neural Network. Each model was evaluated using threshold-independent metrics such as the Area Under the Receiver Operating Characteristic Curve (AUC-ROC), as well as threshold-dependent metrics like Matthew's Correlation Coefficient (MCC), sensitivity, and specificity. The models were calibrated and tuned to optimize their classification performance, with the polynomial Support Vector Machine (SVM) ultimately being designated as the best-performing model. This model was then internally validated and incorporated into an online reoperation risk calculator, demonstrating its practical application in clinical settings.",
  "model/duration": "The execution time for the models was not explicitly detailed in the publication. However, it is mentioned that Bayesian Optimization was performed with a budget of at least 500 iterations per model. This suggests that the models underwent extensive training and tuning processes, which typically require significant computational resources and time. The specific duration for each model's execution would depend on various factors, including the complexity of the model, the computational power used, and the size of the dataset. For precise execution times, further details from the implementation environment would be necessary.",
  "model/availability": "The source code for the models is not publicly released. However, the polynomial Support Vector Machine (SVM) model pipeline was retrained on the entire dataset and incorporated into an online reoperation risk calculator. This calculator is available at https://parity-ml.shinyapps.io/reop-estimator/. Other machine learning algorithms were also retrained and included in the calculator for demonstration purposes. The calculator allows users to input patient characteristics and operative parameters to receive individualized reoperation risk predictions. The specific details about the software used for data preprocessing, model fitting, tuning, and calibration include scikit-learn, lightgbm, xgboost, tensorflow, keras, scikit-optimize, and ml-insights in Python. Feature importance was assessed using eli5 in Python.",
  "evaluation/method": "The evaluation method employed a comprehensive framework to assess the performance of various machine-learning models. The models underwent fivefold cross-validation to ensure robust and generalizable results. This process involved splitting the data into five subsets, where each subset was used once as a validation set while the remaining four subsets were used for training. This approach was repeated five times, with each subset serving as the validation set once.\n\nSeveral metrics were used to evaluate the models' performance. The area under the receiver operating characteristic curve (AUC-ROC) was calculated to measure the models' ability to distinguish between classes. The Brier score was used to assess the accuracy of probabilistic predictions, with lower scores indicating better calibration. Calibration curve slope and intercept were also evaluated to ensure that the predicted probabilities aligned well with the actual outcomes.\n\nThreshold-dependent metrics, including Matthew's correlation coefficient (MCC), sensitivity, and specificity, were assessed at both a balanced threshold and a high-sensitivity threshold. MCC was chosen for its ability to provide a balanced measure of classification performance, considering true positives, false negatives, true negatives, and false positives. Sensitivity and specificity were evaluated to understand the models' performance in identifying positive cases and correctly classifying negative cases, respectively.\n\nFollowing hyperparameter tuning and calibration, the models underwent threshold tuning to maximize classification performance. The optimal threshold was selected based on the highest average MCC across the fivefold cross-validation. Additionally, a high-sensitivity threshold was created to ensure that the models maintained a sensitivity of at least 0.70, which is crucial for clinical applications where high sensitivity is preferred.\n\nThe best-performing model was further validated using an internal validation cohort, ensuring that the model's performance was consistent across different datasets. Feature importance was assessed using the permutation importance method to identify the most influential features in the model's predictions.\n\nIn summary, the evaluation method involved a rigorous process of cross-validation, metric assessment, threshold tuning, and internal validation to ensure that the models were robust, well-calibrated, and clinically relevant.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the machine learning models. These metrics include both threshold-independent and threshold-dependent measures, ensuring a thorough assessment of model performance.\n\nThreshold-independent metrics reported are the area under the receiver operating characteristic curve (AUC-ROC) and the Brier score. The AUC-ROC provides a single scalar value that represents the ability of the model to distinguish between classes across all possible classification thresholds. The Brier score measures the mean squared difference between predicted probabilities and the actual binary outcomes, offering insight into the accuracy of probability estimates.\n\nFor threshold-dependent metrics, we evaluated models at two different thresholds: a balanced threshold and a high-sensitivity threshold. At the balanced threshold, we reported the Matthews correlation coefficient (MCC), sensitivity, and specificity. The MCC is particularly valuable as it considers all four quadrants of the confusion matrix, providing a balanced measure of classification performance. Sensitivity and specificity offer insights into the model's ability to correctly identify positive and negative cases, respectively.\n\nAt the high-sensitivity threshold, we again reported the MCC, sensitivity, and specificity. This threshold is designed to prioritize sensitivity, which is crucial in clinical settings where missing a positive case can have significant consequences. The high-sensitivity threshold ensures that the model maintains a high true positive rate, even if it means accepting a higher false positive rate.\n\nAdditionally, we assessed calibration performance using the calibration curve slope and intercept. These metrics help evaluate how well the predicted probabilities align with the actual outcomes, ensuring that the model's predictions are reliable and well-calibrated.\n\nThis set of metrics is representative of current best practices in the literature, providing a robust evaluation of model performance across various dimensions. By including both threshold-independent and threshold-dependent measures, as well as calibration metrics, we ensure a comprehensive assessment of our models' predictive capabilities and reliability.",
  "evaluation/comparison": "In our evaluation process, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on optimizing and evaluating various machine learning models tailored to our specific dataset and clinical prediction task.\n\nWe did, however, compare the performance of different machine learning models, including both traditional and advanced techniques. These models included Penalized Logistic Regression (Ridge), Support Vector Machine (Polynomial), Random Forest, LightGBM, XGBoost, and Neural Networks. Each model underwent rigorous hyperparameter tuning, calibration, and threshold selection to maximize their classification performance.\n\nFor hyperparameter tuning, we employed Bayesian Optimization, setting a budget of at least 500 iterations per model. This approach allowed us to systematically explore the hyperparameter space and identify the optimal settings for each model.\n\nFollowing hyperparameter tuning, we calibrated the models using three different approaches: Platt/sigmoidal scaling, isotonic regression, and spline-based calibration. The calibration approach that yielded the lowest average cross-entropy loss on fivefold cross-validation was selected for each model.\n\nThreshold selection was another critical step in our evaluation process. We calculated Matthew's correlation coefficient (MCC) for every classification threshold between 0.01 and 0.99 at an interval of 0.01. The threshold that produced the highest average MCC across fivefold cross-validation was chosen as the optimal threshold. Additionally, we created a high-sensitivity threshold to cater to clinical preferences for more conservative risk classifications.\n\nOur evaluation metrics included area under the receiver operating characteristic curve (AUC-ROC), MCC, sensitivity, and specificity for classification performance. Calibration performance was assessed using the Brier score, calibration curve slope, and calibration curve intercept. All metrics were averaged across fivefold cross-validation, and 95% confidence intervals were used to assess performance variance between folds.\n\nIn summary, while we did not compare our methods to publicly available benchmarks, we conducted a comprehensive evaluation of various machine learning models using a robust framework. This approach ensured that our selected model was optimized for our specific dataset and clinical prediction task.",
  "evaluation/confidence": "The evaluation of the machine-learning models included a comprehensive assessment of performance metrics, each accompanied by 95% confidence intervals. This approach ensures that the reported metrics are statistically robust and provides a clear indication of the variance between different folds of cross-validation.\n\nThe confidence intervals for metrics such as the area under the receiver operating characteristic curve (AUC-ROC), Brier score, calibration curve slope, calibration curve intercept, Matthew's correlation coefficient, sensitivity, and specificity are explicitly provided. These intervals help in understanding the reliability and consistency of the model's performance across different validation sets.\n\nStatistical significance is a crucial aspect of evaluating the superiority of the proposed methods over baselines. The use of confidence intervals allows for a rigorous assessment of whether the observed performance differences are likely to be due to chance or represent a true improvement. By comparing these intervals, it is possible to determine if the performance of one model is significantly better than another.\n\nIn summary, the inclusion of confidence intervals in the performance metrics ensures that the results are statistically significant and provides a strong basis for claiming the superiority of the methods over baselines. This rigorous evaluation framework enhances the credibility and reliability of the findings presented in the study.",
  "evaluation/availability": "Not enough information is available."
}