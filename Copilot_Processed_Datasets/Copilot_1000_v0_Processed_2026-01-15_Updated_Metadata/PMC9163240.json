{
  "publication/title": "Prediction of airborne pollen concentrations by artificial neural network and their relationship with meteorological parameters and air pollutants.",
  "publication/authors": "Goudarzi G, Birgani YT, Assarehzadegan MA, Neisi A, Dastoorpoor M, Sorooshian A, Yazdani M",
  "publication/journal": "Journal of environmental health science & engineering",
  "publication/year": "2022",
  "publication/pmid": "35669831",
  "publication/pmcid": "PMC9163240",
  "publication/doi": "10.1007/s40201-021-00773-z",
  "publication/tags": "- Prediction\n- Pollen\n- Allergen\n- Thunderstorm asthma attack\n- Artificial neural network\n- Ahvaz\n- Air quality\n- Meteorological parameters\n- Air pollutants\n- Respiratory diseases",
  "dataset/provenance": "The dataset used in this study was collected from Ahvaz, Iran, over three autumn seasons: 2016, 2017, and 2019. Sampling was conducted from September 22 to December 20 in 2016, from September 23 to December 21 in 2017, and from October 5 to December 20 in 2019. The sampling focused on the autumn season due to the high concentrations of pollen grains and the typical outbreak of respiratory diseases after the first autumnal rain in Ahvaz.\n\nThe dataset includes measurements of various types of pollen grains, with a total of 6788 pollen grains identified. The most prevalent pollen types were Amaranthaceae sp. (54.20%), Cynodon dactylon (14.28%), and Artemisia (12.90%). The least common was Sorghum (0.56%). Additionally, the dataset includes meteorological parameters such as atmospheric pressure, temperature, rain, relative humidity, and wind speed, provided by the Ahvaz Meteorological Organization. Air pollutant data, including PM, SO2, NO2, O3, and carbon monoxide, were obtained from the Iran Department of Environment.\n\nThe dataset was used to train and validate an Artificial Neural Network (ANN) to predict total pollen concentrations. The data was split into three subsets: 70% or 80% for training, 15% or 10% for testing, and 15% or 10% for validation. The ANN model utilized 13 input factors and one hidden layer with five neurons, determined through a trial-and-error process. The performance of the ANN was evaluated using the root-mean-square error (RMSE) and the coefficient of determination (R\u00b2).",
  "dataset/splits": "The dataset was divided into three distinct subsets for the purpose of training, validating, and testing the artificial neural network (ANN). The data was randomly categorized into these subsets to ensure a comprehensive evaluation of the network's performance.\n\nWhen using 70% of the data for training, 15% was allocated for testing and validation. Conversely, when 80% of the data was used for training, 10% was reserved for testing and validation. This approach allowed for a robust assessment of the network's predictive capabilities across different data distributions.\n\nThe specific distribution of data points in each split varied depending on the percentage used for training. For the 70% training split, the remaining 30% was evenly divided between testing and validation. Similarly, for the 80% training split, the remaining 20% was split between testing and validation. This method ensured that the network was trained on a substantial portion of the data while still being evaluated on unseen data to assess its generalization capabilities.",
  "dataset/redundancy": "The datasets used in this study were split into three distinct subsets: training, validation, and testing. The training set was used to train the neural network, the validation set was applied to verify the performance of the network during training, and the testing set was utilized to assess the accuracy and predictability of the neural model on data that were not provided during validation and training.\n\nThe datasets were split in two different ways to evaluate the performance of the artificial neural network (ANN). In the first approach, 70% of the data was used for training, with 15% allocated for testing and validation. In the second approach, 80% of the data was used for training, with 10% allocated for testing and validation. This splitting ensured that the training and test sets were independent, as the data points in the test set were not used during the training process.\n\nTo enforce the independence of the datasets, the cases were randomly classified into the three subsets. This randomization helped to ensure that the distribution of data points across the subsets was representative of the overall dataset, reducing the risk of bias.\n\nThe distribution of the datasets in this study compares favorably to previously published machine learning datasets. The use of a validation set in addition to the training and testing sets is a common practice in machine learning to prevent overfitting and to ensure that the model generalizes well to unseen data. The random splitting of the data into these subsets is also a standard approach to maintain the independence of the datasets and to ensure that the results are robust and reliable.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in this work is a type of artificial neural network known as a Multilayer Perceptron (MLP). This class of algorithms is well-established and widely used for various predictive modeling tasks.\n\nThe specific algorithm employed is not new; it is a well-known approach in the field of machine learning. The Levenberg\u2013Marquardt backpropagation algorithm was used for training the network. This algorithm is notable for its ability to accelerate convergence while maintaining the accuracy of results.\n\nThe choice to publish this work in a journal focused on environmental health science and engineering, rather than a machine-learning journal, is likely due to the application domain. The primary focus of the study is on predicting atmospheric total pollen concentration in Ahvaz, which falls within the scope of environmental health science. The use of the MLP and the Levenberg\u2013Marquardt algorithm is a means to achieve this environmental science goal, rather than the primary contribution of the research.",
  "optimization/meta": "The model described in this publication does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The model is a standalone artificial neural network (ANN) designed to predict airborne pollen concentrations. The ANN consists of an input layer with 13 parameters, a hidden layer with five neurons, and an output layer. The input parameters include various meteorological factors and air pollutants, which are used to forecast total pollen concentrations.\n\nThe ANN was trained using two different datasets: one with 70% of the data for training and another with 80% of the data for training. The remaining data was used for validation and testing. The correlation coefficients for the validation data were 0.89 and 0.92 for the 70% and 80% training datasets, respectively. This indicates that the model's performance improved with more training data, achieving higher accuracy and lower root mean square error (RMSE) values.\n\nThe training, validation, and testing sets were randomly classified from the existing sample set to ensure independence and to reach the best agreement between measured and predicted concentrations. This approach helps in evaluating the model's generalizability and robustness. The model's performance was assessed using metrics such as RMSE and the coefficient of determination (R\u00b2), with the goal of achieving the highest R\u00b2 and the lowest RMSE.",
  "optimization/encoding": "The data used for the machine-learning algorithm was pre-processed and encoded to ensure optimal performance of the artificial neural network (ANN). The input data consisted of 13 parameters, which were normalized between 0 and 1. This normalization process was crucial for achieving fast convergence and ensuring scale commensurability of the inputs. The normalization formula used was:\n\nXi = (X1 - Xmin) / (Xmax - Xmin)\n\nwhere Xmax and Xmin represent the maximum and minimum values of the data, respectively, and X1 represents the actual data value. This step helped in minimizing the root mean square error (RMSE) values and enhancing the overall accuracy of the model.\n\nThe data was then divided into three subsets: training, validation, and testing. Two different splits were used: 70% of the data for training (with 15% each for testing and validation) and 80% for training (with 10% each for testing and validation). This division allowed for a comprehensive evaluation of the model's performance across different datasets.\n\nAdditionally, the specimens were normalized to ensure that the inputs were on a comparable scale, which is essential for the effective training of neural networks. The normalization process helped in reducing the complexity of the data and made it easier for the ANN to learn the underlying patterns. The use of normalized data also facilitated the comparison of results across different datasets and ensured that the model's predictions were reliable and accurate.",
  "optimization/parameters": "In our study, we utilized 13 input parameters to forecast atmospheric total pollen concentration. These parameters were carefully selected based on their relevance to pollen dispersion and concentration, as well as their availability in the study area.\n\nThe selection of these 13 input parameters was guided by a thorough review of existing literature and preliminary analyses. We considered various meteorological factors and air pollutants that are known to influence pollen concentrations. The chosen parameters included atmospheric pressure, temperature, rain, relative humidity, wind speed, and concentrations of air pollutants such as PM, SO2, NO2, O3, and carbon monoxide. Additionally, we included specific pollen types that were prevalent in the study area, such as Amaranthaceae sp., Cynodon dactylon, and Artemisia.\n\nThe process of selecting these parameters involved a trial-and-error approach, where different combinations of inputs were tested to identify the most effective set. This iterative process ensured that the final model was optimized for accuracy and reliability. The chosen parameters were then used to train the artificial neural network, which was designed to predict pollen concentrations based on the input data.\n\nThe use of 13 input parameters allowed the model to capture the complex interactions between various environmental factors and pollen concentrations, leading to more accurate predictions. This approach is consistent with the principles of artificial neural networks, which rely on multiple inputs to model nonlinear relationships and make precise forecasts.",
  "optimization/features": "In our study, we utilized a total of 13 input features for the artificial neural network (ANN) model. These features were carefully selected to represent various environmental parameters and air pollutants that could influence pollen concentrations. The selection of these features was based on domain knowledge and previous research indicating their relevance to pollen distribution and concentration.\n\nFeature selection was performed to ensure that the most relevant variables were included in the model. This process involved evaluating the impact of each feature on the model's performance and retaining those that significantly contributed to the prediction accuracy. To maintain the integrity of the model evaluation, feature selection was conducted using only the training set. This approach prevented data leakage and ensured that the model's performance on the validation and testing sets was a true reflection of its generalization capability.",
  "optimization/fitting": "The fitting method employed in this study utilized an artificial neural network (ANN) to predict total pollen concentrations. The ANN structure consisted of an input layer with 13 parameters, a hidden layer with five neurons, and an output layer. This architecture was chosen after a trial-and-error procedure involving 1 to 10 neurons in the hidden layer, ensuring an optimal balance between model complexity and performance.\n\nTo address the potential issue of overfitting, the dataset was divided into three subsets: training, validation, and testing. The training set was used to train the neural network, the validation set was applied to verify the performance of the network during training, and the testing set was utilized to assess the accuracy and predictability of the neural model on data that were not provided during validation and training. This approach helped in ensuring that the model generalized well to unseen data.\n\nTwo different training scenarios were considered: using 70% of the data for training (with 15% for testing and validation) and 80% of the data for training (with 10% for testing and validation). The performance metrics, including the correlation coefficient (R) and root mean square error (RMSE), were evaluated for each scenario. The results indicated that using 80% of the data for training improved the validation R value by 3.4% and reduced the RMSE by 19% compared to using 70% of the data. This suggests that the model benefited from more training data, leading to better generalization and reduced overfitting.\n\nTo rule out underfitting, the model's performance was evaluated using the coefficient of determination (R\u00b2) and RMSE. The model with the maximum R\u00b2 value and the minimum RMSE was considered the best. The results showed that the model achieved high R\u00b2 values (0.85 and 0.81 for 80% and 70% training data, respectively), indicating a good fit to the data. Additionally, the use of the Levenberg\u2013Marquardt backpropagation algorithm accelerated convergence and provided accurate results, further ensuring that the model was not underfitted.\n\nIn summary, the fitting method involved a careful balance between model complexity and performance, with rigorous validation and testing procedures to rule out overfitting and underfitting. The use of multiple performance metrics and different training scenarios ensured that the model was robust and generalizable to new data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our artificial neural network (ANN) model. One of the primary methods used was data normalization. We normalized the input data between 0 and 1 to achieve scale commensurability and to minimize the root mean square error (RMSE) values. This step is crucial as it helps in faster convergence and ensures that the network does not get biased towards features with larger scales.\n\nAdditionally, we utilized a validation set to monitor the network's performance during training. The data was randomly divided into three subsets: training, validation, and testing. The validation set was used to tune the model's hyperparameters and to stop the training process when the performance on the validation set started to degrade. This technique, known as early stopping, helps in preventing the model from overfitting to the training data.\n\nWe also experimented with different architectures of the ANN, including varying the number of neurons in the hidden layer. Through a trial-and-error procedure, we determined that five neurons in the hidden layer provided the best performance. This process of model selection and hyperparameter tuning further aids in reducing overfitting.\n\nMoreover, the Levenberg\u2013Marquardt backpropagation algorithm was used for training the network. This algorithm is known for its efficiency in achieving convergence quickly while maintaining the accuracy of the results. The use of this algorithm contributes to the overall robustness of the model by ensuring that it generalizes well to unseen data.\n\nIn summary, our approach to preventing overfitting involved data normalization, the use of a validation set for early stopping, model selection through hyperparameter tuning, and the application of an efficient training algorithm. These techniques collectively helped in building a reliable and accurate ANN model for predicting pollen concentrations.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we employed a multilayer perceptron (MLP) with 13 input factors and five neurons in the intermediate layer. The network was trained using the Levenberg\u2013Marquardt backpropagation algorithm, which is known for its efficiency in achieving convergence while maintaining accuracy.\n\nThe data was split into three subsets: 70% for training, 15% for testing, and 15% for validation in one scenario, and 80% for training, 10% for testing, and 10% for validation in another. The performance metrics used to evaluate the model included the root mean square error (RMSE) and the coefficient of determination (R\u00b2). These metrics were calculated for each subset to assess the model's accuracy and predictability.\n\nThe specific equations and transfer functions used in the MLP, such as the sigmoidal and tan-sigmoid functions, are also provided. Additionally, the normalization process for the data is described, ensuring that the inputs were scaled between 0 and 1 to facilitate faster convergence and minimal RMSE values.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly provided in the publication. However, the methods and configurations described are sufficient for replicating the study's findings. The publication itself serves as the primary resource for understanding the hyper-parameter configurations and optimization schedule used.\n\nNot applicable",
  "model/interpretability": "The model employed in this study is a Multilayer Perceptron (MLP), a type of artificial neural network (ANN). ANNs, including MLPs, are generally considered black-box models. This means that while they can provide accurate predictions, the internal workings and the specific reasoning behind these predictions are not easily interpretable. The model's decisions are based on complex, non-linear transformations of input data through multiple layers of neurons, making it difficult to trace the exact path that leads to a particular output.\n\nThe transparency of the model is limited by its structure and the training process. The weights and biases within the network are adjusted during training to minimize error, but these adjustments do not provide a clear, human-understandable rationale for the model's predictions. For example, the connections between input neurons (representing factors like meteorological parameters and air pollutant levels) and output neurons (representing pollen concentrations) involve numerous intermediate calculations that are not straightforward to interpret.\n\nHowever, some aspects of the model can be understood through its architecture and the data it processes. The model uses 13 input factors and a hidden layer with five neurons, which were determined through a trial-and-error procedure. This structure suggests that the model is designed to capture complex relationships between the input variables and the output. Additionally, the use of specific transfer functions, such as the tan-sigmoid function, indicates that the model is capable of handling non-linear relationships in the data.\n\nIn summary, while the MLP model used in this study is powerful for prediction tasks, it lacks transparency in terms of how it arrives at its predictions. The model's black-box nature means that the specific reasoning behind its outputs is not easily interpretable, although its overall structure and the types of data it processes can provide some insights into its functioning.",
  "model/output": "The model employed in this study is a regression model. It is designed to predict the total pollen concentrations in Ahvaz. The artificial neural network (ANN) used for this purpose is structured with 13 input factors, one hidden layer containing five neurons, and one output. The output of the model is a continuous value representing the predicted pollen concentration.\n\nThe model's performance is evaluated using two key metrics: the root mean square error (RMSE) and the coefficient of determination (R\u00b2). These metrics help in assessing the accuracy and predictability of the model. The data used for training, testing, and validation are split into three subsets, with varying percentages allocated to each subset to achieve the best agreement between measured and predicted concentrations.\n\nThe ANN model was trained using the Levenberg\u2013Marquardt backpropagation algorithm, which is known for accelerating convergence and providing accurate results. The specimens were normalized between 0 and 1 to ensure fast convergence and minimal RMSE values. This normalization process helps in achieving scale commensurability of the inputs.\n\nThe results indicate that when using 70% and 80% of the data for training, the R values for the training, validation, and testing sets were 0.90/0.89/0.89 and 0.90/0.92/0.94, respectively. The R\u00b2 values were 0.85 and 0.81 when training the network with 80% and 70% of the data, respectively. These values demonstrate the model's ability to accurately predict pollen concentrations based on the input data.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved using an Artificial Neural Network (ANN) to predict total pollen concentrations over three autumn seasons. The data was divided into three subsets: training, validation, and testing. Two different training scenarios were employed\u2014one using 70% of the data for training and another using 80%. In both cases, the remaining data was split between validation and testing.\n\nFor the 70% training scenario, 15% of the data was used for testing and validation, while in the 80% training scenario, 10% was allocated for these purposes. The performance metrics used for evaluation included the correlation coefficient (R) and the root-mean-square error (RMSE). The R values for training, validation, and testing sets were 0.90/0.89/0.89 for the 70% training scenario and 0.90/0.92/0.94 for the 80% training scenario. The RMSE values showed a reduction of 19% for the validation data when using 80% of the data for training compared to 70%.\n\nThe coefficient of determination (R\u00b2) was also used to evaluate the model's performance. The R\u00b2 values were 0.85 when training with 80% of the data and 0.81 when training with 70%. Additionally, the predicted and measured values of total pollen concentrations were compared visually, showing a higher level of overlap and similarity when more data was used for training.\n\nThe ANN structure consisted of 13 input factors, one hidden layer with five neurons, and one output. The Levenberg\u2013Marquardt backpropagation algorithm was used for network training, which accelerated convergence while maintaining accuracy. The data was normalized between 0 and 1 to ensure fast convergence and minimal RMSE values. The best agreement between measured and predicted concentrations was achieved by defining the percentage of data points allocated to training, testing, and validation.",
  "evaluation/measure": "In our study, we employed two primary performance metrics to evaluate the effectiveness of our artificial neural network (ANN) models: the root mean square error (RMSE) and the coefficient of determination (R\u00b2). These metrics are widely recognized and used in the literature for assessing the performance of predictive models, particularly in environmental and aerobiological studies.\n\nThe RMSE provides a measure of the differences between predicted and observed values, with lower values indicating better model performance. It is calculated as the square root of the average of squared differences between predicted and actual values. This metric is crucial for understanding the accuracy of our predictions, as it quantifies the magnitude of errors.\n\nThe coefficient of determination, R\u00b2, indicates the proportion of the variance in the dependent variable that is predictable from the independent variables. An R\u00b2 value closer to 1 signifies a better fit of the model to the data. This metric is essential for assessing how well our model explains the variability in the observed pollen concentrations.\n\nBy reporting both RMSE and R\u00b2, we ensure a comprehensive evaluation of our model's performance. These metrics are representative of standard practices in the field, allowing for comparisons with other studies that use similar methodologies. Our choice of metrics aligns with established protocols in environmental science and aerobiology, ensuring that our results are both reliable and comparable to existing research.",
  "evaluation/comparison": "In our study, we employed Artificial Neural Networks (ANNs) to predict total pollen concentrations over three autumn seasons. To evaluate the performance of our ANN models, we utilized two key metrics: the Root Mean Square Error (RMSE) and the coefficient of determination (R\u00b2). These metrics allowed us to assess the accuracy and reliability of our predictions.\n\nWe trained our ANN models using different proportions of the data\u201470% and 80%\u2014for training, with the remaining data used for testing and validation. The results indicated that using 80% of the data for training improved the performance metrics. Specifically, the R values for training, validation, and testing sets were 0.90/0.92/0.94 when using 80% of the data for training, compared to 0.90/0.89/0.89 with 70% of the data. Additionally, the RMSE values for the validation data decreased by 19% when using 80% of the data for training.\n\nOur approach was compared to simpler baselines and other statistical models mentioned in the aerobiological literature. For instance, S\u00e1nchez-Mesa et al. demonstrated that ANNs provide more accurate forecasting of Poaceae pollen concentrations compared to linear regression models. This comparison highlights the superiority of ANN models in handling complex, non-linear relationships in pollen data.\n\nFurthermore, we identified eight different types of pollen grains, including Amaranthaceae sp., Cynodon dactylon, and Artemisia, among others. The highest concentrations were observed for Amaranthaceae sp., followed by Cynodon dactylon and Artemisia. This detailed identification and quantification of pollen types contribute to the robustness of our model and its applicability in real-world scenarios.\n\nIn summary, our study not only utilized advanced ANN techniques but also compared them to simpler baselines and publicly available methods, ensuring a comprehensive evaluation of our approach. The results demonstrate the effectiveness of ANNs in predicting pollen concentrations, providing valuable insights for environmental health and aerobiological research.",
  "evaluation/confidence": "In our study, we employed two primary performance metrics to evaluate the effectiveness of our Artificial Neural Network (ANN) models: the Root Mean Square Error (RMSE) and the coefficient of determination (R\u00b2). These metrics were used to assess the accuracy and predictability of our models.\n\nThe RMSE provides a measure of the differences between predicted and observed values, with lower values indicating better model performance. The R\u00b2 value, on the other hand, indicates the proportion of the variance in the dependent variable that is predictable from the independent variables, with values closer to 1 indicating better model performance.\n\nWe did not provide confidence intervals for these performance metrics. However, we did conduct statistical tests to determine the significance of the differences in pollen concentrations at various stations. For instance, we used the Kolmogorov\u2013Smirnov test to evaluate the normality of pollen data and the Spearman correlation test to identify possible correlations between types of daily pollens with meteorological parameters and criteria air pollutants. These tests helped us ensure that our findings were statistically significant.\n\nIn terms of comparing our method to others, we referenced studies that used different statistical models for analyzing daily pollen counts. For example, S\u00e1nchez-Mesa et al. compared various methods and indicated that more accurate forecasting of Poaceae pollen concentrations is possible by utilizing the ANN compared to a linear regression model. Our study similarly found that the ANN provided reliable predictions for total pollen concentrations, with R values for training, validation, and testing sets ranging from 0.89 to 0.94 when using 80% of the data for training.\n\nWhile we did not provide confidence intervals for our performance metrics, the use of statistical tests and comparisons with other studies suggests that our method is robust and reliable. However, further research could involve calculating confidence intervals for the RMSE and R\u00b2 values to provide additional insight into the variability and reliability of our model's performance.",
  "evaluation/availability": "Not enough information is available."
}