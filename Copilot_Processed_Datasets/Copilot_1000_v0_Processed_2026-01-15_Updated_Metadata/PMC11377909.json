{
  "publication/title": "Detection of Mild Cognitive Impairment From Non-Semantic, Acoustic Voice Features: The Framingham Heart Study.",
  "publication/authors": "Ding H, Lister A, Karjadi C, Au R, Lin H, Bischoff B, Hwang PH",
  "publication/journal": "JMIR aging",
  "publication/year": "2024",
  "publication/pmid": "39173144",
  "publication/pmcid": "PMC11377909",
  "publication/doi": "10.2196/55126",
  "publication/tags": "- Mild Cognitive Impairment\n- Acoustic Features\n- Speech Analysis\n- Machine Learning\n- Cognitive Assessment\n- Alzheimer Disease\n- Dementia Detection\n- Voice Recordings\n- Feature Extraction\n- Predictive Modeling",
  "dataset/provenance": "The dataset utilized in this study originates from the Framingham Heart Study (FHS), a longstanding community-based, longitudinal cohort study initiated in 1948. The specific data used in this research includes audio recordings collected from FHS participants who were aged 60 years or older at the time of their neuropsychological (NP) exam visits. Initially, the study included 605 participants with at least one audio recording. From this group, a case-control dataset was created, consisting of 100 individuals diagnosed with mild cognitive impairment (MCI) and 100 cognitively normal (CN) controls. These participants were matched based on age, sex, and education to ensure the reliability of the study results.\n\nThe voice recordings were collected between September 2005 and March 2020, encompassing spoken interactions during NP tests. These recordings were stored in .wav format and downsampled to 16 kHz. The earliest collected voice recording from each participant was included in the analysis. The study population was stratified into six age groups, each spanning a 5-year interval from 60 to 89 years, with an additional category for individuals aged 90 years and older. Participants were also categorized into four education groups: high school non-graduates, high school graduates, individuals with some college education, and college graduates.\n\nThe data used in this study has not been previously analyzed in the same manner, focusing on the utility of acoustic features derived from human speech for the identification of MCI and assessing the impact of the duration of voice recordings on predictive performance. The comprehensive speech data collected during NP exams provides a robust foundation for this research, contributing to the scientific understanding and practical implications for early detection of cognitive impairment.",
  "dataset/splits": "The study utilized a case-control dataset consisting of 200 participants, evenly split between those diagnosed with mild cognitive impairment (MCI) and cognitively normal (CN) controls. The participants were matched on age, sex, and education to ensure the reliability of the study results. The dataset was derived from a larger pool of 605 participants from the Framingham Heart Study (FHS), who were aged 60 years or older at the time of the neuropsychological (NP) exam visit where the recordings were collected.\n\nThe dataset was stratified into specific groups to control for potential confounders. Participants were divided into six age groups, each spanning a 5-year interval from 60 to 89 years, with an additional category for individuals aged 90 years and older. Education was categorized into four groups: high school non-graduates, high school graduates, individuals with some college education, and college graduates. Controls were selected to match the cases based on these stratifications.\n\nThe earliest collected voice recording from each participant was included in the analysis. This resulted in a balanced dataset with 100 MCI cases and 100 CN controls, ensuring an equal distribution of data points in each group. The study did not involve multiple data splits beyond this case-control matching process.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the random forest model. This is a well-established ensemble learning method known for its robustness and ability to handle high-dimensional data.\n\nThe random forest algorithm is not new; it has been extensively used and validated in various fields, including bioinformatics and medical research. The decision to use this algorithm was driven by its proven effectiveness in feature selection and classification tasks, particularly in scenarios involving complex and high-dimensional datasets.\n\nGiven that the primary focus of our research is on the detection of mild cognitive impairment (MCI) from acoustic voice features, we chose to publish our findings in a journal that aligns with our study's domain\u2014aging and cognitive health. The random forest model served as a reliable tool to achieve our objectives, and its application in this context is well-documented and accepted within the scientific community. Therefore, publishing in a machine-learning-specific journal was not necessary for the validation or novelty of the algorithm itself, but rather for the innovative application of these techniques to a critical health issue.",
  "optimization/meta": "The meta-predictor in our study does not use data from other machine-learning algorithms as input. Instead, it relies on a comprehensive set of acoustic features extracted from voice recordings. These features are derived using OpenSMILE and Praat software, which process the audio data to identify segments of each participant's voice.\n\nThe machine learning pipeline developed for this study incorporates several key steps: speaker diarization, feature extraction, feature selection, and classification. The random forest model, which is the primary classification method used, is constructed to classify cognitive status based on the features that exhibit significant differences between the mild cognitive impairment (MCI) and cognitively normal groups.\n\nRegarding the independence of the training data, it is important to note that the study used a community-based sample within a controlled environment for the voice recordings taken during neuropsychological exams. This approach helps ensure that the data is collected in a consistent and standardized manner, reducing the likelihood of biases that could affect the model's performance.\n\nThe study also highlights the importance of using highly interpretable methods throughout the pipeline, from feature selection to predictive model construction. This transparency is crucial for achieving good MCI prediction capability and sets a benchmark for future research attempting more complex analytical approaches.\n\nIn summary, the meta-predictor in our study is designed to leverage acoustic features directly from voice recordings, without relying on data from other machine-learning algorithms. The training data is collected in a controlled environment, ensuring independence and consistency, which is essential for the model's reliability and performance.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, voice recordings were collected and stored in .wav format, downsampled to 16 kHz. These recordings were then processed using speaker diarization to distinguish between the participant and the tester, ensuring that only the participant's speech was analyzed. This process involved segmenting the recordings based on the speaker's identity using the open-source package pyannote.\n\nFollowing speaker diarization, feature extraction was performed using OpenSMILE and Praat software. OpenSMILE facilitated the extraction of a comprehensive set of 6376 features, including energy, spectral, cepstral, and voicing-related descriptors. Each recording was divided into 20-millisecond segments with a 10-millisecond overlap, allowing for the conservation of information continuity. First-order delta regression coefficients were calculated for all low-level descriptors, and various functionals such as mean, maximum, minimum, standard deviation, and linear regression slope were applied to extract statistical characteristics from these descriptors over the full recordings.\n\nPraat software was used to generate an additional 9 features related to syllable nuclei and fill pauses. This resulted in a total of 6385 acoustic features extracted from the voice recordings.\n\nTo preprocess the data for the machine-learning algorithm, z scores were computed for each feature, and those with an absolute z score greater than 2 were removed as outliers. T-tests were then used to determine significant differences in each feature between the mild cognitive impairment (MCI) and cognitively normal (CN) groups. Features with a P value below 0.002 were selected for inclusion in the model.\n\nThe final set of 29 selected features was used to build a random forest model, which was evaluated using 10-fold cross-validation. This preprocessing and encoding ensured that the most relevant and significant acoustic features were used to train the machine-learning algorithm for MCI detection.",
  "optimization/parameters": "In our study, we utilized a set of 29 acoustic features for the optimal model, which were selected based on a P value threshold of .002. These features were chosen from an initial pool of 6376 features extracted using OpenSMILE software and 9 features extracted using Praat software. The selection process involved computing z scores for each feature and removing outliers with an absolute z score greater than 2. Subsequently, t tests were conducted to identify features with significant differences between the mild cognitive impairment (MCI) and cognitively normal (CN) groups. Only features with a P value less than .002 were included in the final model.\n\nThe number of parameters, p, was determined through a rigorous feature selection process aimed at identifying the most relevant acoustic features for MCI classification. This approach ensured that the model was both efficient and effective, balancing the need for a comprehensive feature set with the requirement for computational feasibility. The selected features were then used to build a random forest model, which was evaluated using 10-fold cross-validation to assess its performance in detecting MCI.",
  "optimization/features": "In our study, we initially extracted a comprehensive set of 6376 features using OpenSMILE software and 9 features using Praat software from the voice recordings. To ensure the robustness of our model, feature selection was performed. We computed z scores for each feature and removed those with an absolute z score greater than 2, as they were considered outliers. Subsequently, t tests were employed to identify features that exhibited a significant difference between the mild cognitive impairment (MCI) and cognitively normal (CN) groups, with a P value threshold of .002. This process resulted in a final set of 29 acoustic features that were significantly associated with cognitive status. These 29 features were used as input for our classification model. The feature selection process was conducted using the training set only, ensuring that the model's performance was evaluated on unseen data.",
  "optimization/fitting": "The fitting method employed in this study involved a random forest model, which is known for its robustness and ability to handle a large number of features relative to the number of training samples. The initial feature set consisted of 6376 features extracted using OpenSMILE software and 9 features from Praat software, resulting in a total of 6385 features. This number is indeed much larger than the number of training points, which consisted of 200 participants (100 with mild cognitive impairment (MCI) and 100 cognitively normal (CN)).\n\nTo address the potential issue of overfitting, several steps were taken. First, feature selection was performed to reduce the dimensionality of the data. Z-scores were computed for each feature, and those with an absolute z-score greater than 2 were removed as outliers. Then, t-tests were used to identify features that exhibited a significant difference between the MCI and CN groups, with a P-value threshold of 0.002. This process resulted in a final set of 29 selected features that were used to build the random forest model.\n\nAdditionally, the model's performance was evaluated using 10-fold cross-validation, which helps to ensure that the model generalizes well to unseen data. The area under the receiver operating characteristic curve (AUC) was obtained for the random forest algorithm, along with the 95% confidence interval, providing a measure of the model's predictive performance and stability.\n\nTo rule out underfitting, the model's performance was assessed using various audio recording lengths (5, 10, 15, and 30 minutes), and the results were compared to the performance using the full recording (over 1 hour). The model achieved high AUC values across different recording lengths, indicating that it was able to capture the relevant patterns in the data without being too simplistic.\n\nFurthermore, the importance of each feature was computed using an impurity-based approach, which helps to identify the most relevant features for the classification task. This approach ensures that the model is not relying on noise or irrelevant features, further mitigating the risk of underfitting.\n\nIn summary, the fitting method involved a combination of feature selection, cross-validation, and feature importance analysis to address the potential issues of overfitting and underfitting. The random forest model demonstrated good predictive performance and generalization to unseen data, as evidenced by the high AUC values and the stability of the results across different recording lengths.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One of the key methods used was feature selection. We computed z-scores for each feature and removed those with an absolute z-score greater than 2, as these were considered outliers. This step helped in reducing the dimensionality of the data and focusing on the most relevant features.\n\nAdditionally, we used t-tests to determine significant differences in features between the mild cognitive impairment (MCI) and cognitively normal (CN) groups. Only features with a P-value below 0.002 were selected for the model, further refining the feature set and reducing the risk of overfitting.\n\nWe also utilized 10-fold cross-validation to evaluate the performance of our random forest model. This technique involves dividing the data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. Cross-validation helps in assessing the model's performance on unseen data and ensures that it generalizes well to new samples.\n\nMoreover, the random forest algorithm itself is an ensemble method that inherently reduces overfitting by averaging the predictions of multiple decision trees. Each tree is trained on a different bootstrap sample of the data, and the final prediction is made by aggregating the results from all trees. This approach enhances the model's stability and robustness.\n\nIn summary, our study incorporated feature selection, statistical significance testing, cross-validation, and the use of a random forest algorithm to prevent overfitting and ensure the reliability of our MCI detection model.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model employed in this study is designed with a strong emphasis on interpretability, ensuring that it is not a black box. We utilized highly interpretable methods throughout the entire process, from feature selection to predictive model construction. This approach allows for a clear understanding of how the model makes its predictions.\n\nOne of the key aspects of our model's transparency is the use of a random forest algorithm, which inherently provides insights into feature importance. Each feature's contribution to the model's predictions is quantified, allowing us to identify the most significant acoustic features for mild cognitive impairment (MCI) detection. For instance, the number of filled pauses emerged as the most important feature, highlighting its critical role in distinguishing between MCI cases and cognitively normal controls.\n\nAdditionally, we employed statistical tests to select features that showed significant differences between the two groups. Only features with a P-value less than 0.002 were included in the model, ensuring that the selected features are robust and meaningful. This rigorous feature selection process enhances the model's interpretability by focusing on the most relevant acoustic characteristics.\n\nFurthermore, the model's performance was evaluated using various audio recording lengths, demonstrating that even shorter recordings can yield reliable results. This finding not only underscores the model's efficiency but also its transparency, as it shows that the predictive power is consistent across different durations.\n\nIn summary, the model's design and the methods used ensure that it is transparent and interpretable. The use of random forests, statistical feature selection, and consistent performance across different recording lengths all contribute to a clear understanding of how the model operates and what factors drive its predictions.",
  "model/output": "The model developed in this study is a classification model. Specifically, it is designed to classify individuals as having mild cognitive impairment (MCI) or being cognitively normal (CN). The model utilizes a random forest algorithm, which is a popular method for classification tasks. The performance of the model was evaluated using the area under the receiver operating characteristic curve (AUC), along with the 95% confidence interval (CI). The optimal model achieved an AUC of 0.87 (95% CI 0.81-0.94), indicating strong discriminative ability between the two cognitive statuses.\n\nThe model was built using a final set of 29 selected acoustic features, which were identified through a rigorous feature selection process. This process involved computing z scores for each feature, removing outliers, and performing t tests to determine significant differences between the MCI and CN groups. Features with a P value below 0.002 were included in the model. The importance of each feature was computed using an impurity-based approach, with higher scores indicating greater importance in the classification task.\n\nThe study also investigated the impact of audio recording lengths on model performance. Various audio segments (5, 10, 15, and 30 minutes) were analyzed, and the model demonstrated consistent performance across different durations. This finding suggests that shorter recording lengths can be used without significantly compromising the model's predictive accuracy, which has important implications for minimizing participant burden and data collection time.\n\nIn summary, the model is a classification model that effectively distinguishes between individuals with MCI and those who are cognitively normal using a set of acoustic features derived from voice recordings. The model's performance was robust across different recording lengths, highlighting its practical utility in real-world applications.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a robust approach to ensure the reliability and generalizability of the findings. A random forest model was constructed using a final set of 29 selected features, and its performance was assessed through 10-fold cross-validation. This technique helps in mitigating overfitting and provides a more accurate estimate of the model's performance on unseen data.\n\nTo evaluate the model's effectiveness in detecting mild cognitive impairment (MCI), the area under the receiver operating characteristic curve (AUC) was computed, along with the 95% confidence interval. The AUC is a critical metric as it summarizes the model's ability to distinguish between MCI and cognitively normal (CN) individuals across all possible classification thresholds.\n\nAdditionally, the importance of each feature was determined using an impurity-based approach. This method helps in identifying which acoustic features contribute most significantly to the model's predictive power, providing insights into the underlying mechanisms of cognitive impairment as reflected in speech patterns.\n\nTo further investigate the impact of audio recording length on model performance, segments of 5, 10, 15, and 30 minutes were extracted from the full recordings. The same processing steps, including speaker diarization, feature extraction, and model construction, were applied to these segments. This analysis revealed that even shorter recording durations could yield comparable model performance, suggesting that lengthy audio data may not be necessary for effective MCI detection. This finding has practical implications for reducing participant burden and optimizing data collection processes.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our random forest model in detecting mild cognitive impairment (MCI) using acoustic features. The primary metric reported is the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve, which provides a comprehensive measure of the model's ability to distinguish between individuals with MCI and those who are cognitively normal (CN). The AUC values for different audio recording lengths were reported, with the full recording yielding the highest AUC of 0.87. For shorter recording lengths, the AUC values ranged from 0.79 to 0.82, indicating robust performance even with reduced data collection time.\n\nIn addition to the AUC, we also computed and reported the accuracy, sensitivity, and specificity of our models. These metrics were derived using 10-fold cross-validation to ensure the reliability and generalizability of our results. Accuracy measures the overall correctness of the model's predictions, sensitivity (or recall) assesses the model's ability to correctly identify individuals with MCI, and specificity evaluates the model's ability to correctly identify cognitively normal individuals. These metrics collectively provide a thorough evaluation of the model's performance and are consistent with standard practices in the literature for assessing classification models in healthcare applications.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Our focus was primarily on developing and validating our own machine learning pipeline for detecting mild cognitive impairment (MCI) using acoustic features from voice recordings. We constructed a random forest model to classify cognitive status based on features that showed significant differences between MCI cases and cognitively normal controls.\n\nHowever, we did explore the impact of different audio recording lengths on model performance. We examined the MCI detection capabilities using audio segments of varying durations\u20145, 10, 15, and 30 minutes\u2014and compared these to the performance using full recordings. This approach allowed us to assess the robustness and efficiency of our model under different conditions.\n\nRegarding simpler baselines, our study did not explicitly compare our method to simpler baselines. Instead, we concentrated on identifying a set of acoustic features that exhibited strong MCI detection capability. The random forest model we developed was chosen for its ability to handle a large number of features and provide feature importance scores, which helped us understand the contributions of individual acoustic features to the classification task.\n\nIn summary, while we did not conduct a direct comparison with publicly available methods or simpler baselines, our study provides valuable insights into the potential of using acoustic features from voice recordings for MCI detection. Future research could build on these findings by comparing our approach to other methods and exploring simpler baselines to further validate and refine the use of acoustic features in cognitive assessment.",
  "evaluation/confidence": "The evaluation of our model's performance includes confidence intervals for the area under the receiver operating characteristic curve (AUC). For instance, the optimal model based on whole recordings achieved an AUC of 0.87 with a 95% confidence interval (CI) of 0.81-0.94. This provides a range within which the true AUC is likely to fall, giving an indication of the uncertainty around the point estimate.\n\nFor different audio recording lengths, the AUCs and their corresponding 95% CIs were also reported. For 5-minute segments, the AUC was 0.79 (95% CI 0.73-0.86). For 10-minute segments, the AUC was 0.81 (95% CI 0.75-0.87). For 15-minute segments, the AUC was 0.80 (95% CI 0.75-0.86). For 30-minute segments, the AUC was 0.82 (95% CI 0.76-0.89). These intervals help to assess the reliability of the AUC estimates.\n\nThe statistical significance of the acoustic features was determined using a P value threshold of 0.002. Features with P values below this threshold were considered significantly associated with cognitive status and were included in the model. This stringent threshold ensures that the features selected are unlikely to be due to random chance, thereby increasing the confidence in the model's performance.\n\nThe importance of each feature was computed using an impurity-based approach, which provides a measure of how much each feature contributes to the predictive power of the model. This method helps to identify the most relevant acoustic features for mild cognitive impairment (MCI) detection, further supporting the robustness of the model.\n\nOverall, the inclusion of confidence intervals and the use of a stringent P value threshold enhance the confidence in the reported performance metrics and the superiority of the method over potential baselines.",
  "evaluation/availability": "Not enough information is available."
}