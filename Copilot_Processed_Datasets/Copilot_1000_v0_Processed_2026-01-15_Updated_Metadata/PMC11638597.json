{
  "publication/title": "Utilizing artificial intelligence for the detection of hemarthrosis in hemophilia using point-of-care ultrasonography.",
  "publication/authors": "Tyrrell PN, Alvarez-Rom\u00e1n MT, Bakeer N, Brand-Staufer B, Jim\u00e9nez-Yuste V, Kras S, Martinoli C, Mendez M, Nagao A, Ozelo M, Ricciardi JBS, Zak M, Roth J",
  "publication/journal": "Research and practice in thrombosis and haemostasis",
  "publication/year": "2024",
  "publication/pmid": "39677374",
  "publication/pmcid": "PMC11638597",
  "publication/doi": "10.1016/j.rpth.2024.102602",
  "publication/tags": "- Hemophilia\n- Joint bleeds\n- Ultrasound imaging\n- Artificial intelligence\n- Synovial recess distension\n- Hemarthrosis\n- Point-of-care ultrasonography\n- AI-assisted diagnosis\n- Medical imaging\n- Patient quality of life",
  "dataset/provenance": "The dataset used in this study was sourced from ultrasound (US) images procured from four distinct patient populations: healthy participants with no synovial recess distension (SRD), healthy participants with incidental findings of SRD, persons with hemophilia with no hemarthrosis, and persons with hemophilia with a high clinical suspicion of hemarthrosis. These populations were further divided into adult and pediatric cohorts, resulting in four main cohorts: healthy adult participants, adult persons with hemophilia, healthy pediatric participants, and pediatric persons with hemophilia.\n\nThe overall dataset comprised a total of 10,439 adult joint exams, which included 56,276 US images, and 1,706 pediatric joint exams, which included 5,225 US images. After quality assessment, a total of 9,842 exams (from 52,350 US images) for the knee, 886 exams (from 3,120 US images) for the elbow, and 1,417 exams (from 6,031 US images) for the ankle were used. These images were captured according to the Hemophilia Early Arthropathy Detection with Ultrasound (HEAD-US) protocol, which aimed to demonstrate midsagittal, longitudinal images of specific joint recesses.\n\nThe exams were obtained from seven healthcare centers across the world, with each center obtaining ethics approval for the study. Consent, either opt-out or informed, was obtained from all participants prior to study inclusion. The US images were labeled by extraction from radiologic reports for negative cases or via assessment by two experts in sonography with extensive clinical experience in musculoskeletal US. The experts were blinded to the patients\u2019 clinical information to minimize potential bias.\n\nThe dataset was balanced based on the presence of SRD across all four cohorts, and the test set, which was used to evaluate the performance of the algorithms, was randomly selected separately for each data cohort to ensure representation of all patient populations. The images were assigned to either the training or test set to avoid data leakage. The dataset included images from both acute and subacute hemarthrosis cases, and a small amount of intra-articular fluid was considered normal in asymptomatic patients and healthy participants.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a test set. The test set, which was used to evaluate the performance of the algorithms, comprised 20% of the samples. This split was done randomly and separately for each data cohort to ensure representation of all patient populations in the evaluation.\n\nThe overall dataset included 10,439 adult joint exams (56,276 US images) and 1706 pediatric joint exams (5225 US images). After quality assessment, a total of 9842 exams (from 52,350 US images) for the knee, 886 exams (from 3120 US images) for the elbow, and 1417 exams (from 6031 US images) for the ankle were used.\n\nThe case:control ratio for all cohorts was balanced based on the presence of synovial recess distension (SRD). All images in a study were assigned to either the training set or the test set to avoid data leakage.",
  "dataset/redundancy": "The datasets were split into training and test sets to ensure independent evaluation of the models. The test set, which comprised 20% of the samples, was randomly selected separately for each data cohort. This approach ensured that all patient populations were represented in the evaluation of the AI algorithms. To prevent data leakage, all images from a single study were assigned to either the training set or the test set, but not both. This method helped maintain the independence of the datasets and ensured that the models were evaluated on unseen data.\n\nThe distribution of the datasets in this study is notable for its balance and comprehensiveness. The overall dataset included a total of 10,439 adult joint exams and 1706 pediatric joint exams, with a focus on knee, elbow, and ankle joints. After quality assessment, 9842 exams for the knee, 886 exams for the elbow, and 1417 exams for the ankle were used. This large and diverse dataset allowed for robust training and validation of the models.\n\nCompared to previously published machine learning datasets in similar domains, this dataset stands out due to its size and the inclusion of both adult and pediatric populations. The balanced case:control ratio for all cohorts further enhances the reliability of the models' performance metrics. The use of high-quality ultrasound images captured according to a standardized protocol also ensures consistency and comparability across different patient groups and healthcare centers.",
  "dataset/availability": "Not applicable.",
  "optimization/algorithm": "The machine-learning algorithm class used is a binary convolutional neural network (BCNN) classifier. This type of algorithm is well-established in the field of computer vision and image classification. The BCNN classifier was developed to detect synovial-related disease (SRD) in ultrasound images of knee, elbow, and ankle joints.\n\nThe BCNN classifier is not a new algorithm. It is based on the EfficientNet-B4 architecture, which was developed by Google Brain. EfficientNet-B4 is known for its efficiency and effectiveness in image classification tasks, making it a suitable choice for this application.\n\nThe reason the BCNN classifier was not published in a machine-learning journal is that the focus of this study is on its application in medical imaging, specifically for detecting SRD in ultrasound images. The development and optimization of the BCNN classifier were tailored to this specific medical task, rather than being a general contribution to the field of machine learning. The study aims to demonstrate the feasibility and high performance of AI algorithms in detecting and segmenting SRD in ultrasound images, which is a significant contribution to the field of medical imaging and hemophilia care.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, ultrasound (US) images were collected from diverse patient populations, including healthy individuals and those with hemophilia, across different age groups and joints. These images were obtained from multiple healthcare centers worldwide, ensuring a broad and representative dataset.\n\nThe images were labeled by extracting information from radiologic reports for negative cases or through assessments by experts in sonography. These experts were blinded to the patients' clinical information to minimize bias. The labeled images were then used to create a high-quality dataset for training and validating the AI algorithms. The case-control ratio was balanced based on the presence of synovial-related disease (SRD) to ensure an unbiased training process.\n\nData augmentation techniques such as rotation, scaling, and flipping were applied to increase the diversity of the training set and improve the model's generalization capability. This process helped in making the model robust to variations in the input data.\n\nThe images were captured according to the Hemophilia Early Arthropathy Detection with Ultrasound (HEAD-US) protocol, which standardized the imaging process to ensure consistency. The dataset was split into training and test sets, with the test set comprising 20% of the samples to evaluate the performance of the algorithms. This splitting ensured that all patient populations were represented in the evaluation.\n\nFor the segmentation models, images from adult and pediatric patients were combined into a single training dataset per joint. This approach leveraged the similarities in US characteristics across different age groups to enhance the model's performance.\n\nOverall, the data encoding and preprocessing steps were designed to create a comprehensive and balanced dataset that could effectively train the machine-learning algorithms to detect and segment SRD in various joints and patient populations.",
  "optimization/parameters": "The model architecture utilized for synovial recess distension (SRD) detection was based on EfficientNet-B4, which is known for its efficient use of parameters while maintaining high performance. The specific number of parameters (p) in EfficientNet-B4 is approximately 17 million. This architecture was chosen for its balance between computational efficiency and accuracy, making it suitable for the complex task of medical image analysis.\n\nThe selection of EfficientNet-B4 was driven by its proven effectiveness in various image classification tasks and its ability to generalize well to new datasets. The model's hyperparameters, including the learning rate, batch size, and optimizer settings, were tuned using cross-validation to ensure optimal performance. The final configuration was selected based on its performance on a validation set, which helped in fine-tuning the model to achieve high accuracy and reliability in detecting SRD.\n\nAdditionally, data augmentation techniques such as rotation, scaling, and flipping were employed to increase the diversity of the training set, further enhancing the model's generalization capability. The use of a pretrained state from the ImageNet dataset also contributed to the model's robustness, as it allowed the model to leverage features learned from a large and diverse dataset of natural images.",
  "optimization/features": "Not applicable.",
  "optimization/fitting": "The model architecture employed was based on EfficientNet-B4, which is known for its depth and complexity, thus having a large number of parameters relative to the number of training points. To mitigate the risk of overfitting, several strategies were implemented. Data augmentation techniques such as rotation, scaling, and flipping were used to increase the diversity of the training set, thereby improving the model's generalization capability. Additionally, hyperparameter tuning was performed using cross-validation, and the final configuration was selected based on its performance on a validation set. The model was trained for 50 epochs with a learning rate that decreased over time, which helped in fine-tuning the model parameters without overfitting. The use of binary cross-entropy loss and the Adam optimizer further aided in stabilizing the training process. The model training was initialized from a pretrained state using the ImageNet dataset, which provided a robust starting point and helped in faster convergence. Two GeForce RTX 2080 Ti graphics processing units were used for model training, ensuring efficient computation and reducing the risk of overfitting by allowing for larger batch sizes and more extensive training.\n\nTo rule out underfitting, the model's performance was closely monitored during training. The use of a validation set allowed for the assessment of the model's ability to generalize to unseen data. The model demonstrated robustness and showed no signs of overfitting, achieving convergence approximately at the 40th epoch. The high performance metrics, including an accuracy of 97%, a sensitivity of 96%, a specificity of 97%, and an AUC of 0.97 for the knee joint model, indicated that the model was well-fitted to the data without being overly complex. The consistent performance across different patient cohorts further supported the model's generalizability and ruled out underfitting.",
  "optimization/regularization": "The training process for the models incorporated several techniques to prevent overfitting. Data augmentation methods such as rotation, scaling, and flipping were employed to increase the diversity of the training set, which helped improve the model's generalization capability. Additionally, cross-validation was used to tune the model's hyperparameters, ensuring that the final configuration was selected based on its performance on a validation set. The models were also initialized from a pretrained state using the ImageNet dataset, which provided a robust starting point for training. Furthermore, the models demonstrated convergence around the 40th epoch, indicating effective regularization and preventing overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are detailed within the publication. Specifically, for the BCNN classifier, we utilized an EfficientNet-B4 architecture trained for 50 epochs with a learning rate of 0.001, binary cross-entropy loss, and the Adam optimizer. The batch size was set to 64 images, and the model was initialized from a pretrained state using the ImageNet dataset. For the segmentation models, we employed a DeepLab V3 architecture with a ResNet101 encoder, trained for 150 epochs with a learning rate of 0.0001, binary cross-entropy loss, and the Adam optimizer.\n\nThe exact model files and optimization parameters are not directly provided in the publication but can be inferred from the described configurations. The publication does not specify the availability of these files or the license under which they might be shared. However, the detailed descriptions of the training processes and hyper-parameters should allow for replication of the models by interested researchers.",
  "model/interpretability": "The models developed in this study are primarily black-box models, meaning their internal workings are not easily interpretable. The BCNN classifiers and segmentation models are based on deep learning architectures, such as EfficientNet-B4 and DeepLab V3 with a ResNet101 encoder, which are known for their complexity and lack of transparency. These models learn intricate patterns from the data but do not provide clear, human-understandable explanations for their predictions.\n\nHowever, the models' performance metrics and the results of the 5-fold cross-validation provide some level of interpretability. For instance, the high accuracy, sensitivity, specificity, and AUC values indicate that the models are reliable in detecting synovial recess distension (SRD) in knee, elbow, and ankle joints. The Dice coefficients for segmentation models also show the models' effectiveness in delineating SRD.\n\nAdditionally, the use of data augmentation techniques and pretraining on knee US data helped improve the models' generalization capability and performance on elbow and ankle datasets. This suggests that the models can learn and transfer knowledge from one joint to another, which is a form of interpretability in terms of model behavior.\n\nNot sure if the models' predictions can be explained in a way that a human expert can understand and trust. Further research is needed to develop more interpretable models or to explain the predictions of these black-box models.",
  "model/output": "The model developed is a classification model, specifically a Binary Convolutional Neural Network (BCNN) classifier. It is designed to detect the presence of synovial recess distension (SRD) in ultrasound images of knee, elbow, and ankle joints. The model's performance was evaluated using metrics such as accuracy, sensitivity, specificity, and the area under the curve (AUC). For instance, the knee model achieved an accuracy of 97%, a sensitivity of 96%, a specificity of 97%, and an AUC of 0.97. Similarly, the elbow and ankle models, which were pretrained on knee ultrasound data, also demonstrated high performance metrics. The elbow model had an accuracy of 87%, a sensitivity of 92%, a specificity of 86%, and an AUC of 0.92, while the ankle model achieved an accuracy of 94%, a sensitivity of 92%, a specificity of 97%, and an AUC of 0.93. These results indicate that the model is highly reliable in distinguishing between case and control images in persons with and without hemophilia. Additionally, segmentation models were constructed using a DeepLab V3 architecture with a ResNet101 encoder to identify SRD, achieving high Dice coefficients across all joints. The models were trained using data augmentation techniques, hyperparameter tuning, and a pretrained state from the ImageNet dataset. The training process involved a total of 50 epochs for the classification models and 150 epochs for the segmentation models, with binary cross-entropy loss and the Adam optimizer. The models were evaluated on an independent test set, ensuring robust and generalizable performance across different patient cohorts.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the AI algorithms involved several rigorous methods to ensure their robustness and generalizability. The models were assessed on an independent test set that included images from all data populations, ensuring that the evaluation was unbiased and not influenced by the training process. This test set was not used in model training or optimization, providing a true measure of the models' performance in real-world scenarios.\n\nStandard metrics were employed to evaluate the performance of the classifiers and segmentation models. These metrics included accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (AUC). For segmentation models, the Dice coefficient was used, which is a widely accepted measure for evaluating medical image segmentation algorithms. This coefficient provides numerical values that allow for the comparison of different segmentation results and the assessment of segmentation quality.\n\nAdditionally, a 5-fold cross-validation setting was used during the training and validation phases. This technique helps in assessing the model's performance by dividing the data into five subsets, training the model on four subsets, and validating it on the remaining one. This process is repeated five times, with each subset serving as the validation set once. The results of this cross-validation were included in the evaluation to provide a comprehensive understanding of the models' performance.\n\nThe models demonstrated high accuracy in identifying and delineating synovial recess distension (SRD) across various joints and patient populations. Specifically, the knee model achieved an accuracy of 97%, sensitivity of 96%, specificity of 97%, and an AUC of 0.97. High Dice coefficients, ranging from 80% to 85%, were achieved in segmentation tasks across all joints, indicating the models' effectiveness in precise segmentation.\n\nThe evaluation also included an analysis of the models' performance across different patient cohorts, including adults and pediatric patients with and without hemophilia. The models showed robustness and maintained high performance metrics across these diverse groups, with only minor declines observed in specific cohorts. This comprehensive evaluation underscores the reliability and generalizability of the AI algorithms in clinical settings.",
  "evaluation/measure": "In our evaluation, we employed a comprehensive set of performance metrics to assess the effectiveness of our AI algorithms. These metrics included accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (AUC). For segmentation tasks, we also utilized the Dice coefficient, which is a widely recognized measure in medical image segmentation. These metrics collectively provide a robust evaluation framework, allowing for a thorough comparison of different segmentation results and an assessment of the quality of the segmentation.\n\nAccuracy measures the overall correctness of the model's predictions, sensitivity (or recall) evaluates the model's ability to identify positive cases, and specificity assesses the model's ability to correctly identify negative cases. The AUC provides a single scalar value that summarizes the performance of the model across all classification thresholds. The Dice coefficient, specifically, is crucial for evaluating the overlap between the predicted and ground truth segmentations, offering insights into the precision of the segmentation models.\n\nThese metrics are well-established in the literature and are commonly used in similar studies, ensuring that our evaluation is both representative and comparable to other works in the field. By reporting these metrics, we aim to provide a clear and comprehensive understanding of our models' performance, highlighting their strengths and areas for potential improvement.",
  "evaluation/comparison": "Not applicable. The publication does not provide information about comparisons to publicly available methods or simpler baselines on benchmark datasets. The focus is primarily on the development and evaluation of the AI algorithms for detecting and segmenting synovial recess distension (SRD) in knee, elbow, and ankle joints using specific models and techniques. The evaluation metrics and performance results are presented for the developed models, but there is no mention of comparing these models with other publicly available methods or simpler baselines.",
  "evaluation/confidence": "The evaluation of our AI algorithms included standard metrics such as accuracy, sensitivity, specificity, area under the receiver operating characteristic curve (AUC), and the Dice coefficient for segmentation models. These metrics were calculated on an independent test set, ensuring that the results were not influenced by the training or optimization processes.\n\nThe performance metrics reported include mean values along with standard deviations, providing a measure of variability and confidence in the results. For instance, the AUC exceeded 0.9 for all three joints, indicating high performance and reliability. The Dice coefficients for segmentation tasks ranged from 80% to 85%, demonstrating consistent and effective segmentation across different joints.\n\nStatistical significance was assessed through 5-fold cross-validation, which helps in understanding the model's performance across different subsets of the data. This method ensures that the results are robust and not dependent on a particular split of the data. The models demonstrated high accuracy and sensitivity, with specificity also remaining high across different patient cohorts.\n\nThe use of cross-validation and the reporting of standard deviations for key metrics provide a comprehensive view of the model's performance and its generalizability. This approach ensures that the claims of superiority over other methods and baselines are statistically sound and reliable.",
  "evaluation/availability": "The raw evaluation files for our study are not publicly available. The evaluation was conducted using an independent test set containing images from all data populations, which was not involved in model training or optimization. This test set was randomly selected to ensure the representation of all patient populations in the evaluation of the AI algorithms. All images in a study were assigned to either training or a test set to avoid data leakage. The performance of the models was evaluated using standard metrics such as accuracy, sensitivity, specificity, area under the receiver operating characteristic curve (AUC), and the Dice coefficient for segmentation models. These metrics provide a comprehensive assessment of the models' performance across different patient cohorts and joints. The results of the 5-fold cross-validation setting used to train and validate each model were also included in the evaluation. However, the specific raw evaluation files, including the individual images and their corresponding labels, are not released to the public. This decision was made to protect patient privacy and comply with ethical guidelines. Researchers interested in collaborating or accessing the data for further studies can contact the corresponding author for more information."
}