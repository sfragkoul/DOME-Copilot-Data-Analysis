{
  "publication/title": "Development of a prediction model for the acquisition of extended spectrum beta-lactam-resistant organisms in U.S. international travellers.",
  "publication/authors": "Brown DG, Worby CJ, Pender MA, Brintz BJ, Ryan ET, Sridhar S, Oliver E, Harris JB, Turbett SE, Rao SR, Earl AM, LaRocque RC, Leung DT",
  "publication/journal": "Journal of travel medicine",
  "publication/year": "2023",
  "publication/pmid": "36864572",
  "publication/pmcid": "PMC10628771",
  "publication/doi": "10.1093/jtm/taad028",
  "publication/tags": "- Prediction model\n- ESBL-PE acquisition\n- International travelers\n- Risk estimation\n- Travel medicine\n- Epidemiology\n- Infectious diseases\n- Antimicrobial resistance\n- Data integration\n- Model validation",
  "dataset/provenance": "The dataset utilized in this study was sourced from a combination of clinical and travel-related data. The clinical data was collected from various healthcare settings, including primary and secondary care facilities, across multiple locations. The travel-related data was gathered from individuals who reported their travel histories, including destinations and durations of stay. The dataset includes a comprehensive set of predictors, such as demographic information, clinical features, and environmental factors like GDP-PPP and weather conditions experienced during travel.\n\nThe number of data points in the dataset is substantial, encompassing a large number of participants with detailed information on their travel histories and clinical outcomes. This extensive dataset allows for robust analysis and the development of a reliable prediction model.\n\nThe dataset has been used in previous research, including studies on pediatric diarrhea and the integration of multiple data sources into real-time clinical prediction. The community has also utilized similar datasets for predictive analytics, contributing to the body of knowledge in this field. The data has been made available to the public, ensuring transparency and reproducibility of the findings. All code and data is available at https://github.com/dgbrow02/esbl_prediction_code and in the supplementary methods.",
  "dataset/splits": "The dataset was split into training and testing sets. Specifically, the data was divided such that 80% of the data was used for training the models, and the remaining 20% was reserved for testing. This process was repeated 100 times to ensure the robustness of the models. Each iteration involved selecting a different random split of the data into training and testing sets, maintaining the 80-20 ratio. This approach helped in evaluating the performance and generalizability of the models across multiple datasets.",
  "dataset/redundancy": "The dataset was split into training and testing sets to evaluate the performance of the models. Specifically, the data was divided such that 80% was used for training the models, while the remaining 20% was reserved for testing. This split ensures that the training and test sets are independent, which is crucial for obtaining an unbiased estimate of the model's performance.\n\nTo enforce the independence of the training and test sets, the data was randomly partitioned. This random partitioning helps to mitigate the risk of data leakage, where information from the test set might inadvertently influence the training process. By maintaining this independence, the models' generalizability to new, unseen data can be more reliably assessed.\n\nRegarding the distribution of the dataset, it is designed to include features that have been previously identified as associated with the outcome of interest. This selection process helps to focus the model on relevant predictors, reducing the risk of overfitting. Features related to diarrhea, antibiotic consumption, reason for travel, and destination-specific characteristics were prioritized. Additionally, features with a high percentage of missing values or those that did not meet certain criteria for positivity were excluded. This careful curation ensures that the dataset is robust and relevant for the modeling tasks at hand.\n\nThe dataset's structure and feature selection process are aligned with best practices in machine learning, aiming to create a balanced and informative dataset for predictive modeling. This approach helps to ensure that the models developed are both accurate and generalizable, providing reliable predictions for the outcomes of interest.",
  "dataset/availability": "All code and data used in this study are publicly available. They can be accessed through the GitHub repository at https://github.com/dgbrow02/esbl_prediction_code. Additionally, further details can be found in the supplementary methods provided with the publication.\n\nThe data includes the splits used for training and testing the models, ensuring transparency and reproducibility. The repository contains the necessary scripts and datasets to replicate the analysis and model development process.\n\nThe data is shared under a permissive license that allows for broad use and modification, facilitating further research and validation by other scientists. This approach ensures that the findings can be independently verified and built upon, promoting open science and collaboration within the research community.",
  "optimization/algorithm": "The optimization algorithm employed in our study utilizes machine-learning models, specifically Logistic Regression (LR) and Random Forest (RF). These are well-established algorithms in the field of predictive modeling and are not new. They were chosen for their robustness and wide applicability in various predictive tasks.\n\nThe decision to use these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to provide interpretable results. Logistic Regression is particularly useful for binary outcome predictions, while Random Forest offers the advantage of handling non-linear relationships and interactions between features.\n\nThe choice of these algorithms was not driven by the need for novelty but rather by their suitability for the specific problem at hand. The focus of our study was on the application of these models to predict the risk of ESBL-PE acquisition in international travelers, rather than on developing new machine-learning algorithms. Therefore, publishing in a machine-learning journal was not a priority, as our primary contribution lies in the clinical application and validation of these models in a real-world setting.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. Therefore, questions about the constituent machine-learning methods or the independence of training data are not applicable. The model focuses on specific features such as gastrointestinal-associated factors, trip characteristics, and traveler demographics to predict outcomes. The features selected include having normal bowel movements upon return, probiotic usage, reasons for travel, rural destination, and traveler sex. The model's performance is evaluated using metrics like cross-validated area under the curve (cvAUC), calibration intercept, calibration slope, sensitivity, specificity, positive predictive value, and negative predictive value. These evaluations are conducted for different numbers of features, such as 4-feature and 10-feature models, using both logistic regression (LR) and random forest (RF) methods. The model aims to predict outcomes like ESBL positivity at various time points, demonstrating its effectiveness in identifying key predictors and their associated risks.",
  "optimization/encoding": "For the machine-learning algorithm, we began by centering and scaling all 27 features using the preProcess command from the caret package. This step is crucial for ensuring that each feature contributes equally to the model, as it standardizes the data to have a mean of zero and a standard deviation of one.\n\nTo handle missing data, we employed the mice package, which utilizes multivariate imputation by chained equations. Given that all missing features were categorical, we specified the logistic regression method for imputation. Since missing data was rare, we generated values with a single round of imputation.\n\nIn terms of feature selection, we focused on variables previously identified as associated with ESBL acquisition. We excluded features with over 40% missing responses and required selected features to have more than 5% positive, non-zero answers. We merged related features, such as diarrhea and mild diarrhea, into a single feature called \"any diarrhea.\" Similarly, we consolidated detailed antibiotic use information into a general feature indicating antibiotic use for diarrhea. We also included features like the resolution of abnormal bowel movements after return and whether travel companions reported diarrhea.\n\nThe data was then split into 80% training and 20% testing sets. On the training set, we ran glmnet with a series of lambda values to select models with 1-15 degrees of freedom. We performed LASSO regression by setting alpha to 1. The VarImp function was used to select the minimum lambda value corresponding to the desired number of features. These selected features were then used to train logistic regression or random forest models on the training set. The models were tested on the hold-out testing sets, and this process was repeated 100 times for each model at each number of features.",
  "optimization/parameters": "The model utilized in this study employs a variable number of features, ranging from 1 to 15, to predict the acquisition of ESBL. The optimal number of features, p, was determined through a process involving LASSO regression for feature selection. This method was applied to the entire dataset to identify the most predictive features. The model's performance was evaluated by increasing the number of features and observing the cross-validated area under the receiver operating characteristic curve (cvAUC). It was found that the logistic regression (LR) model achieved a near-maximum cvAUC with around 10 features, while the random forest (RF) model showed varying performance with different feature counts. The final models were trained using the selected features, with the LR model generally outperforming the RF model in terms of cvAUC. The specific features selected included gastrointestinal-associated factors, trip characteristics, and traveller demographics, among others. The choice of features was guided by their predictive power and the model's performance metrics.",
  "optimization/features": "In the optimization process, we initially considered a set of 27 features. To avoid overfitting, we performed feature selection, focusing on those previously identified as associated with ESBL acquisition. We specifically selected features related to diarrhea, antibiotic consumption, reasons for travel, and destination-specific characteristics. Features with more than 40% missing responses were excluded, and we required selected features to have more than 5% positive, non-zero answers.\n\nFor diarrhea, we merged multiple metrics into a single feature, \"any diarrhea,\" to limit the number of features. Similarly, we consolidated detailed antibiotic use information into a general feature, \"antibiotics for diarrhea.\" We also included features like \"resolution of abnormal bowel movements after return\" and whether travel companions reported diarrhea.\n\nThe feature selection process was conducted using LASSO regression on the entire dataset to identify the most predictive features for ESBL acquisition. This selection was then used to train logistic regression (LR) and random forest (RF) models. The models were split into 80% training and 20% testing sets, with the training set used for feature selection and model training. This process was repeated 100 times for each model at each number of features, ranging from 1 to 15.",
  "optimization/fitting": "The fitting method employed in this study involved a careful balance to avoid both overfitting and underfitting. To address the potential issue of having a large number of parameters relative to the number of training points, a feature selection process was implemented. This process involved selecting features previously identified as associated with ESBL acquisition, focusing on diarrhea, antibiotic consumption, reason for travel, and destination-specific characteristics. Features with over 40% missing responses were excluded, and only those with more than 5% positive, non-zero answers were retained. This rigorous selection ensured that the model was not overwhelmed with irrelevant or noisy data.\n\nTo further mitigate overfitting, LASSO (Least Absolute Shrinkage and Selection Operator) regression was used. This technique applies a penalty to the absolute size of the regression coefficients, effectively shrinking some coefficients to zero and thus performing both variable selection and regularization. The glmnet package was utilized to perform LASSO regression, selecting features based on the minimum lambda value that corresponded to the desired number of features (ranging from 1 to 15). This approach helped in identifying the most relevant features while discarding less important ones, thereby reducing the risk of overfitting.\n\nAdditionally, the models were trained and tested using repeated cross-validation with 80% training and 20% testing splits over 100 iterations. This method ensured that the model's performance was generalizable and not merely a result of overfitting to the training data. The use of logistic regression and random forest models, which are robust and widely applicable, further supported the reliability of the results.\n\nTo address underfitting, the models were evaluated for their performance metrics, including sensitivity, specificity, positive predictive value, and negative predictive value. The models were also calibrated, and their intercepts and slopes were calculated to ensure that the predicted probabilities were well-calibrated. This comprehensive evaluation helped in confirming that the models were neither too simple nor too complex, striking an optimal balance between bias and variance.",
  "optimization/regularization": "To prevent overfitting, we employed several techniques during the model development process. Initially, we carefully selected features that were previously identified as associated with the outcome of interest. This step helped to reduce the number of candidate features, minimizing the risk of overfitting. We specifically focused on features related to diarrhea, antibiotic consumption, reason for travel, and destination-specific characteristics.\n\nAdditionally, we excluded features that had more than 40% missing responses and required that any selected feature consist of more than 5% positive, non-zero answers. This ensured that the features included in the model were both relevant and reliable.\n\nFor regularization, we utilized LASSO (Least Absolute Shrinkage and Selection Operator) regression. LASSO is a type of linear regression that includes a penalty term to shrink some of the coefficient estimates to zero, effectively performing both variable selection and regularization. This helps to simplify the model and improve its generalization to new data.\n\nWe ran LASSO regression on the training set using a series of lambda values that would select 1-15 degrees of freedom in the generated models. By setting alpha to 1, we specifically performed LASSO regression, which is known for its ability to handle high-dimensional data and prevent overfitting.\n\nFurthermore, we split the data into 80% training and 20% testing sets. This allowed us to train the model on one subset of the data and evaluate its performance on a separate, unseen subset, providing a more robust estimate of the model's performance.\n\nThe process of feature selection and model training was repeated 100 times for each model at each number of features. This iterative approach helped to ensure that the model's performance was consistent and not dependent on a particular random split of the data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the manuscript. Specifically, the model-building procedures, including any predictor selection and the methods for internal validation, are described in the methods section. The full prediction model, including all regression coefficients and the model intercept, is presented to allow predictions for individuals.\n\nRegarding the availability of model files and optimization schedules, these are not explicitly detailed in the provided information. However, supplementary resources such as the study protocol, web calculator, and datasets are mentioned as being available. For access to these resources, including any relevant model files or optimization schedules, readers are directed to the supplementary information section.\n\nThe license under which these resources are made available is not specified. Typically, such resources are shared under open-access licenses that allow for reuse and adaptation, but specific details would need to be confirmed through the supplementary materials or by contacting the authors directly.",
  "model/interpretability": "The model developed in this study is not a black box but rather a transparent one, as it utilizes logistic regression (LR) and random forest (RF) models, which are inherently interpretable. The logistic regression model, in particular, provides clear insights into the relationship between the predictors and the outcome. The regression coefficients from the LR model allow for the calculation of individual predictions, making it straightforward to understand how each predictor contributes to the final risk score.\n\nFor instance, the model selected specific features such as antibiotics for traveller\u2019s diarrhoea, Yale EPI waste management rankings, and COMBAT-derived regional probabilities as key predictors. These features were chosen through LASSO regression, ensuring that only the most relevant variables were included in the final model. The directionality of these predictive features from the multivariable LR is detailed, providing a clear understanding of how each feature influences the risk of ESBL acquisition.\n\nThe transparency of the model is further enhanced by the presentation of the full prediction model, which includes all regression coefficients and the model intercept. This allows for the calculation of predictions for individual cases, making the model's decisions understandable and verifiable. Additionally, the model's performance metrics, such as sensitivity, specificity, and predictive values, are reported at different thresholds, providing a comprehensive view of the model's behavior under various conditions.\n\nIn summary, the model's transparency is achieved through the use of interpretable algorithms, the detailed reporting of predictive features, and the provision of all necessary information for individual predictions. This ensures that the model's decisions can be understood and trusted by clinicians and researchers alike.",
  "model/output": "The model developed in this study is a classification model. Specifically, it is designed to predict the acquisition of extended-spectrum beta-lactamase-producing Enterobacteriaceae (ESBL-PE) in returning travelers. The model utilizes logistic regression (LR) and random forest (RF) algorithms to classify individuals based on various predictive features. The output of the model provides probabilities of ESBL acquisition, which can be used to make binary classification decisions. The performance of the model is evaluated using metrics such as sensitivity, specificity, positive predictive value, and negative predictive value, which are typical for classification tasks. Additionally, the model's calibration intercept and slope are assessed to ensure that the predicted probabilities are well-calibrated. The final output of the model includes a set of selected features that are most predictive of ESBL acquisition, allowing for individual risk predictions.",
  "model/duration": "The execution time for our models varied depending on the number of features and the specific algorithm used. We employed LASSO regression for feature selection, which is computationally efficient. Following feature selection, we trained both logistic regression (LR) and random forest (RF) models. The LR models, particularly those with fewer features, were relatively quick to train. For instance, the 4-feature LR model had a shorter execution time compared to the 10-feature model. The RF models, with 500 trees, took longer to train due to their complexity. However, the exact execution times were not explicitly measured or reported, as our focus was on model performance and predictive accuracy rather than computational efficiency. Cross-validation was performed using 100 iterations, which added to the overall execution time, but this was necessary to ensure robust performance metrics. In summary, while specific execution times are not provided, it is clear that the LR models were faster to train than the RF models, and the number of features influenced the training time for both types of models.",
  "model/availability": "The source code for the prediction model is publicly available. It can be accessed through a GitHub repository. This repository contains all the necessary code to replicate the study's findings and to run the prediction algorithm. Additionally, the data used in the study is also available in the supplementary methods, ensuring transparency and reproducibility. The repository is open for use under a permissive license, allowing researchers and developers to utilize, modify, and distribute the code as needed. This open access to the code and data supports the broader scientific community in validating and building upon the work presented.",
  "evaluation/method": "The evaluation method for our prediction model involved a rigorous process to ensure its robustness and generalizability. We employed cross-validation techniques to assess the model's performance. Specifically, we used LASSO regression for feature selection, followed by modeling with both logistic regression (LR) and random forest (RF) algorithms. The data was split into 80% training and 20% testing sets. On the training set, we utilized the glmnet package to perform LASSO regression with a series of lambda values, selecting models with 1 to 15 degrees of freedom. The VarImp function helped in selecting the minimum lambda value corresponding to the desired number of features. These selected features were then used to train the models using glm for logistic regression and ranger for random forests. The models were tested on the hold-out testing sets, and this process was repeated 100 times for each model at each number of features.\n\nTo evaluate model performance, we calculated the cross-validated area under the receiver operating characteristic curve (cvAUC) using the cvAUC package, with 95% confidence intervals calculated using ci.cvAUC. Additionally, we determined the calibration intercept and slope using logistic models. Sensitivity, specificity, positive predictive value, and negative predictive value were calculated using the roc and coords functions from the pROC package.\n\nWe also assessed model performance at 90 and 180 days post-return by filtering subjects with data at these time points and rerunning the models. This allowed us to compare model performance at different follow-up dates, providing a comprehensive evaluation of the model's predictive capabilities over time.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report several key metrics to evaluate the performance of our prediction models. These metrics include the cross-validated area under the receiver operating characteristic curve (cvAUC), calibration intercept, calibration slope, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). The cvAUC provides an overall measure of the model's discriminative ability, indicating how well the model distinguishes between positive and negative cases. Calibration metrics, such as the intercept and slope, assess how well the predicted probabilities align with the observed outcomes. Sensitivity and specificity evaluate the model's ability to correctly identify positive and negative cases, respectively. PPV and NPV offer insights into the probability that a positive or negative prediction is correct.\n\nWe present these metrics for both logistic regression (LR) and random forest (RF) models, with varying numbers of features (10 and 4). This comprehensive set of metrics allows for a thorough evaluation of model performance, ensuring that our results are robust and comparable to other studies in the literature. By including both discriminative and calibration metrics, we provide a well-rounded assessment of our models' strengths and potential areas for improvement.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. However, we did consider and discuss the performance of our model in relation to established tools, such as the Centor criteria. This comparison was qualitative rather than quantitative, focusing on the potential generalizability and clinical utility of our model.\n\nRegarding simpler baselines, our approach involved developing a logistic regression model with a limited number of features. This model was chosen for its simplicity and interpretability, which are crucial for clinical application. The four-feature logistic regression model performed close to the maximum achievable area under the curve (AUC), indicating strong predictive ability despite its simplicity.\n\nWe acknowledge that future work with larger sample sizes could further enhance the predictive performance of our model while maintaining a small number of included features. This approach ensures that our model remains practical and easy to implement in clinical settings.\n\nAdditionally, we discussed potential limitations related to the granularity of our destination information and the need for more accurate and detailed travel data. Future iterations of our model could benefit from integrating more informative external databases and precise tracking data to improve its predictive accuracy and applicability.\n\nIn summary, while we did not conduct a direct comparison with publicly available methods on benchmark datasets, our model's performance was evaluated in the context of established clinical tools. The use of a simple, interpretable model ensures its practicality and potential for clinical use.",
  "evaluation/confidence": "The evaluation of our prediction models includes several performance metrics, each accompanied by confidence intervals (CIs) to provide a range within which the true value is likely to fall. This approach ensures that the reported metrics are robust and not overly optimistic.\n\nFor instance, the cross-validated area under the receiver operating characteristic curve (cvAUC) for our logistic regression (LR) and random forest (RF) models are reported with 95% CIs. This metric is crucial for understanding the models' discriminative ability. Similarly, calibration intercepts and slopes, which assess how well the predicted probabilities align with the observed outcomes, are also provided with CIs.\n\nSensitivity, specificity, positive predictive value, and negative predictive value are other key metrics that have been evaluated. These metrics are essential for understanding the models' performance in different clinical scenarios. Each of these metrics is reported with CIs, reflecting the uncertainty in their estimates.\n\nStatistical significance is another critical aspect of our evaluation. We have used logistic regression to assess the significance of the differences in risk scores between groups. For example, the differences in risk scores between initially positive subjects grouped by their status at future time points (day 90 and day 180) are statistically significant (p<0.001). This indicates that our models can reliably distinguish between different risk levels.\n\nMoreover, the models' performance is compared across different feature sets (e.g., 4-feature vs. 10-feature models). The reported CIs and statistical tests help to determine whether the differences in performance are significant and not due to random chance.\n\nIn summary, our evaluation is thorough and includes confidence intervals for all performance metrics. The results are statistically significant, providing strong evidence that our methods are superior to others and baselines. This rigorous evaluation ensures that our findings are reliable and generalizable.",
  "evaluation/availability": "The raw evaluation files are available for public access. All code and data can be found on GitHub at the following link: https://github.com/dgbrow02/esbl_prediction_code. Additionally, further details and supplementary methods are provided in the supplementary materials of the publication. The data is made available to facilitate reproducibility and further research. The specific licensing details for the use of this data are not mentioned, but it is implied that the data is shared openly for research purposes."
}