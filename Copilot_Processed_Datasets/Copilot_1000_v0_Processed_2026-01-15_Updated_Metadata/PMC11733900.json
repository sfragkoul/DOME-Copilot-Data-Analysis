{
  "publication/title": "Early childhood caries (ECC) prediction models using Machine Learning.",
  "publication/authors": "Blanco-Victorio DJ, L\u00f3pez-Ramos RP, Blanco-Rodriguez JD, L\u00f3pez-Luj\u00e1n NA, Le\u00f3n-Untiveros GF, Siccha-Macassi AL",
  "publication/journal": "Journal of clinical and experimental dentistry",
  "publication/year": "2024",
  "publication/pmid": "39822787",
  "publication/pmcid": "PMC11733900",
  "publication/doi": "10.4317/jced.61514",
  "publication/tags": "- Machine Learning\n- Early Childhood Caries\n- Dental Prediction Models\n- Support Vector Machine\n- Neural Networks\n- Oral Health\n- Predictive Analytics\n- Dental Caries\n- Data Mining\n- Pediatric Dentistry",
  "dataset/provenance": "The dataset used in this study was sourced from a sample of 186 preschoolers aged 3 to 6 years who were treated at the Hospital Santa Mar\u00eda del Socorro in Ica, Peru. The data collection involved sociodemographic and clinical information gathered from the children and their respective parents or guardians. Clinical oral examinations were conducted by specialists trained in the ICDAS II assessment system to determine the presence or absence of early childhood caries (ECC). Additionally, parents completed questionnaires to provide further relevant information.\n\nThe dataset included variables such as the age and sex of the child, consumption of sweets, haemoglobin level, type of delivery, birth order, age, sex, marital status, level of education, number of children of the mother/guardian, and monthly family income. Pre-processing steps were undertaken to exclude variables that did not significantly contribute to explaining the variable of interest. The Mann-Whitney-Wilcoxon U-tests were used for continuous variables and chi-square tests for categorical variables, resulting in the selection of significant variables: age, family income level, haemoglobin level in blood, frequency of brushing, consumption of sweets, and parents\u2019 level of education.\n\nThis dataset was then loaded into Orange Data Mining software for processing with different prediction models based on machine learning. Seventy percent of the data was used for training the models, while the remaining thirty percent was used for testing. The study aimed to compare the performance of various machine learning models in predicting the presence or absence of early childhood caries.",
  "dataset/splits": "The dataset was divided into two main splits: training and testing. Seventy percent of the data was used for training the models, while the remaining thirty percent was used for testing their performance. This split was done to ensure that the models could be trained on a substantial amount of data while also having a separate set of data to evaluate their predictive accuracy.\n\nThe dataset consisted of 186 preschoolers treated at a hospital in Ica, Peru. In the final assessment, 143 children, which is 76.88% of the total, had early childhood caries (ECC). The remaining 43 children, making up 23.12% of the total, did not have ECC. This distribution was maintained across the training and testing splits to ensure that the models were trained and tested on representative samples of the data.\n\nNot applicable.",
  "dataset/redundancy": "The dataset used in this study consisted of sociodemographic and clinical data from 186 children aged 3 to 6 years and their respective parents or guardians. The data was collected at the Dental Service of the Hospital Santa Mar\u00eda del Socorro in Ica, Peru, in 2023.\n\nThe dataset was split into training and test sets to evaluate the performance of different prediction models. Specifically, 70% of the data was used for training the models, while the remaining 30% was reserved for testing. This split ensures that the training and test sets are independent, which is crucial for obtaining unbiased performance estimates.\n\nTo enforce the independence of the training and test sets, standard data splitting techniques were employed. The data was randomly shuffled before splitting to ensure that the distribution of the data in both sets is representative of the overall dataset. This random shuffling helps to mitigate any potential biases that might arise from the order of the data.\n\nThe distribution of the dataset in this study can be compared to previously published machine learning datasets in the field of dental health. While specific comparisons to other datasets are not provided, the use of a 70-30 split is a common practice in machine learning to balance the need for sufficient training data with the need for an independent test set. This approach is consistent with many studies that aim to validate the generalizability of their models.\n\nIn summary, the dataset was split into training and test sets with a 70-30 ratio, ensuring independence through random shuffling. This method is in line with standard practices in machine learning and helps to provide reliable performance metrics for the prediction models evaluated.",
  "dataset/availability": "The datasets used and/or analyzed during the current study are available from the corresponding author. This ensures that the data can be accessed by other researchers for verification or further study. The corresponding author can be contacted for access to the datasets, facilitating transparency and reproducibility of the research findings. The data availability statement emphasizes the commitment to sharing research data, although it does not specify a public forum or licensing details. Interested parties should reach out to the corresponding author to obtain the necessary information and permissions for data access.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and widely recognized in the field. The algorithms employed include Support Vector Machine (SVM), Neural Networks (NN), Random Forest (RF), Gradient Boosting Decision Tree (GBDT), Logistic Regression (LR), and K-Nearest Neighbours (k-NN). These algorithms are part of the broader class of supervised learning techniques, which are designed to predict outcomes based on input data.\n\nThe algorithms used are not new; they have been extensively studied and applied in various domains, including healthcare. The choice of these algorithms was driven by their proven effectiveness in handling classification problems, which is crucial for predicting early childhood caries. SVM and NN, in particular, demonstrated the best performance in our study, showcasing their ability to manage non-linear relationships in the data.\n\nThe decision to use these established algorithms rather than novel ones was strategic. The primary focus of this research was to evaluate the predictive power of machine learning models in the context of dental health, specifically early childhood caries. Given the robustness and reliability of SVM and NN, they were selected for their potential to provide accurate and stable predictions. Publishing in a dental journal allowed us to reach a specialized audience that would benefit most from the findings, emphasizing the practical applications of these models in clinical settings.\n\nThe Orange Data Mining software was utilized to implement and compare these models. This software provides a user-friendly interface for data analysis, making it accessible even to those without extensive programming knowledge. It supports various prediction models and allows for real-time observation of changes in procedures, which was advantageous for our comparative analysis. The software's capabilities ensured that the models were applied consistently and that the results were reproducible.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the data was suitable for analysis. Initially, sociodemographic and clinical data were collected from a sample of 186 children aged 3 to 6 years and their respective parents or guardians. The data included variables such as age, sex, consumption of sweets, haemoglobin level, type of delivery, birth order, age and sex of the mother/guardian, marital status, level of education, number of children, and monthly family income.\n\nTo preprocess the data, variables that did not significantly contribute to explaining the presence of early childhood caries were excluded. The Mann-Whitney-Wilcoxon U-tests were used for continuous variables, and chi-square tests were used for categorical variables, with a p-value threshold of less than 0.05. This process identified the following significant variables: age, family income level, haemoglobin level in blood, frequency of brushing, consumption of sweets, and parents\u2019 level of education.\n\nThe dataset with these significant variables was then loaded into the Orange Data Mining software for further processing. Seventy percent of the data was used as training data, while the remaining thirty percent was used for testing the models. This preprocessing ensured that the machine-learning algorithms could effectively learn from the data and make accurate predictions about the presence of early childhood caries.",
  "optimization/parameters": "The study utilized several input parameters to predict early childhood caries using machine learning models. The variables considered significant and included in the final analysis were age, family income level, haemoglobin level in blood, frequency of brushing, consumption of sweets, and parents\u2019 level of education. These parameters were selected through a pre-processing step that involved statistical tests, specifically the Mann-Whitney-Wilcoxon U-tests for continuous variables and chi-square tests for categorical variables, with a significance level of p-value < 0.05. This process ensured that only the most relevant variables were included in the model, thereby optimizing its performance. The final dataset, comprising these six parameters, was then used to train and test various machine learning models within the Orange Data Mining software.",
  "optimization/features": "The study utilized a dataset comprising six significant features as input for the machine learning models. These features were selected based on their contribution to explaining the variable of interest, which was the presence or absence of early childhood caries. The features included age, family income level, haemoglobin level in blood, frequency of brushing, consumption of sweets, and parents\u2019 level of education.\n\nFeature selection was indeed performed to ensure that only the most relevant variables were included in the analysis. This process involved using statistical tests such as the Mann-Whitney-Wilcoxon U-tests for continuous variables and chi-square tests for categorical variables. Variables that did not significantly contribute to explaining the target variable were excluded. This selection process was conducted using the entire dataset before it was split into training and test sets, ensuring that the feature selection was done independently of the training data. This approach helps to prevent data leakage and ensures that the models are evaluated on unseen data, providing a more reliable assessment of their performance.",
  "optimization/fitting": "In our study, we employed several machine learning models to predict early childhood caries, including Support Vector Machine (SVM), Neural Networks (NN), Random Forest (RF), Gradient Boosting Decision Tree (GBDT), Logistic Regression (LR), and K-Nearest Neighbours (k-NN). The number of parameters in these models varied, with some having a larger parameter space than others.\n\nFor models like Neural Networks and SVM, which can have a large number of parameters, we took several steps to mitigate overfitting. We used techniques such as cross-validation to ensure that our models generalized well to unseen data. Specifically, we divided our dataset into training and testing sets, with 70% of the data used for training and 30% for testing. This approach helped us to assess the model's performance on data it had not seen during training, providing a more reliable estimate of its predictive accuracy.\n\nAdditionally, we employed regularization techniques where applicable. For instance, in the SVM model, we used techniques like kernel trick to handle non-linear data without explicitly increasing the number of parameters. For Neural Networks, we used dropout layers to prevent overfitting by randomly setting a fraction of input units to zero at each update during training time.\n\nTo address underfitting, we ensured that our models were complex enough to capture the underlying patterns in the data. We experimented with different architectures and hyperparameters, using techniques like grid search and random search to find the optimal configuration. For example, in the Neural Network model, we varied the number of layers and neurons to find the best architecture that balanced complexity and performance.\n\nFurthermore, we evaluated the performance of our models using multiple metrics, including accuracy, F1-score, precision, recall, and the area under the ROC curve (AUC). These metrics provided a comprehensive view of the model's performance, helping us to identify and address any issues related to overfitting or underfitting.\n\nIn summary, we carefully managed the complexity of our models to avoid both overfitting and underfitting. By using cross-validation, regularization, and thorough hyperparameter tuning, we ensured that our models were robust and generalizable.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting, ensuring that our models generalized well to unseen data. One of the primary methods used was the Random Forest algorithm, which inherently reduces overfitting by averaging the results of multiple decision trees. This ensemble approach helps to mitigate the risk of any single tree overfitting the training data.\n\nAdditionally, we utilized hyperparameter tuning to optimize the performance of our models. This process involved adjusting parameters such as the number of trees in the Random Forest, the depth of the trees, and the learning rate in Gradient Boosting models. Proper hyperparameter tuning helps in finding the right balance between bias and variance, thereby reducing overfitting.\n\nWe also split our dataset into training and test sets, with 70% of the data used for training and 30% reserved for testing. This separation ensures that the model's performance is evaluated on data it has not seen during training, providing a more accurate measure of its generalization capability.\n\nFurthermore, we considered the use of cross-validation techniques, although specific details on this are not provided. Cross-validation involves splitting the data into multiple folds and training the model on different combinations of these folds, which helps in assessing the model's performance more robustly and reduces the likelihood of overfitting.\n\nIn summary, our approach to preventing overfitting included the use of ensemble methods like Random Forest, hyperparameter tuning, and a clear separation of training and test data. These techniques collectively contributed to the robustness and reliability of our predictive models.",
  "optimization/config": "The datasets used and/or analyzed during the current study are available from the corresponding author. The research was not funded by any organization and was self-financed. The authors declare that they have no competing interests. The present study has the approval of the hospital Ethics Committee with N\u00b0 2023-100-16. The software used to perform the predictive model comparison was Orange Data Mining, which is a free, general-purpose machine learning and data mining tool. It can be downloaded from http://orange.biolab.si. The main advantages of this program are that it is not necessary to use code to perform data analysis, it allows observing the changes made in the procedures in real time, and it allows the generation of graphs. The Orange Data Mining software includes widgets with different prediction models, such as Random Forest, Gradient Boosting Decision Tree, Support Vector Machine, Logistic Regression, Neural Networks, and K-Nearest Neighbours. The workflow used in Orange Data Mining with the different prediction models evaluated included widgets such as File, Data Sampler, Test and score, and ROC Analysis. The performance indicators used to assess the prediction models included precision, recall, F1-score, accuracy, and Matthews correlation coefficient. The best-performing models were Support Vector Machine and Neural Networks, with an area under the ROC curve of 0.868 and 0.904, respectively. The study had some limitations, such as the sample coming from a specific population and the limited sample size of 186 children. It is recommended to carry out longitudinal studies with a larger sample number to confirm the predictive capacity of the models.",
  "model/interpretability": "The models used in this study include both transparent and black-box approaches. Logistic Regression and Decision Trees, which are components of Random Forest, are considered transparent models. These models provide clear, interpretable outputs. For instance, Logistic Regression models the probability of a binary outcome using a linear combination of input features, making it straightforward to understand the contribution of each feature to the prediction. Decision Trees, similarly, create a flowchart-like structure of decisions based on feature values, which can be easily visualized and interpreted.\n\nOn the other hand, models like Neural Networks and Support Vector Machines (SVM) are often considered black-box models. These models, while powerful, do not provide clear, human-interpretable explanations for their predictions. Neural Networks, with their complex layers and nodes, can capture intricate patterns in the data but at the cost of interpretability. Similarly, SVMs, which operate by finding optimal hyperplanes in high-dimensional space, do not offer straightforward insights into how predictions are made.\n\nGradient Boosting Decision Trees and K-Nearest Neighbors (k-NN) also fall into the black-box category. Gradient Boosting builds an ensemble of decision trees in a sequential manner, which can make the overall model difficult to interpret despite the transparency of individual trees. k-NN, which classifies data points based on the majority vote of their nearest neighbors, does not provide a clear rule-based explanation for its predictions.\n\nIn summary, while some models used in this study offer transparency and interpretability, others are more opaque, relying on complex internal mechanisms to make predictions. The choice of model depends on the trade-off between predictive performance and the need for interpretability in the specific application.",
  "model/output": "The model discussed in this publication is a classification model. It is designed to predict the occurrence of early childhood caries, which is a binary outcome\u2014either the presence or absence of caries. Several machine learning algorithms were evaluated, including Support Vector Machine (SVM), Neural Networks (NN), Random Forest (RF), Gradient Boosting Decision Tree (GBDT), Logistic Regression (LR), and K-Nearest Neighbours (k-NN). These models were used to classify data into two categories: children with early childhood caries and those without.\n\nThe performance of these models was assessed using various metrics such as accuracy, F1-score, precision, recall, and the area under the ROC curve (AUC). The SVM and Neural Network models demonstrated the best performance, with high values in these metrics, indicating their effectiveness in classifying the data accurately.\n\nThe study highlights that while these models show promise in predicting early childhood caries, more training data is needed to achieve more stable and accurate results. Longitudinal studies with larger sample sizes are recommended to further confirm the predictive capacity of these models.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The software used for the predictive model comparison in this study is Orange Data Mining. This is a free, open-source tool designed for machine learning and data mining. It is accessible to a wide range of users, from beginners to experienced programmers, and supports both Python scripting and visual programming through a graphical user interface (GUI) with widgets.\n\nOrange Data Mining can be downloaded from its official website at http://orange.biolab.si. The software includes a variety of techniques for data management, preprocessing, supervised and unsupervised learning, performance analysis, and data visualization. It operates through workflows in a visual programming environment, making it user-friendly and powerful for data analysis.\n\nThe specific workflow used in this study involved several widgets: File for loading the database, Data Sampler for dividing the data into training and testing sets, Test and Score for evaluating model performance, and ROC Analysis for visualizing the discriminative ability of the models. This workflow was implemented within the Orange Data Mining environment, leveraging its capabilities to handle and analyze the data efficiently.\n\nThe source code for Orange Data Mining is publicly available, and it is licensed under the GNU General Public License (GPL), which allows for free use, modification, and distribution of the software. This ensures that users can access, modify, and share the software as needed for their research and applications.",
  "evaluation/method": "The evaluation method employed in this study involved a cross-sectional analytical approach. The dataset consisted of sociodemographic and clinical data from 186 children aged 3 to 6 years, along with information from their parents or guardians. This data was collected from a hospital in Ica, Peru.\n\nThe dataset was processed using the Orange Data Mining software, which facilitated the implementation of various machine learning prediction models. The performance of these models was assessed using several key indicators: precision, recall, F1-score, and accuracy. Additionally, the discriminatory power of each model was determined by evaluating the area under the ROC curve (AUC).\n\nThe dataset was divided into two groups: 70% for training the models and 30% for testing their performance. This split ensured that the models were trained on a substantial portion of the data while being evaluated on a separate, unseen subset. The use of the ROC curve provided a graphical representation of the models' ability to distinguish between children with and without early childhood caries (ECC).\n\nThe evaluation process included comparing multiple models, such as Logistic Regression, Random Forest, Gradient Boosting, Support Vector Machine (SVM), Neural Networks, and K-Nearest Neighbors (KNN). The performance metrics for each model were carefully analyzed to identify the most effective predictors of ECC. The SVM and Neural Network models demonstrated the highest performance, with similar values for accuracy, F1-score, and recall. The Neural Network model, in particular, showed the highest discriminatory power as indicated by its AUC value.\n\nOverall, the evaluation method was designed to rigorously assess the predictive capabilities of various machine learning models, ensuring that the results were both reliable and generalizable.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of the machine learning models used for predicting early childhood caries (ECC). These metrics include accuracy, precision, recall, F1-score, Matthews correlation coefficient (MCC), and the area under the ROC curve (AUC).\n\nAccuracy measures the overall correctness of the model by calculating the ratio of correctly predicted instances (both true positives and true negatives) to the total number of instances. Precision focuses on the correctness of positive predictions, indicating the proportion of true positives among all positive predictions. Recall, also known as sensitivity, assesses the model's ability to identify all relevant instances by calculating the proportion of true positives among all actual positives. The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. The Matthews correlation coefficient is a balanced measure that considers all four quadrants of the confusion matrix, making it useful even when the classes are of very different sizes. Finally, the AUC provides a comprehensive evaluation of the model's ability to distinguish between the classes across all possible classification thresholds.\n\nThese metrics collectively offer a thorough assessment of the models' performance, ensuring that we capture various aspects of their predictive capabilities. The use of these metrics aligns with established practices in the literature, providing a robust and representative evaluation of our models.",
  "evaluation/comparison": "In our study, we compared the performance of various machine learning models to predict early childhood caries. The models evaluated included Random Forest, Gradient Boosting Decision Tree, Support Vector Machine, Logistic Regression, Neural Networks, and K-Nearest Neighbours. These models were chosen to cover a range of algorithms, from linear to non-linear, and from simple to complex, ensuring a comprehensive comparison.\n\nWe utilized the Orange Data Mining software, an open-source tool that provides a visual programming environment for data analysis. This software allowed us to implement and compare the models without the need for extensive coding, making the process accessible and efficient. The workflow in Orange Data Mining included loading the dataset, splitting it into training and testing sets, evaluating model performance, and visualizing the results through ROC analysis.\n\nThe dataset used for this comparison consisted of sociodemographic and clinical data from 186 children aged 3 to 6 years, collected from the Dental Service of the Hospital Santa Mar\u00eda del Socorro in Ica, Peru. The data underwent preprocessing to exclude variables that did not significantly contribute to explaining the presence of early childhood caries. The final dataset included variables such as age, family income level, haemoglobin level in blood, frequency of brushing, consumption of sweets, and parents\u2019 level of education.\n\nTo assess the performance of the models, we used several indicators, including accuracy, F1-score, precision, recall, and Matthews correlation coefficient. These metrics provided a comprehensive evaluation of the models' ability to correctly predict the presence or absence of early childhood caries. The results showed that the Support Vector Machine and Neural Network models achieved the highest performance values, indicating their effectiveness in this predictive task.\n\nIn summary, our study involved a thorough comparison of different machine learning models using a well-defined dataset and robust evaluation metrics. This approach ensured that we could identify the most effective models for predicting early childhood caries, providing valuable insights for dental health professionals.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not applicable."
}