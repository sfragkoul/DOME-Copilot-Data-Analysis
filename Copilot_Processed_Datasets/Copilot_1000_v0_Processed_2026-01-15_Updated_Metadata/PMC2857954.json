{
  "publication/title": "Identification of Metabotropic Glutamate Receptor Subtype 5 Potentiators Using Virtual High-Throughput Screening.",
  "publication/authors": "Mueller R, Rodriguez AL, Dawson ES, Butkiewicz M, Nguyen TT, Oleszkiewicz S, Bleckmann A, Weaver CD, Lindsley CW, Conn PJ, Meiler J",
  "publication/journal": "ACS chemical neuroscience",
  "publication/year": "2010",
  "publication/pmid": "20414370",
  "publication/pmcid": "PMC2857954",
  "publication/doi": "10.1021/cn9000389",
  "publication/tags": "- Artificial Neural Networks\n- Molecular Descriptors\n- QSAR Models\n- Machine Learning\n- High-Throughput Screening\n- mGluR5 Potentiators\n- Chemoinformatics\n- Descriptor Optimization\n- Predictive Modeling\n- Biological Activity Prediction",
  "dataset/provenance": "The dataset used in this study originated from experimental collaborators who provided the active and inactive molecules as MDL SD files. These molecules were then processed to generate 3D structures using CORINA, which were subsequently used to calculate molecular descriptors with ADRIANA.\n\nA total of 1,382 compounds were confirmed to be active potentiators of the mGluR5 glutamate response, representing a 0.94% hit rate. However, only 1,356 of these active compounds were utilized in model generation due to challenges in encoding charged molecules. All other compounds were classified as inactive.\n\nThe dataset was balanced through oversampling, where active compounds were used 106 times more frequently in training to account for their smaller number compared to the inactive set. This approach ensured that the training data contained an equal number of active and inactive compounds, maximizing the information content and entropy of the final prediction method.\n\nThe entire dataset consisted of 144,477 data points, which were split into three subsets: 80% for training the ANN models, 10% for monitoring during training to initiate early termination, and the remaining 10% for independent testing of the QSAR models. Care was taken to avoid any overlap between these subsets.",
  "dataset/splits": "The dataset was divided into three splits: training, monitoring, and independent data sets. The total experimental data set consisted of 144,477 data points. Of these, 115,581 data points (80%) were allocated to the training set. The monitoring set received 14,448 data points (10%), and the independent set also contained 14,448 data points (10%). Care was taken to ensure that there was no overlap between these splits. Additionally, the major scaffolds identified in the active compounds were evenly distributed across all three data sets. For instance, benzamides were found in the training set (214 compounds, 80.1%), monitoring set (21 compounds, 7.9%), and independent set (32 compounds, 12.0%). Similarly, benzoxazepines were distributed with 114 compounds (83.2%) in the training set, 13 compounds (9.5%) in the monitoring set, and 10 compounds (7.3%) in the independent set. This distribution ensured that the models were trained and validated on representative subsets of the data.",
  "dataset/redundancy": "The datasets were split into three distinct parts: training, monitoring, and independent testing sets. The training set comprised 80% of the total data points, the monitoring set included 10%, and the independent testing set also contained 10%. This division ensured that the models were trained on a substantial portion of the data while allowing for monitoring during training and independent evaluation of the final models.\n\nTo maintain independence between the datasets, care was taken to avoid any overlap. This means that compounds included in the training set were not present in the monitoring or independent testing sets, and similarly, compounds in the monitoring set did not appear in the training or independent testing sets. This strict separation is crucial for obtaining unbiased performance metrics.\n\nThe distribution of major scaffolds, such as benzamides, benzoxazepines, and MPEP-like compounds, was evenly spread across the training, monitoring, and independent data sets. For instance, benzamides were found in the training set 80.1% of the time, in the monitoring set 7.9% of the time, and in the independent set 12.0% of the time. This balanced distribution helps in ensuring that the models generalize well to unseen data.\n\nCompared to previously published machine learning datasets, the approach taken here emphasizes the importance of maintaining a balanced and representative distribution of compounds across all datasets. This strategy helps in building robust models that can accurately predict the activity of compounds, even those not seen during training. The use of oversampling for active compounds further ensures that the models are not biased towards predicting inactivity, which is a common issue in imbalanced datasets.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used is artificial neural networks (ANNs). This class of algorithms is well-established and widely used in various fields, including chemistry and drug discovery.\n\nThe specific ANN algorithm implemented is not new; it is a supervised learning approach known as resilient propagation. This method is part of the broader category of backpropagation algorithms used for training neural networks.\n\nThe algorithm was not published in a machine-learning journal because the focus of the study was on applying machine learning techniques to a specific problem in chemical neuroscience, rather than developing a new machine-learning algorithm. The implementation of the ANN algorithm was done using the BioChemistryLibrary (BCL), an in-house developed object-oriented library written in the C++ programming language. This library consists of approximately 400 classes and 300,000 lines of code, indicating a robust and comprehensive toolset for biochemical and chemical computations. The use of ANNs in this context is to generate quantitative structure-activity relationship (QSAR) models from high-throughput screening (HTS) experimental data sets. The goal is to enrich a local library for compounds with metabotropic glutamate receptor subtype 5 (mGluR5) allosteric activity. The study demonstrates the practical application of ANNs in drug discovery, showcasing their effectiveness in handling complex chemical data and improving the prediction of bioactive compounds.",
  "optimization/meta": "Not applicable. The model described does not use data from other machine-learning algorithms as input. It is an artificial neural network (ANN) that uses molecular descriptors as inputs to predict biological activity. The optimization process involves refining the set of molecular descriptors used in the ANN to improve prediction accuracy. The training data is split into training, monitoring, and independent sets to ensure that the model's performance is evaluated on unseen data. The independent data set is reserved for final testing and is not used during the training or monitoring phases, ensuring that the training data is independent.",
  "optimization/encoding": "The data encoding process involved the use of molecular descriptors to represent chemical structures. A total of 1,252 molecular descriptors across 35 categories were computed using the ADRIANA software. These descriptors included scalar descriptors such as molecular weight, number of hydrogen bonding acceptors and donors, octanol/water partition coefficient, topological polar surface area, mean molecular polarizability, dipole moment, and solubility in water. Additionally, various 2D and 3D autocorrelation descriptors, radial distribution functions, and surface autocorrelation descriptors were utilized. These descriptors were chosen for their ability to encode a wide diversity of chemical scaffold information into mathematical representations that are invariant to scaffold size, composition, and rotation/translation of 3D coordinate molecule representations.\n\nThe encoding process began with the retrieval of active and inactive molecules as MDL SD files from experimental collaborators. 3D structures were then generated using CORINA and used as input for the calculation of molecular descriptors. To balance the dataset, active compounds were oversampled 106 times to account for their smaller number compared to the inactive set. This oversampling approach was chosen to maximize the information content of the final prediction method, ensuring that the dataset contained an equal number of active and inactive compounds during training.\n\nThe dataset was divided into training, monitoring, and independent data sets, with 80%, 10%, and 10% of the molecules randomly included in each set, respectively. This division allowed for the iterative training of artificial neural network (ANN) models coupled with input sensitivity analysis to reduce and optimize the descriptor set until no further improvement in the quality criteria for the independent data set was achieved. The natural logarithm of the experimentally determined EC50 value of each compound was used as the output for the ANN models.",
  "optimization/parameters": "The optimization process began with a comprehensive set of 1,252 molecular descriptors, which were systematically refined to enhance the prediction accuracy of the artificial neural network (ANN) model. The initial training utilized the complete set of descriptors to establish a baseline. Subsequently, the significance of each input was evaluated using input sensitivity, which measures the partial derivative of each input with respect to the output. This sensitivity was determined by perturbing each input value and monitoring the resulting change in output.\n\nThe descriptors were categorized into 35 groups, and the input sensitivity for each category was calculated as the norm over the individual input sensitivity values within that category. Descriptors with an input sensitivity above a certain threshold (0.06) were retained, while those with lower sensitivity were removed. This iterative process continued until further removal of descriptors did not improve the prediction accuracy for the independent data set.\n\nThrough this optimization, the number of descriptors was reduced from 1,252 to 276, which included the eight scalar descriptors and additional categories such as 3D autocorrelation lone pair electronegativity and radial distribution functions. This reduction sped up the training process by a factor of 3 and maintained approximately two-thirds of the total input sensitivity while using only about one-third of the total number of descriptors. The final model, with 276 input descriptors, was considered optimal as it balanced performance on the independent data set with the smallest descriptor set.",
  "optimization/features": "The optimization process began with a comprehensive set of 1,252 molecular descriptors, categorized into 35 groups. These descriptors were used as inputs for the initial training of the artificial neural networks (ANNs). To enhance the model's efficiency and performance, a systematic feature selection process was undertaken. This involved calculating the input sensitivity of each descriptor, which measures the impact of each input on the network's output. Descriptors with the least significance were progressively removed, reducing the total number of inputs and, consequently, the number of weights in the ANN. This approach aimed to minimize the degrees of freedom, accelerate training and prediction, and reduce noise.\n\nThe feature selection was performed iteratively. Initially, descriptors with an input sensitivity above a certain threshold were retained, resulting in a model with 428 descriptors. Subsequent iterations involved removing the least significant descriptor categories until further reductions did not improve the prediction accuracy for the independent data set. The optimal model was achieved with 276 descriptors, which included scalar descriptors and specific categories like 3D autocorrelation and radial distribution functions. This iterative process ensured that the feature selection was conducted using the training set only, maintaining the integrity of the independent test set for unbiased evaluation.",
  "optimization/fitting": "The fitting method employed in this study involved training artificial neural networks (ANNs) to predict the activity of compounds. The initial set of molecular descriptors consisted of 1,252 inputs, which indeed is a large number compared to the training points. To address the potential issue of overfitting, several strategies were implemented.\n\nFirstly, the descriptor set was systematically optimized by removing the least significant molecular descriptor groups. This reduction in the number of inputs helped to minimize the number of degrees of freedom (weights) that needed to be determined, thereby reducing the complexity of the model. This process was iterative, and the removal of descriptors continued until further reductions did not improve the prediction accuracy for the independent data set.\n\nSecondly, the data set was split into training, monitoring, and independent testing sets. The monitoring data set was used to initiate early termination of the training process. Training was stopped once the root-mean-square deviation (rmsd) of the monitoring data set was minimized. This approach ensured that the model did not overfit to the training data by continuously evaluating its performance on a separate set of data.\n\nAdditionally, oversampling was used to balance the training data, ensuring that the model had an equal number of active and inactive compounds. This method maximized the information content of the final prediction method and helped to prevent underfitting by providing a more representative sample of the data.\n\nThe use of a monitoring data set and early termination of training helped to rule out overfitting. Conversely, the systematic optimization of the descriptor set and the use of oversampling helped to ensure that the model was not underfitting by maintaining a balance between model complexity and the amount of data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our artificial neural network (ANN) models. One of the key methods used was the systematic removal of the least significant molecular descriptor groups. This process aimed to reduce the total number of inputs and, consequently, the number of weights in the ANN. By minimizing the number of degrees of freedom, we accelerated the training and prediction processes while reducing noise and increasing the ratio of data points to degrees of freedom.\n\nAdditionally, we utilized a monitoring data set to terminate ANN training early. This approach involved setting aside a portion of the data to monitor the performance of the model during training. Training was halted once the root-mean-square deviation (rmsd) of the monitoring data set reached its minimum, preventing the model from overfitting to the training data.\n\nFurthermore, we balanced the training data through oversampling. This technique involved using active compounds more frequently in training to account for their smaller number compared to inactive compounds. Oversampling ensured that the model had a balanced representation of both active and inactive compounds, which is crucial for developing accurate and reliable predictive models. This method was compared with undersampling strategies, and it was found that oversampling yielded better results by utilizing all available information for model development.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule are not explicitly detailed in the publication. The training process involved using up to 40,000 iterations of resilient back-propagation, but the specific hyper-parameters such as learning rates, batch sizes, or other tuning details are not provided. The architecture of the artificial neural networks (ANNs) used is described, including the number of inputs, hidden neurons, and outputs, but the exact configurations for each iteration of the model are not specified.\n\nThe model files and optimization parameters are also not made available in the publication. There is no mention of where these files can be accessed or under what license they might be distributed. The focus of the publication is on the methodology and results of the descriptor optimization process rather than the provision of the actual model files or detailed hyper-parameter settings.\n\nThe publication does provide a comprehensive overview of the metrics used to evaluate the models, such as root-mean-square deviation (rmsd), area under the receiver operating characteristic curve (auc), and enrichment of active compounds. However, the specific configurations and parameters used to achieve these results are not fully disclosed.",
  "model/interpretability": "The model employed in this study is an Artificial Neural Network (ANN), which is generally considered a black-box model. This means that while the ANN can make accurate predictions, the internal workings and the specific reasoning behind these predictions are not easily interpretable. The ANN operates by processing input data through layers of interconnected neurons, each applying weights and activation functions to transform the data. The final output is a prediction of biological activity, but the exact contributions of individual input features to this prediction are not straightforward to discern.\n\nHowever, efforts were made to enhance the interpretability of the model through the analysis of input sensitivity. This involved determining how changes in specific input descriptors affected the model's output. By perturbing input values and observing the resulting changes in predictions, the sensitivity of the model to different descriptors was assessed. This process helped identify which descriptors were most influential in the model's predictions, providing some insight into the factors driving the ANN's decisions.\n\nFor example, radial distribution functions and electronegativity were found to contribute most significantly to accurate predictions. This information can be used to understand which molecular properties are crucial for the biological activity being modeled. Additionally, the iterative optimization of the descriptor set, where less significant descriptors were systematically removed, further refined the model's focus on the most relevant features. This process not only improved the model's performance but also made it more interpretable by reducing the complexity and highlighting the key descriptors.\n\nIn summary, while the ANN itself remains a black-box model, the use of input sensitivity analysis and descriptor optimization provides a means to gain some interpretability. This approach allows for a better understanding of which molecular features are most important for the predictions made by the model.",
  "model/output": "The model primarily functions as a binary classifier, although it was initially trained using continuous ln EC50 values. The binary classification approach involved assigning an activity of 1 to all active compounds and 0 to all inactive compounds. This method was tested to see if it offered any advantages, but it did not yield an improvement over models trained with continuous ln EC50 values. Therefore, the final model used for in silico screening experiments was the one trained with continuous ln EC50 values, but it was evaluated using binary classification metrics such as the area under the curve (AUC) and enrichment.\n\nThe output of the model is a predicted biological activity based on complex nonlinear relationships derived from machine learning through iterative training. The model's performance was assessed using metrics like the root-mean-square deviation (rmsd), AUC, and enrichment. The final optimized model, which includes 276 descriptors, was used for virtual screening experiments. This model was chosen because it displayed optimal performance on the independent data set combined with the smallest descriptor set.\n\nThe model's output is a single value representing the predicted biological activity of the input molecule. This value is used to make binary decisions for compound acquisition in virtual screening. The model's predictive power was evaluated using receiver operating characteristic (ROC) curves, which plot the rate of true positives versus the rate of false positives. The AUC of the ROC curve indicates the model's overall predictive power, while enrichment values measure the factor by which active compounds are increased relative to inactive compounds when selecting a subset of data predicted with the highest confidence levels.",
  "model/duration": "The training process for the artificial neural network (ANN) models took up to 13 hours per network. This was achieved using 8 cores of a Core2 Quad 2.33 GHz Intel Xeon microprocessor in parallel on the 64-bit version of Red Hat Enterprise Linux 5.2. The training was terminated early when the monitoring data set achieved its minimum root-mean-square deviation (rmsd), which helped in optimizing the execution time.",
  "model/availability": "The source code for the artificial neural network (ANN) algorithm is not publicly released. The algorithm was implemented within an in-house developed object-oriented library called the BioChemistryLibrary (BCL), written in the C++ programming language. This library consists of approximately 400 classes and 300,000 lines of code. The training method used for the ANN is resilient propagation, a supervised learning approach.\n\nFor the generation of chemical descriptors, ADRIANA.Code was used. For the generation of three-dimensional structures, CORINA was employed. However, specific details about the availability or licensing of these tools are not provided.",
  "evaluation/method": "The evaluation of the method involved several key steps and metrics to ensure the robustness and predictive power of the models. Initially, the data set was split into three parts: 80% for training, 10% for monitoring during training, and 10% for independent testing. This split ensured that the models were trained on a substantial portion of the data while allowing for monitoring and independent validation.\n\nThe root-mean-square deviation (rmsd) between the predicted and experimental activity was used as the primary objective function during training. This metric helped in minimizing the difference between the predicted and actual biological activity, thereby improving the model's accuracy.\n\nTo assess the model's performance, multiple measures were employed. These included the rmsd, the enrichment of active compounds in a virtually screened compound library, and the area under the receiver operating characteristic (ROC) curve (auc value). The enrichment factor, determined from biological testing of prioritized compounds, demonstrated the method's predictive power. An enrichment factor of 30 was achieved, which aligned with the theoretically predicted enrichment of 38.\n\nThe independent data set, representing a randomly selected 10% of the experimental high-throughput screening (HTS) data, was crucial for validating the model's performance. This ensured that the models were evaluated on data they had not seen during training, providing a true measure of their generalizability.\n\nAdditionally, the use of oversampling for active compounds helped in balancing the training data, which is essential for developing effective predictive models. This approach maximized the information content of the final prediction method, ensuring that the models were not biased towards the majority class.\n\nThe iterative process of descriptor optimization further refined the models. By systematically removing less sensitive descriptors, the models were streamlined, reducing the number of inputs and accelerating training and prediction. This optimization process was crucial in achieving the best performance metrics, including the lowest rmsd and highest enrichment factors.\n\nIn summary, the evaluation method involved a rigorous process of data splitting, metric calculation, and iterative optimization. This ensured that the models were robust, accurate, and capable of predicting active compounds with high enrichment factors.",
  "evaluation/measure": "In the evaluation of our models, several performance metrics were employed to ensure a comprehensive assessment of their predictive power. The primary metrics reported include the root-mean-square deviation (rmsd) between predicted and experimental ln EC50 values, the area under the receiver operating characteristic curve (auc), and enrichment values.\n\nThe rmsd provides a measure of the average magnitude of the errors between predicted and observed values, offering insight into the accuracy of the model's predictions. However, it is noted that rmsd alone can be a poor indicator of model quality, particularly when dealing with binary classification tasks.\n\nThe auc is a crucial metric for evaluating the binary classification performance of our models. It represents the ability of the model to distinguish between active and inactive compounds across all possible classification thresholds. A higher auc value indicates better model performance. Receiver operating characteristic (ROC) curves were generated to visualize this metric, with the diagonal representing the performance expected from a random predictor.\n\nEnrichment values are particularly important in the context of virtual screening, where only a small fraction of compounds predicted to be maximally active will undergo biological testing. Enrichment measures the factor by which active compounds are increased relative to inactive compounds when selecting a subset of data predicted with the highest confidence levels. This metric is especially relevant for the initial part of the ROC curve, which contains the compounds with the highest predicted biological activity.\n\nThese metrics are representative of those commonly used in the literature for evaluating machine learning models in chemical and biological contexts. The use of rmsd, auc, and enrichment values ensures that our models are assessed not only on their overall predictive accuracy but also on their practical utility in real-world applications such as virtual screening. This set of metrics provides a balanced view of model performance, taking into account both the quantitative accuracy of predictions and the qualitative effectiveness in identifying active compounds.",
  "evaluation/comparison": "In the evaluation of our model, we did not perform a direct comparison to publicly available methods on benchmark datasets. However, we did establish a baseline for our descriptor optimization process. This baseline was created by training an artificial neural network (ANN) using only scalar descriptors. The performance metrics obtained from this baseline, including the root-mean-square deviation (rmsd), area under the receiver operating characteristic curve (auc), and enrichment of active compounds, served as a reference point for subsequent model optimizations.\n\nAdditionally, we compared different strategies for balancing the training data. We evaluated an oversampling approach, which involved using active compounds more frequently in training, against two undersampling strategies. One undersampling strategy involved randomly selecting inactive compounds, while the other involved choosing inactive compounds that were most similar to the active compounds. The oversampling strategy yielded better results in terms of rmsd, auc, and enrichment, indicating that it was more effective for our specific application.\n\nWe also compared the performance of our models using different sets of descriptors. Initially, we retained the most sensitive 428 descriptors and iteratively removed the least sensitive descriptor categories. This process helped us identify the optimal set of descriptors that balanced model performance with complexity. The final model, which used 276 descriptors, was considered optimal as it provided the best performance on the independent data set with the smallest descriptor set.\n\nIn summary, while we did not compare our method directly to other publicly available methods, we established a robust baseline and evaluated different data balancing strategies and descriptor sets to optimize our model's performance.",
  "evaluation/confidence": "In the evaluation of our models, we primarily focused on metrics such as the area under the receiver operating characteristic curve (AUC) and enrichment values to assess predictive power, particularly for virtual screening applications. These metrics were chosen because they are crucial for binary classification tasks, which are essential for deciding compound acquisition in virtual screens.\n\nThe AUC values provide a measure of the model's ability to distinguish between active and inactive compounds. Higher AUC values indicate better model performance. For instance, our optimized model achieved an AUC of 0.757, which is a significant improvement over the baseline model's AUC of 0.673. This improvement suggests that the model's predictive power has been enhanced through descriptor optimization.\n\nEnrichment values, which measure the factor by which active compounds are increased relative to inactive compounds when selecting a subset of data predicted with the highest confidence, were also critical. The enrichment value for our optimized model was 38, compared to the baseline model's enrichment of 6. This substantial increase in enrichment indicates that the model is much more effective at identifying active compounds from a large dataset.\n\nWhile we did not explicitly report confidence intervals for these performance metrics, the statistical significance of our results can be inferred from the consistent improvement across multiple iterations of descriptor optimization. The iterative process of removing the least significant descriptors and retraining the model led to a progressive enhancement in both AUC and enrichment values. This consistent trend suggests that the improvements are statistically significant and not due to random variation.\n\nAdditionally, the root-mean-square deviation (RMSD) between predicted and experimental ln EC50 values was used as an objective function for training the artificial neural networks (ANNs). The RMSD values also showed improvement with descriptor optimization, further supporting the statistical significance of our results.\n\nIn summary, the performance metrics used in our evaluation demonstrate that the optimized model has superior predictive power compared to the baseline model. The consistent improvements in AUC and enrichment values across multiple iterations indicate that the results are statistically significant and that the method is effective for virtual screening applications.",
  "evaluation/availability": "Not enough information is available."
}