{
  "publication/title": "Machine Learning Models Using Routinely Collected Clinical Data Offer Robust and Interpretable Predictions of 90-Day Unplanned Acute Care Use for Cancer Immunotherapy Patients.",
  "publication/authors": "Lu SC, Knafl M, Turin A, Offodile AC 2nd, Ravi V, Sidey-Gibbons C",
  "publication/journal": "JCO clinical cancer informatics",
  "publication/year": "2023",
  "publication/pmid": "37001039",
  "publication/pmcid": "PMC10281452",
  "publication/doi": "10.1200/cci.22.00123",
  "publication/tags": "- Machine Learning\n- Immunotherapy\n- Acute Care Use\n- Predictive Modeling\n- ECOG Score\n- COVID-19 Impact\n- Oncology\n- Data-Driven Analysis\n- Performance Metrics\n- Algorithm Calibration",
  "dataset/provenance": "The dataset used in this study was extracted from the electronic health records (EHR) of patients who initiated immune checkpoint inhibitor (ICI) treatment at a specific cancer center between January 2016 and June 2022. The study included patients diagnosed with solid tumors who received one of the following ICI agents: avelumab, durvalumab, ipilimumab, nivolumab, pembrolizumab, cemiplimab-rwlc, and atezolizumab. Patients who received ICI as part of a blinded randomized ICI/placebo trial were excluded.\n\nThe dataset was split into two main periods: pre-COVID-19 (January 2016 to February 2020) and peri-COVID-19 (March 2020 to June 2022). The pre-COVID-19 dataset was further divided into a training sample and a testing sample with a ratio of 4:1. The training set was used to develop machine learning algorithms, while the testing set was used for internal validation, model calibration, and threshold selection. The peri-COVID-19 dataset was used as an external sample to test the generalizability of the models.\n\nIn total, the study included data from 7,960 patients, with 4,010 in the pre-COVID-19 sample and 3,950 in the peri-COVID-19 sample. For each patient, 269 candidate predictors were collected, covering baseline demographic characteristics, medical history, socioeconomic status, laboratory results, vital signs, comorbidity, physical performance, ICI agent, and prior treatment data. The dataset reflects the fluctuation of laboratory tests, vital signs, and other physiologic variables, enabling the creation of more robust machine learning models.\n\nThe data used in this study is not publicly available due to ethical considerations and restrictions of institutional policy. However, it may be available from the corresponding author upon reasonable request. The programming codes and results generated in this study are available in the main text and appendix.",
  "dataset/splits": "The dataset was divided into two main splits: pre-COVID-19 and peri-COVID-19. The pre-COVID-19 data set was further divided into a training sample and a testing sample with a ratio of 4:1.\n\nThe pre-COVID-19 split contained 4,010 patients, while the peri-COVID-19 split contained 3,950 patients. The pre-COVID-19 training sample consisted of 3,208 patients, and the testing sample consisted of 802 patients.\n\nThe pre-COVID-19 data set was used for modeling, whereas the peri-COVID-19 data set was used as an external sample to test the generalizability of the models. The training set was used to develop machine learning algorithms, and the testing set was used for internal validation, model calibration, and threshold selection.",
  "dataset/redundancy": "The study utilized electronic health records (EHR) data from patients initiating immune checkpoint inhibitor (ICI) treatment between January 2016 and June 2022. To avoid potential inaccuracies from multiple observations of a single patient, the initiation date of the latest ICI treatment was used as the index date for data extraction.\n\nThe data was split into two main datasets: one for the pre-COVID-19 period (January 2016 to February 2020) and another for the peri-COVID-19 period (March 2020 to June 2022). This split was done to account for potential changes in healthcare service utilization patterns due to the COVID-19 pandemic.\n\nThe pre-COVID-19 dataset was further divided into a training set and a testing set with a 4:1 ratio. The training set was used to develop machine learning (ML) algorithms, while the testing set was used for internal validation, model calibration, and threshold selection. This ensures that the training and test sets are independent, as they come from different time periods and the test set was not used in any way during the training process.\n\nThe peri-COVID-19 dataset was used as an external sample to test the generalizability of the models. This dataset was not used in the training process, ensuring that it provides an unbiased evaluation of the models' performance.\n\nThe distribution of the datasets compares favorably to previously published ML datasets in oncology, as it includes a large number of patients and a wide range of predictors. However, it is important to note that the data is from a single institution, which may limit its generalizability to other settings. Additionally, the data is from a specific time period, which may not capture all possible variations in patient characteristics and treatment patterns.",
  "dataset/availability": "The datasets used to train and test the algorithms in this study are not publicly available due to ethical considerations and restrictions of institutional policy. However, they may be available from the corresponding author upon reasonable request. This approach ensures that sensitive patient data is protected while still allowing for potential collaboration and verification of results. All programming codes and results generated in this study are available in the main text and appendix. This transparency allows other researchers to replicate and build upon the findings, promoting further advancements in the field.",
  "optimization/algorithm": "The optimization algorithm used in this study involved training and validating eight widely used machine learning algorithms. These algorithms included logistic regression with elastic net penalty, random forest, extreme gradient boosting tree, support vector machine, k-nearest neighbors, decision tree, multivariate adaptive regression spline, and single hidden layer neural network. These algorithms were selected based on their demonstrated performance in previous work and their representation of varying levels of algorithm complexity and interpretability.\n\nThe algorithms employed are not new; they are well-established in the field of machine learning. The choice to use these algorithms in a medical context, rather than a machine-learning journal, is driven by the specific application and the need to predict unplanned acute care utilization (ACU) for patients with cancer within 90 days of their immune checkpoint inhibitor (ICI) treatment onset. The focus is on applying these algorithms to routinely collected electronic health record (EHR) data to improve patient outcomes and reduce avoidable ACU. The study aims to highlight the practical benefits and relevance of these machine learning techniques in a clinical setting, rather than introducing novel algorithms.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "For the machine-learning algorithms, data preparation involved several preprocessing techniques. Missing data were handled using mode imputation for categorical predictors and median imputation for numerical predictors. Numerical predictors were standardized to equalize their scales and log-transformed for non-normally distributed variables. Categorical predictors with less frequent categories (occurring in fewer than 10% of the training sample) were collapsed into an \"other\" category. Nominal predictors, such as sex, were one-hot encoded to generate binary predictors, with each new predictor representing a category of the original nominal predictor. To enhance model simplicity and performance, predictors were excluded if they had a missing rate higher than 50%, were zero or near-zero variance across the sample, or were highly correlated with one or more predictors (absolute correlation \u22650.90). This thorough preprocessing ensured that the data were in an optimal state for training and validating the machine-learning algorithms.",
  "optimization/parameters": "In our study, the number of parameters (p) used in each model varied depending on the specific machine learning algorithm employed. For instance, the extreme gradient boosting trees (XGBT) model utilized several hyperparameters, including `mtry`, `trees`, `min_n`, `tree_depth`, `learn_rate`, `loss_reduction`, and `sample_size`. Each of these hyperparameters was optimized within predefined search spaces to find the best-performing values.\n\nThe selection of these parameters was conducted through a systematic hyperparameter search process. For example, `mtry` for XGBT was searched within the range of 1 to 298, and the optimal value selected was 21. Similarly, `trees` was searched within 1 to 2000, and the optimal value was found to be 1,799. This process ensured that each hyperparameter was finely tuned to maximize the model's performance.\n\nOther algorithms, such as random forest (RF) and support vector machine (SVM), also had their specific sets of hyperparameters optimized in a similar manner. For RF, `mtry` was searched within 1 to 298, and the optimal value was 16. For SVM, `cost` and `rbf_sigma` were optimized within their respective search spaces.\n\nThis rigorous optimization process was crucial in enhancing the predictive accuracy and robustness of our models. By carefully selecting and tuning these hyperparameters, we aimed to achieve the best possible performance for each algorithm in predicting 90-day unplanned acute care use for cancer immunotherapy patients.",
  "optimization/features": "In our study, we utilized a comprehensive set of candidate predictors to develop robust machine learning models. Specifically, we collected a total of 269 candidate predictors for each patient. These predictors encompassed a wide range of variables, including baseline demographic characteristics, medical history, socioeconomic status, laboratory results, vital signs, comorbidity, physical performance, ICI agent information, and prior treatment data.\n\nTo ensure the robustness of our models, we employed several preprocessing techniques. One key aspect of this preprocessing involved feature selection. We leveraged the longitudinal nature of electronic health record (EHR) data by extracting the means and standard deviations of laboratory tests and vital signs collected within specific time frames before the index date. This approach allowed us to capture the fluctuation of physiological variables, thereby enhancing the models' ability to make accurate predictions.\n\nThe feature selection process was meticulously conducted using only the training set. This ensured that the models were not biased by information from the testing or validation sets, maintaining the integrity of our model evaluation. By focusing on the training set, we aimed to create models that could generalize well to new, unseen data, thereby improving their predictive performance and reliability.",
  "optimization/fitting": "In our study, we employed several machine learning algorithms, each with its own set of hyperparameters that were tuned using a search space. For instance, the extreme gradient boosting trees (XGBT) algorithm had hyperparameters like `mtry`, `trees`, `min_n`, `tree_depth`, `learn_rate`, `loss_reduction`, and `sample_size` that were optimized. Similarly, other algorithms like random forest (RF), support vector machine (SVM), logistic regression with elastic net penalty (LRENP), single hidden layer neural network (SHLNN), k-nearest neighbors (KNN), multivariate adaptive regression splines (MARS), and decision tree (DT) had their respective hyperparameters tuned.\n\nTo address the potential issue of overfitting, we used techniques such as cross-validation and calibration. Specifically, we conducted Platt scaling to calibrate algorithm outputs using the pre\u2013COVID-19 testing sample. This process helped in ensuring that the models did not overfit to the training data. Additionally, we determined thresholds for each algorithm by plotting sensitivity and specificity over a range of thresholds from 0.001 to 1 on the pre\u2013COVID-19 testing sample. The threshold that maximized both sensitivity and specificity was selected, which further aided in mitigating overfitting.\n\nUnderfitting was addressed by ensuring that the models were complex enough to capture the underlying patterns in the data. For example, the XGBT algorithm used a large number of trees (1,799) and a significant tree depth (12), which allowed it to model complex relationships. Similarly, other algorithms like RF and SVM had hyperparameters that were tuned to balance model complexity and performance.\n\nThe performance of our models was evaluated using metrics such as area under the receiver operating characteristic curve (AUROC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) on the peri\u2013COVID-19 sample. All machine learning models significantly outperformed the ECOG univariate model, and several models (LRENP, XGBT, SVM, RF, SHLNN, and KNN) also significantly outperformed the multivariate logistic regression (LR) model. This indicates that our models were neither overfitting nor underfitting the data.\n\nIn summary, we employed a combination of hyperparameter tuning, cross-validation, calibration, and performance evaluation to ensure that our models were neither overfitting nor underfitting the data. The use of these techniques helped in achieving robust and generalizable models.",
  "optimization/regularization": "In our study, we employed several regularization methods to prevent overfitting and enhance the generalization of our models. For the logistic regression with elastic net penalty (LRENP), we used both L1 (lasso) and L2 (ridge) penalties to shrink the coefficients of less important features, thereby reducing model complexity and preventing overfitting. The penalty parameter was tuned within a specified range to find the optimal value that balanced bias and variance.\n\nFor the support vector machine (SVM) model, we utilized a regularization parameter (cost) that controlled the trade-off between achieving a low training error and a low testing error. By adjusting this parameter, we aimed to find a suitable balance that minimized overfitting.\n\nIn the case of the single hidden layer neural network (SHLNN), we applied L2 regularization to the weights, which helped to penalize large weights and encouraged simpler models. Additionally, we set a maximum number of epochs to limit the training process and prevent the model from overfitting to the training data.\n\nFor the extreme gradient boosting trees (XGBT) and random forest (RF) algorithms, we implemented techniques such as setting a minimum number of samples per leaf (min_n) and limiting the maximum depth of the trees (tree_depth). These constraints helped to control the complexity of the individual trees and the ensemble, respectively, thereby reducing the risk of overfitting.\n\nOverall, these regularization techniques were crucial in ensuring that our models generalized well to unseen data and provided robust predictions for 90-day unplanned acute care use in cancer immunotherapy patients.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters for each algorithm are reported in detail. Specifically, the search spaces and optimal values for hyperparameters of various machine learning algorithms, such as extreme gradient boosting trees, random forest, support vector machines, and others, are provided. This information is crucial for replicating the experiments and understanding the model optimization process.\n\nThe code used for this project, including the scripts for data preparation, model training, optimization, and performance examination, is available on GitHub. This repository contains all the necessary programming codes and results generated in the study, ensuring transparency and reproducibility. The GitHub repository is accessible to the public, allowing researchers and practitioners to review, use, and build upon the work.\n\nThe data sets used to train and test the algorithms are not publicly available due to ethical considerations and restrictions of institutional policy. However, they may be available from the corresponding author upon reasonable request. This approach ensures that sensitive patient data is protected while still allowing for potential collaboration and validation of the models.\n\nIn summary, while the hyper-parameter configurations and optimization parameters are thoroughly documented and the code is publicly available, the actual data sets are restricted and can be accessed through the corresponding author. This balance between transparency and data protection is essential for maintaining ethical standards in research.",
  "model/interpretability": "Our model, specifically the extreme gradient boosting tree (XGBT) algorithm, is designed to be transparent and interpretable, rather than a black box. We prioritize providing insights into how the model makes predictions at both the population and individual levels. This approach is crucial for clinical adoption, as clinicians need to understand the underlying features driving the model's predictions to make informed decisions.\n\nOne of the key methods we use for interpretability is the SHAP (SHapley Additive exPlanations) analysis. This technique allows us to visualize the contribution of each variable to the model's output for a specific patient. For example, in a randomly selected patient from the peri\u2013COVID-19 sample, we can see that certain variables, such as the mean of chloride levels or the standard deviation of platelet count, either increase or decrease the risk of acute care utilization (ACU). Variables with red bars in the SHAP plot indicate that their values support a higher ACU risk, while those with blue bars suggest a decreased risk. The length of the bars represents the magnitude of the variable's effect on the final model output.\n\nAdditionally, we provide variable importance plots that show which features have the most significant impact on the model's predictions. This helps clinicians understand which factors are most influential in determining a patient's ACU risk. For instance, variation in red blood cell size and volume, which is not widely documented in the literature, significantly contributes to the accuracy of our XGBT algorithm. This finding highlights the model's ability to identify novel factors associated with ACU.\n\nBy offering these visual explanations and detailed insights into the model's decision-making process, we aim to build trust and facilitate the practical use of our algorithms in real-world clinical settings. This transparency is essential for clinicians to act on the model's predictions and for further research to validate and improve the model's performance.",
  "model/output": "The model discussed in this publication is a classification model. It is designed to predict the likelihood of 90-day unplanned acute care use for cancer immunotherapy patients. The model outputs probabilities that are then calibrated and thresholded to make binary classifications, indicating whether a patient is likely to require acute care or not. Various performance metrics such as AUROC, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) are used to evaluate the model's effectiveness in this classification task. The model's outputs are visualized through calibration plots and other diagnostic tools to ensure reliability and interpretability.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the project is publicly available on GitHub. The repository contains all the programming codes and results generated in the study. The specific link to the code is provided in the appendix of the publication. The repository includes details on data preparation, model training and optimization, and model performance examination and explanation. Additionally, calibration and interpretation details for specific algorithms like extreme gradient boosting trees, random forest, and single hidden layer neural network are available. The code is organized to facilitate reproducibility and further development.",
  "evaluation/method": "The evaluation of the models involved several key steps to ensure robustness and generalizability. Initially, the data was split into pre\u2013COVID-19 and peri\u2013COVID-19 samples. The pre\u2013COVID-19 data set was further divided into a training sample and a testing sample with a 4:1 ratio. The training set was used to develop machine learning algorithms, while the testing set was used for internal validation, model calibration, and threshold selection.\n\nFor model calibration, Platt scaling was applied to the algorithm outputs using the pre\u2013COVID-19 testing sample. Thresholds for each algorithm were determined by plotting sensitivity and specificity over a range of thresholds from 0.001 to 1. The threshold that maximized both sensitivity and specificity was selected for each algorithm.\n\nPerformance metrics, including the area under the receiver operating characteristic curve (AUROC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV), were calculated for each calibrated algorithm using predefined thresholds on the peri\u2013COVID-19 sample. Model calibration was assessed through visual inspection of calibration plots.\n\nDeLong\u2019s test was conducted to determine if the machine learning algorithms performed statistically better than simple logistic regression and the Eastern Cooperative Oncology Group (ECOG)-only logistic regression models. The best-performing algorithm was then examined for its performance on subsamples created from the peri\u2013COVID-19 sample based on immune checkpoint inhibitor (ICI) agent use and cancer conditions.\n\nModel-agnostic techniques, such as variable importance (VI) analysis and Shapley additive explanations (SHAP) analysis, were used to provide intuitive model explanations. VI analysis revealed predictors important to model accuracy, while SHAP analysis helped understand how model outputs were determined by the predictors using a randomly selected case from the peri\u2013COVID-19 sample.",
  "evaluation/measure": "In our study, we evaluated the performance of various machine learning algorithms using several key metrics to ensure a comprehensive assessment. The primary metrics reported include the area under the receiver operating characteristic curve (AUROC), accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). These metrics provide a well-rounded view of each model's performance, covering aspects such as the model's ability to discriminate between positive and negative cases, its overall correctness, and its performance in identifying true positives and true negatives.\n\nThe AUROC is particularly important as it measures the model's ability to distinguish between classes, providing a single scalar value that summarizes the performance across all classification thresholds. Accuracy gives an overall measure of correct predictions, while sensitivity (recall) and specificity focus on the model's performance in identifying positive and negative cases, respectively. PPV and NPV offer insights into the precision of positive and negative predictions, which are crucial for understanding the reliability of the model's outputs in clinical settings.\n\nAdditionally, we calculated true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN) to provide a detailed breakdown of the model's performance. These metrics are essential for understanding how the model performs in real-world scenarios, where the costs of false positives and false negatives can vary significantly.\n\nThe set of metrics used in this study is representative of standard practices in the literature, ensuring that our evaluation is both rigorous and comparable to other studies in the field. By reporting these metrics, we aim to provide a transparent and comprehensive assessment of our models' performance, enabling clinicians and researchers to make informed decisions about their potential applications.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our machine learning algorithms against simpler baselines to benchmark their performance. Specifically, we trained a simple logistic regression (LR) using all predictors and a univariate LR using only the Eastern Cooperative Oncology Group (ECOG) score. The ECOG score is a well-recognized predictor for suboptimal outcomes among patients undergoing immune checkpoint inhibitor (ICI) treatments and is commonly used as a predictive variable when modeling outcomes for these patients.\n\nWe argue that the ECOG score can provide some insight into a patient\u2019s health status and, thus, the risk of acute care use (ACU), although it was not designed specifically for ACU risk assessment. In the absence of any other gold standard metric, the ECOG score was suitable for this purpose.\n\nTo ensure a fair comparison, we examined model calibration for each trained machine learning algorithm and conducted Platt scaling to calibrate algorithm outputs using the pre-COVID-19 testing sample. After calibration, we determined thresholds for each algorithm by plotting sensitivity and specificity over thresholds from 0.001 to 1 on the pre-COVID-19 testing sample. We selected the threshold that maximized both sensitivity and specificity for each algorithm and used these thresholds to determine the final classifications of each algorithm.\n\nWe calculated key performance metrics, including the area under the receiver operating characteristic curve (AUROC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) for each calibrated algorithm using predefined thresholds on the peri-COVID-19 sample. This allowed us to estimate the performance of our algorithms in a real-world setting.\n\nFurthermore, we assessed model calibration using visual inspections of calibration plots and conducted DeLong\u2019s test to determine whether our machine learning algorithms performed statistically better than the simple LR and ECOG-only LR models. This comprehensive evaluation ensured that our algorithms were rigorously compared against simpler baselines, providing a clear understanding of their relative performance.",
  "evaluation/confidence": "The evaluation of our models included several key performance metrics, each accompanied by 95% confidence intervals to provide a range of plausible values for the true metric. This approach allows for a more nuanced understanding of the model's performance, acknowledging the inherent variability in the data.\n\nStatistical significance was assessed using DeLong's test, which determined whether our machine learning algorithms outperformed simpler models, such as logistic regression and the Eastern Cooperative Oncology Group (ECOG) score-only model. The results indicated that all machine learning models significantly outperformed the ECOG univariate model, and several models, including logistic regression with elastic net penalty, extreme gradient boosting trees, support vector machine, random forest, single hidden layer neural network, and k-nearest neighbors, also significantly outperformed the multivariate logistic regression model. This statistical rigor ensures that the observed performance differences are unlikely to be due to chance, thereby bolstering confidence in the superiority of our methods.",
  "evaluation/availability": "The raw evaluation files used to train and test the algorithms in this study are not publicly available due to ethical considerations and restrictions of institutional policy. However, they may be available from the corresponding author upon reasonable request. This approach ensures that sensitive patient data is protected while still allowing for potential verification or further research by other interested parties. All programming codes and results generated in this study are available in the main text and the appendix. This transparency allows for reproducibility and further validation of the methods and findings presented."
}