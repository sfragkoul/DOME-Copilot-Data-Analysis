{
  "publication/title": "Network-based drug sensitivity prediction.",
  "publication/authors": "Ahmed KT, Park S, Jiang Q, Yeu Y, Hwang T, Zhang W",
  "publication/journal": "BMC medical genomics",
  "publication/year": "2020",
  "publication/pmid": "33371891",
  "publication/pmcid": "PMC7771088",
  "publication/doi": "10.1186/s12920-020-00829-3",
  "publication/tags": "- Drug sensitivity prediction\n- Gene expression data\n- Machine learning\n- Bioinformatics\n- Precision oncology\n- Statistical methods\n- Data-driven analytics\n- Biomedical genomics\n- Network-based feature selection\n- Partial Least Squares Regression\n- Support Vector Regression\n- Random Forest\n- Elastic Net\n- Deep Neural Networks\n- Python programming\n- Drug response values\n- mRNA expression\n- Predictive modeling\n- Computational biology\n- Genomic data analysis",
  "dataset/provenance": "The dataset used in our study is an RNA-seq gene expression dataset of 144 non-small cell lung cancer (NSCLC) cell lines. This dataset has been previously utilized in other studies, as indicated by the citation [34]. The cell lines were screened using the same set of 50 drugs, and the area under the dose-response curve (AUC) and median effective dose (ED50) scores for each drug on each cell line are available. The data underwent significant preprocessing steps to ensure compatibility. Genes with low expression or low variance were filtered out, and missing values were handled by replacing them with the mean expression of the respective gene or filtering out the gene if more than 10% of the values were missing. Similarly, drugs with uniform response values across more than 80% of the cell lines were excluded, resulting in a final set of 50 drugs. This dataset provides a robust foundation for evaluating the performance of various feature selection methods and prediction models in the context of drug sensitivity prediction.",
  "dataset/splits": "The dataset used in this study consists of 144 non-small cell lung cancer (NSCLC) cell lines, which were screened using the same set of 50 drugs. The dataset underwent significant preprocessing to ensure compatibility between gene expression and drug response data.\n\nFor the drug sensitivity prediction task, the dataset was split into training and test sets. Specifically, 70% of the cell lines were used as the training set, and the remaining 30% were used as the test set. This split was chosen due to the limited number of cell lines available. The training set was used to select features and train the prediction models, while the test set was used to evaluate the performance of these models.\n\nThe feature selection process involved selecting 100 genes from the training set using different methods. For the network-based feature selection method, the top 100 genes were chosen based on their importance scores. For the correlation-based approach, the genes with the top 100 correlation coefficients between drug response and gene expression were selected.\n\nThe prediction performance was measured using Pearson correlation coefficients between the predicted drug response values and the true values (AUC). This process was repeated 50 times for each drug and each prediction algorithm to ensure robustness and comparability of the results. The same training and test sets were used for all methods in each split to maintain consistency.",
  "dataset/redundancy": "The dataset used in our study consisted of 144 non-small cell lung cancer (NSCLC) cell lines, with RNA-seq gene expression data and drug response information for 50 different drugs. To evaluate the prediction performance, the dataset was split into training and test sets. Given the limited number of cell lines, we directly split the data into 70% for training and 30% for testing. This split was repeated 50 times for each drug to ensure robustness and reliability of the results.\n\nTo maintain independence between the training and test sets, we ensured that the same setup of training and test sets was used for all methods in each splitting. This approach helped in making the prediction results comparable among different feature selection methods and prediction algorithms.\n\nThe distribution of our dataset is comparable to other machine learning datasets in the field of drug sensitivity prediction. The preprocessing steps, such as filtering out genes with low expression or low variance and handling missing values, ensured that the data was compatible and of high quality for analysis. Additionally, drugs with uniform response values across more than 80% of the cell lines were filtered out to avoid redundancy and ensure meaningful predictions.\n\nBy following these steps, we aimed to create a robust and reliable dataset for evaluating the performance of different feature selection methods and prediction models in drug sensitivity prediction.",
  "dataset/availability": "The source code used in this study is publicly available on GitHub. The datasets utilized and analyzed during the current study can be obtained from the corresponding author upon reasonable request. This approach ensures that the data is accessible to other researchers while maintaining control over its distribution. The datasets are not publicly released in a forum, but they are made available to interested parties who request them, following a reasonable request policy. This method allows for the sharing of data while respecting any necessary confidentiality or proprietary considerations.",
  "optimization/algorithm": "The optimization algorithm used in our study is not a novel machine-learning algorithm. Instead, we employed established regression techniques to evaluate the discriminative power of features identified by our network-based feature selection model. Specifically, we compared our methods with four canonical prediction algorithms: Random Forest, Support Vector Regression (SVR), Elastic Net, and Partial Least Squares Regression (PLSR). Additionally, we included a fully connected deep neural network (DNN) in our comparison.\n\nRandom Forest is a nonlinear multiple regression approach that generates multiple regression trees through bootstrap sampling and outputs the mean prediction of individual trees. SVR is a kernel-based method characterized by Vapnik-Chervonenkis control of the margin and the number of support vectors. Elastic Net is a regularized regression method that combines L1 and L2 penalties. PLSR is a statistical method that projects both independent variables (mRNA expression) and predicted variables (drug response values) into a new space to find a linear model between them.\n\nThe DNN used in our study is a two-hidden-layer fully-connected feedforward neural network model. We utilized ReLU as the activation function for both hidden layers and Softmax for the output layer. Each neuron in the input layer represents the expression of one gene across all cell lines.\n\nThese algorithms were chosen for their robustness and widespread use in similar predictive tasks. The comparison aimed to demonstrate the effectiveness of our network-based feature selection model in improving drug sensitivity prediction performance. The results showed that while graph-based neural network models improved prediction performance compared to DNN, the overall performance was still worse than the canonical prediction methods due to issues like overfitting and high-variance gradients in high-dimensional, low-sample-size data.",
  "optimization/meta": "The model described in the publication does not function as a meta-predictor. Instead, it employs a network-based feature selection method and graph-based neural network models for drug sensitivity prediction. The comparison involves several canonical prediction algorithms, including Random Forest, Support Vector Regression (SVR), Elastic Net, and Partial Least Squares Regression (PLSR), as well as a deep neural network (DNN). These methods are used to evaluate the discriminative power of the features identified by the network-based feature selection model.\n\nThe network-based feature selection method identifies representative features for drug response prediction by utilizing gene co-expression networks. This approach integrates gene network information directly into the neural network for outcome prediction. The performance of these models is compared against the canonical prediction algorithms and the DNN on a non-small cell lung cancer (NSCLC) cell line RNA-seq gene expression dataset with 50 different drug treatments.\n\nThe training and test sets are split directly from the dataset, with 70% used for training and 30% for testing. This setup ensures that the same training and test sets are used for all methods in each splitting, maintaining consistency in the evaluation process. The Pearson correlation coefficients between predicted drug response values and true values are used to measure performance, and the results are statistically significant for most methods except the graph-based DNN.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for ensuring compatibility and quality of the datasets used for drug sensitivity prediction. We began with a non-small cell lung cancer (NSCLC) cell line RNA-seq gene expression dataset, which included 144 cell lines screened with 50 different drugs. The gene expression data, denoted as X, consisted of m genes and n cell lines, where each element xi represented the expression of the i-th gene across all samples.\n\nTo preprocess the data, we first filtered out genes with low expression or low variance, as these were unlikely to contribute meaningful information. For genes with numerical values for more than 90% of the cell lines, any remaining NaNs were replaced with the mean expression of that gene. Genes that did not meet this criterion were filtered out. Similarly, drugs that had the same response value for more than 80% of the cell lines were also removed to ensure variability in the drug response data.\n\nThe drug response information, which included the area under the dose-response curve (AUC) and the median effective dose (ED50), was used as the target dataset to measure cell line sensitivity. This information was denoted as y, where yj represented the response of the j-th cell line to the drug. The drug sensitivity prediction was formulated as a regression problem, where the goal was to predict the drug response of test cell lines based on their gene expression data and known drug response information.\n\nFor the machine-learning algorithms, the gene expression data X was used as the input features, and the drug response data y was used as the target variable. The data was split into training and test sets, with 70% of the cell lines used for training and 30% used for testing. This split was repeated 50 times to ensure robust evaluation of the models. The features selected for the models included the top 100 genes based on importance scores from the network-based feature selection method or the top 100 correlation coefficients from the Pearson correlation-based approach.\n\nIn summary, the data encoding involved representing gene expression and drug response information in a structured format suitable for regression analysis. Preprocessing steps included filtering low-variance genes and drugs with uniform responses, as well as handling missing values, to ensure the quality and reliability of the datasets used in our machine-learning models.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the specific method employed. For the Support Vector Regression (SVR) model, the parameter C was fixed to 1, and \u03b3 was set to 1 divided by the product of the number of features in X and the variance of X. For the Partial Least Squares Regression (PLSR) model, the number of latent components, c, was fixed to 1. In the Elastic Net model, the mixing parameter \u03b1 was fixed to 0.5, and the regularization strength was selected based on the deviance likelihood ratio. The Random Forest model used 500 trees and selected 50 random features for node splitting from all the features. For the Deep Neural Network (DNN), the architecture consisted of two hidden layers with ReLU activation functions, and the output layer used a Softmax activation function. The specific number of neurons in each layer was not explicitly stated, but the input layer had as many neurons as there were genes in the dataset. The selection of these parameters was based on standard practices and previous literature to ensure robust and comparable performance across different methods.",
  "optimization/features": "In the optimization process, feature selection was indeed performed. The dataset used consisted of 144 non-small cell lung cancer (NSCLC) cell lines, and the initial gene expression data underwent significant preprocessing. This included filtering out genes with low expression or low variance, and handling missing values by replacing them with the mean expression of the respective gene if more than 90% of the cell lines had numerical values for that gene. Drugs with the same response value from more than 80% of the cell lines were also filtered out, resulting in a final set of 50 drugs.\n\nFor the feature selection, 100 genes were selected from the training set using two different methods: network-based feature selection and Pearson correlation coefficients. The network-based method selected genes based on the top 100 importance scores, while the correlation-based approach selected genes with the top 100 correlation coefficients between drug response and gene expression. This selection process was performed using only the training set, ensuring that the test set remained independent for performance evaluation.\n\nTherefore, the number of features (f) used as input for the prediction models was 100, and feature selection was conducted using the training set only.",
  "optimization/fitting": "In our study, we employed several machine learning algorithms for drug sensitivity prediction, each with its own set of parameters and potential for overfitting or underfitting.\n\nFor the Random Forest regression, we grew 500 trees in the forest and selected 50 random features for node splitting. This ensemble method helps to reduce overfitting by averaging the predictions of multiple trees. Additionally, the use of bootstrap sampling ensures that each tree is trained on a different subset of the data, further mitigating overfitting.\n\nThe Elastic Net model incorporates both L1 and L2 regularization, which helps to prevent overfitting by penalizing large coefficients. We fixed the alpha parameter at 0.5, balancing the contributions of L1 and L2 penalties. The lambda parameter was selected based on the deviance likelihood ratio, ensuring an optimal trade-off between bias and variance.\n\nSupport Vector Regression (SVR) with a Radial Basis Function (RBF) kernel was used, with the regularization parameter C fixed at 1. The gamma parameter was set to 1 divided by the product of the number of features and the variance of the feature matrix X. These settings help to control the complexity of the model and prevent overfitting.\n\nPartial Least Squares Regression (PLSR) was implemented with a single latent component (c = 1). PLSR inherently reduces the dimensionality of the feature space, which can help to mitigate overfitting. However, with a single latent component, there is a risk of underfitting. To address this, we ensured that the selected features had high discriminative power, as identified by our network-based feature selection model.\n\nThe Deep Neural Network (DNN) consisted of two hidden layers with ReLU activation functions. To prevent overfitting, we used a relatively simple architecture and monitored the performance on a validation set. However, due to the high dimensionality and low sample size of our data, the DNN models suffered from overfitting and high-variance gradients. A larger sample size would be needed to further improve the prediction performance and reduce overfitting.\n\nIn summary, we employed various regularization techniques and model selection strategies to address overfitting and underfitting in our prediction models. The use of ensemble methods, regularization, and dimensionality reduction helped to control the complexity of the models and improve their generalization performance. However, the high dimensionality and low sample size of our data posed challenges for the DNN models, highlighting the need for larger datasets in future studies.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and improve the generalization of our models. For the Elastic Net regression, we used a combination of L1 and L2 regularization, which helps in both feature selection and preventing overfitting. The L1 penalty encourages sparsity in the model by driving some coefficients to zero, while the L2 penalty helps in shrinking the coefficients, reducing the model complexity.\n\nAdditionally, for the Support Vector Regression (SVR) model, we utilized the Radial Basis Function (RBF) kernel with a regularization parameter C. This parameter controls the trade-off between achieving a low training error and a low testing error, helping to prevent overfitting.\n\nIn the context of our deep neural network (DNN) model, we applied dropout regularization. Dropout is a technique where, during training, a random subset of neurons is temporarily removed from the network. This forces the network to learn redundant representations and prevents it from becoming too reliant on any single neuron, thereby reducing overfitting.\n\nFurthermore, we ensured that our models were evaluated using a separate test set that was not used during training. This approach helps in assessing the true performance of the models on unseen data, providing a more reliable estimate of their generalization capabilities.",
  "optimization/config": "The source code used in this study is available at a public repository. This includes the configurations and parameters used for the optimization of the models discussed. The repository contains the necessary details to replicate the experiments and understand the hyper-parameter settings, optimization schedules, and model files. The code is open-source, allowing researchers to access and utilize it under the terms specified in the repository. This transparency ensures that the methods and results can be verified and built upon by the scientific community.",
  "model/interpretability": "The models presented in this study encompass a mix of interpretability, ranging from more transparent to black-box approaches. The canonical prediction algorithms, such as Random Forest, Elastic Net, Support Vector Regression (SVR), and Partial Least Squares Regression (PLSR), offer varying degrees of interpretability.\n\nRandom Forest, for instance, is relatively interpretable. It provides feature importance scores, indicating which genes contribute most to the predictions. This allows for an understanding of which genes are significant in drug sensitivity prediction. Elastic Net also offers interpretability through its regularization terms, which can highlight important features by shrinking less important coefficients to zero.\n\nSVR, while powerful, is less interpretable due to its reliance on kernel functions and support vectors. However, the use of the Radial Basis Function (RBF) kernel can provide some insight into the relationships between features.\n\nPLSR projects both independent variables (gene expressions) and predicted variables (drug responses) into a new space, making it somewhat interpretable. The latent components derived from PLSR can be analyzed to understand the underlying structure of the data.\n\nIn contrast, the deep neural network (DNN) and graph-based neural network models, including the network-based embedding method and the graphical neural network (GNN), are more black-box in nature. These models integrate complex, non-linear relationships within their layers, making it challenging to interpret the exact contributions of individual features. However, the use of ReLU activation functions and the layer-wise propagation rules in the GNN provide some structural insight.\n\nThe network-based feature selection model, which identifies representative features for drug response prediction using gene co-expression networks, adds another layer of interpretability. By focusing on the relationships between genes, this model can highlight biologically meaningful gene signatures that are enriched in specific pathways, such as metabolic pathways for certain drugs.\n\nOverall, while some models offer clear examples of interpretability through feature importance and regularization, others remain more opaque, requiring additional techniques for interpretation.",
  "model/output": "The model is a regression model. The objective is to predict drug sensitivity, which is defined as a regression problem. The target data set measures the sensitivity of cell lines, denoted by drug response information such as the area under the dose response curve (AUC) and median effective dose (ED50). The model aims to predict the drug response information of test cell lines based on gene expression data and known drug response information.\n\nThe output of the model can be considered as a new feature matrix for drug sensitivity prediction. In the framework, the ReLU activation function and mean squared error (MSE) loss function are used. The model's performance is evaluated using correlation coefficients between the predicted and true AUC scores.\n\nSeveral prediction algorithms, including Random Forest, Support Vector Regression (SVR), Elastic Net, Partial Least Squares Regression (PLSR), and a fully connected deep neural network (DNN), are applied to evaluate the discriminative power of the features identified by the network-based feature selection model. The results indicate that the model's performance varies depending on the drug and the feature selection method used.",
  "model/duration": "The execution time for the models varied depending on the feature selection method used. Correlation-based feature selection was relatively quick, taking approximately 0.30 seconds of CPU time per iteration. In contrast, network-based feature selection was more time-intensive, requiring about 1.16 seconds per iteration. The predictive algorithm's running time was generally insignificant compared to the feature selection step, except for random forest and deep neural network models. For instance, the support vector regression classifier took around 0.0017 seconds for a single prediction, regardless of whether correlation-based or network-based feature selection was employed. The feature selection step's duration remained consistent, around 0.30 seconds for correlation-based methods, irrespective of the classifier used. The computations were performed using an Intel Core i7-8700 CPU @ 3.20GHz.",
  "model/availability": "The source code for this study is publicly available. It can be accessed via GitHub at the repository named \"Drug-sensitivity-prediction\". This repository contains the necessary code to replicate the analysis and methods described in the publication. The specific URL for the repository is provided in the publication. However, no executable, web server, virtual machine, or container instance is released.",
  "evaluation/method": "The evaluation of our methods involved a comprehensive drug sensitivity prediction task using a dataset of 144 non-small cell lung cancer (NSCLC) cell lines. The dataset included RNA-seq gene expression data and drug response values, specifically AUC and ED50 scores, for 50 drugs. To ensure compatibility, the data underwent significant preprocessing steps, including filtering out genes with low expression or variance and handling missing values.\n\nThe dataset was split into a training set (70%) and a test set (30%). For each feature selection method, 100 genes were selected from the training set. Network-based feature selection identified genes based on importance scores, while the correlation-based approach selected genes with the top correlation coefficients. The prediction performance was then measured on the test set using five different algorithms: Elastic Net, Partial Least Squares Regression (PLSR), Random Forest, Support Vector Regression (SVR), and Deep Neural Network (DNN).\n\nTo ensure robustness, the random splitting of the dataset was repeated 50 times for each drug and each algorithm. This resulted in 2500 repeats in total. The performance was evaluated using Pearson correlation coefficients between the predicted and true drug response values (AUC). The average Pearson correlation coefficients for each prediction method were reported, along with p-values to indicate statistical significance.\n\nThe results showed that all methods except the graph-based DNN produced statistically significant results (p-value < 0.05). Notably, the network-based feature selection method outperformed the correlation-based approach on four canonical prediction methods. While the correlation-based method performed better on DNN, the network-based method had a higher median value, indicating better performance on more drugs. Among the canonical methods, Random Forest demonstrated the best overall performance. The limited number of cell lines suggested that DNN might benefit from a larger sample size for improved performance.",
  "evaluation/measure": "In our study, we primarily used the Pearson correlation coefficient to evaluate the prediction performance of drug responses. This metric was applied to measure the correlation between predicted drug response values (AUC and ED50) and the true response values. The average Pearson correlation coefficients across multiple repeats (50 splittings for each drug and 50 drugs in total) for each prediction method were reported. This approach allowed us to assess the accuracy and consistency of our models' predictions.\n\nAdditionally, we reported p-values for each method to determine the statistical significance of the results. Methods that produced statistically significant results (p-value < 0.05) were highlighted, indicating their reliability and robustness.\n\nWe also provided detailed performance results for the top-20 drugs, which were selected based on their performance across all methods. This allowed us to compare the effectiveness of different prediction algorithms and feature selection methods on specific drugs.\n\nThe use of Pearson correlation coefficients is a standard practice in the literature for evaluating the performance of predictive models in similar contexts. This metric is representative and widely accepted for assessing the accuracy of predictions in high-dimensional biological data, such as gene expression and drug response datasets.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of our proposed methods with several publicly available and widely used prediction algorithms. We evaluated these methods on a benchmark dataset consisting of 144 non-small cell lung cancer (NSCLC) cell lines, which were screened by the same set of 50 drugs. This dataset underwent significant preprocessing to ensure compatibility between gene expression and drug response data.\n\nWe compared our methods against four canonical prediction algorithms that were used in the DREAM 7 Drug Sensitivity Prediction Challenge: Random Forest, Support Vector Regression (SVR), Elastic Net, and Partial Least Squares Regression (PLSR). Additionally, we included a fully connected deep neural network (DNN) in our comparison. These algorithms were applied to evaluate the discriminative power of the features identified by our network-based feature selection model.\n\nTo ensure a fair comparison, we used the same training and test setups for all methods. The dataset was split into 70% training and 30% testing. We selected 100 genes in the training set using each feature selection method. For the network-based feature selection, the top 100 genes were chosen based on their importance scores, while for the correlation-based approach, genes with the top 100 correlation coefficients were selected. The drug sensitivity performance was then measured on the test set.\n\nWe repeated the random splitting 50 times for each algorithm and each drug to make the prediction results comparable among different feature selection methods and prediction algorithms. The Pearson correlation coefficients between the predicted drug response values and the true values (AUC) were used to measure the performance. The average Pearson correlation coefficients of the 2500 repeats (50 splittings for each drug and 50 drugs in total) for each prediction method were reported, along with the p-value for each method.\n\nOur results showed that prediction using all the methods except the graph-based DNN produced statistically significant results (p-value < 0.05). The genes selected by the network-based feature selection method performed better than those selected by Pearson correlation coefficients on four canonical prediction methods. However, the correlation-based method performed better on DNN. The median value of the network-based method was higher than the correlation-based method, indicating that the network-based method performed better on more cases among the 50 drugs.\n\nIn summary, we performed a thorough comparison with publicly available methods and simpler baselines on a benchmark dataset, providing a robust evaluation of our proposed approaches.",
  "evaluation/confidence": "The evaluation of our methods included a thorough statistical analysis to ensure the reliability and significance of our results. We employed Pearson correlation coefficients to estimate prediction accuracy, and these coefficients were used to measure the performance of our models. The results were evaluated using p-values to determine statistical significance.\n\nFor the comparison of different prediction methods, we repeated the random splitting of the dataset 50 times for each drug and each algorithm. This extensive repetition allowed us to calculate average Pearson correlation coefficients across 2500 repeats, providing a robust measure of performance. The p-values associated with these correlations indicated that, except for the graph-based DNN, all methods produced statistically significant results (p-value < 0.05). This statistical significance is crucial in claiming that our methods are superior to others and baselines.\n\nAdditionally, we compared the performance of network-based feature selection methods against correlation-based approaches. The difference in performance between these two methods was found to be statistically significant (p-value < 0.01), further validating the superiority of the network-based method. The results showed that network-based feature selection performed better than correlation-based methods across four canonical prediction methods, although correlation-based methods performed slightly better with DNN.\n\nIn summary, our evaluation included rigorous statistical testing, ensuring that the performance metrics have confidence intervals and that the results are statistically significant. This thorough approach allows us to confidently claim the superiority of our methods over baselines and other comparative approaches.",
  "evaluation/availability": "The datasets used and analyzed during the current study are available from the corresponding author upon reasonable request. This approach ensures that the data can be accessed by other researchers for verification or further study, promoting transparency and reproducibility in scientific research. However, the raw evaluation files are not publicly released."
}