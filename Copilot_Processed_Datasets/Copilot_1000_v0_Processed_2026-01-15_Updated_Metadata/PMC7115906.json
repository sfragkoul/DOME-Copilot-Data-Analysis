{
  "publication/title": "A CNN-aided method to predict glaucoma progression using DARC (Detection of Apoptosing Retinal Cells).",
  "publication/authors": "Normando EM, Yap TE, Maddison J, Miodragovic S, Bonetti P, Almonte M, Mohammad NG, Ameen S, Crawley L, Ahmed F, Bloom PA, Cordeiro MF",
  "publication/journal": "Expert review of molecular diagnostics",
  "publication/year": "2020",
  "publication/pmid": "32310684",
  "publication/pmcid": "PMC7115906",
  "publication/doi": "10.1080/14737159.2020.1758067",
  "publication/tags": "- Glaucoma\n- Retinal Cell Apoptosis\n- DARC (Detection of Apoptosing Retinal Cells)\n- Convolutional Neural Networks (CNN)\n- MobileNetV2\n- Template Matching\n- OCT (Optical Coherence Tomography)\n- Retinal Imaging\n- Machine Learning in Ophthalmology\n- Biomarker for Glaucoma Progression",
  "dataset/provenance": "The dataset used in this study consists of retinal images from glaucoma patients and control subjects. The images were captured at baseline and 120 minutes later. The dataset includes a total of 985 manually observed DARC spots that were agreed upon by two observers. These spots were used for training and validation purposes, with 70% of the spots used for training and 30% for validation. Additionally, the retinal images of the remaining 50% of control patients were used to test the classification accuracy, which included 48,610 candidate spots, of which 898 were manually observed DARC spots agreed upon by two observers.\n\nThe data was augmented to increase the network's tolerance by rotating, reflecting, and varying the intensity of the spot images. The class weights for DARC spots were set to 50 for spots and 1 for other objects to compensate for the 50-1 unbalanced data.\n\nThe training, validation, and testing were performed at the spot level rather than at the image or eye level. This approach was taken to prevent over-training, given the relatively small sample numbers. The CNN was trained using candidate spots marked as DARC if observed by two or more manual observers. The dataset was split by eye to ensure that the training and testing sets were independent.\n\nThe dataset was specifically curated for this study and has not been used in previous publications or by the community. The focus was on developing a CNN-aided algorithm to analyze DARC as a marker of retinal cell apoptosis in glaucoma patients. The algorithm's performance was evaluated using this dataset, and the results showed high accuracy, sensitivity, and specificity in predicting glaucoma progression.",
  "dataset/splits": "There were three main data splits: training, validation, and testing.\n\nThe training set consisted of 70% of the manually observed DARC spots, amounting to a significant number of images. The validation set comprised 30% of these manually observed spots. For the testing phase, the retinal images of the remaining 50% of control patients were used, which included 48,610 candidate spots, of which 898 were manually observed DARC spots.\n\nThe data was augmented to enhance the network's tolerance by rotating, reflecting, and varying the intensity of the spot images. Additionally, class weights were set to 50 for DARC spots and 1 for other objects to address the 50-1 data imbalance.\n\nThree training runs were conducted, resulting in three CNN models. For inference, these models were combined, with each spot classified based on the mean probability given by the three models. This approach ensured a robust and reliable classification process.",
  "dataset/redundancy": "The datasets were split by eye to ensure independence between training and test sets. This approach was taken to prevent over-training and to maintain the integrity of the evaluation process. Specifically, the training and validation sets were created using spots from 50% of control eyes, while the remaining 50% were reserved for validation and testing. This split was done at the eye level to avoid any overlap between the datasets, ensuring that the model's performance could be accurately assessed on unseen data.\n\nThe distribution of the datasets is notable for its imbalance, with a 50-1 ratio of candidate spots to DARC spots. This imbalance was addressed by setting class weights, with DARC spots given a weight of 50 and other objects a weight of 1. This weighting helped the model to better handle the disparity in the number of samples between the two classes.\n\nThe training process involved using candidate spots marked as DARC if they were observed by at least two manual observers. This criterion ensured a high level of agreement and reliability in the training data. The model was trained using these candidate spots, and its performance was evaluated on the validation set to ensure it generalized well to new data.\n\nThe final testing was conducted on the entire glaucoma dataset, which had not been used in the training or validation phases. This independent test set allowed for an unbiased evaluation of the model's performance. The use of independent datasets at each stage of the process\u2014training, validation, and testing\u2014ensures that the results are robust and not due to overfitting.\n\nIn comparison to previously published machine learning datasets, the approach taken here is rigorous in maintaining the independence of the datasets. This is crucial for developing a reliable model that can be trusted in clinical applications. The use of eye-level splitting and the careful handling of imbalanced data are key aspects that contribute to the model's effectiveness and reliability.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is a Convolutional Neural Network (CNN). Specifically, we employed MobileNetV2, which is a well-established architecture in the field of deep learning for image classification tasks. This choice was made after evaluating the performance of other networks such as VGG16, which, at the time of writing, did not perform as well as MobileNetV2 for our specific needs.\n\nThe algorithm is not new; MobileNetV2 has been previously published and is widely used in the machine learning community. The reason it was not published in a machine-learning journal in this context is that our focus was on applying this established algorithm to a novel problem in ophthalmology\u2014detecting progressive glaucoma 18 months ahead of alternative methods. Our contribution lies in the application and optimization of MobileNetV2 for this specific medical task, rather than the development of a new algorithm.\n\nWe also explored alternative classification algorithms such as Support Vector Machines (SVMs) and Random Forests, but these require \"hand-crafted\" features that are challenging to produce due to the complexities in retinal image capture and biological variations. MobileNetV2, on the other hand, can learn these features directly from the data, making it a more suitable choice for our application.\n\nAdditionally, we considered using YOLO3, a detection and segmentation algorithm that combines spot detection and classification in a single step. While this method shows promise, especially with more data, our current implementation of YOLO3 did not achieve the same level of accuracy as our CNN-aided algorithm.\n\nIn summary, while the algorithm itself is not new, its application to detect DARC spots in retinal images for predicting glaucoma progression is innovative and contributes significantly to the field of ophthalmology.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. Instead, it employs a Convolutional Neural Network (CNN) called MobileNet v2 for spot classification. The CNN is trained and validated using retinal images, specifically focusing on candidate spots identified through template matching. The training and validation sets are split by eye to ensure independence, and the final testing is conducted on the entire glaucoma dataset to avoid over-training.\n\nThree separate training runs were performed to create three CNN models. For inference, these models are combined, and each spot is classified based on the mean probability given by the three models. This approach enhances the robustness and accuracy of the classification process. The use of multiple models ensures that the final predictions are more reliable and less prone to errors associated with a single model. The training, validation, and testing processes are conducted at the spot level, with careful selection at the eye level to prevent over-training and ensure the independence of the datasets.",
  "optimization/encoding": "For the machine-learning algorithm, the data encoding and preprocessing involved several steps to ensure the images were suitable for analysis. Initially, the retinal images were preprocessed to account for lighting differences and to remove high-frequency noise using a Gaussian blur with a sigma of 5 pixels. This step helped in standardizing the images and reducing irrelevant details that could interfere with the detection of DARC spots.\n\nThe spot candidate detection process utilized template matching, specifically Zero Normalised Cross-Correlation (ZNCC). This method involved creating a template from 30x30 pixel images of spots identified by manual observers. The template was then applied to the retinal images to produce a correlation map. Local maxima in this map were selected and filtered using thresholds for the correlation coefficient and intensity standard deviation. These thresholds were set low enough to include all spots observed by manual observers, even those that were very subtle.\n\nThe candidate spots were then classified using a Convolutional Neural Network (CNN) called MobileNet v2. The input layer of the CNN was adapted to accept 64x64 pixel spot candidate images, providing context around each spot. The last layer of the CNN was replaced with a dense layer with sigmoid activation to enable binary classification, determining whether a spot was a DARC spot or not. An alpha value of 0.85 was used for MobileNet, adjusting the number of filters in each layer to optimize performance.\n\nData augmentation techniques were employed to increase the robustness of the network. This included rotating, reflecting, and varying the intensity of the spot images. The class weights for DARC spots were set to 50 for spots and 1 for other objects to compensate for the unbalanced data, addressing the 50-1 ratio of candidate spots to actual DARC spots.\n\nThe training, validation, and testing sets were split by eye to prevent over-training. The CNN was trained using candidate spots marked as DARC if observed by two or more manual observers. This approach ensured that the algorithm was trained on a diverse and representative dataset, enhancing its ability to accurately detect and classify DARC spots in retinal images.",
  "optimization/parameters": "The model utilized in this study is a Convolutional Neural Network (CNN) based on MobileNetV2 architecture. The specific number of parameters (p) in the model is not explicitly stated, as the focus was on the performance metrics rather than the architectural details. However, MobileNetV2 is known for its efficient design, balancing between the number of parameters and computational efficiency.\n\nThe selection of the MobileNetV2 architecture was driven by its performance in handling image classification tasks, particularly in medical imaging. It was chosen after evaluating other networks such as VGG16, which had limitations in batch processing due to its architecture. The decision to use MobileNetV2 was also influenced by its ability to handle the complexities of retinal images, including variations in intensity, optical blur, and biological patterns.\n\nAdditionally, the model's performance was enhanced through data augmentation techniques, which involved rotating, reflecting, and varying the intensity of spot images. This helped in increasing the network's tolerance and robustness. The class weights for DARC spots were set to 50 for spots and 1 for other objects to compensate for the unbalanced data, ensuring that the model could effectively learn from the available data.\n\nThe training process involved 300 epochs, with a good accuracy achieved around 200 epochs. Three training runs were performed to create three CNN models, which were then combined for inference. Each spot was classified based on the mean probability given by the three models, improving the reliability of the classifications.",
  "optimization/features": "The input features for the Convolutional Neural Network (CNN) used in this study are the spot candidate images, which are 64x64 pixel images. This size was chosen to include more of the area around the spot, providing the network with some context. The first layer of the CNN was adapted to be a 64x64x1 input layer to accommodate these images.\n\nFeature selection in the traditional sense was not performed, as the CNN automatically learns relevant features from the input images. However, a template matching method using Zero Normalised Cross-Correlation (ZNCC) was employed to detect spot candidates. This method involved creating a spot template from manually observed spots and applying it to the retinal images to produce a correlation map. Local maxima were then selected and filtered based on thresholds for the correlation coefficient and intensity standard deviation. These thresholds were set to include all spots seen by manual observers, ensuring that the classifier's accuracy reflects its ability to discern DARC spots from other spot-like objects.\n\nThe data was split by eye, with training performed on spots from 50% of control eyes and validation/testing on the remaining 50%. This approach ensured that the selection of training and testing data was done at the eye level, preventing over-training. The CNN was trained using candidate spots marked as DARC if observed by two or more manual observers. This method ensured that the input features were relevant and that the network could learn to distinguish DARC spots from other objects in the retinal images.",
  "optimization/fitting": "The fitting method employed in this study involved training a Convolutional Neural Network (CNN) to classify spot candidates in retinal images. The CNN used was MobileNet v2, which was adapted for this specific task. The input layer was modified to accept 64x64x1 pixel spot candidate images, and the last layer was replaced with a dense layer with sigmoid activation for binary classification.\n\nThe training process involved 58,730 spot candidates, including 985 manually observed DARC spots. These spots were split into training and validation sets, with 70% used for training and 30% for validation. Additionally, data augmentation techniques such as rotation, reflection, and intensity variation were applied to increase the network's tolerance and robustness.\n\nTo address the issue of overfitting, the training and validation sets were split by eye, ensuring that the network did not learn to recognize specific eyes but rather the general features of DARC spots. Training was performed for 300 epochs, although a good accuracy of 97% was achieved in 200 epochs. The matching validation accuracy showed similar performance without signs of overfitting. Furthermore, three separate training runs were performed to create three CNN models, and for inference, the models were combined by taking the mean probability given by each model. This ensemble approach helped to reduce the risk of overfitting.\n\nUnderfitting was mitigated by using a sufficiently complex model (MobileNet v2) and by augmenting the data to provide a diverse range of examples. The network's performance was evaluated using sensitivity and specificity metrics, which were found to be 91.1% and 97.1%, respectively. These high values indicate that the model was able to generalize well to the validation data, suggesting that underfitting was not a significant issue.\n\nIn summary, the fitting method involved careful splitting of training and validation data, extensive data augmentation, and the use of an ensemble of models to ensure robust performance and to rule out overfitting and underfitting.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our Convolutional Neural Network (CNN) model. One of the primary methods used was data augmentation. This involved rotating, reflecting, and varying the intensity of the spot images. This process helped the network to generalize better by exposing it to a wider variety of image variations, thereby increasing its tolerance to different lighting conditions and other image artifacts.\n\nAdditionally, we used class weights to address the imbalance in our dataset. The DARC spots class weights were set to 50 for spots and 1 for other objects, which compensated for the 50-1 unbalanced data. This weighting scheme ensured that the model did not become biased towards the more frequent class, thereby improving its performance on the minority class.\n\nAnother crucial step was the splitting of the dataset. The training, validation, and testing sets were split by eye and then trained at the spot level. This approach helped in preventing overfitting by ensuring that the model did not learn specific patterns from individual eyes but rather generalized patterns from the entire dataset.\n\nFurthermore, we performed multiple training runs to create three separate CNN models. For inference, these models were combined, and each spot was classified based on the mean probability given by the three models. This ensemble approach further enhanced the model's robustness and reduced the risk of overfitting.\n\nThe training process was also monitored closely. Although a good accuracy was achieved in 200 epochs, the training was continued for 300 epochs to verify the stability of the model. This additional training helped in ensuring that the model did not overfit to the training data and maintained its performance on the validation set.\n\nIn summary, data augmentation, class weighting, dataset splitting, ensemble modeling, and extended training epochs were all employed to prevent overfitting and enhance the generalization capability of our CNN model.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we utilized MobileNet v2 for our Convolutional Neural Network (CNN) architecture, with modifications to the first and last layers to accommodate our specific needs. The first layer was adapted to a 64x64x1 input layer to process 64x64 pixel spot candidate images, providing context around each spot. The last layer was replaced with a dense layer featuring a sigmoid activation function to facilitate binary classification, distinguishing between DARC spots and other objects.\n\nThe alpha value for MobileNet was set to 0.85, which was determined to be optimal for our dataset. This value adjusts the number of filters in each layer, ensuring the network can handle the 50-1 unbalanced data effectively. Training was conducted over 300 epochs, although a good accuracy of 97% was achieved by the 200th epoch. This extended training period was to verify the stability of the model.\n\nThe model files and optimization parameters are not explicitly provided in the publication. However, the methods and configurations described are sufficient for replication. The data augmentation techniques, including rotation, reflection, and intensity variation, are also outlined to enhance the network's tolerance and performance.\n\nFor those interested in replicating or building upon our work, the detailed descriptions of the training process, validation methods, and performance metrics should serve as a comprehensive guide. The publication is available under the terms that allow for academic use and further research, ensuring that the methods and findings can be utilized by the scientific community.",
  "model/interpretability": "The model employed in this study is primarily a Convolutional Neural Network (CNN), specifically MobileNetV2, which is generally considered a black-box model. This means that while the model can provide highly accurate predictions, the internal workings and the specific features it uses to make these predictions are not easily interpretable.\n\nHowever, there are aspects of the process that offer some transparency. The initial step of spot candidate detection uses a template-matching method called Zero Normalised Cross-Correlation (ZNCC). This method creates a correlation map by comparing a template of known spots to the retinal image, making it somewhat interpretable. The local maxima in this correlation map are selected as candidate spots, which are then classified by the CNN. This two-step process\u2014template matching followed by CNN classification\u2014provides a clearer understanding of how candidate spots are identified before the black-box classification occurs.\n\nAdditionally, the data augmentation techniques used, such as rotating, reflecting, and varying the intensity of spot images, help in understanding how the model is trained to handle different variations in the input data. The setting of class weights to compensate for unbalanced data also provides insight into how the model is calibrated to handle imbalanced datasets.\n\nThe use of multiple training runs and combining the models for inference adds another layer of interpretability. By averaging the probabilities from three different models, the final classification becomes more robust and less dependent on the idiosyncrasies of a single model.\n\nIn summary, while the core classification model is a black-box CNN, the overall process includes interpretable steps that provide some transparency into how the model identifies and classifies DARC spots.",
  "model/output": "The model developed is a classification model. It is designed to identify and classify spots in retinal images as either DARC spots or not. The Convolutional Neural Network (CNN) used, MobileNet v2, was adapted for binary classification with a sigmoid activation function in the final layer. This allows the model to output a probability indicating whether a given spot is a DARC spot or not. The training process involved classifying candidate spots as DARC if they were observed by at least two manual observers, which is a binary classification task. The model's performance was evaluated using metrics such as accuracy, sensitivity, and specificity, which are typical for classification models. The final output of the model is a classification of each spot candidate as either a DARC spot or not, based on the mean probability given by three trained CNN models.",
  "model/duration": "The model training process involved 300 epochs, with a good accuracy achieved by the 200th epoch. The training was left to run for the full 300 epochs to verify the stability of the results. The training and validation accuracy curves showed convergence without signs of over-training. The model was trained using a Convolutional Neural Network (CNN) architecture, specifically MobileNet v2, which was adapted for this task. The training was performed at the spot level, using candidate spots marked as DARC if observed by at least two manual observers. The data was augmented to increase the network's tolerance, including rotations, reflections, and variations in spot intensity. Three separate training runs were conducted, creating three CNN models. For inference, these models were combined, with each spot classified based on the mean probability given by the three models. The final testing was done on the entire glaucoma dataset to ensure robustness and avoid over-training. The execution time for the model training and validation is not explicitly stated, but the process involved significant computational effort to achieve the reported accuracy, sensitivity, and specificity.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The method was evaluated using a combination of training, validation, and testing datasets. The retinal images were split by eye, with 50% of control eyes used for training and the remaining 50% for validation and testing. This approach ensured that the evaluation was robust and prevented over-training.\n\nThe training set consisted of 70% of the manually observed DARC spots, while the validation set included the remaining 30%. The classification accuracy was assessed using these datasets, and the training validation accuracy converged without signs of over-training. The matching validation accuracy also showed similar accuracy, indicating the reliability of the model.\n\nFor testing, the retinal images of the remaining 50% of control patients were used. This included 48,610 candidate spots, of which 898 were manually observed DARC spots. The testing phase evaluated the classification accuracy of the model on an independent dataset, providing a realistic assessment of its performance.\n\nThe data was augmented to increase the network's tolerance by rotating, reflecting, and varying the intensity of the spot images. This augmentation helped in making the model more robust and generalizable. Additionally, the DARC spots class weights were set to 50 for spots and 1 for other objects to compensate for the 50-1 unbalanced data.\n\nThree training runs were performed, creating three CNN models. For inference, the three models were combined, and each spot was classified based on the mean probability given by each of the three models. This ensemble approach improved the overall accuracy and reliability of the classification.\n\nThe evaluation also included testing on glaucoma DARC images. The CNN-aided algorithm was tested on the glaucoma cohort of patients in images captured at baseline and 120 minutes. Spots were identified by manual observers and the algorithm, and the DARC count was defined as the number of ANX776-positive spots seen in the retinal image at 120 minutes after baseline spot subtraction.\n\nStatistical analysis was performed using GraphPad Prism, SPSS, and Python. Receiver-operating characteristic (ROC) curves were constructed with the area under the curve (AUC), standard errors, maximal sensitivities, and specificities generated for CNN training, validation, and testing data. Interobserver agreements were calculated for manual observer DARC counts and clinician progression status using Cohen\u2019s kappa coefficient. Rates of progression were calculated from serial tests of visual field MDs and VFIs, and OCT RNFL and MRW parameters. Progression was defined by a significant negative slope greater than 1 \u03bcm/year, based on the 5% lower limit for age-related change.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our CNN-aided algorithm in predicting glaucoma progression. The primary metrics reported include sensitivity, specificity, and the area under the curve (AUC) derived from receiver-operating characteristic (ROC) curves. These metrics were calculated for both the CNN training, validation, and testing data, as well as for comparisons with manual observer counts.\n\nThe CNN algorithm achieved a high accuracy of 97%, with a sensitivity of 91.1% and a specificity of 97.1%. These values indicate that the algorithm is highly effective in correctly identifying both positive and negative cases. The ROC curves further supported these findings, with the CNN algorithm demonstrating an AUC of 0.89. This AUC value is notably higher than that achieved by manual observer counts, which had an AUC of 0.79. The maximal sensitivity and specificity for the CNN algorithm were 90.0% and 85.71%, respectively, at a DARC count of 23. In contrast, manual observer counts achieved a maximal sensitivity of 85% and specificity of 71.43% at a DARC count of 12.\n\nThese performance metrics are representative of the current literature on glaucoma detection and progression prediction. Sensitivity and specificity are standard metrics used to evaluate the performance of diagnostic tools, and the AUC provides a comprehensive measure of the algorithm's ability to discriminate between progressing and stable glaucoma cases. The high sensitivity and specificity values, along with the superior AUC, suggest that our CNN-aided algorithm performs well compared to existing methods.\n\nAdditionally, we calculated interobserver agreements using Cohen\u2019s kappa coefficient for manual observer DARC counts and clinician progression status. This metric is crucial for assessing the reliability and consistency of manual observations, which are often subject to variability among different observers.\n\nIn summary, the performance metrics reported in our study are comprehensive and align with established standards in the field. The high sensitivity, specificity, and AUC values demonstrate the robustness and effectiveness of our CNN-aided algorithm in predicting glaucoma progression.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. However, we did evaluate alternative classification algorithms to MobileNetV2, such as Support Vector Machines (SVMs) and Random Forests. These methods require \"hand-crafted\" features, which are challenging to produce due to complexities in image capture and biological variations. We found that these alternative methods were not as effective as MobileNetV2 for our specific task.\n\nWe also compared MobileNetV2 with another Convolutional Neural Network (CNN), VGG16. VGG16 was found to be limited in handling the number of spots in a batch, which could hinder training. Additionally, we explored an alternative method using YOLO3, which detects and classifies spots in a single step. While this method showed potential, the highest accuracy achieved with YOLO3 was not as good as the method outlined in our document.\n\nIn summary, while we did not compare our method directly with publicly available methods on benchmark datasets, we did evaluate and compare several alternative approaches to ensure the robustness and effectiveness of our chosen method.",
  "evaluation/confidence": "The evaluation of our CNN-aided algorithm included the construction of Receiver Operating Characteristic (ROC) curves, which provided metrics such as the area under the curve (AUC), standard errors, maximal sensitivities, and specificities. These metrics were generated for both the CNN training, validation, and testing data, as well as comparisons with manual observer counts.\n\nThe ROC curves demonstrated that the CNN algorithm achieved a maximal sensitivity of 90.0% and specificity of 85.71% at a DARC count of 23, with an AUC of 0.89. In contrast, the manual observer count had a maximal sensitivity of 85% and specificity of 71.43% at a DARC count of 12, with an AUC of 0.79. These results indicate that the CNN-aided algorithm performed superiorly in predicting glaucoma progression.\n\nStatistical significance was assessed using various tests. For instance, rates of progression (RoP) were defined by a significant negative slope greater than 1 \u03bcm/year, based on a p-value threshold of less than 0.05. This threshold was used to distinguish between progressing and stable glaucoma patients. Additionally, comparisons between stable and progressing glaucoma eyes were made using the unpaired t-test, with significance levels indicated by *p<0.05 and **p<0.01.\n\nThe training and validation accuracy of the CNN converged without signs of over-training, achieving a good accuracy in 200 epochs, although training was extended to 300 epochs to verify stability. The matching validation accuracy also showed similar accuracy, further supporting the reliability of the results.\n\nIn summary, the performance metrics included confidence intervals and statistical significance tests, providing a robust evaluation of the CNN-aided algorithm's superiority over manual observer counts and baseline methods. The results are statistically significant, supporting the claim that the method is effective in predicting glaucoma progression.",
  "evaluation/availability": "Not enough information is available."
}