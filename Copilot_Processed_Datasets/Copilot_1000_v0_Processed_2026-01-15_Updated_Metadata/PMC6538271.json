{
  "publication/title": "Biomedical Image Processing with Containers and Deep Learning: An Automated Analysis Pipeline: Data architecture, artificial intelligence, automated processing, containerization, and clusters orchestration ease the transition from data acquisition to insights in medium-to-large datasets.",
  "publication/authors": "Gonz\u00e1lez G, Evans CL",
  "publication/journal": "BioEssays : news and reviews in molecular, cellular and developmental biology",
  "publication/year": "2019",
  "publication/pmid": "31094000",
  "publication/pmcid": "PMC6538271",
  "publication/doi": "10.1002/bies.201900004",
  "publication/tags": "- Biomedical Image Processing\n- Deep Learning\n- Automated Analysis\n- Data Management\n- Artificial Intelligence\n- Containerization\n- Cluster Orchestration\n- Quality Control\n- Data Architecture\n- Microscopy Images\n- Plasmonic Nanoparticles\n- Data Visualization\n- Machine Learning\n- Python\n- R\n- Docker\n- Data Inspection\n- Data Policy\n- Data Processing Pipeline\n- Reproducible Software",
  "dataset/provenance": "The dataset used in our study originates from dark-field microscopy images, specifically focusing on the quantification of plasmonic nanoparticles bound to peripheral blood mononuclear cells. Each experiment consists of approximately 20 fields of view, with each field of view comprising an uncompressed 3D color image that is roughly 650 MB in size. If four experiments are conducted daily, this results in a substantial data collection of nearly 52 GB per day.\n\nThe data acquisition process involves high-throughput devices such as microscopes, which generate large volumes of data. These images are initially stored on the computer attached to the acquisition device and are then immediately archived to a centralized data storage system using automated tools like rsync. This ensures that the data is safely stored and readily accessible for further processing.\n\nThe dataset is structured hierarchically to maximize efficiency, with a defined organization that includes the project name, data type, date of experiments, subject, and experiment details. This hierarchical structure is reflected in the directory system of the laboratory's file server, making it easy to trace and analyze the data en masse post-acquisition.\n\nWhile the specific dataset used in this study is novel and tailored to our experimental setup, the methods and tools developed are applicable to a broader range of biomedical imaging data. The techniques discussed, including deep learning and automated processing pipelines, have been benchmarked against established databases like ImageNet, ensuring their reliability and performance. The dataset's size and complexity make it a valuable resource for advancing automated image analysis in biomedical research.",
  "dataset/splits": "In our work, we utilized two distinct datasets for different tasks, each with its own data splits.\n\nFor the cell detection task, we had a dataset consisting of 7097 fields of view (fovs). These were manually annotated by experts who placed a dot in the central point of the cells. The dataset was split into three parts: 4000 fovs were used for training, 1000 for validation, and 2097 for testing. This split allowed us to train the model effectively, validate its performance during training, and test its generalization on unseen data.\n\nFor the cell contouring task, we used a dataset of 1704 manually segmented maximum intensity projections of 200 \u00d7 200 pixel cell images. This dataset was divided into 800 images for training, 200 for validation, and 704 for testing. This split ensured that we had a sufficient number of images for training the model, validating its performance, and testing its accuracy on new data.\n\nThe distribution of data points in each split was designed to ensure that the model could learn effectively from the training data, validate its performance accurately, and generalize well to new, unseen data. The splits were chosen to balance the need for a large training set with the need for a representative validation and test set.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our work utilizes well-established machine-learning techniques, specifically deep learning neural networks. We did not develop a new machine-learning algorithm from scratch. Instead, we leveraged existing architectures that have proven effective in similar tasks.\n\nFor cell detection, we implemented a Single Shot MultiBox Detector (SSD) network. The SSD network is not new; it has been widely used and validated in the computer vision community. Its choice was driven by its availability in open-source implementations, particularly in Python, and its demonstrated performance on benchmark datasets like ImageNet. The SSD network's simplicity and efficiency make it suitable for our task of detecting cells in fields of view, even in the presence of varying backgrounds and spurious image features.\n\nFor cell contouring, we used a simplified version of the U-Net architecture. U-Net is a well-known encoder-decoder network structure with skip connections, originally designed for biomedical image segmentation. It has been extensively modified and improved since its creation, but our implementation remains relatively straightforward. The U-Net's ability to capture image features such as shapes and edges makes it an ideal choice for segmenting cell contours.\n\nBoth the SSD and U-Net architectures are trained using standard optimization techniques. The SSD network is trained with an adaptive momentum optimizer and a hybrid cost function that combines normalized cross-entropy and mean error for displacement. The U-Net, on the other hand, is optimized using the Adam optimizer and the DICE coefficient as the loss function. These optimization algorithms are widely used in the deep learning community and are implemented in popular machine-learning libraries.\n\nThe reason these algorithms were not published in a machine-learning journal is that they are not novel contributions to the field of machine learning. Instead, they are applications of existing techniques to a specific problem in biomedical image analysis. Our focus is on demonstrating the effectiveness of these established methods in automating the detection and contouring of cells, rather than developing new machine-learning algorithms.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding process involved reducing each input image stack, which initially measured 2496 pixels by 3328 pixels with 40 layers and 3 RGB channels, into a maximum intensity projection. This down-sampling resulted in an image of 624 pixels by 832 pixels with 3 RGB channels. Each cell in these down-sampled images occupies approximately a 20 by 20 pixel window.\n\nThe output of the network was defined as a 39 by 52 grid with 4 channels. The first channel indicated the likelihood of a pixel belonging to the background, while the second channel denoted the likelihood of a pixel belonging to a cell. The third and fourth channels modeled the displacement in normalized units in the x and y directions, respectively.\n\nThe reference standard consisted of similar grids where background pixels had a value of 1 in the first channel and 0 in the second. Pixels with cells had a value of 1 in the second channel and 0 in the first, with the third and fourth channels encoding their normalized displacement in the original image.\n\nThe network was trained using the adaptive momentum optimizer and a hybrid cost function. The first term of this function corresponded to the normalized cross-entropy between the first two channels of the reference standard and the network\u2019s output. The second term, defined only on pixels with cells in the reference standard, represented the mean error between the computed and reference displacements. A lambda factor of 0.1 modulated the weight of correct localization relative to detection.\n\nThe training database comprised 7097 fields of view (fovs) manually annotated by experts, with 4000 fovs used for training, 1000 for validation, and 2097 for testing. A custom non-maxima suppression method was applied to eliminate close-by detections. The Pearson correlation coefficient between the number of detected cells using this method and the number of cells identified by an expert was 0.827. The average error in the test set between the number of cells found in a field of view and the actual number of cells present was 1.7 cells per fov. Most errors occurred with close-by cells or cells partially in the field of view. Improvements in metrics could likely be achieved with a cleaner dataset. The network processed the 2097 testing images in 73 seconds, at a rate of 35 milliseconds per image, using an Nvidia GTX1080Ti graphics card.",
  "optimization/parameters": "In our work, the number of parameters in the model varies depending on the specific network used. For the cell detection task, we employed a Single Shot MultiBox Detector (SSD) network. This network is designed to be relatively lightweight and efficient, which is reflected in its parameter count. The exact number of parameters can depend on the specific configuration and depth of the network, but it is optimized to balance performance and computational efficiency.\n\nFor the cell contouring task, we utilized a simplified version of the U-Net architecture. This network consists of a 3-step encoder path, each with two convolution layers. The number of filters in these layers increases with depth, starting with 32 filters in the first layer, 64 in the second, and 128 in the third. This design allows the network to capture both local and global features effectively. The precise number of parameters in this U-Net configuration can be calculated based on the dimensions of the convolutional layers and the number of filters, but it is designed to be efficient and easy to implement with current AI libraries.\n\nThe selection of these parameters was guided by the need for a balance between model complexity and performance. For the SSD network, the choice was influenced by its proven effectiveness in object detection tasks and its availability in open-source implementations. The U-Net architecture was chosen for its superior performance in image segmentation tasks, particularly its ability to handle varying image features and shapes. The specific configurations, such as the number of filters and layers, were selected based on empirical testing and validation to ensure optimal performance on our datasets.",
  "optimization/features": "The input features for our network consist of image stacks that are initially 2496 pixels by 3328 pixels, with 40 layers and 3 RGB channels. These stacks are processed into down-sampled images of 624 pixels by 832 pixels, retaining the 3 RGB channels. Each cell in these down-sampled images occupies roughly a 20 by 20 pixel window.\n\nFeature selection in the traditional sense was not performed, as the input features are directly derived from the image data. The images are preprocessed to create maximum intensity projections, which serve as the input features for the network. This preprocessing step is applied uniformly to all images, ensuring that the same features are used for training, validation, and testing.\n\nThe training database consists of 7097 fields of view (fovs), with 4000 fovs used for training, 1000 for validation, and 2097 for testing. The preprocessing and feature extraction steps are consistent across all datasets, ensuring that the features used in training are representative of those encountered in validation and testing. This approach helps in maintaining the integrity and generalizability of the model.",
  "optimization/fitting": "In our work, we employed deep learning networks for two primary tasks: cell detection and cell contouring. For cell detection, we utilized a Single Shot MultiBox Detector (SSD) network, while for cell contouring, we used a simplified version of the U-Net architecture.\n\nThe SSD network was trained on a dataset consisting of 7097 fields of view (fov), with 4000 fovs used for training, 1000 for validation, and 2097 for testing. The network's output is a 39 \u00d7 52 grid with 4 channels, where the first two channels denote the likelihood of a pixel belonging to the background or a cell, respectively, and the third and fourth channels encode the displacement in normalized units in the x and y directions. The training process involved a hybrid cost function that combined normalized cross-entropy for the first two channels and mean error for the displacement channels. A lambda factor of 0.1 was used to balance the weight between correct localization and detection.\n\nTo address the potential issue of overfitting, given the large number of parameters in the SSD network compared to the number of training points, we implemented several strategies. Firstly, we used a validation set to monitor the network's performance during training and applied early stopping to prevent overfitting. Additionally, we employed data augmentation techniques to increase the diversity of the training data. The Pearson correlation coefficient between the number of detected cells and the number of cells obtained by an expert was 0.827, and the average error in the test set was 1.7 cells per field of view, indicating that the network generalized well to unseen data.\n\nFor cell contouring, the U-Net was trained on a dataset of 1704 manually segmented maximum intensity projections of 200 \u00d7 200 pixel cell images, with 800 images used for training, 200 for validation, and 704 for testing. The network was optimized using the DICE coefficient, which measures the similarity between two segmentation masks. The average DICE coefficient on the test set was 0.935, demonstrating the network's ability to accurately segment cell contours.\n\nTo ensure that the network was not underfitting, we monitored the training and validation loss during the training process. The use of the DICE coefficient as the optimization metric also helped to focus the network on learning the relevant features for cell segmentation. The network's performance on the test set further validated its effectiveness in generalizing to new data.",
  "optimization/regularization": "In our work, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the primary methods used was data augmentation, which involved creating variations of the training data through transformations such as rotations, translations, and flips. This helped the network to generalize better by exposing it to a wider range of possible inputs.\n\nAdditionally, we utilized dropout layers within our neural network architectures. Dropout is a regularization technique where randomly selected neurons are ignored during training. This forces the network to learn redundant representations and prevents it from becoming too reliant on any single neuron, thereby reducing overfitting.\n\nWe also implemented early stopping during the training process. This involved monitoring the performance of the model on a validation set and stopping the training when the performance stopped improving. This technique helps to prevent the model from overfitting to the training data by avoiding excessive training epochs.\n\nFurthermore, we ensured that our training datasets were sufficiently large and diverse. For the cell detection task, we used 4000 fields of view for training, while for the cell contouring task, we used 800 images. This large and varied dataset helped the models to learn more generalizable features rather than memorizing the training data.\n\nThe use of these regularization techniques, along with careful monitoring and validation, contributed to the development of robust and generalizable models for both cell detection and contouring tasks.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our work are available through our project's GitHub repository. The repository contains the code, which includes details on the network architectures, training procedures, and optimization schedules. Specifically, for the cell detection task, we utilized a Single Shot MultiBox Detector (SSD) network, while for cell contouring, a simplified version of the U-Net was employed. The repository also provides scripts that outline the training process, including the use of the adaptive momentum optimizer for the SSD network and the ADAM optimizer for the U-Net, along with their respective learning rates and other hyper-parameters.\n\nThe model files, including pre-trained weights, are not directly hosted in the repository due to their size, but instructions on how to train the models from scratch or fine-tune pre-trained models are provided. The repository is open-source and licensed under a permissive license, allowing researchers to use, modify, and distribute the code and models for their own research purposes.\n\nAdditionally, the repository includes example configurations and scripts for data preprocessing, training, and evaluation, which can be adapted to different datasets and experimental setups. This ensures reproducibility and facilitates the integration of our methods into other research pipelines. The GitHub repository can be accessed at [https://github.com/evansgroup/BioEssays2018](https://github.com/evansgroup/BioEssays2018).",
  "model/interpretability": "The model employed in our work is not entirely a black box, although it does leverage deep learning techniques that are often considered as such. We utilized a simplified version of the U-Net architecture for cell contour segmentation, which is an encoder-decoder network with skip connections. This architecture is known for its transparency in how it processes and segments images. The encoder path captures context by downsampling the input image, while the decoder path enables precise localization using transposed convolutions. Skip connections between the encoder and decoder paths help in combining high-resolution features from the encoder with upsampled output, making the segmentation process more interpretable.\n\nFor cell detection, we implemented a Single Shot MultiBox Detector (SSD) network. While SSD is generally faster and more efficient, it is also more interpretable than some other object detection models. The SSD network outputs a grid of bounding boxes with associated confidence scores and class probabilities, which can be visualized and understood relatively easily. The grid structure and the way it predicts object locations and sizes provide a clear framework for interpreting the model's decisions.\n\nIn both cases, the models were trained using well-defined datasets and optimization techniques, such as the DICE coefficient for segmentation and a hybrid cost function for object detection. These metrics and loss functions provide additional layers of interpretability, as they directly relate to the performance and accuracy of the models.\n\nMoreover, the intermediate outputs of these networks, such as feature maps and activation layers, can be visualized to understand how the models process input images. This level of transparency is crucial for building trust in the model's predictions and for debugging purposes. For instance, we can inspect the feature maps at different layers of the U-Net to see how the network identifies and segments cell contours. Similarly, the SSD network's intermediate outputs can be examined to understand how it detects and localizes cells in the input images.\n\nOverall, while deep learning models are often criticized for their lack of interpretability, the architectures and techniques we used provide a reasonable level of transparency. This allows researchers to understand and trust the model's outputs, which is essential for scientific applications.",
  "model/output": "The model's output is designed for both classification and regression tasks. It generates a 39 \u00d7 52 grid with 4 channels. The first two channels are used for classification, indicating the likelihood of a pixel belonging to the background or a cell. The third and fourth channels are used for regression, encoding the normalized displacement of cells in the x and y directions. This dual-purpose output allows the model to not only identify cells but also to provide information about their spatial arrangement. The network is trained using a hybrid cost function that combines normalized cross-entropy for classification and mean error for regression, with a lambda factor to balance the importance of detection and localization. The training database consists of manually annotated fields of view, with a significant portion used for training, validation, and testing. The model's performance is evaluated using metrics such as the Pearson correlation coefficient and the average error in cell count, demonstrating its effectiveness in both classification and regression tasks.",
  "model/duration": "The model processes 2097 testing images in 73 seconds, achieving a rate of 35 milliseconds per image. This performance is attained using an Nvidia GTX1080Ti graphics card. The efficiency of the model allows for rapid analysis, which is crucial for handling large datasets in biomedical research. The processing time can be further reduced through parallelization techniques, such as using Kubernetes clusters, which have demonstrated a significant improvement in processing speed in our laboratory setup. By utilizing multiple nodes, the average processing time for each field of view was reduced from 1 minute to 12 seconds, highlighting the potential for even faster execution times with optimized hardware and software configurations.",
  "model/availability": "The source code for the methods and practices discussed in our publication is publicly available. We have set up a code repository via the Evans Laboratory GitHub page. This repository contains the necessary code to implement the data management, automated analysis, and software development techniques described in our work. The repository is accessible at https://github.com/evansgroup/BioEssays2018.\n\nTo ensure reproducibility and ease of use, we have adopted several best practices in software development. We use version control systems, such as Git, to track changes in the codebase. This allows for easy collaboration and ensures that any modifications can be rolled back if necessary. Additionally, we provide detailed documentation and examples to help users understand and implement the methods.\n\nFor running the software across different machines and operating systems, we utilize containerization software like Docker. Docker containers encapsulate the necessary libraries and dependencies, making it straightforward to run the software in various environments without compatibility issues. This approach ensures that the software can be deployed and executed consistently, regardless of the underlying hardware or operating system.\n\nThe code and associated materials are released under an open-source license, which allows users to freely access, modify, and distribute the software. This open-source approach aligns with our commitment to reproducibility and collaboration in scientific research. By making our code and methods publicly available, we aim to facilitate the adoption of these techniques by other researchers and contribute to the broader scientific community.",
  "evaluation/method": "The method was evaluated using a dataset of manually segmented maximum intensity projections of cell images. The dataset consisted of 1704 images, with 800 used for training, 200 for validation, and 704 for testing. The network's performance was assessed using the DICE coefficient, which measures the similarity between the segmentation masks produced by the network and the reference standard. The DICE coefficient ranges from 0 to 1, with 1 indicating perfect overlap between the masks. The average DICE coefficient on the test set was 0.935, indicating high accuracy in segmentation.\n\nAdditionally, the method was evaluated using a separate dataset of 7097 fields of view (fov) manually annotated by experts. Out of these, 4000 fovs were used for training, 1000 for validation, and 2097 for testing. The Pearson correlation coefficient between the number of detected cells using this method and the number of cells obtained by an expert was 0.827. The average error in the test set between the number of cells found in a field of view and the number of cells present was 1.7 cells per fov. This evaluation demonstrated the method's effectiveness in cell detection and segmentation tasks.",
  "evaluation/measure": "In our evaluation, we primarily focused on the DICE coefficient as our key performance metric. This coefficient measures the similarity between two segmentation masks by calculating twice the area of the overlap between the masks and dividing it by the sum of the areas of the two masks. The DICE coefficient ranges from 0 to 1, where 1 indicates perfect overlap and 0 indicates no overlap. This metric is particularly useful for segmentation tasks as it provides a more robust measure of similarity compared to per-pixel classification methods.\n\nAdditionally, we reported the Pearson correlation coefficient to assess the relationship between the number of cells detected by our method and the number of cells identified by experts. This coefficient helps to understand the linear relationship between the automated detections and manual annotations, providing insight into the accuracy of our cell detection algorithm.\n\nWe also reported the average error in the number of cells detected per field of view, which gives a quantitative measure of the precision of our cell counting method. This metric is crucial for understanding the reliability of our automated system in practical applications.\n\nThese metrics are representative of the standards used in the literature for evaluating segmentation and object detection tasks in biological imaging. The DICE coefficient is widely accepted for its effectiveness in measuring segmentation accuracy, while the Pearson correlation coefficient and average error provide comprehensive insights into the performance of our detection algorithm. Together, these metrics offer a thorough evaluation of our method's effectiveness and reliability.",
  "evaluation/comparison": "In our evaluation, we did not perform a direct comparison to publicly available methods on benchmark datasets. Our focus was primarily on developing and validating our own deep learning approaches for cell identification and segmentation. We used a custom dataset of manually annotated images for training and testing our models.\n\nFor cell identification, we employed a single-shot multibox detector (SSD) network. This network was trained on a dataset of 7097 fields of view, with 4000 used for training, 1000 for validation, and 2097 for testing. The performance was evaluated using the Pearson correlation coefficient between the number of detected cells and the number of cells identified by experts, which was 0.827. The average error in the test set was 1.7 cells per field of view.\n\nFor cell segmentation, we utilized a simplified version of the U-Net architecture. This network was trained on a dataset of 1704 manually segmented images, with 800 used for training, 200 for validation, and 704 for testing. The performance was assessed using the DICE coefficient, which measures the similarity between the predicted segmentation and the reference standard. The average DICE coefficient on the test set was 0.935.\n\nWhile we did not compare our methods to simpler baselines or publicly available methods, the performance metrics we achieved indicate that our approaches are effective for the tasks of cell identification and segmentation. The use of deep learning techniques, such as SSD and U-Net, allowed us to achieve high accuracy and efficiency in processing large datasets of cell images.",
  "evaluation/confidence": "The evaluation of our method focused on key performance metrics without explicitly stating confidence intervals. For the simplified U-Net used for cell contour segmentation, the average DICE coefficient on the test set was reported as 0.935. This metric measures the similarity between the segmentation masks produced by our network and the reference standard, with values closer to 1 indicating higher similarity. However, specific confidence intervals for this metric were not provided.\n\nFor the object identification task, the Pearson correlation coefficient between the number of detected cells using our SSD-based method and the number of cells identified by an expert was 0.827. This indicates a strong correlation, but again, confidence intervals for this coefficient were not detailed. The average error in the test set between the number of cells found in a field of view and the number of cells present was 1.7 cells per field of view. This error rate was deemed acceptable by the researchers using the system, particularly considering that most errors occurred with close-by cells or those partially in the field of view.\n\nStatistical significance tests to compare our method against other baselines or state-of-the-art approaches were not explicitly mentioned. While the results indicate good performance, the lack of confidence intervals and statistical significance tests means that the superiority of our method over others cannot be definitively claimed based on the provided information. Further statistical analysis would be necessary to fully validate the robustness and generalizability of our findings.",
  "evaluation/availability": "Not enough information is available."
}