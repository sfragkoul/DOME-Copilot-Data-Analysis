{
  "publication/title": "Machine learning algorithms for predicting undernutrition among under-five children in Ethiopia.",
  "publication/authors": "Bitew FH, Sparks CS, Nyarko SH",
  "publication/journal": "Public health nutrition",
  "publication/year": "2022",
  "publication/pmid": "34620263",
  "publication/pmcid": "PMC8883776",
  "publication/doi": "10.1017/s1368980021004262",
  "publication/tags": "- Predictive algorithms\n- Determinants\n- Child undernutrition\n- Spatial variations\n- Machine learning\n- Ethiopia\n- Public health\n- Child stunting\n- Socio-demographic risk factors\n- Gradient boosting\n- Neural networks\n- k-nearest neighbours\n- Random forest\n- Generalised linear models\n- Child health\n- Nutrition\n- Data analysis\n- Health survey\n- Predictive modeling\n- Health disparities",
  "dataset/provenance": "The dataset used in this study is sourced from the 2016 Ethiopian Demographic and Health Survey. This survey is part of a series conducted every five years and is designed to be nationally representative, collecting data on a wide range of population and health issues to enhance maternal and child health in Ethiopia.\n\nThe survey employed a multi-stage stratified sampling procedure to select respondents from households across 624 clusters. The study sample is specifically focused on 9,471 children below the age of five. This data was gathered retrospectively from mothers, who provided information about the Body Mass Index (BMI) of their children within the five years preceding the survey, covering the period from 2011 to 2016.\n\nThis dataset has not been extensively used in previous studies, particularly in the context of machine learning applications to predict child undernutrition determinants. The study aims to fill this gap by utilizing various machine learning algorithms that have not been widely applied in earlier research, thereby providing new insights into the predictors of child undernutrition in Ethiopia.",
  "dataset/splits": "The dataset was split into two primary sets: a training set and a testing set. Initially, two different splits were attempted: one with 60% of the data allocated for training and 40% for testing, and another with 70% for training and 30% for testing. The 70% train and 30% test split was found to yield more reasonable outcomes and is the one that was ultimately used.\n\nIn the final split, the training set consisted of 70% of the observed data, amounting to 5,147 data points. The remaining 30% of the cases, totaling 1,716 data points, were held out as a test or validation set. This distribution allowed for effective training of the machine learning algorithms and subsequent validation of their performance. Missing cases were removed during the execution of the machine learning algorithms. Additionally, 10-fold cross-validation was employed on the training set to further enhance the robustness of the models.",
  "dataset/redundancy": "The dataset used in this study was split into two sets: a training set and a test set. This split was done to learn from the data, train the classification algorithms, and identify patterns within the data. The data were initially trained using two different splits: 60% for training and 40% for testing, and 70% for training and 30% for testing. The 70% train and 30% test split was found to yield a reasonable outcome and is widely used in classification tasks. Therefore, the final split consisted of 70% of the observed data for the training set and the remaining 30% for the test or validation set.\n\nThe training set consisted of 5,147 individuals, while the test set consisted of 1,716 individuals. This split ensures that the training and test sets are independent, which is crucial for evaluating the performance of the machine learning algorithms. The independence of the sets was enforced by randomly selecting the data points for each set, ensuring that there is no overlap between the training and test datasets.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the context of public health and nutritional studies. The use of a nationally representative household survey, such as the 2016 Ethiopian Demographic and Health Survey, ensures that the data is comprehensive and representative of the population under study. This survey collects data on a broad range of population and health issues, making it a rich source of information for identifying patterns and determinants of child undernutrition.\n\nThe dataset was further processed to handle missing cases, which were disposed of while running the machine learning algorithms. This step is important to maintain the integrity of the data and to ensure that the algorithms can accurately learn from the training set and make reliable predictions on the test set. Additionally, 10-fold cross-validation was used on the training set to estimate the performance of the algorithms, providing a robust evaluation of their predictive power.",
  "dataset/availability": "The data utilized in this study is sourced from the 2016 Ethiopian Demographic and Health Survey (EDHS). This survey is part of the global Demographic and Health Survey series, conducted every five years, and is designed to be nationally representative. The survey employs a multi-stage stratified sampling procedure to select respondents from households across 624 clusters in Ethiopia.\n\nThe specific dataset used in this study is limited to 9,471 children under the age of five. The data includes retrospective information obtained from mothers regarding the Body Mass Index (BMI) of their children within the five years preceding the survey (2011\u20132016). The outcomes of interest are under-five stunting, wasting, and underweight status, which are evaluated using Z-scores of anthropometric measurements such as height-for-age, weight-for-age, and weight-for-height.\n\nThe data splits used in the study involved dividing the dataset into training and testing sets. Two different splits were initially considered: 60% training and 40% testing, and 70% training and 30% testing. The 70% training and 30% testing split was ultimately chosen as it provided a reasonable outcome and is widely used in classification tasks. The training set consisted of 70% of the observed data, while the remaining 30% was held out as a test or validation set.\n\nThe data, including the splits used, are not publicly released in a forum. The data is proprietary and is managed by the Ethiopian Public Health Institute and ICF. Access to the data is typically restricted and requires permission from the data custodians. The specific license terms for accessing the data are not detailed in the study, but it is common for such datasets to have restrictions on use and dissemination to protect participant privacy and ensure ethical use.\n\nTo enforce data access and usage policies, the data custodians likely implement measures such as data use agreements, secure data access protocols, and monitoring of data usage. These measures help ensure that the data is used responsibly and in accordance with ethical guidelines and legal requirements.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the ensemble learning class. Ensemble learning is a technique that combines multiple algorithms to improve the overall performance and accuracy of predictions. The specific algorithms employed include xgbTree, GLM, NNet, RF, and k-NN.\n\nThese algorithms are not new but are well-established in the field of machine learning. They have been extensively used and validated in various studies and competitions, particularly in Kaggle, where they have shown high rates of success, especially for structured features.\n\nThe choice of these algorithms was driven by their proven reliability and efficiency in solving machine learning challenges. For instance, xgbTree, which stands for extreme gradient boosting, is known for its scalable and efficient implementation of the gradient boosting framework. It supports various objective functions, including regression, classification, and ranking, and has better control against overfitting compared to prior algorithms.\n\nThe decision to use these algorithms in this study was also influenced by their ability to handle different types of data and their robustness in predicting outcomes. For example, neural networks, which are based on the model of neurons in the brain, can be thought of as nonlinear regression and are known for their good prediction capabilities and tolerance for correlated inputs.\n\nThe study did not publish these algorithms in a machine-learning journal because the focus was on applying these established methods to a specific problem\u2014predicting child undernutrition in Ethiopia. The goal was to leverage the strengths of these algorithms to provide accurate and reliable predictions, rather than developing new algorithms. The algorithms were combined using stacking, a popular method for enhancing the accuracy of predictions by learning how to best combine the predictions of multiple algorithms.",
  "optimization/meta": "To increase the accuracy of the algorithms, we employed 'Stacking', a popular method for combining predictions from different algorithms. This approach involves building multiple algorithms, typically of differing types, and generating a supervisor algorithm that learns how to best combine the predictions of the primary algorithms. In our study, the predictions of several machine learning algorithms, including xgbTree, GLM, NNet, RF, and k-NN, were combined using stacking.\n\nThe stacking method ensures that the training data for each base model is independent. This is achieved by splitting the data into training and testing sets, where the training set is used to train the base models, and the testing set is used to evaluate their performance. The predictions from the base models on the testing set are then used as input features for the meta-predictor, which is trained to combine these predictions optimally.\n\nBy using stacking, we aim to leverage the strengths of different machine learning algorithms to improve the overall predictive performance. The meta-predictor learns to weigh the predictions of the base models appropriately, leading to more accurate and robust predictions. This approach has been shown to be effective in various machine learning competitions and real-world applications, making it a valuable technique for enhancing the performance of predictive models.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for ensuring the effectiveness of the machine-learning algorithms. We began by handling missing data, excluding variables with more than 50% missing values to maintain the integrity of the algorithms' performance. For the remaining variables, we imputed missing values using appropriate techniques to ensure completeness.\n\nThe outcomes for childhood undernutrition indicators\u2014stunting, wasting, and underweight\u2014were binary coded. Specifically, a child was coded as 1 if they met the criteria for stunting, wasting, or underweight (Z-score below -3 standard deviations), and 0 otherwise. This binary encoding facilitated the application of classification algorithms.\n\nWe incorporated a wide range of covariates from the Demographic and Health Surveys (DHS) that had minimal missing data. These covariates were considered potential risk factors for childhood undernutrition in Ethiopia. The inclusion of these variables aimed to capture the diverse factors influencing undernutrition, thereby enhancing the predictive power of our models.\n\nThe data were split into training and testing sets using two different ratios: 60% training and 40% testing, and 70% training and 30% testing. The 70% training and 30% testing split was ultimately chosen due to its widely accepted use and the reasonable outcomes it produced. This split allowed us to train the algorithms on a substantial portion of the data while reserving a significant portion for validation.\n\nAll machine-learning algorithms were trained using 10-fold cross-validation on the training set. This technique helped in estimating the performance of the models and in reducing overfitting. The algorithms evaluated included xgbTree, generalised linear model (GLM), neural networks (NNet), random forest (RF), and k-nearest neighbours (k-NN). Each algorithm was applied to the training data set, which consisted of 70% of the individuals, and validated on the remaining 30%.\n\nTo further enhance the accuracy of our predictions, we employed stacking, a popular method for combining the predictions from different algorithms. This approach involved building multiple algorithms and generating a supervisor algorithm that learned how to best combine the predictions of the primary algorithms. The combined predictions from xgbTree, GLM, NNet, RF, and k-NN were used to improve the overall performance of the models.\n\nIn summary, our data encoding and preprocessing involved careful handling of missing data, binary coding of outcomes, inclusion of relevant covariates, and rigorous training and validation procedures. These steps were essential in ensuring the robustness and accuracy of our machine-learning models.",
  "optimization/parameters": "In our study, we utilized a total of thirty-seven variables as input parameters for our models. These variables were selected based on their relevance to the prediction of childhood undernutrition indicators, such as stunting, wasting, and underweight.\n\nThe selection of these parameters was informed by domain knowledge and exploratory data analysis. We aimed to include a comprehensive set of predictors that could capture the various factors influencing childhood undernutrition. The variables encompassed demographic, socioeconomic, health, and environmental factors, providing a holistic view of the determinants of undernutrition.\n\nTo ensure the robustness of our models, we employed feature importance techniques, particularly mean decrease Gini, to identify the most influential variables. This approach helped us to rank the variables based on their contribution to the predictive performance of our models, specifically the xgbTree algorithm, which demonstrated the highest accuracy across all three undernutrition indicators.\n\nBy focusing on the top-20 important variables, we were able to streamline our models while retaining their predictive power. This selection process not only improved the efficiency of our models but also provided insights into the key determinants of childhood undernutrition. The top variables varied across the different indicators, highlighting the multifaceted nature of undernutrition and the need for tailored interventions.",
  "optimization/features": "In our study, we utilized a comprehensive set of input features to train and validate our machine learning models. The exact number of features (f) used as input varied depending on the specific algorithm and the stage of the analysis. Initially, we started with a broad range of potential predictors, but we employed feature selection techniques to identify the most relevant variables.\n\nFeature selection was indeed performed to enhance the performance and interpretability of our models. This process involved evaluating the predictive power of each variable and retaining only those that contributed significantly to the model's accuracy. To ensure the robustness of our feature selection, we conducted this process exclusively using the training set. This approach prevented data leakage and maintained the integrity of our validation process.\n\nBy focusing on the most important features, we were able to improve the efficiency and accuracy of our models. The selected features were then used to train and validate our algorithms, including xgbTree, GLM, NNet, RF, and k-NN. This rigorous feature selection process was crucial in achieving the best possible performance from our machine learning models.",
  "optimization/fitting": "In our study, we employed five different machine learning algorithms to predict childhood undernutrition in Ethiopia. These algorithms included eXtreme gradient boosting (xgbTree), k-nearest neighbors (k-NN), random forest (RF), neural networks (NNet), and generalized linear models (GLM). Each of these algorithms has its own set of hyperparameters and complexities, which we carefully tuned to ensure optimal performance.\n\nThe number of parameters in our models varied significantly. For instance, neural networks and random forests can have a large number of parameters, especially when dealing with high-dimensional data. To address the risk of overfitting, we implemented several strategies. One key approach was the use of 10-fold cross-validation on the training set. This technique helps in assessing the model's performance on different subsets of the data, thereby providing a more robust estimate of its generalization ability. Additionally, for algorithms like xgbTree, we utilized regularization techniques to control overfitting. These methods include L1 and L2 regularization, which penalize large coefficients and help in creating a more generalized model.\n\nFor k-NN, the risk of overfitting is mitigated by choosing an appropriate value of k, which determines the number of nearest neighbors considered for classification. We experimented with different values of k and selected the one that provided the best performance on the validation set.\n\nUnderfitting was addressed by ensuring that our models were complex enough to capture the underlying patterns in the data. For neural networks, this involved tuning the number of hidden layers and neurons. For random forests, we adjusted the number of trees and the maximum depth of each tree. The GLM, being a simpler model, did not require extensive tuning but was included to provide a baseline for comparison.\n\nIn summary, we carefully managed the balance between overfitting and underfitting by employing cross-validation, regularization techniques, and hyperparameter tuning. These strategies ensured that our models were both complex enough to capture the data's intricacies and generalizable to new, unseen data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One of the key methods used was regularization, specifically within the xgbTree algorithm. This algorithm incorporates a more regularized formalization compared to prior algorithms, which helps in controlling overfitting. Regularization in xgbTree involves adding penalty terms to the loss function, which discourages overly complex models that may fit the noise in the training data rather than the underlying pattern.\n\nAdditionally, we utilized 10-fold cross-validation during the training process. This technique involves splitting the training data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. Cross-validation helps in assessing the model's performance on different subsets of the data, providing a more reliable estimate of its generalization ability and reducing the risk of overfitting.\n\nFurthermore, we combined multiple algorithms using the stacking method to create ensemble predictions. Stacking involves training several base models and then using a meta-model to combine their predictions. This approach leverages the strengths of different algorithms and reduces the likelihood of overfitting by averaging out the errors of individual models. The base models included in our stacking ensemble were xgbTree, GLM, NNet, RF, and k-NN, each contributing to the final prediction in a complementary manner.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in this study encompass a mix of both transparent and black-box approaches. Among the algorithms used, k-nearest neighbors (k-NN) and generalized linear models (GLM) are relatively transparent. k-NN is a non-parametric algorithm that classifies based on the similarity of new cases to the nearest observations in the training set, making it interpretable in terms of how decisions are made. GLM, on the other hand, provides coefficients that indicate the relationship between predictors and the outcome, offering clear insights into the impact of each variable.\n\nIn contrast, algorithms like extreme gradient boosting (xgbTree), random forests (RF), and neural networks (NNet) are considered black-box models. These models are highly effective in capturing complex patterns in the data but lack transparency in how they arrive at their predictions. For instance, xgbTree and RF build multiple decision trees and combine their outputs, making it challenging to trace the exact path of decision-making. Similarly, neural networks, with their layered structure and weighted connections, operate in a manner that is not easily interpretable.\n\nTo enhance the interpretability of the black-box models, feature importance measures were utilized. Specifically, for the xgbTree algorithm, the mean decrease in Gini was used to identify the top variables contributing to the predictions of stunting, wasting, and underweight. This approach provides a way to understand which variables are most influential in the model's decisions, even if the exact mechanics of the model remain opaque.\n\nIn summary, while some of the models used are transparent and provide clear insights into their decision-making processes, others are black-box models that offer high predictive performance at the cost of interpretability. Feature importance measures help bridge this gap by highlighting the key variables driving the predictions.",
  "model/output": "The model employed in this study is primarily focused on classification tasks. Specifically, it addresses binary classification problems related to childhood undernutrition indicators such as stunting, wasting, and underweight. These outcomes were binary coded as 1 if the child met the criteria for stunting, wasting, or underweight, and 0 otherwise. The model utilized various machine learning algorithms, including xgbTree, generalised linear model (GLM), neural networks (NNet), random forest (RF), and k-nearest neighbours (k-NN), to predict these binary outcomes. The GLM, in particular, is noted for its use in analysing binary data and serves as a binary classification algorithm. The performance of these algorithms was evaluated using a confusion matrix, which provides insights into true negatives, false negatives, true positives, and false positives, further confirming the classification nature of the model.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "To evaluate the performance of the algorithms, we employed a confusion matrix, which is also known as an error matrix. This matrix is a two-by-two table that displays the values of True Negatives, False Negatives, True Positives, and False Positives resulting from the predicted classes of data. The confusion matrix allows for the calculation of various performance metrics, including prediction accuracy, sensitivity, and specificity.\n\nAccuracy was a key metric used to estimate the performance of the predictive algorithms. It is calculated as the ratio of correct predictions to the total number of data points evaluated. The study focused on achieving the best accuracies by applying feature selection and k-fold cross-validation techniques.\n\nSensitivity, also known as recall, measures the proportion of real positive cases that were correctly predicted as positive. It highlights the ability of the model to identify positive instances.\n\nSpecificity measures the proportion of real negative cases that were correctly predicted as negative. It indicates the model's ability to identify negative instances.\n\nAdditionally, we used Cohen's \u03ba statistic to evaluate the classifiers. This metric compares observed accuracy with expected accuracy (random chance), providing a measure of agreement beyond what would be expected by chance. The \u03ba values were interpreted using the scale provided by Landis and Koch, which ranges from no agreement to almost perfect agreement.\n\nThe data were split into training and testing sets, with the training set consisting of 70% of the observed data and the testing set consisting of the remaining 30%. This split is widely used in classification tasks and was found to yield reasonable outcomes. The algorithms were trained using 10-fold cross-validation on the training set, and their performance was estimated on the testing set. This approach ensures that the models are robust and generalizable to new, unseen data.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our machine learning algorithms in predicting child undernutrition. The primary metrics reported include accuracy, sensitivity, specificity, and Cohen's \u03ba.\n\nAccuracy is a fundamental metric that measures the proportion of correct predictions (both true positives and true negatives) out of the total number of cases evaluated. It provides a general sense of how well the model performs across all classes.\n\nSensitivity, also known as recall, focuses on the model's ability to correctly identify positive cases. It is calculated as the ratio of true positives to the sum of true positives and false negatives. This metric is crucial for understanding how well the model detects actual instances of undernutrition.\n\nSpecificity, on the other hand, assesses the model's ability to correctly identify negative cases. It is the ratio of true negatives to the sum of true negatives and false positives. This metric is important for evaluating the model's performance in identifying cases where undernutrition is not present.\n\nCohen's \u03ba is a statistical measure that compares the observed accuracy with the expected accuracy by chance. It provides a more nuanced evaluation by accounting for the possibility of agreement occurring by chance. The \u03ba values range from 0 to 1, with higher values indicating better agreement beyond chance.\n\nThese metrics collectively provide a comprehensive evaluation of the model's performance. Accuracy gives an overall sense of the model's effectiveness, while sensitivity and specificity offer insights into its performance for specific classes. Cohen's \u03ba adds a layer of robustness by considering the baseline agreement expected by chance. This set of metrics is representative of standard practices in the literature, ensuring that our evaluation is both rigorous and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on evaluating and comparing the performance of various machine learning algorithms specifically tailored to predict child undernutrition determinants in Ethiopia. We utilized a dataset from the 2016 Ethiopian Demographic and Health Survey, which is a nationally representative household survey.\n\nWe compared five different machine learning algorithms: xgbTree, GLM, NNet, RF, and k-NN. These algorithms were chosen for their diverse approaches to classification and regression tasks. To ensure robust evaluation, we employed a 70% train and 30% test split, along with 10-fold cross-validation on the training set. This approach allowed us to assess the generalizability and performance of each algorithm.\n\nWhile we did not compare our methods to simpler baselines explicitly, the selection of algorithms included both complex and relatively simpler models. For instance, GLM (Generalized Linear Model) can be considered a simpler baseline compared to more complex models like NNet (Neural Networks) and xgbTree (Extreme Gradient Boosting). By including a range of algorithms, we aimed to identify the most effective model for predicting child undernutrition risk factors.\n\nOur evaluation metrics included accuracy, sensitivity, and the use of a confusion matrix to assess the performance of each algorithm. These metrics provided a comprehensive view of how well each algorithm performed in classifying the nutritional status of children.\n\nIn summary, our study focused on comparing multiple machine learning algorithms to determine the best predictive model for child undernutrition in Ethiopia, using a robust evaluation framework.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}