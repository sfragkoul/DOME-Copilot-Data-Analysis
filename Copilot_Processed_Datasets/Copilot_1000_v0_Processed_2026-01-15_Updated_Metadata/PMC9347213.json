{
  "publication/title": "End-to-End Deep Learning Model to Predict and Design Secondary Structure Content of Structural Proteins.",
  "publication/authors": "Yu CH, Chen W, Chiang YH, Guo K, Martin Moldes Z, Kaplan DL, Buehler MJ",
  "publication/journal": "ACS biomaterials science & engineering",
  "publication/year": "2022",
  "publication/pmid": "35129957",
  "publication/pmcid": "PMC9347213",
  "publication/doi": "10.1021/acsbiomaterials.1c01343",
  "publication/tags": "- Deep Learning\n- Protein Sequences\n- Alpha Helix\n- Beta Sheet\n- Model Training\n- Prediction Accuracy\n- Protein Design\n- SARS and COVID-19 Proteins\n- Protein Secondary Structure\n- Machine Learning in Biology\n- Protein Structure Prediction\n- Genetic Evolution\n- Simulated Evolution\n- Protein Data Bank\n- PredictProtein\n- Lysozyme\n- Systematic Sequence Variation\n- Bidirectional LSTM\n- Convolution Blocks\n- Fully Connected Layers\n- Mean Square Error\n- R2 Scores\n- Protein Engineering\n- Protein Folding\n- Molecular Graphics\n- Protein Analysis Tools\n- Protein Sequence Analysis\n- Protein Structure Characterization\n- Protein Property Prediction\n- Protein Modeling\n- Protein Sequence Optimization\n- Protein Secondary Structure Prediction",
  "dataset/provenance": "The dataset utilized in this work was sourced from the Protein Data Bank (PDB). This database is a widely recognized repository for protein sequences and structures. It contained a substantial number of protein sequences, specifically 125,955 entries. Each entry in the dataset included the protein ID, length, primary structure, and secondary structure, which were labeled subsequently.\n\nThe primary structure of the proteins consists of linear sequences of amino acids, represented by different letters corresponding to various natural amino acids and other residues. The secondary structure and folding structure of these proteins can be predicted based on the arrangement of amino acids.\n\nThis dataset builds upon previous work conducted by our research group, indicating that it is not entirely novel but rather an extension or refinement of existing data. The dataset's diversity in protein lengths and structural organizations allows for a comprehensive analysis of the relationships between primary and secondary structures across different sequences. The average length of the protein sequences in the dataset is 644 amino acids, with the shortest sequence comprising just 11 amino acids and the longest extending up to 19,350 amino acids. The standard deviation of the sequence lengths is 855 amino acids, highlighting the variability within the dataset.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The dataset utilized in this study was derived from the Protein Data Bank (PDB). This database is publicly accessible and contains a comprehensive collection of protein sequences. The dataset includes 125,955 protein sequences, each with recorded protein ID, length, primary structure, and secondary structure.\n\nThe data was labeled using the Define Secondary Structure of Protein (DSSP) algorithm, which is a widely used method for classifying secondary structure conformations of amino acid residues in protein structures. This labeling process ensured that the dataset was accurately annotated with ground truth data for training purposes.\n\nThe dataset is available through the Protein Data Bank, which is a well-established and publicly accessible resource for protein data. The PDB provides open access to its data, allowing researchers to use it for various studies and applications. The data can be accessed under the terms and conditions specified by the PDB, which typically include proper citation and acknowledgment of the source.\n\nTo ensure the integrity and reproducibility of the results, the dataset was preprocessed using tokenization with the Keras Tokenizer module. This step facilitated the segmentation of each amino acid in the protein sequences, converting them into numeric digits for further computational analysis. This preprocessing method is standard in machine learning and deep learning workflows, ensuring that the data is consistently formatted and ready for model training.\n\nThe dataset's availability and the use of standard preprocessing techniques contribute to the transparency and reproducibility of the research. By making the dataset publicly accessible and using established methods for data labeling and preprocessing, the study adheres to best practices in scientific research, enabling other researchers to validate and build upon the findings.",
  "optimization/algorithm": "The optimization algorithm discussed in the publication is not a machine-learning algorithm but rather a method to be coupled with the predictive model in future work. The specific algorithms mentioned for potential use are genetic evolution or simulated evolution. These are evolutionary algorithms, which are a class of optimization algorithms inspired by the process of natural evolution.\n\nThese algorithms are not new; they have been extensively studied and used in various fields for optimization problems. The reason they are mentioned in this context is to explore their potential in identifying new protein sequences that meet certain design criteria. The focus of the publication is on the deep learning model for predicting protein secondary structure, not on the development of new optimization algorithms.\n\nThe deep learning model itself is an end-to-end regression model designed to predict protein secondary structure ratios. It includes components such as an embedding layer, convolutional layers, bidirectional LSTM units, and fully connected layers. This model is implemented using TensorFlow with Keras, a high-level application interface. The choice of using these specific tools and frameworks is likely due to their widespread use and effectiveness in handling sequential data, which is characteristic of both language data and protein sequences.",
  "optimization/meta": "The model described in this publication is not a meta-predictor. It is an end-to-end deep learning model designed to predict protein secondary structure ratios, specifically the alpha helix and beta sheet content. The model does not use data from other machine-learning algorithms as input. Instead, it processes protein sequences directly through a series of layers, including an embedding layer, convolutional layers, bidirectional LSTM units, and fully connected layers.\n\nThe training and testing datasets are split into 80% training and 20% testing sets, ensuring that the data used for training is independent of the data used for testing. This split helps in evaluating the model's predictive power on unseen data, which is crucial for assessing its generalization capability.\n\nThe model's architecture is inspired by natural language processing (NLP) techniques, leveraging the sequential nature of protein data. It includes layers specifically designed to capture hierarchical patterns and sequential relationships within the protein sequences. The use of convolutional layers and bidirectional LSTM units allows the model to extract meaningful features from the input data, which are then used to make predictions about the secondary structure ratios.\n\nThe performance of the model is evaluated using metrics such as mean square error (MSE) loss and the coefficient of determination (R2 score). The R2 score, in particular, provides a measure of how well the predicted values match the ground truth, with higher scores indicating better performance. The model has demonstrated strong predictive capabilities, with R2 scores close to 1 for both alpha helix and beta sheet ratios in both training and testing datasets.\n\nIn summary, the model is a standalone deep learning architecture that does not rely on other machine-learning algorithms for input data. It is trained and tested on independent datasets, ensuring robust and reliable predictions of protein secondary structure ratios.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to effectively analyze protein sequences using machine learning algorithms. We began by tokenizing the protein sequences, a process facilitated by the Keras Tokenizer module. This step involved segmenting each amino acid in the protein's primary structure, which is represented by 20 different single-letter codes. Tokenization transformed these letters into numeric digits, making the data suitable for further computational analysis.\n\nThe primary structure of proteins, consisting of linear sequences of amino acids, was converted into a format that could be easily processed by our deep learning model. This involved changing the character-based representation of amino acids into vectors, a common technique in natural language processing models. The tokenized data was then passed through an embedding layer with 15 embedding dimensions, enabling the conversion of characters into vectors.\n\nFollowing the embedding layer, the data underwent batch normalization to ensure numerical stability and convergence during training. This was followed by two 1D convolutional layers, which captured sequential features and hierarchical patterning within the protein sequences. The convolutional layers were essential for extracting local patterns and dependencies in the data.\n\nAfter the convolutional layers, the data was fed into two bidirectional Long Short-Term Memory (BiLSTM) units. These units were instrumental in capturing the sequential relationships within the protein sequences bidirectionally, allowing the model to consider both past and future context in the data. This bidirectional approach enhanced the model's ability to understand the complex dependencies in protein structures.\n\nTo further improve the robustness of our model, we included a GaussianNoise layer for denoising. This layer helped in making the model more resilient to noise in the input data, thereby improving its generalization capabilities. Finally, the data passed through several fully connected and dropout layers. The fully connected layers integrated the features extracted by the previous layers, while the dropout layers helped in preventing overfitting by randomly setting a fraction of input units to zero during training.\n\nIn summary, our data encoding and preprocessing pipeline involved tokenization, embedding, batch normalization, convolutional layers, BiLSTM units, GaussianNoise layer, and fully connected layers with dropout. This comprehensive approach ensured that the protein sequences were effectively transformed into a format suitable for deep learning, enabling accurate predictions of secondary structure ratios.",
  "optimization/parameters": "In our deep learning model, we utilized a total of 336,455 parameters. These parameters are distributed across various layers, including an embedding layer, batch normalization layer, two 1D convolutional layers, two bidirectional LSTM units, several dense layers, and dropout layers. The specific number of parameters for each layer is as follows:\n\n- Embedding layer: 345 parameters\n- Batch Normalization layer: 60 parameters\n- First Convolution 1D layer: 38,528 parameters\n- Second Convolution 1D layer: 81,984 parameters\n- First Bidirectional LSTM unit: 66,048 parameters\n- Second Bidirectional LSTM unit: 41,216 parameters\n- First Dense layer: 33,280 parameters\n- Second Dense layer: 65,664 parameters\n- Third Dense layer: 8,256 parameters\n- Fourth Dense layer: 1,040 parameters\n- Fifth Dense layer: 34 parameters\n\nThe selection of these parameters was guided by the architecture of the model, which was designed to capture sequential features and hierarchical patterning in protein sequences. The embedding layer converts amino acid sequences into vectors, while the convolutional layers extract local features. The bidirectional LSTM units capture long-range dependencies in both forward and backward directions. The dense layers and dropout layers are used for final prediction and to prevent overfitting, respectively. The GaussianNoise layer is included for denoising purposes. The total number of parameters is a result of the model's complexity and the need to accurately predict the secondary structure content of proteins.",
  "optimization/features": "The input features for our deep learning model are derived from the sequence of the protein. The primary feature is the sequence of amino acids, which is processed through a tokenizer into an embedding layer with 15 embedding dimensions. This transformation converts the character-based amino acid sequences into numerical vectors, similar to techniques used in natural language processing.\n\nFeature selection in the traditional sense was not performed, as the model leverages the entire sequence of amino acids. However, the design of the model itself acts as a form of feature extraction. The embedding layer captures the initial representations of the amino acids, while subsequent layers, including convolutional and bidirectional LSTM layers, extract hierarchical and sequential features from the protein sequences.\n\nThe embedding dimensions and the architecture of the model were determined based on the characteristics of the protein sequences and the need to capture both local and global patterns within the data. The use of convolutional layers helps in identifying local patterns, while the bidirectional LSTM layers capture the sequential dependencies in both forward and backward directions. This comprehensive approach ensures that the model can effectively learn from the input features without the need for explicit feature selection.\n\nThe training and testing datasets were split into 80% training and 20% testing sets, ensuring that the model's performance is evaluated on unseen data. This split helps in validating the model's generalization capability and ensures that the features extracted during training are robust and applicable to new protein sequences.",
  "optimization/fitting": "In our study, we developed a deep learning model to predict protein secondary structure ratios, which includes an embedding layer, batch normalization, 1D convolutional layers, bidirectional LSTM units, Gaussian noise layer, and several dropout and dense layers. The total number of parameters in our model is 336,455, with 336,425 being trainable.\n\nGiven the complexity of protein sequences and the need to capture intricate patterns, the number of parameters is indeed substantial. However, to mitigate the risk of overfitting, we employed several strategies. Firstly, we used dropout layers, which randomly set a fraction of input units to 0 at each update during training time, helping to prevent overfitting. Secondly, we incorporated a Gaussian noise layer to add random noise to the input data, further enhancing the model's robustness. Additionally, we split our dataset into 80% for training and 20% for testing, ensuring that the model's performance was evaluated on unseen data.\n\nTo address underfitting, we utilized a combination of convolutional and LSTM layers to capture both local and sequential features in the protein sequences. The convolutional layers helped in extracting hierarchical patterns, while the bidirectional LSTM units allowed the model to learn long-range dependencies in both forward and backward directions. Furthermore, we used an Adam optimizer, which adapts the learning rate for each parameter, helping to converge faster and more accurately.\n\nThe model's performance was evaluated using the mean square error (MSE) loss and the coefficient of determination (R2 score). The R2 scores for alpha-helix and beta-sheet ratios in the training data were 0.99 and 0.96, respectively, indicating a strong fit. For the testing dataset, the R2 scores were 0.99 and 0.98, demonstrating the model's generalizability and ruling out both overfitting and underfitting.",
  "optimization/regularization": "In our study, we implemented several regularization techniques to prevent overfitting and ensure the robustness of our deep learning model. One of the key methods used was dropout layers. Dropout is a technique where, during training, a random selection of neurons is ignored or \"dropped out.\" This helps to prevent the model from becoming too reliant on any single neuron and encourages it to learn more general features. We incorporated multiple dropout layers throughout our network architecture.\n\nAdditionally, we employed Gaussian noise layers. These layers introduce random noise to the inputs of the layers during training, which can help the model generalize better by making it less sensitive to small variations in the input data.\n\nAnother regularization technique we used was batch normalization. This method normalizes the inputs of each layer to have a mean of zero and a standard deviation of one, which can stabilize and accelerate the training process. Batch normalization also has a regularizing effect, as it adds a small amount of noise to the inputs during training.\n\nThese techniques collectively helped to improve the model's ability to generalize to unseen data, thereby reducing the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule are reported within the publication. The model was trained using a batch size of 512 and the Adam optimizer. The training process involved 80% of the data for training and 20% for testing. The mean square error (MSE) loss was used to measure the performance of the model prediction. The training and testing processes were conducted with one GeForce RTX 2080Ti GPU, and the model was implemented in TensorFlow with Keras.\n\nThe specific details of the model architecture, including the number of parameters in each layer, are provided in Table 2. This table outlines the layers used in the deep learning model, such as the embedding layer, batch normalization layer, 1D convolutional layers, Bi-directional Long Short-Term Memory Units (BiLSTM), GaussianNoise layer, and several dropout and dense layers.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly mentioned in the provided information. Therefore, it is not clear whether the model files and optimization parameters are publicly available or under what license they might be distributed.",
  "model/interpretability": "The model we have developed is primarily a black-box model, meaning that while it is highly effective in predicting the secondary structure content of proteins, the internal workings and the specific features it uses to make these predictions are not immediately transparent. This is characteristic of deep learning models, which often learn complex, non-linear relationships from data that are difficult to interpret directly.\n\nThe architecture of our model includes several layers designed to capture and process sequential data, similar to natural language processing models. These layers include an embedding layer, convolutional layers, bidirectional LSTM units, and dense layers. The embedding layer converts amino acid sequences into vectors, allowing the model to recognize patterns in the data. The convolutional layers capture local features and hierarchical patterns within the sequences, while the bidirectional LSTM units extract sequential relationships in both forward and backward directions. These components work together to produce predictions about the alpha-helix and beta-sheet content in proteins.\n\nWhile the model's architecture provides a high-level understanding of how it processes input data, the specific features and patterns it learns are not explicitly interpretable. For example, the convolutional layers might identify certain motifs or patterns in the amino acid sequences that are indicative of secondary structures, but without further analysis, it is challenging to pinpoint exactly what these motifs are. Similarly, the LSTM units capture temporal dependencies in the sequences, but the exact nature of these dependencies is not straightforward to interpret.\n\nIn summary, while our model is effective in predicting secondary structure content, it operates largely as a black-box. The internal mechanisms by which it makes predictions are complex and not easily interpretable, reflecting the general characteristics of deep learning models. Future work could involve techniques such as feature importance analysis or visualization methods to gain more insight into the model's decision-making process.",
  "model/output": "The model is a regression model designed to predict the ratios of protein secondary structures. Specifically, it focuses on determining the proportions of alpha-helix and beta-sheet content within protein sequences. The output of the model consists of scalar values representing these secondary structure ratios. This end-to-end regression model processes protein sequences through various layers, including embedding, convolutional, and bidirectional LSTM layers, to capture sequential features and relationships. The final output layer is a dense layer that provides the predicted ratios, which are then compared to ground truth labels using metrics such as the coefficient of determination (R2 score) to evaluate the model's performance. The training and testing processes demonstrate that the model achieves high R2 scores, indicating a strong predictive capacity for both alpha-helix and beta-sheet ratios.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several steps to ensure its reliability and predictive power. Initially, the training process was monitored by tracking the mean square error (MSE) loss for both training and validation datasets. This process showed significant decreases in error, converging to an MSE loss lower than 0.005 after 50 epochs, indicating effective learning. The R2 scores for alpha-helix and beta-sheet ratios in the training data were 0.99 and 0.96, respectively, demonstrating a slight difference between ground truth labels and predicted values, thus confirming the model's reliability.\n\nTo visualize the prediction ability, scatter plots were used to compare predictions against ground truth for both training and testing datasets. The overlap of testing and training points in these plots supported the model's accuracy, with a high coincidence rate for beta-sheet content predictions. The R2 scores for alpha-helix and beta-sheet in the testing dataset were 0.99 and 0.98, respectively, further validating the model's strong predictive capacity.\n\nThe model was also tested on protein sequences from the Protein Data Bank that were not included in the training dataset, specifically large SARS and COVID-19 proteins. The predicted proportions of alpha-helix and beta-sheet ratios were compared with results from other tools like PredictProtein, showing reasonable agreement and validating the model's predictive power against new experimental data.\n\nAdditionally, a case study involving the sequence of hen egg-white lysozyme was conducted. The model's predictions for alpha-helix and beta-sheet content were close to the values reported in the Protein Data Bank entry, further supporting the model's accuracy. Systematic sequence variations were explored to demonstrate the model's ability to predict secondary structure content for novel protein designs.\n\nOverall, the evaluation method combined training error analysis, visual comparisons, testing on independent datasets, and case studies to comprehensively assess the model's performance and reliability.",
  "evaluation/measure": "To evaluate the performance of our deep learning model, we primarily report the mean square error (MSE) loss and the coefficient of determination (R2 score). The MSE loss is a common metric used to measure the average squared difference between the predicted and actual values, providing a clear indication of the model's prediction accuracy. We track the MSE loss for both the training and validation datasets over epochs, observing a significant decrease and convergence to a value lower than 0.005 after 50 epochs. This convergence indicates that the model is effectively learning from the data and improving its predictions over time.\n\nIn addition to MSE, we use the R2 score to assess the model's predictive capacity. The R2 score, also known as the coefficient of determination, measures the proportion of the variance in the dependent variable that is predictable from the independent variables. An R2 score close to 1 indicates a high degree of predictability. For our model, the R2 scores for alpha-helix and beta-sheet ratios in the training data are 0.99 and 0.96, respectively, demonstrating a strong correlation between the predicted and actual values. Similarly, the R2 scores for the testing dataset are 0.99 for alpha-helix and 0.98 for beta-sheet, further confirming the model's reliability and generalizability.\n\nThese metrics are representative of standard practices in the literature for evaluating regression models, particularly in the context of protein structure prediction. The use of MSE and R2 scores allows for a comprehensive assessment of the model's performance, ensuring that it not only minimizes prediction errors but also captures the underlying patterns in the data effectively. The high R2 scores, in particular, indicate that our model's predictions are highly accurate and reliable, making it a robust tool for predicting protein secondary structure ratios.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our deep learning model with publicly available methods to assess its performance. Specifically, we compared our model's predictions against those generated by PredictProtein, a well-established tool that incorporates over 30 different methods to predict protein structures and derive information such as secondary structure content. This comparison was crucial for validating our model's accuracy and reliability.\n\nOur approach differs significantly from PredictProtein. While PredictProtein relies on a set of different tools and databases, our model takes a more streamlined approach. We feed protein sequences directly into our model, which then predicts the content of secondary protein structures. This end-to-end method allows for a quicker and more direct prediction of structural features and other properties.\n\nTo ensure a fair comparison, we used benchmark datasets that included proteins with known structures deposited in the Protein Data Bank (PDB). These datasets were not part of our training data, providing an unbiased assessment of our model's predictive power. The results demonstrated that our model's predictions for alpha-helix and beta-sheet ratios agreed reasonably well with those from PredictProtein and the PDB results. This agreement underscores the robustness and accuracy of our deep learning model.\n\nAdditionally, we performed comparisons with simpler baselines to understand the unique contributions of our model. These baselines included traditional machine learning algorithms and simpler neural network architectures. The results showed that our deep learning model, which includes convolution blocks, bidirectional LSTM blocks, and fully connected layers, outperformed these baselines in terms of prediction accuracy and convergence speed.\n\nOverall, our evaluation highlights the effectiveness of our end-to-end deep learning approach in predicting secondary structure content in proteins. The comparisons with publicly available methods and simpler baselines provide strong evidence of our model's superiority and its potential for applications in protein design and structural biology.",
  "evaluation/confidence": "The evaluation of our deep learning model's performance primarily focuses on the mean square error (MSE) loss and the coefficient of determination (R2 score). The MSE loss for both training and validation datasets significantly decreased and converged to a value lower than 0.005 after 50 epochs, indicating a high level of accuracy in our predictions. The R2 scores for alpha-helix and beta-sheet ratios in the training data are 0.99 and 0.96, respectively, which are very close to 1. This suggests that the model's predictions are highly reliable and that the difference between the ground truth labels and the predicted values is minimal.\n\nFor the testing dataset, the R2 scores for alpha-helix and beta-sheet ratios are 0.99 and 0.98, respectively. These high R2 scores indicate that the model has a strong predictive capacity for both alpha-helix and beta-sheet ratios. The scatter plots comparing the predictions against the ground truth further support this, as the training and testing points roughly overlap, particularly for alpha-helix predictions. While the beta-sheet predictions show slightly less overlap, they still demonstrate a high coincidence rate.\n\nThe statistical significance of our results is evident in the close agreement between the predicted ratios and the ground truth values, as well as the high R2 scores. However, specific confidence intervals for the performance metrics are not explicitly provided in the current evaluation. Future work could include a more detailed statistical analysis to provide confidence intervals and further validate the model's performance.\n\nAdditionally, the model's performance was tested against newly deposited protein sequences from the Protein Data Bank, which were not included in the training dataset. The results showed a reasonable agreement with predictions from other tools like PredictProtein and the PDB results, further supporting the model's reliability and generalizability.",
  "evaluation/availability": "Not enough information is available."
}