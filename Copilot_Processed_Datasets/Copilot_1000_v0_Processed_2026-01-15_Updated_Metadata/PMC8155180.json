{
  "publication/title": "Deep convolutional neural networks based ECG beats classification to diagnose cardiovascular conditions.",
  "publication/authors": "Rashed-Al-Mahfuz M, Moni MA, Lio' P, Islam SMS, Berkovsky S, Khushi M, Quinn JMW",
  "publication/journal": "Biomedical engineering letters",
  "publication/year": "2021",
  "publication/pmid": "34150350",
  "publication/pmcid": "PMC8155180",
  "publication/doi": "10.1007/s13534-021-00185-w",
  "publication/tags": "- Biomedical Engineering\n- ECG Classification\n- Deep Learning\n- Convolutional Neural Networks\n- VGG16\n- Arrhythmia Detection\n- Signal Processing\n- Continuous Wavelet Transform\n- Hilbert-Huang Transform\n- SHAP Values\n- Transfer Learning\n- Fine-Tuning\n- Performance Metrics\n- Sensitivity\n- Specificity\n- Accuracy\n- AUC\n- Gradient-Based Methods\n- Model Interpretation\n- MIT-BIH Arrhythmia Database",
  "dataset/provenance": "The dataset used in this study is sourced from the MIT-BIH arrhythmia database. This database consists of 48 records, each lasting half an hour, and was recorded from 47 subjects between 1975 and 1979 at the BIH Arrhythmia Laboratory. The ECG signals were obtained using two channels: the first lead is modified lead II (MLII), used for 45 recordings, and the second lead is a pericardial lead (V1 for 40 of the recordings, and V2, V4, or V5 for the others). For this study, only the MLII lead was utilized.\n\nFrom this database, 36 files containing the beats to be studied were selected. These files include records from subjects 100 through 124, 200 through 233. Notably, records 201 and 202 come from a single subject, while the remaining 34 records are from individual subjects. The dataset comprises a total of 38,000 beat segments, which were randomly sub-sampled into training, validation, and testing sets. Specifically, 8,291 samples for normal (N) beats, 7,659 for left bundle branch block (L) beats, 6,835 for right bundle branch block (R) beats, 6,612 for paced beats (PB), and 6,603 for premature ventricular contractions (V) beats were used for training. The remaining beat segments were used for testing and validation, with 200 beat segments of each type randomly selected to create a test set, and 1,000 beat segments each for validation and testing.\n\nThe dataset includes five different types of ECG beats: normal (N), left bundle branch block (L), right bundle branch block (R), paced beat (PB), and premature ventricular contractions (V). These beats were chosen for classification and analysis. The dataset has been widely used in the community for ECG arrhythmia classification and detection of characteristic frequencies in ECG beats.",
  "dataset/splits": "The dataset was split into three distinct parts: training, validation, and testing. For the training set, there were 8291 samples for normal (N) beats, 7659 for left bundle branch block (L) beats, 6835 for right bundle branch block (R) beats, 6612 for paced beats (PB), and 6603 for premature ventricular contractions (V) beats. The validation and testing sets each contained 1000 beat segments. From the remaining beat segments, 200 beat segments of each type were randomly selected to create the test set. The distribution of the number of beat segments from each of the classes and the datasets is provided in a table within the publication.",
  "dataset/redundancy": "The datasets used in this study were derived from the MIT-BIH arrhythmia database, which consists of 48 records of half-hour ECG signals from 47 subjects. For our analysis, we focused on the modified lead II (MLII) recordings and selected five different types of ECG beats: normal (N), left bundle branch block (L), right bundle branch block (R), paced beat (PB), and premature ventricular contractions (V).\n\nThe ECG signals were segmented into pieces containing three beats, each fixed to 2.4 seconds in length. This segmentation was based on the annotation files, which provided information about the rhythm type and individual heartbeat occurrences. A total of 38,000 beat segments were considered for analysis, collected from 36 specific files within the MIT-BIH database.\n\nTo ensure independence between training and test sets, the entire dataset was randomly sub-sampled. Specifically, 8,291 N beats, 7,659 L beats, 6,835 R beats, 6,612 PB beats, and 6,603 V beats were used for training the model. The remaining beat segments were used for testing and validation. From these remaining segments, 200 beat segments of each type were randomly selected to create a test set, resulting in a total of 1,000 beat segments for both validation and testing.\n\nThe distribution of beat segments across different datasets (DB I, DB II, DB III, and DB IV) varied based on the number of beat classes included. For instance, DB I contained only N and L beats, while DB IV included all five beat types. This approach ensured that the datasets were balanced and representative of the different beat classes, which is crucial for training robust classification models.\n\nThe random sub-sampling method used to split the datasets into training, validation, and testing sets helps to mitigate overfitting and ensures that the model's performance can be generalized to new, unseen data. This method is consistent with best practices in machine learning, where independence between training and test sets is essential for reliable model evaluation.",
  "dataset/availability": "The data used in this study is from the MIT-BIH arrhythmia database, which is publicly available. This database consists of 48 half-hour recordings of two-channel ambulatory ECG signals obtained from 47 subjects. The database is widely used in the research community for the classification of ECG arrhythmias and the detection of characteristic frequencies in ECG beats. The specific files used in this study were chosen based on the presence of the beats to be analyzed.\n\nThe data was split into training, validation, and testing sets. The entire dataset, consisting of 38,000 beat segments, was randomly sub-sampled. Specifically, 8,291 samples for normal beats, 7,659 for left bundle branch block beats, 6,835 for right bundle branch block beats, 6,612 for paced beats, and 6,603 for premature ventricular contractions were used for training. The remaining beat segments were used for testing and validation, with 1,000 beat segments each for validation and testing.\n\nThe MIT-BIH arrhythmia database is available under a license that allows for its use in research and educational purposes. The specific license details can be found on the PhysioNet website, where the database is hosted. The use of this database in our study was enforced by adhering to the terms and conditions specified in the license agreement. This includes proper citation of the database in all publications and presentations resulting from the use of the data.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is a convolutional neural network (CNN), specifically the VGG16 architecture. This is a well-established and widely used model in the field of deep learning for image classification tasks.\n\nThe VGG16 architecture is not a new algorithm; it was introduced by Simonyan and Zisserman in 2014. It consists of 16 layers, including convolutional layers, pooling layers, and fully connected layers. The model is known for its simplicity and uniformity, using only 3x3 convolutional filters and 2x2 pooling layers throughout the network.\n\nThe reason this algorithm was not published in a machine-learning journal is that it is already well-documented and established in the literature. Our work focuses on applying and fine-tuning this existing architecture for a specific task\u2014arrhythmia detection using ECG signals transformed into scalograms. We customized the fully connected layers of the VGG16 model to better suit our classification problem and fine-tuned the entire network using transfer learning techniques.\n\nThe use of VGG16 in our study is justified by its proven performance in various image classification tasks and its suitability for handling the high-dimensional data generated from ECG signals. By leveraging this established architecture, we aim to achieve robust and accurate classification of ECG beats, which is crucial for reliable arrhythmia detection.",
  "optimization/meta": "Not applicable. The model described does not use data from other machine-learning algorithms as input. It is a convolutional neural network (CNN) based classifier, specifically using the VGG16 architecture, which is trained and fine-tuned on ECG beat images. The model's performance is evaluated using metrics such as sensitivity, specificity, accuracy, and the area under the ROC curve. The training and validation processes involve using scalograms and HHT spectra, but these are image representations of ECG signals, not outputs from other machine-learning algorithms. The model interpretation methods used, such as SHAP values and expected gradients, are applied to the CNN itself to understand feature importance, rather than combining predictions from multiple machine-learning methods.",
  "optimization/encoding": "In our study, the data encoding process involved converting ECG signals into images using two different signal-to-image conversion approaches. The first approach utilized the continuous wavelet transform to generate scalograms, which represent the wavelet coefficient matrix of each ECG segment. These scalograms were then used as inputs for the classification model. The second approach employed the Hilbert-Huang transform to produce the model input, allowing us to evaluate the effectiveness of different signal-to-image conversion methods for the classifier.\n\nThe ECG signals were segmented into pieces containing three beats, each fixed at a length of 2.4 seconds. This segmentation ensured consistency in the input data for the classification model. The MIT-BIH arrhythmia database was used, which consists of 48 half-hour records from 47 subjects. The signals were band-pass filtered at 0.1-100 Hz and sampled at 360 Hz. Only the modified lead II (MLII) was used for the analysis. Five different types of ECG beats were selected for classification: normal (N), left bundle branch block (L), right bundle branch block (R), paced beat (PB), and premature ventricular contractions (V).\n\nThe data preprocessing steps included filtering and resampling the ECG signals to ensure they met the required specifications for input into the classification model. The annotation file in the MIT-BIH database provided information about the rhythm types and individual heartbeat occurrences, which was crucial for segmenting the ECG signals accurately. The dataset was divided into four different databases (DB I, DB II, DB III, DB IV), each containing a varying distribution of the selected beat types.\n\nThe scalograms and Hilbert-Huang spectra were then used as inputs for a deep learning model based on the VGG16 architecture. This model was customized with additional fully-connected layers, including a softmax layer for classifying the input into one of the five beat types. The model was fine-tuned using transfer learning, where the weights of the VGG16 layers were initialized with pre-trained values, and the dense layers were randomly initialized. This approach improved the model's performance by leveraging pre-existing knowledge from related tasks.",
  "optimization/parameters": "The model utilized in this study is based on the VGG16 architecture, which is a convolutional neural network (CNN) consisting of 16 layers. The input to this model is a segmented ECG beat image, specifically sized at 224 x 224 pixels in RGB format.\n\nThe first convolutional layer of the VGG16 architecture employs 64 kernels, each with a small receptive field of 3 x 3 pixels. This layer is designed to extract features from the input images with a convolution stride size of 1 pixel. Spatial padding of 1 pixel is applied to preserve the spatial resolution after convolution. Following the convolutional layer, a max-pooling operation is performed over a 2 x 2 pixel window with a 2-pixel stride.\n\nThe customized classifier layers of the model include five fully-connected layers. The first fully-connected (FC) layer contains 256 nodes activated by the ReLU function. The second FC layer has 512 ReLU-activated nodes. The third FC layer is a dropout layer with a 50% dropout rate, which helps in regularizing the model and preventing overfitting. The final layer is a softmax layer that provides the probability of each input belonging to a specific beat class.\n\nThe weights of the VGG16 parts of the model are initialized using transfer learning, while the dense layers are randomly initialized. The entire model is then fine-tuned using the training datasets to optimize the weights and achieve the desired performance.\n\nThe selection of parameters, such as the number of kernels and the architecture of the fully-connected layers, was based on empirical evidence and the need to balance computational efficiency with model performance. The use of transfer learning for initializing the VGG16 weights leverages pre-trained models that have shown good performance on related tasks, thereby reducing computational complexity and improving the model's accuracy.",
  "optimization/features": "In our study, the input features for the model are derived from the ECG beats, which are converted into images using two different signal-to-image conversion approaches: the continuous wavelet transform (CWT) and the Hilbert-Huang transform (HHT). The CWT produces a scalogram, which is a wavelet coefficient matrix representing the time-frequency characteristics of the ECG segment. This scalogram is used as the input to the classification model. The input image size for the VGG16 architecture, which is used in this study, is 224 x 224 RGB.\n\nThe number of features, f, is not explicitly stated as a fixed number because the features are extracted from the images generated by the CWT or HHT. The features are the pixel values of the input images, which are processed through the convolutional layers of the VGG16 network. The first convolutional layer uses 64 kernels with a 3 x 3 filter size to extract features from the input images. Subsequent layers further process these features.\n\nFeature selection in the traditional sense was not performed because the features are automatically extracted by the convolutional neural network (CNN) from the input images. The CNN learns to identify the most relevant features for classification during the training process. The model uses transfer learning, where the weights of the VGG16 parts of the model are initialized using a pre-trained model. The dense layers, which include fully-connected layers and a dropout layer, are randomly initialized and fine-tuned using the training datasets.\n\nThe training datasets were used to fine-tune the model and optimize the weights of the network. This process ensures that the model learns the most relevant features from the training data. The model's performance is evaluated using metrics such as sensitivity, specificity, accuracy, and the area under the ROC curve. The SHAP (SHapley Additive exPlanations) values are used to measure the importance of the input features based on the model's prediction results. This interpretation process helps to understand the contribution of each feature to the model's outcome.",
  "optimization/fitting": "The model utilized in this study is a convolutional neural network (CNN) based on the VGG16 architecture, which consists of 16 layers designed to classify 224 x 224 input images. The customized classifier layers include five fully-connected layers, with the last being a softmax layer that provides the probability of each input belonging to a specific beat class. The first fully-connected layer has 256 ReLU-activated nodes, the second has 512 ReLU-activated nodes, and the third is a dropout layer with a 50% dropout rate. This architecture ensures a sufficient number of parameters to capture complex patterns in the data.\n\nTo address the potential issue of overfitting, given the large number of parameters relative to the training points, several strategies were employed. Firstly, transfer learning was used to initialize the network weights, leveraging a pre-trained model that had shown good performance on related tasks. This approach helps in reducing the number of parameters that need to be learned from scratch, thereby mitigating overfitting. Secondly, fine-tuning was performed using the training datasets to optimize the weights of the network, ensuring that the model generalizes well to unseen data. Additionally, the inclusion of a dropout layer in the fully-connected layers helps in regularizing the model by preventing it from becoming too reliant on specific neurons.\n\nUnderfitting was ruled out by ensuring that the model had enough capacity to learn the underlying patterns in the data. The use of a deep CNN architecture with multiple convolutional and fully-connected layers allowed the model to capture both low-level and high-level features effectively. The fine-tuning process further ensured that the model could adapt to the specific characteristics of the training data, thereby avoiding underfitting. The high accuracy and performance metrics achieved on the test datasets, including sensitivity, specificity, and area under the ROC curve, indicate that the model was neither overfitted nor underfitted.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and improve the generalization of our model. One of the key methods used was transfer learning, where we initialized the network weights with a pre-trained model that had shown good performance on a related task. This approach helped to leverage the learned features from the pre-trained model and provided a better starting point for our specific classification task.\n\nAdditionally, we incorporated dropout regularization in our customized classifier layers. Specifically, we included a dropout layer with a 50% dropout rate. Dropout is a technique where, during training, a random subset of neurons is temporarily removed from the network, which helps to prevent the model from becoming too reliant on any single neuron and encourages it to learn more robust features.\n\nFurthermore, we fine-tuned the entire model using our training datasets. This process involved adjusting the weights of both the pre-trained layers and the newly added dense layers to optimize the model's performance on our specific task. Fine-tuning helps to adapt the pre-trained features to the nuances of the new dataset, thereby improving the model's accuracy and reducing the risk of overfitting.\n\nBy combining transfer learning, dropout regularization, and fine-tuning, we were able to achieve a well-generalized model that performed effectively on various test datasets. These techniques collectively contributed to the robustness and reliability of our classification model.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are not explicitly detailed in the provided information. However, the experiments were conducted using specific software tools and programming languages, including Python 3.5, CUDA 9.0, and Matlab R2018a. The classification and explanation experiments were implemented using the Keras library.\n\nThe model training platform consisted of a gaming laptop with an Intel(R) Core (TM) i7-7700HQ CPU, an NVIDIA GeForce GTX 1060 6GB GPU, and 16 GB of memory, running on a Windows 10 64-bit system. This setup was used to train and validate the models, with accuracy and loss plots generated for each epoch.\n\nRegarding the availability of model files and optimization parameters, the information provided does not specify where these can be accessed or under what license. The study focuses on the performance metrics and the effectiveness of different configurations of the VGG16 model, including transfer learning and fine-tuning, but does not provide direct links or details on how to access the specific model files or optimization parameters used.\n\nNot sure if the model files and optimization parameters are publicly available or if there are any specific licenses associated with their use. For detailed information on hyper-parameter configurations, optimization schedules, and access to model files, it would be necessary to refer to additional resources or supplementary materials that may accompany the publication.",
  "model/interpretability": "The model employed in this study is a deep convolutional neural network (CNN) based on the VGG16 architecture, which is inherently a black-box model. This means that the internal workings of the model are not easily interpretable, making it challenging to understand how specific input features contribute to the model's predictions.\n\nTo address this issue, a model interpretation method was utilized to reveal the importance of the features based on the model's prediction results. The primary objective of this interpretation process is to measure the SHAP (SHapley Additive exPlanations) values of the input features. SHAP values provide a way to quantify the total contribution of each feature to the model's outcome when all features are considered. For a given feature, the Shapley value can be computed as the average marginal contribution of that feature to the model's prediction.\n\nThe interpretation approach used in this study falls under the gradient-based method. This method involves calculating the gradient using backpropagation and measuring the contribution score of input features for the target class from this gradient. Specifically, the SHAP value for the features in the input layer of the VGG16 model was calculated using expected gradients. Expected gradients are an improved version of integrated gradients, which avoid using an arbitrary baseline by integrating the value of the feature over a dataset. This method helps in explaining the difference between the current prediction of the model and the model prediction with a given baseline input.\n\nBy using SHAP values, the model's interpretability is enhanced, allowing for a better understanding of the significant features of frequency components in temporal ECG waveforms. This interpretability is crucial for medical practitioners who need to diagnose and identify cardiovascular conditions accurately. The SHAP values increase the transparency of the model, making it applicable for the automation of cardiovascular diagnosis systems and usable by clinicians.",
  "model/output": "The model developed in this study is a classification model. It is designed to classify ECG beats into specific categories. The model utilizes a customized VGG16 convolutional neural network (CNN) architecture, which is a type of deep learning model commonly used for image classification tasks. The input to the model consists of ECG beat segments converted into scalograms or Hilbert-Huang transform (HHT) spectra. The model's output is the probability of each input ECG beat segment belonging to a particular class, such as normal (N), left bundle branch block (L), right bundle branch block (R), paced beat (PB), or premature ventricular contractions (V). The classification performance is evaluated using metrics such as sensitivity, specificity, accuracy, and the area under the ROC curve (AUC). The model achieved high performance, with accuracy and AUC values close to 100% for most test datasets, indicating its effectiveness in classifying ECG beats.",
  "model/duration": "The model training was conducted on a gaming laptop equipped with an Intel(R) Core (TM) i7-7700HQ CPU and an NVIDIA GeForce GTX 1060 6GB GPU, running on a Windows 10 64-bit system with 16 GB of memory. The experiments utilized both Matlab and Python programming languages, with specific software tools including Python 3.5, CUDA 9.0, and Matlab R2018a. The classification and explanation experiments were implemented using the Keras library.\n\nThe training process involved multiple epochs, with accuracy and loss plots generated for each epoch during both training and validation phases. The model achieved a final accuracy of 100% after 21 epochs for one set of data and 3 epochs for another, demonstrating the efficiency of the VGG16 CNN-based classifier. The use of transfer learning and fine-tuning significantly enhanced the model's performance, allowing it to reach high accuracy levels quickly. This setup ensured that the model could be trained and validated efficiently, leveraging the computational power of the GPU to handle the complexity of the tasks.",
  "model/availability": "The experiments conducted in this study were implemented using the Keras library. Keras is an open-source software library that provides a Python interface for artificial neural networks. It is capable of running on top of several lower-level deep learning frameworks, including TensorFlow, Microsoft Cognitive Toolkit, or Theano. The source code for Keras is publicly available on GitHub under the MIT License, which permits free use, modification, and distribution.\n\nThe software tools used in this study included Python 3.5, CUDA 9.0, and Matlab R2018a. These tools are widely used in the scientific community and are available for download from their respective official websites. Python and CUDA are open-source and free to use, while Matlab is a proprietary software that requires a license for use.\n\nNot applicable.",
  "evaluation/method": "The evaluation of the method involved several steps and datasets to ensure robust performance assessment. The experiments were conducted using a gaming laptop with an Intel Core i7-7700HQ CPU, an NVIDIA GeForce GTX 1060 6GB GPU, and 16 GB of memory, running on a Windows 10 64-bit system. The software tools used included Python 3.5, CUDA 9.0, and Matlab R2018a, with the classification and explanation experiments implemented using the Keras library.\n\nThe evaluation focused on comparing different configurations of the VGG16 convolutional neural network (CNN) classifier. These configurations included the original VGG16 with transfer learning, the original VGG16 with fine-tuning, and a modified VGG16 with fine-tuning. The performance was assessed using various imaging techniques, specifically the Continuous Wavelet Transform (CWT) scalogram and the Hilbert-Huang Transform (HHT) spectrum.\n\nAccuracy and loss plots were generated for each epoch of model training and validation, showing the progression of the classifier's performance. The final accuracy reached 100% after 21 epochs for one configuration and 3 epochs for another, demonstrating the effectiveness of the VGG16 CNN-based classifier.\n\nTest accuracy was evaluated across multiple datasets, including DB I, DB II, DB III, and DB IV. The original VGG16 with transfer learning and the HHT spectrum showed varying levels of accuracy, with a general trend of decreasing accuracy as the number of test classes increased. In contrast, the original VGG16 with the CWT scalogram achieved high sensitivity, specificity, accuracy, and AUC values, particularly for DB I and DB II.\n\nThe fine-tuned VGG16 with customized fully-connected (FC) layers was also evaluated, showing superior performance with the CWT scalogram compared to the HHT spectrum. This configuration achieved high sensitivity, specificity, accuracy, and AUC values across all datasets, with the CWT scalogram consistently outperforming the HHT spectrum.\n\nAdditionally, the performance of the proposed model was tested with different input dimensions of ECG beat segments, showing consistent results. The model was also evaluated using ECG beats from the American Heart Association (AHA) Ventricular Arrhythmia ECG Database and the Lobachevsky University Electrocardiography Database, further validating its robustness and accuracy.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our ECG beats classifier. These metrics include sensitivity, specificity, accuracy, and the area under the receiver operating characteristic curve (AUC).\n\nSensitivity, also known as recall, measures the proportion of true positive beats out of the total true positive and false negative beats. It indicates how well the model identifies positive cases.\n\nSpecificity measures the proportion of true negative beats out of the total true negative and false positive beats. It reflects the model's ability to correctly identify negative cases.\n\nAccuracy provides an overall measure of the model's performance by calculating the proportion of true results (both true positives and true negatives) out of the total number of cases examined.\n\nThe AUC is derived from the receiver operating characteristic curve, which plots sensitivity against 1-specificity. The AUC provides a single scalar value that summarizes the model's performance across all classification thresholds. A higher AUC indicates better model performance.\n\nThese metrics are widely used in the literature for evaluating classification models, particularly in medical and biomedical applications. They provide a comprehensive view of the model's performance, covering aspects such as the correct identification of positive and negative cases, as well as the overall accuracy and the trade-off between sensitivity and specificity.",
  "evaluation/comparison": "In the evaluation of our methods, a comprehensive comparison was conducted with publicly available methods on benchmark datasets. Specifically, we compared our approach using the VGG16 CNN with Continuous Wavelet Transform (CWT) time-frequency representation against various other studies and techniques. These comparisons included methods such as simple features with a Deep Belief Network, R-T interval CNN, Discrete Wavelet Transform (DWT) with a Multilayer Probabilistic Neural Network (MPNN), time-domain CNN, and others. Our method consistently achieved high accuracy, often reaching 100% in multiple datasets, demonstrating its robustness and effectiveness.\n\nAdditionally, we evaluated the performance of our model using different imaging techniques, such as the CWT scalogram and the Hilbert-Huang Transform (HHT) spectrum. The results showed that the CWT scalogram outperformed the HHT spectrum across all test datasets, highlighting the superiority of our chosen approach.\n\nWe also compared our fine-tuned VGG16 model with customized fully connected (FC) layers against simpler baselines, such as the original VGG16 without any modifications and transfer learning. The fine-tuned model showed significantly better performance, achieving higher sensitivity, specificity, accuracy, and Area Under the Curve (AUC) values. This comparison underscored the benefits of fine-tuning and customization in improving model performance.\n\nFurthermore, we tested the model's robustness by evaluating it on ECG beat segments of slightly different dimensions (2.2, 2.3, 2.4, 2.5 seconds). The results remained consistent, indicating that our model can accurately classify ECG beats even when the input dimensions vary slightly. This flexibility is crucial for real-world applications where ECG segments may not always be of uniform length.\n\nIn summary, our evaluation included a thorough comparison with publicly available methods and simpler baselines, demonstrating the superior performance and robustness of our approach. The use of the CWT scalogram and fine-tuned VGG16 model with customized FC layers proved to be highly effective in classifying ECG beats accurately.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}