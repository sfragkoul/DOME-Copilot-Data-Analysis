{
  "publication/title": "Forme fruste keratoconus detection with OCT corneal topography using artificial intelligence algorithms.",
  "publication/authors": "Mourgues E, Saunier V, Smadja D, Touboul D, Saunier V",
  "publication/journal": "Journal of cataract and refractive surgery",
  "publication/year": "2024",
  "publication/pmid": "39223730",
  "publication/pmcid": "PMC11556849",
  "publication/doi": "10.1097/j.jcrs.0000000000001542",
  "publication/tags": "- Artificial Intelligence\n- Machine Learning\n- Corneal Ectasia\n- Keratoconus\n- Forme Fruste Keratoconus\n- OCT Topography\n- Logistic Regression\n- Random Forest\n- Ectasia Screening Index\n- Corneal Mapping",
  "dataset/provenance": "The dataset used in our study was collected using the SS-OCT CASIA 2, which is a type of topographer. This device provided a comprehensive set of parameters related to corneal topography, including curvatures, Fourier transform data, elevation settings, pachymetry, and wavefront aberrations. The dataset consisted of 88 cases of forme fruste keratoconus (FFKC), which is a significant number compared to previous studies but still relatively small for AI algorithms that typically require thousands of data entries.\n\nOur study is one of the few conducted on the CASIA 2 and is the only one comparing the performance of the Ectasia Screening Index (ESI) with other AI models. The parameters collected included various keratometric values, elevation data, and aberration metrics, which were then analyzed using AI algorithms such as logistic regression and random forest. These algorithms helped identify the most discriminant variables for detecting FFKC.\n\nThe use of the CASIA 2 distinguishes our study from most others that have used the Pentacam. The dataset's robustness comes from the integration of multiple parameters, with a focus on selecting the most critical ones for accurate detection of FFKC. However, one limitation is the absence of epithelial mapping data, which could have provided additional insights. This data was not accessible due to the software version available at the time of the study.",
  "dataset/splits": "The dataset was divided into five equal subsets for the purpose of cross-validation. This process involved training the model on four of these subsets and testing it on the remaining fifth subset. This operation was repeated five times, ensuring that each subset was used once for testing and four times for training. This method is known as 5-fold cross-validation. The distribution of data points in each split was equal, as the data was divided into five subsets of approximately the same size.",
  "dataset/redundancy": "To ensure the robustness of our analysis, we employed a cross-validation method with a 5-fold increase. This involved dividing the dataset into 5 equal subsets. The model was then trained on 4 of these subsets and tested on the remaining subset. This process was repeated 5 times, ensuring that each subset was used once for testing and four times for training. This approach helped to mitigate overfitting and provided a more reliable estimate of the model's performance.\n\nThe training and test sets were kept independent by ensuring that the data used for training did not overlap with the data used for testing in each fold. This independence was crucial for evaluating the model's generalizability and its ability to perform well on unseen data.\n\nRegarding the distribution of our dataset, it is important to note that we analyzed 205 variables initially, but only the most discriminant ones were retained through automated relevance selection by the models. This selection process was not influenced by human intervention, ensuring an objective and data-driven approach.\n\nOur dataset included a significant number of cases, with 88 instances of forme fruste keratoconus (FFKC), which is a substantial cohort compared to previous studies. However, it is acknowledged that the overall patient count is still relatively small for AI algorithms, which typically require thousands of data entries for optimal training.\n\nThe demographic and topographical characteristics of the groups were carefully considered. For instance, there were more men in the FFKC group, and the patients were significantly younger than those in the normal eye group. These demographic differences were taken into account to ensure that the model's performance was not biased by these factors.\n\nIn summary, our dataset was split using a rigorous cross-validation method to ensure independence between training and test sets. The distribution of our dataset, while substantial in terms of FFKC cases, is still relatively small for AI algorithms. However, the automated relevance selection of variables and the consideration of demographic factors helped to enhance the robustness and reliability of our analysis.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in our study belong to the supervised learning class. Specifically, we employed logistic regression and random forest algorithms. These algorithms are well-established in the field of machine learning and have been extensively used in various applications, including medical diagnostics.\n\nNeither of these algorithms is new; they are widely recognized and have been thoroughly studied and validated in numerous research papers and practical applications. Logistic regression is a fundamental algorithm used for binary classification problems, while random forest is an ensemble learning method that combines multiple decision trees to improve predictive accuracy and control over-fitting.\n\nThe choice to use these established algorithms was driven by their proven effectiveness in handling complex datasets and their ability to provide robust and interpretable results. Given the specific requirements of our study, which involved detecting corneal ectasia using OCT topography data, these algorithms were well-suited to the task.\n\nThe focus of our publication is on the application of these algorithms to a specific medical problem rather than the development of new machine-learning techniques. Therefore, publishing in a machine-learning journal was not the primary objective. Instead, we aimed to contribute to the field of ophthalmology by demonstrating the potential of these algorithms in improving the diagnosis of corneal conditions.",
  "optimization/meta": "The study did not employ a meta-predictor model. Instead, it utilized two distinct machine-learning algorithms: logistic regression and random forest. These algorithms were applied independently to analyze the data, with each model processing the same set of input variables derived from the CASIA-2 OCT system.\n\nThe logistic regression model focused on determining a function that predicts the probability of an eye being affected by KC or FFKC based on an input vector of eye-related characteristics. The random forest model, on the other hand, built multiple decision trees using bootstrap resampling and selected variables to make predictions. Each tree in the random forest contributed to the final decision, and the method assessed variable importance to identify key features for prediction.\n\nBoth algorithms were validated using a cross-validation method with a fold-increase of 5, ensuring that the data was divided into subsets for training and testing. This approach helped in assessing the robustness and generalization capability of the models.\n\nThe study did not combine the outputs of these algorithms into a meta-predictor. Instead, it compared the performance of each algorithm individually against the ESI for detecting corneal ectasia. The logistic regression and random forest models demonstrated high sensitivity and specificity in detecting FFKC, outperforming the ESI in terms of sensitivity.",
  "optimization/encoding": "In our study, the data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms. We utilized a comprehensive set of 205 topographic parameters collected using the CASIA 2 OCT system. These parameters included various metrics such as curvatures, Fourier transform data, elevation settings, ESI, pachymetry, and wavefront aberrations.\n\nThe preprocessing involved several key steps. First, we performed automated relevance selection to retain only the most discriminant variables. This step was essential to filter out irrelevant data and focus on the parameters that significantly contributed to the detection of FFKC. The selection process was entirely automated, ensuring that no human bias influenced the choice of variables.\n\nNext, we encoded the input data into a structured format suitable for the machine-learning algorithms. The input vector X encapsulated various eye-related characteristics, which were then used to predict the target variable Y. In our case, Y represented whether the eye was affected by KC or FFKC. The primary objective was to determine a function h that operated on the input vector X and yielded a probability value between 0 and 1, indicating the likelihood of the eye being affected by FFKC.\n\nFor the logistic regression model, the sigmoid function played a crucial role in transforming the input vector into a probability, ensuring that the output always fell within the 0 to 1 range. This function was parameterized by multiple variables, which were fine-tuned to achieve the most accurate predictions.\n\nIn the random forest model, each tree was constructed using a random subset of data through bootstrap resampling. The best variable for each split was chosen from a predefined set of randomly selected variables, with each tree contributing a \"vote\" to the final decision. This method assessed variable importance to identify key features for prediction, addressing issues like overfitting and enhancing the model's resilience to noisy or missing data.\n\nOverall, the data encoding and preprocessing steps were designed to optimize the performance of the machine-learning algorithms, ensuring that they could effectively detect FFKC and KC with high accuracy and reliability.",
  "optimization/parameters": "In our study, we analyzed a total of 205 variables collected using the CASIA 2 OCT system. These variables encompassed a wide range of topographic parameters, including curvatures, Fourier transform data, elevation settings, ESI, pachymetry, and wavefront aberrations. To ensure the robustness of our models, we employed automated relevance selection, allowing the algorithms themselves to identify and retain the most discriminant variables. This process was crucial as it eliminated the need for human intervention and ensured that only the most relevant data were used for detecting forme fruste keratoconus (FFKC). Consequently, some data from the CASIA 2 system were not utilized in the final models. The parameters that were ultimately deemed most important included optical aberration parameters and Fourier transform data, which played significant roles in distinguishing between FFKC and normal eyes.",
  "optimization/features": "In our study, we analyzed a total of 205 variables as potential input features. However, not all of these features were retained for the final models. Instead, we employed automated relevance selection within our machine learning algorithms to identify and retain only the most discriminant features. This process ensured that the final models used the most informative variables for detecting FFKC.\n\nThe feature selection was performed using the training data only, adhering to best practices in machine learning to prevent data leakage and ensure the robustness of our models. This approach allowed us to focus on the most relevant features without human bias, enhancing the reliability and generalizability of our results.",
  "optimization/fitting": "The study utilized two AI-based algorithms, logistic regression and random forest, for data analysis. In logistic regression, the input vector X encapsulated various eye-related characteristics, and the target variable Y represented whether the eye was affected by KC or FFKC. The function h, parameterized by N parameters, was fine-tuned to achieve accurate predictions. The sigmoid function ensured that the output always fell within the 0 to 1 range, signifying the probability of a specific event.\n\nRandom forest, a machine-learning technique, built a robust model by combining multiple forest trees. Each tree was constructed using a random subset of data through bootstrap resampling. The best variable for each split was chosen from a predefined set of randomly selected variables. Each tree contributed a \"vote\" to the final decision, and the method assessed variable importance to identify key features for prediction.\n\nTo address overfitting, random forest built trees on different samples, enabling more efficient generalization and resilience to noisy or missing data. This technique helped in ruling out overfitting by ensuring that the model did not memorize the training data but rather generalized well to unseen data.\n\nThe study used a cross-validation method with a fold-increase of 5. The data was divided into 5 equal subsets, with the model trained on 4 subsets and tested on the fifth subset. This operation was repeated 5 times, ensuring that each subset was used once for testing and four times for training. This approach helped in ruling out underfitting by ensuring that the model was trained on a diverse set of data points, covering various scenarios and variations within the dataset.\n\nThe algorithms analyzed 205 variables, retaining only the most discriminant ones through automated relevance selection by the models. This process ensured that the models were not underfitted by including only the most relevant features for prediction. The performance metrics, including accuracy, precision, recall, and F1 score, were used to assess the robustness of the models. The area under the receiver operating characteristic curve was also used to visualize the trade-off between true positives and false positives, providing a comprehensive assessment of the model's performance.",
  "optimization/regularization": "In our study, we employed the random forest algorithm, which inherently includes a regularization method to prevent overfitting. This technique builds multiple decision trees using different subsets of the data through a process called bootstrap resampling. Each tree is constructed using a random subset of variables, and the best variable for each split is chosen from a predefined set of randomly selected variables. This approach helps to reduce the correlation between trees and enhances the model's generalization ability.\n\nAdditionally, the random forest method assesses variable importance, which helps in identifying key features for prediction and further reduces the risk of overfitting by focusing on the most relevant variables. By building trees on different samples, the random forest algorithm ensures more efficient generalization and resilience to noisy or missing data, thereby mitigating overfitting issues commonly encountered in machine learning models.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are not explicitly detailed in the provided information. However, the main analysis was conducted using the DATAIKU software with AI, specifically employing machine learning techniques such as logistic regression and random forest. The study involved a cross-validation method with a 5-fold increase, where the data was divided into 5 equal subsets, and the model was trained on 4 subsets while tested on the fifth subset. This process was repeated 5 times to ensure each subset was used once for testing and four times for training.\n\nThe performance metrics, including sensitivity, specificity, precision, recall, F1 score, and accuracy, were derived from the confusion matrix and used to assess the robustness of the models. The area under the receiver operating characteristic curve (AUC-ROC) was also utilized to evaluate the models' performance.\n\nRegarding the availability of model files and optimization parameters, there is no specific mention of where these can be accessed or under what license. The study focuses on the comparison of the developed algorithms with the ESI for detecting corneal ectasia, highlighting the superior performance of logistic regression and random forest in terms of sensitivity and specificity.\n\nFor further details on the hyper-parameter configurations and optimization parameters, additional information from the original study or supplementary materials would be required.",
  "model/interpretability": "The models employed in our study, specifically logistic regression and random forest, offer varying degrees of interpretability. Logistic regression is generally considered more transparent. It operates by determining a function that takes an input vector, comprising various eye-related characteristics, and outputs a probability. This function is parameterized and fine-tuned to achieve accurate predictions. The sigmoid function is a crucial component, ensuring that the output always falls within the 0 to 1 range, representing the probability of an event, such as the eye being affected by FFKC.\n\nRandom forest, on the other hand, is somewhat more of a black-box model. It builds a robust model by combining multiple decision trees, each constructed using a random subset of data through bootstrap resampling. Each tree contributes a \"vote\" to the final decision, and the method assesses variable importance to identify key features for prediction. While random forest provides insights into variable importance, the exact decision paths within each tree can be complex and less interpretable.\n\nIn our study, the random forest model highlighted the importance of optical aberration parameters and Fourier transform data. Specifically, variables such as the decentering on the vertical axis (Y-axis) of different data points like IPIy, EPHy, and CTPy were identified as significant, with their importance factor ranging between 4% and 11%. This indicates that while random forest can pinpoint critical variables, the overall decision-making process within the model remains less transparent compared to logistic regression.",
  "model/output": "The model employed in our study is primarily focused on classification tasks. We utilized two distinct AI-based algorithms: logistic regression and random forest. Both of these algorithms were designed to classify whether an eye is affected by keratoconus (KC) or forme fruste keratoconus (FFKC). The logistic regression model works by determining a function that operates on an input vector of eye-related characteristics to yield a probability value between 0 and 1, indicating the likelihood of the eye being affected by FFKC. The random forest model, on the other hand, builds multiple decision trees using bootstrap resampling and combines their outputs to make a final classification decision. The performance of these models was evaluated using metrics derived from the confusion matrix, including sensitivity, specificity, precision, recall, and the F1 score. Additionally, we used the area under the receiver operating characteristic curve to visualize the trade-off between true positives and false positives. The models demonstrated high accuracy and robustness in detecting FFKC and distinguishing it from normal eyes, with logistic regression showing slightly better performance overall.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the algorithms used in this study is not publicly released. The main analysis was conducted using the DATAIKU software, which employs AI and machine learning techniques, specifically logistic regression and random forest algorithms. However, the specific implementations and details of these algorithms tailored for this study are not made available to the public.\n\nThe study utilized the CASIA-2 software for data collection, but the algorithms developed for detecting forme fruste keratoconus (FFKC) and keratoconus (KC) are proprietary and not distributed as standalone executables, web servers, virtual machines, or container instances. The performance and validation of these algorithms are thoroughly discussed in the publication, but the actual code and methods to run the algorithms independently are not provided.\n\nFor those interested in replicating or building upon this work, the detailed methodology and results are documented in the paper, including the use of cross-validation techniques and performance metrics such as sensitivity, specificity, precision, recall, and the F1 score. However, the specific implementation details and software tools used are not openly available.",
  "evaluation/method": "The evaluation method employed in our study involved a robust cross-validation technique to ensure the reliability and generalizability of our models. Specifically, we used a 5-fold cross-validation approach. This method involved dividing the dataset into five equal subsets. The model was then trained on four of these subsets and tested on the remaining fifth subset. This process was repeated five times, ensuring that each subset was used once for testing and four times for training. This technique helps in mitigating overfitting and provides a more accurate estimate of the model's performance on unseen data.\n\nIn addition to cross-validation, we utilized several performance metrics derived from the confusion matrix to assess the robustness of our algorithms. These metrics included sensitivity (recall), specificity, precision, and the F1 score. The F1 score, in particular, combines recall and precision into a single metric, offering a comprehensive assessment of the model's performance. We also computed the accuracy, which is the ratio of correct classifications to the total number of classifications. To visualize the trade-off between true positives and false positives, we employed the receiver operating characteristic (ROC) curve.\n\nThe performance of our algorithms was compared with the Ectasia Screening Index (ESI) for detecting corneal ectasia. The ESI's sensitivity and specificity were calculated to assess its ability to detect forme fruste keratoconus (FFKC) within our study population. Our logistic regression and random forest algorithms demonstrated superior performance in terms of sensitivity and specificity compared to the ESI. The logistic regression model achieved 100% sensitivity and specificity, while the random forest model had a sensitivity of 84% and a specificity of 90%. These results highlight the effectiveness of our AI-based approaches in detecting FFKC.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our AI algorithms in detecting corneal conditions. The primary metrics reported include the area under the receiver operating characteristic curve (AUC ROC), recall (sensitivity), precision, F1 score, and accuracy. These metrics were derived from the confusion matrix, which compares the algorithms' predictions with actual values.\n\nThe AUC ROC provides a measure of the model's ability to distinguish between different classes, with values closer to 1 indicating better performance. Recall, also known as sensitivity, measures the proportion of actual positives correctly identified by the model. Precision, or the positive predictive value, indicates the proportion of predicted positives that are actual positives. The F1 score combines recall and precision into a single metric, offering a balanced measure of a model's accuracy. Accuracy is the ratio of correct classifications to the total number of classifications, providing an overall measure of the model's correctness.\n\nThese metrics are widely used in the literature and are representative of standard evaluation practices in machine learning and medical diagnostics. They allow for a thorough assessment of the algorithms' performance, ensuring that both the true positive and false positive rates are considered. The use of these metrics enables a clear comparison with other studies and provides a robust evaluation of our models' effectiveness in detecting corneal ectasia.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of our developed algorithms with an existing, publicly available method, the Ectasia Screening Index (ESI), which is part of the CASIA 2 software. This comparison was crucial for evaluating the performance of our algorithms in detecting corneal ectasia, specifically forme fruste keratoconus (FFKC).\n\nWe assessed the sensitivity and specificity of the ESI score within our study population and compared these metrics with those of our logistic regression and random forest algorithms. The ESI exhibited a sensitivity of 28% and a specificity of 100% in our population, indicating that it correctly identified normal corneas but missed a significant portion of FFKC cases. In contrast, our logistic regression algorithm achieved 100% sensitivity and specificity, while the random forest algorithm had a sensitivity of 84% and a specificity of 90%. These results highlight the superior performance of our algorithms in detecting FFKC compared to the ESI.\n\nAdditionally, we evaluated the area under the receiver operating characteristic curve (AUC-ROC) for both our algorithms and the ESI. The AUC-ROC for detecting FFKC was 0.99 for logistic regression and 0.98 for the random forest, demonstrating excellent discriminative ability. These values are significantly higher than what would be expected from simpler baselines, such as basic statistical models or rule-based systems, which typically do not achieve such high performance in complex medical diagnostics.\n\nThe comparison with the ESI, a method that is widely used in clinical practice, provides a robust benchmark for our algorithms. The ESI's underperformance in identifying FFKC cases underscores the need for more advanced and sensitive diagnostic tools. Our algorithms not only matched but exceeded the ESI's specificity while significantly improving sensitivity, making them more reliable for detecting early-stage corneal ectasia.\n\nIn summary, our study included a comprehensive comparison with a publicly available method and demonstrated that our algorithms outperform simpler baselines and existing clinical tools in detecting FFKC. This comparison validates the effectiveness and potential clinical utility of our AI-based diagnostic approaches.",
  "evaluation/confidence": "The evaluation of our algorithms involved a rigorous assessment using various performance metrics, which were derived from the confusion matrix. These metrics included sensitivity (recall), specificity, precision, and the F1 score. The area under the receiver operating characteristic curve (AUC-ROC) was also used to evaluate the performance of our models.\n\nThe performance metrics for both logistic regression and random forest algorithms were reported with high values, indicating strong performance. For instance, the AUC-ROC for detecting forme fruste keratoconus (FFKC) was 0.99 for logistic regression and 0.98 for the random forest. The accuracy, precision, recall, and F1 score were also reported, with logistic regression achieving an accuracy of 0.97, precision of 0.93, recall of 1, and an F1 score of 0.96. The random forest had an accuracy of 0.94, precision of 1.0, recall of 0.84, and an F1 score of 0.91.\n\nStatistical significance was assessed through the use of cross-validation, specifically a 5-fold cross-validation method. This method involved dividing the data into 5 equal subsets, training the model on 4 subsets, and testing it on the fifth subset. This process was repeated 5 times, ensuring that each subset was used once for testing and four times for training. This approach helps to ensure that the results are robust and generalizable.\n\nThe comparison with the Ectasia Screening Index (ESI) showed that our algorithms significantly outperformed the ESI in terms of sensitivity. The ESI had a sensitivity of only 28% and a specificity of 100%, while our logistic regression algorithm achieved 100% sensitivity and specificity. The random forest algorithm also performed well, with a sensitivity of 84% and a specificity of 90%.\n\nIn summary, the performance metrics for our algorithms are robust and statistically significant. The use of cross-validation and the comparison with the ESI provide confidence in the superiority of our methods. The high values of the performance metrics and the statistical significance of the results support the claim that our algorithms are effective in detecting FFKC and other corneal conditions.",
  "evaluation/availability": "The raw evaluation files, such as the confusion matrices for the random forest and logistic regression algorithms, are available as supplemental tables. These can be accessed at the provided link. Additionally, the receiver operating characteristic curves for the ESI and the algorithms are available as supplemental figures, also accessible via a link. These resources are intended to support the transparency and reproducibility of the study's findings. The supplemental materials are freely available to the public, facilitating further analysis and validation by other researchers."
}