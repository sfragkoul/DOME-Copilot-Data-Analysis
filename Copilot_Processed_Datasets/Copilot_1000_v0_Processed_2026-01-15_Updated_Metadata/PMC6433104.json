{
  "publication/title": "Prediction of cardiovascular outcomes with machine learning techniques: application to the Cardiovascular Outcomes in Renal Atherosclerotic Lesions (CORAL) study.",
  "publication/authors": "Chen T, Brewster P, Tuttle KR, Dworkin LD, Henrich W, Greco BA, Steffes M, Tobe S, Jamerson K, Pencina K, Massaro JM, D'Agostino RB Sr, Cutlip DE, Murphy TP, Cooper CJ, Shapiro JI",
  "publication/journal": "International journal of nephrology and renovascular disease",
  "publication/year": "2019",
  "publication/pmid": "30962703",
  "publication/pmcid": "PMC6433104",
  "publication/doi": "10.2147/ijnrd.s194727",
  "publication/tags": "- Cardiovascular Outcomes\n- Renal Atherosclerotic Lesions\n- Machine Learning\n- Prediction Models\n- Random Forest\n- Support Vector Machine\n- Decision Tree\n- Neural Network\n- Logistic Regression\n- Composite Endpoint\n- Renal Artery Stenosis\n- Medical Therapy\n- Stenting\n- ROC Curve\n- Sensitivity and Specificity\n- Baseline Laboratory Values\n- Hypertension\n- Renal Function\n- Glycosylated Hemoglobin\n- Blood Pressure",
  "dataset/provenance": "The dataset used in our study is derived from the Cardiovascular Outcomes in Renal Atherosclerotic Lesions (CORAL) trial. This was a prospective, international, multicenter clinical trial that enrolled 931 participants with atherosclerotic renal artery stenosis (ARAS). The trial aimed to compare the outcomes of optimal medical therapy plus stenting versus medical therapy alone.\n\nInitially, the dataset contained 931 patients. However, due to missing values, particularly in baseline laboratory values, the dataset was cleaned. Variables with more than 40% missing values were excluded, and for variables with less than 20% missing values, the average value was used to fill in the gaps. Non-numerical data with missing values led to the exclusion of those subjects from further analysis. Ultimately, 573 subjects with complete records were analyzed.\n\nThe CORAL trial data has not been extensively analyzed using machine learning approaches prior to our study. To the best of our knowledge, neither the CORAL nor the ASTRAL datasets have been subjected to such analyses. Our reanalysis of the CORAL dataset was undertaken with the idea that machine learning methods might discern patterns that are not apparent through routine clinical judgment. The data cleaning process was performed using the open-source program R, and the specific R code used for this process is detailed in the supplementary materials.",
  "dataset/splits": "The dataset was split into training and testing sets using an 80% training and 20% testing distribution. This split was achieved by varying seed values to initiate randomization. Additionally, a strategy of three repeats of ten-fold cross-validation was employed on the training subsets. This means that the training data was further divided into ten parts, with the model trained on nine parts and tested on the remaining one part, repeated ten times, and this process was repeated three times to ensure robustness. The testing set remained constant throughout these iterations. This approach helps in evaluating the model's performance more comprehensively by ensuring that each data point is used for both training and testing across different folds.",
  "dataset/redundancy": "The dataset used in this study was derived from the Cardiovascular Outcomes in Renal Atherosclerotic Lesions (CORAL) trial, which included 931 participants. However, due to missing values, especially in baseline laboratory values, the analysis was performed on 573 subjects with complete records. The data was split into training and testing subsets using an 80%:20% distribution. This split was achieved by employing various seed values to initiate randomization, ensuring that the training and testing sets were independent. The strategy of three repeats of the ten-folds with CARET was used on the different training subsets, further varying the seed to divide the set into training and testing subsets.\n\nThe use of different seed values for splitting the data helped to ensure that the results were robust and not dependent on a particular random split. This approach is consistent with best practices in machine learning to prevent overfitting and to ensure that the model generalizes well to unseen data. The distribution of the data in this study is comparable to other machine learning datasets where similar splitting strategies are employed to maintain the independence of training and testing sets. The use of multiple seed values and the ten-fold cross-validation strategy helps to mitigate the risk of dataset redundancy and ensures that the model's performance is evaluated on diverse subsets of the data.",
  "dataset/availability": "The data used in this study is not publicly available. The dataset originates from the CORAL trial, a prospective, international, multicenter clinical trial. The trial randomly assigned 931 participants with atherosclerotic renal artery stenosis (ARAS) to either medical therapy plus stenting or medical therapy alone. The primary endpoint was the first occurrence of a major cardiovascular or renal event.\n\nThe data cleaning and preparation process involved handling missing values and merging clinical and laboratory data. However, the specific datasets used for the machine learning analyses, including the data splits for training and testing, are not released in a public forum. The analyses were performed using the open-source program R, and the R code employed for data cleaning is provided in the supplementary materials. The parameters used for subsequent analysis are also detailed in the supplementary materials.\n\nThe study adhered to the principles of the Declaration of Helsinki and was approved by the institutional review boards or ethics committees at each participating site. All participating subjects provided written informed consent. The data sharing policies and licenses related to the CORAL trial data are not specified in the available information.",
  "optimization/algorithm": "The machine-learning algorithms employed in our study are well-established and widely used in the field. We utilized several different classes of algorithms, including a generalized linear model (logistic regression), support vector machine (SVM), decision tree, feed-forward neural network, and random forest. These algorithms are not new but are robust and effective for various predictive modeling tasks.\n\nThe SVM, for instance, involves multidimensional sorting of data based on the development of a hyperplane that best segregates the two classes. We used the radial kernel option from the CARET package, which is a complex function that transforms input from a lower dimension to a higher dimension, useful for nonlinear separation problems. The tuning parameters for the SVM included the kernel and C, where C is a regularization parameter specifying the penalty for misclassification.\n\nThe decision tree method, while straightforward, mirrors human decision-making with a limited number of measurements used for categorization. However, it performed quite poorly in our study.\n\nThe random forest approach generally outperformed the other methods, particularly in identifying patients who achieved composite endpoints. This method aggregates the results of multiple decision trees to improve predictive accuracy and control over-fitting.\n\nThe neural network model provided a good balance between sensitivity and specificity, although it did not yield the highest accuracy in most simulations. The logistic regression model served as our default method, using only baseline variables for the prediction of composite endpoint outcomes.\n\nThese algorithms were chosen for their proven effectiveness in similar studies and their ability to handle the complexity of the data from the Cardiovascular Outcomes in Renal Atherosclerotic Lesions (CORAL) study. The algorithms were optimized within the CARET package of R, ensuring that the models were fine-tuned for the best performance on the given dataset.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "For the machine-learning algorithms, the data underwent several preprocessing steps to ensure optimal performance. Initially, variables with moderate amounts of missing numeric data had their average values imputed into the missing value categories, specifically for variables with less than 20% missing values. Non-numeric data with missing values, such as race, gender, and smoking status, led to the exclusion of those subjects from further analysis. This resulted in a dataset of 573 subjects with complete records.\n\nThe dataset was then cleaned using R code, which involved merging clinical and laboratory data. Empty columns were automatically removed, and the outcome variable was converted into a binary format (yes or no). The dataset was further cleaned by removing columns with excessive missing values and imputing the mean for numerical variables with missing data. This process ensured that the final dataset was complete and ready for analysis.\n\nBefore applying the machine-learning models, all numerical data were centered and scaled. This standardization is crucial for algorithms like support vector machines and neural networks, which are sensitive to the scale of the input features. The data was then split into training and testing sets using different seed values to initiate randomization, ensuring that the models were evaluated on diverse subsets of the data.\n\nFor the support vector machine (SVM), two tuning parameters were employed: the kernel and the regularization parameter C. The radial kernel was used, which requires specifying the Sigma parameter. The values of Sigma and C were optimized within the CARET package, with values of 1e-3 and 32 being used thereafter. For the random forest, the number of trees (ntree) was set to 1,000, and the number of variables randomly sampled as candidates at each split (mtry) was optimized at 9. The neural network used a feed-forward architecture with one hidden layer containing nine hidden neurons and a decay value of 0.24.\n\nThese preprocessing and encoding steps were essential to prepare the data for the machine-learning algorithms, ensuring that the models could effectively learn from the data and make accurate predictions.",
  "optimization/parameters": "In our study, we utilized several machine learning models, each with its own set of tuning parameters. For the random forest model, two key parameters were considered: the number of trees (ntree) and the number of variables randomly sampled as candidates at each split in each tree (mtry). Through our analysis, we set ntree at 1,000 and optimized mtry at 9.\n\nFor the neural network model, three tuning parameters were essential: the number of hidden layers, the number of nodes in each hidden layer, and the decay parameter, which restricts the weighting from becoming too large. After initial exploration, we found optimal performance with one hidden layer containing nine hidden neurons and a decay value of 0.24.\n\nThe support vector machine (SVM) model involved two tuning parameters: the kernel and the regularization parameter C. We used the radial kernel option, which required an additional parameter, Sigma. Through optimization within the CARET package, we determined that Sigma and C values of 1e-3 and 32, respectively, yielded the best performance.\n\nIn summary, the selection of these parameters was driven by a combination of initial exploration and optimization techniques to ensure the models performed effectively on the given dataset.",
  "optimization/features": "In our study, we utilized a comprehensive set of features derived from patient data to train and test our machine learning models. The input features included a wide range of clinical and demographic variables, such as age, presence of angina, use of antiplatelet agents, baseline systolic and diastolic blood pressure, body mass index, estimates of glomerular filtration rate, presence of diabetes, gender, hemoglobin levels, cholesterol levels, and many others. In total, there were 36 features used as input.\n\nFeature selection was not explicitly performed as a separate step in our analysis. Instead, we relied on the variable importance measures provided by the different machine learning models to identify the most influential features. This approach allowed us to understand which variables were consistently highlighted by the models as strong predictors of adverse outcomes. For instance, baseline systolic blood pressure and estimates of renal function were frequently emphasized across different models.\n\nAll preprocessing steps, including centering and scaling of numerical data, were performed using the training set only. This ensured that the testing set remained unseen during the model training phase, maintaining the integrity of our validation process. The use of cross-validation with multiple seeds further ensured that our results were robust and not dependent on a particular split of the data.",
  "optimization/fitting": "In our study, we employed several machine learning methods, each with its own set of tuning parameters to optimize performance. For the random forest model, we set the number of trees (ntree) to 1,000 and optimized the number of variables (mtry) randomly sampled as candidates at each split in each tree. For the neural network, we explored different architectures and found optimal performance with one hidden layer containing nine hidden neurons and a decay value of 0.24. The decay parameter helped to restrict the weighting from becoming too large, thereby mitigating overfitting.\n\nFor the support vector machine (SVM), we used the radial kernel and optimized the Sigma and C values. Sigma controls the width of the radial basis function, and higher values can lead to overfitting. The C parameter is a regularization term that controls the trade-off between achieving a low training error and a low testing error. We optimized these parameters using cross-validation to ensure that the model generalized well to unseen data.\n\nTo address the potential for overfitting, we employed cross-validation techniques, specifically using ten folds with three repeats. This approach helped to ensure that the models were not overly complex and could generalize well to new data. Additionally, we used regularization techniques, such as the decay parameter in the neural network and the C parameter in the SVM, to prevent the models from fitting the noise in the training data.\n\nUnderfitting was addressed by carefully selecting the model complexity. For instance, in the neural network, we experimented with different numbers of hidden layers and neurons to find the optimal architecture that captured the underlying patterns in the data without being too simplistic. Similarly, for the random forest, we set a sufficiently large number of trees (1,000) to ensure that the model captured the complexity of the data.\n\nIn summary, we balanced model complexity and regularization to avoid both overfitting and underfitting. Cross-validation and regularization techniques were crucial in ensuring that our models were robust and generalizable.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting, particularly when using the support vector machine (SVM) and neural network models. For the SVM, we used a radial kernel, which is effective for nonlinear separation problems. To control overfitting, we optimized the Sigma parameter, ensuring it was not set too high, as larger values of Sigma can lead to overfitting. Additionally, we used the regularization parameter C, which specifies the penalty for misclassification. By optimizing C, we aimed to find a balance where the model does not overfit the training data.\n\nFor the neural network, we included a decay parameter, which restricts the weighting from becoming too large. This helps in preventing the model from fitting the noise in the training data. We also explored different architectures, including the number of hidden layers and the number of nodes in each hidden layer, to find the optimal configuration that generalizes well to unseen data.\n\nFurthermore, we used cross-validation techniques, specifically three repeats of ten-fold cross-validation, to assess the performance of our models. This approach helps in ensuring that the models are not overfitting to the training data and provides a more reliable estimate of their performance on new data. By varying the seed values to split the training and testing sets, we further ensured the robustness of our models.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, for the random forest model, we set the number of trees (ntree) to 1,000 and optimized the number of variables (mtry) to 9. For the support vector machine (SVM), we optimized the Sigma and C values using cross-validation, ultimately using values of 1e-3 and 32, respectively. The neural network model was optimized with one hidden layer containing nine hidden neurons and a decay value of 0.24.\n\nThe R code used for these analyses, including the data cleaning process and the specific parameters employed, is provided in the supplementary materials. Supplementary material 1 details the R code for data cleaning, while Supplementary material 2 includes the R code for the analyses. These supplementary materials are intended to be accessible to readers who wish to replicate or build upon our work.\n\nRegarding the availability of model files, these are not explicitly mentioned in the provided context. However, the R packages used for the different models, such as CARET for model comparison, randomForest for random forests, and nnet and neuralnet for neural networks, are well-documented and publicly available. The use of these packages ensures that the methods and configurations can be reproduced by other researchers.\n\nThe license under which these supplementary materials and R code are provided is not specified in the context. However, it is standard practice in academic publishing to make supplementary materials available under a license that permits reuse and modification for non-commercial purposes. Readers are encouraged to refer to the journal's policies or contact the authors for specific details regarding the license.\n\nIn summary, the hyper-parameter configurations, optimization parameters, and R code used in our study are reported and available in the supplementary materials. The use of publicly available R packages ensures that the methods can be reproduced, and the supplementary materials are intended to be accessible for further research.",
  "model/interpretability": "The models employed in our study exhibit varying degrees of interpretability. The decision tree model is the most transparent among them, as it closely mirrors human decision-making processes. It uses a limited number of measurements and binary rules to categorize outcomes, making it relatively easy to understand and interpret. For instance, the decision tree model might use variables like systolic blood pressure and the presence of diabetes to make predictions, providing clear insights into the decision-making process.\n\nIn contrast, models like the support vector machine (SVM) and the neural network are considered more black-box in nature. The SVM, for example, operates by finding a hyperplane that best separates the data, which can be complex and less intuitive to interpret. Similarly, the neural network processes information through multiple layers of neurons, making it difficult to trace the exact path that leads to a specific prediction.\n\nThe random forest model, while more interpretable than SVMs and neural networks, still retains some level of complexity. It combines the results from many decision trees, which can make it challenging to interpret the overall model. However, it is possible to gain insights into the importance of different variables by examining the individual trees within the forest. For example, systolic blood pressure and creatinine were identified as top predictors in the random forest model, indicating their significance in the predictive process.\n\nOverall, while some models offer clear examples of transparency, others remain more opaque, requiring additional techniques to interpret their decision-making processes.",
  "model/output": "The model employed in this study is a classification model. It was used to predict a composite outcome, which is a binary classification problem. The model's performance was evaluated using metrics such as sensitivity, specificity, and accuracy, which are typical for classification tasks. Additionally, the use of receiver operating characteristic (ROC) curves and confusion matrices further supports that the model is designed for classification rather than regression.\n\nSeveral machine learning algorithms were utilized, including generalized linear models, support vector machines, decision trees, neural networks, and random forests. Each of these algorithms was trained and tested using a dataset split into training and testing sets. The predictions were made on the test set, and the results were compared using ROC curves and confusion matrices. The confusion matrices provided details on true positives, true negatives, false positives, and false negatives, which are key components in evaluating the performance of classification models.\n\nThe study also highlighted the importance of various variables in the models, such as systolic blood pressure, cholesterol levels, and creatinine levels. These variables were identified as top predictors in different models, indicating their significance in classifying the outcomes. The random forest approach generally outperformed the other methods, demonstrating its effectiveness in handling the classification task. The support vector machine had difficulties with identifying patients who achieved composite endpoints when trained with an unbalanced dataset, while the decision tree performed poorly, likely due to its reliance on a limited number of measurements for categorization.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models and analyses presented in this publication is not publicly released. However, the R code used for these analyses is provided in supplementary material. This code includes the steps for data preprocessing, model training, and evaluation, allowing readers to replicate the methods described.\n\nThe specific packages and functions used in the analyses are detailed within the text, such as the CARET package for model training and comparison, and other packages like nnet and randomForest for constructing neural networks and random forests, respectively. These packages are widely available and can be installed from standard R repositories.\n\nWhile an executable, web server, virtual machine, or container instance is not provided, the detailed R code and the use of well-documented packages should enable interested researchers to implement and run the algorithms described in the study. The supplementary material serves as a comprehensive guide for reproducing the analyses and understanding the methodological approach taken.",
  "evaluation/method": "The evaluation of the machine learning methods involved a rigorous process to ensure the robustness and generalizability of the results. We employed a strategy of three repeats of ten-fold cross-validation, which is a common technique to assess the performance of predictive models. This approach involved splitting the data into training and testing sets multiple times with different seed values to initiate randomization. Specifically, we used various seed values to divide the dataset into 80% training and 20% testing subsets. This method helped in mitigating the risk of overfitting and provided a more reliable estimate of the model's performance.\n\nFor the support vector machine (SVM), we varied the Sigma and C values from 0.1 to 1 to optimize the parameters. Similarly, for other methods, we used different seed values to split the training and testing sets, ensuring that the models were evaluated under diverse conditions. The areas under the receiver operator curve (ROC) were improved by approximately 5% to 7% by the inclusion of baseline laboratory values, indicating the importance of these features in the predictive models.\n\nStatistical comparisons of ROC values were performed using ten different seed values for splitting the training and testing sets. The overall datasets were compared using one-way ANOVA, and individual group means were compared using unpaired t-tests with Holm\u2013Sidak correction for multiple comparisons. This statistical rigor ensured that the differences observed in the model performances were statistically significant.\n\nThe results of the training and testing subsets were presented in tables and figures, showing that all machine learning methods, except the simple decision tree, yielded very similar ROC areas and accuracy values. Representative confusion matrices were also provided to illustrate the performance of different models. The SVM method was noted to be heavily biased towards negative values, but balancing the training set with outcomes avoided this issue without improving overall accuracy. The neural network model provided the best balance between sensitivity and specificity, while the random forest method yielded the highest accuracy in most simulations.",
  "evaluation/measure": "In our evaluation, we primarily focused on the area under the receiver operating characteristic curve (ROC AUC) as our key performance metric. This metric was chosen because it provides a comprehensive measure of a model's ability to discriminate between positive and negative classes across all classification thresholds. We reported the ROC AUC values for both training and testing sets, with results expressed as the mean and standard deviation of multiple trials using different seed values to split the data.\n\nIn addition to ROC AUC, we also examined confusion matrices to gain insights into the models' performance in terms of sensitivity, specificity, and accuracy. These matrices allowed us to assess the trade-offs between true positive, true negative, false positive, and false negative rates for each model. We found that while some models, like the support vector machine (SVM), had high specificity, they struggled with sensitivity, leading to an imbalance in their performance. Other models, such as the random forest and neural network, provided a more balanced trade-off between sensitivity and specificity.\n\nTo statistically compare the performance of different models, we employed one-way ANOVA and unpaired t-tests with Holm\u2013Sidak correction for multiple comparisons. This approach enabled us to determine if the differences in ROC AUC values between models were statistically significant. Our results indicated that some models, like the random forest, generally outperformed others, such as the decision tree, across multiple trials.\n\nWhile our set of performance metrics is representative of common practices in the literature, we acknowledge that other metrics, such as precision, recall, and F1-score, could also provide valuable insights into model performance, especially in imbalanced datasets. However, given our focus on discriminative power and the trade-offs between sensitivity and specificity, ROC AUC served as our primary performance metric.",
  "evaluation/comparison": "In our study, we employed a variety of machine learning methods to predict outcomes in atherosclerotic renal artery disease using the CORAL dataset. The methods included generalized linear models, support vector machines, decision trees, neural networks, and random forests. Each method was optimized using cross-validation techniques, such as varying the Sigma and C values for the support vector machine and tuning parameters like the number of trees and variables sampled for the random forest.\n\nTo ensure robustness, we used different seed values to split the training and testing sets, employing a strategy of three repeats of ten-fold cross-validation. This approach helped in assessing the stability and generalizability of our models. The performance of these models was evaluated using the area under the receiver operator curve (ROC), which provided insights into their predictive accuracy.\n\nWe also compared the performance of these machine learning methods with simpler baselines, such as decision trees. The decision tree method, which mirrors human decision-making with a limited number of measurements, performed quite poorly. This highlights the superiority of more complex models in handling the intricacies of the dataset.\n\nIn addition to comparing different machine learning methods, we examined the factors emphasized by these models. Interestingly, treatment assignment was not considered a strong predictor by any model, supporting the overall conclusion of the CORAL study that stenting did not significantly add to medical therapy in avoiding composite cardiovascular outcomes. Instead, baseline systolic blood pressure and estimates of renal function were consistently featured as top predictors of adverse outcomes.\n\nThe random forest approach generally outperformed the other methods, particularly when dealing with unbalanced training sets. The support vector machine had issues identifying patients who achieved composite endpoints when trained with unbalanced data. These comparisons provided a comprehensive evaluation of the strengths and weaknesses of each method, guiding us towards more effective predictive modeling strategies.",
  "evaluation/confidence": "The evaluation of our machine learning models involved a rigorous statistical approach to ensure the reliability and significance of our results. We employed a strategy of three repeats of ten-fold cross-validation, using different seed values to split the training and testing sets. This method helped to mitigate the risk of overfitting and provided a more robust estimate of model performance.\n\nTo assess the statistical significance of our results, we performed one-way ANOVA on the overall datasets and used unpaired t-tests with Holm\u2013Sidak correction for multiple comparisons. These statistical tests allowed us to compare the performance metrics of different models and determine if the observed differences were significant.\n\nThe performance metrics, such as the areas under the receiver operator curve (ROC), were improved by approximately 5%\u20137% through the inclusion of baseline laboratory values. This improvement suggests that the models are not only reliable but also clinically relevant.\n\nThe confusion matrices and ROC curves generated from our models showed consistent performance across different seed values, varying only by a few percentage points. This consistency indicates that our models are stable and not overly sensitive to the specific split of the data.\n\nIn summary, the performance metrics of our models are supported by rigorous statistical analysis, and the results are statistically significant. This provides a high level of confidence in the superiority of our methods over baselines and other comparative models.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The data used in the study was cleaned and processed as described in the supplementary materials. The specific datasets and scripts used for the analyses are not released publicly. However, the R code used for data cleaning and analysis is provided in the supplementary materials, which can be used as a guide for similar analyses. The study adheres to ethical guidelines and regulatory requirements, ensuring the confidentiality and security of the data. For further details or access to the data, interested parties may contact the corresponding author or the relevant institutional review board."
}