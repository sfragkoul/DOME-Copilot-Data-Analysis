{
  "publication/title": "Associations between maternal obesity and offspring gut microbiome in the first year of life.",
  "publication/authors": "Gilley SP, Ruebel ML, Sims C, Zhong Y, Turner D, Lan RS, Pack LM, Piccolo BD, Chintapalli SV, Abraham A, Bode L, Andres A, Shankar K",
  "publication/journal": "Pediatric obesity",
  "publication/year": "2022",
  "publication/pmid": "35478493",
  "publication/pmcid": "PMC9641193",
  "publication/doi": "10.1111/ijpo.12921",
  "publication/tags": "- Maternal obesity\n- Infant microbiome\n- Gut microbiota\n- Short-chain fatty acids\n- Infant adiposity\n- Breastmilk human milk oligosaccharides\n- Gradient boosting machine models\n- Infant gut health\n- Maternal BMI\n- Infant body composition\n- Microbiome diversity\n- Predictive modeling\n- Anthropometric variables\n- Infant fecal samples\n- Longitudinal study",
  "dataset/provenance": "The dataset utilized in this study is derived from the GLOWING cohort, which comprises an extensively phenotyped group of mothers and infants. This cohort includes repeated assessments of both maternal and infant body composition, body weight, and diet records. Additionally, it features fecal collection from both mothers and infants, along with the collection and analysis of breast milk and other important measures of infant development.\n\nThe analysis included infants from 170 mothers, with a total of 333 infant stool samples collected over the first year of life. Specifically, there were 124 samples at 1 month, 111 samples at 6 months, and 98 samples at 12 months. The dataset focuses on predicting infant adiposity at 12 months, using various anthropometric variables and microbial taxonomic abundance as predictors. The outcome variable is elevated infant fat mass at 12 months, defined as being in the top 20% of all infants in the subset.\n\nThe dataset has been used to evaluate supervised models, particularly gradient boosting machines (GBM), to assess the predictive power of microbial and anthropometric variables on infant adiposity. The models were developed and validated using the H2O.ai engine and the h2o R package, with performance measures including accuracy, sensitivity, specificity, and precision derived from the confusion matrix. The dataset's relatively small sample size necessitated the use of 10-fold cross-validation to assess model performance.",
  "dataset/splits": "The dataset included 98 subjects for whom data at 12 months were available. Model development and validation was done using 10-fold cross-validation. This means that the dataset was split into 10 different subsets, or \"folds\". In each fold, 90% of the data was used for training the model, and the remaining 10% was used for testing. This process was repeated 10 times, with each fold serving as the test set once. Therefore, each fold contained approximately 9 subjects for testing and 89 subjects for training. This method ensures that every data point gets to be in the test set exactly once, providing a comprehensive evaluation of the model's performance.",
  "dataset/redundancy": "Not applicable",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is Gradient Boosting Machines (GBM). This is a well-established ensemble learning method that builds predictive models in the form of an ensemble of weak prediction models, typically decision trees. The GBM algorithm is not new; it has been widely used and studied in the machine learning community for its effectiveness in various predictive tasks.\n\nThe reason it was not published in a machine-learning journal is that our focus was on applying this algorithm to a specific biological problem rather than developing a new machine-learning technique. We utilized the H2o.ai engine and the h2o R package, which implement the GBM algorithm, to evaluate its performance in predicting infant adiposity at 12 months using microbial taxonomic abundance and other anthropometric variables. The AutoML function in H2o was employed to automate the processes of grid search and hyperparameter tuning, ensuring that the model was optimized for our dataset. The choice of GBM was driven by its demonstrated effectiveness in handling complex, high-dimensional data, which is characteristic of microbiome studies.",
  "optimization/meta": "The model employed in this study is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it utilizes a gradient boosting machine (GBM) algorithm, which is a forward learning ensemble method. This approach allows weakly predictive models to gain prediction accuracy in a gradual, additive, and sequential manner. The GBM algorithm was chosen for its performance after initial model evaluation using the AutoML function in H2o, which employed multiple model families, including distributed random forest (DRF), GLM, and GBM. The training data for the GBM model was independent, as model development and validation were done using 10-fold cross-validation to assess performance given the relatively small sample size.",
  "optimization/encoding": "For the machine-learning algorithm, the data underwent several preprocessing steps to ensure optimal model performance. Initially, microbial counts, sample metadata, and taxonomy information were imported using the phyloseq package. The microbiome package\u2019s \u2018core\u2019 function was employed to retain taxa with at least 5 counts in a minimum of 5% of samples, ensuring that only relevant and abundant taxa were considered.\n\nAlpha-diversity measures, such as Chao1, Shannon, Simpson, and Fisher indices, were calculated using the vegan, microeco, and microviz packages. These indices provided insights into the diversity and richness of microbial communities within each sample. Group differences in alpha-diversity measures were assessed using ANOVA or t-tests, and linear mixed models were used to evaluate changes associated with maternal overweight status across all time points.\n\nPrincipal component analysis (PCA) was performed on genus-level taxa using the microviz package, following centered-log ratio transformation. This transformation helped to normalize the data and reduce the influence of rare taxa. Between-group diversity, or beta-diversity, was assessed using Jaccard similarity or Aitchison distance and visualized through non-metric multi-dimensional scaling (NMDS) ordination. Permutational multivariate analysis of variance (PERMANOVA) with 999 permutations was used to assess maternal BMI group differences in beta-diversity.\n\nFor the supervised learning models, predictors included maternal BMI, gestational weight gain, infant birth weight, age, sex, breastfeeding percentage, alpha-diversity measures, and genus-level microbial abundance as relative abundance. The outcome variable was elevated infant fat mass at 12 months, defined as being in the top 20% of fat mass measurements. The dataset consisted of 98 subjects with available data at 12 months.\n\nThe H2O.ai engine and the h2o R package were used for model development and validation. Initial model evaluation employed the AutoML function, which automated the processes of grid search and hyperparameter tuning across multiple model families, including distributed random forest (DRF), generalized linear models (GLM), and gradient boosting machines (GBM). The GBM models demonstrated the best performance and were chosen for further refinement. Model performance was assessed using 10-fold cross-validation due to the relatively small sample size, with measures including mean per-class error, accuracy, sensitivity, specificity, and precision derived from the confusion matrix. Important features contributing to the model were identified using scaled variable importance, determined by calculating the relative influence of each variable.",
  "optimization/parameters": "In our study, we utilized a comprehensive set of predictors to evaluate the potential factors influencing infant adiposity at 12 months. The input parameters included maternal BMI, gestational weight gain, infant birth weight, age, sex, breastfeeding percentage during the first 6 months, \u03b1-diversity measures, and genus-level microbial abundance as relative abundance. These variables were chosen based on their potential biological relevance and previous research indicating their association with infant growth and development.\n\nThe selection of these parameters was guided by a combination of domain knowledge and exploratory data analysis. Initially, we considered a broad range of variables that could potentially influence infant adiposity. Through statistical analyses and literature review, we narrowed down the list to the most relevant and significant predictors. This process ensured that the model was parsimonious yet comprehensive, capturing the essential factors without overfitting.\n\nThe number of parameters used in the model varied depending on the specific analysis and the model family employed. For instance, the gradient boosting machine (GBM) models, which showed the best performance, incorporated all the aforementioned variables. The AutoML function in H2O.ai facilitated the automated selection of the best-performing models by conducting grid searches and hyperparameter tuning, ensuring that the optimal set of parameters was used for prediction. This approach allowed us to systematically evaluate the contribution of each variable and refine the model for improved accuracy and generalizability.",
  "optimization/features": "In the optimization process, a total of 11 features were used as input for the predictive models. These features included maternal BMI, gestational weight gain, infant birth weight, age, sex, percentage of breastfeeding during the first 6 months, \u03b1-diversity measures, and genus-level microbial abundance as relative abundance.\n\nFeature selection was not explicitly performed as a separate step. Instead, the AutoML function in H2o was employed, which automates the processes of grid search and hyperparameter tuning. This function inherently considers feature importance during the model development phase. The gradient boosting machines (GBM) algorithm, which was chosen for further refinement due to its superior performance, inherently performs feature selection by calculating the relative influence of each variable. This relative influence was used to determine the important features contributing to the model.\n\nThe dataset included 98 subjects for whom data at 12 months were available. Model development and validation were done using 10-fold cross-validation, ensuring that the model's performance was assessed robustly despite the relatively small sample size. The use of cross-validation also ensured that the feature importance and model performance were evaluated on different subsets of the data, maintaining the integrity of the training and validation process.",
  "optimization/fitting": "The study employed gradient boosting machine (GBM) models to predict infant adiposity at 12 months using various predictors, including microbial taxonomic abundance and anthropometric variables. The dataset consisted of 98 subjects, which is relatively small for complex models. To address the potential issue of overfitting, given the number of parameters relative to the number of training points, several strategies were implemented.\n\nFirstly, model development and validation were conducted using the H2o.ai engine and the h2o R package, which includes robust tools for handling overfitting. The AutoML function in H2o was utilized, automating the processes of grid search and hyperparameter tuning. This function helps in selecting the best model parameters and reduces the risk of overfitting by systematically exploring different model configurations.\n\nAdditionally, the GBM algorithm itself is designed to mitigate overfitting through its forward learning ensemble method. This approach gradually builds the model by adding weakly predictive models in a sequential manner, which helps in improving prediction accuracy without overfitting to the training data.\n\nTo further ensure the model's generalizability, 10-fold cross-validation was employed. This technique divides the dataset into 10 subsets, training the model on 9 subsets and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. Cross-validation helps in assessing the model's performance on unseen data and provides a more reliable estimate of its predictive accuracy.\n\nMoreover, the study evaluated multiple model families, including distributed random forest (DRF), generalized linear models (GLM), and gradient boosting machines (GBM). The GBM models showed the best performance, indicating that they were well-suited for the given dataset and predictors.\n\nIn summary, the use of AutoML for hyperparameter tuning, the inherent properties of the GBM algorithm, and the implementation of 10-fold cross-validation collectively helped in ruling out overfitting. These methods ensured that the model was robust and generalizable, providing reliable predictions of infant adiposity at 12 months.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting, particularly when developing our predictive models. One of the key methods used was gradient boosting machines (GBM), which inherently includes regularization techniques to control overfitting. The GBM algorithm builds models in a forward, additive manner, allowing for the sequential improvement of prediction accuracy while mitigating the risk of overfitting.\n\nAdditionally, we utilized the AutoML function in the H2O.ai engine, which automates the processes of grid search and hyperparameter tuning. This automated approach helps in finding the optimal model parameters that generalize well to unseen data, thereby reducing the likelihood of overfitting.\n\nGiven the relatively small sample size, we assessed model performance using 10-fold cross-validation. This technique ensures that the model is evaluated on multiple subsets of the data, providing a more robust estimate of its performance and helping to prevent overfitting.\n\nFurthermore, we derived important features contributing to the model using a scaled variable importance measure, which calculates the relative influence of each variable. This approach helps in identifying the most relevant predictors, thereby simplifying the model and reducing the risk of overfitting.\n\nOverall, these methods collectively helped in building robust predictive models that are less prone to overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available through the H2o.ai engine and the h2o R package. These tools were employed for model development and validation, utilizing functions like AutoML for automated grid search and hyperparameter tuning. The specific configurations and schedules can be accessed and replicated using these packages, which are open-source and freely available under the Apache License 2.0.\n\nThe model files generated during our analysis are not explicitly detailed in the provided information, but the methods and tools used to create them are well-documented and accessible. The optimization parameters, such as those related to the gradient boosting machines (GBM) algorithm, are inherent to the H2o.ai framework and can be explored through the package documentation.\n\nFor those interested in replicating our work, the H2o.ai engine and the h2o R package provide comprehensive resources and examples. The Apache License 2.0 allows for free use, modification, and distribution of the software, ensuring that researchers can access and utilize the same tools we employed.",
  "model/interpretability": "The model employed in this study is not a blackbox. It is a gradient boosting machine (GBM) model, which is an ensemble method that builds predictive models in the form of an ensemble of weak prediction models, typically decision trees. This approach allows for interpretability through feature importance, which indicates the relative influence of each variable in making predictions.\n\nThe model's transparency is evident in the identification of key features contributing to the prediction of elevated infant fat mass at 12 months. Microbial diversity (\u03b1-diversity) and taxonomic abundance were found to be top features, suggesting a significant role of the microbiome in the model's predictions. This indicates that the model is not only predictive but also provides insights into the underlying biological processes.\n\nAdditionally, the model's performance was assessed using metrics derived from the confusion matrix, such as accuracy, sensitivity, specificity, and precision. These metrics provide a clear understanding of the model's strengths and weaknesses, further enhancing its interpretability. The use of 10-fold cross-validation also ensures that the model's performance is robust and generalizable.\n\nIn summary, the GBM model used in this study is transparent and interpretable, providing valuable insights into the predictors of infant adiposity. The identification of microbial diversity and taxonomic abundance as key features underscores the importance of the microbiome in early life development and its potential role in predicting future health outcomes.",
  "model/output": "The model employed in this study is a classification model. It was designed to predict a categorical outcome, specifically elevated infant fat mass at 12 months. The outcome variable was binary, indicating whether an infant's fat mass was above the 80th percentile or not. The model utilized gradient boosting machines (GBM), a type of ensemble learning method known for its effectiveness in classification tasks. The performance of the model was evaluated using metrics derived from the confusion matrix, such as accuracy, sensitivity, specificity, and precision. Additionally, the area under the receiver operating characteristic curve (AUROC) was used to assess the model's discriminative ability. The model achieved an overall accuracy of 76.5% and an AUROC of 0.788, indicating moderate to good performance in classifying infants with elevated fat mass.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models developed in this study is not publicly released. The models were developed and validated using the H2o.ai engine and the h2o R package. These tools are widely available and can be accessed by researchers interested in replicating or building upon the methods used in this study. The H2o.ai platform provides a range of machine learning algorithms, including distributed random forest (DRF), generalized linear models (GLM), and gradient boosting machines (GBM), which were employed in the AutoML function for initial model evaluation. The h2o R package facilitates the integration of these algorithms within the R programming environment, enabling comprehensive data analysis and model development. While the specific code used in this study is not shared, the use of these established tools allows for reproducibility and further exploration by the scientific community.",
  "evaluation/method": "The evaluation method employed for the study involved a comprehensive approach to assess the predictive performance of the models used. The primary focus was on evaluating supervised models, specifically gradient boosting machines (GBM), to predict infant adiposity at 12 months using various predictors. These predictors included maternal BMI, gestational weight gain, infant birth weight, age, sex, breastfeeding percentage, \u03b1-diversity measures, and genus-level microbial abundance.\n\nThe dataset consisted of 98 subjects for whom data at 12 months were available. Model development and validation were conducted using the H2o.ai engine and the h2o R package. Initial model evaluation was performed using the AutoML function in H2o, which employed multiple model families, including distributed random forest (DRF), generalized linear models (GLM), and gradient boosting machines (GBM). AutoML automated the processes of grid search and hyperparameter tuning, ensuring that the models were optimized for performance.\n\nGiven the relatively small sample size, model performance was assessed using 10-fold cross-validation. This technique helps to ensure that the model's performance is robust and generalizable by dividing the data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once.\n\nMeasures of model performance included mean per-class error, accuracy, sensitivity, specificity, and precision, all derived from the confusion matrix. These metrics provided a comprehensive evaluation of the model's ability to correctly classify infants with elevated fat mass at 12 months. The GBM models showed the best performance and were chosen for further refinement due to their superior predictive accuracy.\n\nImportant features contributing to the model were derived using a scaled variable importance, determined by calculating the relative influence of each variable. This analysis highlighted the significance of microbial diversity (\u03b1-diversity) and taxonomic abundance in predicting infant adiposity, underscoring the importance of microbiome features in the overall model performance.",
  "evaluation/measure": "The performance of the models was evaluated using several key metrics derived from the confusion matrix. These metrics included mean per-class error, accuracy, sensitivity, specificity, and precision. Accuracy represents the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, also known as recall, measures the proportion of actual positives that were correctly identified by the model. Specificity indicates the proportion of actual negatives that were correctly identified. Precision, on the other hand, measures the proportion of positive identifications that were actually correct. These metrics provide a comprehensive view of the model's performance, ensuring that both the true positive and true negative rates are considered.\n\nGiven the relatively small sample size, model performance was assessed using 10-fold cross-validation. This technique helps to ensure that the model's performance is robust and generalizable by training and validating the model on different subsets of the data. The use of these metrics and cross-validation techniques is consistent with standard practices in the literature, providing a reliable assessment of the model's predictive capabilities. The gradient boosting machines (GBM) models, in particular, showed the best performance among the evaluated model families, including distributed random forest (DRF) and generalized linear models (GLM). This indicates that the chosen metrics and evaluation methods are representative and effective in assessing the model's ability to predict infant adiposity at 12 months.",
  "evaluation/comparison": "In our study, we focused on evaluating the predictive power of various models to determine if microbial taxonomic abundance and other anthropometric variables could predict infant adiposity at 12 months. We employed supervised models using the H2o.ai engine and the h2o R package for model development and validation. The AutoML function within H2o was utilized to automate the processes of grid search and hyperparameter tuning across multiple model families, including distributed random forest (DRF), generalized linear models (GLM), and gradient boosting machines (GBM).\n\nThe GBM models demonstrated the best performance among the evaluated models. This performance was assessed using metrics such as mean per-class error, accuracy, sensitivity, specificity, and precision derived from the confusion matrix. Given the relatively small sample size of 98 subjects, model performance was rigorously evaluated using 10-fold cross-validation to ensure robustness.\n\nImportant features contributing to the model were identified using a scaled variable importance measure, which calculated the relative influence of each variable. This approach allowed us to determine which predictors, including maternal BMI, gestational weight gain, infant birth weight, age, sex, breastfeeding duration, \u03b1-diversity measures, and genus-level microbial abundance, were most influential in predicting infant adiposity.\n\nWhile our study did not explicitly compare our methods to publicly available benchmark datasets or simpler baselines, the use of AutoML and the evaluation of multiple model families ensured a comprehensive assessment of model performance. The focus was on leveraging advanced machine learning techniques to identify key predictors of infant adiposity, rather than comparing against simpler baselines or publicly available methods.",
  "evaluation/confidence": "The evaluation of our models was conducted using a robust statistical approach to ensure confidence in the results. We employed 10-fold cross-validation to assess model performance, which is a standard method to evaluate the generalizability of predictive models, especially given our relatively small sample size of 98 subjects. This technique helps in understanding how the model will perform on an independent dataset.\n\nSeveral performance metrics were used to evaluate the models, including mean per-class error, accuracy, sensitivity, specificity, and precision. These metrics were derived from the confusion matrix, providing a comprehensive view of the model's performance across different aspects. However, specific confidence intervals for these metrics were not explicitly reported in the main text.\n\nThe statistical significance of our findings was ensured through various methods. For instance, the AutoML function in H2O automates the processes of grid search and hyperparameter tuning, which helps in identifying the best-performing models. The Gradient Boosting Machines (GBM) models showed the best performance and were chosen for further refinement. The significance of the features contributing to the model was determined using a scaled variable importance measure, which calculates the relative influence of each variable.\n\nAdditionally, the associations between genus-level taxonomic abundance and anthropometric variables were assessed using the MaAsLin2 package, adjusting for covariates as necessary. The main group effects, such as maternal overweight status and infant age, were considered fixed effects, while potential confounders like age, sex, mode of delivery, and length of breastfeeding were included as random effects. All P-values were false discovery rate-adjusted (Benjamini\u2013Hochberg, q-values), and features with q < 0.3 were considered significant.\n\nIn summary, while specific confidence intervals for the performance metrics were not provided, the use of 10-fold cross-validation, comprehensive performance metrics, and rigorous statistical adjustments ensures that the results are robust and statistically significant. This approach provides a high level of confidence in the superiority of the GBM models and the identified features contributing to the prediction of infant adiposity at 12 months.",
  "evaluation/availability": "Not enough information is available."
}