{
  "publication/title": "High-Throughput Prediction of MHC Class I and II Neoantigens with MHCnuggets.",
  "publication/authors": "Shao XM, Bhattacharya R, Huang J, Sivakumar IKA, Tokheim C, Zheng L, Hirsch D, Kaminow B, Omdahl A, Bonsack M, Riemer AB, Velculescu VE, Anagnostou V, Pagel KA, Karchin R",
  "publication/journal": "Cancer immunology research",
  "publication/year": "2020",
  "publication/pmid": "31871119",
  "publication/pmcid": "PMC7056596",
  "publication/doi": "10.1158/2326-6066.cir-19-0464",
  "publication/tags": "- Cancer Immunology\n- Neoantigen Prediction\n- MHC Class I\n- MHC Class II\n- Deep Learning\n- LSTM Neural Networks\n- Transfer Learning\n- Immunotherapy\n- Tumor Immunoediting\n- Computational Biology\n- High-Throughput Data\n- Immunogenic Mutations\n- Binding Affinity\n- HLAp Data\n- Patient Response Prediction",
  "dataset/provenance": "The datasets used in our study were sourced from various reputable databases and experiments. For network training and testing, we utilized data from The Cancer Genome Atlas (TCGA) for somatic mutations and tumor gene expression. Additionally, we incorporated haplotype calling data. A curated version of the Immune Epitope Database (IEDB) from 2018, which includes chemical binding affinity measurements for a substantial number of peptide-allele pairs covering numerous class I and class II alleles, was also used. This dataset was supplemented with immunopeptidome data from 16 class I monoallelic B-cell lines and decoy random peptides sampled from the human proteome.\n\nFor benchmarking, we employed several datasets. The Kim et al. benchmark contained IC50 measurements for a large number of peptides across multiple MHC Class I alleles, published before and after 2009. The Bonsack et al. dataset included synthetic peptides derived from HPV16 E6 and E7 proteins, tested for binding to several HLA alleles using competition-based cellular binding assays. The Bassani-Sternberg et al. dataset consisted of unique peptides eluted from cell lines with multi-allelic MHCs, with peptide/MHC pairs identified through deconvolution. The Trolle et al. dataset featured peptides eluted from soluble HLA-transfected HeLa cells, allowing for the separation of binding peptides to single MHC alleles.\n\nThe BST benchmark combined data from Bassani-Sternberg et al. and Trolle et al., along with a large number of random decoy peptides. The Jensen et al. benchmark was designed to assess both allele-specific and rare MHC class II binding affinity predictors. Additionally, we included a dataset of class I rare alleles to apply the LOMO protocol. All datasets used in this work are publicly available for further research and validation.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "All datasets used in this work are publicly available. They can be accessed at http://dx.doi.org/10.17632/8c26kkrfpr.2. This ensures that other researchers can reproduce and build upon the findings presented in the study. The data includes various benchmarks and training sets used for evaluating and training the MHCnuggets model. The availability of these datasets promotes transparency and reproducibility in scientific research. The data is released under a license that allows for open access and use by the research community, facilitating further advancements in the field of immunogenetics and neoantigen prediction.",
  "optimization/algorithm": "The machine-learning algorithm class used in our work is long short-term memory (LSTM) neural networks. LSTMs are a type of recurrent neural network (RNN) that are particularly well-suited for handling sequential data, such as peptide sequences. They excel at capturing long-term dependencies and can manage variable-length inputs, making them ideal for our application in predicting MHC binding peptides.\n\nThe LSTM architecture employed in our study is not entirely new, as LSTMs have been widely used in various domains, including natural language processing and time-series analysis. However, our implementation is tailored specifically for the task of MHC binding prediction, incorporating unique features such as transfer learning and allele clustering to accommodate both common and rare MHC alleles.\n\nThe reason this work was published in a cancer research journal rather than a machine-learning journal is that the primary focus of our study is on the biological application and clinical utility of the method. While the machine-learning aspects are crucial to the development of our predictor, the main contributions lie in the biological insights gained and the potential impact on cancer immunotherapy. The integration of binding affinity and HLAp data, along with the demonstration of the method's performance on large patient cohorts, highlights the practical applications of our work in the field of cancer research.",
  "optimization/meta": "The model presented in this publication is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it is a standalone long short-term memory (LSTM) neural network method called MHCnuggets. This method is designed for predicting neoantigens for MHC class I and II alleles within a single framework. It leverages transfer learning and allele clustering to handle both common and rare MHC alleles effectively.\n\nThe training data for MHCnuggets includes binding affinity measurements from in vitro experiments and immunopeptidomic (HLAp) binary labels. The model is trained using mean-squared error (MSE) loss for continuous binding affinity data and binary cross-entropy (BCE) loss for binary HLAp data. The training process involves backpropagation with the Adam optimizer and regularization techniques such as dropout and recurrent dropout.\n\nThe architecture of MHCnuggets consists of an LSTM layer with 64 hidden units, a fully connected layer with 64 hidden units, and a final output layer with a single sigmoid unit. This architecture is capable of handling peptides of variable lengths, with a maximum length set for computational efficiency. Peptides are represented as one-hot encoded vectors and padded to the maximum length if necessary.\n\nTransfer learning is employed to improve the model's performance for MHC alleles with limited characterized peptides. The base allele-specific networks are trained using alleles with the most training examples, and their final weights are used to initialize the training of networks for other alleles. This approach allows the model to leverage information from well-characterized alleles to enhance predictions for less-studied alleles.\n\nIn summary, MHCnuggets is a standalone LSTM neural network method that does not rely on other machine-learning algorithms for input. It is trained on independent binding affinity and HLAp data, ensuring that the training data is independent and specific to the model's objectives.",
  "optimization/encoding": "For the MHCnuggets method, peptides were represented as sequences of amino acids, with each amino acid encoded as a 21-dimensional smoothed, one-hot encoded vector. This means that instead of using binary values (0 or 1), the encoding used 0.9 and 0.005 to represent the presence and absence of an amino acid, respectively. This smoothing technique helps to reduce the sparsity of the data and can improve the performance of the neural network.\n\nPeptides of varying lengths were handled by padding shorter peptides with a special character (\"Z\") that is not part of the standard amino acid alphabet. This padding was done until the peptides reached a maximum length of 15 amino acids for class I and 30 amino acids for class II. This approach allowed the neural network to process peptides of any length while maintaining a consistent input size.\n\nThe binding affinity data was transformed using a logarithmic scale to match the weakest binding affinity of interest. For most benchmarks, an upper limit of 50,000 nM was used, and the predicted binding affinity was calculated as the maximum of 0 and 1 minus the logarithm base 50k of the IC50 value. This transformation helped to normalize the binding affinity values and make them more suitable for neural network training.\n\nThe data was split into training, validation, and test sets using a three-fold cross-validation approach. This method involved dividing the data into three parts and training the model on two parts while validating it on the remaining part. This process was repeated three times, with each part serving as the validation set once. This approach helped to ensure that the model was robust and generalizable to new data.\n\nRegularization techniques such as dropout and recurrent dropout were applied during training to prevent overfitting. The dropout rate was set to 0.2, meaning that 20% of the neurons were randomly dropped during each training epoch. This helped to improve the model's ability to generalize to new data.\n\nThe networks were implemented using the Keras Python package with a TensorFlow backend. This framework provided a flexible and efficient way to build and train the neural networks used in the MHCnuggets method. The open-source software is available on GitHub and can be installed via pip or Docker. It has also been integrated into several pipelines, including PepVacSeq, pvactools, and Neoepiscope.",
  "optimization/parameters": "The model utilizes a neural network architecture with specific layers and units. Each network comprises an LSTM layer with 64 hidden units, a Fully Connected (FC) layer also with 64 hidden units, and a final output layer featuring a single sigmoid unit. The number of hidden units in both the LSTM and FC layers was determined through three-fold cross-validation on the MHC Class I A*02:01 allele, which is a common allele with a substantial number of experimentally characterized binding peptides. This process ensured that the selected number of hidden units optimized the model's performance.\n\nThe dropout and recurrent dropout probabilities were set to 0.2 to regularize the model and prevent overfitting. The learning rate for the Adam optimizer was fixed at 0.001. The number of training epochs was also estimated through the same cross-validation process, ensuring that the model was trained adequately without overfitting.\n\nPeptides were represented using a 21-dimensional smoothed, one-hot encoded vector, where 0.9 and 0.005 replaced the typical 1 and 0, respectively. This encoding method helps in smoothing the input data, which can improve the model's generalization capabilities. Peptides were padded to a maximum length of 15 for class I and 30 for class II using the character \"Z,\" which is not part of the standard amino acid alphabet. This padding ensures that all input peptides have a consistent length, facilitating uniform processing by the neural network.",
  "optimization/features": "The input features for MHCnuggets are peptides represented as sequences of amino acids. Each amino acid is encoded as a 21-dimensional smoothed, one-hot encoded vector. This means that for each amino acid in a peptide, a vector of 21 values is used, where one value is set to 0.9 and the rest are set to 0.005. This encoding allows the network to handle peptides of variable lengths, with a maximum length of 15 for class I and 30 for class II peptides, which are padded with a special character (\"Z\") if necessary.\n\nFeature selection in the traditional sense was not performed, as the features are directly derived from the amino acid sequence of the peptides. The encoding scheme ensures that all relevant information about the peptide sequence is preserved and fed into the network. The use of one-hot encoding with smoothing is a standard practice in sequence-based neural network models, ensuring that the input features are consistent and informative for the learning process.\n\nThe encoding process is applied uniformly to all peptides, regardless of whether they are in the training, validation, or test sets. This ensures that the input features are consistent across all stages of the model development and evaluation process. The maximum peptide lengths were selected based on the observed lengths in naturally presented MHC-bound peptides, ensuring that the model can handle the majority of peptide lengths encountered in practice.",
  "optimization/fitting": "In our study, we employed a long short-term memory (LSTM) neural network architecture for MHCnuggets, which inherently has a large number of parameters due to its recurrent nature and the complexity of the sequences it processes. The number of parameters in our networks is indeed much larger than the number of training points, especially for less-studied alleles.\n\nTo mitigate overfitting, several strategies were implemented. Firstly, we used dropout and recurrent dropout with probabilities of 0.2 during training. This technique randomly sets a fraction of input units to zero at each update during training time, which helps prevent overfitting. Secondly, we employed early stopping based on the positive predictive value with respect to the top-ranked n peptides (PPVn). Training was halted after 100 epochs, but if the best PPVn was achieved earlier, the network weights from that epoch were used. This ensures that the model does not continue to train beyond the point of optimal performance on the validation set. Additionally, we used transfer learning, which allows networks for less well-characterized alleles to leverage information from extensively studied alleles, thereby reducing the risk of overfitting.\n\nTo address underfitting, we utilized transfer learning and allele clustering. Transfer learning helps by initializing the networks with weights from a well-trained base network, providing a good starting point for training on less-characterized alleles. This approach ensures that the model can generalize well even with limited data. Furthermore, the use of a maximum peptide length of 15 for class I and 30 for class II, along with padding for shorter peptides, ensures that the model can handle variable-length sequences effectively, reducing the likelihood of underfitting. The choice of these lengths was based on computational efficiency and coverage of naturally presented MHC-bound peptides.\n\nIn summary, our methods to prevent overfitting included dropout, early stopping based on PPVn, and transfer learning. To avoid underfitting, we employed transfer learning, allele clustering, and appropriate peptide length handling. These strategies collectively ensure that our models are robust and generalizable.",
  "optimization/regularization": "Regularization techniques were employed to prevent overfitting during the training process. Dropout and recurrent dropout were used with probabilities of 0.2. These techniques help to reduce overfitting by randomly setting a fraction of input units to zero at each update during training time, which prevents units from co-adapting too much. Additionally, the number of hidden units, dropout rate, and number of training epochs were estimated through three-fold cross-validation on a common allele with a large number of experimentally characterized binding peptides. This approach ensures that the model generalizes well to unseen data.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are available through open-source software. This software can be accessed via GitHub at the repository https://github.com/KarchinLab/mhcnuggets. It is installable via pip or Docker, making it accessible for a wide range of users. The software has been integrated into several pipelines, including PepVacSeq, pvactools, and Neoepiscope, which further demonstrates its utility and availability. The specific details of the hyper-parameters, such as the number of hidden units, dropout rates, and training epochs, were estimated using three-fold cross-validation on MHC Class I A*02:01. The optimization parameters, including the use of the Adam optimizer with a learning rate of 0.001 and regularization with dropout and recurrent dropout probabilities of 0.2, are also documented. The software is designed to handle both common and rare MHC alleles, leveraging transfer learning and allele clustering to accommodate various alleles effectively.",
  "model/interpretability": "The MHCnuggets model is primarily a black-box model, as it leverages a long short-term memory (LSTM) neural network architecture. LSTMs are a type of recurrent neural network (RNN) that are particularly well-suited for handling sequential data, such as the amino acid sequences of peptides. However, the internal workings of LSTMs are complex and not easily interpretable, making it difficult to understand the specific features or patterns that the model uses to make its predictions.\n\nThe model's architecture includes an LSTM layer with 64 hidden units, followed by a fully connected layer with 64 hidden units, and a final output layer with a single sigmoid unit. This architecture allows the model to capture long-term dependencies between non-contiguous elements in the peptide sequences, but it does not provide a clear, human-interpretable explanation of how these dependencies contribute to the model's predictions.\n\nThe use of transfer learning in the MHCnuggets model further complicates interpretability. Transfer learning allows the model to leverage information from well-characterized MHC alleles to improve predictions for rare, less-studied alleles. While this approach can improve the model's performance, it does not provide a clear explanation of how the model is able to generalize from one allele to another.\n\nIn summary, while the MHCnuggets model is a powerful tool for predicting peptide-MHC binding, it is not easily interpretable. The model's use of LSTM architecture and transfer learning allows it to capture complex patterns in the data, but it does not provide a clear, human-interpretable explanation of how these patterns contribute to the model's predictions.",
  "model/output": "The model, MHCnuggets, is primarily designed for classification tasks. It predicts whether a given peptide is likely to bind to a specific MHC allele, which is a binary classification problem. The output of the model is a single sigmoid unit, indicating the probability of a peptide being a binder. This is achieved using binary cross-entropy loss for training when dealing with immunopeptidomic (HLAp) binary labels. However, the model can also handle regression tasks when predicting continuous binding affinity measurements from in vitro experiments, using mean-squared error loss. In this case, the output is a continuous value representing the predicted binding affinity. The model's architecture, which includes a long short-term memory (LSTM) neural network, allows it to handle variable length sequence inputs and learn long-term dependencies between non-contiguous elements, making it versatile for both classification and regression tasks in the context of peptide-MHC binding predictions.",
  "model/duration": "The execution time of our model, MHCnuggets, is notably efficient, especially when leveraging GPU architecture. For an input of one million peptides, MHCnuggets demonstrated significant speed advantages over other methods. Specifically, it ran 4.5 times faster than MHCflurry 1.2.0, 3.2 times faster than NetMHC 4.0, and 18 times faster than NetMHCpan 4.0 for class I peptides. The performance improvement was even more pronounced for class II peptides, where MHCnuggets was 65.6 times faster than NetMHCII2.3 and 126 times faster than NetMHCIIpan 3.2.\n\nIn practical terms, when processing a large cohort of patients, such as those in The Cancer Genome Atlas (TCGA), MHCnuggets handled 26,284,638 allele-peptide comparisons supported by RNAseq expression in under 2.3 hours. This efficiency is crucial for scalable analysis in clinical settings, allowing for rapid prediction of candidate immunogenic missense mutations across extensive datasets. The model's ability to decrease runtime per peptide exponentially as the total number of input peptides increases further underscores its scalability and efficiency.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of MHCnuggets involved a comprehensive assessment using multiple benchmarking strategies and datasets. We utilized six benchmark sets, including MHC class I alleles, MHC class II alleles, common alleles with trained models, and rare alleles. The evaluation methods included independent benchmark tests, previously published paired training/testing benchmarks, five-fold cross-validation, and leave-one-molecule-out (LOMO) benchmarks.\n\nFor MHC class I predictors, we compared MHCnuggets with several methods, including MHCflurry, NetMHC, and NetMHCpan, using an independent set of MHC-bound peptides identified by mass spectrometry across seven cell lines for six MHC I alleles. The performance was evaluated using metrics such as PPVn, auROC, Kendall\u2019s tau, and Pearson\u2019s r correlations. Additionally, we assessed the methods on the Kim et al. dataset, where predictors were trained with the BD2009 data and tested on BLIND data.\n\nFor MHC class II predictors, we used the Jensen et al. five-fold cross-validation benchmark to assess allele-specific MHC class II prediction. We also compared MHCnuggets to NetMHC group\u2019s MHC class II methods using self-reported performance statistics. The LOMO benchmarks were designed to estimate the performance of peptide binding prediction for rare MHC alleles. This involved holding out data for a single allele and training networks for the remaining alleles, then generating predictions for each peptide using the remaining networks.\n\nThe performance metrics included positive predictive value with respect to the top-ranked n peptides (PPVn), area under the ROC curve (auROC), Kendall\u2019s tau, and Pearson\u2019s r correlations. These metrics were calculated for various datasets, including the Bassani-Sternberg/Trolle (BST) dataset, which served as an independent benchmark. The evaluation also considered the top 50 and 500 ranked peptides to assess the methods' ability to minimize false positives among top-scored peptides.",
  "evaluation/measure": "In our evaluation, we employed several performance metrics to comprehensively assess the predictive capabilities of MHCnuggets and other tools. The primary metrics reported include the positive predictive value (PPV), specifically PPVn, which evaluates the proportion of true positives among the top n ranked peptides, where n is the number of true binders. This metric is crucial for understanding the precision of the predictions at the top of the ranked list.\n\nAdditionally, we calculated the area under the receiver operating characteristic curve (auROC), which provides a measure of the overall performance of the predictor across all threshold levels. This metric is widely used in the literature and offers a single value that summarizes the trade-off between the true positive rate and the false positive rate.\n\nTo assess the correlation between predicted and actual binding affinities, we computed Pearson's r and Kendall's tau correlations. These statistical measures help to evaluate the strength and direction of the relationship between the predicted and observed values, providing insights into the consistency and reliability of the predictions.\n\nFor the BST benchmark, we also calculated PPV over the top 50 and 500 ranked peptides, offering a more granular view of the predictor's performance at different levels of the ranked list. This approach allows for a detailed analysis of how well the tool performs not just at the very top of the list but also at deeper levels.\n\nThese metrics collectively provide a robust evaluation framework, aligning with common practices in the field. The use of PPVn, auROC, Pearson's r, and Kendall's tau ensures that our assessment is both comprehensive and representative of the standards in the literature. This set of metrics allows for a thorough comparison of MHCnuggets with other established tools, such as those from the NetMHC group and MHCflurry, ensuring that our evaluations are rigorous and meaningful.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the performance of MHCnuggets against several publicly available methods using various benchmark datasets. We compared MHCnuggets with MHCflurry tools, as well as tools from the NetMHC group, including NetMHC3.0, NetMHC 4.0, NetMHCpan2.0, and NetMHCpan 4.0. These comparisons were conducted using multiple benchmarking strategies, including independent test sets, previously published paired training/testing benchmarks, five-fold cross-validation, and leave-one-molecule-out (LOMO) benchmarks.\n\nFor MHC class I predictions, we used independent binding affinity and HLAp datasets. We tested MHCnuggets against MHCflurry 1.2.0, MHCflurry (train-MS), NetMHC 4.0, and NetMHCpan 4.0 using an independent set of MHC-bound peptides identified by mass spectrometry across seven cell lines for six MHC I alleles. Additionally, we evaluated MHCnuggets (noMS) and MHCflurry (noMS) against NetMHC 3.0 and NetMHCpan 2.0 using the Kim et al. dataset, where each predictor was trained with the BD2009 data and tested on BLIND data.\n\nFor MHC class II predictions, we compared MHCnuggets with the self-reported performance statistics of NetMHC group\u2019s MHC class II methods. We used the Jensen et al. five-fold cross-validation benchmark to assess allele-specific MHC class II prediction for 27 alleles.\n\nThe LOMO benchmarks were designed to estimate the performance of peptide binding prediction with respect to rare MHC alleles. We selected 20 alleles with 30 to 100 characterized peptides in the Immune Epitope Database (IEDB) for class I rare allele prediction and used the Jensen et al. LOMO benchmark for class II rare allele prediction.\n\nRegarding simpler baselines, not applicable.",
  "evaluation/confidence": "The evaluation of MHCnuggets involved multiple benchmarking strategies, including independent test sets, paired training/testing benchmarks, five-fold cross-validation, and leave-one-molecule-out (LOMO) cross-validation. These strategies were employed to ensure robust and comprehensive performance assessment.\n\nFor the independent test set, peptides not included in the training data were used to evaluate the predictors. This approach helps in assessing the generalizability of the models. The performance metrics calculated include positive predictive value (PPVn), area under the ROC curve (auROC), Kendall\u2019s tau, and Pearson\u2019s r correlations. These metrics provide a detailed view of the model's performance, but specific confidence intervals for these metrics are not mentioned.\n\nIn the five-fold cross-validation benchmark, the data was split into five parts, with the model trained on four parts and tested on the remaining one. This process was repeated five times, ensuring that each part was used for testing once. The results from these iterations were averaged to provide a more reliable estimate of the model's performance. However, the statistical significance of these results in comparison to other methods is not explicitly stated.\n\nThe LOMO benchmark was designed to evaluate the performance on rare MHC alleles. This method involves holding out data for a single allele and training the model on the remaining alleles. Predictions are then made for the held-out allele. The mean PPVn and auROC values were reported for the class I and class II alleles, but again, specific confidence intervals or statistical significance tests are not provided.\n\nWhile the evaluation methods are rigorous and cover various aspects of model performance, the lack of reported confidence intervals and explicit statistical significance tests makes it challenging to definitively claim superiority over other methods. The performance metrics are presented as averages or means, which provide a general idea of the model's capabilities but do not offer a detailed statistical analysis.",
  "evaluation/availability": "All datasets used in this work are publicly available. They can be accessed at http://dx.doi.org/10.17632/8c26kkrfpr.2. This includes the data sources for network training and testing, as well as the benchmark datasets used for evaluating the performance of MHCnuggets. The datasets encompass a wide range of information, including TCGA somatic mutations, TCGA tumor gene expression, and haplotype calling. Additionally, curated versions of the IEDB database and immunopeptidomes from various sources were utilized. The availability of these datasets ensures reproducibility and allows other researchers to validate and build upon the findings presented in this work."
}