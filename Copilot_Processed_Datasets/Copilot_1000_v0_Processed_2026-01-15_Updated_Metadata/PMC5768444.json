{
  "publication/title": "Results of the 2016 International Skin Imaging Collaboration International Symposium on Biomedical Imaging challenge: Comparison of the accuracy of computer algorithms to dermatologists for the diagnosis of melanoma from dermoscopic images.",
  "publication/authors": "Marchetti MA, Codella NCF, Dusza SW, Gutman DA, Helba B, Kalloo A, Mishra N, Carrera C, Celebi ME, DeFazio JL, Jaimes N, Marghoob AA, Quigley E, Scope A, Y\u00e9lamos O, Halpern AC,",
  "publication/journal": "Journal of the American Academy of Dermatology",
  "publication/year": "2018",
  "publication/pmid": "28969863",
  "publication/pmcid": "PMC5768444",
  "publication/doi": "10.1016/j.jaad.2017.08.016",
  "publication/tags": "- IRB\n- SVM\n- CNN\n- Skin cancer\n- Dermoscopic images\n- Melanoma diagnosis\n- Computer algorithms\n- Dermatologist accuracy\n- Image classification\n- Diagnostic performance",
  "dataset/provenance": "The dataset used in our study originated from the International Skin Imaging Collaboration (ISIC) Archive, an open-access repository of dermoscopic images of skin lesions. This archive was created to facilitate education and research in the field of skin imaging. For this particular challenge, 1,552 lesions were initially selected from approximately 12,000 dermoscopic images available in the ISIC Archive. After excluding 273 images due to inadequate quality, 1,279 lesions remained. These included 248 melanomas and 1,031 nevi or lentigines. The dataset was then randomly divided into training and test sets, with 900 images used for training and 379 for testing. The images were obtained using various camera/dermatoscope combinations and came from over 12 dermatology clinics worldwide. This diversity in data sources enhances the generalizability of our findings. The dataset has been used in previous studies and challenges, notably the 2016 International Symposium on Biomedical Imaging (ISBI) Melanoma Detection Challenge, where 25 competing teams participated. The results and rankings from this challenge are publicly available, contributing to the ongoing development and validation of computer vision algorithms for melanoma detection.",
  "dataset/splits": "The dataset used in the study consisted of 1,279 lesions, which were divided into two main splits: a training set and a test set. The training set contained 900 images, with 19.2% of these being melanomas. The test set comprised 379 images, with 19.8% being melanomas. All melanomas and a majority of the nevi/lentigines in the dataset had been histopathologically examined. The images were obtained from multiple camera/dermatoscope combinations and originated from more than 12 dermatology clinics around the world. The dataset was curated to exclude images with poor focus or those that included multiple lesions or lesions that encompassed the entire field of view. Additionally, a subset of 100 images from the test set was used in a reader study involving eight experienced dermatologists. This subset included 50 melanomas and 50 benign neoplasms.",
  "dataset/redundancy": "The dataset used in our study consisted of 1,552 lesions initially chosen from approximately 12,000 dermoscopic images in the ISIC Archive. After excluding 273 images due to inadequate quality, 1,279 lesions remained, comprising 248 melanomas and 1,031 nevi or lentigines. The dataset was randomly divided into training and test sets. The training set included 900 images, with 19.2% being melanomas, while the test set consisted of 379 images, with 19.8% being melanomas. This random split ensured that the training and test sets were independent, reducing the risk of data leakage and ensuring that the performance metrics accurately reflected the models' generalizability.\n\nTo enforce independence between the training and test sets, images were excluded if they were poorly focused or included multiple lesions or lesions that encompassed the entire field of view. Additionally, all melanomas and a majority of the nevi/lentigines in the dataset had been histopathologically examined, ensuring a high level of diagnostic certainty. Non-histopathologically examined nevi were included only if they were reviewed by two or more dermatologists to confirm their benign nature.\n\nThe distribution of our dataset compares favorably to previously published machine learning datasets in dermatology. Our dataset included a diverse range of images obtained with multiple camera/dermatoscope combinations from over 12 dermatology clinics around the world. This diversity enhances the generalizability of our findings and ensures that the dataset is representative of real-world clinical scenarios. Furthermore, the public availability of our dataset allows for external and independent analysis, promoting transparency and reproducibility in research.",
  "dataset/availability": "The dataset used in our study is publicly available, which is a significant aspect of our work. We created an open-access archive of dermoscopic images of skin lesions through the International Skin Imaging Collaboration (ISIC) Melanoma Project. This archive is designed for educational and research purposes.\n\nThe dataset originated from more than 12 dermatology clinics worldwide, ensuring a diverse and representative sample. It includes 1,279 lesions, with 248 melanomas and 1,031 nevi or lentigines. The images were obtained using multiple camera/dermatoscope combinations, further enhancing the dataset's variability and generalizability.\n\nThe dataset was randomly divided into training and test sets. The training set consists of 900 images, with 19.2% being melanomas, while the test set includes 379 images, with 19.8% being melanomas. All melanomas and a majority of the nevi/lentigines had been histopathologically examined to ensure accuracy.\n\nThe public availability of our dataset allows for external and independent analysis, as well as its use as a future reference dataset by developers of diagnostic tools. This openness is crucial for advancing the field of computerized lesion classification, education, and clinical decision support. The ISIC Archive, where our dataset is hosted, permits comparison of the performance of individual algorithms and supports fusion experiments.\n\nThe dataset is accessible through the ISIC Archive, and its use is governed by the terms and conditions specified by the ISIC, ensuring that researchers and developers can utilize it responsibly and ethically. This public availability and the enforcement of usage terms are designed to foster collaboration and innovation in the field of melanoma detection and diagnosis.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages machine learning fusion techniques, specifically utilizing predictions from multiple algorithms to enhance diagnostic performance. The class of machine-learning algorithms used includes support vector machines (SVM) and convolutional neural networks (CNN). These are well-established methods in the field of computer vision and medical imaging.\n\nThe fusion algorithm is not entirely new but represents an innovative application in the context of melanoma diagnosis. It aggregates the performance of multiple classifiers from around the world, which increases the likelihood that the results reflect the current state-of-the-art in computer vision. This approach was chosen to compare the aggregated model's performance against that of dermatologists, providing a robust benchmark for diagnostic accuracy.\n\nThe decision to publish this work in a dermatology journal rather than a machine-learning journal is driven by the clinical relevance and impact of the study. The primary focus is on the diagnostic performance of computer algorithms in comparison to human experts, which is of significant interest to the dermatology community. The study's findings contribute to the advancement of diagnostic tools in clinical practice, making it more appropriate for a specialized medical journal. Additionally, the dataset used is public, allowing for external and independent analysis, which further supports the clinical applicability of the research.",
  "optimization/meta": "In our study, we implemented a meta-predictor approach by combining the predictions of multiple automated algorithms to create a single, more robust prediction. This meta-predictor utilized data from 25 participating teams in the ISBI challenge, integrating their outputs to enhance performance.\n\nThe meta-predictor consisted of five different methods:\n\n1. **Prediction Score Averaging**: This non-learned approach simply averaged the prediction scores from all participating algorithms.\n2. **Voting**: Another non-learned method where the final prediction was determined by majority vote among the algorithms.\n3. **Greedy Ensemble Fusion**: This method selected the top-performing algorithms from the 25 total to create a fused prediction.\n4. **Linear Binary Support Vector Machine (SVM)**: A machine learning method that used a linear kernel to combine the predictions.\n5. **Non-Linear Binary SVM (Histogram Intersection Kernel)**: A more complex machine learning method that employed a non-linear kernel to integrate the predictions.\n\nTo ensure the independence of the training data, we used a separate set of 279 test images that were not involved in the reader study. These images were used to train the fusion methods, ensuring that the training data was independent of the data used to evaluate the performance of the meta-predictor. This approach helped to validate the generalizability and robustness of our meta-predictor.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithms involved several steps to ensure the images were suitable for analysis. The dataset consisted of dermoscopic images of skin lesions, which were obtained using multiple camera/dermatoscope combinations from over 12 dermatology clinics worldwide. Initially, 1,552 lesions were selected from approximately 12,000 images in the ISIC Archive. Images with poor focus or those including multiple lesions or lesions that encompassed the entire field of view were excluded, resulting in 1,279 lesions. These lesions were divided into training and test sets, with 900 images used for training and 379 for testing. The training set included 19.2% melanomas, while the test set had 19.8% melanomas. All melanomas and a majority of the nevi/lentigines had been histopathologically examined to confirm their diagnoses.\n\nFor the reader study, 50 randomly selected melanomas and 50 benign neoplasms were used. Non-histopathologically examined benign lesions were excluded from this subset. The images were reviewed by experienced dermatologists to confirm their benign nature. The dataset was further refined to include a diverse range of lesions, although it did not cover the complete spectrum of skin lesions encountered in clinical practice. This included limitations such as the absence of certain types of melanomas and benign lesions that would not typically undergo biopsy.\n\nThe images were preprocessed to ensure consistency and quality. This involved standardizing the image resolution, color balance, and other parameters to minimize variability introduced by different imaging devices. The preprocessing steps were crucial for the machine-learning algorithms, which relied on deep learning techniques. These algorithms used multiple processing layers to automatically identify increasingly abstract concepts present in the data. The final dataset was used to train and evaluate the performance of the algorithms, with the results ranked using average precision. This metric corresponded to the integral under a precision-recall curve, which depicts positive predictive value and sensitivity. The preprocessing and encoding steps were designed to enhance the accuracy and reliability of the machine-learning models in classifying skin lesions.",
  "optimization/parameters": "Not applicable",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "In our study, we employed several statistical methods and models to ensure that our results were robust and that issues of overfitting and underfitting were addressed appropriately.\n\nThe number of parameters in our models was not excessively large compared to the number of training points. We used regression models with a general estimating equations approach, which included readers as a covariate. This approach allowed us to account for the potential clustering of observations within readers, thereby adjusting the standard error estimates and mitigating the risk of overfitting.\n\nTo further ensure that overfitting was not a concern, we utilized cross-validation techniques. Specifically, we trained our fusion methods on a separate test set of images (n=279) that were not involved in the reader study. This independent test set helped us to validate the performance of our models and ensure that they generalized well to new, unseen data.\n\nAdditionally, we evaluated the performance of our models using multiple metrics, including sensitivity, specificity, and area under the receiver operating characteristic (ROC) curves. These metrics provided a comprehensive assessment of model performance and helped us to identify any potential issues with underfitting. By comparing the performance of individual algorithms and fusion methods, we were able to select the most effective models and ensure that they were not overly simplistic.\n\nIn summary, our approach to model fitting included the use of appropriate statistical methods, cross-validation, and multiple performance metrics. These steps helped us to address potential issues of overfitting and underfitting, ensuring that our results were reliable and generalizable.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting when developing and evaluating our algorithms. One of the key methods used was ensemble fusion, which combines the predictions of multiple algorithms to improve overall performance and robustness. Specifically, we implemented five different fusion methods, including greedy ensemble fusion, linear binary support vector machine (SVM), and non-linear binary SVM with a histogram intersection kernel. These fusion techniques help to mitigate overfitting by leveraging the strengths of multiple models and reducing the reliance on any single algorithm.\n\nAdditionally, we used a separate test set of images that were not involved in the reader study to train our fusion methods. This approach ensures that the models are evaluated on unseen data, providing a more accurate assessment of their generalizability and reducing the risk of overfitting to the training data.\n\nFurthermore, we considered the performance of our algorithms in terms of average precision, which is a metric that takes into account the precision-recall trade-off. This metric is particularly useful for imbalanced datasets, such as those involving melanoma detection, where the number of benign lesions far outweighs the number of malignant ones. By focusing on average precision, we ensure that our models are not overly biased towards the majority class, thereby enhancing their ability to generalize to new, unseen data.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "Not enough information is available.",
  "model/output": "The model discussed in the publication primarily focuses on classification tasks. Specifically, it involves the classification of skin lesions as either benign or malignant. This is evident from the use of metrics such as sensitivity, specificity, and the area under the receiver operating characteristic (ROC) curves, which are commonly used to evaluate the performance of classification models. The models, including support vector machines (SVM) and convolutional neural networks (CNN), were trained and evaluated on datasets where the goal was to distinguish between different types of skin lesions. The output of these models provides probability scores ranging from 0 to 1, where scores closer to 0 indicate a higher probability of a benign diagnosis, and scores closer to 1 indicate a higher probability of malignancy. These probability scores are then used to make dichotomous predictions, typically with a threshold of 0.5 to classify lesions as benign or malignant. Additionally, the models were evaluated on their ability to make management decisions, such as recommending a biopsy or observation/reassurance, which further supports the classification nature of the task.",
  "model/duration": "Not applicable.",
  "model/availability": "Not applicable",
  "evaluation/method": "The evaluation method involved a comprehensive reader study and the performance of automated algorithms. Eight experienced dermatologists from four countries evaluated a subset of 100 dermoscopic images, classifying lesions as benign or malignant and indicating management decisions (biopsy or observation/reassurance). The dermatologists were blinded to the diagnosis and clinical metadata, with no time restrictions for their evaluations.\n\nThe performance of automated systems was assessed on the same 100 images. Five top-ranked individual algorithms from the ISBI 2016 Challenge and five fusion methods were evaluated. The fusion methods included non-learned approaches like prediction score averaging and voting, as well as machine learning methods such as greedy ensemble fusion, linear binary support vector machine (SVM), and non-linear binary SVM.\n\nThe main outcomes measured were sensitivity, specificity, and the area under the receiver operating characteristic (ROC) curves. Sensitivity was defined as the percentage of melanomas correctly identified as malignant, while specificity was the percentage of benign lesions correctly identified as benign. The ROC area provided a comprehensive measure of the algorithms' performance.\n\nStatistical analyses included descriptive statistics, agreement levels among dermatologists, and comparisons between reader and algorithm performance. Two-sample tests for proportions, chi-square tests, and regression models were used to evaluate diagnostic accuracy and inter-rater agreement. ROC curves were estimated for individual readers and the group, with comparisons made to assess differences in performance.\n\nThe study also highlighted the importance of public challenges and open-access image resources, such as the ISIC Archive, for advancing the field of computerized lesion classification and clinical decision support. The results underscored the value of larger and more diverse datasets for improving diagnostic tools and educational resources.",
  "evaluation/measure": "In our study, we focused on several key performance metrics to evaluate the diagnostic accuracy of both dermatologists and computer algorithms. These metrics included sensitivity, specificity, and the area under the receiver operating characteristic (ROC) curve.\n\nSensitivity, also known as the true positive rate, measures the proportion of actual melanomas that were correctly identified as malignant. Specificity, or the true negative rate, indicates the proportion of benign lesions that were correctly classified as benign. The ROC curve is a graphical representation of the trade-off between sensitivity and specificity at various threshold settings, and the area under the ROC curve (AUC) provides a single scalar value that summarizes this trade-off. A higher AUC indicates better overall performance.\n\nWe reported these metrics for both lesion classification and management decisions. For lesion classification, sensitivity and specificity were defined as the percentage of melanomas and benign lesions, respectively, that were correctly identified. For management decisions, sensitivity was the percentage of melanomas indicated for biopsy, and specificity was the percentage of benign lesions indicated for observation or reassurance.\n\nIn addition to these primary metrics, we also evaluated the performance of computer algorithms using average precision, which is the integral under a precision-recall curve. This metric is particularly useful for imbalanced datasets, where the number of benign lesions far outweighs the number of malignant lesions.\n\nOur choice of metrics is representative of the literature in the field of medical image analysis and computer-aided diagnosis. Sensitivity and specificity are standard metrics for evaluating diagnostic tests, and the ROC curve is a well-established method for assessing the performance of binary classifiers. The inclusion of average precision provides additional insight into the performance of computer algorithms, especially in the context of imbalanced datasets.\n\nOverall, our set of performance metrics provides a comprehensive evaluation of the diagnostic accuracy of both dermatologists and computer algorithms, allowing for a fair and meaningful comparison between the two.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of our computer vision algorithms with various methods and baselines to ensure a thorough evaluation. We compared multiple computer classifiers from around the world, including an aggregated model of their performance, to dermatologists. This approach increased the likelihood that our computer-vision results reflect the current state-of-the-art.\n\nWe also implemented five methods of fusing all automated predictions from the 25 participating teams in the ISBI challenge into a single prediction. These methods included two non-learned approaches: prediction score averaging and voting. Additionally, we employed three machine learning methods: greedy ensemble fusion, linear binary support vector machine (SVM), and non-linear binary SVM (histogram intersection kernel).\n\nTo evaluate the performance of these algorithms, we used a dataset of 100 dermoscopic images evaluated by dermatologists. The fusion algorithms were ranked by average precision on this reader set. This comparison allowed us to assess the effectiveness of different fusion techniques and their potential to improve diagnostic accuracy.\n\nFurthermore, we compared the performance of our algorithms to simpler baselines. For instance, we evaluated non-learning methods and more complex SVM models to understand their relative performance. The re-learned probabilistic SVM thresholds increased the sensitivity of the corresponding systems considerably, highlighting the importance of advanced techniques in improving diagnostic accuracy.\n\nOverall, our study involved a rigorous comparison of various algorithms and fusion methods, providing a comprehensive evaluation of their performance in melanoma detection.",
  "evaluation/confidence": "The evaluation of the performance metrics in this study includes several statistical measures to assess the confidence and significance of the results. Descriptive statistics such as means and standard deviations were used to describe the dermatologist lesion classifications and management decisions. Overall percent agreement, kappa, and intraclass correlation were employed to evaluate reader responses for lesion classification and management, providing a robust assessment of inter-rater agreement.\n\nFor the comparison between readers and algorithms, two-sample tests for proportions along with chi-square tests were used. These tests help in determining whether the differences in diagnostic performance between readers and algorithms are statistically significant. Additionally, regression models for binary outcomes using a general estimating equations approach with a log link and an exchangeable correlation structure were utilized. These models allowed for between-reader comparisons of accuracy and stratified analyses, adjusting the standard error estimates for the potential of clustered observations within readers.\n\nReceiver operating characteristic (ROC) curves were estimated for individual readers and for the readers as a group. Comparisons of the area under the ROC curves were performed to assess differences in reader performance and to make comparisons between reader and algorithm performance. The area under the ROC curve is equivalent to the average of sensitivity and specificity for dichotomous predictions, providing a comprehensive measure of diagnostic accuracy.\n\nThe alpha level was set at 5%, and all presented p-values are two-sided, ensuring a rigorous standard for statistical significance. The analyses were performed with StataSE v14.1, a widely used statistical software that ensures the reliability and validity of the results. This comprehensive statistical approach provides confidence in the performance metrics and the significance of the findings, supporting the claim that the methods evaluated are robust and comparable to existing baselines.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study utilized a dataset from the International Skin Imaging Collaboration (ISIC) Archive, which is an open-access archive of dermoscopic images of skin lesions. This archive permits comparison of the performance of individual algorithms and supports fusion experiments. However, the specific files used for the evaluation in this particular study are not released publicly. The ISIC Archive itself is accessible for educational and research purposes, allowing for external and independent analysis. For those interested in using the dataset, it is important to review the licensing terms associated with the ISIC Archive to ensure compliance with its usage policies."
}