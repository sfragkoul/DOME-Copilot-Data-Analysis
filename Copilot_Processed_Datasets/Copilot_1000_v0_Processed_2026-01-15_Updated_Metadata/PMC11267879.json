{
  "publication/title": "Random forest machine-learning algorithm classifies white- and brown-rot fungi according to the number of the genes encoding Carbohydrate-Active enZyme families.",
  "publication/authors": "Hasegawa N, Sugiyama M, Igarashi K",
  "publication/journal": "Applied and environmental microbiology",
  "publication/year": "2024",
  "publication/pmid": "38832775",
  "publication/pmcid": "PMC11267879",
  "publication/doi": "10.1128/aem.00482-24",
  "publication/tags": "- Machine Learning\n- Random Forest\n- Fungi Classification\n- Carbohydrate-Active Enzymes\n- Genomics\n- Comparative Genomics\n- White-Rot Fungi\n- Brown-Rot Fungi\n- Data Oversampling\n- Phylogenetic Analysis",
  "dataset/provenance": "The dataset used in this study focuses on the decay modes of fungi, specifically white-rot and brown-rot fungi. The data consists of 232 samples of CAZymes (Carbohydrate-Active enZymes) annotated data from 15 different orders. Among these, two samples have unknown classifications. The dataset is imbalanced, with 183 samples representing white-rot fungi and 49 samples representing brown-rot fungi. The white-rot fungi are distributed across 11 orders, with significant representations from Agaricales and Polyporales, accounting for 41% and 33% of the total samples, respectively. Brown-rot fungi are found in seven orders, with Polyporales being the most prevalent, comprising 37% of the brown-rot samples.\n\nThe dataset has been utilized to build and evaluate machine learning models, specifically Linear Discriminant Analysis (LDA) and Random Forest (RF) models. These models were trained to classify the decay modes of fungi based on the number of genes encoding CAZyme families. The performance of these models was assessed using various metrics, including the percentage of correct responses, fit rate, recall rate, and F1 score. The models demonstrated high accuracy, with LDA achieving 94.6% and RF achieving 96.8% correct responses even when trained on the original imbalanced dataset.\n\nThe dataset has also been used to construct phylogenetic trees, which align well with established classifications, such as those provided by NCBI and JGI MycoCosm. This alignment helps in understanding the evolutionary relationships among the fungi and the distribution of decay modes within different orders. The phylogenetic analysis identified four major groups of brown-rot fungi, with some exceptions like Leptoporus mollis and Fistulina hepatica, which were placed in the white-rot group despite being brown-rot fungi.",
  "dataset/splits": "The dataset was split into training and test data using a 7:3 ratio. This process was repeated 100 times with randomized oversampling and data splitting. Additionally, 100 models were constructed from the original dataset using only data splitting, and 1,000 models were constructed from the oversampled dataset using both oversampling and data splitting. The dataset consisted of 232 samples of CAZymes annotated data, with 183 samples for white-rot fungi and 49 samples for brown-rot fungi.",
  "dataset/redundancy": "The dataset used in this study was split into training and test sets with a 7:3 ratio. This means that 70% of the data was used for training the models, while the remaining 30% was reserved for testing their performance. The process of splitting the data was randomized and repeated 100 times to ensure the robustness of the results. This randomization helps to enforce the independence of the training and test sets, as each split is unique and not predetermined.\n\nThe dataset consisted of 232 samples of CAZymes annotated data from 15 orders, with a significant imbalance between white-rot and brown-rot fungi. Specifically, there were 183 samples of white-rot fungi and 49 samples of brown-rot fungi. This imbalance is a common challenge in machine learning, and it was addressed through oversampling techniques to create a balanced dataset for training.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the field of comparative genomics. The use of oversampling techniques, such as SMOTE, helped to mitigate the bias that often arises from imbalanced datasets. This approach ensures that the models are trained on a representative sample of both classes, leading to more accurate and reliable predictions.\n\nThe independence of the training and test sets was further enforced by the repeated randomization of the data splitting process. This method ensures that the models are not overfitted to a specific subset of the data and that their performance can be generalized to new, unseen data. The use of multiple oversampling techniques and the repetition of the training and testing process 100 times provide a comprehensive evaluation of the models' performance and robustness.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm used in our study is the Random Forest (RF) algorithm. This algorithm is not new; it was proposed by Breiman in 2001. It follows the concept of ensemble learning called bagging and uses majority voting of a large number of weak learners, specifically decision trees, to obtain the final prediction result. This approach improves the generalization ability and produces a highly accurate model.\n\nThe RF algorithm is particularly well-suited for our work in comparative genomics because it can handle datasets with a large number of variables and a small number of samples, which is common in the life science field. Additionally, RF can calculate the importance of each explanatory variable, allowing us to identify the genes involved in determining traits. This interpretability is crucial for our research, as it helps us understand the causal relationships between genes and traits.\n\nWhile the RF algorithm is well-established, its application in comparative genomics to classify white- and brown-rot fungi based on the number of Carbohydrate-Active enZyme (CAZyme) families is novel. This specific application and the insights gained from it are the primary contributions of our study.\n\nThe reason this work was published in a microbiology journal rather than a machine-learning journal is that the focus is on the biological implications and the novel application of the RF algorithm in comparative genomics. The study provides valuable insights into the decay modes of wood-rotting basidiomycetes and the enzyme families involved, which are of significant interest to the microbiology community.",
  "optimization/meta": "The models employed in this study do not function as meta-predictors. Instead, they utilize two distinct machine-learning algorithms: Linear Discriminant Analysis (LDA) and Random Forest (RF). These algorithms are applied independently to the dataset, which consists of annotated data samples of CAZymes from various fungal orders.\n\nLDA is used to calculate a discriminant function that maximizes the ratio of within-group variance to between-group variance, thereby discriminating between two groups based on the side of the hyperplane each sample lies on. This function is expressed as a linear combination of explanatory variables, allowing for the assessment of each variable's contribution to the discrimination.\n\nOn the other hand, the RF algorithm follows the concept of ensemble learning known as bagging. It uses majority voting from a large number of weak learners, specifically decision trees, to obtain the final prediction result. Each decision tree is built from randomly selected data, ensuring that the model's generalization ability is improved through the aggregation of these weak learners.\n\nThe training data for both LDA and RF models is derived from the original dataset, which is split into training and test sets in a 7:3 ratio. For the RF models, oversampling techniques such as SMOTE are used to balance the dataset, addressing the imbalance between white-rot and brown-rot fungi samples. This process ensures that the models are trained on independent data, enhancing their predictive accuracy and robustness.\n\nIn summary, the models utilized in this study are not meta-predictors but rather standalone LDA and RF algorithms. The training data for these models is independently split and, in the case of RF, potentially oversampled to improve performance and address data imbalance.",
  "optimization/encoding": "For the machine-learning algorithms employed in our study, the data encoding and preprocessing steps were tailored to the specific requirements of each algorithm.\n\nFor Linear Discriminant Analysis (LDA), we began by addressing multicollinearity, a condition where explanatory variables are highly correlated, which can lead to invalid results. To mitigate this, we calculated the correlation ratio between the number of genes in each Carbohydrate-Active enZyme (CAZy) family and the decay mode trait (white-rot or brown-rot). Families with a correlation ratio below 0.1 were eliminated. For remaining pairs of families with correlation coefficients above 0.55, we retained the family with the higher correlation ratio to the trait. This refinement process resulted in 18 CAZy families/subfamilies, which were then standardized and used as explanatory variables in the LDA analysis.\n\nIn contrast, the Random Forest (RF) algorithm does not suffer from multicollinearity issues. Therefore, all CAZy families/subfamilies were used as explanatory variables without any elimination. Additionally, standardization of explanatory variables was not performed for RF, as the specific values of the explanatory variables are not meaningful in this context.\n\nThe dataset used consisted of 232 CAZymes annotated data samples from 15 orders, with a significant imbalance between white-rot and brown-rot fungi. To address this imbalance, we employed the Synthetic Minority Over-sampling Technique (SMOTE) to generate hypothetical brown-rot fungus samples, creating a balanced dataset. This oversampled dataset, along with the original dataset, was split into training and test data in a 7:3 ratio. The training data were used to build the LDA and RF models, with their performance evaluated using metrics such as the percentage of correct responses, fit rate, recall rate, and F1 score against the test data. This process was repeated multiple times with randomized data splitting and oversampling to ensure robustness.",
  "optimization/parameters": "In our study, the number of parameters (p) used in the model varied between the two machine learning algorithms employed.\n\nFor the Linear Discriminant Analysis (LDA), we initially had a large number of CAZy families/subfamilies as potential explanatory variables. However, to avoid multicollinearity issues, we narrowed down these variables through a specific procedure. First, we calculated a correlation ratio between the number of genes in each family and the white-rot/brown-rot trait, eliminating those with a value of less than 0.1. Then, for remaining pairs of families with correlation coefficients greater than 0.55, we eliminated the family with the lower correlation ratio to the trait. This refinement process resulted in 18 CAZy families/subfamilies being used as explanatory variables in the LDA analysis.\n\nIn contrast, for the Random Forest (RF) analysis, all CAZy families/subfamilies were used as explanatory variables. This is because RF is less sensitive to multicollinearity compared to LDA. Therefore, the number of parameters in the RF model was much higher, encompassing all available CAZy families/subfamilies.\n\nThe selection of parameters for LDA was driven by the need to mitigate multicollinearity, ensuring that the importance of each explanatory variable could be accurately assessed. For RF, the inclusion of all CAZy families/subfamilies was feasible due to the algorithm's robustness to multicollinearity and its ability to handle a large number of variables.",
  "optimization/features": "In the optimization process, the number of input features used varied between the two models employed. For the Linear Discriminant Analysis (LDA) model, feature selection was performed to mitigate the issue of multicollinearity. This involved calculating a correlation ratio between the number of genes in each CAZy family and the decay mode trait, eliminating families with a value below 0.1. Additionally, for pairs of families with correlation coefficients above 0.55, the family with the lower correlation ratio to the trait was removed. This refinement process resulted in 18 CAZy families/subfamilies being used as explanatory variables in the LDA analysis. These selected features were then standardized before being used in the model.\n\nIn contrast, the Random Forest (RF) model did not require feature selection due to its robustness against multicollinearity. Therefore, all CAZy families/subfamilies were used as explanatory variables without any standardization. This approach leverages the RF algorithm's ability to handle a large number of variables and its inherent feature importance calculation, which helps in interpreting the model's predictions.",
  "optimization/fitting": "In our study, we dealt with a dataset where the number of variables (genes encoding Carbohydrate-Active enZyme families) was indeed much larger than the number of samples. This scenario is common in life science data and can lead to overfitting, where the model performs well on training data but poorly on unseen data.\n\nTo mitigate overfitting, we employed several strategies. Firstly, we used oversampling techniques to balance the dataset, which helped in reducing bias and improving the model's generalization ability. Specifically, we used SMOTE, which is known for its effectiveness in handling imbalanced datasets. We also compared the performance of models trained on data oversampled using different methods, ensuring that our choice of oversampling technique was robust.\n\nSecondly, we utilized the Random Forest (RF) algorithm, which is designed to handle high-dimensional data and is less prone to overfitting compared to other algorithms. RF uses ensemble learning, aggregating the predictions of multiple decision trees, which helps in improving the model's generalization ability.\n\nTo further ensure that our models were not overfitting, we performed repeated randomized oversampling and data splitting. We split the dataset into training and test sets 100 times, each time with randomized oversampling and data splitting. This process helped in assessing the stability and robustness of our models.\n\nRegarding underfitting, we evaluated our models using multiple performance metrics, including correctness, fit, recall, and F-measure. The high percentages of correct responses (LDA: 94.6%, RF: 96.8%) even when trained on the original imbalanced dataset indicate that our models were not underfitting. Additionally, the use of RF, which is known for its high accuracy even with a large number of variables and a small number of samples, further supports that underfitting was not a concern in our study.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods we used was oversampling. We utilized four different oversampling techniques\u2014SMOTE, Borderline SMOTE, ADASYN, and ROSE\u2014to address the imbalance in our dataset. This approach helped to eliminate bias in the model's predictive ability. Among these, SMOTE was found to be the most effective and was subsequently used in our experiments.\n\nAdditionally, we split our dataset into training and test sets with a 7:3 ratio. This process was repeated 100 times with randomized oversampling and data splitting to construct multiple models. By averaging the results across these iterations, we reduced the risk of overfitting to any single split of the data.\n\nFor the Random Forest (RF) models, the ensemble learning approach inherently helps to prevent overfitting. RF uses majority voting from a large number of decision trees, each built from randomly selected data, which improves generalization and reduces the likelihood of overfitting.\n\nIn the case of Linear Discriminant Analysis (LDA), we carefully selected explanatory variables to avoid multicollinearity, which can lead to overfitting. We calculated correlation ratios and eliminated highly correlated families, ensuring that the remaining variables were independent and contributed meaningfully to the model.\n\nOverall, these techniques\u2014oversampling, randomized data splitting, ensemble learning, and variable selection\u2014were integral to our optimization process and helped to mitigate overfitting in our models.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are not explicitly detailed in the provided text. However, the methods and techniques employed for model building and evaluation are thoroughly described.\n\nFor the model building process, we utilized the Python libraries scikit-learn and imbalanced-learn. Specifically, we employed LinearDiscriminantAnalysis and RandomForestClassifier from scikit-learn to construct our LDA and RF models, respectively. The importance of each explanatory variable was measured using the coefficients of the discriminant function for LDA and the Gini importance for RF, both of which were normalized.\n\nTo address the imbalance in our dataset, we used SMOTE from the imbalanced-learn library to generate hypothetical brown-rot fungus samples through oversampling. This approach helped in creating a balanced dataset, which was then split into training and test data in a 7:3 ratio. The performance of the models was evaluated using metrics such as the percentage of correct responses, fit rate, recall rate, and F1 score.\n\nThe process of model building and evaluation was repeated multiple times with randomized oversampling and data splitting to ensure robustness. While the specific hyper-parameter configurations and optimization schedules are not provided, the methodologies and tools used are well-documented and can be replicated using the mentioned libraries.\n\nFor access to the detailed configurations and parameters, one would typically refer to the supplementary materials or the code repository associated with the publication. However, since this information is not available in the provided text, it is not possible to specify the exact location or licensing details for these resources.",
  "model/interpretability": "The models employed in this study, specifically Linear Discriminant Analysis (LDA) and Random Forest (RF), offer varying degrees of interpretability. LDA is relatively transparent, as it calculates a discriminant function that is a linear combination of the explanatory variables. This function represents a hyperplane that maximizes the separation between groups, allowing for the interpretation of each variable's contribution to the discrimination based on its coefficient. The coefficients of the discriminant function can be normalized and examined to understand the importance of each explanatory variable in the model.\n\nOn the other hand, RF is an ensemble learning method that uses multiple decision trees to make predictions. While individual decision trees can be interpreted, the ensemble nature of RF makes it more of a \"black box.\" However, RF provides a measure of variable importance known as Gini importance, which indicates how much each variable contributes to the predictive accuracy of the model. This measure can be used to identify the most influential variables, although it does not provide a straightforward linear interpretation like LDA. For instance, in our study, the number of genes in the AA9 family was identified as the most important variable in the RF model, highlighting its significant role in predicting decay modes.\n\nIn summary, while LDA offers a more transparent and interpretable model due to its linear nature, RF provides insights through variable importance measures, making it a valuable tool for understanding the underlying data structure and identifying key explanatory variables.",
  "model/output": "The model we developed is primarily focused on classification rather than regression. Specifically, we utilized both Linear Discriminant Analysis (LDA) and Random Forest (RF) algorithms to classify white- and brown-rot fungi based on the number of genes encoding Carbohydrate-Active enZyme (CAZyme) families. The LDA model calculates a discriminant function that maximizes the ratio of within-group to between-group variance, effectively separating the two groups. On the other hand, the RF model employs an ensemble learning approach called bagging, where multiple decision trees are combined to improve predictive accuracy. Both models were evaluated using metrics such as the percentage of correct responses, fit rate, recall rate, and F1 score, demonstrating their effectiveness in classifying the decay modes of fungi.\n\nThe Random Forest algorithm, in particular, is well-suited for classification tasks in the life sciences due to its ability to handle datasets with a large number of variables and a small number of samples. Additionally, RF can calculate the importance of each explanatory variable, which is crucial for identifying the genes involved in determining the decay modes of fungi. This feature makes RF an excellent choice for comparative genomics, where the focus is not only on prediction but also on understanding the underlying genetic factors.\n\nIn our experiments, we constructed multiple models using both the original imbalanced dataset and an oversampled balanced dataset. The models showed high percentages of correct responses, with the RF model achieving 96.8% accuracy and the LDA model achieving 94.6% accuracy. These results indicate that both models are effective in classifying the decay modes of fungi, even when trained on imbalanced data. The use of oversampling techniques, such as SMOTE, further improved the model's performance by addressing the class imbalance issue. Overall, the classification models developed in this study provide valuable insights into the genetic basis of fungal decay modes and demonstrate the potential of machine learning in comparative genomics.",
  "model/duration": "The execution time for the models varied depending on the specific processes involved. The data set was divided into training and test data in a 7:3 ratio, and this process was repeated multiple times to construct models. Specifically, 100 models were built from the original data set, and 1,000 models were constructed from the oversampled data set. The oversampling was performed using the SMOTE technique from the Python library imbalanced-learn.\n\nThe training data were used to build the LDA/RF model, and its performance was evaluated using metrics such as the percentage of correct responses, fit rate, recall rate, and F1 score against the test data. For each test sample, LDA and RF models were built using tools from the scikit-learn Python library. The coefficients of the discriminant function for LDA and the Gini importance for RF were obtained as measures of the importance of each explanatory variable in the model. These importance values were normalized by dividing them by the maximum value for each model.\n\nThe series of processes, including data splitting and model construction, were repeated with randomized data splitting only or with both oversampling and data splitting. This repetition was necessary to ensure the robustness and reliability of the models. The exact execution time for these processes was not explicitly mentioned, but it involved significant computational effort due to the large number of models constructed and the complexity of the algorithms used.",
  "model/availability": "The source code for the models and algorithms used in this study is not publicly released. However, the methods and tools employed are well-documented and can be replicated using publicly available software.\n\nFor the phylogenetic analysis, the W-IQ-TREE web server was utilized, which is accessible online and allows users to perform phylogenetic tree estimation using multiple alignments. The MAFFT online service was used for multiple alignments, and trimAI on Phylemon 2.0 was employed for trimming poor alignments. These tools are freely available and can be accessed through their respective websites.\n\nFor the machine learning models, the scikit-learn Python library was used, which is open-source and can be installed via standard package managers. The oversampling technique, SMOTE, was implemented using the imbalanced-learn library in Python, which is also open-source and freely available.\n\nWhile the specific implementations and scripts used in this study are not provided, the methodologies and tools are thoroughly described, enabling researchers to reproduce the analyses using the mentioned software and libraries.",
  "evaluation/method": "The evaluation method employed in this study involved a rigorous process to ensure the robustness and reliability of the models. The dataset was split into training and test sets in a 7:3 ratio. This split was repeated 100 times with randomized oversampling and data splitting to construct 100 models from the original dataset and 1,000 models from the oversampled dataset. This approach helped in assessing the models' performance under various conditions and ensured that the results were not dependent on a specific split of the data.\n\nThe performance of the models was evaluated using several metrics, including correctness, fit, recall, and F-measure. These metrics were computed for the test data using Linear Discriminant Analysis (LDA) and Random Forest Classifier (RF) models built from the training data. The use of these metrics provided a comprehensive evaluation of the models' predictive ability.\n\nIn addition to these metrics, the importance of each explanatory variable in the models was also assessed. For LDA, the coefficients of the discriminant function were obtained, while for RF, the Gini importance was calculated. Both importance values were normalized to facilitate comparison. This step was crucial in understanding the contribution of each variable to the models' predictions.\n\nThe oversampling techniques used in this study included SMOTE, Borderline SMOTE, ADASYN, and ROSE. These techniques were employed to address the imbalance in the dataset, which is a common issue in many real-world datasets. The effectiveness of these techniques in eliminating bias in the models' predictive ability was evaluated, with SMOTE being identified as the most effective and orthodox method.\n\nThe models were also evaluated in terms of their ability to handle multicollinearity, which is a common issue in datasets with a large number of variables. For LDA, the explanatory variables were carefully selected to avoid multicollinearity, while for RF, all variables were used as multicollinearity is not an issue in this method.\n\nOverall, the evaluation method used in this study was comprehensive and rigorous, ensuring that the models' performance was thoroughly assessed under various conditions. The use of multiple metrics, oversampling techniques, and importance assessment provided a holistic evaluation of the models.",
  "evaluation/measure": "In our study, we evaluated the performance of our models using several key metrics to ensure a comprehensive assessment. The primary metrics reported include correctness, fit, recall, and F-measure. These metrics were chosen to provide a well-rounded evaluation of the models' predictive abilities.\n\nCorrectness refers to the percentage of correct responses made by the model, indicating how often the model's predictions match the actual outcomes. Fit rate assesses how well the model's predictions align with the training data, reflecting the model's ability to learn from the data. Recall, also known as sensitivity, measures the model's ability to identify positive instances correctly. F-measure, or F1 score, is the harmonic mean of precision and recall, providing a single metric that balances both concerns.\n\nThese metrics are widely used in the literature and are representative of standard practices in model evaluation. They offer a robust framework for comparing the performance of different models and techniques, ensuring that our findings are both reliable and comparable to other studies in the field. By using these metrics, we can confidently report on the effectiveness of our oversampling techniques and the overall performance of our models.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of different oversampling techniques to address the imbalance in our dataset. We employed four methods: SMOTE, Borderline SMOTE, ADASYN, and ROSE, all sourced from the Python library imbalanced-learn. These techniques were used to oversample the dataset, and we then trained models using the oversampled data. The performance of these models was evaluated using metrics such as correctness, fit, recall, and F-measure.\n\nThe dataset was split into training and test sets in a 7:3 ratio, and this process was repeated 100 times with randomized oversampling and data splitting to ensure robustness. We used Linear Discriminant Analysis (LDA) and Random Forest (RF) as our modeling techniques, utilizing the scikit-learn library for implementation.\n\nOur comparison included evaluating the effectiveness of each oversampling method in eliminating bias in the model's predictive ability. While all four methods were effective, SMOTE was identified as the most effective and was subsequently used in further experiments. This approach allowed us to ensure that our models were not biased towards the majority class, thereby improving their generalizability and accuracy.\n\nAdditionally, we constructed models using both oversampling and data splitting, as well as data splitting alone. This involved creating 100 models from the original dataset and 1,000 models from the oversampled dataset. The performance of these models was assessed to understand the impact of oversampling on model accuracy and robustness.\n\nIn summary, our methods comparison involved a detailed evaluation of different oversampling techniques and their impact on model performance. This comprehensive approach ensured that our models were robust and unbiased, providing reliable insights into the genetic backgrounds of white- and brown-rot modes of decay in wood-rotting fungi.",
  "evaluation/confidence": "The evaluation of our models included the computation of performance metrics such as correctness, fit, recall, and F-measure. To assess the confidence in these metrics, we employed a rigorous process involving repeated randomized oversampling and data splitting. Specifically, we constructed 100 models from the original dataset and 1,000 models from the oversampled dataset. This approach allowed us to generate error bars representing standard deviations, providing a measure of variability and confidence in our performance metrics.\n\nStatistical significance was evaluated using Welch's t-test, which was applied between adjacent groups at the 5% significance level. This test helped us determine if there were meaningful differences in gene numbers between groups, with significance levels indicated by asterisks (e.g., *: p < 0.05, **: p < 0.01). This statistical analysis ensured that our claims about the superiority of certain methods, such as SMOTE, were grounded in robust evidence.\n\nAdditionally, the models demonstrated high percentages of correct responses even when trained on the original imbalanced dataset, with LDA achieving 94.6% and RF achieving 96.8%. These results, combined with the statistical tests and error bars, provide a strong foundation for claiming the effectiveness and reliability of our methods.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The evaluation process involved comparing the performance of models trained on both original and oversampled datasets. We used metrics such as correctness, fit rate, recall rate, and F1 score to assess model performance. The oversampling techniques, including SMOTE, were applied to balance the dataset, and the models were built using tools from the scikit-learn library. While the methods and results are detailed in the publication, the specific raw evaluation files are not released to the public."
}