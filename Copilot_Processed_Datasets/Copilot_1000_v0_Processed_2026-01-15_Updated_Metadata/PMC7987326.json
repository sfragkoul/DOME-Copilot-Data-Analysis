{
  "publication/title": "The Random Forest Model Has the Best Accuracy Among the Four Pressure Ulcer Prediction Models Using Machine Learning Algorithms.",
  "publication/authors": "Song J, Gao Y, Yin P, Li Y, Li Y, Zhang J, Su Q, Fu X, Pi H",
  "publication/journal": "Risk management and healthcare policy",
  "publication/year": "2021",
  "publication/pmid": "33776495",
  "publication/pmcid": "PMC7987326",
  "publication/doi": "10.2147/rmhp.s297838",
  "publication/tags": "- Pressure Ulcer Prediction\n- Machine Learning\n- Random Forest Model\n- Logistic Regression\n- K-Fold Cross-Validation\n- Model Performance\n- Feature Selection\n- Healthcare Data\n- Predictive Analytics\n- Medical Risk Assessment",
  "dataset/provenance": "The dataset used in this study was sourced from a hospital's digital medical record database. A total of 7356 cases were extracted, which included 1839 patients in the pressure ulcer group and 5517 patients in the non-pressure ulcer group. The data encompassed various features such as general demographic data, basic vital signs, medical care measures, disease-related data, nursing evaluation items, and types of drugs.\n\nThe dataset underwent rigorous cleaning and preprocessing. This involved eliminating cases with more than 10% missing data, filling in missing values using appropriate statistical methods, and ensuring data consistency by comparing extracted values with those in clinicians' electronic health records. The final dataset, after cleaning, consisted of 1673 patients with pressure ulcer events and 4141 patients without such events.\n\nThis dataset was then divided into a training set and a test set, each containing 50% of the total data points. The training set comprised 2883 data points, while the test set included 2931 data points. This division was crucial for evaluating the performance of the machine learning models developed in the study. The models were trained using the training set and validated using the test set to ensure their predictive accuracy and reliability.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a test set. The training set consisted of 2883 data points, while the test set contained 2931 data points. These splits were created by randomly selecting 50% of the patient data for training and the remaining 50% for testing.\n\nAdditionally, during the training stage, the training set was further divided using 10-fold cross-validation. This process involved splitting the training set into 10 parts through non-repetitive sampling. In each iteration of the cross-validation, 9 parts were used for model training, and the remaining 1 part was used for model verification. This process was repeated 10 times, resulting in 10 different models. The test set was then used to evaluate the performance of these models, and the average of the output results was taken as the final indicator.",
  "dataset/redundancy": "The dataset used in this study consisted of 7356 cases extracted from a digital medical record database. These cases were divided into two groups: 1839 patients in the pressure ulcer group (PU) and 5517 patients in the non-pressure ulcer group (No-PU). To ensure the accuracy of the extracted data, the values and timestamps were manually validated against the clinician\u2019s electronic health records, achieving 100% agreement.\n\nData cleaning was performed using Stata 13 software, which involved excluding cases without data on the day of pressure ulcer occurrence, repeated reported cases, unstructured data, non-related data, and cases with more than 10% missing data. This process resulted in the elimination of 1184 unqualified data points. Additionally, missing values were filled using specific methods, such as taking the average value of corresponding structural data on the day of the pressure ulcer event or using the average or mode of data from the previous three days. This step led to the elimination of 358 more cases and the repair of 6.3% of the data.\n\nThe final dataset, consisting of 1673 patients with pressure ulcer events and 4141 patients without, was mixed and merged. For machine learning algorithms using cross-validation, the data were split into training and testing sets. 50% of the patient data were randomly selected for training (n = 2883), and the remaining 50% were used for testing (n = 2931). This split ensured a uniform sampling in the factor analysis, maintaining the independence of the training and test sets.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets in terms of the thoroughness of data cleaning and the rigorous validation process. The use of cross-validation further ensures that the model's performance is robust and generalizable. The training and test sets are independent, enforced through random sampling and strict data cleaning protocols. This approach helps in mitigating overfitting and ensures that the model's predictions are reliable and accurate.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study include Random Forest (RF), Decision Tree (DT), Logistic Regression, and Artificial Neural Network (ANN). These are well-established algorithms in the field of machine learning and are not new. The Random Forest model, in particular, demonstrated superior performance in predicting pressure ulcers, with an average precision of 0.998. This model is composed of 500 decision trees, each randomly selecting 8 variables from a set of 40 to build the trees.\n\nThe choice of these algorithms was driven by their effectiveness in handling complex, multi-factor problems like pressure ulcer prediction. The study compared the performance of these models to find the most suitable one for the task. The Random Forest model was found to be the most efficient, particularly in processing classification problems.\n\nThe decision to use these established algorithms rather than proposing a new one was likely due to their proven track record in similar applications. Publishing in a healthcare-focused journal, rather than a machine-learning journal, aligns with the study's primary goal of improving clinical outcomes through better predictive models for pressure ulcers. The focus is on the application of machine learning in healthcare, rather than the development of new machine-learning techniques.",
  "optimization/meta": "The models developed in this research do not use data from other machine-learning algorithms as input. Instead, they are built using four different machine-learning methods: support vector machine (SVM), decision tree (DT), random forest (RF), and artificial neural network (ANN). Each of these models is trained independently using the same dataset, which includes various patient features and potential predictors related to pressure ulcers.\n\nThe decision tree-based model employs the C5.0 algorithm with a minimum number of leaf nodes to avoid excessive branching. The SVM model uses a Gaussian inner product as the kernel function. The ANN model consists of an input layer, an output layer, and a hidden layer. The random forest model, which achieved the best performance among the four, combines multiple decision trees to improve predictive accuracy and control overfitting.\n\nThe training process involves k-fold cross-validation, where the dataset is divided into 10 parts. Nine parts are used for training, and the remaining part is used for verification. This process is repeated 10 times to obtain 10 different models. The test set, consisting of 2931 samples, is used to evaluate the performance of these models. The final indicator is the average of the output results from the 10 models.\n\nThe performance of the models is evaluated using common indicators such as accuracy, recall, precision, F1 value, and the area under the ROC curve (AUC). The models are also calibrated, and their performance is assessed using histograms and reliability diagrams. The comparison of these models shows that the random forest model outperforms the others in terms of accuracy, recall, precision, and F1 value. This indicates that the random forest model has a superior ability to predict pressure ulcers compared to the other models.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, data was extracted from a digital medical record database, including a variety of features such as demographic data, vital signs, medical care measures, disease-related data, nursing evaluation items, and types of drugs. This extraction resulted in a dataset comprising 7356 cases, which were then divided into a pressure ulcer group and a non-pressure ulcer group.\n\nThe data preparation process included elimination of unqualified data, filling of missing values, and unification of data formats and units. Two professionals skilled in biomedical information data processing handled these tasks. Cases without data on the day of pressure ulcer occurrence, repeated reported cases, unstructured data, non-related data, and cases with more than 10% missing data were excluded. This resulted in the elimination of 1184 unqualified data points.\n\nFor missing value filling, the average value of corresponding structural data on the day of the pressure ulcer event was used. If data was missing on the reporting day, the average value of continuous data or the mode of non-continuous data from the previous three days was utilized. Cases without appropriate values were eliminated, leading to the removal of an additional 358 cases. Ultimately, 316 cases with missing values were filled, repairing approximately 6.3% of the data.\n\nThe dataset was then segmented into training and testing sets, each containing 50% of the data. This segmentation was performed to ensure uniform sampling in factor analysis. The training set consisted of 2883 cases, while the test set included 2931 cases.\n\nThe data was further processed to create a dataset with no missing values, and 19 important variables were identified through logistic regression. These variables were used to build the machine learning models, which included support vector machine (SVM), decision tree (DT), random forest (RF), and artificial neural network (ANN) models. The random forest model, in particular, was composed of 500 decision trees, with each tree randomly selecting 8 variables from a set of 40 variables.\n\nThe models were trained using k-fold cross-validation, where the training set was divided into 10 parts. Nine parts were used for training, and the remaining part was used for verification. This process was repeated 10 times to obtain 10 different models. The test set was then used to evaluate the performance of these models, and the average of the output results was taken as the final indicator. This approach ensured that the models were robust and capable of generalizing well to new data.",
  "optimization/parameters": "In our study, we initially considered 40 variables for model input. However, to balance model complexity and reliability, we employed a logistic regression model to select the most important variables. This process involved selecting variables with a P-value less than 0.05, resulting in a final set of 19 key predictor variables. These variables were chosen based on their statistical significance and correlation with pressure ulcer risk.\n\nThe selection of these 19 variables was crucial for enhancing the model's predictive accuracy. By focusing on these significant variables, we aimed to reduce overfitting and improve the model's generalizability. The chosen variables were then used as input parameters for the four different machine learning models: support vector machine (SVM), decision tree (DT), random forest (RF), and artificial neural network (ANN).\n\nFor the decision tree model, we used the C5.0 algorithm with a minimum number of leaf nodes to avoid excessive branching. The SVM model utilized a Gaussian kernel function with a gamma parameter set to 0.024. The ANN model consisted of an input layer, an output layer, and a hidden layer with 8 neurons. The random forest model was composed of 500 decision trees, each selecting 8 variables randomly from the 19 key predictors.\n\nThese input parameters were carefully tuned to optimize the performance of each model. The hyperparameters for each model were adjusted through a systematic process to achieve the best possible predictive accuracy. For instance, the number of hidden neurons in the ANN and the mtry parameter in the RF were fine-tuned to enhance model performance. The final settings for these hyperparameters are listed in Table 1, ensuring that each model was optimized for the task of predicting pressure ulcer risk.",
  "optimization/features": "In our study, we initially obtained 108 features from the hospital digital medical record database. These features encompassed a wide range of data types, including general demographic data, basic vital signs, medical care measures, disease-related data, nursing evaluation items, and various types of drugs.\n\nTo refine our model, we performed feature selection using a logistic regression model. This process was crucial for identifying the most relevant variables associated with the occurrence of pressure ulcers. Through this statistical analysis, we selected 19 important variables that exhibited significant statistical differences (P < 0.05). These selected features were then used as input for building our machine learning models.\n\nThe feature selection process was conducted using the training set only, ensuring that the test set remained independent and unbiased. This approach helped us achieve a balance between model complexity and reliability, ultimately enhancing the predictive performance of our models.",
  "optimization/fitting": "In our study, we employed k-fold cross-validation to train our models, specifically dividing the training set into 10 parts. Each fold involved training on 9 parts and validating on the remaining 1 part, a process repeated 10 times to obtain 10 different models. This approach helps to ensure that each data point gets to be in the training set and the validation set exactly once, providing a robust estimate of model performance and helping to mitigate overfitting.\n\nTo address the potential issue of overfitting, particularly in models like the random forest (RF) which can have a large number of parameters, we performed hyperparameter tuning. For the RF model, we set the number of trees to 500 and the number of variables randomly sampled as candidates at each split (mtry) to 8. These settings were chosen through careful tuning to balance model complexity and performance. Additionally, we evaluated model performance using a separate test set (n=2931), which was not used during the training process. This independent test set provided an unbiased evaluation of the models' generalization capabilities.\n\nUnderfitting was addressed by ensuring that our models were sufficiently complex to capture the underlying patterns in the data. For instance, the decision tree (DT) model used the C5.0 algorithm with pruning to avoid overly simplistic trees. The support vector machine (SVM) model utilized a Gaussian kernel, which is effective for capturing non-linear relationships. The artificial neural network (ANN) model included a single hidden layer with 8 neurons, chosen through hyperparameter tuning to achieve a good balance between model capacity and generalization.\n\nFurthermore, we compared the performance of our models using common metrics such as accuracy, recall, precision, F1 value, and the area under the ROC curve (AUC). These metrics provided a comprehensive evaluation of model performance, ensuring that our models were neither overfitting nor underfitting the data. The RF model, for example, achieved the highest performance metrics, indicating that it effectively captured the complexities of the data without overfitting.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our models. One of the primary methods used was k-fold cross-validation. This technique involved dividing the training set into 10 parts, using 9 parts for training and the remaining 1 part for validation in each iteration. This process was repeated 10 times, resulting in 10 different models. The final performance indicator was obtained by averaging the output results from these models. This approach helped to maximize the use of data and improve the model's generalization ability, especially given the moderate size of our dataset.\n\nAdditionally, hyperparameter tuning was performed for each model to optimize their performance. For instance, the decision tree model used the C5.0 algorithm with a minimum number of instances per leaf and pruning to control the complexity of the tree. The support vector machine model utilized a Gaussian kernel with a specific gamma parameter. The artificial neural network model had a single hidden layer with a set number of neurons, and the random forest model consisted of 500 decision trees, each considering a subset of variables.\n\nFurthermore, logistic regression was used to select important variables, ensuring that only statistically significant predictors were included in the models. This step helped to reduce the risk of overfitting by limiting the number of variables and focusing on those with a high correlation to the outcome.\n\nIn summary, k-fold cross-validation, hyperparameter tuning, and variable selection through logistic regression were key techniques used to prevent overfitting and enhance the predictive performance of our models.",
  "optimization/config": "The hyper-parameter configurations for each model are detailed and available. For instance, the Decision Tree (DT) model's minimum number of instances per leaf was set to 2, the Support Vector Machine (SVM) model's gamma was set to 0.02, the Artificial Neural Network (ANN) model's number of hidden neurons was set to 8, and the Random Forest (RF) model's mtry was set to 8 with 500 trees. These configurations were determined through a tuning process, and the final settings are listed in a table for transparency.\n\nThe optimization schedule involved using k-fold cross-validation, specifically 10-fold, to train the models. The training set was divided into 10 parts, with 9 parts used for training and 1 part for verification in each iteration. This process was repeated 10 times to obtain 10 different models, and the test set was used to evaluate the average performance of these models.\n\nModel files and optimization parameters are not explicitly mentioned as being available for download. However, the source code for the Random Forest model is illustrated in supplementary material, which provides insights into the implementation details. The performance evaluation metrics, such as accuracy, recall, precision, F1 value, and AUC, are thoroughly reported for each model, ensuring that the optimization process and results are reproducible.\n\nThe data used for training and testing, including the division into training and test sets, is described, but specific model files are not provided. The study emphasizes the importance of hyper-parameter settings and their impact on model performance, which is crucial for reproducibility. The use of standard software like SPSS and R for analysis further supports the transparency and reproducibility of the optimization process.\n\nNot applicable.",
  "model/interpretability": "The models employed in this study encompass a range of interpretability levels. The decision tree (DT) model, utilizing the C5.0 algorithm, is particularly transparent. This algorithm constructs a decision tree with a clear structure, where each node represents a decision based on a specific variable, and each branch represents the outcome of that decision. This makes it straightforward to trace the path from the root to the leaf nodes, understanding how different variables influence the final prediction. Pruning is applied to manage complexity, ensuring the tree remains interpretable without excessive branching.\n\nThe random forest (RF) model, while more complex, still offers some level of interpretability. Composed of 500 decision trees, each built using a random subset of variables, the RF model can provide insights into variable importance. By averaging the predictions of all trees, the model's overall decision-making process can be understood, although the individual trees themselves may not be as easily interpretable as a single decision tree.\n\nThe support vector machine (SVM) model, using a Gaussian kernel, is less transparent. The SVM operates by finding the optimal hyperplane that separates different classes in the feature space, which is not as intuitive to interpret. The model's decisions are based on complex transformations of the input data, making it more of a black-box model.\n\nThe artificial neural network (ANN) model, specifically the multi-layer perceptron (MLP) with a single hidden layer, is the least interpretable among the models used. ANNs learn patterns through layers of interconnected nodes, where the relationships between inputs and outputs are encoded in the weights of these connections. This makes it difficult to trace how specific variables influence the final prediction, as the model's decisions are distributed across many nodes and layers.\n\nIn summary, the DT model is the most transparent, providing clear decision paths and easily interpretable results. The RF model offers some interpretability through variable importance, while the SVM and ANN models are more black-box in nature, with the ANN being the least interpretable.",
  "model/output": "The model is a classification model. It is designed to predict the probability of a patient developing a pressure ulcer within 48 hours based on various risk factors. The output of the model provides a binary classification, indicating whether a patient is likely to develop a pressure ulcer or not. This is evident from the use of performance metrics such as accuracy, recall, precision, F1 value, and the ROC curve area (AUC), which are commonly used to evaluate classification models. The model's performance is assessed by comparing its predictions to actual diagnosis results, further confirming its classification nature. Additionally, the model's output is calibrated to improve its predictive performance, which is a typical step in refining classification models.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the random forest model is available as supplementary material. This material illustrates the source code used to build the random forest, which consists of 500 decision trees. Each decision tree randomly selects 8 variables from a set of 40 variables to construct the model. However, no specific details are provided about the availability of the source code for the other models (SVM, DT, ANN) or any executable, web server, virtual machine, or container instance for running the algorithms. Not sure if the source code is publicly available or under what license it might be released.",
  "evaluation/method": "The evaluation method employed for the models involved a rigorous process to ensure robustness and accuracy. K-fold cross-validation was utilized, specifically with 10 folds. This technique involved dividing the training set into 10 parts through non-repetitive sampling. In each iteration, 9 parts were used for training the model, while the remaining part was reserved for verification. This process was repeated 10 times, resulting in 10 different models. The test set, consisting of 2931 samples, was then used to evaluate these models, and the average of the output results was taken as the final indicator.\n\nAdditionally, 50% of the test set was used to further assess the model's performance. The predictions made by the models were compared against the actual diagnosis results to determine the final outcome. The performance of the models was significantly influenced by the hyperparameter settings, such as the minimum number of instances per leaf for Decision Trees, gamma for Support Vector Machines, the number of hidden neurons for Artificial Neural Networks, and mtry and the number of trees for Random Forests.\n\nThe evaluation metrics used included accuracy, recall, precision, F1 value, and the area under the ROC curve (AUC). These metrics were calculated using a confusion matrix and compared across different models. Model calibration was also performed, and the performance after calibration was evaluated using histograms and reliability diagrams. The statistical software SPSS V.22.0 was used for descriptive statistics, while all model analyses were conducted using R language (version 2.9.0 for Windows).",
  "evaluation/measure": "In our study, we employed several key performance metrics to evaluate the effectiveness of our prediction models. These metrics include accuracy, recall, precision, F1 value, and the area under the curve (AUC) for the receiver operating characteristic (ROC) curve. Additionally, we considered the area under the precision-recall curve (AUPRC).\n\nAccuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. Recall, also known as sensitivity, indicates the ability of the model to identify positive cases correctly. Precision, on the other hand, reflects the proportion of true positive results among all positive results predicted by the model. The F1 value is the harmonic mean of precision and recall, providing a single metric that balances both concerns.\n\nThe AUC of the ROC curve is a widely used metric that summarizes the model's ability to discriminate between positive and negative classes across all possible classification thresholds. A higher AUC indicates better model performance. Similarly, the AUPRC is useful for evaluating models, especially when dealing with imbalanced datasets, as it focuses on the performance of the model in terms of precision and recall.\n\nThese metrics are commonly used in the literature for evaluating machine learning models, particularly in the context of medical and healthcare applications. They provide a comprehensive view of the model's performance, covering aspects such as correctness, sensitivity, and specificity. By reporting these metrics, we aim to ensure that our evaluation is thorough and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we compared the performance of four different machine learning models: Support Vector Machine (SVM), Decision Tree (DT), Random Forest (RF), and Artificial Neural Network (ANN). Each model was evaluated using a test set consisting of 2931 samples. The performance metrics used for comparison included accuracy, recall, precision, F1 value, and the area under the ROC curve (AUC).\n\nThe SVM model demonstrated an accuracy of 94.94%, with a recall of 93.90%, precision of 96.90%, and an F1 value of 94.42%. The DT model outperformed the SVM model across all metrics, notably achieving a higher F1 value by 3.57%. The RF model exhibited the best performance overall, with an accuracy, recall, and F1 value all at 99.88%, and a precision of 99.93%. However, the exceptionally high values for the RF model may indicate overfitting.\n\nThe ANN model had the lowest performance among the four, with an accuracy of 79.02%, recall of 87.21%, precision of 90.89%, and an F1 value. This comparison highlights the strengths and weaknesses of each model in predicting the risk of pressure ulcers.\n\nAdditionally, we compared these machine learning models to the conventional Norton scale, which is widely used for assessing the risk of pressure ulcers. The Norton scale had an accuracy of 47.14%, recall of 33.95%, precision of 55.89%, and an F1 value of 39.48%. The machine learning models consistently outperformed the Norton scale, demonstrating higher predictive accuracy and diagnostic value. This suggests that the multivariate nature of machine learning models and their training processes contribute to their superior performance.\n\nIn summary, our evaluation involved a thorough comparison of different machine learning models and a widely used clinical scale, providing insights into the effectiveness of each method in predicting pressure ulcer risk.",
  "evaluation/confidence": "The evaluation of our models included several performance metrics such as accuracy, recall, precision, F1 value, and AUC. However, specific confidence intervals for these metrics were not explicitly provided in our results. The performance of each model was assessed using a k-fold cross-validation approach, which helps to ensure the robustness of the results by training and validating the models multiple times on different subsets of the data.\n\nThe statistical significance of our models' performance was not directly addressed in terms of p-values or confidence intervals for the metrics themselves. However, the use of k-fold cross-validation and the consistent performance across different folds suggest that the results are reliable. Additionally, the comparison of our machine learning models with the Norton scale showed that all four models outperformed the traditional scale, indicating a superior predictive performance.\n\nThe results demonstrated that the Random Forest (RF) model achieved the highest performance metrics, suggesting it may be the most reliable among the models tested. The other models, including Support Vector Machine (SVM), Decision Tree (DT), and Artificial Neural Network (ANN), also showed improved performance compared to the Norton scale, but to varying degrees. The ANN model, in particular, had the lowest performance metrics, which might indicate less reliability compared to the other models.\n\nIn summary, while explicit confidence intervals for the performance metrics were not provided, the use of cross-validation and the consistent superiority of the machine learning models over the Norton scale suggest that the results are statistically significant and reliable. The RF model stands out as the most robust and accurate predictor among the models evaluated.",
  "evaluation/availability": "Not applicable."
}