{
  "publication/title": "Interpretable machine learning model for predicting clinically significant prostate cancer: integrating intratumoral and peritumoral radiomics with clinical and metabolic features.",
  "publication/authors": "Zhao W, Hou M, Wang J, Song D, Niu Y",
  "publication/journal": "BMC medical imaging",
  "publication/year": "2024",
  "publication/pmid": "39736623",
  "publication/pmcid": "PMC11684284",
  "publication/doi": "10.1186/s12880-024-01548-2",
  "publication/tags": "- Radiomics\n- Machine Learning\n- Prostate Cancer\n- Medical Imaging\n- XGBoost\n- Prediction Models\n- Clinical Variables\n- MRI Findings\n- Feature Selection\n- Model Validation\n- Statistical Analysis\n- ROC Curves\n- AUC\n- Sensitivity\n- Specificity\n- SHAP Analysis\n- Temporal Validation\n- Intra-radiomics\n- Peri-radiomics\n- MRS Model",
  "dataset/provenance": "The dataset used in this study was sourced from patients who underwent MRI and MRS evaluations for suspicious prostate lesions at our institution. The initial pool consisted of 658 patients reviewed retrospectively from April 2020 to January 2024. An additional set of 256 patients collected between January 2024 and November 2024 was used as a temporal validation set. However, only 350 patients met the inclusion criteria and were included in the final analysis.\n\nThe inclusion criteria for the study were as follows: patients aged 18 years or older, those who underwent MRI including at least T2-weighted imaging (T2WI), diffusion-weighted imaging (DWI), and magnetic resonance spectroscopy (MRS), and those with available biopsy pathology outcomes after MRI. Exclusion criteria included patients who did not undergo prostate biopsy, those with incomplete clinicopathological data, patients who had biopsy or treatment performed prior to MRI, cases with poor image quality due to artifacts or incomplete examination, and those with inconsistent scanners.\n\nThe final dataset of 350 patients was divided into a training set and a testing set at a 7:3 ratio, resulting in 191 patients in the training set and 83 patients in the testing set. An additional temporal validation set of 76 patients was also used. The detailed clinical characteristics and MRI findings of these sets are presented in the supplementary tables.\n\nThis dataset has not been used in previous publications by our group or by the community. The data collection and analysis were specifically conducted for this study to develop and validate a machine learning model integrating intratumoral and peritumoral radiomic, clinicoradiological, and metabolic information to predict clinically significant prostate cancer (csPCa).",
  "dataset/splits": "The dataset consists of three splits: a training set, a testing set, and a temporal validation set. The initial cohort of 350 patients was randomly divided into a training set containing 191 patients and a testing set with 83 patients, maintaining a 7:3 ratio. An additional temporal validation set was created using 76 patients collected between January 2024 and November 2024. This temporal validation set reflects distributional differences of patients over time. No significant differences were observed between the training and testing sets, ensuring a balanced distribution for model training and evaluation.",
  "dataset/redundancy": "The dataset consisted of 350 patients, with a median age of 72. The patients were diagnosed with either clinically significant prostate cancer (csPCa) or non-csPCa, including benign lesions and those with a Gleason score of 3 + 3 or lower. The dataset was split into a training set and a testing set at a 7:3 ratio, resulting in 191 patients in the training set and 83 in the testing set. Additionally, a temporal validation set of 76 patients was collected to reflect distributional differences over time.\n\nThe training and testing sets were designed to be independent. No significant differences were observed between these two sets, ensuring that the model's performance could be reliably evaluated on unseen data. The temporal validation set, however, showed some differences from the training set, which was expected due to the temporal gap and changes in patient characteristics over time.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in prostate cancer research. The use of a temporal validation set is a strength, as it provides an additional layer of validation that accounts for real-world variations over time. This approach enhances the generalizability of the findings and ensures that the model performs well across different time periods and patient populations.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the eXtreme Gradient Boosting (XGBoost) algorithm. This is a well-established and widely used machine-learning algorithm known for its efficiency and effectiveness in handling structured/tabular data.\n\nThe XGBoost algorithm is not new; it has been extensively used and validated in various fields, including medical imaging and radiomics. The reason it was not published in a machine-learning journal is that our focus was on applying this algorithm to a specific medical problem\u2014predicting the risk of clinically significant prostate cancer (csPCa)\u2014rather than developing a new machine-learning algorithm.\n\nOur study leveraged the XGBoost classifier from the XGBoost library, which is a popular and robust implementation of the gradient boosting framework. We conducted a grid search with 5-fold cross-validation to identify the optimal hyperparameters, ensuring that our model was well-tuned for the task at hand. The hyperparameters that were tuned included max_depth, n_estimators, learning_rate, subsample, colsample_bytree, alpha, and lambda. This approach allowed us to optimize the model's performance and achieve reliable predictions.",
  "optimization/meta": "The model described in this publication does indeed use data from other machine-learning algorithms as input, making it a type of meta-predictor. The combined model integrates various types of information, including clinical data, magnetic resonance spectroscopy (MRS) data, and radiomic features from both intra-tumoral and peritumoral regions.\n\nThe machine-learning methods that constitute the whole include:\n\n* Clinical model: This model incorporates clinical variables such as age, PSA, PSAD, smoking status, location, and PI-RADS scores.\n* MRS model: This model integrates metabolic parameters like Cr, Cho, Cit, and the ratio (Cho + Cr)/Cit.\n* Intra-radiomics model: This model is developed using the intra-rad-score, which is derived from radiomic features within the tumor.\n* Peri-radiomics model: This model is developed using the peri-rad-score, which is derived from radiomic features in the region surrounding the tumor.\n\nAll these models are constructed using the eXtreme Gradient Boosting (XGBoost) machine-learning algorithm. The combined model then integrates the outputs or scores from these individual models to make a final prediction.\n\nRegarding the independence of the training data, it is clear that the data used for training the individual models (clinical, MRS, intra-radiomics, and peri-radiomics) is independent. The models are trained on a training set and validated on separate testing and validation sets, ensuring that the data used for training is not contaminated by the data used for validation. This independence is crucial for the reliable performance and generalization of the meta-predictor.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithms. We began by extracting a comprehensive set of radiomic features from medical images, specifically from T2-weighted imaging (T2WI) and apparent diffusion coefficient (ADC) maps. A total of 1,702 radiomic features were derived from the volumes of interest (VOIs) within and around the tumors.\n\nTo ensure the reliability of these features, we first removed those with an intraclass correlation coefficient (ICC) less than 0.8, indicating poor reproducibility. Subsequently, we applied univariate statistical tests, such as the Student\u2019s t-test or Mann-Whitney U test, to filter out features that did not show significant differences between the groups being compared. This step helped in retaining only the most relevant features for further analysis.\n\nNext, we employed Pearson correlation coefficient (PCC) analysis to identify and retain low-correlation features, reducing redundancy in the dataset. This process ensured that the selected features were independent and provided unique information.\n\nFor feature selection, we utilized a recursive feature elimination (RFE) algorithm based on logistic regression. This algorithm iteratively constructed models and removed the least significant features, ultimately identifying the optimal subset of features that maximized model accuracy. Through 5-fold cross-validation, we determined that the best models for intratumoral and peritumoral radiomics contained 10 and 16 features, respectively.\n\nIn addition to radiomic features, we included clinical variables such as age, PSA levels, PSAD, smoking status, location, and PI-RADS scores, as well as metabolic features like Cr, Cho, Cit, and their ratios. These variables were selected based on their statistical significance in univariate analyses.\n\nBefore feeding the data into the XGBoost machine-learning algorithm, we conducted a grid search with 5-fold cross-validation to tune the hyperparameters, including max_depth, n_estimators, learning_rate, subsample, colsample_bytree, alpha, and lambda. This optimization process ensured that our models were robust and generalizable.\n\nOverall, our data encoding and preprocessing pipeline involved rigorous feature selection and optimization steps to enhance the performance and interpretability of our machine-learning models.",
  "optimization/parameters": "In the optimization process of our models, we employed a recursive feature elimination (RFE) algorithm based on logistic regression for feature selection. This algorithm was applied to both the training and testing sets. To determine the optimal number of features, we performed five-fold cross-validation and selected the feature subset that yielded the highest average model accuracy in the validation set.\n\nFor the training set, the best model was built with 10 features. In the testing set, the optimal model included 16 features. These feature subsets were identified based on the mean and standard deviation of the model\u2019s cross-validation accuracy for each number of features selected.\n\nThe selection of these parameters was crucial in ensuring that our models were both efficient and accurate. By using cross-validation, we were able to mitigate overfitting and ensure that the selected features were generalizable to new, unseen data. The final models incorporated these optimal feature subsets, which were determined through a rigorous and systematic process.",
  "optimization/features": "In the optimization process, a total of 1,702 radiomic features were initially extracted from T2-weighted imaging (T2WI) and apparent diffusion coefficient (ADC) maps. These features were derived from both intratumoral and peritumoral regions of interest (VOI intra and VOI peri).\n\nFeature selection was performed to identify the most relevant features for model construction. This process involved several steps:\n\n1. **Consistency Evaluation**: Features with an intraclass correlation coefficient (ICC) less than 0.8 were removed to ensure consistency between readers.\n2. **Univariate Analysis**: Features were further filtered using Student\u2019s t-test or Mann-Whitney U test, depending on the data distribution, to retain only those with a significance level of p < 0.05.\n3. **Correlation Analysis**: Pearson correlation coefficient (PCC) analysis was conducted to eliminate collinearity among features. Pairs of features with a correlation coefficient |r| > 0.80 were examined, and one feature from each pair was randomly removed.\n4. **Recursive Feature Elimination (RFE)**: A logistic regression-based RFE algorithm was employed to iteratively train the model and eliminate the least important features. This process was repeated to construct 19 RFE models, each selecting between 1 and 19 features. The optimal feature subset was determined through 5-fold cross-validation, based on the average accuracy of the validation set.\n\nThe final models used different numbers of features:\n\n* The intratumoral radiomics model (intra-rad-score) was built with 10 features.\n* The peritumoral radiomics model (peri-rad-score) was constructed with 16 features.\n\nFeature selection was conducted using only the training set to ensure that the testing and validation sets remained independent and unbiased. This approach helped in identifying the most relevant features that contributed significantly to the model's performance.",
  "optimization/fitting": "In our study, we constructed multiple prediction models using various features and machine learning algorithms. The number of parameters in our models was indeed larger than the number of training points, particularly in the initial stages of feature selection. To address potential overfitting, we employed several strategies.\n\nFirstly, we used a recursive feature elimination (RFE) algorithm based on logistic regression to select the optimal feature subset. This process involved performing five-fold cross-validation and selecting features based on the average model accuracy in the validation set. This approach helped in identifying the most relevant features and reducing the risk of overfitting.\n\nSecondly, we conducted a grid search with five-fold cross-validation to identify the optimal hyperparameters for our XGBoost models. This method ensured that the models were not overly complex and generalised well to unseen data.\n\nTo rule out underfitting, we evaluated the performance of our models using multiple metrics, including the area under the ROC curve (AUC), accuracy, F1 score, sensitivity, and specificity. The models were validated on both the testing set and a temporal validation set, ensuring that they performed well on different datasets.\n\nAdditionally, we used SHAP analysis to interpret and visualize the prediction process of the combined model. This helped in understanding the contribution of each feature to the model's predictions and ensured that the models were not too simplistic.\n\nIn summary, we addressed the risk of overfitting through feature selection and hyperparameter tuning, while ensuring that the models were not underfitting by evaluating their performance on multiple datasets and using various evaluation metrics.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method used was recursive feature elimination (RFE) with logistic regression. This technique helped us to select the optimal subset of features by iteratively removing the least significant features and building models to identify the best-performing feature set. We performed five-fold cross-validation during this process to evaluate the model's performance and prevent overfitting to the training data.\n\nAdditionally, we conducted a grid search with five-fold cross-validation to identify the optimal hyperparameters for our XGBoost models. This process involved tuning several hyperparameters, including max_depth, n_estimators, learning_rate, subsample, colsample_bytree, alpha, and lambda. By systematically exploring different combinations of these parameters, we aimed to find the configuration that generalized best to unseen data, thereby reducing the risk of overfitting.\n\nFurthermore, we performed univariate analysis on all candidate variables before constructing the XGBoost models. Only variables that were statistically significant in this analysis were included in the model construction process. This step helped to ensure that only relevant features were considered, further mitigating the risk of overfitting.\n\nIn summary, our approach to preventing overfitting involved feature selection using RFE, hyperparameter tuning through grid search with cross-validation, and univariate analysis to filter out irrelevant variables. These techniques collectively contributed to the development of robust and generalizable prediction models.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available and have been detailed within the publication. Specifically, we conducted a grid search with 5-fold cross-validation to identify the optimal hyperparameters for our XGBoost models. The hyperparameters that were tuned include max_depth, n_estimators, learning_rate, subsample, colsample_bytree, alpha, and lambda. These details are provided to ensure reproducibility of our results.\n\nRegarding model files and optimization parameters, the specific files and parameters used in our study are not directly available for download. However, the methods and configurations described in the paper are comprehensive enough for other researchers to replicate the models and optimization processes. The software used for statistical analyses, such as SPSS version 23.0 and Python 3.10.4, are widely accessible, and the algorithms employed, like XGBoost and SHAP analysis, are standard and well-documented in the machine learning community.\n\nThe publication itself serves as the primary resource for accessing the configurations and optimization details. While the exact model files are not provided, the thorough documentation of our methods and parameters ensures that the study can be replicated by other researchers interested in similar predictive modeling approaches.",
  "model/interpretability": "The model developed in this study is not a black box. To ensure transparency and interpretability, SHapley Additive exPlanations (SHAP) analysis was employed. SHAP analysis provides a unified approach to explain the outputs of various machine learning models, allowing for feature importance assessment and offering both local and global interpretations.\n\nIn the global visualization, the SHAP bar plot illustrates the ranking of feature importance in the model. Key features such as the intratumoral radiomic score, prostate-specific antigen density, peritumoral radiomic score, Prostate Imaging Reporting and Data System score, prostate-specific antigen, and citrate demonstrated high importance. The SHAP beeswarm plot offers an information-dense summary, showing the impact of top features on the model\u2019s output. Each dot represents an individual sample, with its position on the x-axis determined by the SHAP value of the corresponding feature. The color of the dots indicates the magnitude of the feature value, transitioning from blue (low values) to red (high values). For instance, high values of the intratumoral radiomic score, prostate-specific antigen density, peritumoral radiomic score, Prostate Imaging Reporting and Data System score, and prostate-specific antigen positively influence the prediction probability, while high citrate values negatively impact the prediction result.\n\nIn the individual visualization, the SHAP force plot shows how the model arrives at its decision in a specific single case. The main features contributing to the prediction of a specific sample, along with their SHAP values, are displayed as bar-shaped arrows. Positive values (red arrows) indicate positive contributions (an increased risk of clinically significant prostate cancer), whereas negative values (blue arrows) denote negative contributions. For example, in a correctly predicted clinically significant prostate cancer case, the Prostate Imaging Reporting and Data System score, citrate level, peritumoral radiomic score, and intratumoral radiomic score increase the prediction probability, while prostate-specific antigen density and prostate-specific antigen have the opposite effect. This detailed visualization helps in understanding the model's decision-making process for individual cases, enhancing its transparency and clinical applicability.",
  "model/output": "The model constructed in our study is a classification model. We utilized eXtreme Gradient Boosting (XGBoost) machine-learning algorithms to build five different models: an intra-radiomics model, a peri-radiomics model, a clinical model, an MRS model, and a combined model. These models were designed to predict outcomes based on various features, including radiomics scores, clinical variables, and MRS data.\n\nThe performance of these models was evaluated using several metrics, including the area under the receiver operating characteristic curve (AUC), accuracy, F1 score, sensitivity, and specificity. The combined model, which integrates clinical, MRS, and radiomics scores, demonstrated the highest performance across these metrics in both the training and testing sets. For instance, in the training set, the combined model achieved a perfect AUC of 1.000, indicating excellent discriminative ability. In the testing set, the combined model showed a significantly superior AUC of 0.968 compared to the clinical and MRS models, but performed similarly to the intra-radiomics and peri-radiomics models.\n\nTo ensure the robustness of our models, we conducted a grid search with 5-fold cross-validation to identify the optimal hyperparameters. This process involved tuning parameters such as max_depth, n_estimators, learning_rate, subsample, colsample_bytree, alpha, and lambda. Additionally, we employed SHAP (SHapley Additive exPlanations) analysis to interpret and visualize the prediction process of the combined model. SHAP values helped us understand the contribution of each feature to the model's output, with larger SHAP values indicating a greater positive influence on the prediction outcome.\n\nIn summary, our classification models, particularly the combined model, showed strong performance in predicting the outcomes of interest. The use of advanced machine-learning techniques and thorough validation processes ensures the reliability and interpretability of our results.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our study, we employed a rigorous evaluation method to assess the performance of our prediction models. We constructed five different models using the training set and validated them on both the testing and validation sets. These models included an intra-radiomics model, a peri-radiomics model, a clinical model, an MRS model, and a combined model that integrated clinical, MRS, and radiomics scores.\n\nTo ensure the robustness of our models, we utilized a recursive feature elimination (RFE) algorithm based on logistic regression for feature selection. This process involved performing five-fold cross-validation to select the optimal feature subset based on the average model accuracy in the validation set. For each number of features selected, we plotted the mean and standard deviation of the model\u2019s cross-validation accuracy. The best models for the training and testing sets were built with 10 and 16 features, respectively.\n\nFor model construction, we used the eXtreme Gradient Boosting (XGBoost) machine-learning algorithm. Before constructing the XGBoost models, we conducted univariate analysis on all candidate variables, including only those that were statistically significant in the model construction process. We trained the XGBoost classifier using the training dataset and identified the optimal hyperparameters through a grid search with five-fold cross-validation. The hyperparameters tuned included max_depth, n_estimators, learning_rate, subsample, colsample_bytree, alpha, and lambda.\n\nModel performance was evaluated using receiver operating characteristic (ROC) curves, with metrics such as the area under the ROC curve (AUC), accuracy, F1 score, sensitivity, and specificity. Delong\u2019s test was employed to statistically compare the AUCs of the models. Additionally, SHAP analysis was used to interpret and visualize the prediction process of the combined model.\n\nStatistical analyses were performed using SPSS software version 23.0 and Python 3.10.4. Differences in continuous variables between the training and testing sets, as well as between the training and validation sets, were assessed using the Student\u2019s t-test or the Mann\u2013Whitney U test, depending on the results of the normality test. The chi-square test was used to evaluate differences in categorical variables. A two-sided p-value less than 0.05 was considered statistically significant in all analyses. P-values were adjusted for multiple comparisons using the Bonferroni correction.",
  "evaluation/measure": "In the evaluation of our models, we employed a comprehensive set of performance metrics to ensure a thorough assessment. These metrics include the area under the receiver operating characteristic curve (AUC), accuracy, F1 score, sensitivity, and specificity. The AUC provides a measure of the model's ability to distinguish between classes, while accuracy reflects the proportion of true results (both true positives and true negatives) among the total number of cases examined. The F1 score is the harmonic mean of precision and recall, offering a balance between these two metrics. Sensitivity, also known as recall or true positive rate, indicates the proportion of actual positives that are correctly identified by the model. Specificity, or the true negative rate, measures the proportion of actual negatives that are correctly identified.\n\nThese metrics were evaluated across three different datasets: the training set, the testing set, and the validation set. This approach allows us to assess the model's performance not only on the data it was trained on but also on unseen data, providing a more robust evaluation of its generalizability.\n\nThe use of these metrics is representative of standard practices in the literature, ensuring that our evaluation is comparable to other studies in the field. Additionally, we employed Delong\u2019s test to statistically compare the AUCs of the models, further enhancing the rigor of our evaluation. This set of metrics provides a well-rounded view of model performance, covering aspects of discrimination, accuracy, and balance between precision and recall.",
  "evaluation/comparison": "In our study, we constructed and compared multiple prediction models to evaluate their performance in distinguishing between clinically significant prostate cancer (csPCa) and non-csPCa. We developed five distinct models: a clinical model, an MRS model, an intra-radiomics model, a peri-radiomics model, and a combined model.\n\nThe clinical model incorporated traditional clinical variables such as age, PSA, PSAD, smoking status, location, and PI-RADS scores. The MRS model integrated metabolic features like Cr, Cho, Cit, and the ratio (Cho + Cr)/Cit. The intra-radiomics and peri-radiomics models were built using radiomic features extracted from intratumoral and peritumoral regions, respectively. These features were selected through a rigorous process involving intraclass correlation coefficient (ICC) analysis, univariate statistical tests, and recursive feature elimination (RFE) with logistic regression (LR).\n\nThe combined model integrated clinical, radiological, metabolic, and radiomic features, aiming to leverage the strengths of each modality. This model was constructed using the eXtreme Gradient Boosting (XGBoost) machine-learning algorithm, which is known for its robustness and efficiency in handling complex datasets.\n\nTo ensure the reliability and generalizability of our models, we conducted a comprehensive evaluation using receiver operating characteristic (ROC) curves and calculated the area under the ROC curve (AUC), accuracy, F1 score, sensitivity, and specificity. We performed this evaluation on three distinct datasets: the training set, the testing set, and a temporal validation set. The temporal validation set included patients collected over a different time period, providing an additional layer of validation to assess the models' performance over time.\n\nThe combined model demonstrated superior performance across all evaluation metrics in the testing and validation sets, indicating its potential for clinical utility. The intra-radiomics and peri-radiomics models also showed competitive performance, highlighting the value of radiomic features in cancer prediction.\n\nIn summary, our study involved a thorough comparison of various prediction models, including simpler baselines and more complex integrated models. This approach allowed us to identify the most effective model for predicting csPCa, providing valuable insights for future clinical applications.",
  "evaluation/confidence": "The evaluation of our models included several performance metrics, each accompanied by confidence intervals to provide a range within which the true value is likely to fall. These metrics included the area under the receiver operating characteristic curve (AUC), accuracy, F1 score, sensitivity, and specificity. The AUC, in particular, is a critical metric for assessing the discriminative power of our models, and its confidence intervals offer insights into the reliability of these estimates.\n\nStatistical significance was a key consideration in our analysis. We employed Delong\u2019s test to compare the AUCs of different models, ensuring that any claimed superiority was backed by robust statistical evidence. Additionally, we adjusted p-values for multiple comparisons using the Bonferroni correction to maintain the overall significance level. This rigorous approach helps to mitigate the risk of Type I errors, providing greater confidence in our findings.\n\nIn the training set, the combined model demonstrated a significantly higher AUC compared to several other models, indicating its superior performance. Similarly, in the testing and validation sets, the combined model showed statistically significant improvements in AUC and other metrics compared to some baseline models. These results suggest that the combined model is not only more accurate but also more reliable in predicting outcomes.\n\nOverall, the inclusion of confidence intervals and the use of statistical tests to assess significance enhance the credibility of our evaluation. This thorough approach ensures that our conclusions about the model's performance are well-supported and can be trusted for practical applications.",
  "evaluation/availability": "Not enough information is available."
}