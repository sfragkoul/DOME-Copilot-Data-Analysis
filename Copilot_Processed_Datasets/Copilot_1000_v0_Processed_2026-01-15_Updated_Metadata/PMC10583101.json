{
  "publication/title": "Performance of predictive algorithms in estimating the risk of being a zero-dose child in India, Mali and Nigeria.",
  "publication/authors": "Biswas A, Tucker J, Bauhoff S",
  "publication/journal": "BMJ global health",
  "publication/year": "2023",
  "publication/pmid": "37821114",
  "publication/pmcid": "PMC10583101",
  "publication/doi": "10.1136/bmjgh-2023-012836",
  "publication/tags": "- Machine Learning\n- Zero-Dose Children\n- Vaccination Coverage\n- Predictive Modeling\n- Class Imbalance\n- Specificity\n- Sensitivity\n- Geographical Targeting\n- Public Health\n- Cost-Sensitive Models\n- Feature Importance\n- Algorithm Performance\n- Data Retraining\n- Immunization Campaigns\n- Health System Proxies",
  "dataset/provenance": "The dataset used in this study is derived from Demographic and Health Surveys (DHS) conducted in various countries and years. Specifically, data from India (2006, 2015, and 2020), Mali (2006 and 2018), and Nigeria (2008 and 2018) were utilized. These surveys provide comprehensive information on health and demographic indicators, including vaccination status.\n\nThe dataset includes a substantial number of data points. For instance, the India 2020 dataset contains around 40,555 data points, while the India 2015 dataset includes approximately 46,209 data points. Similarly, the Mali 2018 dataset has about 1,803 data points, and the Nigeria 2018 dataset consists of around 2,350 data points. These data points represent individual children, with each data point containing various predictors such as place of residence, facility delivery information, and antenatal care frequency.\n\nThe data used in this study have been preprocessed to ensure meaningful predictions. Categorical features were one-hot encoded, and children with missing information for any predictor were removed. This preprocessing step resulted in the exclusion of a small percentage of data points from each dataset. For example, around 265 children were excluded from the India 2020 dataset due to missing values.\n\nThe dataset includes three sets of predictors. The first set consists of basic administrative data such as place of residence and facility delivery information. The second set adds contextual information like population density and urbanicity. The third set further includes survey information such as the mother\u2019s education and household wealth quintile. These predictors have been previously associated with vaccine uptake and are crucial for identifying zero-dose (ZD) children.\n\nThe data have been used to train and test machine learning models, specifically the cost-sensitive Ridge Classification (RC) algorithm. The performance of these models has been evaluated using metrics such as sensitivity, specificity, and F1 score. The dataset has also been used to compare the performance of models trained on older data when applied to newer data, highlighting the impact of temporal changes on model performance.\n\nThe dataset has not been used in previous papers by the community, but it builds upon well-established demographic and health survey data that are widely recognized and utilized in public health research. The use of DHS data ensures that the findings are robust and generalizable to similar populations and settings.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a test set. The training set comprised 70% of the overall dataset, while the remaining 30% constituted the test set. This split was designed to maintain regional stratification, ensuring that the distribution of data points across different states or regions was proportional to the total number of children in each stratum.\n\nThe training set was used to develop the predictive models, while the test set was employed to evaluate the models' performance on unseen data points. This approach helped in assessing how well the models generalize to new, unobserved data.\n\nThe specific number of data points in each split varied depending on the dataset from different countries and years. For instance, in the India 2020 dataset, approximately 28,389 children were used for training, and around 12,166 children were used for testing after removing samples with missing values. Similar proportions were maintained for datasets from other countries and years, ensuring a representative distribution of data points in both the training and test sets.",
  "dataset/redundancy": "After creating the training dataset, the remaining samples were used for testing how the models perform on completely unseen data points. This ensures that the training and test sets are independent. The independence of the datasets was enforced by using a portion of the data for training the models and reserving the rest for testing their performance. This approach helps in evaluating the models' ability to generalize to new, unseen data.\n\nThe distribution of the datasets used in this study is comparable to previously published machine learning datasets in the context of public health and vaccination coverage. The datasets are characterized by a high class imbalance, with a relatively small fraction of zero-dose (ZD) children. This imbalance is a common challenge in real-world datasets, particularly in public health applications where the target class (e.g., ZD children) is often underrepresented. The datasets include various predictors such as administrative data, contextual information, and additional survey-based information, which are typical in studies aiming to predict health outcomes. The use of these diverse predictors helps in capturing different aspects of the data, thereby improving the models' predictive performance.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the Ridge Classifier (RC). This is a well-established algorithm in the field of machine learning, known for its ability to handle high-dimensional data and mitigate overfitting through regularization.\n\nThe Ridge Classifier is not a new algorithm. It has been extensively studied and used in various applications, including classification tasks with imbalanced datasets. The reason it was not published in a machine-learning journal is that our focus was on applying this algorithm to a specific public health problem\u2014identifying zero-dose (ZD) children\u2014rather than developing a new machine-learning technique. Our work contributes to the field by demonstrating the effectiveness of the Ridge Classifier in addressing real-world challenges, particularly in the context of class imbalance and the need for high specificity in identifying at-risk children.\n\nWe employed a cost-sensitive version of the Ridge Classifier to address the class imbalance issue, where ZD children are significantly fewer than non-ZD children. This approach allows us to penalize the misclassification of ZD children as non-ZD more heavily, thereby improving the model's specificity. The algorithm's performance was evaluated using metrics such as specificity, sensitivity, and the F1 Score, which are crucial for understanding the model's effectiveness in imbalanced datasets.",
  "optimization/meta": "The models used in this study do not employ a meta-predictor approach. Instead, they utilize classical supervised learning algorithms directly on the dataset. The specific algorithms employed include Ridge Classifier, Decision Tree, Nearest Neighbor, and Multi-Layer Perceptron. Each of these algorithms is trained independently on the dataset, and their performance is evaluated based on various metrics such as specificity, sensitivity, and F1 Score.\n\nThe training data for these models is derived from administrative, contextual, and additional predictors related to children's health and demographic information. The datasets used for training and testing are clearly defined and independent of each other. For instance, models trained on older datasets (e.g., Mali 2006, Nigeria 2008) are tested on newer datasets (e.g., Mali 2018, Nigeria 2018) to assess their predictive power over time. This approach ensures that the training data is independent and that the models are evaluated on completely unseen data points.\n\nThe performance of these models is assessed across different feature sets, including sparse predictors, contextual predictors, and additional predictors. The results indicate that the Ridge Classifier generally performs well across various datasets and feature sets, particularly in terms of specificity. The study also explores the impact of class imbalance and the importance of different features in predicting the outcome variable, which is whether a child is at risk of being zero-dose (ZD).\n\nIn summary, the models used in this study do not rely on a meta-predictor approach but rather on individual supervised learning algorithms trained on independent datasets. The performance of these models is thoroughly evaluated to ensure their effectiveness in predicting the risk of ZD children.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure meaningful predictions from the algorithms. We began by one-hot encoding all categorical features, including the region identifier. This process involved creating a new binary variable for each categorical value. For instance, population density was replaced by three new binary variables based on the degree of urbanization, where a value of 1 indicated that the child belonged to that category. Similarly, we created 11 categories (zero and deciles) for the night light composite. This encoding approach helped us avoid creating fictional ordinal relationships in the data, ensuring that the algorithms could generate accurate predictions.\n\nAdditionally, we removed children with missing information for at least one of the predictors. This step was necessary to maintain the integrity of the dataset. Specifically, we excluded around 265 children from the India 2020 dataset, 232 from the India 2015 dataset, 60 from the Mali 2018 dataset, and 58 from the Nigeria 2018 dataset, all of whom had one or more missing values.\n\nAfter preprocessing, we created a training dataset using a 70% sample of the overall data, maintaining regional stratification to account for heterogeneity across regions. This involved drawing a random sample of children within each state or regional stratum, proportional to the total number of children in that stratum. The remaining samples were then used for testing how the models performed on completely unseen data points. This approach ensured that our models were robust and could generalize well to new, unseen data.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the feature set employed. We initially used a sparse set of predictors, which included crucial features such as the region of residence, facility delivery information, and antenatal care frequency. These features were selected based on their importance in identifying zero-dose (ZD) children.\n\nWe also explored the impact of expanding the predictor set beyond these sparse features. In some cases, adding more predictors improved the model's performance, while in others, it did not significantly change the outcomes. For instance, for the India 2015 dataset, the Ridge Classifier (RC) achieved a specificity of 0.54 with the sparse feature set, and this performance improved marginally to 0.61 after including additional features. However, for the India 2006 dataset, the specificity slightly decreased from 0.78 to 0.77 when more features were added.\n\nThe selection of parameters was guided by the goal of optimizing specificity while maintaining reasonable sensitivity. We employed cost-sensitive learning to address class imbalance, which is a critical issue in our datasets due to the relatively low proportion of ZD children. This approach allowed us to penalize the misclassification of ZD children as non-ZD more heavily, thereby improving the model's ability to correctly identify ZD children.\n\nAdditionally, we used fivefold cross-validation for hyperparameter tuning. This involved splitting the training data into five parts, using four parts for training and the fifth for validation. We tested various combinations of hyperparameters to find the optimal settings for each model. For example, the Ridge Classifier has a hyperparameter called the learning rate, which acts as a penalty on the sum of squares of the coefficients. By systematically testing different values, we identified the hyperparameter combination that yielded the best performance.\n\nIn summary, the number of parameters used in the model was determined by the feature set, and the selection process involved a balance between model complexity and performance metrics, particularly specificity. Cost-sensitive learning and cross-validation were key methods used to optimize the model's parameters.",
  "optimization/features": "The input features used in our models can be categorized into three sets: sparse predictors, contextual predictors, and additional predictors. The sparse predictors include administrative data such as the age of the child, gender, antenatal visits, place of delivery, and urban/rural classification. Contextual predictors add cluster-based information like population density, travel time, and night lights as a proxy for economic activity. Additional predictors include survey-based information such as the mother's education level and household wealth.\n\nThe total number of features (f) varies depending on the predictor set used. The sparse set includes the most critical features, while the other sets expand on this with more detailed information. Feature selection was inherently performed by creating these distinct sets, ensuring that the most relevant features were used for prediction. This selection process was conducted using the training data only, maintaining the integrity of the test data for unbiased evaluation. The models were trained on 70% of the data, with the remaining 30% used for testing, ensuring that the feature importance and model performance were assessed on unseen data.",
  "optimization/fitting": "In our study, we employed machine learning models to predict the vaccination status of children, focusing on identifying those who are zero-dose (ZD). Given the class imbalance in our dataset, where only a small percentage of children are ZD, we faced the challenge of potential overfitting to the majority class.\n\nTo address overfitting, we utilized cost-sensitive learning. This approach modifies the cost function to assign different penalties to different types of misclassifications. Specifically, we penalized the misclassification of ZD children as non-ZD more heavily than the reverse. This adjustment helped to ensure that our models did not simply predict all children as non-ZD, which would have maximized accuracy but resulted in a specificity of zero.\n\nAdditionally, we performed hyperparameter tuning using fivefold cross-validation. This technique involves splitting the training data into five parts, using four parts for training and the remaining part for validation. By repeating this process five times, we could select the hyperparameter combination that performed best, thereby reducing the risk of overfitting.\n\nTo rule out underfitting, we tested various models and hyperparameter combinations. For instance, the Ridge Classifier has a hyperparameter called the learning rate, which acts as a penalty on the sum of squares of the coefficients. We experimented with different values of this hyperparameter to find the optimal setting that balanced model complexity and performance.\n\nFurthermore, we evaluated model performance using metrics such as specificity, sensitivity, and the F1 Score. Specificity was particularly important in our context, as our objective was to correctly identify as many ZD children as possible. The F1 Score provided a comprehensive evaluation of model performance, especially in the presence of class imbalance.\n\nIn summary, we mitigated overfitting through cost-sensitive learning and cross-validation, while ensuring that our models were not underfitting by thoroughly testing and tuning hyperparameters. This approach allowed us to develop robust models capable of accurately predicting the vaccination status of children.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting, particularly in the context of class imbalance. One of the primary methods used was cost-sensitive learning. This approach modifies the cost function to assign different penalties to different types of misclassifications. By doing so, we addressed the issue of models overfitting to the majority class, which is a common problem in imbalanced datasets. For instance, in our dataset, only a small percentage of children were zero-dose (ZD), leading to a tendency for naive models to predict non-ZD for almost every child to maximize accuracy. Cost-sensitive learning helped mitigate this by penalizing the misclassification of ZD children more heavily.\n\nAdditionally, we performed hyperparameter tuning using fivefold cross-validation. This involved splitting the training data into five parts, using four parts for training and one part for validation in each round. This process ensured that the model's performance was evaluated on different subsets of the data, reducing the risk of overfitting to any single subset. We tested various combinations of hyperparameters, such as the learning rate in the Ridge Classifier, to find the optimal settings that performed best across all validation folds.\n\nThese techniques collectively helped in building robust models that generalized well to new data, even in the presence of significant class imbalance.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are available and have been thoroughly documented. We employed a cost-sensitive learning approach to address class imbalance, which involved modifying the cost function to penalize different types of misclassifications differently. This approach was crucial for improving the specificity of our models, as misclassifying a zero-dose (ZD) child as non-ZD is more serious than the reverse.\n\nFor hyperparameter tuning, we utilized fivefold cross-validation. This method involved randomly splitting the training data into five parts, using four parts for training and the remaining part for validation. We tested various combinations of hyperparameters to find the optimal settings for each model. For instance, the Ridge Classifier has a hyperparameter called the learning rate, which acts as a penalty on the sum of squares of the coefficients. We evaluated the model's performance across different values of this hyperparameter to determine the best combination.\n\nThe specific details of the hyperparameter configurations and the optimization process are provided in the supplementary materials of our publication. These materials include tables and figures that outline the performance metrics, such as specificity, sensitivity, and F1 Score, for different models and datasets. Additionally, the mathematical expressions for computing these performance metrics are available in the online supplemental appendix table A2.\n\nThe models and their configurations are not publicly available for download due to licensing restrictions and the proprietary nature of some of the data used. However, the methods and results are fully described in the publication, allowing other researchers to replicate the study using similar datasets and techniques. The publication is open-access, ensuring that the findings and methodologies are accessible to the scientific community.",
  "model/interpretability": "The models employed in our study, particularly the cost-sensitive Ridge Classifier (RC), are not entirely black-box systems. While machine learning models can often be seen as complex and opaque, our approach includes mechanisms to enhance interpretability.\n\nOne key aspect of interpretability in our models is the use of feature importance analysis. For each individual country-year dataset, we identified the most important features that contribute to identifying zero-dose (ZD) children. This analysis helps in understanding which predictors are most influential in the model's decisions. For instance, features such as \"no antenatal care\" and \"child delivered in a health facility\" were found to be strong predictors. The former is a significant indicator for identifying ZD children, while the latter is a positive predictor of non-ZD children. These insights provide a clear understanding of the factors driving the model's predictions.\n\nAdditionally, the Ridge Classifier's coefficients can be examined to understand the contribution of each feature. Positive coefficients indicate that the corresponding predictor receives a positive weight when calculating the probability of a child being non-ZD. This transparency allows stakeholders to see which factors are most critical in the model's decision-making process.\n\nFurthermore, the use of cost-sensitive learning modifies the cost function to associate different penalties on different types of misclassifications. This approach not only improves the model's performance in imbalanced datasets but also makes the decision-making process more interpretable by explicitly accounting for the costs of different types of errors.\n\nIn summary, while the models used are sophisticated, they are designed with interpretability in mind. The feature importance analysis and the examination of model coefficients provide clear examples of how the models operate, making them more transparent and understandable to stakeholders.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict whether a child is at risk of being zero-dose (ZD), meaning the child has not received any vaccinations. The outcome variable is binary, where 1 denotes a non-ZD child and 0 denotes a ZD child. The model outputs a predicted score between 0 and 1 for each data point (child), which is then classified into ZD or non-ZD using a threshold.\n\nSeveral classical supervised learning algorithms were employed to create these prediction models, including Ridge Classifier, Decision Tree, Nearest Neighbor, and Multi-Layer Perceptron. These algorithms were chosen for their ability to handle classification tasks effectively.\n\nThe performance of these models was evaluated using various metrics, such as sensitivity, specificity, and the F1 score. These metrics provide a comprehensive evaluation of the model\u2019s performance, particularly in scenarios with imbalanced data, which is common in this application.\n\nThe models were trained on datasets from different countries and years, and their performance was tested on unseen data to ensure robustness. The results indicate that the models can effectively predict the risk of a child being ZD, with varying levels of accuracy depending on the specific algorithm and dataset used.\n\nThe output of the model is a binary classification, indicating whether a child is at risk of being ZD or not. This information can be crucial for public health interventions aimed at ensuring that all children receive necessary vaccinations.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the analysis is publicly available on GitHub. This allows other researchers and practitioners to access, review, and potentially build upon the methods used in our study. The repository can be found at https://github.com/arpita-biswas/zero-dose-child-vaccination-targetting. The code is released under a license that permits its use and modification, facilitating its integration into other projects or adaptations for different datasets. This open-access approach supports transparency and reproducibility, key principles in scientific research. While the code is available, it is important to note that the data used in the analysis are not publicly available and must be obtained from third-party sources such as www.dhsprogram.com and www.idhsdata.org/idhs. This ensures that the data privacy and ethical considerations are maintained while still providing the methodological framework for others to utilize.",
  "evaluation/method": "We evaluated our models using several key metrics: specificity, sensitivity, and the F1 Score. These metrics were chosen to provide a comprehensive assessment of model performance, particularly in the context of class imbalance, which is prevalent in our dataset.\n\nSpecificity was the most critical metric for our application, as it measures the fraction of zero-dose (ZD) children correctly identified among all actual ZD children. This is crucial because our policy objective is to accurately identify as many ZD children as possible. Sensitivity, which measures the fraction of correctly classified non-ZD children among all non-ZD children, was also considered important to avoid deploying costly interventions to children at low risk of being zero-dose.\n\nThe F1 Score, a harmonic mean of precision and recall, was used to provide a balanced measure of model performance, especially in the presence of class imbalance. This metric is preferred over accuracy because it offers a more nuanced evaluation of the model's effectiveness in handling imbalanced data.\n\nTo ensure robust model evaluation, we employed fivefold cross-validation for hyperparameter tuning. This involved randomly splitting the training data into five parts, using four parts for training and the remaining part for validation in each round. This process was repeated five times, with each part serving as the validation set once. The best-performing hyperparameters were then selected for training the final model.\n\nAdditionally, we conducted three supplementary analyses. First, we assessed whether the best-performing model needed to be retrained by evaluating models trained on older datasets (Mali 2006, Nigeria 2008, India 2006, and India 2015) on newer datasets (Mali 2018, Nigeria 2018, India 2020). This helped us understand the temporal stability of our models.\n\nSecond, we identified the most important features in predicting ZD children using the best-performing machine learning model for each individual country-year dataset. This analysis provided insights into the key predictors of zero-dose status.\n\nThird, we compared the performance of a simple geographical rule (targeting districts with less than 80% full immunization coverage) to a prediction model that included full immunization coverage as a predictor. This comparison helped us evaluate the added value of our predictive models over straightforward geographical targeting.",
  "evaluation/measure": "In our evaluation of the prediction models, we focused on several key performance metrics to comprehensively assess their effectiveness, particularly in the context of class imbalance. The primary metrics we reported include specificity, sensitivity, and the F1 Score.\n\nSpecificity is the fraction of individuals correctly classified as zero-dose (ZD) children among all ZD children. This metric is crucial for our application because the policy objective is to correctly identify as many ZD children as possible. High specificity ensures that the model accurately targets the children who need interventions the most.\n\nSensitivity, on the other hand, represents the fraction of correctly classified non-ZD children among all non-ZD children. While high sensitivity is not critical for our application, it is desirable to avoid deploying costly interventions to children who are at low risk of being zero-dose.\n\nThe F1 Score is particularly important in our context due to the class imbalance in the data. This metric provides a more comprehensive evaluation of the model\u2019s performance by balancing precision and recall. It is preferred over accuracy because it accounts for both false positives and false negatives, offering a more nuanced view of the model's effectiveness in imbalanced datasets.\n\nAdditionally, we considered other metrics such as exclusion error and inclusion error. Exclusion error measures the percentage of ZD children not targeted by the policy, while inclusion error measures the percentage of non-ZD children within all the children targeted by the policy. These metrics help in understanding the trade-offs involved in the model's predictions.\n\nThe set of metrics we used is representative of the literature on classification tasks, especially in scenarios with class imbalance. These metrics are commonly employed to evaluate the performance of machine learning models in similar contexts, ensuring that our evaluation is both rigorous and comparable to other studies in the field.",
  "evaluation/comparison": "In our evaluation, we conducted several analyses to compare the performance of our models. We examined whether the best-performing model needed to be retrained by assessing the performance of models trained on older data and tested on newer data. This involved training on datasets from Mali 2006, Nigeria 2008, India 2006, and India 2015, and then testing the prediction power on datasets from Mali 2018, Nigeria 2018, and India 2020.\n\nAdditionally, we used the best-performing machine learning model for each individual country-year dataset to identify the most important features in identifying zero-dose (ZD) children. This step helped us understand which features were crucial for the model's predictions.\n\nFurthermore, we compared the performance of a simple geographical rule (targeting districts with less than 80% full immunization coverage) to a prediction model that includes full immunization coverage as a predictor. This comparison allowed us to evaluate the effectiveness of our model against a straightforward baseline method.\n\nWe also assessed the performance of various classifiers, including Ridge Classifier, Decision Tree, Nearest Neighbor, and Multi-Layer Perceptron, across different datasets and feature sets. The Ridge Classifier generally performed the best or nearly the best in terms of specificity across most country-years and feature sets.\n\nThe performance metrics, such as sensitivity, specificity, and F1 Score, were evaluated for each classifier. The Ridge Classifier showed consistent performance, particularly when the cost of misclassifying ZD children as non-ZD was adjusted. The trade-off between specificity and other metrics like sensitivity and F1 Score was also analyzed, showing that prioritizing specificity can worsen these other metrics.\n\nIn summary, our evaluation included comparisons to simpler baselines and assessments of model performance over time and across different feature sets. These comparisons provided a comprehensive understanding of our model's strengths and areas for improvement.",
  "evaluation/confidence": "The evaluation of our models focused on several key performance metrics, including sensitivity, specificity, and the F1 score. These metrics were chosen to provide a comprehensive assessment of model performance, particularly in the context of class imbalance.\n\nConfidence intervals for these performance metrics were not explicitly reported in our study. The primary focus was on the point estimates of these metrics across different datasets and model configurations. This approach allowed us to highlight the relative performance of various classifiers and feature sets without delving into the variability of these estimates.\n\nStatistical significance was not formally tested to claim the superiority of one method over others or baselines. Instead, the evaluation relied on comparative performance across multiple datasets and scenarios. For instance, the Ridge Classifier (RC) consistently showed high specificity across different country-years and feature sets, indicating its robustness. However, without formal statistical tests, we cannot definitively claim that the differences in performance are statistically significant.\n\nThe trade-offs between different metrics were also explored. For example, prioritizing specificity often led to a decrease in sensitivity, F1 score, and accuracy. This trade-off was particularly evident when adjusting the cost of misclassification for ZD children. Balancing these metrics was crucial for achieving a model that performed well across various evaluation criteria.\n\nIn summary, while the performance metrics provide valuable insights into the effectiveness of different models, the lack of confidence intervals and formal statistical significance tests means that the results should be interpreted with caution. The comparative performance across multiple datasets suggests that certain models, like the Ridge Classifier, are generally more effective, but further statistical analysis would be needed to confirm these findings definitively.",
  "evaluation/availability": "Not enough information is available."
}