{
  "publication/title": "Radiomics Analysis of Computed Tomography for Prediction of Thyroid Capsule Invasion in Papillary Thyroid Carcinoma: A Multi-Classifier and Two-Center Study.",
  "publication/authors": "Wu X, Yu P, Jia C, Mao N, Che K, Li G, Zhang H, Mou Y, Song X",
  "publication/journal": "Frontiers in endocrinology",
  "publication/year": "2022",
  "publication/pmid": "35692398",
  "publication/pmcid": "PMC9174423",
  "publication/doi": "10.3389/fendo.2022.849065",
  "publication/tags": "- Thyroid Capsule Invasion\n- Papillary Thyroid Carcinoma\n- Radiomics\n- Machine Learning\n- CT Imaging\n- Predictive Modeling\n- Clinicoradiological Risk Factors\n- Support Vector Machines\n- Logistic Regression\n- Medical Imaging Analysis",
  "dataset/provenance": "The dataset used in this study was sourced from two institutions: Qilu Hospital of Shandong University and another unnamed institution. The study included a total of 33 eligible patients recruited from Qilu Hospital of Shandong University between September 2020 and December 2020, serving as the external test cohort. These patients underwent preoperative non-enhanced and contrast-enhanced CT scans within two weeks before surgery. The inclusion criteria ensured that all patients had pathologically confirmed papillary thyroid carcinoma (PTC) and well-preserved clinical, imaging, and pathological data. Exclusion criteria removed patients who had received prior treatments, had multiple primary carcinomas, Hashimoto\u2019s thyroiditis, or poor-quality CT images.\n\nThe dataset utilized in this study is unique and specifically curated for the prediction of thyroid capsule invasion (TCI) in PTC. Previous studies have often relied on single-center data, which can limit the generalizability of their findings. By including an external test cohort from a different institution, this study addresses the limitations of single-center studies and enhances the robustness of the model's performance. The use of diverse CT scanners from different manufacturers further ensures that the radiomics features extracted are robust and not dependent on specific imaging equipment. This approach provides a more comprehensive and reliable dataset for predicting TCI in PTC.",
  "dataset/splits": "The dataset was divided into three distinct cohorts: a training cohort, an internal test cohort, and an external test cohort. The training cohort consisted of 265 data points, the internal test cohort had 114 data points, and the external test cohort included 33 data points.\n\nThe training cohort was further divided into two groups based on the presence of thyroid capsule invasion (TCI): 130 data points with TCI (pTCI+) and 135 data points without TCI (pTCI-). Similarly, the internal test cohort was split into 56 data points with TCI and 58 data points without TCI. The external test cohort had 17 data points with TCI and 16 data points without TCI.\n\nThe distribution of data points across various clinicoradiological characteristics was also analyzed. For instance, the gender distribution showed a higher proportion of females in the training cohort compared to males, while the internal and external test cohorts had a higher proportion of males. Age, thyroid-stimulating hormone (TSH) levels, and maximum tumor diameter (CT-MTD) were also compared across the cohorts, with no significant differences observed.\n\nThe location of the lesions varied significantly across the cohorts, with the majority of lesions located on the left or right side of the thyroid. The CT-reported TCI status and lymph node (LN) status were also evaluated, showing notable differences between the cohorts. For example, a higher percentage of CT-reported TCI was observed in the external test cohort compared to the training and internal test cohorts. Similarly, the CT-reported LN status showed variations, with a higher percentage of positive LN status in the external test cohort.",
  "dataset/redundancy": "The dataset used in this study consisted of 412 patients, who were divided into two groups based on postoperative pathological findings: pTCI+ (pathological positive thyroid capsule invasion) and pTCI\u2212 (pathological negative thyroid capsule invasion). The dataset was split into three cohorts: a training cohort, an internal test cohort, and an external test cohort. The training cohort was used to construct the models, while the internal and external test cohorts were used to evaluate the models' performance.\n\nThe training and test sets were independent. To enforce this independence, an external test cohort was included to assess model performance, addressing the limitations of single-center studies. This approach helped to confirm the generalizability of the models. Additionally, image preprocessing was performed before feature extraction to reduce dependency on image specifications, further ensuring the independence and robustness of the datasets.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the field. Many previous studies have relied solely on internal test cohorts and data from a single piece of equipment within a single center. This approach can lead to variability in feature values due to differences in scanning parameters and post-processing algorithms across different equipment. In contrast, our study included data from multiple centers and various CT scanners, enhancing the diversity and representativeness of the dataset. This methodology aligns with recommendations that radiomics studies should use multiple machine learning methods and diverse data sources to improve model performance and generalizability.",
  "dataset/availability": "The raw data supporting the conclusions of this article will be made available by the authors, without undue reservation. This means that the dataset, including the data splits used, is not publicly available in a forum at the moment. However, it can be obtained by requesting it to the authors. The authors did not specify any particular license for the data, nor any specific enforcement mechanism to ensure the data is shared as stated.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is Support Vector Machines (SVM). Specifically, we employed linear SVM (L-SVM), Gaussian SVM (G-SVM), and polynomial SVM (P-SVM) for model construction. These algorithms are well-established and widely used in the field of radiomics and machine learning for their robustness in handling high-dimensional data and solving non-linear problems.\n\nThe algorithms used are not new; they have been extensively studied and applied in various domains, including medical imaging and radiomics. The choice of SVM was driven by its proven effectiveness in similar studies and its ability to manage complex relationships within the data.\n\nGiven that SVM is a mature and widely recognized algorithm, it was not necessary to publish the algorithm itself in a machine-learning journal. Instead, our focus was on applying these algorithms to the specific problem of predicting thyroid capsule invasion (TCI) in papillary thyroid carcinoma (PTC) using radiomics features and clinicoradiological data. The novelty of our work lies in the application of these algorithms to this particular medical challenge, rather than in the development of new machine-learning techniques.",
  "optimization/meta": "The model employed in this study is a meta-predictor, which integrates multiple machine learning methods to enhance predictive performance. Specifically, six types of supervised machine learning classifiers were used: Logistic Regression (LR), K-Nearest Neighbors (KNN), Decision Tree (DT), Linear Support Vector Machine (L-SVM), Gaussian Support Vector Machine (G-SVM), and Polynomial Support Vector Machine (P-SVM). These classifiers were utilized to construct models based on optimal radiomics features, clinicoradiological risk factors, and a combination of both.\n\nThe combined-L-SVM model demonstrated superior performance across various cohorts, including the training, internal test, and external test cohorts. This model's robustness was validated through its consistent performance metrics, such as the Area Under the Curve (AUC), which indicated its stability and generalizability.\n\nTo ensure the independence of the training data, the study included an external test cohort. This cohort was used to assess the model's performance on data that was not part of the training process, thereby confirming the model's generalizability and reducing the risk of overfitting. The external test cohort provided an additional layer of validation, ensuring that the model's predictions were reliable and applicable to new, unseen data.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for ensuring the robustness and generalizability of the radiomics features extracted from the CT images. The images were obtained from various CT scanners, which can lead to variability in voxel values and, consequently, in feature values. To mitigate this issue, image preprocessing was performed to standardize the data.\n\nThe preprocessing involved several steps. First, the images were normalized to reduce the dependency on image specifications. This step is essential because different scanners can produce images with varying intensity values, which can affect the radiomics features. By normalizing the images, we ensured that the features extracted were consistent across different scanners.\n\nNext, the volume of interest (VOI) segmentation was manually drawn slice by slice on the entire tumor\u2019s boundary by a radiologist. This manual segmentation process was necessary to accurately delineate the tumor boundaries, which is critical for extracting relevant radiomics features.\n\nAfter segmentation, the images underwent further preprocessing to extract robust radiomics features. This included calculating intraclass correlation coefficients (ICCs) to determine the reproducibility of the features. Features with ICCs greater than 0.75 were retained, indicating good agreement and reliability.\n\nAdditionally, feature selection strategies were employed to reduce dimensionality and select the best subset of features. This involved using one-way analysis of variance (ANOVA) to select features with p-values less than 0.05, followed by the least absolute shrinkage and selection operator (LASSO) algorithm with penalty tuning conducted by 10-fold cross-validation. This process helped in identifying key radiomics features with nonzero coefficients, which were then used for model construction.\n\nThe preprocessing steps ensured that the radiomics features were robust and reliable, enabling the construction of accurate and generalizable machine learning models.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the feature selection process. Initially, 4227 radiomics features were extracted from each patient. To reduce dimensionality and select the best subset of features, several strategies were employed. First, features with intraclass correlation coefficients (ICCs) greater than 0.75 were retained, indicating good agreement. Next, features with a p-value less than 0.05 were selected after performing a one-way analysis of variance (ANOVA). Finally, the least absolute shrinkage and selection operator (LASSO) algorithm with penalty tuning conducted by 10-fold cross-validation was applied to select the key radiomics features with nonzero coefficients. This process resulted in 23 optimal radiomics features being selected for model construction.\n\nThe selection of these 23 features was crucial for ensuring that the model was both efficient and effective. By using LASSO with cross-validation, we were able to identify the most predictive features while minimizing the risk of overfitting. This approach allowed us to construct models that performed well across different cohorts, including the training, internal test, and external test cohorts. The use of multiple feature selection strategies ensured that only the most relevant and reproducible features were included in the final models.",
  "optimization/features": "In the study, a total of 4227 radiomics features were initially extracted from each patient. To reduce dimensionality and select the most relevant features, a multi-step feature selection process was employed.\n\nFirst, features with intraclass correlation coefficients (ICCs) greater than 0.75 were retained, indicating good agreement and reproducibility. Subsequently, features with a p-value less than 0.05 were selected after performing a one-way analysis of variance (ANOVA). Finally, the least absolute shrinkage and selection operator (LASSO) algorithm with penalty tuning conducted by 10-fold cross-validation was applied to select the key radiomics features with nonzero coefficients.\n\nThis rigorous feature selection process ensured that only the most predictive and reliable features were used as input for model construction. The final set of optimal radiomics features consisted of 23 features, which included first-order statistical features, shape-based features, and textural features from various matrices such as Gray-Level Dependence Matrix (GLDM), Gray-Level Run Length Matrix (GLRLM), and Gray-Level Size Zone Matrix (GLSZM).\n\nAll feature selection steps were performed using the training set only, ensuring that the models were evaluated on unseen data during the internal and external test phases. This approach helps to prevent overfitting and ensures the generalizability of the models.",
  "optimization/fitting": "In our study, we employed several strategies to address the challenges of overfitting and underfitting, particularly given the high dimensionality of radiomics features relative to the number of training samples.\n\nTo mitigate overfitting, we utilized feature selection techniques to reduce dimensionality. Initially, we retained features with intraclass correlation coefficients (ICCs) greater than 0.75, indicating good reproducibility. Subsequently, we applied one-way analysis of variance (ANOVA) to select features with p-values less than 0.05. Further refinement was achieved using the least absolute shrinkage and selection operator (LASSO) algorithm with penalty tuning conducted by 10-fold cross-validation. This process ensured that only the most relevant features with nonzero coefficients were retained, thereby reducing the risk of overfitting.\n\nIn the model construction phase, we employed an iterative grid search procedure to tune the hyperparameters of each classifier. This method helped in optimizing the model parameters and avoiding overfitting. Additionally, we used 5-fold cross-validation to assess model performance and repeated this process 10 times to calculate the mean of performance estimates. This rigorous cross-validation approach provided a robust evaluation of the model's generalizability and helped in identifying the optimal hyperparameters.\n\nTo address underfitting, we constructed models using six different machine learning classifiers: logistic regression (LR), k-nearest neighbor (KNN), decision tree (DT), linear support vector machine (L-SVM), Gaussian support vector machine (G-SVM), and polynomial support vector machine (P-SVM). By comparing the performance of these classifiers, we ensured that the model was not too simplistic and could capture the complex relationships in the data. The linear support vector machine (L-SVM) classifier ultimately demonstrated the best performance, indicating its effectiveness in balancing model complexity and generalization.\n\nFurthermore, we included an external test cohort to evaluate the model's performance on unseen data, which helped in assessing the model's generalizability and ensuring that it was not underfitting. The consistent performance across different cohorts provided confidence in the model's ability to generalize to new data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was the Least Absolute Shrinkage and Selection Operator (LASSO) algorithm. This technique is particularly useful for feature selection and regularization, as it applies a penalty to the absolute size of the regression coefficients, effectively shrinking some of them to zero. This process helps in selecting the most relevant features and reducing the complexity of the model, thereby mitigating overfitting.\n\nAdditionally, we utilized 10-fold cross-validation during the LASSO algorithm to tune the penalty parameter. This approach ensures that the model's performance is evaluated on multiple subsets of the data, providing a more reliable estimate of its generalization capability.\n\nFurthermore, we performed hyperparameter tuning using an iterative grid search procedure for each classifier. This method systematically works through multiple combinations of hyperparameter values to determine the optimal settings that maximize model performance and minimize the risk of overfitting.\n\nTo further validate our models, we employed 5-fold cross-validation during the training process. This technique involves dividing the data into five subsets, training the model on four of them, and validating it on the remaining one. This process is repeated five times, with each subset serving as the validation set once. The average performance across these iterations provides a more robust estimate of the model's performance and helps in identifying any signs of overfitting.\n\nBy integrating these regularization and validation techniques, we aimed to construct models that are not only accurate but also generalizable to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we employed an iterative grid search procedure to tune the hyperparameters of each classifier, aiming to avoid overfitting and maximize model performance. This process was conducted using 5-fold cross-validation to ensure robustness.\n\nThe models were constructed using various machine learning classifiers, including logistic regression (LR), k-nearest neighbor (KNN), decision tree (DT), linear support vector machine (L-SVM), Gaussian support vector machine (G-SVM), and polynomial support vector machine (P-SVM). These classifiers were implemented using Python (version 3.6) with the scikit-learn package.\n\nThe specific details of the models' predictive performances, including the area under the receiver operating characteristic curve (AUC) values, are summarized in the results section. For instance, the combined-P-SVM model achieved an AUC of 0.905 in the training cohort, while the combined-L-SVM model had the highest AUC of 0.820 in the internal test cohort.\n\nThe calibration curves and decision curve analysis (DCA) for the combined-L-SVM model are also provided, demonstrating good calibration and clinical usefulness across different cohorts.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly detailed in the publication. However, the methods and results sections provide comprehensive information on the feature selection strategies, model construction, and evaluation metrics used. This includes the use of the least absolute shrinkage and selection operator (LASSO) algorithm for feature selection and the application of receiver operating characteristic (ROC) curves for performance evaluation.\n\nFor those interested in replicating or building upon our work, the scikit-learn package and the specific versions of Python and R software used are mentioned, along with the relevant packages and their functions. This information should facilitate the reproduction of our methods and the evaluation of similar models.\n\nNot applicable.",
  "model/interpretability": "The models constructed in this study primarily rely on machine learning algorithms, which are often considered black-box models due to their complexity and the lack of straightforward interpretability. However, efforts were made to enhance the transparency and interpretability of the models.\n\nThe study employed a linear support vector machine (L-SVM) as the primary classifier, which is more interpretable than some other machine learning models. L-SVMs provide a clear decision boundary and can be analyzed to understand the contribution of individual features to the model's predictions. This makes it easier to interpret the importance of specific radiomics features in predicting thyroid capsule invasion (TCI).\n\nAdditionally, the study used the least absolute shrinkage and selection operator (LASSO) algorithm for feature selection. LASSO not only helps in reducing the dimensionality of the feature set but also provides coefficients that indicate the importance of each selected feature. This allows for a more transparent understanding of which radiomics features are most influential in the model's predictions.\n\nThe use of clinicoradiological risk factors, such as CT-reported TCI, further enhances the interpretability of the combined models. By incorporating these clinical variables, the models can provide insights into how both radiomics features and clinical characteristics contribute to the prediction of TCI.\n\nIn summary, while the models are not entirely transparent, the use of L-SVM and LASSO, along with the inclusion of clinicoradiological risk factors, provides a level of interpretability that helps in understanding the underlying mechanisms of the predictions.",
  "model/output": "The model constructed in this study is a classification model. It is designed to predict thyroid capsule invasion (TCI) in papillary thyroid carcinoma (PTC) patients. The model uses various machine learning classifiers, including logistic regression (LR), k-nearest neighbor (KNN), decision tree (DT), linear support vector machine (L-SVM), Gaussian support vector machine (G-SVM), and polynomial support vector machine (P-SVM). These classifiers are used to distinguish between patients with and without TCI based on radiomics features extracted from CT scans and clinicoradiological risk factors.\n\nThe performance of these models was evaluated using the area under the receiver operating characteristic curve (AUC), which is a common metric for assessing the performance of classification models. The combined-L-SVM model, which incorporates both radiomics features and clinicoradiological risk factors, demonstrated the best performance with AUCs of 0.905 in the training cohort, 0.820 in the internal test cohort, and 0.776 in the external test cohort. This indicates that the model is effective in classifying patients into those with and without TCI.\n\nAdditionally, the calibration curves and decision curve analysis (DCA) were used to evaluate the agreement between predicted probabilities and observed outcomes, as well as the clinical usefulness of the model. The results showed that the combined-L-SVM model provides a good calibration and net benefit across a range of threshold probabilities, further supporting its classification capability.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models constructed in this study was implemented using Python, specifically version 3.6, with the scikit-learn package. The scikit-learn package is an open-source machine learning library that is widely used and freely available. The specific packages utilized within scikit-learn include \"selectKbest,\" \"LassoCV,\" \"LogisticRegression,\" \"svm,\" \"neighbors,\" \"tree,\" and \"roccurve.\"\n\nAdditionally, statistical analysis was performed using R software, version 4.0.3, with packages such as \"rms,\" \"rmda,\" and \"irr.\" These tools and packages are all publicly available and can be accessed through their respective repositories.\n\nWhile the specific code used for this study is not explicitly released as a standalone repository, the methods and tools employed are well-documented and can be replicated using the mentioned software and packages. Researchers interested in replicating or building upon this work can refer to the scikit-learn and R documentation for guidance on implementing similar models.",
  "evaluation/method": "The evaluation of the models involved several steps to ensure robustness and generalizability. Initially, all models were trained using a training cohort, and their performance was assessed through 5-fold cross-validation, repeated 10 times to calculate the mean performance estimates. This approach helped in understanding the model's stability and performance variability.\n\nThe prediction performance was evaluated using the receiver operating characteristic (ROC) curve and the area under the ROC curve (AUC). The ROC curve provides a visual representation of the trade-off between the true positive rate and the false positive rate, while the AUC quantifies the overall ability of the model to discriminate between positive and negative classes.\n\nAdditionally, calibration curves were used to evaluate the agreement between the observed results and the predicted probabilities. These curves help in assessing how well the predicted probabilities match the actual outcomes, indicating the model's reliability.\n\nDecision curve analysis (DCA) was employed to calculate the net benefits for different threshold probabilities, determining the clinical usefulness of the optimal combined model. DCA helps in understanding the range of threshold probabilities for which the model provides more benefit than treating all patients or treating none.\n\nThe models were evaluated in three different cohorts: the training cohort, an internal test cohort, and an external test cohort. This multi-cohort evaluation ensured that the models were not only performing well on the data they were trained on but also generalizing to new, unseen data. The external test cohort, in particular, was crucial in assessing the model's generalizability and stability.\n\nIn summary, the evaluation method involved a combination of cross-validation, ROC analysis, calibration curves, and decision curve analysis across multiple cohorts to ensure a comprehensive assessment of the models' performance and reliability.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our models. The primary metric used was the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve. This metric provides a comprehensive measure of the model's ability to distinguish between positive and negative cases across all threshold levels. We reported AUC values for various models in different cohorts, including the training, internal test, and external test cohorts. For instance, the combined-P-SVM model achieved an AUC of 0.905 in the training cohort, while the combined-L-SVM model had an AUC of 0.820 in the internal test cohort and 0.776 in the external test cohort.\n\nIn addition to AUC, we also utilized calibration curves to assess the agreement between predicted probabilities and observed outcomes. These curves help to evaluate the reliability of the model's predictions. Decision Curve Analysis (DCA) was another important metric we used to determine the clinical usefulness of our models. DCA calculates the net benefits for different threshold probabilities, providing insights into the practical value of the models in clinical settings.\n\nThese performance metrics are widely recognized and used in the literature, ensuring that our evaluation is representative and comparable to other studies in the field. The use of multiple metrics allows for a thorough assessment of model performance, covering aspects such as discriminative ability, calibration, and clinical utility.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. However, we did compare our approach with simpler baselines to evaluate its effectiveness.\n\nWe constructed models using various machine learning classifiers, including logistic regression (LR), k-nearest neighbor (KNN), decision tree (DT), linear support vector machine (L-SVM), Gaussian support vector machine (G-SVM), and polynomial support vector machine (P-SVM). This allowed us to assess the performance of different classifiers and determine which one was most effective for our specific task.\n\nAdditionally, we compared the performance of models based solely on radiomics features, clinical factors, and a combined approach. The combined model, which integrated both radiomics features and clinical factors, demonstrated superior performance. This comparison highlighted the added value of incorporating multiple types of data in our predictive model.\n\nFurthermore, we evaluated the generalizability of our model by including an external test cohort. This step was crucial in addressing the limitations of single-center studies and ensuring that our model could perform well on diverse datasets. The results showed that our model maintained good prediction performance in the external test cohort, indicating its robustness and generalizability.\n\nIn summary, while we did not compare our method directly with publicly available benchmarks, we conducted thorough comparisons with simpler baselines and different model configurations to validate the effectiveness and generalizability of our approach.",
  "evaluation/confidence": "The evaluation of our models included several key performance metrics, all of which were accompanied by confidence intervals to provide a clear understanding of their reliability. The area under the receiver operating characteristic curve (AUC) was a primary metric used to assess the predictive performance of our models. For instance, in the training cohort, the radiomics-G-SVM model achieved an AUC of 0.786 with a 95% confidence interval (CI) of 0.736\u20130.832. Similarly, in the internal test cohort, the radiomics-L-SVM model had an AUC of 0.733 with a 95% CI of 0.654\u20130.812. These confidence intervals indicate the range within which the true AUC values are likely to fall, providing a measure of the precision of our estimates.\n\nStatistical significance was also a crucial aspect of our evaluation. All statistical tests conducted were two-sided, and a p-value of less than 0.05 was considered statistically significant. This threshold ensured that the differences observed in our model performances were unlikely to have occurred by chance. For example, the clinicoradiological risk factor of CT-reported TCI was identified with an odds ratio (OR) of 1.80, a 95% CI of 1.60\u20132.02, and a p-value of less than 0.001, indicating a strong and statistically significant association.\n\nIn addition to AUC, we used calibration curves and decision curve analysis (DCA) to further evaluate our models. The calibration curves of the combined-L-SVM model showed good agreement between predicted probabilities and observed outcomes across the training, internal test, and external test cohorts. The DCA demonstrated that our combined-L-SVM model provided more clinical benefit than treating all patients or treating none, across a range of threshold probabilities.\n\nOverall, the inclusion of confidence intervals and the use of statistical significance testing provide a robust framework for evaluating the performance and reliability of our models. These measures ensure that our claims of superiority over other methods and baselines are well-supported by the data.",
  "evaluation/availability": "Not enough information is available."
}