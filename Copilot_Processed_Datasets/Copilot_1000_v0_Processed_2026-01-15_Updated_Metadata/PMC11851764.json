{
  "publication/title": "Treatment response of patients with tuberculosis and HIV co-infection: a retrospective analysis of secondary data from Shanghai, China, 2010-2020.",
  "publication/authors": "Dong C, Zhang R, Li S, Chen J, Liu Y, Xia X, Liu G, Shen Y, Liu L, Zeng L",
  "publication/journal": "Therapeutic advances in infectious disease",
  "publication/year": "2025",
  "publication/pmid": "40007941",
  "publication/pmcid": "PMC11851764",
  "publication/doi": "10.1177/20499361241308641",
  "publication/tags": "- HIV/TB co-infection\n- TB treatment effectiveness\n- Clinical indicators\n- Drug regimens\n- Logistic regression\n- Support vector machine\n- Random forest\n- Machine learning algorithms\n- Prediction models\n- CD4+ T cell counts\n- Fisher\u2019s exact test\n- Stratified analysis\n- Consensus clustering\n- Proportion of Ambiguous Clustering\n- Multivariable analysis\n- Odds ratio\n- Confidence interval\n- Receiver operating characteristic curve\n- Area under the curve\n- Model performance evaluation",
  "dataset/provenance": "The dataset used in this study originates from a cohort of HIV/TB co-infected patients. Initially, the dataset included 1566 patients, of whom 1483 had treatment records and 1430 had TB test records. The test records from different tissues of the 1430 patients at the same time were unified into one record. The results of TB tests with an interval of 7 days or less were combined. This screening process resulted in 1166 co-infected patients being detected. After excluding patients with only one TB test record, 743 co-infected patients remained, which were divided into 1698 samples. Further extraction of sample information using the screening model retained only samples with positive initial TB test results, resulting in 742 effective samples corresponding to 461 patients.\n\nThe dataset includes demographic information, clinical indicators, and drug use records. Demographics were coded as follows: age < 60 as 0, age \u2265 60 as 1; male as 0, female as 1; non-HBV as 0, HBV as 1; no smoking/drinking/drug-taking as 0, smoking/drinking/drug-taking as 1. Clinical indicator results were coded as 0 for normal (within reference range) and 1 for abnormal (outside reference range). Drug use was coded as 0 for no medication use and 1 for medication use.\n\nThe dataset has been used to construct prediction models using logistic regression (LR), support vector machine (SVM), and random forest (RF) algorithms. The models were evaluated using metrics such as accuracy, precision, recall, specificity, F1 score, and AUC with five-fold cross-validation. The dataset has not been used in previous papers by the community, but it has been used to explore potential factors affecting TB treatment effectiveness in HIV/TB co-infected patients.",
  "dataset/splits": "In our study, we employed a five-fold cross-validation approach to ensure robust model evaluation. This method involved splitting the dataset into five distinct folds. For each fold, the data was divided into a training set and a test set. The training set consisted of approximately 80% of the data, while the test set comprised the remaining 20%.\n\nFor the first prediction model, we initially filtered the dataset to exclude variables with insufficient categories or missing values, resulting in 347 samples. These samples were then used to construct the prediction models. The training set for this model contained 278 samples, and the test set had 69 samples.\n\nIn the second prediction model, we selected variables with a higher number of records and excluded those with insufficient categories, leading to 527 samples. The training set for this model included 422 samples, while the test set consisted of 105 samples.\n\nThe distribution of data points in each split was designed to ensure that the models were trained and tested on diverse subsets of the data, thereby enhancing the generalizability of the results. The five-fold cross-validation process was repeated five times, and the mean and standard deviation of the evaluation metrics were reported to provide a comprehensive assessment of model performance.",
  "dataset/redundancy": "The dataset used in this study was derived from a larger pool of HIV/TB co-infected patients. Initially, there were 1566 patients, but after filtering for those with treatment and TB test records, 1166 patients remained. Further exclusion of patients with only one TB test record left 743 patients, which were divided into 1698 samples. Only samples with positive initial TB test results were retained, resulting in 742 effective samples corresponding to 461 patients.\n\nFor the prediction models, two datasets were created. The first dataset, used for Prediction Model I, included 347 samples with 18 variables after excluding those with insufficient categorical data. The second dataset, used for Prediction Model II, included 527 samples with 32 variables, again after filtering out insufficient categorical data.\n\nThe datasets were split using five-fold cross-validation. This method ensures that the training and test sets are independent by rotating the data such that each fold serves as the test set once while the remaining folds form the training set. This process was repeated five times, and the results were averaged to provide a robust evaluation of the models.\n\nThe distribution of the datasets compares favorably with previously published machine learning datasets in the medical field, particularly those involving complex diseases like TB. The use of cross-validation helps mitigate issues related to data redundancy and ensures that the models are evaluated on unseen data, which is crucial for assessing their generalizability. The models were constructed to predict TB treatment effectiveness, and the evaluation metrics, such as accuracy, precision, recall, specificity, F1 score, and AUC, were reported to provide a comprehensive assessment of model performance.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in this study utilized three well-established machine learning algorithms: Logistic Regression (LR), Support Vector Machine (SVM), and Random Forest (RF). These algorithms are widely recognized and have been extensively used in various predictive modeling tasks.\n\nThe algorithms used are not new; they are standard machine learning techniques that have been thoroughly researched and applied in numerous fields. Logistic Regression is a statistical method for analyzing a dataset in which there are one or more independent variables that determine an outcome. The Support Vector Machine is a supervised learning model that analyzes data for classification and regression analysis. Random Forest is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n\nThe choice of these algorithms was driven by their robustness and effectiveness in handling complex datasets, making them suitable for the predictive modeling tasks in this study. The decision to use these established methods rather than novel ones was likely influenced by the need for reliability and comparability with existing research. Publishing in a specialized machine learning journal was not the primary goal of this study, which focused on applying these algorithms to a specific medical dataset to predict treatment effectiveness for TB. The results and insights gained from using these algorithms are valuable in the context of medical research and clinical applications, even though the algorithms themselves are not novel.",
  "optimization/meta": "The models discussed in this publication do not use data from other machine-learning algorithms as input. Instead, they employ three distinct machine learning algorithms: Logistic Regression (LR), Support Vector Machine (SVM), and Random Forest (RF). Each of these algorithms was used to construct two prediction models, labeled as Prediction Model I and Prediction Model II.\n\nThe evaluation of these models involved comparing their performance metrics, such as accuracy, precision, recall, specificity, F1 score, and AUC, across both training and test sets. The results indicated that while Logistic Regression generally showed higher accuracy compared to SVM, the Random Forest models exhibited signs of overfitting, particularly in the training sets.\n\nThe construction of these models did not involve a meta-predictor approach, where the outputs of multiple machine learning algorithms would be combined to make a final prediction. Instead, each algorithm was evaluated independently to determine its effectiveness in predicting treatment outcomes.\n\nRegarding the independence of the training data, the models were constructed using five-fold cross-validation. This method ensures that the data is split into training and test sets multiple times, with each fold serving as the test set once while the remaining folds form the training set. This process helps to validate the models' performance and generalizability, ensuring that the training data is independent for each fold.",
  "optimization/encoding": "In our study, we encoded various demographic, clinical, and drug use variables to prepare the data for machine learning algorithms. Demographic information such as age was coded as 0 for individuals under 60 and 1 for those 60 or older. Gender was encoded as 0 for male and 1 for female. The presence of hepatitis B virus (HBV) was coded as 0 for non-HBV and 1 for HBV. Smoking, drinking, and drug-taking behaviors were coded as 0 for no use and 1 for use.\n\nClinical indicator results were coded as 0 for normal values (within the reference range) and 1 for abnormal values (outside the reference range). Drug use was encoded as 0 for no medication use and 1 for medication use. This binary encoding facilitated the application of machine learning algorithms by converting categorical and continuous variables into a format suitable for analysis.\n\nWe employed Fisher\u2019s exact test and stratified analysis to assess treatment effectiveness among different immune level groups or drug regimen groups. The stepwise method based on CD4+ T cell counts was used to determine the best grouping criteria for immune levels. Consensus clustering was applied to drug data, considering the duration of each drug used in the samples. The Proportion of Ambiguous Clustering (PAC) method helped select the optimal number of drug classifications.\n\nEach sample was classified into a drug regimen or combination drug regimen group based on the drug use conditions. We then analyzed the odds ratio (OR), 95% confidence interval (CI), and p-values of TB treatment effectiveness and confounding variables between groups using Fisher\u2019s exact test. This comprehensive approach ensured that the data was appropriately encoded and pre-processed for accurate and reliable machine learning model training and evaluation.",
  "optimization/parameters": "In our study, we constructed two multifactor models to predict patients' TB treatment effectiveness. For the first model, we started with 547 samples and excluded variables that contained only one category or had fewer than five non-zero recorded values. This process resulted in 347 samples with 18 variables, all of which had no missing values. These variables included demographic information, time interval for TB detection, duration of medication, CD4+ T cell count, CD8+ T cell count, and CD4+/CD8+ ratio.\n\nFor the second model, we selected 169 variables with more than 500 records and excluded those with only one category or fewer than five non-zero recorded values. This led to 527 samples with 32 variables, again with no missing values.\n\nThe selection of these parameters was driven by the need to ensure the capacity of the models while maintaining consistency with univariate analysis results. We aimed to include factors that showed little consistency with univariate analysis to enhance the model's predictive power. The final number of parameters (p) used in the models were 18 for the first model and 32 for the second model.",
  "optimization/features": "In the optimization process, two distinct prediction models were constructed using different sets of input features.\n\nFor the first prediction model, a total of 18 variables were used as input features. These variables included demographic information, time interval for TB detection, duration of medication, CD4+ T cell count, CD8+ T cell count, and CD4+/CD8+ ratio. Feature selection was performed to exclude variables that contained only one category or had fewer than five non-zero recorded values, ensuring that only relevant and informative features were included.\n\nFor the second prediction model, 32 variables were selected as input features. These variables were chosen based on having more than 500 records and excluding those with only one category or fewer than five non-zero recorded values. This selection process aimed to include a broader range of features while maintaining data quality.\n\nIn both cases, feature selection was conducted using the training set only, ensuring that the models were evaluated on unseen data during the testing phase. This approach helps to prevent overfitting and ensures that the models generalize well to new, unseen data.",
  "optimization/fitting": "In our study, we constructed two multifactor models to predict patients' TB treatment effectiveness. For the first model, we started with 547 samples and excluded variables with insufficient categories or values, resulting in 347 samples with 18 variables. For the second model, we selected 169 variables with more than 500 records, excluding those with insufficient categories, resulting in 527 samples with 32 variables.\n\nWe employed logistic regression (LR), support vector machine (SVM), and random forest (RF) algorithms for model construction. To evaluate model performance, we used five-fold cross-validation and assessed metrics such as accuracy, precision, recall, specificity, F1 score, and the area under the receiver operating characteristic curve (AUC).\n\nOverfitting was a concern, particularly with the RF models, which showed serious overfitting in the training sets. To mitigate this, we selected the training set samples corresponding to the test set with the highest prediction accuracy. This approach helped ensure that our models generalized well to unseen data. Additionally, we compared the performance of different algorithms and selected the most accurate and reliable one for further analysis.\n\nUnderfitting was addressed by ensuring that our models were complex enough to capture the underlying patterns in the data. We used stepwise regression for the LR models to include only the most significant variables, balancing model complexity and performance. For the RF models, we evaluated variable importance to understand which features contributed most to the predictions.\n\nThe best AUC values for the test sets varied across the models and algorithms. For the first model, SVM achieved the highest AUC of 0.763, followed by LR at 0.659 and RF at 0.653. For the second model, SVM again led with an AUC of 0.686, with RF at 0.650 and LR at 0.560. These results indicate that while our models showed average prediction effectiveness, they were not overly simplistic or complex, striking a balance between bias and variance.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the primary methods used was five-fold cross-validation. This technique involves dividing the dataset into five subsets, training the model on four of these subsets, and validating it on the remaining subset. This process is repeated five times, with each subset serving as the validation set once. By doing so, we ensured that our models were evaluated on different portions of the data, reducing the risk of overfitting to any single subset.\n\nAdditionally, we constructed two multifactor models using factors that showed little consistency with univariate analysis results. This approach helped in identifying variables that contributed significantly to the model prediction, thereby enhancing the model's generalizability.\n\nFurthermore, we used stepwise regression for constructing logistic regression models. Stepwise regression is a method that involves adding or removing variables from the model based on predefined criteria, such as the significance of the variables. This method helps in selecting the most relevant variables, reducing the complexity of the model and preventing overfitting.\n\nWe also drew the ROC curves of the train and test sets to evaluate the performance of our models. The ROC curve of the train set showed serious overfittings in random forest modeling, indicating that this model was too rigid and did not generalize well to the test data. This observation highlighted the importance of using regularization techniques to prevent overfitting.\n\nIn summary, we employed five-fold cross-validation, multifactor modeling, stepwise regression, and ROC curve analysis to prevent overfitting and ensure the robustness of our models. These techniques helped in selecting the most relevant variables, reducing the complexity of the models, and enhancing their generalizability.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in this study, particularly the Logistic Regression (LR) models, offer a degree of transparency that makes them interpretable. Unlike black-box models, LR models provide clear insights into the relationship between the input variables and the predicted outcome. This transparency is achieved through the coefficient parameters associated with each variable in the model.\n\nFor instance, in the LR prediction model I, variables such as \"age\" and \"CD8+ T cell count\" were found to have significant positive contributions to the prediction. This means that as these variables increase, the likelihood of the predicted outcome (e.g., treatment effectiveness) also increases. Similarly, in the LR prediction model II, variables like \"time interval for TB detection,\" \"rifabutin,\" and \"sodium aminosalicylate\" showed positive correlations, indicating that higher values of these variables are associated with better treatment outcomes. Conversely, variables like \"cycloserine\" and \"neutrophil percentage\" had negative correlations, suggesting that higher values of these variables are linked to less effective treatment.\n\nThis interpretability allows clinicians and researchers to understand which factors are most influential in predicting treatment effectiveness, thereby aiding in the development of more targeted and effective treatment strategies. However, it is important to note that while LR models provide this level of interpretability, they may still be too rigid to fully capture the complex relationships between clinical indicators, drug regimens, and treatment effectiveness. Further research and validation are needed to improve the predictive performance and interpretability of these models.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the effectiveness of tuberculosis (TB) treatment based on various clinical indicators and drug regimens. The model uses three different machine learning algorithms: Logistic Regression (LR), Support Vector Machine (SVM), and Random Forest (RF). The performance of these models was evaluated using several metrics, including accuracy, precision, recall, specificity, F1 score, and the Area Under the Curve (AUC) for the Receiver Operating Characteristic (ROC) curve. The results indicate that the models have varying levels of predictive accuracy, with Logistic Regression generally performing better than the other two algorithms in terms of accuracy. However, the best AUC values for the test sets were observed with SVM, followed by RF and then LR. The models were constructed using two different sets of variables, leading to Prediction Model I and Prediction Model II. Despite the efforts to optimize the models, the predictive performance was found to be average, with the best AUC values being less than 0.7. This suggests that while the models have some predictive capability, there is room for improvement in accurately forecasting TB treatment effectiveness.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the prediction models involved several key steps and metrics to ensure robustness and reliability. We employed five-fold cross-validation to assess the models' performance, which involved splitting the data into five subsets, training the model on four subsets, and testing it on the remaining subset. This process was repeated five times, with each subset serving as the test set once.\n\nTo evaluate the models comprehensively, we used multiple performance metrics. These included accuracy, precision, recall, specificity, F1 score, and the area under the receiver operating characteristic curve (AUC). The results were reported as mean \u00b1 standard deviation, obtained by randomly splitting the train and test sets five times. This approach provided a thorough assessment of the models' performance and variability.\n\nFor each algorithm\u2014logistic regression (LR), support vector machine (SVM), and random forest (RF)\u2014we constructed two prediction models (Model I and Model II). Model I was built using a subset of variables that showed little consistency with univariate analysis results, while Model II included a broader set of variables with more than 500 records. The best AUC values for the test sets under the three algorithms were compared to determine the most effective model.\n\nAdditionally, we analyzed the ROC curves of the train and test sets to identify any overfitting issues, particularly in the RF models. The coefficient parameters of the LR models were determined using stepwise regression, and the importance ranking of variables in the RF models was also considered.\n\nOverall, the evaluation method ensured that the models were rigorously tested and that their performance was thoroughly assessed using multiple metrics and validation techniques.",
  "evaluation/measure": "In our evaluation, we reported several key performance metrics to assess the effectiveness of our prediction models. These metrics include accuracy, precision, recall, specificity, F1 score, and the area under the curve (AUC). These metrics were chosen to provide a comprehensive evaluation of the models' performance across different aspects.\n\nAccuracy measures the overall correctness of the model's predictions, while precision and recall focus on the model's performance in predicting the positive class. Specificity evaluates the model's ability to correctly identify negative instances. The F1 score provides a balance between precision and recall, and the AUC assesses the model's ability to distinguish between positive and negative classes across all threshold levels.\n\nThe reported metrics are representative of standard practices in the literature for evaluating machine learning models, particularly in the context of medical and biological data. These metrics allow for a thorough assessment of the models' strengths and weaknesses, providing insights into their potential applicability in real-world scenarios. The use of these metrics ensures that our evaluation is robust and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we compared the performance of three machine learning algorithms\u2014Logistic Regression (LR), Support Vector Machine (SVM), and Random Forest (RF)\u2014to evaluate their effectiveness in predicting TB treatment outcomes. We constructed two prediction models, Model I and Model II, using these algorithms.\n\nFor Model I, we used a dataset of 347 samples with 18 variables, while for Model II, we used a larger dataset of 527 samples with 32 variables. Both models were evaluated using five-fold cross-validation to ensure robustness.\n\nThe performance of these models was assessed using several metrics, including accuracy, precision, recall, specificity, F1 score, and the Area Under the Curve (AUC). The results showed that while all three algorithms had varying degrees of success, LR consistently outperformed SVM and RF in terms of accuracy for both models. However, RF exhibited serious overfitting, particularly in the training sets.\n\nThe best AUC values for the test sets were observed with SVM for both models, followed by RF and then LR. This indicates that while SVM and RF might capture more complex patterns in the data, LR provides a more reliable and generalizable prediction model.\n\nAdditionally, we explored the importance of different variables in the models. For LR, variables such as \"age\" and \"CD8+ T cell count\" were significant contributors to prediction in Model I, while \"time interval for TB detection,\" \"rifabutin,\" \"sodium aminosalicylate,\" \"cycloserine,\" and \"neutrophil percentage\" were important in Model II. In the RF model, \"cycloserine\" and \"sodium para-aminosalicylate\" were the top contributors.\n\nWe also reconstructed univariate models using the LR algorithm with variables that showed significant contributions in the multivariate models. However, these univariate models did not lead to improvements in performance compared to the multivariate LR models.\n\nIn summary, our comparison of these machine learning algorithms highlights the strengths and weaknesses of each method in predicting TB treatment effectiveness. LR emerged as the best choice for creating a reliable prediction model for this dataset, although the overall prediction effect was average, with the best AUC values for the models being less than 0.7. This suggests that while these models provide valuable insights, there is still room for improvement in accurately predicting TB treatment outcomes.",
  "evaluation/confidence": "The evaluation of the prediction models in this study was conducted using several performance metrics, including accuracy, precision, recall, specificity, F1 score, and the area under the receiver operating characteristic curve (AUC). These metrics were reported with mean values and standard deviations, which provide a measure of confidence in the results. The standard deviations indicate the variability of the metrics across different splits of the data, offering insight into the reliability of the performance estimates.\n\nStatistical significance was assessed using Fisher\u2019s exact test and multivariable logistic regression. The p-values from these tests help determine whether the observed differences in treatment effectiveness between groups are likely due to chance or represent true differences. Significant p-values (typically \u2264 0.05) indicate that the results are statistically significant, providing confidence that the findings are robust and not due to random variation.\n\nThe study also employed five-fold cross-validation to ensure that the models were evaluated on diverse subsets of the data, reducing the risk of overfitting and providing a more generalizable assessment of model performance. The best AUC values for the test sets under the three algorithms were reported, with Support Vector Machine (SVM) achieving the highest AUC of 0.686, followed by Random Forest (RF) at 0.650, and Logistic Regression (LR) at 0.560. These values, along with the associated confidence intervals, help in understanding the relative performance and reliability of each algorithm.\n\nOverall, the inclusion of confidence intervals and statistical significance testing enhances the credibility of the evaluation results, allowing for a more informed assessment of the models' performance and their potential superiority over other methods.",
  "evaluation/availability": "Not enough information is available."
}