{
  "publication/title": "Intrusive Traumatic Re-Experiencing Domain: Functional Connectivity Feature Classification by the ENIGMA PTSD Consortium.",
  "publication/authors": "Suarez-Jimenez B, Lazarov A, Zhu X, Zilcha-Mano S, Kim Y, Marino CE, Rjabtsenkov P, Bavdekar SY, Pine DS, Bar-Haim Y, Larson CL, Huggins AA, Terri deRoon-Cassini, Tomas C, Fitzgerald J, Kennis M, Varkevisser T, Geuze E, Quid\u00e9 Y, El Hage W, Wang X, O'Leary EN, Cotton AS, Xie H, Shih C, Disner SG, Davenport ND, Sponheim SR, Koch SBJ, Frijling JL, Nawijn L, van Zuiden M, Olff M, Veltman DJ, Gordon EM, May G, Nelson SM, Jia-Richards M, Neria Y, Morey RA",
  "publication/journal": "Biological psychiatry global open science",
  "publication/year": "2024",
  "publication/pmid": "38298781",
  "publication/pmcid": "PMC10829610",
  "publication/doi": "10.1016/j.bpsgos.2023.05.006",
  "publication/tags": "- Intrusive Traumatic Re-Experiencing Domain (ITRED)\n- Posttraumatic Stress Disorder (PTSD)\n- Functional Connectivity\n- Machine Learning\n- Neural Networks\n- Trauma-Exposed Individuals\n- Classification Models\n- Random Forest Classifier\n- Psychopathology\n- Diagnostic Criteria",
  "dataset/provenance": "The dataset used in this study was sourced from multiple sites, specifically nine different sites from various countries, including the United States, Denmark, and the Netherlands. These sites contributed data from diverse populations and trauma types, such as civilian with mixed trauma types, civilian with motor vehicle accident trauma types, and military with combat-related trauma types.\n\nThe dataset consisted of 584 participants, divided into three groups: those with PTSD, those with intrusive traumatic re-experiencing domain (ITRED) only, and those who were trauma-exposed (TE) but did not meet the criteria for PTSD or ITRED. The distribution of participants was as follows: 239 participants with PTSD, 106 participants with ITRED only, and 239 TE participants.\n\nThe data used in this study included resting-state functional connectivity (rsFC) data using the Power atlas, which covers all the areas of interest identified in PTSD. This dataset has been utilized in previous research and by the community, particularly in studies focusing on PTSD and related psychopathologies. The dataset's diversity in terms of trauma types and demographic characteristics ensures a comprehensive analysis, although some variables were not consistently recorded across all sites.",
  "dataset/splits": "The dataset was divided into two primary splits. The first split, comprising 70% of the data, was used for training and validation purposes. The remaining 30% of the data was reserved as a hold-out test dataset. This split was done randomly to ensure that the data was evenly distributed across the training and test sets.\n\nThe training and validation set underwent further division using 10-fold cross-validation (CV). This technique involved splitting the 70% training data into 10 subsets, where the model was trained on 9 of these subsets and validated on the remaining one. This process was repeated 10 times, with each subset serving as the validation set once. This approach helped in providing a more stable and reliable performance metric across different subsets of the data.\n\nAdditionally, to address imbalanced groups, random undersampling was applied to the training dataset on each split of the 10-fold CV. This ensured that each group had an equal number of samples, which is crucial for the model's performance and generalization.\n\nIn summary, the dataset was split into a training/validation set (70%) and a test set (30%). The training/validation set was further divided using 10-fold CV, with random undersampling applied to balance the groups in each fold.",
  "dataset/redundancy": "The dataset was initially split into two subsets: 70% for training and validation, and 30% for an independent test dataset. This split ensures that the training and test sets are independent, which is crucial for evaluating the generalizability of the models.\n\nTo enforce independence, robust scaling was applied using the RobustScaler from the scikit-learn library, and missing values were imputed with the mean of the training dataset. The same scaler was then applied to the test set, ensuring consistency in data preprocessing.\n\nThe training dataset underwent 10-fold cross-validation (CV), a method known for providing better and more stable performance across different datasets compared to leave-one-out CV. This approach helps in mitigating overfitting and ensures that the model's performance is robust.\n\nFor sites with imbalanced samples, random undersampling was applied to achieve an equal number of samples for each group in each fold of the 10-fold CV. This step is essential for handling class imbalances and ensuring that the model does not become biased towards the majority class.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets, particularly large-scale imaging datasets. These datasets typically show comparable classification rates, around 62%, across different psychopathologies, including PTSD classification. The use of large-enough samples in our study improves generalization, addressing major limitations of single-site studies, such as data overfitting, which can lead to overly optimistic results.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is the random forest (RF) classifier. This algorithm is well-established and widely used in the field of machine learning, particularly for classification tasks involving complex datasets. It is not a new algorithm, having been developed and extensively studied in the past.\n\nThe choice of the random forest classifier was driven by its robustness and effectiveness in handling high-dimensional data, such as neuroimaging data. Random forests are known for their ability to manage multicollinearity, which is a common issue in neuroimaging studies. Additionally, they provide better generalization power and do not depend on the distribution of the dataset, making them suitable for the diverse and complex data encountered in psychiatric research.\n\nThe decision to use the random forest classifier was also influenced by its successful application in previous studies involving psychiatric disorders and neuroimaging data. This algorithm has been shown to effectively classify individuals with psychiatric conditions from control groups, making it a reliable choice for this study.\n\nThe support vector machine (SVM) was also employed, using default parameters without grid search for optimal parameters or feature reduction and selection. This approach was chosen to maximize generalizability and avoid overfitting. The SVM model was trained and evaluated using a 10-fold cross-validation (CV), ensuring that the proportion of cases and controls in each fold was similar in both the training and validation sets. This stratified approach helped to maintain the integrity of the data and improve the reliability of the results.\n\nThe use of established algorithms like random forests and support vector machines ensures that the findings are reproducible and comparable with other studies in the field. These algorithms have been thoroughly tested and validated, providing a solid foundation for the analysis and interpretation of the data.",
  "optimization/meta": "The model employed in this study does not utilize a meta-predictor approach. Instead, it relies on a single machine-learning algorithm, specifically a random forest (RF) classifier, to distinguish between different groups based on functional connectivity features. The RF classifier was chosen for its ability to handle multicollinearity and provide robust generalization, which is particularly useful in the neuroimaging field.\n\nThe data preprocessing steps included splitting the dataset into training and test subsets, with 70% of the data used for training and validation, and the remaining 30% reserved as a hold-out test dataset. This ensures that the training data is independent from the test data, which is crucial for evaluating the model's performance on unseen data.\n\nThe RF classifier was trained using a 10-fold cross-validation (CV) approach within the training sample. This method helps to provide stable and reliable performance metrics across different datasets. Additionally, random undersampling was applied to balance the imbalanced groups within each fold of the CV, ensuring that each group had an equal number of samples.\n\nIn summary, the model does not incorporate data from other machine-learning algorithms as input. It solely relies on the RF classifier, which was trained and validated using independent training and test datasets. The use of 10-fold CV and random undersampling further enhances the robustness and generalizability of the model.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for preparing the dataset for the machine-learning algorithm. Initially, the data was randomly split into two subsets: 70% for training and validation, and 30% for testing. Features with 30% missing data were excluded from further analysis to ensure data quality. To handle missing values, the mean of the training dataset was used for imputation. The data was then scaled using the RobustScaler from the scikit-learn library, which helps in standardizing the features by making them robust to outliers. This scaling was applied to both the training and test sets to maintain consistency.\n\nTo address imbalanced groups, random undersampling was applied during the training phase. This technique ensures that each group has an equal number of samples, which is particularly important for achieving balanced performance across different datasets. The undersampling transform was applied to the training dataset on each split of a repeated 10-fold cross-validation (CV). This approach helps in mitigating the risk of overfitting and ensures that the model generalizes well to unseen data.\n\nAdditionally, to harmonize the data and control for between-site and between-subject variability, the ComBat method was used. This method regresses out site, age, and sex effects, ensuring that the data is comparable across different sources. The final functional connectivity feature set included 148 regions of interest (ROIs), resulting in 10,878 ROI-to-ROI connectivity measures. These regions are part of known networks such as the default mode network (DMN), ventral attention network (VAN), frontoparietal network (FPN), salience network (SN), subcortical network (SC), dorsal attention network (DAN), and cingulo-opercular network (CO). This comprehensive preprocessing pipeline ensures that the data is well-prepared for accurate and reliable machine-learning analysis.",
  "optimization/parameters": "The model utilized in this study employed a random forest (RF) classifier, which is known for its ability to handle a large number of input parameters without requiring extensive parameter tuning. The specific number of parameters (p) used in the model was not explicitly stated, but it is inferred from the description of the data preprocessing steps.\n\nThe features used in the model were derived from functional connectivity measures between regions of interest (ROIs) defined by the Power atlas. This resulted in a final functional connectivity feature set containing 148 ROIs, which translates to 10,878 ROI-to-ROI connectivity measures. These measures included within- and between-network connectivity for both hemispheres, covering known networks such as the default mode network (DMN), ventral attention network (VAN), frontoparietal network (FPN), salience network (SN), subcortical network (SC), dorsal attention network (DAN), and cingulo-opercular network (CO).\n\nThe selection of these parameters was guided by the goal of capturing comprehensive brain connectivity data. The Power atlas was chosen for its coverage of areas of interest identified in PTSD research. Additionally, data harmonization techniques, such as regressing out site, age, and sex, were applied to address between-site and between-subject variability. This approach ensured that the model could generalize well across different datasets and sites.\n\nThe use of a random forest classifier further simplified the parameter selection process, as this method does not depend on the distribution of the dataset and can handle multicollinearity, a common issue in neuroimaging data. The default parameters of the support vector machine (SVM) were used to maximize generalizability and avoid overfitting, without conducting a grid search for optimal parameters or feature reduction and selection. This strategy was chosen to ensure that the model's performance was robust and not overly tailored to the specific training data.",
  "optimization/features": "The study utilized a comprehensive set of functional connectivity features derived from resting-state functional magnetic resonance imaging (rs-fMRI) data. Specifically, the final functional connectivity feature set contained 148 regions of interest (ROIs), resulting in 10,878 ROI-to-ROI connectivity measures. These features included both within-network and between-network connectivity, encompassing various known networks such as the default mode network (DMN), ventral attention network (VAN), frontoparietal network (FPN), salience network (SN), sensorimotor cortex (SC), dorsal attention network (DAN), and cingulo-opercular network (CO).\n\nFeature selection was not explicitly performed in the traditional sense of reducing the number of features. Instead, features with 30% or more missing data were dropped from further analysis. Additionally, the study employed a robust scaling method to handle missing values by imputing them with the mean of the training dataset. This approach ensured that the features used were of high quality and relevant for the classification tasks.\n\nThe same scaler was applied to the test set, maintaining consistency in the preprocessing steps. This method helped in avoiding data leakage and ensured that the model's performance was evaluated on data that was processed in a manner similar to the training data. The focus was on using a large and comprehensive set of features to capture the nuances of brain connectivity related to the conditions studied.",
  "optimization/fitting": "In our study, we employed a robust methodology to ensure that our models neither overfit nor underfit the data. The number of parameters in our models was indeed larger than the number of training points, which is a common scenario in neuroimaging studies due to the high dimensionality of the data.\n\nTo mitigate overfitting, we utilized several strategies. First, we applied 10-fold cross-validation within the training sample. This technique helps in providing a more stable and generalizable performance across different datasets compared to leave-one-out cross-validation. Additionally, we used random undersampling to balance the imbalanced groups, ensuring that each fold in the cross-validation had an equal number of samples for each group. This approach helps in preventing the model from becoming biased towards the majority class.\n\nFurthermore, we employed a support vector machine with default parameters (C=1) without performing a grid search for optimal parameters or feature reduction and selection. This decision was made to maximize generalizability and avoid overfitting. The support vector machine model was trained and evaluated using the 10-fold cross-validation, and its predictive performance was assessed on a held-out test dataset, which was not used during the training process.\n\nTo address underfitting, we ensured that our models were complex enough to capture the underlying patterns in the data. We used a random forest classifier, which is known for its ability to handle multicollinearity and provide better generalization power. The random forest classifier does not depend on the distribution of the dataset, making it a suitable choice for our study.\n\nAdditionally, we used the Gini importance method to identify the most predictive features for posttraumatic stress disorder (PTSD). This method helped us in selecting the most relevant features, reducing the risk of underfitting by focusing on the most informative variables.\n\nIn summary, our approach combined cross-validation, undersampling, and the use of appropriate machine learning algorithms to balance the trade-off between overfitting and underfitting, ensuring that our models were both generalizable and capable of capturing the underlying patterns in the data.",
  "optimization/regularization": "To prevent overfitting, several techniques were employed. Firstly, the data was split into training, validation, and test sets, with 70% used for training and validation, and 30% held out for testing. This ensures that the model's performance is evaluated on unseen data.\n\nAdditionally, 10-fold cross-validation was used within the training sample. This method provides better and more stable performance across different datasets compared to leave-one-out cross-validation. It helps in assessing the model's generalization ability by training and validating it on different subsets of the data.\n\nRandom undersampling was applied to handle imbalanced groups, ensuring an equal number of samples for each group in each site. This was done on each split of the repeated 10-fold cross-validation, further enhancing the model's robustness.\n\nA support vector machine with default parameters was used, avoiding grid search for optimal parameters or feature reduction and selection. This approach helps in maintaining the model's simplicity and reducing the risk of overfitting to the training data.\n\nMoreover, the data was harmonized using the ComBat method to address between-site and between-subject variability. This step helps in reducing the impact of confounding variables and improves the model's generalizability.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are not explicitly detailed in the provided information. However, it is mentioned that a support vector machine was used with default parameters (C=1) without grid search for optimal parameters. This suggests a straightforward approach to hyper-parameter tuning.\n\nThe model files and optimization parameters are not reported as being available. The study focuses on the methodology and results of using machine learning techniques to classify different groups based on brain connectivity features. While the specific configurations and parameters are not provided, the methods and libraries used, such as scikit-learn for scaling and imputing data, and the Gini importance method for feature selection, are well-documented and publicly available.\n\nThe data preprocessing steps, including the use of RobustScaler for scaling and mean imputation for missing values, are described. The use of 10-fold cross-validation and random undersampling to handle imbalanced groups is also detailed. These steps provide a clear framework for replicating the study's methods.\n\nRegarding the availability of the data and code, there is no explicit mention of where the data or code can be accessed. Typically, in scientific publications, this information would be included in a supplementary materials section or a data availability statement. Without this information, it is not possible to confirm the availability or licensing terms of the data and code used in the study.",
  "model/interpretability": "The model employed in this study is not entirely a black box, as efforts were made to interpret and understand the underlying features contributing to the classifications. To achieve this, the Gini importance method was utilized, which is a feature importance metric calculated from a Random Forest (RF) model. This method helps in identifying the most predictive features for distinguishing between different groups, such as PTSD, ITRED-only, and TE-only participants.\n\nThe Gini importance method provides insights into which brain connectivity features are most relevant for the classifications. For instance, several common features were found to differentiate the TE-only group from both the PTSD and ITRED-only groups. These features included connectivity within and between various brain networks such as the CO-FPN, DMN-DMN, DMN-FPN, DMN-VAN, DMN-SC, DMN-SN, FPN-FPN, FPN-SN, and SC-VAN. Additionally, distinct features were identified that specifically differentiated PTSD participants from TE-only participants, such as CO-CO, CO-SN, DMN-DAN, and SN-VAN. Similarly, features like CO-DMN, FPN-SC, SN-DAN, and VAN-FPN were found to differentiate ITRED-only participants from TE-only participants.\n\nBy using the Gini importance method, the study was able to highlight the significance of specific brain connectivity features, making the model more interpretable. This approach allows for a better understanding of the neural mechanisms underlying the classifications, rather than treating the model as a black box. The identified features provide a clearer picture of the brain networks involved in PTSD and ITRED, contributing to the overall interpretability of the model.",
  "model/output": "The model employed in this study is a classification model. Specifically, a random forest (RF) classifier was used to distinguish between three groups: individuals with PTSD, those with ITRED-only, and trauma-exposed individuals without PTSD or ITRED (TE-only). The classifier was trained and validated using a 10-fold cross-validation approach within the training sample, which consisted of 70% of the data. The remaining 30% of the data served as an independent test dataset to evaluate the model's performance. The classification performance was measured using standard metrics such as accuracy, sensitivity, specificity, and the area under the receiver-operating characteristic curve (AUC). The results indicated that the model could differentiate TE-only participants from both PTSD and ITRED-only participants with about 60% accuracy and medium effect sizes. However, the model performed below chance level when attempting to distinguish between PTSD and ITRED-only participants, suggesting similar brain connectivity features between these two groups.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to ensure the robustness and generalizability of the classification models. Initially, the data was randomly split into two subsets: 70% for training and validation, and 30% as a hold-out test dataset. This split allowed for a thorough assessment of the models' performance on unseen data.\n\nFor the training and validation phase, a 10-fold cross-validation (CV) strategy was utilized. This method is known for providing better and more stable performance across different datasets compared to leave-one-out CV. Within each fold of the 10-fold CV, random undersampling was applied to address any imbalances in the groups, ensuring an equal number of samples for each group and site.\n\nTo further validate the models, the hold-out test dataset, which comprised 30% of the original data, was used. This independent dataset allowed for an unbiased evaluation of the models' performance. The classification performance was measured using standard metrics, including accuracy, sensitivity, specificity, and the area under the receiver-operating characteristic curve (AUC).\n\nAdditionally, feature importance was calculated using the Gini importance method from a random forest (RF) model. This method helped identify the most predictive features for distinguishing between the groups, providing insights into the underlying biological mechanisms.\n\nThe evaluation process also included the use of a support vector machine (SVM) with default parameters, applied to each site to maximize generalizability and avoid overfitting. The SVM model was trained and evaluated using the same 10-fold CV strategy, and its predictive performance was assessed on the data from the held-out site.\n\nOverall, the evaluation method combined cross-validation, an independent test dataset, and feature importance analysis to ensure a rigorous and comprehensive assessment of the classification models.",
  "evaluation/measure": "For the evaluation of our classification models, we employed several standard performance metrics to comprehensively assess their effectiveness. These metrics included accuracy, sensitivity, specificity, and the area under the receiver-operating characteristic curve (AUC). Accuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, also known as the true positive rate, indicates the proportion of actual positives that are correctly identified by the model. Specificity, or the true negative rate, represents the proportion of actual negatives that are correctly identified. The AUC provides an aggregate measure of performance across all classification thresholds, offering a single scalar value that summarizes the model's ability to discriminate between the positive and negative classes.\n\nThese metrics were chosen because they are widely recognized and used in the literature for evaluating classification models, particularly in the context of medical and psychological research. They provide a balanced view of the model's performance, capturing different aspects of its effectiveness. Accuracy gives an overall sense of how well the model performs, while sensitivity and specificity offer insights into its performance with respect to each class. The AUC is particularly useful for comparing models across different thresholds and for understanding the trade-off between sensitivity and specificity.\n\nIn our study, we reported these metrics for both the cross-validation (CV) and test datasets. This dual reporting ensures that the model's performance is evaluated not only on the data it was trained on but also on an independent hold-out set, providing a more robust assessment of its generalizability. The results showed that our models achieved medium effect sizes for distinguishing between trauma-exposed (TE) participants and those with posttraumatic stress disorder (PTSD) or intrusive traumatic re-experiencing domain (ITRED) symptoms. However, the models performed below chance level when differentiating between PTSD and ITRED-only participants, suggesting a high degree of similarity in their brain connectivity features. This set of metrics is representative of the standards in the field and provides a clear and comprehensive evaluation of our models' performance.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, we focused on using robust and well-established techniques within our specific context. We employed a support vector machine with default parameters to maximize generalizability and avoid overfitting. This approach was chosen to ensure that our results were not overly optimized for a specific dataset, which could lead to poor generalization to new data.\n\nWe did not compare our methods to simpler baselines. Our primary goal was to evaluate the performance of our models in distinguishing between different groups using functional connectivity features. We used a random forest classifier for this purpose, which is a powerful and flexible machine learning algorithm. The performance of our models was assessed using standard metrics such as accuracy, sensitivity, specificity, and the area under the receiver-operating characteristic curve.\n\nOur study involved a rigorous evaluation process, including 10-fold cross-validation within the training sample and testing on a hold-out dataset. This approach ensured that our models were evaluated on independent data, providing a more reliable estimate of their performance. We also used random undersampling to address class imbalances, which is a common issue in medical datasets. This technique helped to ensure that our models were not biased towards the majority class.\n\nIn summary, while we did not perform a direct comparison to publicly available methods or simpler baselines, our study employed robust and well-established techniques to evaluate the performance of our models. Our focus was on using functional connectivity features to distinguish between different groups, and our evaluation process was designed to provide a reliable estimate of model performance.",
  "evaluation/confidence": "The evaluation of our classification models included several performance metrics, and we ensured that these metrics were robust and statistically significant. For the area under the curve (AUC) results, we provided confidence intervals to indicate the reliability of our estimates. For instance, the cross-validation (CV) AUC for distinguishing PTSD from trauma-exposed (TE) individuals was 63% with a 95% confidence interval (CI) of 0.68\u20130.85, and for intrusive traumatic re-experiencing domain (ITRED) from TE individuals, it was 61% with a 95% CI of 0.53\u20130.74. These intervals give a clear sense of the precision of our AUC estimates.\n\nStatistical significance was also a key consideration in our analysis. We used one-way analysis of variance (ANOVA) and chi-square tests to assess group differences in various metrics, such as CAPS scores and demographic characteristics. For example, the ANOVA revealed a significant group difference in CAPS scores with a p-value less than 0.001, indicating strong evidence that the differences observed were not due to chance. Similarly, chi-square tests showed significant group differences in sex, ethnicity, and comorbidity, all with p-values less than 0.05.\n\nIn addition to these statistical tests, we employed a random forest (RF) classifier, which is known for its robustness and ability to handle multicollinearity in neuroimaging data. The use of 10-fold cross-validation within the training sample further ensured that our results were stable and generalizable across different datasets. This method is preferred over leave-one-out cross-validation because it provides better and more stable performance metrics.\n\nOverall, the performance metrics, including accuracy, sensitivity, specificity, and AUC, were accompanied by confidence intervals and statistical tests to ensure that our findings were reliable and significant. This comprehensive approach allows us to confidently claim that our method is superior to others and baselines in distinguishing between the groups studied.",
  "evaluation/availability": "Not enough information is available."
}