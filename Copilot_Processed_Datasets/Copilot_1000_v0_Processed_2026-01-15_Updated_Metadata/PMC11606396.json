{
  "publication/title": "Analysis of the Predictors of Mortality from Ischemic Heart Diseases in the Southern Region of Brazil: A Geographic Machine-Learning-Based Study.",
  "publication/authors": "de Carvalho Dutra A, Silva LL, Borba IM, Dos Santos AGA, Marquezoni DP, Beltrame MHA, do Lago Franco R, Hatoum US, Miyoshi JH, Leandro GCW, Bitencourt MR, Nihei OK, Vissoci JRN, de Andrade L",
  "publication/journal": "Global heart",
  "publication/year": "2024",
  "publication/pmid": "39619634",
  "publication/pmcid": "PMC11606396",
  "publication/doi": "10.5334/gh.1371",
  "publication/tags": "- Myocardial Ischemia\n- Spatial Analysis\n- Supervised Machine Learning\n- Epidemiology\n- Ischemic Heart Disease\n- Geospatial Analysis\n- Mortality Predictors\n- Health Data\n- Geographic Machine Learning\n- Public Health",
  "dataset/provenance": "The dataset utilized in this study is derived from various sources to ensure comprehensive and accurate analysis. The primary outcome variable, the mortality rate due to ischemic heart disease (IHD), was calculated using data from the Mortality Information System (SIM). This system provided the number of deaths due to IHD in individuals aged 30 to 70 years in the municipalities of the southern region of Brazil from 2018 to 2022. The deaths were classified under codes I20 to I25 of the International Statistical Classification of Diseases and Related Health Problems (ICD-10).\n\nThe population data for the same period was obtained from another source, ensuring that the mortality rates were accurately normalized. Independent variables related to health accessibility and procedures were created using rates to normalize the data. For instance, the rate of cardiologists was calculated by dividing the number of cardiologists per 1,000 inhabitants. Similarly, the number of electrocardiographs and various medical procedures were normalized per 10,000 inhabitants.\n\nThe variables for accessibility to hemodynamics and pre-hospital mobile units were created using the Enhanced Two-Step Floating Catchment Area (E2SFCA) method. This method calculates the ratio of available health services to the population in a given area, adjusted by geographic distance, resulting in an index that incorporates various factors.\n\nThe dataset includes a shapefile from southern Brazil, covering 1,191 municipalities. This region had an estimated population of approximately 29.9 million inhabitants in 2022, distributed over a total area of 576,774 square kilometers. The data spans from 2018 to 2022, providing a robust temporal dimension for analysis.\n\nFor the Random Forest (RF) and Geographically Weighted Random Forest (GWRF) analyses, the outcome variable was divided into three datasets: mortality rates from 2020 were used for training, 2021 for validation, and 2022 for testing. This approach ensures that the models are trained, validated, and tested on different temporal slices of the data, enhancing the reliability of the results.\n\nThe methodological quality of the study was guaranteed by following the Transparent Reporting of a multivariable prediction model for Individual Prognosis or Diagnosis (TRIPOD) recommendations. This ensures that the reporting of the prediction model is transparent and reproducible, adhering to high standards of scientific rigor.",
  "dataset/splits": "In our study, the data was divided into three distinct sets for the analyses using Random Forest (RF) and Geographically Weighted Random Forest (GWRF). These splits were designed to maintain the same variables across all sets and correspond to complete years.\n\nThe training set utilized data from the year 2020. The validation set comprised data from the year 2021. Finally, the test set included data from the year 2022. This approach ensured that each set was independent and representative of different time periods, allowing for robust model training, validation, and testing.\n\nFor the RF model, a 10-part cross-validation method was employed to determine the best hyperparameter combinations across 200 models. This method helps in selecting the most effective model by evaluating its performance on multiple subsets of the data.\n\nIn the case of the GWRF model, the RF model with the lowest Root Mean Squared Error (RMSE) in the validation dataset was selected. This selection criterion ensures that the chosen model generalizes well to unseen data, providing reliable predictions.\n\nThe distribution of data points in each split reflects the annual mortality rates due to ischemic heart disease (IHD) in the southern region of Brazil. The training set covered the year 2020, the validation set covered the year 2021, and the test set covered the year 2022. This temporal separation helps in assessing the model's performance over different time periods, ensuring its robustness and reliability.",
  "dataset/redundancy": "The datasets used in this study were divided into three distinct sets, each corresponding to complete years, ensuring that the same variables were maintained across all sets. Specifically, the data from the year 2020 were used for the training set, the data from the year 2021 were used for the validation set, and the data from the year 2022 were used for the test set. This approach ensures that the training and test sets are independent, as they come from different time periods.\n\nTo enforce the independence of the datasets, the data were split chronologically. This method helps to simulate a real-world scenario where the model is trained on past data and then tested on future, unseen data. This temporal split is crucial for evaluating the model's predictive performance and generalizability.\n\nComparing this dataset split to previously published machine learning datasets, the approach aligns with best practices in time-series analysis and predictive modeling. Many studies, especially in the field of healthcare and epidemiology, use temporal splits to ensure that the model's performance is evaluated on data that was not available during the training phase. This method helps to mitigate overfitting and provides a more robust evaluation of the model's predictive capabilities.",
  "dataset/availability": "The data used in this study is not publicly released in a forum. The dataset consists of mortality rates due to ischemic heart disease (IHD) and various independent variables related to health accessibility and procedures in the southern region of Brazil. The mortality data was obtained from the Mortality Information System (SIM), considering deaths classified under codes I20 to I25 of the International Statistical Classification of Diseases and Related Health Problems (ICD-10). The independent variables were created using rates to normalize the data, allowing for a better understanding of the proportions of the procedures. The variables include rates of cardiologists, electrocardiographs, cardiac catheterization, echocardiography, exercise stress test, and myocardial scintigraphy, as well as accessibility indices to hemodynamics and pre-hospital mobile units calculated using the Enhanced Two-Step Floating Catchment Area (E2SFCA) method. The data was divided into three distinct sets for the analyses using Random Forest (RF) and Geographically Weighted Random Forest (GWRF): the training set used data from the year 2020, the validation set used data from the year 2021, and the test set used data from the year 2022. The data splits were maintained consistently across all sets. The study ensured methodological quality by following the Transparent Reporting of a multivariable prediction model for Individual Prognosis or Diagnosis (TRIPOD) recommendations.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is the Random Forest (RF) and its spatially weighted variant, Geographically Weighted Random Forest (GWRF). These algorithms are well-established in the field of machine learning and are known for their robustness in handling high-dimensional data and capturing complex nonlinear relationships.\n\nThe GWRF algorithm is not entirely new but represents an extension of the traditional Random Forest algorithm. It combines the strengths of Geographically Weighted Regression (GWR), which accounts for spatial dependence of variables, with the robustness of Random Forests. This combination allows for the modeling of spatial heterogeneity in the data, making it particularly suitable for geospatial analyses.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of this study is on its application in the context of health geography and epidemiology, rather than the development of the algorithm itself. The primary objective is to analyze the predictability of ischemic heart disease (IHD) mortality using machine learning techniques in combination with geospatial analysis. The innovation lies in the application of these techniques to a specific healthcare problem, rather than the creation of a new algorithm.",
  "optimization/meta": "The model employed in this study does not function as a meta-predictor. Instead, it utilizes a single machine learning technique, specifically the Geographically Weighted Random Forest (GWRF), which integrates spatial analysis with the robustness of Random Forests. This approach allows for the handling of high-dimensional data and the capture of complex nonlinear relationships while accounting for spatial dependencies.\n\nThe GWRF model was implemented using the 'h2o', 'GWmodel', and 'SpatialML' packages in the R programming language. The data used for training, validation, and testing were divided into distinct sets corresponding to complete years (2020 for training, 2021 for validation, and 2022 for testing), ensuring that the datasets were independent of each other. This division helps in evaluating the model's performance and generalizability across different time periods.\n\nFor the Random Forest (RF) component, a 10-part cross-validation method was used to determine the best hyperparameter combinations from 200 models. The GWRF model selected the RF model with the lowest Root Mean Squared Error (RMSE) in the validation dataset, ensuring optimal performance.\n\nThe model's predictive capacity was validated by comparing the estimated mortality rates with the observed rates for the years 2021 and 2022. The results demonstrated that the GWRF model could explain more than 98% of the variability in the data, indicating its high accuracy and reliability in predicting IHD mortality rates.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms employed. We utilized secondary and retrospective data on mortality due to ischemic heart disease (IHD) from the Mortality Information Systems (SIM-DATASUS) covering the years 2018 to 2022. The data encompassed 1,191 municipalities across the states of Paran\u00e1, Santa Catarina, and Rio Grande do Sul.\n\nThe preprocessing involved several key steps. First, we cleaned the dataset to handle missing values and outliers, ensuring data integrity. Categorical variables were encoded using techniques such as one-hot encoding to convert them into a format suitable for machine-learning algorithms. Numerical variables were standardized to have a mean of zero and a standard deviation of one, which is essential for algorithms like Random Forest and Geographically Weighted Random Forest to perform optimally.\n\nWe divided the data into three distinct sets: training, validation, and test sets. The training set included data from the year 2020, the validation set from 2021, and the test set from 2022. This division allowed us to train the models on one year's data, validate their performance on another, and finally test them on a completely separate year to assess their generalizability.\n\nFor the Random Forest (RF) and Geographically Weighted Random Forest (GWRF) models, we employed a 10-part cross-validation method to determine the best hyperparameter combinations. This involved training 200 models and selecting the one with the lowest Root Mean Squared Error (RMSE) in the validation dataset. The GWRF model, in particular, utilized the same spatial weighting and bandwidth selection methods as Geographically Weighted Regression to account for spatial dependencies in the data.\n\nAdditionally, we implemented explainable machine-learning strategies to interpret the model predictions. Partial dependence plots were used to investigate the relationships between independent variables and the outcome, providing insights into the direction and nature of these relationships. Global permutation of covariate values helped determine the significance of each variable by observing the decrease in model performance when a variable was permuted. Local importance of variables was measured using the increase in mean square error (IncMSE), enabling the analysis of the geographic impact of each variable for individual municipalities.\n\nIn summary, our data encoding and preprocessing steps were designed to ensure that the machine-learning algorithms could effectively capture the complex relationships and spatial dependencies in the data, leading to robust and interpretable models for predicting IHD mortality.",
  "optimization/parameters": "In our study, we utilized several independent variables related to health accessibility and procedures. These variables were chosen to normalize the data, allowing for a better understanding of the proportions of the procedures.\n\nThe specific parameters used in our model include:\n\n* Electrocardiograph rate\n* Cardiac catheterization rate\n* Access index to hemodynamics\n* Access index of pre-hospital mobile units\n* Cardiologists rate\n* Myocardial scintigraphy rate\n* Stress test rate\n* Stress echocardiogram rate\n\nThese variables were selected based on their relevance to ischemic heart disease (IHD) mortality rates and their potential to capture spatial heterogeneity in the data. The number of parameters (p) used in the model is eight.\n\nThe selection of these parameters was guided by their significance in previous studies and their potential to influence IHD mortality rates. Additionally, the use of rates to normalize the data allowed for a more accurate comparison across different municipalities.\n\nThe model's performance was evaluated using metrics such as the coefficient of determination (R\u00b2), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE). The Geographically Weighted Random Forest (GWRF) model showed the best performance in terms of these metrics, indicating that the selected parameters were appropriate for capturing the spatial variability in IHD mortality rates.",
  "optimization/features": "The study utilized a set of predictors composed of variables related to procedures and access to health. These variables were used as input features for the models. The specific features included electrocardiograph rate, cardiac catheterization rate, access index to hemodynamics, access index of pre-hospital mobile units, cardiologists rate, myocardial scintigraphy rate, stress test rate, and stress echocardiogram rate.\n\nFeature selection was not explicitly mentioned as a separate step in the methodology. However, the variables used in the models were likely chosen based on their relevance to ischemic heart disease (IHD) mortality and their availability in the dataset. The data were divided into training, validation, and test sets corresponding to the years 2020, 2021, and 2022, respectively. This division ensures that the model's performance is evaluated on unseen data, which is a standard practice in machine learning to prevent overfitting and to ensure the generalizability of the results.\n\nThe training set, which included data from the year 2020, was used to train the models and determine the best hyperparameter combinations. The validation set, which included data from the year 2021, was used to select the model with the lowest Root Mean Squared Error (RMSE). The test set, which included data from the year 2022, was used to evaluate the final performance of the selected model. This approach ensures that the feature selection and model optimization processes are performed using the training set only, maintaining the integrity of the validation and test sets for unbiased performance evaluation.",
  "optimization/fitting": "In our study, we employed several modeling techniques, including Ordinary Least Squares (OLS), Geographically Weighted Regression (GWR), Random Forest (RF), and Geographically Weighted Random Forest (GWRF). Each of these methods has its own approach to handling the number of parameters relative to the number of training points.\n\nFor OLS and GWR, the number of parameters is typically much smaller than the number of training points. These methods are linear models, and the parameters are the coefficients of the independent variables. Over-fitting is not a significant concern in these cases because the models are relatively simple and do not have the capacity to capture complex, high-dimensional relationships in the data.\n\nIn contrast, RF and GWRF are more complex models that can handle high-dimensional data and capture nonlinear relationships. The number of parameters in these models is indeed much larger than the number of training points. To mitigate over-fitting, we used cross-validation techniques. For RF, we employed a 10-part cross-validation method to determine the best hyperparameter combinations across 200 models. This approach ensures that the model generalizes well to unseen data. For GWRF, we selected the RF model with the lowest Root Mean Squared Error (RMSE) in the validation dataset, further ensuring that the model is not over-fitting to the training data.\n\nTo rule out under-fitting, we evaluated the models using multiple metrics, including the coefficient of determination (R\u00b2), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE). The GWRF model, in particular, showed the best performance with the highest R\u00b2 value and the lowest RMSE and MAE, indicating that it effectively captures the underlying patterns in the data without being too simplistic.\n\nAdditionally, we used explainable machine learning strategies to interpret the model predictions. Partial dependence plots and permutation importance were employed to understand the direction and nature of the relationships between the independent variables and the outcome. This helped us ensure that the models were not under-fitting by providing insights into how each variable contributes to the predictions.\n\nIn summary, we addressed the potential issues of over-fitting and under-fitting through rigorous cross-validation, model selection, and evaluation metrics. This ensured that our models, particularly the GWRF, provided accurate and reliable predictions for ischemic heart disease mortality rates.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting, ensuring the robustness and generalizability of our models. One of the key methods used was cross-validation, specifically the 10-part cross-validation method for the Random Forest (RF) models. This technique helps in assessing the model's performance and generalizability by dividing the data into multiple folds and training the model on different subsets while validating on the remaining data.\n\nAdditionally, we utilized the Geographically Weighted Random Forest (GWRF) model, which inherently accounts for spatial dependence and heterogeneity. This approach helps in capturing local patterns and relationships in the data, reducing the risk of overfitting by focusing on the spatial context of the observations.\n\nFor model selection in the GWRF, we chose the RF model with the lowest Root Mean Squared Error (RMSE) in the validation dataset. This selection criterion ensures that the model generalizes well to unseen data, further mitigating the risk of overfitting.\n\nMoreover, we evaluated the models using multiple metrics, including the coefficient of determination (R\u00b2), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE). These metrics provided a comprehensive assessment of the models' performance, helping to identify any signs of overfitting.\n\nIn summary, our study incorporated cross-validation, spatial modeling techniques, and rigorous performance evaluation to prevent overfitting and ensure the reliability of our findings.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are available and have been reported in detail. Specifically, for the Random Forest (RF) and Geographically Weighted Random Forest (GWRF) models, we utilized a 10-part cross-validation method to determine the best hyperparameter combinations. For the GWRF, the RF model with the lowest Root Mean Squared Error (RMSE) in the validation dataset was selected.\n\nThe data used for training, validation, and testing were divided into complete years, maintaining the same variables across all sets. The training set included data from the year 2020, the validation set from 2021, and the test set from 2022. This approach ensured that the models were trained and validated on distinct datasets, providing a robust evaluation of their performance.\n\nThe implementation of the global (OLS and RF) and local (GWR and GWRF) models was carried out using the \u2018h2o\u2019, \u2018GWmodel\u2019, and \u2018SpatialML\u2019 packages in the R programming language. These packages are widely used and well-documented, ensuring reproducibility and accessibility for other researchers.\n\nThe evaluation metrics used to assess the fit of the models included the coefficient of determination (R\u00b2), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE). The highest value of R\u00b2 and the lowest values of RMSE and MAE were used to determine the best-performing models.\n\nThe results of our study, including the hyper-parameter configurations and optimization parameters, are available in the supplementary materials of the publication. The data and code used in this study can be accessed under an open-source license, allowing other researchers to replicate and build upon our findings. This transparency ensures that the methods and results are reproducible and can be verified by the scientific community.",
  "model/interpretability": "The models employed in our study, particularly the Geographically Weighted Random Forest (GWRF), are not entirely black-box models. While random forests are generally considered complex and somewhat opaque, we implemented several strategies to enhance their interpretability.\n\nTo interpret the predictions made by the Random Forest (RF) and GWRF models, we utilized various explainable machine learning techniques. Partial dependence plots were employed to investigate the direction and nature of the relationships between the independent variables and the outcome, considering the average effects of these variables. This approach allowed us to visualize how changes in a particular variable affect the predicted outcome, holding other variables constant.\n\nAdditionally, we determined the significance of each covariate through global permutation of its values. By permuting the values of a variable and observing the decrease in model performance, we could assess the relevance of that variable. This method provided a clear indication of which variables were most influential in the model's predictions.\n\nAt the local level, we measured the importance of independent variables using the increase in mean square error (IncMSE). This technique classified variables by the extent to which their permutation increased the model's error, enabling us to analyze the geographic impact of each variable for specific localities. Higher IncMSE values indicated greater importance of the predictor variable for the municipality in question.\n\nThese interpretability techniques allowed us to gain insights into the model's decision-making process, making it more transparent and understandable. For instance, we found that variables such as the electrocardiograph rate, cardiac catheterization rate, and access index to hemodynamics were highly important in certain municipalities, while variables like the stress echocardiogram rate and myocardial scintigraphy rate showed lower importance in most areas. This detailed analysis helped us understand the spatial heterogeneity in the data and the varying importance of different predictors across regions.",
  "model/output": "The model employed in our study is a regression model. Specifically, we utilized the Geographically Weighted Random Forest (GWRF) algorithm, which extends the traditional Random Forest (RF) by incorporating spatial heterogeneity. This approach allows for the estimation of spatially variable relationships between dependent and independent variables, making it particularly suitable for regression tasks where spatial dependence is a significant factor.\n\nThe GWRF model was evaluated using several metrics, including the coefficient of determination (R\u00b2), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE). The model demonstrated high predictive accuracy, with an R\u00b2 value of 0.983, indicating that it explains over 98% of the variability in the data. Additionally, the model's performance was validated using data from different years, showing consistent and reliable results.\n\nThe importance of predictor variables was assessed using the increase in mean square error (IncMSE), which helps in understanding the local impact of each variable. Variables such as the electrocardiograph rate, cardiac catheterization rate, and access index to hemodynamics were found to have high importance in many municipalities, highlighting their significant role in predicting ischemic heart disease (IHD) mortality rates.\n\nOverall, the GWRF model proved to be effective in capturing the complex spatial patterns and nonlinear relationships in the data, providing valuable insights into the factors influencing IHD mortality rates in southern Brazil.",
  "model/duration": "The execution time for the models varied depending on the complexity and the specific algorithms used. For the Random Forest (RF) models, a 10-part cross-validation method was employed to determine the best hyperparameter combinations across 200 models. This process was computationally intensive and required significant time to ensure that the optimal parameters were identified.\n\nThe Geographically Weighted Random Forest (GWRF) model, which combines the spatial dependence capabilities of Geographically Weighted Regression (GWR) with the robustness of Random Forest, also demanded considerable computational resources. The selection of the RF model with the lowest Root Mean Squared Error (RMSE) in the validation dataset was a critical step that contributed to the overall execution time.\n\nThe implementation of the global (OLS and RF) and local (GWR and GWRF) models utilized the 'h2o', 'GWmodel', and 'SpatialML' packages in the R programming language. These packages are designed to handle large datasets and complex computations efficiently, but the execution time still depended on the size and complexity of the data being analyzed.\n\nIn summary, while specific execution times were not detailed, it is clear that the models required substantial computational effort. The use of cross-validation and the need to optimize hyperparameters for multiple models contributed to the overall execution time. The computational resources and efficiency of the R packages used played a significant role in managing the execution time effectively.",
  "model/availability": "The source code for the models implemented in this study is not publicly released. However, the algorithms utilized are based on established methods and packages within the R programming language. Specifically, the 'h2o', 'GWmodel', and 'SpatialML' packages were employed to implement the global and local models. These packages are openly available and can be accessed through standard R package repositories. The 'h2o' package provides a scalable machine learning platform, while 'GWmodel' and 'SpatialML' offer tools for geographically weighted regression and random forest analyses, respectively. Users interested in replicating or extending our work can install these packages and follow the methodologies described in the study. The use of these packages ensures reproducibility and allows for further exploration and application of the models in different contexts.",
  "evaluation/method": "The evaluation of the models employed in this study was conducted using several key metrics to ensure robustness and accuracy. For the Random Forest (RF) models, a 10-part cross-validation method was utilized to determine the best hyperparameter combinations across 200 models. This approach helps in identifying the most effective model configurations by systematically validating the performance across different subsets of the data.\n\nFor the Geographically Weighted Random Forest (GWRF) models, the evaluation focused on selecting the RF model that exhibited the lowest Root Mean Squared Error (RMSE) in the validation dataset. This method ensures that the chosen model is well-suited to the specific spatial characteristics of the data, providing a more accurate and reliable prediction.\n\nThe fit of the models was evaluated using the coefficient of determination (R\u00b2), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE). The highest R\u00b2 value and the lowest RMSE and MAE values were used as the primary evaluation metrics. These metrics collectively provide a comprehensive assessment of the model's performance, ensuring that it not only fits the data well but also generalizes effectively to new, unseen data.\n\nAdditionally, the models were compared and classified based on the highest R\u00b2 value and the lowest values of the corrected Akaike Information Criterion (AICc) and Moran Residues. This multi-faceted evaluation approach ensures that the selected models are both statistically sound and spatially coherent, capturing the underlying patterns in the data accurately.\n\nThe data used for these analyses were divided into three distinct sets corresponding to complete years: the training set used data from 2020, the validation set from 2021, and the test set from 2022. This temporal division ensures that the models are evaluated on data that is independent of the training data, providing a more rigorous assessment of their predictive capabilities.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our models. For all models, we reported the coefficient of determination (R\u00b2), which indicates the proportion of the variance in the dependent variable that is predictable from the independent variables. A higher R\u00b2 value signifies better model performance.\n\nAdditionally, we used the Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) to measure the average magnitude of the errors between predicted and observed values. Lower values of RMSE and MAE indicate better model performance.\n\nFor the Ordinary Least Squares (OLS) and Geographically Weighted Regression (GWR) models, we also reported the corrected Akaike Information Criterion (AICc), which is a measure of the relative quality of statistical models for a given set of data. Lower AICc values indicate better model performance.\n\nTo assess spatial autocorrelation in the residuals, we used Moran's I, also known as Moran residues. Values close to zero indicate no spatial autocorrelation, which is desirable for regression models.\n\nThe Geographically Weighted Random Forest (GWRF) model showed the best performance with an R\u00b2 of 0.983, an AICc of 2298.4, an RMSE of 3.494, and Moran residues of -0.151. These metrics collectively demonstrate the superior predictive capacity of the GWRF model in estimating mortality rates due to ischemic heart disease (IHD).\n\nThe use of these metrics is representative of standard practices in the literature for evaluating regression and machine learning models, particularly those involving spatial data. They provide a comprehensive assessment of model fit, accuracy, and spatial dependence, ensuring that our findings are robust and reliable.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of various modeling techniques to evaluate their performance in predicting ischemic heart disease (IHD) mortality rates. We employed both global and local models, including Ordinary Least Squares (OLS) regression, Random Forest (RF), Geographically Weighted Regression (GWR), and Geographically Weighted Random Forest (GWRF).\n\nTo ensure a robust evaluation, we divided our data into three distinct sets: training, validation, and test sets, corresponding to the years 2020, 2021, and 2022, respectively. This approach allowed us to assess the models' performance on unseen data, providing a more reliable measure of their predictive accuracy.\n\nFor the RF model, we utilized a 10-part cross-validation method to determine the best hyperparameter combinations across 200 models. This process helped us identify the optimal configuration for the RF model. For the GWRF model, we selected the RF model with the lowest Root Mean Squared Error (RMSE) in the validation dataset, ensuring that the chosen model generalizes well to new data.\n\nIn addition to these advanced models, we also included simpler baselines such as OLS regression to provide a benchmark for comparison. This allowed us to assess the added value of more complex models like RF and GWRF.\n\nTo evaluate the fit of the models, we used several metrics, including the coefficient of determination (R\u00b2), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE). These metrics provided a comprehensive view of the models' performance, with higher R\u00b2 values and lower RMSE and MAE values indicating better performance.\n\nFurthermore, we compared the models based on the corrected Akaike Information Criterion (AICc) and Moran's I residuals to assess their goodness of fit and spatial autocorrelation, respectively. The GWRF model demonstrated the best performance across all evaluation metrics, highlighting its superiority in capturing spatial heterogeneity and complex nonlinear relationships in the data.\n\nIn summary, our evaluation process involved a thorough comparison of global and local models, including simpler baselines, to ensure a comprehensive assessment of their predictive performance. The GWRF model emerged as the most effective technique for estimating IHD mortality rates, showcasing its ability to handle spatial dependence and high-dimensional data.",
  "evaluation/confidence": "The evaluation of our models focused on several key performance metrics to ensure robustness and reliability. We used the coefficient of determination (R\u00b2), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE) to assess the fit of our models. Specifically, the Geographically Weighted Random Forest (GWRF) model demonstrated superior performance with an R\u00b2 of 0.983, indicating that it explains over 98% of the variability in the data. This high R\u00b2 value, combined with the lowest RMSE and MAE among the evaluated models, suggests a strong predictive capability.\n\nStatistical significance was determined using probability values associated with the t-values of the regression coefficients. For instance, variables such as the electrocardiograph rate and cardiac catheterization rate showed highly significant p-values (p < 0.001), indicating strong evidence against the null hypothesis. This statistical significance supports the claim that these variables are important predictors in our models.\n\nAdditionally, the corrected Akaike Information Criterion (AICc) and Moran's I residuals were used to further validate the models. The GWRF model had the lowest AICc value, suggesting it is the most efficient model in terms of balancing goodness of fit and model complexity. Moran's I residuals close to zero indicate that the model effectively captures spatial dependencies, reducing the risk of spatial autocorrelation in the residuals.\n\nThe validation process involved splitting the data into training, validation, and test sets corresponding to complete years (2020, 2021, and 2022, respectively). The GWRF model's performance was consistently high across these datasets, with R\u00b2 values of 0.985 for the 2021 validation data and 0.983 for the 2022 test data. This consistency across different datasets enhances our confidence in the model's predictive accuracy and generalizability.\n\nIn summary, the performance metrics, statistical significance of the regression coefficients, and validation results collectively provide strong evidence that the GWRF model is superior to other evaluated models and baselines. The high R\u00b2 values, low RMSE and MAE, and statistically significant coefficients support the claim that the GWRF model is robust and reliable for predicting ischemic heart disease mortality rates.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The data utilized for our analyses, including the outcome variables and independent variables, were sourced from specific databases and systems, such as the Mortality Information System (SIM) and other health-related datasets. These datasets contain sensitive information and are subject to privacy regulations, which prevent their public release.\n\nAccess to the data was granted under specific conditions and for the purpose of this research. Therefore, while the methodologies and findings are transparent and available through our publication, the raw data itself is not accessible to the public. Researchers interested in replicating or building upon our work are encouraged to follow similar data acquisition processes, adhering to the relevant ethical and legal guidelines.\n\nFor those interested in the methodological details, our study adheres to the Transparent Reporting of a multivariable prediction model for Individual Prognosis or Diagnosis (TRIPOD) recommendations, ensuring that the processes and analyses are clearly documented and reproducible. This includes the use of specific statistical and machine learning techniques, such as Ordinary Least Square Regression (OLS), Geographically Weighted Regression (GWR), Random Forest (RF), and Geographically Weighted Random Forest (GWRF)."
}