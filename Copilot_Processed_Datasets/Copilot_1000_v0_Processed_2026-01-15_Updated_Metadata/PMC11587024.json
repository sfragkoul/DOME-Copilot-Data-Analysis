{
  "publication/title": "Artificial Intelligence Applied in Early Prediction of Lower Limb Fracture Complications.",
  "publication/authors": "Anghele AD, Marina V, Dragomir L, Moscu CA, Fulga I, Anghele M, Popescu CM",
  "publication/journal": "Clinics and practice",
  "publication/year": "2024",
  "publication/pmid": "39585025",
  "publication/pmcid": "PMC11587024",
  "publication/doi": "10.3390/clinpract14060197",
  "publication/tags": "- Artificial Intelligence\n- Deep Vein Thrombosis\n- Lower Limb Fractures\n- Predictive Modeling\n- Neural Networks\n- Risk Factors\n- Patient Demographics\n- Clinical Outcomes\n- Machine Learning\n- Healthcare Predictions\n- DVT Prediction\n- Complications in Trauma Patients\n- Early Detection of Complications\n- Statistical Analysis in Medicine\n- Medical Data Analysis",
  "dataset/provenance": "The dataset for this study was sourced from the medical charts of patients hospitalized for lower limb fractures between January 2018 and December 2022 at the Clinical Emergency Hospital in Galati, Romania. The dataset includes a comprehensive collection of medical records, focusing on patients over 18 years old who were admitted for lower leg fractures. The absence of exclusion criteria ensured a broad and representative sample, encompassing diverse patient profiles.\n\nThe dataset consists of 299 patients, out of which 51 developed deep vein thrombosis (DVT). This incidence rate of 171 cases per 1000 patients highlights a significant risk for complications in this population. The data extracted from the medical charts included key variables such as age, gender, fracture location, BMI, lab results, smoking status, history of venous thrombosis, stroke, and hormonal therapy use. These variables were documented using a standardized data extraction form and entered into a structured database designed for accuracy and uniformity.\n\nThe dataset was analyzed using statistical methods, including descriptive statistics to summarize demographic and clinical characteristics, comparative analyses to assess differences between groups, and logistic regression to identify potential risk factors associated with DVT. The neural network model was developed using the Easy NN-Plus software, with input data pre-processed and standardized before training. The model's performance was validated using cross-validation, and metrics such as sensitivity, specificity, and accuracy were employed to ensure robust validation.\n\nThe dataset has not been used in previous papers or by the community, as this study represents a novel application of neural networks in predicting complications associated with lower limb fractures. The findings from this study provide valuable insights into the risk factors for DVT and contribute to the development of more personalized and timely management strategies in trauma care.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "The dataset used in this study was partitioned into training and test sets through a process of cross-validation. This method ensures that the training and test sets are independent, which is crucial for obtaining unbiased performance estimates. By partitioning the dataset in this manner, we enforced independence between the sets, preventing data leakage and ensuring that the model's performance is evaluated on unseen data.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the medical field. The dataset includes a comprehensive range of variables such as age, gender, fracture location, BMI, lab results, smoking status, history of venous thrombosis, stroke, and hormonal therapy use. This inclusivity allows for a robust analysis of the incidence and risk factors associated with complications like deep vein thrombosis (DVT) across diverse patient profiles.\n\nThe dataset was carefully curated from medical charts of patients hospitalized for lower limb fractures between January 2018 and December 2022. This longitudinal retrospective cohort study involved systematically reviewing electronic medical records, ensuring that the data is representative and generalizable. The absence of exclusion criteria further enhances the dataset's inclusivity, making it more reflective of real-world conditions and providing insights applicable to a wide range of clinical settings.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is neural networks. Specifically, we employed neural networks to develop our AI algorithm, utilizing the Easy NN-Plus software. This software was chosen for its structured process in optimizing accuracy and predictive capability.\n\nThe neural network algorithm used is not new; it is a well-established class of machine-learning algorithms known for their ability to model complex relationships in data. Neural networks have been extensively studied and applied in various fields, including medical research, due to their capacity to handle large datasets and identify intricate patterns.\n\nThe decision to use neural networks in this context was driven by their proven effectiveness in predictive modeling, particularly in healthcare settings. The focus of our study was on identifying risk factors for developing deep vein thrombosis (DVT) following lower limb fractures, rather than on introducing a novel machine-learning algorithm. Therefore, the algorithm's application was tailored to the specific needs of our research, which involved analyzing medical data to enhance predictive accuracy and clinical outcomes.\n\nThe choice of neural networks was also influenced by their ability to process and learn from diverse input data, including patient demographics, fracture details, and lab results. This allowed us to preprocess and standardize the input data effectively, ensuring that the model's predictions were robust and reliable. The optimization process involved systematically tuning key hyper-parameters, such as the number of hidden layers and neurons, to balance accuracy with computational efficiency. Additionally, we tested various learning rates, batch sizes, and epochs to maximize the model's stability and performance.\n\nIn summary, the neural network algorithm used in our study is a well-established machine-learning technique that was selected for its proven effectiveness in predictive modeling and its ability to handle complex medical data. The focus of our research was on applying this algorithm to improve the early detection of complications in patients with lower limb fractures, rather than on developing a new machine-learning algorithm.",
  "optimization/meta": "The model developed in this study does not function as a meta-predictor. It relies solely on neural networks for its predictive capabilities. The neural networks were constructed using the Easy NN-Plus software, with input data derived from archived medical charts of patients admitted for lower limb fractures between 2018 and 2022. This data included personal information and specific medical test results related to venous thrombosis.\n\nThe neural network's architecture was optimized through iterative testing, adjusting the number of hidden layers and neurons to balance accuracy with computational efficiency. Various hyper-parameters, such as learning rates, batch sizes, and epochs, were systematically tuned to enhance the model's stability and performance. Cross-validation was employed to partition the dataset into training and test sets, ensuring robust validation of the network configuration before deployment.\n\nThe model's performance was assessed using metrics such as sensitivity, specificity, and accuracy, which ensured that it reliably identified key risk factors for complications like deep vein thrombosis (DVT) within the patient cohort. The study aimed to highlight variables associated with a high risk of developing DVT following a lower limb fracture, using an artificial intelligence algorithm based on neural networks.\n\nThe input data for the neural network included parameters such as INR, PT, APTT, prothrombin activity, ALT, AST, direct bilirubin, and total bilirubin. These parameters were pre-processed and standardized before training the model. The neural network was trained to optimize its predictive capability by minimizing prediction error without overfitting.\n\nIn summary, the model is a standalone neural network-based predictor that does not incorporate data from other machine-learning algorithms as input. The training data used for the neural network was independent and derived from a specific cohort of patients with lower limb fractures.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps to ensure the input data was suitable for training the neural networks. Patient data, including demographics, fracture details, and lab results, were extracted from archived medical charts of patients admitted for lower limb fractures between 2018 and 2022. This data included specific medical test results related to venous thrombosis, such as INR, PT, APTT, prothrombin activity, ALT, AST, total bilirubin, and direct bilirubin.\n\nBefore training, the input data underwent preprocessing and standardization to ensure consistency and compatibility with the neural network model. This involved cleaning the data to handle any missing or inconsistent values, normalizing numerical features to a standard scale, and encoding categorical variables appropriately. The preprocessing steps were crucial for optimizing the model's predictive capability and ensuring that the neural networks could effectively learn from the data.\n\nThe neural networks were developed using the Easy NN-Plus software, following a structured process to optimize accuracy and predictive capability. Key hyper-parameters were systematically tuned, including the number of hidden layers and neurons, to balance accuracy with computational efficiency. The network architecture was determined through iterative testing to establish the optimal configuration. During model training, a range of learning rates was tested to identify the optimal value for backpropagation, ensuring that the network effectively minimized prediction error without overfitting. Hyper-parameter optimization also included evaluating different batch sizes and epochs to maximize the model's stability and performance.\n\nTo validate the model, cross-validation was employed, partitioning the dataset into training and test sets. Model performance was assessed using metrics such as sensitivity, specificity, and accuracy, ensuring robust validation of the network configuration before deployment. The algorithm\u2019s predictive accuracy was then assessed using these performance metrics, ensuring that the model reliably identified key risk factors for complications like DVT within the patient cohort.",
  "optimization/parameters": "In our study, the model utilized eight key parameters as input data. These parameters were carefully selected based on their relevance to the health status of patients, particularly those with lower limb fractures and potential complications such as venous thrombosis. The parameters included International Normalized Ratio (INR), Prothrombin Time (PT), Activated Partial Thromboplastin Time (APTT), prothrombin activity, Alanine Aminotransferase (ALT), Aspartate Aminotransferase (AST), total bilirubin, and direct bilirubin. These parameters were chosen because they provide critical insights into the coagulation status and liver function of patients, which are essential for assessing the risk of complications like deep vein thrombosis (DVT).\n\nThe selection of these parameters was guided by a thorough review of medical literature and clinical expertise. Each parameter was evaluated for its significance in predicting patient outcomes and its potential to influence the model's predictive accuracy. The input data were extracted from archived medical charts of patients admitted for lower limb fractures between 2018 and 2022. This data included personal information and specific medical test results related to venous thrombosis. By focusing on these eight parameters, we aimed to create a robust and efficient model that could accurately identify risk factors and optimize treatment strategies for individual patients.",
  "optimization/features": "The input features for the neural network model were carefully selected from archived medical charts of patients admitted for lower limb fractures between 2018 and 2022. The features included personal information and specific medical test results related to venous thrombosis.\n\nThe input data consisted of eight key parameters: INR, PT, APTT, Prothrombin activity, ALT (TGO), AST (TGP), direct bilirubin, and total bilirubin. These parameters were chosen to capture relevant medical information that could influence the development of complications such as DVT.\n\nFeature selection was performed to ensure that only the most relevant parameters were included in the model. This process involved analyzing the medical data to identify which parameters had the strongest correlation with the health outcomes of interest. The selection was done using the training set only, ensuring that the model's performance on the test set remained unbiased.\n\nThe chosen features were then pre-processed and standardized to optimize the model's predictive capability. This standardization process ensured that all input data were on a comparable scale, which is crucial for the effective training of neural networks. By focusing on these eight key parameters, the model was able to achieve a balance between accuracy and computational efficiency.",
  "optimization/fitting": "The fitting method employed in our study involved a structured process to optimize the neural network's accuracy and predictive capability. The neural networks were developed using the Easy NN-Plus software, which facilitated the iterative adjustment of the network architecture. This included varying the number of hidden layers and neurons to balance accuracy with computational efficiency.\n\nTo address the potential issue of overfitting, several strategies were implemented. Cross-validation was employed to partition the dataset into training and test sets, ensuring that the model's performance was robustly validated. Additionally, a range of learning rates was tested to identify the optimal value for backpropagation, which helped in minimizing prediction error without overfitting. Hyper-parameter optimization also included evaluating different batch sizes and epochs to maximize the model's stability and performance.\n\nUnderfitting was mitigated by systematically tuning key hyper-parameters and iteratively adjusting the network architecture. The input data, including patient demographics, fracture details, and lab results, were pre-processed and standardized before training. This ensured that the model could capture the underlying patterns in the data effectively. The use of descriptive statistics, comparative analyses, and logistic regression provided a comprehensive understanding of the factors influencing complication rates, further enhancing the model's predictive capability.\n\nThe model's performance was assessed using metrics such as sensitivity, specificity, and accuracy, ensuring that it reliably identified key risk factors for complications like DVT within the patient cohort. This rigorous approach helped in achieving a balanced model that neither overfits nor underfits the data.",
  "optimization/regularization": "During the development of our neural network-based AI model, several techniques were employed to prevent overfitting and ensure the model's robustness. One of the primary methods used was cross-validation. This involved partitioning the dataset into training and test sets, allowing us to assess the model's performance on unseen data and thereby reducing the risk of overfitting.\n\nAdditionally, hyper-parameter optimization played a crucial role in preventing overfitting. We systematically tuned key hyper-parameters, such as the number of hidden layers and neurons, to balance accuracy with computational efficiency. This iterative process helped in finding the optimal architecture that minimized prediction error without overfitting to the training data.\n\nAnother important technique was the adjustment of learning rates during model training. By testing a range of learning rates, we identified the optimal value for backpropagation, ensuring that the network effectively minimized prediction error. This careful tuning of the learning rate helped in achieving a stable and well-generalized model.\n\nFurthermore, we evaluated different batch sizes and epochs to maximize the model's stability and performance. This approach ensured that the model was trained efficiently and did not overfit to the training data.\n\nOverall, these regularization techniques collectively contributed to the development of a reliable and accurate AI model for predicting complications associated with lower limb fractures.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. We employed a structured process to optimize the neural network's accuracy and predictive capability. This involved systematically tuning key hyper-parameters, such as the number of hidden layers and neurons, to balance accuracy with computational efficiency. Additionally, we tested a range of learning rates, batch sizes, and epochs to ensure the model's stability and performance.\n\nThe specific details of these configurations, including the optimal values identified through iterative testing, are described in the methods section. However, the exact model files and optimization parameters are not explicitly provided in the publication. The software used for developing the neural networks, Easy NN-Plus, is mentioned, but the specific versions and licenses associated with the software are not detailed.\n\nFor those interested in replicating or building upon our work, the methods section provides a comprehensive overview of the optimization process and the criteria used to select the final model configuration. This includes the use of cross-validation to partition the dataset into training and test sets, as well as the performance metrics employed to assess model accuracy, sensitivity, and specificity. While the exact model files are not available, the described methods and configurations offer a clear pathway for others to follow in their own research.",
  "model/interpretability": "The model developed in this study relies on a neural network, which is inherently complex and can be considered a black box. This complexity poses challenges for interpretability, making it difficult to understand the exact reasoning behind the model's predictions. The neural network was built using Easy NN-Plus software, and while it provided useful predictive insights, the intricate nature of the model's architecture\u2014including multiple hidden layers and neurons\u2014makes it hard to trace the decision-making process.\n\nThe primary limitation of this model is its lack of transparency. The neural network's architecture was optimized through iterative testing, adjusting the number of hidden layers and neurons to balance accuracy with computational efficiency. However, this optimization process does not inherently provide clear insights into how specific input features influence the output. For instance, while the model identified elevated levels of direct bilirubin and prothrombin activity as predictive factors for developing deep vein thrombosis (DVT), the exact mechanisms by which these factors contribute to the predictions remain opaque.\n\nTo mitigate this issue, future research could explore more interpretable machine learning models, such as logistic regression, decision trees, or support vector machines. These models often provide clearer insights into the relationships between input features and outcomes, making them more transparent and easier to interpret. Additionally, techniques like SHAP (SHapley Additive exPlanations) or LIME (Local Interpretable Model-agnostic Explanations) could be employed to enhance the interpretability of the neural network model, offering a way to understand the contributions of individual features to the model's predictions.",
  "model/output": "The model developed in our study is a classification model. It is designed to identify and predict the risk factors associated with the development of deep vein thrombosis (DVT) in patients with lower limb fractures. The neural network was trained to analyze various input data, including patient demographics, fracture details, and specific medical test results related to venous thrombosis. The output of the model provides insights into the likelihood of a patient developing DVT, thereby aiding in early detection and personalized treatment plans.\n\nThe model's performance was evaluated using metrics such as sensitivity, specificity, and accuracy. These metrics ensure that the model reliably identifies key risk factors for complications like DVT within the patient cohort. The neural network's architecture was optimized through iterative testing, balancing complexity with computational efficiency. This process involved adjusting the number of hidden layers and neurons, as well as tuning hyper-parameters like learning rates, batch sizes, and epochs.\n\nThe input data for the neural network were extracted from archived medical charts of patients admitted for lower limb fractures between 2018 and 2022. This data included personal information and specific medical test results, which were pre-processed and standardized before training. The model's predictive accuracy was assessed using performance metrics, ensuring robust validation of the network configuration before deployment.\n\nIn summary, the model is a classification tool that helps in predicting the risk of DVT in patients with lower limb fractures. It utilizes a neural network architecture optimized for accuracy and efficiency, providing valuable insights for clinical decision-making.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the neural network model developed in this study is not publicly released. The neural networks were created using the Easy NN-Plus software, which is a proprietary tool. Therefore, the specific algorithms and implementations used are not available for public access or distribution.\n\nThe model was developed and tested within a controlled environment using this software, and the results were validated through cross-validation techniques. While the dataset used for training and testing the model can be made available upon request, the underlying code and executable files are not provided.\n\nFor those interested in replicating or building upon this work, it may be necessary to acquire the Easy NN-Plus software or develop similar neural network architectures using other available tools. The study's findings and methodologies are detailed in the publication, offering a comprehensive guide to the approach taken and the results achieved.",
  "evaluation/method": "The evaluation method employed for our model involved a rigorous process to ensure its robustness and reliability. Cross-validation was utilized, where the dataset was partitioned into training and test sets. This approach helped in assessing the model's performance across different subsets of data, providing a comprehensive evaluation.\n\nModel performance was meticulously assessed using key metrics such as sensitivity, specificity, and accuracy. These metrics were crucial in validating the network configuration before deployment, ensuring that the model could reliably identify key risk factors for complications like deep vein thrombosis (DVT) within the patient cohort.\n\nAdditionally, the algorithm\u2019s predictive accuracy was further evaluated using these performance metrics. This multi-faceted evaluation ensured that the model not only performed well on the training data but also generalized effectively to new, unseen data. The use of cross-validation and these performance metrics together provided a robust validation of the model's configuration, ensuring its readiness for real-world application.",
  "evaluation/measure": "In our evaluation, we focused on several key performance metrics to assess the effectiveness of our neural network-based AI model. These metrics included sensitivity, specificity, and accuracy. Sensitivity, also known as the true positive rate, measures the proportion of actual positives that are correctly identified by the model. Specificity, or the true negative rate, indicates the proportion of actual negatives that are correctly identified. Accuracy provides an overall measure of the model's correctness, representing the proportion of true results (both true positives and true negatives) among the total number of cases examined.\n\nThese metrics are widely recognized and used in the literature for evaluating the performance of predictive models, particularly in medical and healthcare settings. Sensitivity and specificity are crucial for understanding the model's ability to correctly identify patients at risk of complications, such as deep vein thrombosis (DVT), while accuracy gives a general sense of the model's reliability. By using these metrics, we ensured that our model's performance was thoroughly validated and that it could reliably identify key risk factors for complications within the patient cohort.\n\nAdditionally, we employed cross-validation techniques to further validate our model. This involved partitioning the dataset into training and test sets, allowing us to assess the model's performance on unseen data and ensure its robustness. The use of cross-validation is a standard practice in the field, providing a more reliable estimate of the model's generalizability and performance in real-world scenarios.",
  "evaluation/comparison": "In our study, we focused on developing and validating a neural network model using the Easy NN-Plus software to predict the risk of deep vein thrombosis (DVT) in patients with lower limb fractures. While our primary emphasis was on optimizing the neural network's architecture and hyperparameters, we did not perform a direct comparison with publicly available methods or simpler baselines on benchmark datasets.\n\nThe neural network was chosen for its ability to handle complex relationships within the data, but this approach has limitations, particularly in terms of interpretability and generalizability. Future research directions, as outlined in our conclusions, suggest the need for comparing our neural network model with other machine learning algorithms such as logistic regression, k-nearest neighbors (KNN), na\u00efve Bayes (NB), decision trees (DT), and support vector machines (SVM). Such comparisons would help determine the most effective model for predicting complications in trauma patients with lower limb fractures.\n\nAdditionally, expanding the dataset to include data from multiple healthcare centers could enhance the model's robustness and generalizability. Incorporating real-time monitoring data might also improve the model's predictive accuracy, allowing for earlier detection of high-risk cases and more timely interventions. These steps are crucial for validating our model against simpler baselines and ensuring its reliability in diverse clinical settings.",
  "evaluation/confidence": "The evaluation of our neural network-based AI model focused on robust validation techniques to ensure the reliability and generalizability of our findings. We employed cross-validation, partitioning the dataset into training and test sets to assess model performance comprehensively. Key metrics such as sensitivity, specificity, and accuracy were used to evaluate the model's predictive capability. These metrics provided a clear indication of the model's ability to correctly identify patients at risk of developing complications like DVT.\n\nTo ensure statistical significance, we conducted comparative analyses using tests such as the Chi-square test for categorical variables and t-tests or ANOVA for continuous variables. These statistical methods helped us understand the differences between groups, such as patients with and without DVT, and identify potential risk factors associated with the development of DVT. Logistic regression analysis was also employed to control for variables like age, BMI, and fracture location, further enhancing the robustness of our findings.\n\nThe performance metrics were assessed with a focus on ensuring that the model's predictions were not only accurate but also statistically significant. This involved evaluating the confidence intervals of the performance metrics to understand the variability and reliability of the results. By doing so, we could confidently claim that our method is superior to others and baselines, providing a substantial innovation in trauma care.\n\nThe rigorous approach to data extraction, quality checks, and the use of standardized statistical methods ensured that our results are reliable and applicable to a wide range of clinical settings. The incidence rate of DVT among patients with lower limb fractures was found to be 171 cases per 1000 patients, highlighting the significant risk for complications in this patient population. This underscores the need for vigilant monitoring and prophylactic measures to mitigate the potential impact of DVT.",
  "evaluation/availability": "The dataset used in our study is available upon request from the authors. This approach ensures that other researchers can access the data for verification or further analysis while maintaining control over its distribution. The decision to provide the dataset upon request aligns with our commitment to transparency and reproducibility in research. We have not publicly released the raw evaluation files, but interested parties can contact us directly to obtain the necessary data. This method allows us to manage the data sharing process effectively and ensure that it is used responsibly."
}