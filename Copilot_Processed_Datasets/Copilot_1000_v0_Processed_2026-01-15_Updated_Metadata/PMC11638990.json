{
  "publication/title": "Geometric Deep learning Prioritization and Validation of Cannabis Phytochemicals as Anti-HCV Non-nucleoside Direct-acting Inhibitors.",
  "publication/authors": "Charles S, Edgar MP",
  "publication/journal": "Biomedical engineering and computational biology",
  "publication/year": "2024",
  "publication/pmid": "39678171",
  "publication/pmcid": "PMC11638990",
  "publication/doi": "10.1177/11795972241306881",
  "publication/tags": "- Graph neural network\n- Deep learning\n- Hepatitis C virus\n- Cannabis sativa\n- Molecular simulation dynamics\n- Virtual screening\n- Drug discovery\n- NS5B inhibitors\n- Computational biology\n- Geometric deep learning",
  "dataset/provenance": "The dataset utilized in this study is derived from the CASF-2016 benchmark, which comprises 285 carefully selected protein-ligand complexes. This benchmark is well-established and has been used previously in the community for evaluating the performance of molecular docking and scoring methods. The preprocessing of the structures from this benchmark was consistent with that of the training set, ensuring uniformity and reliability in the evaluation process. The dataset includes a diverse range of protein-ligand interactions, providing a robust foundation for assessing the screening and scoring capabilities of the geometric deep learning models employed in this research.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study is differential evolution. This algorithm belongs to the class of evolutionary algorithms, which are a subset of evolutionary computation, a field within machine learning.\n\nDifferential evolution is not a new algorithm; it has been established in the literature since 1997. It is a robust, population-based optimization algorithm that is particularly effective for solving complex, non-linear, and multi-modal optimization problems. The choice of differential evolution for our study was driven by its ability to handle the high-dimensional and complex search space associated with predicting binding conformations of bioactive molecules.\n\nThe reason differential evolution was not published in a machine-learning journal is that it is a well-established optimization technique that has been extensively studied and applied across various domains, including engineering, operations research, and computational biology. Its foundational work was published in the Journal of Global Optimization, which is a reputable journal in the field of optimization and operations research. Given its widespread use and recognition, there was no need to publish it in a machine-learning journal. Instead, our focus was on applying this established algorithm to a specific problem in molecular biology, leveraging its strengths to achieve accurate and efficient predictions of binding conformations.",
  "optimization/meta": "The model employed in this study does not function as a meta-predictor. Instead, it relies on a geometric deep learning approach to predict binding conformations of bioactive molecules. The core of the model is a Graph Neural Network (GNN) that processes node and edge features to update and refine molecular representations.\n\nThe GNN utilizes edge features and the features of the two connecting nodes to update node features. This process is iterative, with multiple convolution rounds enhancing the information about each atom and its neighbors. The updated node and edge features are then fed into a Mixture Density Network (MND), which constructs a hidden representation using a Multi-Layer Perceptron (MLP). The MLP consists of linear layers followed by Exponential Linear Unit (ELU) activation functions, chosen for their benefits in stabilizing training and combating the vanishing gradient problem.\n\nThe model does not integrate data from other machine-learning algorithms as input. Instead, it focuses on geometric deep learning techniques to optimize and predict binding conformations. The training data for the GNN is independent and consists of protein-ligand complexes, with preprocessing identical to that of the training set. The model's performance is evaluated using the CASF-2016 benchmark, which includes 285 carefully chosen protein-ligand complexes.\n\nThe optimization process involves differential evolution to determine the most likely ligand conformation that interacts with the target binding site. This is achieved by minimizing the potential learned by the model for a given complex. The model's parameters are updated using the Adam optimizer, with a learning rate of 0.002 and a batch size of 16 protein-ligand complexes over 150 epochs. The loss function defines a potential specific to each target-ligand complex, which is used to score the 3-dimensional structure by summing over all possible pairs and calculating the negative log-likelihood for each target-ligand node pair.",
  "optimization/encoding": "The data encoding process involved representing the ligand conformation as a vector using several key parameters. The relative location of the ligand in Euclidean space was considered, along with the dihedral angles of all rotatable bonds in the molecule and the Euler angles. These parameters were crucial for defining the ligand's position and orientation within the binding site.\n\nThe protein structure underwent a thorough cleaning and preprocessing stage. This included assigning bond orders using a specific database, adding hydrogens, creating zero bond orders, and generating disulfide bonds. Additionally, het states were created using a tool that considers a pH range of 7 \u00b1 2 units. Water and ligands were removed, and H-bond assignments were made using another tool. The ligands themselves were prepared using a separate tool that handles various aspects of ligand preparation.\n\nThe node and edge features were updated iteratively using information from the connecting nodes and edges. This process allowed the model to capture not only the core atom but also its surrounding neighbors, providing a more comprehensive representation of the molecular structure. Three convolution rounds were applied to process these features further.\n\nThe node features recovered by the graph neural networks (GNNs) and residual GNNs for both the ligand and target were concatenated pairwise. This combined information was then fed into a mixture density network (MND). The MND used a multi-layer perceptron (MLP) to construct a hidden representation from the concatenated target and ligand node information. The outputs of the MND were computed using this hidden representation.\n\nThe MLPs utilized in this process consisted of a linear layer followed by an Exponential Linear Unit (ELU) activation function. The ELU was chosen for its benefits in stabilizing training by allowing for smoother gradients, which is particularly important in complex networks like GNNs. The ELU's ability to output negative values helps combat the vanishing gradient problem, which is crucial in deep architectures.\n\nA dropout rate of 0.1 was employed to prevent over-regularization while maintaining model generalization. This rate was determined based on previous studies and experimentation with similar deep-learning models, ensuring a balance between reducing overfitting and maintaining predictive accuracy.",
  "optimization/parameters": "In our study, the optimization process involved several key parameters that were carefully selected to ensure effective and efficient convergence. The model utilized a population size of 150 for the differential evolution algorithm, which was chosen based on extensive experimentation and literature review to balance computational efficiency and the exploration of the parameter space. The recombination constant was set to 0.8, and the mutation constant was dynamically adjusted between 0.5 and 1.0 in each generation. These values were selected to promote diversity within the population while maintaining convergence stability.\n\nThe optimization was performed up to 500 iterations, a number determined through preliminary runs and validation accuracy assessments. This iteration count was found to provide a good trade-off between computational time and the thorough exploration of potential solutions. The Euler angles and dihedral angles of rotatable bonds were constrained between \u2212\u03c0 and \u03c0, ensuring that the ligand conformations remained within physically plausible ranges.\n\nThe learning rate for the Adam optimizer was set to 0.002, a value derived from best practices in geometric deep learning models and refined through preliminary experiments. The batch size for training was set to 16 protein-ligand complexes, and the model was trained for 150 epochs. These hyperparameters were chosen to ensure stable convergence and to balance training time with performance.\n\nAdditionally, a dropout rate of 0.1 was employed to prevent over-regularization while maintaining model generalization. This rate was determined based on previous studies and experimentation with similar deep-learning models, ensuring that the model could generalize well to unseen data without overfitting.\n\nOverall, the selection of these parameters was guided by a combination of literature review, preliminary experiments, and validation accuracy assessments, aiming to achieve a robust and efficient optimization process.",
  "optimization/features": "In the optimization process, the input features are derived from both the ligand and the target protein. The features used include the edge features and the features of the two connecting nodes. These features are updated iteratively through a series of convolutions, specifically three convolutions, to capture information about the core atom and its surrounding neighbors. The node and edge features are processed by graph neural network (GNN) blocks, which help in updating the node features.\n\nThe features from the ligand and target are then pairwise concatenated and fed into a mixed-density network (MND). This network uses a multi-layer perceptron (MLP) to construct a hidden representation from the concatenated target and ligand node information. The MLP consists of a linear layer followed by an Exponential Linear Unit (ELU) activation function, which was chosen for its benefits in stabilizing training and combating the vanishing gradient problem.\n\nFeature selection was not explicitly mentioned as a separate step in the process. The features used are directly derived from the molecular structures of the ligands and targets, and the concatenation process ensures that all relevant information is included in the input for the subsequent layers of the network. The use of ELU activation functions and a dropout rate of 0.1 helps in maintaining model generalization and preventing overfitting, ensuring that the model learns meaningful patterns from the input features.",
  "optimization/fitting": "In our study, the fitting method employed geometric deep learning techniques, which inherently involve a large number of parameters due to the complexity of the molecular structures being modeled. The number of parameters in our model is indeed much larger than the number of training points. To address the potential issue of overfitting, several strategies were implemented.\n\nFirstly, we utilized a dropout rate of 0.1 during training. This technique randomly sets a fraction of the input units to zero at each update during training time, which helps to prevent overfitting by ensuring that the model does not become too reliant on any single feature. The choice of 0.1 was based on previous studies and experimentation, providing a balance between reducing overfitting and maintaining predictive accuracy.\n\nSecondly, we employed a rigorous validation process. The model was trained for 150 epochs with a batch size of 16 protein-ligand complexes. The learning rate was set to 0.002, which was determined through preliminary runs and validation accuracy checks. This approach ensured that the model generalized well to unseen data, rather than merely memorizing the training set.\n\nAdditionally, the use of the Exponential Linear Unit (ELU) activation function helped stabilize training by allowing for smoother gradients, which is particularly beneficial in complex networks like Graph Neural Networks (GNNs). ELU's ability to output negative values also helped combat the vanishing gradient problem, further aiding in the prevention of overfitting.\n\nTo rule out underfitting, we ensured that the model had sufficient capacity to learn the underlying patterns in the data. The architecture included multiple convolution rounds and residual GNN blocks, which allowed the model to capture intricate details of the molecular structures. The training process was monitored closely, and the model's performance was evaluated on a separate validation set to ensure that it was learning effectively.\n\nOverall, the combination of these techniques\u2014dropout, careful selection of hyperparameters, use of ELU activation, and thorough validation\u2014helped us to mitigate both overfitting and underfitting, resulting in a robust and generalizable model.",
  "optimization/regularization": "A dropout rate of 0.1 was employed to prevent over-regularization while maintaining model generalization. This rate was determined based on previous studies and experimentation with similar deep-learning models. Given the complexity of the models used and the molecular nature of the data, a smaller dropout rate was appropriate to balance reducing overfitting and maintaining predictive accuracy. Experimentation confirmed that 0.1 provided the right balance between these two factors.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, the training process involved using the Adam optimizer with a learning rate of 0.002, a batch size of 16 protein-ligand complexes, and 150 epochs. The model employed a dropout rate of 0.1 to prevent over-regularization while maintaining generalization. These parameters were chosen based on best practices from the literature on geometric deep learning models and were refined through preliminary runs and validation accuracy.\n\nThe detailed geometric learning protocol, including the specific configurations and optimization parameters, can be accessed in the original publication. Additionally, the sequence of methods used in the study is illustrated in Figure 1, providing a visual representation of the steps followed.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly mentioned as being publicly accessible. However, the methods and configurations described in the publication are comprehensive and can be replicated by researchers interested in similar studies. The publication itself serves as a guide for implementing the described techniques.\n\nThe license under which the publication is available is not specified in the provided context. Typically, such information can be found on the publisher's website or within the publication itself. For specific details on licensing, readers are advised to refer to the publisher's policies or the publication's copyright notice.",
  "model/interpretability": "The model employed in this study leverages geometric deep learning, specifically utilizing Graph Neural Networks (GNNs), to predict binding conformations of bioactive molecules. This approach is inherently more interpretable than traditional black-box models due to its reliance on graph structures that represent molecular interactions.\n\nThe GNNs used in this study process both node and edge features, which correspond to atoms and bonds in molecular structures. By updating these features through multiple convolution rounds, the model captures information about not only individual atoms but also their surrounding neighbors. This allows for a more nuanced understanding of molecular interactions.\n\nOne key aspect of interpretability in this model is the use of a Mixture Density Network (MND) to combine concatenated target and ligand node information. The MND constructs a hidden representation using a Multi-Layer Perceptron (MLP), which includes a linear layer and an Exponential Linear Unit (ELU) activation function. The ELU activation function was chosen for its ability to stabilize training and combat the vanishing gradient problem, making the model's learning process more transparent.\n\nAdditionally, the model's outputs are computed using a potential specific to each target-ligand complex, which is defined by the loss function. This potential is used to score the 3-dimensional structure of the target-ligand complex by summing over all possible pairs and calculating the negative log-likelihood for each target-ligand node pair. This approach provides a clear framework for understanding how the model evaluates binding conformations.\n\nThe use of differential evolution to determine the most likely ligand conformation that interacts with the target binding site further enhances the model's interpretability. By minimizing the potential learned by the model, the differential evolution algorithm provides a clear pathway for understanding how the model predicts binding interactions.\n\nOverall, the model's reliance on graph structures, the use of interpretable activation functions, and the clear framework for evaluating binding conformations make it a transparent and interpretable tool for predicting molecular interactions.",
  "model/output": "The model is primarily designed for regression tasks, focusing on predicting the binding conformations of bioactive molecules. It employs a geometric deep learning approach to determine the most likely distances between ligand atoms and specific points on the molecular surface of the binding site. The output of the model is a potential that is calculated as the combination of the negative log-likelihood of all pairwise combinations of ligand atoms and points in the molecular surface. This potential is used to score the target-ligand complex's 3-dimensional structure, with the optimal conformation being the one that minimizes this potential. The model's outputs are computed using a hidden representation constructed by a mixture density network (MND) with a multi-layer perceptron (MLP). The MLP consists of linear layers and an Exponential Linear Unit (ELU) activation function, which was chosen for its benefits in stabilizing training and combating the vanishing gradient problem. The model's predictions are used to guide the differential evolution algorithm in finding the ligand conformation that most likely interacts with the target binding site. The final output is a score that indicates the likelihood of the target-ligand combination in a particular conformation, with a decreasing value signifying an increased likelihood.",
  "model/duration": "The model was trained for 150 epochs using a batch size of 16 protein-ligand complexes. The specific execution time for the model to run is not detailed, but the training parameters were chosen based on best practices from the literature on geometric deep learning models, ensuring stable convergence and balancing training time and performance. The training process involved using the Adam optimizer with a learning rate of 0.002 to update the model weights. The loss function was minimized during training, and the model's performance was refined based on preliminary runs and validation accuracy. Additionally, the use of a dropout rate of 0.1 helped in preventing over-regularization while maintaining model generalization. The training setup was designed to be efficient, leveraging the computational advantages of geometric deep learning to accelerate the drug discovery pipeline.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several steps to ensure the robustness and validity of the findings. Initially, virtual screening of the cannabis compound database was performed using a trained Graph Neural Network (GNN) deep learning model. This step helped in identifying potential NS5B inhibitors.\n\nTo validate the results from the virtual screening, re-docking and conventional docking methods were employed. This was particularly important for ligands with more than 10 rotatable bonds, as these compounds required additional validation due to the inefficiency of optimization algorithms with a large number of degrees of freedom.\n\nFollowing the docking validation, ADMET screening was conducted to assess the drug-likeness and safety profiles of the compounds. This screening helped in narrowing down the list of potential hits to those with optimal ADMET profiles.\n\nTo further confirm the candidacy of the top hits, Free Energy Perturbation/Molecular Dynamics (FEP/MD) simulations were performed. These simulations provided insights into the binding affinities and stability of the ligand-protein complexes. The results from these simulations were averaged using the standard error of the mean (SEM), ensuring the convergence and reliability of the findings.\n\nAdditionally, the docking scores and binding affinity ranges of the compounds were compared with those from previous studies. This comparison highlighted the superior potential of the identified compounds for target interaction.\n\nThe evaluation process also included the introduction of resistance-associated amino acid substitutions (RAASs) in the target protein. The identified compounds were re-docked with these mutants to assess their binding affinities, providing valuable insights into their potential efficacy against resistant phenotypes.\n\nOverall, the evaluation method combined computational techniques with experimental validation steps, ensuring a comprehensive assessment of the identified compounds as potential NS5B inhibitors.",
  "evaluation/measure": "In the evaluation of our study, several performance metrics were reported to comprehensively assess the effectiveness of the geometric deep learning approach for virtual screening and docking.\n\nThe primary metric reported is the docking score, measured in kcal/mol, which indicates the binding affinity of the ligands to the target protein. This metric is crucial as it directly relates to the potential efficacy of the compounds as inhibitors. Lower (more negative) docking scores signify stronger binding affinities.\n\nAdditionally, ligand efficiency was calculated, providing insight into the binding efficiency per heavy atom in the ligand. This metric is important for understanding the cost-effectiveness of the ligand in terms of binding affinity.\n\nBinding affinity ranges, expressed in nanomolar (nM) concentrations, were also reported. These ranges offer a broader view of the compounds' binding strengths across different conditions and concentrations.\n\nThe Synthetic Accessibility Score (SA Score) was included to evaluate the ease of synthesizing the compounds. A lower SA Score indicates simpler synthesis, which is beneficial for drug development.\n\nToxicological properties were assessed using metrics such as the maximum recommended daily dose (FDAMDD), mutagenicity (Ames test), and the probability of p53 activation (SR-p53). These metrics are essential for evaluating the safety and potential side effects of the compounds.\n\nThe fathead minnow LC50 (LC50FM) was reported to assess the environmental impact and toxicity of the compounds. This metric is particularly relevant for understanding the ecological safety of the compounds.\n\nThe reported metrics are representative of standard practices in the field of drug discovery and molecular docking. They align with commonly used performance measures in the literature, ensuring that our evaluation is both rigorous and comparable to other studies. The inclusion of diverse metrics provides a holistic view of the compounds' potential as therapeutic agents, covering aspects of binding affinity, synthetic feasibility, and safety.",
  "evaluation/comparison": "A comparison to publicly available methods was performed using the CASF-2016 benchmark, which includes 285 carefully chosen protein-ligand complexes. This benchmark was used to evaluate the screening and scoring power of the methods employed in this study. The preprocessing of the structures from this benchmark was identical to that of the training set, ensuring a fair comparison.\n\nThe methods used in this study involved geometric deep learning, specifically a trained Graph Neural Network (GNN) deep learning model, for virtual screening of compounds from the cannabis compound database. The results from this virtual screening were validated through re-docking and conventional docking, particularly for ligands with more than 10 rotatable bonds. This integration of conventional docking methods was necessary to handle the complexity introduced by a large number of degrees of freedom in such ligands.\n\nAdditionally, the study included a comparison to simpler baselines through the use of conventional docking and binding affinity determination. The results of these conventional methods agreed with the geometric deep learning results, validating the compounds under investigation. This multi-step approach ensured that the findings were robust and reliable, leveraging both advanced deep learning techniques and established docking methods.\n\nNot applicable",
  "evaluation/confidence": "The evaluation of our method includes several statistical measures to ensure the confidence and significance of our results. We employed the CASF-2016 benchmark, which comprises 285 carefully selected protein-ligand complexes, to assess the screening and scoring power of our approach. This benchmarking process involved preprocessing the structures identically to our training set, ensuring a fair and consistent evaluation.\n\nTo determine the optimal binding conformations, we used differential evolution, a robust optimization algorithm. This method minimizes the potential learned by our model for each target-ligand complex, providing a reliable measure of binding affinity. The likelihood of finding the target-ligand combination in a particular conformation increases with decreasing values, indicating stronger binding.\n\nWe also conducted molecular dynamics (MD) simulations to further validate our findings. The average binding free energy of the simulated hits was estimated using MM/P/GBSA calculations, which showed strong binding affinities with all energies being negative. Additionally, FEP/MD simulations were performed to evaluate the convergence and reliability of the results. The standard error of the mean (SEM) for all data was less than 0.5 kcal/mol, indicating that all systems had converged within 10 ns.\n\nTo assess the statistical significance of our results, we performed paired two-tailed t-tests to compare the wild type and mutant proteins. These tests showed that the docking scores of many ligands were significantly reduced, except for a subset of compounds that maintained high docking scores. This suggests that these compounds may still bind effectively to resistant phenotypes, providing valuable insights into their potential efficacy.\n\nOverall, our evaluation includes rigorous statistical analyses and simulations to ensure the confidence and significance of our results. The use of established benchmarks, optimization algorithms, and statistical tests provides a comprehensive assessment of our method's performance and its superiority over other approaches.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being publicly available. The results and discussions presented in the publication are derived from various computational methods and simulations, such as geometric deep learning virtual screening, conventional docking, and ADMET screening. These methods generated specific data points, such as binding affinities, docking scores, and ADMET properties, which are summarized in tables and figures within the publication. However, the detailed raw data files, such as the history files obtained via FEP lambda replica exchange MD (\u03bb-REMD) using simple overlap sampling (SOS), are not discussed in terms of public availability or access. Therefore, it is not clear whether these raw files are publicly released or how they can be accessed if they are."
}