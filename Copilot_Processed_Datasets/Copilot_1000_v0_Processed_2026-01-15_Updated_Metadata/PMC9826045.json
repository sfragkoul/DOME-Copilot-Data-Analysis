{
  "publication/title": "Explainable haemoglobin deferral predictions using machine learning models: Interpretation and consequences for the blood supply.",
  "publication/authors": "Vinkenoog M, van Leeuwen M, Janssen MP",
  "publication/journal": "Vox sanguinis",
  "publication/year": "2022",
  "publication/pmid": "36102148",
  "publication/pmcid": "PMC9826045",
  "publication/doi": "10.1111/vox.13350",
  "publication/tags": "- Blood donation testing\n- Donor health\n- Haemoglobin measurement\n- Machine learning\n- Support vector machines\n- Predictive modeling\n- Deferral prediction\n- Blood supply management\n- SHAP values\n- Data-driven decision making",
  "dataset/provenance": "The dataset used in this study originates from blood bank visits, focusing on the relative change in visits per time unit and the relative yield of blood donations. The analyses were conducted using Python 3.9, with various modules such as numpy, pandas, sklearn, shap, and matplotlib for data processing, model training, predictions, calculating SHAP values, and creating graphs, respectively.\n\nThe dataset includes training and test sets for different models, with sample sizes varying based on the number of previous blood bank visits. For instance, the training set for SVM-1 includes 128,173 data points for women and 121,746 for men, while the test set includes 110,372 data points for women and 98,324 for men. The deferral rates in the training datasets are 3.19% for women and 1.22% for men, and in the test sets, they are 3.42% for women and 1.21% for men.\n\nThe dataset has been used to train five different support vector machine (SVM) models for each sex, named SVM-1 through SVM-5, based on the number of previous blood bank visits used as predictor variables. The models were trained on blood bank visits before 2021 and tested on visits in 2021 to validate performance on unseen data. This division was chosen to simulate real-world application, where models would be trained on historical data and applied to future data.\n\nThe dataset includes various predictor variables such as age, number of previous donations, ferritin levels, and haemoglobin levels from previous visits. The marginal distributions of these predictor variables are described in detail, showing how they vary with the number of previous donations. The dataset has also been used to assess the potential impact of using SVMs to guide donor invitations by predicting deferral for all blood bank visits that took place in 2021. The analysis code is available as a GitHub repository and indexed on Zenodo, making it accessible for further use by the community.",
  "dataset/splits": "There are two main data splits: a time split and a random split. The time split uses blood bank visits before 2021 as the training set and visits in 2021 as the test set. This split was chosen to simulate real-world application, where models are trained on historical data and applied to future data.\n\nThe random split, used to assess the generalizability of the model to new donors, comprises the last blood bank visit of 20% of all unique donors as the test set, with the training set including all donations from the remaining 80% of donors.\n\nFor each sex, five support vector machines (SVMs) were trained, named SVM-1 through SVM-5, indicating the number of previous blood bank visits used as predictor variables. Sample sizes decrease from SVM-1 to SVM-5 as donors are only included in SVM-n if they have at least n previous visits.\n\nThe size of the training and test datasets varies across models. For instance, in SVM-1, the training set for women includes 128,173 data points with 4,084 deferrals (3.19%), while the test set includes 110,372 data points with 3,696 deferrals (3.35%). For men, the training set in SVM-1 includes 121,746 data points with 1,339 deferrals (1.10%), and the test set includes 98,324 data points with 1,074 deferrals (1.09%).\n\nDeferral rates in the training datasets are 3.19% for women and 1.22% for men; in the test sets, they are 3.42% for women and 1.21% for men. The difference in deferral rate between the training and test datasets is significant for women but not for men. This difference is not corrected for, as the models are intended for future predictions where the deferral rate is unknown.",
  "dataset/redundancy": "The datasets were split based on time, with blood bank visits before 2021 used as the training set and visits in 2021 as the test set. This approach was chosen to simulate real-world application, where models are trained on historical data and applied to future data. The training and test sets are not entirely independent due to the inclusion of repeat donors, but the test set consists of unseen data from the training period. To enforce independence and assess generalizability, a separate experiment was conducted where the test set comprised the last blood bank visit of 20% of all unique donors, with the training set including all donations from the remaining 80% of donors.\n\nThe distribution of the datasets differs from some previously published machine learning datasets due to the temporal split and the inclusion of repeat donors. The deferral rates in the training and test datasets are similar, with a significant difference for women but not for men. The sample sizes of the training and test datasets are comparable, despite the training set spanning three years and the test set only one year. This is because donations were only included from donors with at least one ferritin measurement, and ferritin screening was implemented using a stepped wedge approach. The marginal distributions of predictor variables, such as age, number of previous donations, and ferritin levels, vary across models due to the differing number of previous visits required for inclusion. The datasets are imbalanced, with deferral rates ranging from approximately 1% to 3.5%, which is typical for blood donation datasets focusing on deferral prediction.",
  "dataset/availability": "The data used in this study is not publicly released. The data on blood bank visits by whole-blood donors was extracted from Sanquin\u2019s database system eProgesa. Only data from donors who explicitly provided informed consent for the use of their data for scientific research were used. This consent is given by more than 99% of all donors. The data includes information such as donor sex, donor age, donation date, donation time, haemoglobin level, and ferritin level. Ferritin is measured at every new donor intake and upon every fifth donation in repeat donors. Therefore, ferritin levels are unavailable for most donations. The analysis code is available as a GitHub repository and indexed on Zenodo with the DOI: 10.5281/zenodo.6938112. The code includes the scripts used for data processing, model training, and predictions, as well as the creation of graphs and calculation of SHAP values. The data splits used for training and testing the models are described in the methods section. The training datasets consist of 3 years of data, and the test datasets consist of 1 year of data. The sample sizes of the training and test datasets for each model are provided in Table 3. The deferral rates in the training and test datasets are also provided in the results section. The models were trained on data from blood bank visits before 2021, and the test set consisted of visits in 2021. This division was chosen to validate the performance of the models on unseen data. The models were also tested on a separate experiment in which the test set is comprised of the last blood bank visit of 20% of all unique donors, and the training set includes all donations from the remaining 80% of donors.",
  "optimization/algorithm": "The machine-learning algorithm class used is Support Vector Machines (SVMs). SVMs are supervised learning models that are well-established in the field of machine learning. They are used to find the optimal hyperplane that separates outcome classes based on predictor variables.\n\nThe SVM algorithm is not new; it has been extensively used and studied in various applications, including classification problems. The choice of SVM in this context is driven by the nature of the predictor variables, which are numeric, and the computational efficiency of SVMs compared to other algorithms like K-nearest neighbors or dynamic linear mixed models.\n\nThe focus of this work is on the application of SVMs to predict hemoglobin deferral in blood donors, rather than the development of a new machine-learning algorithm. Therefore, it is published in a journal focused on blood transfusion and hematology, rather than a machine-learning journal. The emphasis is on the practical implications and the potential impact on blood supply management, leveraging the explanatory power of SHAP values to understand the model's predictions.",
  "optimization/meta": "The models used in this study are not meta-predictors. They are support vector machines (SVMs) that directly predict haemoglobin deferral based on numeric predictor variables. The SVMs are trained and optimized independently for each sex and for different numbers of previous blood bank visits, but they do not use data from other machine-learning algorithms as input.\n\nFive separate SVMs were trained for each sex, named SVM-n for n between one and five, indicating the number of previous blood bank visits used as predictor variables. The predictor variables include haemoglobin levels from previous visits and the time since those measurements. The models are trained on historical data and validated on unseen data from 2021. The training and test sets are divided based on time, ensuring that the models are evaluated on future data, which simulates their practical use.\n\nHyperparameters for each of the 10 models (SVM-1 through SVM-5 for both sexes) were optimized separately using stratified five-fold cross-validation within the training set data. This process ensures that the training data is independent of the test data, maintaining the integrity of the validation process. The optimization used grid search with balanced accuracy as the scoring method, which is suitable for imbalanced datasets. The models' performance was assessed using precision and recall for both outcome classes, and SHAP values were used to explain the model predictions.",
  "optimization/encoding": "The data used for the machine-learning algorithm was extracted from a database system, focusing on whole-blood donations. The dataset included information such as donor sex, age, donation date and time, haemoglobin level, and ferritin level. Ferritin levels were measured at new donor intakes and every fifth donation for repeat donors, so they were not available for most donations.\n\nPredictor variables were calculated for each visit, including the number of previous blood bank visits and the days since the last visit. The outcome variable was dichotomous, indicating whether a donor was deferred due to haemoglobin levels below the eligibility threshold or not.\n\nThe dataset consisted of 938,710 blood bank visits by 241,131 unique donors between October 2017 and December 2021. After excluding visits without previous ferritin measurements, 458,615 visits by 157,423 unique donors remained for analysis.\n\nThe data was split into training and test sets. For the main analysis, visits before 2021 were used for training, and visits in 2021 were used for testing. This approach simulated real-world application, where models are trained on historical data and applied to future data. Additionally, a separate experiment used a random split, where the test set consisted of the last visit of 20% of all unique donors, and the training set included all donations from the remaining 80% of donors. This was done to assess the generalizability of the model to new donors.\n\nHyperparameters were optimized using stratified five-fold cross-validation within the training set, ensuring that each fold had a balanced representation of the outcome variable. Grid search was employed to find the best hyperparameters, with balanced accuracy as the scoring method. This approach was particularly suitable for handling imbalanced datasets, as it used class-balanced sample weights to determine the average recall.",
  "optimization/parameters": "The models used in this study are Support Vector Machines (SVMs), and the number of input parameters varies depending on the specific model. For each sex, five different SVMs were trained, named SVM-n, where n ranges from one to five. This naming convention indicates the number of previous blood bank visits used as predictor variables. Therefore, the number of input parameters increases with each subsequent model:\n\n- SVM-1 uses 2 parameters: HbPrev1 and DaysSinceHb1.\n- SVM-2 uses 4 parameters: HbPrev1, DaysSinceHb1, HbPrev2, and DaysSinceHb2.\n- SVM-3 uses 6 parameters: HbPrev1, DaysSinceHb1, HbPrev2, DaysSinceHb2, HbPrev3, and DaysSinceHb3.\n- SVM-4 uses 8 parameters: HbPrev1, DaysSinceHb1, HbPrev2, DaysSinceHb2, HbPrev3, DaysSinceHb3, HbPrev4, and DaysSinceHb4.\n- SVM-5 uses 10 parameters: HbPrev1, DaysSinceHb1, HbPrev2, DaysSinceHb2, HbPrev3, DaysSinceHb3, HbPrev4, DaysSinceHb4, HbPrev5, and DaysSinceHb5.\n\nThe selection of these parameters was based on the availability of previous blood bank visits for each donor. Donors are only included in SVM-n if they have at least n previous visits. This approach ensures that the models can leverage historical data to make more accurate predictions. The choice of using up to five previous visits was likely determined by the balance between the availability of data and the potential improvement in predictive performance.",
  "optimization/features": "The models utilized in this study employed a specific set of input features to predict haemoglobin deferral. The number of features varied depending on the model, ranging from one to five previous blood bank visits. For each model, the features included previous haemoglobin measurements and the days since these measurements. Specifically, the features used were HbPrevn and DaysSinceHbn, where n represents the number of previous visits considered by the model.\n\nFeature selection was not explicitly performed in the traditional sense, as the features were predetermined based on the number of previous visits. Instead, the models were designed to incorporate different numbers of previous visits, with each model (SVM-n) using n previous visits. This approach inherently selected the features based on the availability of previous visits for each donor.\n\nThe division of data into training and test sets was done temporally, with visits before 2021 used for training and visits in 2021 used for testing. This temporal split ensured that the models were trained on historical data and tested on future data, simulating real-world application. The hyperparameters were optimized using stratified five-fold cross-validation within the training set, ensuring that the test data remained unseen during this process. This method helped in assessing the model's performance on unseen data and ensured that the feature selection and model optimization were done using the training set only.",
  "optimization/fitting": "The fitting method employed in this study utilized support vector machines (SVMs) for predicting haemoglobin deferral. SVMs are well-suited for handling numeric predictor variables and are computationally efficient compared to other algorithms like K-nearest neighbours or dynamic linear mixed models.\n\nTo address the potential issue of overfitting, especially given the large number of parameters relative to the training points, several strategies were implemented. Firstly, hyperparameters were optimized using stratified five-fold cross-validation within the training set. This approach ensures that the model's performance is evaluated on multiple subsets of the data, reducing the risk of overfitting to any single subset. Additionally, balanced accuracy was used as the scoring method, which is particularly suitable for imbalanced datasets. This method uses class-balanced sample weights to determine the average recall, further mitigating the risk of overfitting.\n\nTo rule out underfitting, the models were evaluated on both training and test datasets. The performance metrics, including precision and recall, were compared across these datasets. The similarity in performance between the training and test sets indicates that the models are well-fitted and generalize well to unseen data. Furthermore, the use of SHAP values provided insights into the contribution of each variable to the predictions, ensuring that the models are not overly simplistic.\n\nThe models were trained on data from blood bank visits before 2021 and tested on visits in 2021, simulating a real-world scenario where the models would be applied to future data. This temporal split helps in assessing the model's ability to generalize to new, unseen data, further confirming that underfitting is not a concern. The consistent performance across different splits and the minimal differences in precision and recall between the time split and random split also support the robustness of the models.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method was the use of stratified five-fold cross-validation within the training set data. This approach helps to ensure that each fold of the cross-validation process is representative of the overall class distribution, which is particularly important for imbalanced datasets like ours. By optimizing hyperparameters using grid search and balanced accuracy as the scoring method, we aimed to find the best model configuration that generalizes well to unseen data.\n\nAdditionally, we divided our data into training and test sets based on time rather than randomly. This division mimics the real-world scenario where models are trained on historical data and applied to future data. This temporal split helps to assess the model's performance on truly unseen data, reducing the risk of overfitting to the training set.\n\nFurthermore, we used SHapley Additive exPlanations (SHAP) values to explain the model predictions. SHAP values provide a way to understand the contribution of each variable to the prediction for individual observations, which helps in interpreting the model's decisions and ensuring that the model is not relying on spurious correlations in the training data.\n\nLastly, we compared the performance of our models on different types of splits, including a random split and a time split. The similar performance metrics across these splits indicate that our models are robust and not overly fitted to the specific characteristics of the training data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available. We employed grid search for hyperparameter optimization, utilizing stratified five-fold cross-validation within the training set data. The balanced accuracy, defined as the weighted average of recall in both classes, was used as the scoring method. This approach is particularly suitable for imbalanced datasets, as it uses class-balanced sample weights to determine the average recall.\n\nThe analysis code, including the specifics of the hyper-parameter configurations and optimization schedule, is available as a GitHub repository. This repository is indexed on Zenodo with the DOI [10.5281/zenodo.6938112](https://doi.org/10.5281/zenodo.6938112). The code is released under an open-source license, allowing other researchers to access, use, and build upon our work. This transparency ensures that the methods and results can be replicated and verified by the scientific community.",
  "model/interpretability": "The model employed in this study is not a black box. To ensure transparency and interpretability, SHapley Additive exPlanations (SHAP) values were utilized. SHAP values provide a clear and detailed explanation of how each variable contributes to the model's predictions for individual observations. This method is model-agnostic, meaning it can be applied to any machine learning model to make its predictions more interpretable.\n\nBy using SHAP values, we can quantify the contribution of each predictor variable to the outcome, whether it indicates deferral or non-deferral. For instance, variables such as the donor's age, the number of previous donations, and the most recent ferritin level can be analyzed to understand their impact on the prediction. The SHAP summary plots visually represent these contributions, showing the direction and magnitude of each variable's effect on the model's output.\n\nThis approach allows us to relate the model's predictions to known physiological processes, making the decisions more understandable and trustworthy. For example, if a donor's ferritin level is low, the SHAP value will indicate a higher likelihood of deferral, aligning with medical knowledge that low ferritin levels are associated with anemia. This transparency is crucial for practical implementation, as it ensures that stakeholders can understand and trust the model's decisions.",
  "model/output": "The model employed in our study is a classification model. Specifically, we utilized support vector machines (SVMs) to predict haemoglobin deferral. This type of model is designed to find the optimal hyperplane that separates the outcome classes based on the predictor variables. The outcome in our case is binary: deferred or not deferred. Therefore, the model classifies each observation into one of these two categories. The performance of the model was evaluated using metrics such as precision and recall for both outcome classes, which are typical for classification tasks. Additionally, the model provides the probability of an observation belonging to each outcome class, further supporting its classification nature.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The analysis code for the models presented in this publication is publicly available. It can be accessed via a GitHub repository, which is indexed on Zenodo. The specific DOI for accessing the code is provided for convenience. The code is released under a license that permits use, distribution, and reproduction in any medium, provided the original work is properly cited and is not used for commercial purposes. This ensures that other researchers can reproduce the results and build upon the work presented.",
  "evaluation/method": "The evaluation method employed for the models involved several key steps to ensure robustness and generalizability. For each sex, five support vector machine (SVM) models were trained, labeled SVM-1 through SVM-5, based on the number of previous blood bank visits used as predictor variables. The training set consisted of blood bank visits before 2021, while visits in 2021 were used as the test set to validate performance on unseen data. This temporal division was chosen to simulate real-world application, where models would be trained on historical data and applied to future data.\n\nHyperparameters for each of the 10 models (SVM-1 through SVM-5 for both sexes) were optimized using stratified five-fold cross-validation within the training set data. This process did not use the test data, ensuring an unbiased evaluation. The optimization was performed using grid search, with balanced accuracy as the scoring method. Balanced accuracy, defined as the weighted average of recall in both classes, is particularly suitable for imbalanced datasets.\n\nPrecision and recall were determined and compared for both training and test datasets for each model. These metrics were calculated for both outcome classes, providing a comprehensive evaluation of the models' performance. Additionally, a paired t-test was used to assess the difference in deferral rates between the training and test sets of donors of the same sex with the same number of previous donations.\n\nTo assess the generalizability of the models to new donors, a separate experiment was conducted. In this experiment, the test set comprised the last blood bank visit of 20% of all unique donors, while the training set included all donations from the remaining 80% of donors. This approach helped evaluate how well the models perform on donors who were not part of the training data.\n\nSHapley Additive exPlanations (SHAP) values were used to explain the model predictions. SHAP values show the contribution of each variable to the prediction for each individual observation, providing insights into the model's decision-making process. By summarizing these contributions, variable importance measures were obtained for models that do not have interpretable coefficients.",
  "evaluation/measure": "In our evaluation, we focused on several key performance metrics to assess the effectiveness of our models. We primarily reported precision and recall for both outcome classes, deferral and non-deferral. Precision for the deferral class indicates the proportion of donations correctly classified as deferrals out of all donations predicted as deferrals. Recall for the deferral class shows the proportion of true deferrals correctly identified by the model. For the non-deferral class, precision represents the proportion of true non-deferrals out of all predicted non-deferrals, while recall indicates the proportion of predicted non-deferrals out of all true non-deferrals.\n\nThese metrics are crucial for understanding the model's ability to accurately predict deferrals and non-deferrals, which is essential for guiding donor invitations and ensuring an efficient blood supply. The choice of these metrics is representative of common practices in the literature, particularly for imbalanced datasets where the classes are not equally represented. This set of metrics allows us to evaluate both the model's accuracy in predicting deferrals and its ability to correctly identify non-deferrals, which is vital for maintaining a reliable blood donation system.\n\nAdditionally, we used balanced accuracy as a scoring method during hyperparameter optimization. Balanced accuracy is the weighted average of recall in both classes and is particularly suitable for imbalanced datasets. This method ensures that the model performs well across both classes, providing a more comprehensive evaluation of its overall performance.\n\nIn summary, the reported performance metrics\u2014precision and recall for both deferral and non-deferral classes, along with balanced accuracy\u2014are well-aligned with established practices in the field. They provide a thorough assessment of the model's predictive capabilities and its potential impact on blood donation management.",
  "evaluation/comparison": "In our evaluation, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on developing and validating our own models using support vector machines (SVMs) tailored to predict haemoglobin deferral in blood donations. We trained and tested our models using specific datasets derived from blood bank visits, with a clear division between training and test sets based on time periods.\n\nFor the comparison to simpler baselines, we did not explicitly mention the use of simpler models as baselines in our evaluation. Our approach involved optimizing hyperparameters for each of the 10 models (SVM-1 through SVM-5 for both sexes) using stratified five-fold cross-validation within the training set data. This method ensured that our models were robust and well-fitted to the data. We assessed the performance of our models using metrics such as precision and recall for both outcome classes, which provided a comprehensive evaluation of their predictive accuracy.\n\nThe performance metrics indicated that our models handled the differences between the training and test sets effectively, showing similar precision and recall on both datasets. This suggests that our models are generalizable and can perform well on unseen data. Additionally, we used SHAP values to explain the model predictions, providing insights into the contribution of each variable to the predictions. This approach allowed us to understand the importance of different predictor variables in determining haemoglobin deferral.",
  "evaluation/confidence": "The evaluation of our models includes a detailed assessment of performance metrics, which are crucial for understanding the reliability and generalizability of our predictions. Precision and recall are key metrics used to evaluate the models' performance. Precision for the non-deferral class indicates the proportion of successful donations among all predicted non-deferrals, while recall for the non-deferral class shows the proportion of successful donations that are correctly predicted. These metrics are essential for assessing how well the models can guide donor invitations.\n\nStatistical significance is addressed through the use of paired t-tests to compare deferral rates between training and test datasets. For women, the difference in deferral rates between the training and test datasets is statistically significant (p = 0.002), indicating a notable difference. For men, the difference is not statistically significant (p = 0.070), suggesting more consistency between the datasets. This statistical analysis helps in understanding the robustness of the models across different datasets.\n\nThe models' performance is further validated by comparing the observed and predicted differences in deferral proportions between the training and test sets. The mean difference of these differences is only 0.05 percentage points, with a maximum difference of 0.12 percentage points. This indicates that the models are robust against modest changes in deferral rates, providing confidence in their ability to handle real-world variations.\n\nAdditionally, the performance metrics are compared between different types of training/test splits, including a time split and a random split. The results show minimal differences in precision and recall between these splits, reinforcing the models' generalizability. For women, the random split slightly outperforms the time split in terms of precision and recall. For men, the time split performs marginally better. These comparisons provide further evidence of the models' reliability and consistency across different evaluation scenarios.\n\nIn summary, the performance metrics are thoroughly evaluated with statistical significance tests and comparisons across different datasets and splits. The results demonstrate the models' robustness and generalizability, providing a strong foundation for their practical application in guiding donor invitations.",
  "evaluation/availability": "The analysis code used for the evaluation is publicly available and can be accessed via a GitHub repository. This repository is also indexed on Zenodo, with a specific DOI provided for easy access. The code includes all necessary scripts and modules used for data processing, model training, predictions, and the calculation of SHAP values. Additionally, the code for creating graphs and visualizations is included. The availability of this code ensures reproducibility and allows other researchers to validate or build upon the findings presented in the study. The specific DOI for accessing the repository is 10.5281/zenodo.6938112."
}