{
  "publication/title": "Clinical risk prediction models and informative cluster size: Assessing the performance of a suicide risk prediction algorithm.",
  "publication/authors": "Coley RY, Walker RL, Cruz M, Simon GE, Shortreed SM",
  "publication/journal": "Biometrical journal. Biometrische Zeitschrift",
  "publication/year": "2021",
  "publication/pmid": "34031916",
  "publication/pmcid": "PMC9134927",
  "publication/doi": "10.1002/bimj.202000199",
  "publication/tags": "- Correlated data\n- Electronic health records\n- Machine learning\n- Nonignorable cluster size\n- Predictive analytics\n- Suicide risk prediction\n- Clinical risk prediction\n- Logistic regression\n- Random forest\n- Informative cluster size",
  "dataset/provenance": "The dataset used in this study was sourced from electronic health records of seven health systems. These systems included HealthPartners, Henry Ford Health System, and various regions of Kaiser Permanente. The data encompassed outpatient mental health visits made by individuals aged 13 years or older between January 1, 2012, and June 30, 2015. A random sample of 40% of people with at least one outpatient visit to a mental health specialty provider was selected for analysis. This sampling approach was chosen to enable the examination of different sampling frameworks and prediction models, as using the entire sample would have been computationally prohibitive.\n\nThe development dataset included 1,518,968 outpatient mental health visits made by 207,915 unique individuals. This dataset was divided into training and test sets to develop and evaluate prediction models. The training dataset was used to fit the models, while the test dataset was used to estimate future performance. Additionally, a prospective validation dataset was used to mimic the prospective evaluation of the clinical performance of the prediction models. This validation dataset included data on all outpatient mental health visits for the same population from October 1, 2015, to September 30, 2017.\n\nThe dataset included a wide range of information available at the time of the outpatient visit, such as current and past diagnoses, prescriptions, and mental health care encounters. This information was used to estimate prediction models for suicide risk. The dataset also included data from the Patient Health Questionnaire (PHQ-9), which measures depressive symptoms and suicidal ideation. The resulting dataset comprised 149 predictors and an additional 164 interactions.\n\nThe dataset used in this study builds upon previous work, including data collection and variable definitions reported in Simon et al. (2018). The dataset has not been previously used by the community in its entirety, but components of it have been analyzed in prior studies. The focus of this study is on predicting the risk of suicide attempts within 90 days following a mental health visit, using data gathered from electronic health records. The dataset's size and comprehensiveness allow for the development of accurate risk prediction models and the precise assessment of their performance.",
  "dataset/splits": "In our study, we utilized two primary data splits for training and testing our prediction models: visit-level and person-level splits. For the visit-level split, the training set consisted of 1,518,968 visits, while the testing set had 987,329 visits. Within these visits, 10,171 and 6,614 visits, respectively, had associated events. The event rate per 1,000 visits was consistent at 6.7 for both sets. The number of unique people in the training and testing sets were 207,915 and 180,508, respectively, with 1,949 and 1,669 people experiencing any event. The event rate per 1,000 people was slightly varied, at 9.4 and 9.2, respectively. There were 2,444 and 2,061 unique events in the training and testing sets, respectively.\n\nFor the person-level split, the training set included 531,639 visits from 141,968 unique people, with 3,557 visits having events. The testing set comprised 987,038 visits from 135,144 unique people, with 6,638 visits having events. The event rates per 1,000 visits were consistent at 6.7 and 6.6, respectively. The number of people with any event was 1,271 in the training set and 1,278 in the testing set, with event rates per 1,000 people at 9.0 and 9.5, respectively. The unique events in the training and testing sets were 1,517 and 1,603, respectively.\n\nAdditionally, we explored cross-validation techniques within these splits. For the visit-level split, we compared visit-level and person-level cross-validation. For the person-level split, we only examined person-level cross-validation. We also considered two approaches for model estimation: observed cluster analysis and within-cluster resampling. The within-cluster resampling approach was only applied to the person-level training and test set split.",
  "dataset/redundancy": "The datasets were split using two distinct approaches: visit-level and person-level splits. In the visit-level split, visits from the same person could appear in both the training and test sets, which is not the case for the person-level split. This approach was chosen to reflect the clinical context of predicting suicide risk following a visit and to accommodate the time-varying nature of suicide risk within a person.\n\nThe independence of the training and test sets was not strictly enforced in the visit-level split, as visits from the same person could be present in both sets. This overlap could potentially lead to overfitting, particularly when using flexible models like random forest. To mitigate this, person-level cross-validation was also explored within the visit-level training/test split approach. This method reduced overfitting by ensuring that tuning parameters were selected based on out-of-cluster predictions, rather than on correlated observations across folds.\n\nIn contrast, the person-level split ensured that all visits from a given person were included together in either the training or test set, thereby maintaining independence between the two sets. This approach was found to be more robust and less prone to overfitting, especially when using models like random forest.\n\nThe distribution of visits, people, and events in the training and test sets varied depending on the split method used. For the visit-level split, the number of unique people and events available for model training was higher, which could improve power. However, this came at the cost of potential overfitting due to the overlap of visits between the training and test sets. The person-level split, while having fewer unique people and events in the training set, provided a more independent test set, leading to more reliable performance estimates.\n\nCompared to previously published machine learning datasets, the approach taken here highlights the importance of considering the clinical context and the nature of the data when deciding on a split method. The visit-level split is more aligned with the clinical purpose of intervening at the time of a visit, while the person-level split offers a more traditional approach to ensuring independence between training and test sets. The choice between these methods should be guided by the specific goals of the analysis and the characteristics of the data.",
  "dataset/availability": "Not applicable",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the ensemble learning method known as random forest. This approach involves creating multiple decision trees during training and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random forests are well-established and widely used in various fields due to their robustness and ability to handle complex datasets.\n\nThe random forest algorithm is not new; it was introduced by Breiman in 2001. It is a mature and extensively studied method in the machine-learning community. The reason it was not published in a machine-learning journal in the context of our study is that our focus was on applying established methods to a specific problem in clinical risk prediction, rather than developing a new algorithm. Our work aimed to evaluate the performance of random forests, along with logistic regression with LASSO, within the context of clustered clinical data to predict suicide attempts. The emphasis was on the application and comparison of these methods in a real-world healthcare setting, rather than the innovation of a new algorithm.",
  "optimization/meta": "The models discussed in this publication do not use data from other machine-learning algorithms as input. Instead, they rely on a variety of predictors collected at each visit, including demographics, comorbidity burden, and history of mental health and substance use diagnoses, prescription fills for psychiatric medications, mental health encounters, and past suicide attempts.\n\nThe prediction models were estimated using two primary methods: logistic regression with least absolute shrinkage and selection operator (LASSO) and random forest. Logistic regression with LASSO uses the L1 penalty to select stronger predictors of the outcome while shrinking the coefficient for weaker predictors towards zero. The degree of shrinkage is determined by the tuning parameter, \u03bb, which is selected using 10-fold cross-validation within the training dataset.\n\nRandom forest is a non-parametric ensemble learning method that comprises many decision trees, each estimated on a bootstrap sample of the training dataset. Random selection of predictors at each split in a tree reduces correlation between trees. The models were estimated using the Gini index measuring node impurity as the splitting rule. For each tree, suicide attempt risk predictions were returned equal to the proportion of events among visits in each terminal node. Predicted risk of a suicide for a visit was obtained by averaging across predictions from each tree. The tuning parameters\u2014minimum terminal node size and the number of predictors considered at each split\u2014were selected using cross-validation. The models were estimated with 200 trees to limit the number of tuning parameter combinations evaluated.\n\nThe training and test datasets were divided using either a visit-level or person-level split. The visit-level split included more unique people and events for model training than the person-level split. The performance of the models was evaluated using the area under the receiver operating characteristic curve (AUC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). The models were validated using a prospective validation set to assess their performance on future visits.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for ensuring the machine-learning algorithms could effectively learn from the clinical data. We began by structuring our dataset to reflect the hierarchical nature of the data, where visits are nested within individuals. This structure is essential for accurately modeling the risk of suicide attempts, as it accounts for the correlation between multiple visits from the same person.\n\nFor the logistic regression with LASSO, we applied standard preprocessing techniques, including normalization and handling of missing values. The LASSO method inherently performs feature selection by shrinking the coefficients of less important predictors to zero, which helps in reducing overfitting and improving the model's generalization to new data. The degree of shrinkage was determined using 10-fold cross-validation within the training dataset, ensuring that the model was optimized for predictive performance.\n\nFor the random forest models, we utilized a non-parametric approach that does not require extensive preprocessing of the data. However, we did ensure that categorical variables were appropriately encoded, typically using one-hot encoding, to make them suitable for the algorithm. The random forest method involves creating multiple decision trees, each trained on a bootstrap sample of the data. This ensemble approach helps to reduce overfitting and improve the robustness of the predictions. We selected the number of trees and other hyperparameters through cross-validation to optimize the model's performance.\n\nBoth logistic regression with LASSO and random forest models were trained using a visit-level loss function. This choice was made because the risk of suicide can vary over time within an individual, and our goal was to develop a model that could be applied at the time of a mental health visit to identify high-risk individuals. The tuning parameters for both models were selected to optimize the visit-level area under the receiver operating characteristic curve (AUC), ensuring that the models were well-calibrated for predicting suicide attempts.\n\nIn summary, our data encoding and preprocessing steps were designed to leverage the strengths of both logistic regression with LASSO and random forest methods. By carefully handling the hierarchical structure of the data and optimizing the models through cross-validation, we aimed to create robust and accurate prediction models for suicide risk.",
  "optimization/parameters": "In our study, we employed two main prediction models: logistic regression with LASSO and random forest. For the logistic regression with LASSO, the number of non-zero coefficients, which can be considered as the effective number of parameters, varied depending on the sampling framework and the value of the tuning parameter \u03bb. This parameter controls the degree of shrinkage, with larger values leading to more shrinkage and fewer non-zero coefficients. The specific values of \u03bb and the corresponding number of non-zero coefficients for different sampling frameworks are detailed in our results.\n\nFor the random forest models, the number of predictors considered at each split was a crucial tuning parameter. We examined different values for this parameter, including the default recommendation, which is the square root of the total number of predictors (17 in our dataset), as well as twice and half of this default value. The minimum terminal node size was another important parameter, affecting the depth of the trees and thus the complexity of the model.\n\nThe selection of these tuning parameters was performed using cross-validation within the training dataset. This process ensured that the chosen parameters optimized the visit-level area under the receiver operating characteristic curve (AUC), which was our primary metric for model performance. By using cross-validation, we aimed to balance model complexity and generalization, thereby reducing the risk of overfitting.\n\nIn summary, the number of parameters in our models was determined by the tuning parameters selected through cross-validation. For logistic regression with LASSO, this involved selecting the optimal \u03bb to control the number of non-zero coefficients. For random forest, it involved choosing the minimum terminal node size and the number of predictors considered at each split. These selections were made to enhance the models' predictive performance and robustness.",
  "optimization/features": "The number of features used as input is not explicitly stated. However, it is mentioned that the default recommendation for the number of predictors randomly sampled for consideration at each split in the random forest model is the square root of the total number of predictors, which is 17 for the dataset. This suggests that the total number of predictors (features) is likely around 289, as the square root of 289 is 17.\n\nFeature selection was performed using logistic regression with the least absolute shrinkage and selection operator (LASSO). The degree of shrinkage, controlled by the tuning parameter \u03bb, was selected using 10-fold cross-validation within the training dataset. This indicates that feature selection was done using the training set only, ensuring that the model's performance on the test set is not overestimated. The number of non-zero coefficients, which represents the number of selected features, varies depending on the sampling framework and the value of \u03bb. For example, in the \"Visit Visit Observed cluster analysis\" framework, 250 non-zero coefficients were selected, while in the \"Visit Person Observed cluster analysis\" framework, 86 non-zero coefficients were selected.",
  "optimization/fitting": "In our study, we employed two main methods for estimating prediction models: logistic regression with least absolute shrinkage and selection operator (LASSO) and random forest. Both methods were used across four different sampling frameworks.\n\nLogistic regression with LASSO was utilized to select stronger predictors of the outcome while shrinking the coefficients of weaker predictors towards zero. The degree of shrinkage was controlled by the tuning parameter \u03bb, which was selected using 10-fold cross-validation within the training dataset. This approach helped in preventing overfitting by ensuring that the model did not become too complex. The use of cross-validation also aided in selecting the optimal level of shrinkage, thereby balancing the trade-off between bias and variance.\n\nRandom forest, a non-parametric ensemble learning method, was comprised of many decision trees, each estimated on a bootstrap sample of the training dataset. This method inherently reduces overfitting by averaging the predictions from multiple trees, which helps in mitigating the risk of the model becoming too sensitive to the training data. The Gini index was used to measure node impurity as the splitting rule, and the number of trees was set to 200 to limit the number of tuning parameter combinations evaluated. The tuning parameters\u2014minimum terminal node size and the number of predictors considered at each split\u2014were selected using cross-validation to optimize the visit-level area under the receiver operating characteristic curve (AUC).\n\nTo address the potential issue of overfitting, especially with random forest models, we used a person-level training/test split. This approach reduced the correlation between training and test sets, as visits from the same person were not split across both sets. Additionally, we explored using a person-level split for cross-validation after dividing the training and test sets on the visit-level. This method further reduced overfitting by selecting tuning parameters that maximized performance in out-of-cluster predictions.\n\nUnderfitting was addressed by ensuring that the models were complex enough to capture the underlying patterns in the data. For logistic regression with LASSO, the selection of non-zero coefficients was guided by the tuning parameter \u03bb, which was optimized through cross-validation. For random forest, the depth of the trees and the number of predictors considered at each split were tuned to balance model complexity and generalization performance.\n\nIn summary, we employed cross-validation and careful selection of tuning parameters to prevent both overfitting and underfitting. The use of person-level splits and ensemble methods further ensured that our models were robust and generalizable to new data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our prediction models. One of the key methods used was regularization through logistic regression with the least absolute shrinkage and selection operator (LASSO). This technique applies an L1 penalty, which helps in selecting stronger predictors of the outcome while shrinking the coefficients of weaker predictors towards zero. The degree of shrinkage is controlled by the tuning parameter \u03bb, which was selected using 10-fold cross-validation within the training dataset. This process helps in reducing the complexity of the model and prevents it from overfitting the training data.\n\nAdditionally, we utilized random forest, a non-parametric ensemble learning method that consists of multiple decision trees. Each tree is estimated on a bootstrap sample of the training dataset, and random selection of predictors at each split reduces correlation between trees. This approach inherently provides a form of regularization by averaging the predictions from multiple trees, which helps in mitigating overfitting.\n\nFor both logistic regression with LASSO and random forest, we used a visit-level loss function because the risk of suicide varies over time within a person. This approach aligns with our goal of estimating a prediction model that can be used at the time of a mental health visit to prevent suicidal behavior for those at high risk. Tuning parameters were selected via cross-validation to optimize the visit-level area under the receiver operating characteristic curve (AUC), further ensuring that our models generalize well to new data.\n\nMoreover, we explored different sampling frameworks, including person-level and visit-level splits for training and test sets. Our results indicated that using a visit-level split led to overestimating model performance in the development validation set and did not improve performance in the prospective validation set relative to a person-level split. This finding underscores the importance of using person-level splits to avoid overfitting, particularly when using flexible models like random forest.\n\nIn summary, our study employed regularization techniques such as LASSO and random forest, along with careful selection of tuning parameters and sampling frameworks, to prevent overfitting and ensure the reliability of our prediction models.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in detail within the publication. Specifically, the selected tuning parameters for the prediction models, including those for logistic regression with LASSO and random forest, are provided in a table. This table includes details such as the lambda (\u03bb) values for LASSO, the minimum terminal node size, and the number of predictors sampled at each split for the random forest models. These configurations were chosen through cross-validation to optimize the visit-level area under the receiver operating characteristic curve (AUC).\n\nThe optimization schedule and model files are not explicitly detailed in the publication, as the focus is on the methodology and results rather than the specific implementation details. However, the methods section describes the processes used for model estimation and tuning, providing a clear understanding of how the models were developed and optimized.\n\nRegarding the availability and licensing of the reported configurations and parameters, the publication is available in the public domain through PMC (PubMed Central). This means that the information can be accessed freely by anyone, and there are no restrictions on its use for research or educational purposes. The publication adheres to standard academic practices, ensuring that the methods and results are transparent and reproducible.",
  "model/interpretability": "The models we employed in our study include both transparent and black-box approaches. Logistic regression with LASSO (Least Absolute Shrinkage and Selection Operator) is a transparent model. This method uses an L1 penalty to select stronger predictors of the outcome while shrinking the coefficients of weaker predictors towards zero. The degree of shrinkage is determined by a tuning parameter, \u03bb, which is selected using cross-validation. This process results in a model where the most influential predictors are clearly identified, making it interpretable. For instance, if a particular variable, such as a specific symptom or demographic factor, has a non-zero coefficient, it indicates that this variable is a significant predictor of the outcome.\n\nOn the other hand, random forest is a black-box model. It is a non-parametric ensemble learning method that combines multiple decision trees. Each tree is estimated on a bootstrap sample of the training dataset, and random selection of predictors at each split reduces correlation between trees. While random forests are powerful for prediction, they are less interpretable because the final model is an average of many trees, making it difficult to pinpoint the exact contribution of individual predictors. However, techniques such as feature importance scores can provide some insight into which variables are most influential in the model's predictions.\n\nIn summary, our study utilizes both interpretable and black-box models. Logistic regression with LASSO offers transparency by highlighting key predictors, while random forest provides robust predictive performance but with less interpretability.",
  "model/output": "The models developed in this study are classification models. Specifically, they are designed to predict the risk of suicide attempts following outpatient visits using electronic health records. Two primary methods were employed for model estimation: logistic regression with least absolute shrinkage and selection operator (LASSO) and random forest. Logistic regression with LASSO is a type of regression that uses a penalty to shrink coefficients, effectively performing both variable selection and regularization. Random forest, on the other hand, is an ensemble learning method that constructs multiple decision trees and merges them together to get a more accurate and stable prediction.\n\nThe models were evaluated using various performance measures, including the area under the receiver operating characteristic curve (AUC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). These measures help assess the models' ability to discriminate between high-risk and low-risk visits accurately. The development test set was used to estimate model performance, which was then compared to the prospective validation set to evaluate the models' generalizability and to check for overfitting.\n\nThe models were estimated using a visit-level loss function because the risk of suicide can vary over time within an individual. This approach allows for the estimation of a prediction model that can be used at the time of a mental health visit to identify individuals at high risk of suicidal behavior. The tuning parameters for both logistic regression with LASSO and random forest were selected via cross-validation to optimize the visit-level AUC.\n\nIn summary, the models are classification models aimed at predicting the likelihood of suicide attempts based on electronic health records. They were developed and validated using robust statistical methods to ensure their accuracy and reliability in clinical settings.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not applicable.",
  "evaluation/method": "The evaluation of the prediction models involved several key steps and methods to ensure robustness and generalizability. We utilized both a development test set and a prospective validation set to assess model performance. The development test set represented the data available for model development and initial evaluation, while the prospective validation set demonstrated how the models would perform on future visits, providing a more realistic assessment of their practical utility.\n\nWe employed cross-validation to optimize the models' tuning parameters, specifically focusing on the visit-level area under the receiver operating characteristic curve (AUC). This approach ensured that the models were fine-tuned to predict suicide risk accurately at the time of a mental health visit.\n\nThe performance of the models was evaluated using several metrics, including the area under the curve (AUC) for risk discrimination and classification accuracy for the highest risk visits. Additionally, we calculated sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) to provide a comprehensive assessment of the models' predictive capabilities.\n\nThe development test set included a substantial number of visits and unique events, ensuring that the models were trained on a diverse and representative dataset. The prospective validation set, which included a larger number of visits and unique events, provided a rigorous test of the models' ability to generalize to new data.\n\nOverall, the evaluation process was designed to ensure that the models were not only accurate but also practical for real-world application in predicting suicide risk during mental health visits.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we evaluated the performance of our prediction models using several key metrics. We primarily focused on risk discrimination, which was assessed using the area under the curve (AUC) of the receiver operating characteristic (ROC) curve. This metric is widely used in the literature and provides a comprehensive measure of a model's ability to distinguish between high-risk and low-risk visits.\n\nIn addition to AUC, we also evaluated classification accuracy for the highest risk visits. This included calculating sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). These metrics are crucial for understanding the practical utility of the models, especially in a clinical setting where the consequences of false positives and false negatives can be significant.\n\nThe development test set was used to estimate the initial performance of the models, while the prospective validation set provided a real-world assessment of how the models would perform on future data. This approach allowed us to compare the estimated performance with the true performance, helping us to identify any overfitting or optimism in the original model estimation process.\n\nThe use of these metrics is representative of standard practices in the field. AUC is a well-established measure for evaluating the discriminative power of predictive models, and the inclusion of sensitivity, specificity, PPV, and NPV ensures a thorough assessment of the models' classification performance. This comprehensive evaluation provides a robust understanding of the models' strengths and limitations, making our findings reliable and comparable to other studies in the literature.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on evaluating and comparing different sampling frameworks and prediction methods within our specific dataset and context. We used two primary methods for estimating prediction models: logistic regression with least absolute shrinkage and selection operator (LASSO) and random forest. These methods were chosen for their robustness and widespread use in similar predictive modeling tasks.\n\nWe did, however, compare the performance of these methods across different sampling frameworks. This included visit-level and person-level splits for training and testing, as well as different cross-validation strategies. By doing so, we aimed to understand how these variations affect model complexity, discrimination, and calibration. For instance, we found that using a visit-level split resulted in more complex models but also higher optimism, which was mitigated by using a person-level split.\n\nAdditionally, we evaluated the models using a development test set and a prospective validation set. This allowed us to assess not only the immediate performance of the models but also their generalizability to future data. The comparison between these sets helped us identify the degree of overfitting or optimism in our models.\n\nIn summary, while we did not compare our methods to external benchmarks or simpler baselines in the traditional sense, our comprehensive internal comparison across different sampling frameworks and validation strategies provided valuable insights into the strengths and limitations of our approaches. This thorough evaluation ensures that our models are robust and reliable for predicting suicide attempt risk in mental health visits.",
  "evaluation/confidence": "The evaluation of our prediction models included several performance metrics, each accompanied by confidence intervals to provide a measure of uncertainty. For instance, the area under the curve (AUC) for our models was reported with 95% confidence intervals, allowing for an assessment of the precision of these estimates. This is crucial for understanding the reliability of our model's discriminative ability.\n\nBootstrapped 95% confidence intervals were calculated for all measures, ensuring that the variability in the population was reflected. This approach was particularly important given that the cluster size is not fixed in real-life clinical settings, and the number of visits a person makes contributes to variability in the population.\n\nIn terms of statistical significance, we compared the performance of our models in both the development test set and the prospective validation set. The comparison indicated the degree of overfitting or optimism in the original model estimation process. For example, the random forest models using a person-level training/test split showed the smallest optimism and demonstrated better performance in the prospective validation set. This suggests that these models are more robust and generalizable to new data.\n\nAdditionally, we evaluated the classification accuracy using sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). These metrics were calculated at different risk thresholds, and the results were consistent across various models. The specificity was close to 0.995, 0.99, and 0.95 at risk thresholds defined at the 99.5th, 99th, and 95th percentiles of the risk score distribution, respectively. The negative predictive value was near 1 at all thresholds, indicating high reliability in identifying visits without suicide attempts.\n\nFurthermore, we examined the calibration of our models given cluster size. The odds ratios (ORs) indicated whether observed event rates varied by cluster size after conditioning on model predictions. For larger clusters, there was evidence of informative cluster size for most modeling approaches, with ORs increasing as the number of visits per cluster increased. This analysis provided insights into how well our models calibrated predictions across different cluster sizes.\n\nOverall, the performance metrics and statistical analyses conducted provide a comprehensive evaluation of our prediction models. The inclusion of confidence intervals and the comparison of model performance across different datasets ensure that our findings are reliable and statistically significant.",
  "evaluation/availability": "Not enough information is available."
}