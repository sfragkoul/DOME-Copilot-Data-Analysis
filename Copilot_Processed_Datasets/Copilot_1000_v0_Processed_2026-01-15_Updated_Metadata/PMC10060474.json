{
  "publication/title": "Factors Predicting Quality of Life Impairment in Adult Patients with Atopic Dermatitis: Results from a Patient Survey and Machine Learning Analysis.",
  "publication/authors": "Paul C, Griffiths CEM, Costanzo A, Herranz P, Grond S, Mert C, Tietz N, Riedl E, Augustin M",
  "publication/journal": "Dermatology and therapy",
  "publication/year": "2023",
  "publication/pmid": "36862306",
  "publication/pmcid": "PMC10060474",
  "publication/doi": "10.1007/s13555-023-00897-0",
  "publication/tags": "- Atopic Dermatitis\n- Quality of Life\n- Machine Learning\n- Predictive Models\n- Dermatology\n- Patient Survey\n- Disease Burden\n- Activity Impairment\n- Hospitalization\n- Body Surface Area",
  "dataset/provenance": "The dataset used in this study was sourced from an international, cross-sectional survey conducted in ten countries: Australia, Belgium, Canada, France, Germany, Italy, Japan, the Netherlands, Spain, and the United Kingdom. The survey was carried out between July and September 2019. Participants were recruited via internet panels and completed a web-based survey conducted by Hall and Partners alongside Toluna and Axanteus.\n\nA total of 2,314 patients completed the survey. The data collected included patient-reported information on demographics, disease characteristics, and disease burden. This information was gathered using established patient-reported outcomes.\n\nSome of the data from this survey have been previously disclosed in prior publications, which reported baseline characteristics. However, the results of the machine learning approach applied in this study have not been previously reported. The survey aimed to identify factors influencing atopic dermatitis-related quality of life impairment, as measured by the Dermatology Life Quality Index (DLQI). The dataset is not publicly available because consent was not received from participants for the sharing of their data with third parties.",
  "dataset/splits": "The dataset was split into two main subsets: a training set and a testing set. The training set comprised 75% of the sample, while the testing set included 25% of the sample. This split was done randomly to ensure unbiased model validation.\n\nAdditionally, the training set underwent repeated cross-validation with 50 repeats and 5 splits. This cross-validation process was used to fit and validate the model parameters, ensuring robustness and reliability in the predictive performance. The final model fit was then applied to the test data to provide an unbiased model estimate.",
  "dataset/redundancy": "The dataset was split randomly into training and testing subsets, with 75% of the data allocated to the training set and 25% to the test set. This split was designed to ensure that the training and test sets were independent, allowing for unbiased model validation.\n\nTo enforce independence, the data were split randomly, and only patients without any missing data were included in the machine learning analyses. This approach helped to maintain the integrity of the dataset and prevent data leakage, which could otherwise compromise the model's performance.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in dermatology. The use of a large, international sample from ten countries enhances the generalizability of the findings. The dataset includes a diverse range of patient demographics, disease characteristics, and quality of life measures, providing a comprehensive view of the factors influencing atopic dermatitis-related quality of life.\n\nThe random split and the inclusion of only complete data records ensure that the training and test sets are representative of the overall population, reducing the risk of overfitting and improving the robustness of the models. This methodology aligns with best practices in machine learning, ensuring that the results are reliable and applicable to real-world clinical settings.",
  "dataset/availability": "The datasets generated and analyzed during the current study are not publicly available. This decision was made because consent was not received from participants for the sharing of their data with third parties. The study was conducted under a market research code of conduct, ensuring that all consents received were freely given and participants could withdraw from any aspect of the research at any time. This approach prioritizes the privacy and consent of the participants, aligning with ethical guidelines and legal requirements.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are bagged decision trees and gradient boosted trees. These algorithms are not new and are well-established in the field of machine learning. They were chosen for their predictive performance, which was assessed using various accuracy measures such as positive predictive value, negative predictive value, sensitivity, specificity, and overall accuracy. The models were compared using DeLong\u2019s test for receiver operating characteristic curves, with a critical p-value of 0.05 for a two-sided test.\n\nThe decision to use these specific algorithms was driven by their robustness and ability to handle large-scale data, as demonstrated by the consistent accuracy measures across different models. The study applied eight different machine learning models to the data, ultimately selecting three\u2014logistic regression, random forest, and neural network\u2014based on their predictive performance. These models were chosen for their ability to identify factors influencing the quality of life (QoL) impairment in patients with atopic dermatitis (AD), as measured by the Dermatology Life Quality Index (DLQI).\n\nThe focus of the study was on applying these machine-learning techniques to a novel context within dermatology research, rather than developing a new algorithm. The results highlight the importance of considering patient-perspective disease impact when assessing AD severity, alongside traditional clinical measures. The study's findings contribute to the broader understanding of how machine learning can support physicians in managing AD patients, from diagnosis through to personalized therapies.",
  "optimization/meta": "The models employed in this study did not use data from other machine-learning algorithms as input. Instead, they utilized a variety of predictive variables derived directly from the survey data. These variables included demographics, hospitalization history, disease extent, activity impairment, flare characteristics, affected body regions, and current treatments.\n\nThe study applied multiple machine learning methods to predict high levels of quality of life (QoL) impairment, defined as a Dermatology Life Quality Index (DLQI) score of 10 or higher. The methods used included logistic regression, stochastic gradient boosting, neural networks, random forests, decision trees, and sparse linear discriminant analysis. These models were trained and validated using a dataset split into training (75% of the sample) and testing (25% of the sample) subsets. The training set was used to fit the models, while the testing set ensured unbiased prediction.\n\nTo ensure the robustness and independence of the data, the methodology involved repeated cross-validation with 50 repeats and 5 splits. This approach helped to validate the models and ensure that the training data was independent of the testing data. Optimal parameter tuning was performed using grid search algorithms in the caret package of the statistical program R. This process ensured that the models were well-calibrated and performed consistently across different subsets of the data.\n\nThe final model fit was provided by applying the training dataset to the test data, giving an unbiased model estimate. This methodology was applied to all eight machine learning procedures mentioned earlier. The results showed that all models performed well, indicating that the findings are robust and model-independent. The use of multivariate factors in the models increased the likelihood of accurate predictions, providing support to physicians in their routine clinical practice.",
  "optimization/encoding": "Not enough information is available.",
  "optimization/parameters": "In our study, we utilized a comprehensive set of input parameters to predict the quality of life (QoL) impairment in patients with atopic dermatitis (AD). The parameters included demographics, affected body surface area (BSA) and affected body areas, flare characteristics, activity impairment, hospitalisation, and various AD therapies. These variables were selected based on their potential relevance to QoL as reported in the literature and their availability in the survey data.\n\nThe selection of these parameters was guided by the goal of capturing a wide range of factors that could influence QoL in AD patients. This multivariate approach allowed us to assess the contribution of each factor to the predictive performance of our models. The models we applied\u2014logistic regression, random forest, and neural network\u2014were chosen for their ability to handle complex interactions among these variables.\n\nThe specific number of parameters (p) varied depending on the model and the variable importance values computed for each predictor. The variable importance measures were scaled from 0 to 100, providing a relative ranking of each parameter's contribution to the model's predictive accuracy. This scaling helped us identify the most influential factors across different models, ensuring that our findings were robust and model-independent.",
  "optimization/features": "The study utilized a comprehensive set of features to predict the quality of life (QoL) impairment in patients with atopic dermatitis (AD). The input features included a variety of demographic, clinical, and treatment-related variables. These features were carefully selected based on medical knowledge and scientific literature to ensure relevance to the predictive models.\n\nThe features encompassed demographics such as age and gender, hospitalization history within the past year, disease extent measured by body surface area (BSA) involvement, activity impairment assessed using the Work Productivity and Activity Impairment Questionnaire: Atopic Dermatitis (WPAI-AD), flare characteristics, affected body regions, and current treatments. The treatments included topical corticosteroids, topical calcineurin inhibitors, systemic corticosteroids, systemic immunosuppressants, and biologics.\n\nFeature selection was performed to identify the most predictive factors for AD-related QoL burden. This process involved applying machine learning models to the data with dichotomized Dermatology Life Quality Index (DLQI) as the response variable. The data were split into training and testing subsets, with only patients without any missing data included in the analyses. The models were fitted on the training set and validated using repeated cross-validation. Optimal parameter tuning was achieved through grid search algorithms.\n\nThe final models were selected based on their predictive performance, assessed using measures such as positive predictive value (PPV), negative predictive value (NPV), sensitivity, specificity, and overall accuracy. The logistic regression model, random forest, and neural network were chosen for their robustness and model independence. The variable importance of each predictor was computed and visualized, with the activity impairment subscale of the WPAI-AD and hospitalization during the past year being the most significant factors across all selected models.",
  "optimization/fitting": "The study employed a robust methodology to ensure that the models were neither overfitting nor underfitting the data. To address the potential issue of having a large number of parameters relative to the number of training points, several techniques were utilized.\n\nFirstly, the data was split randomly into training (75% sample) and testing (25% sample) subsets. This division helped in validating the models on unseen data, thereby reducing the risk of overfitting. Additionally, repeated cross-validation with 50 repeats and 5 splits was performed. This technique ensures that each data point gets to be in the training set multiple times and in the validation set multiple times, providing a more reliable estimate of model performance.\n\nOptimal parameter tuning was achieved using grid search algorithms within the caret package in R. This method systematically works through multiple combinations of parameter tunes to determine the optimal settings for each model. By doing so, it helps in finding the best parameters that generalize well to unseen data, further mitigating overfitting.\n\nMoreover, the models were validated using repeated cross-validation, which provides a more robust estimate of model performance compared to a single train-test split. This approach helps in ensuring that the models are not underfitting by providing a comprehensive evaluation across different subsets of the data.\n\nIn summary, the use of cross-validation, repeated splits, and grid search for parameter tuning ensured that the models were neither overfitting nor underfitting. These techniques collectively contributed to the robustness and generalizability of the predictive models used in the study.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in this study, including the logistic regression model, random forest, and neural network, offer varying degrees of interpretability. The logistic regression model is inherently transparent, providing clear insights into the relationship between predictors and the outcome. Each coefficient in the model represents the log-odds change in the dependent variable for a one-unit change in the predictor, making it straightforward to interpret the impact of individual features.\n\nThe random forest model, while more complex, offers interpretability through variable importance measures. These measures indicate the contribution of each feature to the predictive performance of the model. By scaling the variable importance values from 0 to 100, it becomes possible to visualize and understand which factors are most influential in predicting quality of life impairment in patients with atopic dermatitis.\n\nThe neural network, on the other hand, is generally considered a black-box model due to its complex architecture involving multiple layers and nodes. However, techniques such as SHAP (SHapley Additive exPlanations) values can be used to interpret the contributions of individual features within the neural network. This allows for a deeper understanding of how different variables influence the model's predictions, even in the context of a more opaque structure.\n\nIn summary, while the logistic regression model provides direct interpretability, the random forest and neural network models offer insights through variable importance and SHAP values, respectively. This multifaceted approach ensures that the predictive models are not only accurate but also interpretable, aiding in the practical application of the findings in clinical settings.",
  "model/output": "The model employed in this study is a classification model. It was used to predict the quality of life (QoL) impairment in patients with atopic dermatitis (AD), specifically dichotomizing the Dermatology Life Quality Index (DLQI) into two categories: DLQI \u2264 10 and DLQI > 10. This categorization helps in identifying factors that are most predictive of a very-large-to-extremely-large impact on QoL.\n\nSeveral machine learning algorithms were applied, including logistic regression, stochastic gradient boosting, neural network, random forest, decision tree, and sparse linear discriminant analysis. Among these, the logistic regression model, random forest, and neural network were selected based on their predictive performance and robustness. These models were chosen because they showed high accuracy, positive predictive value (PPV), and negative predictive value (NPV), indicating their effectiveness in classifying the DLQI categories.\n\nThe models were trained and validated using a dataset split into training (75% of the sample) and testing (25% of the sample) subsets. The training set was used to fit the models, while the testing set ensured unbiased prediction. Optimal parameter tuning was performed using grid search algorithms to enhance the models' performance. The final model fit was provided by applying the training dataset on the test data, ensuring an unbiased model estimate.\n\nThe output of these models included variable importance values, which were scaled from 0 to 100 to assess the contribution of each feature to the predictive performance. This allowed for the identification of key factors, such as activity impairment and hospitalization, which were found to be highly predictive of QoL impairment in AD patients. The models' consistency across different algorithms indicated their robustness and independence from the data on a large scale.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the machine learning models involved assessing their predictive performance using several key metrics. These metrics included positive predictive value (PPV), negative predictive value (NPV), sensitivity, specificity, and overall accuracy. To compare the models' performance, DeLong\u2019s test was used to evaluate their receiver operating characteristic (ROC) curves. A critical p-value of 0.05 was set for a two-sided test.\n\nThe models were selected based on their predictive performance, ensuring robustness and independence from the data on a large scale. The logistic regression model, random forest, and neural network were chosen due to their consistent accuracy measures across different evaluations. This consistency indicated that the models were reliable and not overly dependent on specific data characteristics.\n\nTo assess the contribution of each feature to the predictive performance, variable importance values were computed for each predictor within the corresponding model. These values were then visualized and scaled from 0 to 100, providing a clear understanding of which factors most significantly influenced the models' predictions. This approach allowed for a comprehensive evaluation of the models' effectiveness in predicting quality of life impairment in patients with atopic dermatitis.",
  "evaluation/measure": "To evaluate the performance of our models, we focused on several key metrics that are widely recognized in the literature for assessing predictive models, particularly in the context of medical and health-related outcomes. The primary metrics we reported include prediction accuracy, positive predictive value (PPV), negative predictive value (NPV), sensitivity, specificity, and overall accuracy.\n\nPrediction accuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. This metric provides a general indication of how well the model performs across all predictions.\n\nPositive predictive value (PPV) indicates the proportion of positive identifications that are actually correct. It is crucial for understanding the reliability of positive predictions made by the model.\n\nNegative predictive value (NPV) measures the proportion of negative identifications that are actually correct. This metric is essential for assessing the model's ability to correctly identify cases that do not meet the criteria for a positive outcome.\n\nSensitivity, also known as the true positive rate, measures the proportion of actual positives that are correctly identified by the model. High sensitivity is important for ensuring that the model can detect most of the positive cases.\n\nSpecificity, or the true negative rate, measures the proportion of actual negatives that are correctly identified by the model. High specificity is crucial for minimizing false positives.\n\nOverall accuracy provides a single metric that combines both sensitivity and specificity, giving an overall measure of the model's performance.\n\nIn addition to these metrics, we used DeLong\u2019s test to compare the receiver operating characteristic (ROC) curves of the models. The ROC curve is a graphical representation of the diagnostic ability of a binary classifier system, and DeLong\u2019s test helps to determine if there is a statistically significant difference between the areas under the ROC curves of different models.\n\nThe critical p-value for our two-sided tests was set at 0.05, which is a standard threshold in many scientific studies to determine statistical significance.\n\nBy reporting these metrics, we aimed to provide a comprehensive evaluation of our models' performance, ensuring that our findings are robust and comparable to other studies in the field. These metrics are representative of those commonly used in the literature, allowing for a clear and standardized assessment of model performance.",
  "evaluation/comparison": "In our study, we employed several machine learning models to predict the quality of life (QoL) impairment in patients with atopic dermatitis (AD). The models selected for detailed analysis were the logistic regression model, the random forest, and the neural network. These models were chosen based on their cross-model predictive performance and the results of DeLong\u2019s test, which compared the receiver operating characteristic (ROC) curves of the models.\n\nThe predictive performance of the models was assessed using various accuracy measures, including positive predictive value (PPV), negative predictive value (NPV), sensitivity, specificity, and overall accuracy. These measures were consistent across the models, indicating robustness and model independence of the data on a large scale.\n\nIn addition to these primary models, we also evaluated other algorithms such as the bagged decision tree and gradient boosted trees. The performance of these models was similarly assessed using the same accuracy measures, providing a comprehensive comparison of different machine learning approaches.\n\nThe variable importance of each predictor was computed for the corresponding model and visualized. This allowed us to identify the most significant factors contributing to the predictive performance of each model. The variable importance measures were scaled from 0 to 100, providing a clear ranking of the predictors' influence.\n\nWhile we did not perform a direct comparison to publicly available benchmark datasets, the consistency and high accuracy of our models across different algorithms suggest that our approach is robust and generalizable. The use of multiple models and the assessment of various accuracy measures ensure that our findings are reliable and not dependent on a single method.\n\nIn summary, our study involved a thorough comparison of different machine learning models to predict QoL impairment in AD patients. The consistent performance across models and the detailed assessment of predictive accuracy measures highlight the strength and reliability of our approach.",
  "evaluation/confidence": "The evaluation of our models focused on several key performance metrics, including prediction accuracy measures such as positive predictive value (PPV), negative predictive value (NPV), sensitivity, specificity, and overall accuracy. These metrics were crucial in assessing the predictive performance of our models.\n\nTo ensure the robustness of our findings, we employed DeLong\u2019s test to compare the receiver operating characteristic (ROC) curves of the models. This statistical test helped us determine whether the differences in the ROC curves were significant. We set a critical p-value of 0.05 for a two-sided test, which is a standard threshold for statistical significance in many scientific studies.\n\nThe results indicated that all applied machine learning models showed high accuracy, PPV, and NPV, with ranges of 74\u201378%, 70\u201375%, and 77\u201381%, respectively. The consistency of these accuracy measures across different models suggested that the results were robust and model-independent, reinforcing the reliability of our predictive analysis.\n\nAdditionally, the variable importance values for each predictor were computed and visualized, providing insights into the contribution of each feature to the predictive performance. These values were scaled from 0 to 100, allowing for a clear comparison of the importance of different predictors across the models.\n\nOverall, the evaluation process included rigorous statistical testing and performance metric analysis, ensuring that the conclusions drawn from the study are reliable and statistically significant.",
  "evaluation/availability": "The datasets generated and analyzed during the current study are not publicly available. Consent was not received from participants for the sharing of their data with third parties."
}