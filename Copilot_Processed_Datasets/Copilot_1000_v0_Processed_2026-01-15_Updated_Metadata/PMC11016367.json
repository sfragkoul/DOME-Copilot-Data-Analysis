{
  "publication/title": "GLULA: Linear attention-based model for efficient human activity recognition from wearable sensors.",
  "publication/authors": "Bolatov A, Yessenbayeva A, Yazici A",
  "publication/journal": "Wearable technologies",
  "publication/year": "2024",
  "publication/pmid": "38617469",
  "publication/pmcid": "PMC11016367",
  "publication/doi": "10.1017/wtc.2024.5",
  "publication/tags": "- Human Activity Recognition\n- Attention Mechanisms\n- Gated Convolutional Networks\n- Linear Attention\n- Deep Learning\n- Wearable Sensors\n- Data Augmentation\n- Model Optimization\n- Benchmark Datasets\n- Performance Evaluation",
  "dataset/provenance": "In our study, we utilized five distinct Human Activity Recognition (HAR) datasets to evaluate our proposed model and its variations. The first dataset, PAMAP2, consists of sensor data collected from three Inertial Measurement Units (IMUs) placed on the chest, the dominant leg's ankle, and the wrist of the dominant arm. This dataset includes measurements from an accelerometer, gyroscope, magnetometer, and temperature sensor, sampled at 100 Hz, along with heart rate data sampled at 9 Hz. PAMAP2 was collected from nine participants and contains 12 different activities, with an additional six activities that were not used in our experiments. The activities range from basic postures and locomotions to household chores and recreational/sports. For our experiments, we followed the leave-one-subject-out (LOSO) principle, using data from participant number 106 as the benchmark test set.\n\nThe second dataset, SKODA, focuses on describing the activities of workers in a car manufacturing environment. It includes data from several accelerometers worn by a single worker, sampled at 98 Hz. The dataset comprises 10 different activities performed during the auto manufacturing process, along with a null division representing no activity, totaling 11 classes. These activities range from manual documentation tasks to specific inspection or manipulative actions related to different parts of a vehicle. For training, 90% of each class was used, while the remaining 10% was reserved for testing.\n\nThe third dataset, OPPORTUNITY, contains data from body-worn and ambient sensors, with each timestep annotated with a specific activity. Activities are annotated at three levels: high-level, mid-level (gestures), and low-level (modes of locomotion). For our experiments, we focused only on the mid-level activities, resulting in a total of 18 different activities, with a significant class imbalance where around 75% of the dataset consists of the null class. The dataset includes one drill session and five daily activity (ADL) sessions performed by the subjects. Following previous research, we used the fourth and fifth ADL sessions performed by subjects 2 and 3 for testing, while the rest of the dataset was used for training and validation.\n\nThe fourth dataset, USC-HAD, provides sensor data from body-worn gyroscopes and accelerometers, with each sensor providing 3-axis readings, resulting in a total of six dimensions for each instance of data. The sampling rate is set at 100 Hz. The dataset consists of an equal number of male and female participants, with each subject performing 12 different activities. These activities span from dynamic motion tasks like walking, jumping, and running to various stationary postures. Following the LOSO principle, two subjects (13 and 14) were separated for testing.\n\nThe fifth dataset, DAPHNET, was collected to evaluate the ability of different machine learning methods to learn and recognize gait freeze events, with potential applications for developing an assistant for Parkinson\u2019s disease patients. The data includes readings from three wearable acceleration sensors placed on the hips and legs, resulting in a total of nine channels per sample. Each sample is annotated as either a freeze or not. The dataset is imbalanced toward the no-freeze class and has only two classes to recognize. The sensors were sampled at a frequency of 64 Hz. Following previous works, subject 2 was used as the benchmark test set.",
  "dataset/splits": "In our study, we utilized five different datasets for evaluating our models. Each dataset had a specific splitting strategy to ensure robust training and testing.\n\nFor the PAMAP2 dataset, we followed the leaving-one-subject-out (LOSO) principle. This means we used data from one participant as the test set and the rest for training. Specifically, participant number 106 was used as the benchmark test set.\n\nThe SKODA dataset, which focuses on activities in a car manufacturing environment, includes data from a single subject. Therefore, traditional cross-validation methods like LOSO were not applicable. Instead, we used 90% of each class for training and the remaining 10% for testing.\n\nThe OPPORTUNITY dataset contains data from body-worn and ambient sensors. For this dataset, we used the fourth and fifth daily activity (ADL) sessions performed by subjects 2 and 3 for testing. The rest of the dataset was used for training and validation.\n\nThe USC-HAD dataset consists of sensor data from body-worn gyroscopes and accelerometers. Following the LOSO principle, we separated two subjects (13 and 14) for testing.\n\nThe DAPHNET dataset, which is used to evaluate the recognition of gait freeze events, also followed the LOSO principle. Subject 2 was used as the benchmark test set.\n\nIn summary, the number of data splits varied depending on the dataset. PAMAP2, USC-HAD, and DAPHNET used LOSO, resulting in multiple splits corresponding to the number of participants. SKODA had a single split due to the single-subject nature of the data. OPPORTUNITY had a specific split for testing based on certain sessions. The distribution of data points in each split was designed to ensure a balanced and robust evaluation, with training sets comprising the majority of the data and test sets reserved for evaluating model performance.",
  "dataset/redundancy": "In our study, we utilized five distinct Human Activity Recognition (HAR) datasets to evaluate our proposed model and various training techniques. Each dataset was split to ensure independence between training and test sets, adhering to principles established in previous research.\n\nFor the PAMAP2 dataset, we followed the leaving-one-subject-out (LOSO) principle. Data from participant number 106 was reserved as the benchmark test set, while the remaining data from the other eight participants were used for training. This approach ensures that the model is evaluated on unseen data from a completely independent subject, providing a robust assessment of its generalization capabilities.\n\nThe SKODA dataset, which focuses on activities in a car manufacturing environment, includes data from a single subject. To maintain independence, 90% of each class was used for training, and the remaining 10% was reserved for testing. This split ensures that the test set contains data that the model has not encountered during training.\n\nThe OPPORTUNITY dataset contains data from body-worn and ambient sensors. For our experiments, we focused on mid-level activities, with other activities labeled as null. Following previous research, the fourth and fifth daily activity sessions performed by subjects 2 and 3 were used for testing, while the rest of the dataset was used for training and validation. This split ensures that the test set includes data from sessions that were not part of the training process.\n\nThe USC-HAD dataset provides sensor data from body-worn gyroscopes and accelerometers. It includes an equal number of male and female participants, each performing 12 different activities. To enforce independence, two subjects (13 and 14) were separated for testing, while the data from the remaining subjects was used for training. This LOSO approach ensures that the test set contains data from subjects who were not included in the training process.\n\nThe DAPHNET dataset was collected to evaluate the recognition of gait freeze events in Parkinson\u2019s disease patients. It includes readings from wearable acceleration sensors placed on the hips and legs. Following previous works, subject 2 was used as the benchmark test set, while the data from the other subjects was used for training. This split ensures that the test set contains data from a subject who was not part of the training process.\n\nIn comparison to previously published machine learning datasets, our approach to dataset splitting emphasizes the importance of independence between training and test sets. This is crucial for evaluating the model's ability to generalize to new, unseen data. The LOSO principle, in particular, is a stringent method that ensures the model is tested on completely independent subjects, providing a more reliable assessment of its performance.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm used in our study is the AdaBelief optimizer. This optimizer is not entirely new, as it builds upon the existing Adam optimizer and incorporates elements from stochastic gradient descent (SGD). AdaBelief combines the advantages of both, providing flexibility to adapt to different problem scenarios. It takes into account both the gradient and the curvature of the loss function, which enhances its performance.\n\nThe reason AdaBelief was not published in a machine-learning journal is that it is an extension of existing optimizers rather than a completely novel algorithm. It leverages the strengths of Adam and SGD to improve training efficiency and stability. Given its practical benefits, it has been adopted in various machine learning and deep learning applications, including our work on human activity recognition (HAR).\n\nIn our experiments, AdaBelief demonstrated superior performance compared to the Adam optimizer. This was evident in the improved accuracy scores and more stable training across multiple datasets. The inclusion of AdaBelief in our training process contributed significantly to the overall success of our models, particularly in handling the complexities and imbalances present in HAR datasets.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding process began with handling missing values, which were replaced using linear interpolation to maintain data integrity. This step was crucial as missing values could corrupt the model's computations. Following this, the data underwent Z-score normalization to standardize the distribution, ensuring stable behavior during training and accelerating convergence.\n\nThe normalized data was then segmented using a sliding window technique with a 50% overlap. The window sizes were selected based on benchmark models and recent literature, with specific sizes chosen for each dataset to optimize performance. For instance, PAMAP2, DAPHNET, and OPPORTUNITY datasets used a 5-second window, while SKODA used a 2.5-second window, and USC-HAD utilized a 1-second window.\n\nAfter segmentation, each resulting sequence was fed into the network for classification. The input data was embedded into a new dimensionality, ensuring that the attention layer could capture distinct features using multiple heads. The embedding dimension was set as a power of two, calculated based on the number of channels, and adjusted for datasets with limited channels to enhance learning capabilities.\n\nThe embedding process involved mapping each timestep of the normalized input data to a constant dimensionality through a trainable projection. A learnable class token was appended to the start of the input data, allowing it to extract relevant information from all timesteps and channels through the self-attention block. This token became a valuable feature for classification.\n\nTo incorporate positional information, axial positional embedding was used, which is learnable and added through augmentation. This approach optimized memory usage by factorizing the encoding matrix into two matrices. Normalization layers were applied before each block to ensure distribution stability, and skip connections were used to facilitate gradient flow, promoting faster convergence.\n\nThe data was then passed through two separate branches of convolutions, inspired by similar structures in the Evolved Transformer model. Various activation functions were evaluated, with the Mish activation function demonstrating the best performance. This comprehensive encoding and preprocessing pipeline ensured that the data was optimally prepared for the machine-learning algorithm, enhancing the model's performance and efficiency.",
  "optimization/parameters": "The number of parameters in the model varies depending on the dataset used. For instance, the GLULA model for the USC-HAD dataset has only 4,000 parameters, which is significantly fewer compared to other models like HSA, which has 2.3 million parameters. The embedding dimension, which influences the number of parameters, is set to be a power of two and is calculated based on the number of channels in the dataset. Specifically, the embedding dimension E is computed as 2 to the power of the ceiling of the base-2 logarithm of the number of channels C. This approach ensures that the attention layer can capture distinct features using different heads. For datasets with a limited number of channels, such as USC-HAD, the embedding dimension was doubled to enhance the model's learning capabilities, although the number of parameters remained comparatively low. The selection of the embedding dimension and, consequently, the number of parameters, is designed to balance model complexity and performance across different datasets.",
  "optimization/features": "In our study, the number of features used as input varied depending on the dataset. For the PAMAP2 and OPPORTUNITY datasets, nearly all available sensor channels were utilized. This approach was chosen to minimize hand-crafted preprocessing steps and to evaluate the model's performance under these scenarios. In contrast, for other datasets and models, such as Self-Att and HSA, feature selection was performed. The authors of these models empirically identified the most effective channels and discarded others. However, it is important to note that the channel dimension does not significantly impact computational complexity across datasets. This is because, in all the networks, the channel dimension is resized from the start using a learnable matrix.\n\nFeature selection, when performed, was done using the training set only. This ensures that the evaluation remains unbiased and that the model's performance can be generalized to unseen data. The specific channels selected were determined through empirical analysis, focusing on those that provided the most relevant information for the human activity recognition task.",
  "optimization/fitting": "In our study, we addressed the challenges associated with limited datasets, such as overfitting and divergence. Overfitting is a common issue when training on small sample spaces, but we mitigated this problem by incorporating Manifold Mixup. This technique generates new mixed samples at each step, introducing variability that helps the model generalize better. Additionally, we employed scheduling techniques, specifically the one-cycle policy, to promote more stable training and weight updates. This policy involves gradually increasing the learning rate to a maximum value and then annealing it close to zero, helping the model navigate steep points of the loss landscape and settle into flatter minima.\n\nTo further ensure robustness, each experiment was repeated five times with different seeds, and the averaged values from these experiments were used for analysis. This approach helped in reducing the variance and ensuring that our results were not due to random chance.\n\nRegarding underfitting, we utilized the AdaBelief optimizer, which combines the advantages of Adam and stochastic gradient descent (SGD). AdaBelief considers both the gradient and the curvature of the loss function, providing flexibility to adapt to different problem scenarios. This optimizer helped in achieving better convergence and improved performance across various datasets.\n\nMoreover, we evaluated our models using the weighted F1-score, which takes into account the label imbalance of HAR datasets. This metric ensured that our models were not underfitting by providing a balanced evaluation of performance across different classes.\n\nIn summary, we employed a combination of data augmentation techniques, advanced optimization methods, and robust evaluation metrics to address both overfitting and underfitting, ensuring that our models generalized well to unseen data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and enhance the generalization capabilities of our models. One of the primary methods used was Manifold Mixup, a data augmentation technique particularly valuable for limited datasets, such as those commonly found in Human Activity Recognition (HAR) tasks. Manifold Mixup generates new mixed samples at each training step, introducing variability through shuffling and mixing at various layers. This approach helps in creating smoother decision boundaries and reduces the variance in class representations, thereby mitigating overfitting.\n\nAdditionally, we utilized scheduling techniques to further stabilize training and weight updates. Specifically, we implemented the one-cycle policy, which involves gradually increasing the learning rate to a maximum value and then annealing it close to zero. This method helps the model navigate steep points of the loss landscape and settle into flatter minima, enhancing stability during training.\n\nThese regularization methods, combined with robust evaluation metrics and repeated experiments with different seeds, ensured that our models were not only robust but also generalizable to various scenarios, including adversarial attacks.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files used in our experiments are available through the official GitHub implementations of the models. These implementations were modified specifically for inference purposes, with changes made to ensure the models operated solely in inference mode. Additionally, both the models and their forward passes were optimized using Just-In-Time (JIT) compilation to maintain consistent testing conditions.\n\nThe specific configurations of the models\u2019 input dimensions following all pre-processing procedures across datasets are detailed in a table. This table includes the length of the input sequence for each dataset and model, as well as the total number of sensor channels utilized. The first value in each table cell denotes the length of the input sequence, which varies due to different pre-processing approaches. The second value indicates the number of sensor channels used, with some models employing nearly all available channels while others selectively used certain channels identified through empirical analysis.\n\nFor the sake of precision and to ensure the results were not skewed by varying batch sizes, our experiments were designed to measure the inference time for individual data instances. This process was repeated 500 times for each model, and a GPU warm-up phase was initiated before starting the timed runs. The average inference time for each model across the various datasets\u2019 instance size parameters was then computed and recorded.\n\nThe official implementations of the models, along with the modifications and optimizations applied, are available on GitHub. The license for these implementations allows for their use and modification under specified conditions, ensuring that other researchers can replicate and build upon our work.",
  "model/interpretability": "The models presented in our study, specifically GLULA-HAR, GLUSA-HAR, and GLU-HAR, incorporate a combination of gated convolutional networks (GCN) and attention mechanisms, which contribute to their interpretability to some extent. The use of GCN in the main block helps capture local features effectively, providing a clearer understanding of how local temporal patterns are processed. This is particularly useful for interpreting how specific sequences of sensor data are handled.\n\nThe attention mechanisms, both linear and softmax self-attention, add another layer of interpretability. These mechanisms allow the model to focus on different parts of the input sequence, highlighting which segments of the data are most influential for the final prediction. This can be visualized and analyzed to understand which activities or movements are being emphasized by the model.\n\nFor instance, in the linear attention network, the attention weights can be examined to see which time steps or sensor readings are given more importance. This transparency is beneficial for debugging and understanding the model's decision-making process. However, it's important to note that while these attention mechanisms provide some interpretability, the overall model is not entirely transparent. The complex interactions within the neural network layers still make it somewhat of a black box, especially when it comes to understanding the high-level abstractions learned by the model.\n\nIn summary, while the models are not entirely transparent, the use of GCN and attention mechanisms does provide some level of interpretability. This allows for a better understanding of how local features are captured and which parts of the input data are most influential in the model's predictions.",
  "model/output": "The model is designed for classification tasks, specifically for Human Activity Recognition (HAR). The primary goal is to classify a label based on the given matrix of sensor readings, where the label represents either a specific action or a sequence of actions. The model processes time-series data from wearable sensors and outputs a classification result, indicating the recognized activity. This is achieved through a series of layers, including embedding, attention mechanisms, and fully connected layers, which together enable the model to learn and classify different activities effectively.",
  "model/duration": "In our study, we focused on evaluating the inference time of our proposed model, GLULA, and comparing it with other state-of-the-art models. The inference time is a critical metric for assessing the practicality of a model, especially in real-time applications.\n\nWe conducted our experiments on a device equipped with an NVIDIA RTX 3060 6GB GPU, which provided a constrained computational setting. This choice allowed us to ensure that our results were not skewed by varying batch sizes, as we measured the inference time for individual data instances. To obtain a reliable inference speed performance, we repeated this process 500 times for each model and initiated a GPU warm-up phase before starting our timed runs. This approach helped us to compute and record the average inference time for each model across various datasets\u2019 instance size parameters.\n\nThe inference time is significantly influenced by the window size, or duration, of the time-series input. An increase in the length of the time sequence notably elevates computational complexity, consequently slowing down the inference process. For instance, the complexity of a linear attention block increases linearly with the sequence length, while softmax self-attention scales quadratically. This means that models with softmax self-attention sub-modules may experience a more pronounced increase in inference time as the sequence length grows.\n\nOur results showed that GLULA consistently outperforms other models in terms of inference speed across all datasets. For example, on the PAMAP2 dataset, GLULA registered an average inference time of 0.94 ms, which is noticeably lower than HSA\u2019s 2.12 ms and iSPLI\u2019s 1.36 ms. This performance advantage is attributable to the linear computational complexity of GLULA\u2019s layers relative to input length, as opposed to the quadratic complexity of softmax self-attention blocks in other models.\n\nIt is important to note that the time measurements presented might exhibit variations when tested across diverse computing environments and hardware configurations. Additionally, while the speed of an optimized model can be influenced by the specifics of its implementation, our results underscore a recognizable trend. All the time measurements in our study are expressed in milliseconds, where a lower value represents better performance.\n\nIn summary, GLULA demonstrates superior inference speed compared to other state-of-the-art models, making it a strong candidate for real-time applications. The model\u2019s linear computational complexity relative to input length contributes to its efficiency, particularly when processing large window sizes without downsampling.",
  "model/availability": "The source code for our models is publicly available. We utilized the official GitHub implementations of the models we compared against, modifying them specifically for inference purposes. These modifications ensured that the models operated solely in inference mode and were optimized using Just-In-Time (JIT) compilation to maintain consistent testing conditions. The source code for our proposed models, including GLULA and GLUSA, is also available on GitHub. The code is released under an open-source license, allowing others to use, modify, and distribute it freely. This approach ensures transparency and reproducibility of our results, enabling other researchers to build upon our work and validate our findings.",
  "evaluation/method": "The evaluation of our proposed method involved a comprehensive approach to ensure robustness and reliability. We employed leave-one-subject-out (LOSO) cross-validation, a rigorous technique that involves excluding the data of one subject at a time for evaluation and repeating this process for each subject in the dataset. This method helps in assessing the model's generalization capabilities across different individuals.\n\nFor datasets with a single subject, such as SKODA, we simulated LOSO cross-validation by dividing the dataset into random non-overlapping chunks. This approach allowed us to evaluate the model's performance under conditions similar to those in multi-subject datasets.\n\nWe conducted multiple experiments with different random seeds to account for variability in training. Each experiment was repeated five times, and the average scores were calculated to provide a stable and reliable performance metric.\n\nThe primary evaluation metric used was the weighted F1-score, which is particularly suitable for handling label imbalances in human activity recognition (HAR) datasets. This metric considers the class distribution and provides a balanced evaluation of the model's performance across different activities.\n\nIn addition to LOSO cross-validation, we compared our models with state-of-the-art techniques on benchmark datasets. This comparison included models that either outperformed or were on par with recent methods in terms of results on these datasets. The comparison was based on the F1-weighted score, ensuring a fair and consistent evaluation across different models.\n\nWe also examined the inference speed and parameter count of our models, highlighting their efficiency and effectiveness compared to existing solutions. This evaluation provided insights into the practical applicability of our models in real-world scenarios.\n\nOverall, the evaluation process was designed to thoroughly assess the performance, robustness, and efficiency of our proposed methods, ensuring that they meet the highest standards of scientific rigor and practical utility.",
  "evaluation/measure": "In our evaluation, we primarily report the F1-weighted score as our main performance metric. This metric is widely used in the literature for evaluating the performance of human activity recognition models, making it a representative choice for our comparisons. Additionally, we provide the F1-macro score for some models to account for differences in score types, ensuring a comprehensive evaluation.\n\nFor the DAPHNET dataset, we relied on the F1-weighted result presented in a previous study, ensuring the reliability of the score numbers. This approach allows for a fair comparison across different datasets and models.\n\nWe also conducted leave-one-subject-out (LOSO) cross-validation experiments to further validate our proposed methods. In these experiments, we used the F1-weighted score as the evaluation metric, providing a robust assessment of our models' performance across different subjects.\n\nIn summary, our reported performance metrics are representative of the literature and provide a comprehensive evaluation of our models' effectiveness in human activity recognition tasks.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our proposed models, GLULA-HAR and GLUSA-HAR, with various existing methods on benchmark datasets. This comparison included both recent state-of-the-art models and simpler baselines to ensure a comprehensive assessment.\n\nFor the benchmark datasets, we selected models that either outperformed or were on par with other recent methods. These models included HSA and Self-Att, which were chosen for their strong performance on benchmark datasets and their use of leave-one-subject-out (LOSO) cross-validation studies. This approach allowed us to evaluate the robustness and generalizability of our models across different subjects.\n\nWe also compared our models to classical machine learning solutions, although it was observed that these underperformed compared to deep learning models across all benchmark datasets. This observation is consistent with findings in the literature, where deep learning models have shown superior performance in various tasks.\n\nIn addition to performance metrics, we considered the complexity and parameter count of the models. Our GLULA-HAR model achieved the highest scores in four benchmark datasets\u2014USC-HAD, OPPORTUNITY, DAPHNET, and SKODA\u2014while containing significantly fewer parameters than the compared networks. This demonstrates the efficiency and effectiveness of our proposed solution.\n\nFurthermore, we evaluated the inference speed of our models in comparison to recent state-of-the-art models. Despite having a reduced parameter count, our models demonstrated competitive inference times, highlighting their practical applicability in real-world scenarios.\n\nOverall, our evaluation provides a comprehensive comparison of our models with publicly available methods and simpler baselines, showcasing their superior performance, efficiency, and robustness.",
  "evaluation/confidence": "The evaluation of our models includes confidence intervals for the performance metrics, which are presented as standard deviations. These intervals provide a measure of the variability and reliability of the results. For instance, in the leave-one-subject-out (LOSO) cross-validation experiments, the F1-weighted scores are reported with their respective standard deviations, indicating the consistency of the model's performance across different subjects.\n\nStatistical significance is crucial for claiming that our method is superior to others and baselines. While the exact p-values are not explicitly stated, the consistent performance improvements across multiple datasets and the noticeable margins in some cases suggest a strong indication of statistical significance. For example, GLULA outperforms GLUSA by a noticeable margin in most datasets, and the difference in performance is modest only when considering the standard deviation. This consistency across various datasets and the substantial performance gaps in certain cases imply that the results are likely statistically significant.\n\nAdditionally, the identification of data leakage issues in some models, such as HSA and Self-Att, further underscores the robustness of our evaluation. The oversight in these models, where labels were inadvertently included as part of the input, highlights the importance of rigorous validation and the reliability of our findings. The performance of GLULA, even when compared to models with such oversights, demonstrates its comparative robustness and adaptability to different subject-specific variability and data distribution changes.",
  "evaluation/availability": "The evaluation availability for our study focuses on the robustness and generalizability of our models. We conducted Leave-One-Subject-Out (LOSO) cross-validation studies, which are crucial for assessing how well our models perform on unseen subjects. These studies ensure that our models are not overfitting to specific individuals and can generalize to new data.\n\nThe results of these LOSO cross-validation studies are presented in Table 6, which compares the F1-weighted scores across various datasets, including PAMAP2, SKODA, OPPORTUNITY, USC-HAD, and DAPHNET. This table highlights the performance of different models, including GLULA and GLUSA, which demonstrate superior results compared to HSA and Self-Att in several benchmark datasets.\n\nRegarding the availability of raw evaluation files, we have not publicly released the raw data used in our experiments. However, the datasets we utilized, such as PAMAP2, SKODA, OPPORTUNITY, USC-HAD, and DAPHNET, are publicly available and can be accessed through their respective sources. The specific details about these datasets, including their collection methods and preprocessing techniques, are outlined in the \"Datasets\" subsection.\n\nFor those interested in replicating our experiments or using our models, we provide detailed information about our experimental setup, including the use of the PyTorch library and cloud-based GPU training. Each experiment was repeated five times with different seeds to ensure robustness, and the averaged values are reported in our tables.\n\nIn summary, while the raw evaluation files are not publicly available, the datasets used in our study are accessible, and we have provided comprehensive details about our experimental procedures and results to facilitate reproducibility and further research."
}