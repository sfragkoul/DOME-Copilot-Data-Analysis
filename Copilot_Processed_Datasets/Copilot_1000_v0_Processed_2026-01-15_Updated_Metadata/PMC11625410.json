{
  "publication/title": "Molecular de-extinction of ancient antimicrobial peptides enabled by machine learning.",
  "publication/authors": "Maasch JRMA, Torres MDT, Melo MCR, de la Fuente-Nunez C",
  "publication/journal": "Cell host & microbe",
  "publication/year": "2023",
  "publication/pmid": "37516110",
  "publication/pmcid": "PMC11625410",
  "publication/doi": "10.1016/j.chom.2023.07.001",
  "publication/tags": "- Molecular de-extinction\n- Antimicrobial peptides\n- Machine learning\n- Drug discovery\n- Paleoproteomes\n- Protease cleavage sites\n- Bioinformatics\n- Evolutionary medicine\n- Innate immunity\n- Proteomics",
  "dataset/provenance": "The dataset used for training and testing the panCleave model is sourced from the MEROPS Peptidase Database, specifically all human protease substrates available as of June 2020. This dataset encompasses 369 proteases, representing six catalytic types: Cysteine, Metallo, Serine, Aspartic, Threonine, and Mixed. These proteases are categorized into 31 clans and 73 families.\n\nThe dataset consists of a balanced set of 49,634 observations, all of which are 8 residues in length. This length is chosen because MEROPS reports cleavage sites as 8-residue P4:P4\u2019 flanking sites. The positive observations (24,817 unique) were curated from MEROPS, while the negative observations (24,817 unique) were generated from the human proteome and random protein space. The negative dataset includes sequences generated by three methods: randomly selected 8-residue contiguous subsequences of the human proteome, randomly generated sequences adhering to the amino acid frequencies of the human proteome, and randomly generated sequences with no amino acid frequency constraints. No sequences are present in both the positive and negative datasets.\n\nThe training and 10-fold cross-validation were performed using 80% of the total observations (39,707), while the remaining 20% (9,927) were reserved as an independent test set. The train-test split was stratified by label to ensure that each split maintained a label distribution representative of the entire dataset, with 50% positive observations and 50% negative observations.\n\nThe complete training dataset, testing dataset, and Python code are freely available on GitLab. This availability is crucial for reproducibility and further research in the community. The dataset has not been used in previous papers by the community, as it was specifically curated for this study.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a test set. The training set comprised 80% of the total observations, amounting to 39,707 data points. This set was further used for 10-fold cross-validation, ensuring robust model training. The test set, which was reserved for independent evaluation, consisted of the remaining 20% of the observations, totaling 9,927 data points. Both splits were stratified by label to maintain a balanced distribution of positive and negative observations, with each split containing 50% positive and 50% negative observations. This stratification ensured that the model's performance could be accurately assessed across different data subsets.",
  "dataset/redundancy": "The datasets used for training and testing the panCleave model were carefully split to ensure independence and prevent data leakage. The complete dataset consisted of 49,634 observations, balanced equally between positive and negative samples. This dataset was divided into a training set and a test set, with 80% of the data (39,707 observations) used for training and 10-fold cross-validation, and the remaining 20% (9,927 observations) reserved as an independent test set. The train-test split was stratified by label to maintain a representative distribution of positive and negative observations in each subset, ensuring that both the training and test sets contained 50% positive and 50% negative observations.\n\nTo enforce the independence of the training and test sets, no sequences were present in both datasets. This was achieved by curating cleavage site data from the MEROPS Peptidase Database for positive observations and generating negative observations through three distinct methods: randomly selected subsequences from the human proteome, sequences adhering to human proteome amino acid frequencies, and randomly generated sequences without frequency constraints. Redundant sequences and those containing non-canonical amino acids were removed to further ensure dataset integrity.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the field, particularly in terms of balance and independence. The use of a stratified split and the removal of redundant sequences help mitigate common issues such as overfitting and data leakage, which are critical for the reproducibility and reliability of machine learning models. The balanced nature of the dataset, with an equal number of positive and negative observations, also aligns with best practices for training robust models.",
  "dataset/availability": "The data used in this study, including the data splits for training and testing, have been made publicly available. All relevant datasets, as well as the code used to develop the machine learning model, can be accessed on GitLab. The specific repository is located at [https://gitlab.com/machine-biology-group-public/pancleave](https://gitlab.com/machine-biology-group-public/pancleave). This public release ensures transparency and reproducibility, allowing other researchers to verify and build upon the findings presented in this work.\n\nThe availability of these datasets and code is crucial for preventing data leakage and ensuring that the performance metrics are not biased. By making the data publicly accessible, we aim to address the reproducibility challenges often faced in the machine learning community. This approach also facilitates cross-referencing with other datasets, ensuring that the models developed are robust and generalizable.\n\nAdditionally, all data pertaining to the experimental validation of generated peptides are available in the Supplementary Data. Any further information required to reanalyze the data reported in this paper can be obtained from the lead contact upon request. This comprehensive approach to data sharing underscores our commitment to open science and collaborative research.",
  "optimization/algorithm": "The machine-learning algorithms used in our study belong to several well-established classes, including ensemble methods, neural networks, and support vector machines. Specifically, we implemented Random Forest (RF), Recurrent Neural Network (RNN), and Support Vector Machine (SVM) classifiers. These algorithms are not new; they are widely recognized and utilized in the machine learning community.\n\nThe choice of these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to capture intricate patterns. The Random Forest algorithm, for instance, is known for its robustness and ability to handle high-dimensional data, making it suitable for our protein encoding tasks. Similarly, Recurrent Neural Networks are powerful for sequential data, and Support Vector Machines are effective for classification tasks with clear margin separation.\n\nGiven that these algorithms are established and widely used, there was no need to publish them in a machine-learning journal. Instead, our focus was on applying these algorithms to the specific problem of protein fragment curation and antimicrobial peptide prediction. The novelty of our work lies in the application of these algorithms to this particular biological context, rather than in the development of new machine-learning techniques.",
  "optimization/meta": "The optimization process involved selecting three finalists\u2014Random Forest (RF), Recurrent Neural Network (RNN), and Support Vector Machine (SVM)\u2014based on superior 10-fold cross-validation performance. These models were trained using the ProtFP encoding.\n\nOne of the curation methods used for selecting fragments for synthesis was a machine learning model consensus. This method involved using six publicly available antimicrobial peptide (AMP) classifiers. These classifiers were amPEPpy, iAMP-2L, AMPScanner, dAMPpred, MLAMP, and iAMPpred. The consensus prediction method aggregated the results from these six models to select fragments.\n\nThe training and testing data for these prior models were necessary to ensure that there was no data leakage, which could bias performance metrics. This highlights the importance of publicly releasing all training and testing data to ensure reproducibility and prevent bias in performance metrics. The data used for training and testing the final model was independently curated to avoid any overlap with the data used to train the individual AMP classifiers. This ensures that the final model's performance is a true reflection of its predictive power and not an artifact of data leakage.",
  "optimization/encoding": "The data encoding process for the machine-learning algorithm involved several steps. Initially, a sliding window approach was applied to each input protein sequence, generating every possible 8-residue contiguous subsequence. These subsequences were then converted into numerical feature vectors using the ProtFP encoding method. This encoding method was chosen for its effectiveness in representing protein sequences in a format suitable for machine learning models.\n\nThe encoded subsequences were then used to predict the label and estimated probability of class membership for each subsequence. This prediction step was crucial for identifying potential cleavage sites within the protein sequences. Following the prediction, the full protein string was tokenized at each predicted cleavage site, resulting in a list of peptide fragments.\n\nAdditional utility functions were provided to support the pipeline, including prediction filtering functionality and FASTA file conversion. These functions enhanced the flexibility and usability of the pipeline, allowing for further analysis and manipulation of the encoded data.\n\nThe final model, a random forest classifier, was trained and tested on a balanced dataset of 49,634 observations, with each observation being an 8-residue sequence. The dataset included both positive observations, curated from the MEROPS Peptidase Database, and negative observations generated from the human proteome and random protein space. This balanced approach ensured that the model could accurately distinguish between true cleavage sites and non-cleavage sites.\n\nThe training and testing process involved a stratified train-test split, maintaining a representative label distribution in each split. This careful preprocessing and encoding of the data were essential for the successful training and validation of the machine-learning model.",
  "optimization/parameters": "The optimization process involved six different classifiers, each trained and tested on five distinct input representations. This resulted in a total of 30 candidate models. The classifiers used were Gaussian Process, K-Nearest Neighbor, Naive Bayes, Random Forest, Recurrent Neural Network, and Support Vector Machine. The input representations included one-hot encoding, ProtFP, ST-Scale, Z-Scale, and UniRep.\n\nHyperparameter tuning was performed using Bayesian search with 10-fold cross-validation on the training set. This method systematically explored the hyperparameter space to identify the optimal settings for each model. The final model selected was a Random Forest classifier trained on the ProtFP encoding. This model was chosen based on its superior performance in 10-fold cross-validation accuracy, AUC-ROC, average precision, and estimated probability thresholding.\n\nThe Random Forest model used 400 estimators (individual trees) and a Shannon information gain criterion. The hyperparameters for this model were set as follows: bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion=\"entropy\", max_depth=None, max_features=3, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=5, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=-1, oob_score=False, random_state=5, verbose=0, and warm_start=False.\n\nThe selection of the number of parameters (p) was driven by the need to balance model complexity and performance. The use of 400 estimators in the Random Forest model was determined through the hyperparameter tuning process, which aimed to optimize the model's ability to generalize to unseen data. The other hyperparameters were similarly tuned to ensure the model's robustness and accuracy.",
  "optimization/features": "The input features used in our study were derived from the ProtFP encoding method. This encoding method converts each 8-residue subsequence of a protein sequence into a numerical feature vector. The specific number of features (f) generated by ProtFP encoding is not explicitly stated, but it is a fixed number determined by the encoding method itself.\n\nFeature selection was not explicitly performed as a separate step in our process. Instead, the ProtFP encoding method inherently selects relevant features based on the properties of the amino acids in the subsequences. This encoding method was applied uniformly to all subsequences in the dataset, ensuring that the same set of features was used for all observations.\n\nThe training and testing of our models were conducted using a balanced dataset of 49,634 observations, with 80% of the data used for training and 10-fold cross-validation, and the remaining 20% reserved as an independent test set. The feature encoding was applied consistently across both the training and test sets, ensuring that the models were evaluated on features that were representative of the entire dataset.",
  "optimization/fitting": "The fitting method employed in this study involved training six different classifiers using scikit-learn and TensorFlow. These classifiers included Gaussian Process (GP), K-Nearest Neighbor (KNN), Naive Bayes (NB), Random Forest (RF), Recurrent Neural Network (RNN), and Support Vector Machine (SVM). Each classifier was trained and tested on five different input representations: one-hot encoding, ProtFP, ST-Scale, Z-Scale, and UniRep.\n\nThe Random Forest (RF) classifier, which ultimately became the final model, used 400 estimators (individual trees) and a Shannon information gain criterion. The hyperparameters for the RF model were tuned using Bayesian search with 10-fold cross-validation on the training set. This approach helped in selecting the optimal hyperparameters and mitigating the risk of overfitting.\n\nTo address the potential issue of overfitting, several measures were taken. First, the dataset was balanced, ensuring that both positive and negative observations were equally represented. Second, the training and testing data were split in a stratified manner to maintain the label distribution representative of the entire dataset. Additionally, the model's performance was evaluated using multiple metrics, including test set accuracy, area under the receiver-operating characteristic curve (AUC-ROC), and average precision. The tradeoff between accuracy and the number of valid observations at different probability thresholds was also quantified and visualized.\n\nThe final RF model's performance was assessed on an independent test set, which was not used during the training or cross-validation process. This independent evaluation provided a robust measure of the model's generalizability and helped in ruling out overfitting. The model's performance was further validated by comparing it with pre-existing models for specific proteases, demonstrating its superior accuracy.\n\nUnderfitting was addressed by ensuring that the model had sufficient complexity to capture the underlying patterns in the data. The use of 400 estimators in the RF model provided a robust ensemble of decision trees, reducing the risk of underfitting. Additionally, the Bayesian search for hyperparameter tuning helped in finding the optimal configuration that balanced model complexity and performance.\n\nIn summary, the fitting method involved a rigorous process of hyperparameter tuning, cross-validation, and independent testing to ensure that the final model was neither overfitted nor underfitted. The use of multiple performance metrics and comparisons with existing models further validated the model's robustness and generalizability.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was hyperparameter tuning through Bayesian search, which helped in selecting the optimal parameters for each classifier. This process was conducted using the skopt Python package and involved 10-fold cross-validation on the training set. Cross-validation is a powerful technique that helps in assessing how the statistical model will generalize to an independent data set.\n\nAdditionally, we utilized a Random Forest classifier as our final model, which inherently includes mechanisms to prevent overfitting. The Random Forest algorithm aggregates the predictions of multiple decision trees, each trained on a different subset of the data. This ensemble approach reduces the risk of overfitting by averaging out the errors made by individual trees. Furthermore, the Random Forest classifier used in our study included a criterion based on Shannon information gain, which helps in selecting the most informative features for splitting the data, thereby improving the model's generalization capability.\n\nAnother regularization technique employed was the use of a minimum number of samples required to split an internal node and a minimum number of samples required to be at a leaf node. These parameters help in controlling the complexity of the trees in the forest, preventing them from becoming too deep and overfitting the training data. Specifically, the final Random Forest model used a minimum of 2 samples required to split an internal node and a minimum of 5 samples required to be at a leaf node.\n\nMoreover, the use of different input representations for training the classifiers, such as one-hot encoding, ProtFP, ST-Scale, Z-Scale, and UniRep, provided diverse feature sets that helped in capturing different aspects of the data. This diversity in input representations contributed to the model's ability to generalize better to unseen data.\n\nIn summary, our study incorporated multiple overfitting prevention techniques, including hyperparameter tuning, ensemble learning with Random Forests, and the use of diverse input representations. These methods collectively ensured that our models were robust and capable of generalizing well to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters are indeed available. All relevant data and code used to develop the machine learning model, including the final preserved model and a Jupyter Notebook that replicates the training process, can be found on GitLab. The repository contains all hyper-parameter values listed in the README file. Additionally, the training data, testing data, and code are freely accessible on the same platform. This ensures that the entire optimization process is transparent and reproducible. The experimental validation data for the generated peptides are also available in the Supplementary Data. For any additional information required to reanalyze the data, one can contact the lead author.",
  "model/interpretability": "The model employed in our study is not entirely a black box, as it leverages a random forest algorithm, which inherently provides some level of interpretability. This algorithm offers in-built feature importance calculations, allowing us to gain insights into which features are most significant in making predictions.\n\nTo assess feature importance, we conducted an analysis based on the mean decrease in impurity using functions provided by scikit-learn for random forest classifiers. The results indicated that the most significant features correspond to the residues closest to the cleavage site, specifically the P1 and P1\u2019 positions within the eight-residue P4:P4\u2019 flanking site. This finding aligns with expectations, as residues near the cleavage site are likely to play a crucial role in determining cleavage events.\n\nHowever, it is important to note that relative feature importances can vary across different scoring methods. Therefore, while these results provide valuable insights, we avoid overinterpreting them. The interpretability of the model is thus somewhat limited by the variability in feature importance scoring methods, but the random forest algorithm does offer a degree of transparency that other models might lack.",
  "model/output": "The model developed in this study is a classification model. It was designed to predict cleavage sites in proteins, specifically focusing on identifying antimicrobial peptides (AMPs). The model evaluates the probability of class membership, which in this context refers to the likelihood of a peptide being an AMP. The final model selected was a Random Forest classifier, which is a type of machine learning algorithm used for classification tasks. The performance of the model was assessed using metrics such as test set accuracy, area under the receiver-operating characteristic curve (AUC-ROC), and average precision, all of which are commonly used to evaluate classification models. Additionally, the model's accuracy was evaluated at various probability thresholds to understand the trade-off between accuracy and the number of valid observations. The Random Forest classifier provides feature importance calculations, which help in interpreting the model by identifying the most significant features contributing to the predictions. These features are primarily the residues closest to the cleavage site, indicating the model's focus on local sequence information for classification.",
  "model/duration": "The execution time for the model involved several stages, primarily centered around hyperparameter tuning and training. Six different classifiers were implemented and tested on five input representations, resulting in 30 candidate models. Each of these models underwent Bayesian search hyperparameter tuning using 10-fold cross-validation on the training set. This process was conducted on the Stampede2 supercomputer, which facilitated the computational demands of the hyperparameter tuning.\n\nThe final model selected was a Random Forest (RF) classifier trained on the ProtFP protein encoding. This model was chosen based on its superior performance metrics, including 10-fold cross-validation accuracy, AUC-ROC, average precision, and estimated probability thresholding. The RF model used 400 estimators and a Shannon information gain criterion.\n\nThe specific execution time for each stage, such as data preprocessing, model training, and evaluation, is not detailed. However, the use of a high-performance computing resource like the Stampede2 supercomputer indicates that the process was computationally intensive and likely required significant time to complete. The final preserved model and the Jupyter Notebook that replicates the training are available on GitLab, providing further details on the execution process.",
  "model/availability": "The source code for the machine learning model developed in this study is freely available. It can be accessed on GitLab at the following URL: https://gitlab.com/machine-biology-group-public/pancleave. This repository contains the training data, testing data, and the code used to develop the model. Additionally, a Jupyter Notebook that replicates the training process is provided, along with all hyperparameter values listed in the repository README. This ensures that the model can be reproduced and utilized by other researchers. The repository also includes any additional information required to reanalyze the data reported in the paper, which is available from the lead contact upon request.",
  "evaluation/method": "The evaluation of the method involved a rigorous process to ensure the robustness and accuracy of the final model. Initially, six different classifiers were implemented using scikit-learn and TensorFlow. These classifiers included Gaussian Process, K-Nearest Neighbor, Naive Bayes, Random Forest, Recurrent Neural Network, and Support Vector Machine. Each classifier was trained and tested on five different input representations: one-hot encoding, ProtFP, ST-Scale, Z-Scale, and UniRep. This resulted in 30 candidate models, each of which underwent Bayesian search hyperparameter tuning using the skopt Python package. The tuning process utilized 10-fold cross-validation on the training set to optimize the performance of each model.\n\nFrom these 30 candidate models, three finalists were selected based on superior 10-fold cross-validation performance: Random Forest, Recurrent Neural Network, and Support Vector Machine, all trained on the ProtFP encoding. These finalists were then assessed using three key performance metrics: test set accuracy, area under the receiver-operating characteristic curve (AUC-ROC), and average precision. Additionally, accuracy was evaluated at various estimated probability thresholds of class membership, specifically at \u226550%, \u226560%, \u226570%, \u226580%, and \u226590%. The tradeoff between increased accuracy and decreased total valid observations at these thresholds was quantified and visualized.\n\nAmong the finalists, the Random Forest model trained on the ProtFP protein encoding was chosen as the final model. This selection was based on its marginally superior performance across all evaluated metrics, including 10-fold cross-validation accuracy, AUC-ROC, average precision, and estimated probability thresholding. The final Random Forest model used 400 estimators and a Shannon information gain criterion. The hyperparameter values for this model were documented and are available in the repository README on GitLab. The final preserved model and a Jupyter Notebook replicating the training process are also available on GitLab, ensuring reproducibility and transparency.",
  "evaluation/measure": "In the evaluation of our model, we employed several performance metrics to comprehensively assess its effectiveness. The primary metrics reported include test set accuracy, the area under the receiver-operating characteristic curve (AUC-ROC), and average precision. These metrics were computed using the scikit-learn library, ensuring consistency and reliability in our evaluations.\n\nTest set accuracy provides a straightforward measure of the model's correctness on unseen data, indicating how often the model's predictions match the actual outcomes. The AUC-ROC curve offers a more nuanced view by evaluating the model's ability to distinguish between positive and negative classes across various threshold levels. This metric is particularly useful for imbalanced datasets, as it considers the trade-off between true positive and false positive rates.\n\nAverage precision complements the AUC-ROC by focusing on the precision-recall trade-off, which is crucial for understanding the model's performance in scenarios where the cost of false positives and false negatives varies. Additionally, we assessed accuracy at different estimated probability thresholds (50%, 60%, 70%, 80%, and 90%), quantifying the trade-off between increased accuracy and the reduction in the number of valid observations at each threshold.\n\nNegative predictive value, positive predictive value, sensitivity, and specificity were also reported, providing a detailed view of the model's diagnostic ability. These metrics are essential for understanding the model's performance in clinical and biological contexts, where the consequences of false positives and false negatives can be significant.\n\nThe set of metrics used is representative of standard practices in the literature, ensuring that our evaluations are comparable to other studies in the field. By including a diverse range of metrics, we aim to provide a thorough assessment of the model's performance, addressing various aspects of its predictive capability and reliability.",
  "evaluation/comparison": "A comparison to publicly available methods was performed using six different machine learning-based antimicrobial peptide (AMP) classifiers. These classifiers were used to achieve a consensus prediction for fragment curation. However, direct comparison with pre-existing models was limited to three specific caspases, where the performance of the panCleave model was evaluated relative to these existing models.\n\nRegarding simpler baselines, the evaluation focused on more complex models, specifically a Random Forest (RF), Recurrent Neural Network (RNN), and Support Vector Machine (SVM), all trained on the ProtFP encoding. These models were selected based on superior 10-fold cross-validation performance. The final model chosen was an RF with 400 estimators and a Shannon information gain criterion, which demonstrated marginally superior performance in terms of accuracy, area under the receiver-operating characteristic curve (AUC-ROC), average precision, and estimated probability thresholding.\n\nThe evaluation also included an assessment of accuracy at various probability thresholds (50%, 60%, 70%, 80%, and 90%), quantifying the tradeoff between increased accuracy and decreased valid observations. This comprehensive evaluation ensured that the selected model was robust and performed well across multiple metrics.\n\nThe performance of the panCleave model was further validated on an independent test dataset consisting of 9,927 observations. The evaluation included accuracy-probability threshold tradeoff curves, receiver operating characteristic curves, precision-recall curves, and accuracy assessments for different proteases and catalytic types. Additionally, the positive hit rates were compared across different fragment curation methods and antimicrobial activity classifiers.\n\nIn summary, while a direct comparison to simpler baselines was not explicitly detailed, the evaluation process involved a rigorous assessment of complex models against benchmark datasets and publicly available methods, ensuring the reliability and superiority of the final selected model.",
  "evaluation/confidence": "The evaluation of our method, panCleave, includes several performance metrics that provide a comprehensive assessment of its effectiveness. These metrics include accuracy, area under the receiver-operating characteristic curve (AUC-ROC), and average precision. The accuracy was evaluated at various probability thresholds, such as \u226550%, \u226560%, \u226570%, \u226580%, and \u226590%, to understand the tradeoff between accuracy and the number of valid observations.\n\nStatistical significance was determined using appropriate tests. For instance, in the mouse experiments, one-way ANOVA followed by Dunnett\u2019s test was used to assess statistical significance. In cases where the data did not meet the assumptions of normality and equal variance, the Kruskal-Wallis test was employed. All p-values are provided for each group, and comparisons were made against the untreated control group.\n\nThe random forest model, which was selected as the final model, was evaluated using 10-fold cross-validation. This approach ensures that the model's performance is robust and not dependent on a specific subset of the data. The final model's hyperparameters were carefully tuned to optimize performance, and the results indicate that it outperforms other candidate classifiers in terms of cross-validation accuracy, AUC-ROC, and average precision.\n\nAdditionally, the model's performance was disaggregated by protease, showing that it achieves high accuracy for specific proteases such as caspase-3, caspase-6, granzyme B, legumain, and cathepsin S. This disaggregated analysis provides confidence in the model's applicability across different proteases.\n\nThe feature importance analysis, conducted using the mean decrease in impurity, further supports the model's interpretability. The most significant features correspond to the residues proximal to the cleavage site, which is consistent with biological expectations.\n\nIn summary, the performance metrics and statistical analyses provide strong evidence of the model's superiority and reliability. The use of cross-validation, appropriate statistical tests, and detailed performance evaluations ensures that the claims of superiority are well-supported.",
  "evaluation/availability": "The raw evaluation files used in our study are freely available. Specifically, the training data, testing data, and code used to develop the machine learning model can be accessed on GitLab. This includes all necessary information to replicate the training process and evaluate the model's performance. Additionally, all data pertaining to the experimental validation of generated peptides are available in the Supplementary Data. For any further information required to reanalyze the data reported in this paper, it is available from the Lead Contact upon request. This ensures transparency and reproducibility, allowing other researchers to verify and build upon our findings."
}