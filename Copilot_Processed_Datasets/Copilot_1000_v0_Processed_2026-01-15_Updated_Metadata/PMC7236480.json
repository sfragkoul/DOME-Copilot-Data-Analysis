{
  "publication/title": "Machine Learning and Statistical Models to Predict Postpartum Hemorrhage.",
  "publication/authors": "Venkatesh KK, Strauss RA, Grotegut CA, Heine RP, Chescheir NC, Stringer JSA, Stamilio DM, Menard KM, Jelovsek JE",
  "publication/journal": "Obstetrics and gynecology",
  "publication/year": "2020",
  "publication/pmid": "32168227",
  "publication/pmcid": "PMC7236480",
  "publication/doi": "10.1097/aog.0000000000003759",
  "publication/tags": "- Machine Learning\n- Postpartum Hemorrhage\n- Predictive Modeling\n- Obstetrics\n- Statistical Models\n- Logistic Regression\n- Random Forest\n- Extreme Gradient Boosting\n- Maternal Health\n- Pregnancy Complications",
  "dataset/provenance": "The dataset used for this study was sourced from the Eunice Kennedy Shriver National Institute of Child Health and Human Development Consortium on Safe Labor (CSL). This dataset is a retrospective cohort study that includes data from women who delivered at 23 weeks of gestation or more between 2002 and 2008. The data was collected from 12 clinical sites with 19 hospitals across 9 American College of Obstetricians and Gynecologists (ACOG) districts in the U.S.\n\nThe CSL cohort comprises a total of 228,438 deliveries. However, data for estimated blood loss was available from only 10 of the 12 sites, resulting in 152,279 data points being used for the analysis. This subset represents approximately 66.6% of the total births in the CSL cohort.\n\nThe dataset includes a variety of information abstracted from electronic medical records, such as demographics, prenatal complications, labor and delivery information, and maternal and neonatal outcomes. This data has been utilized in previous research and by the community, particularly in studies focused on labor and delivery outcomes. The CSL dataset has been instrumental in developing and validating prediction models for various obstetric conditions, including postpartum hemorrhage.",
  "dataset/splits": "The dataset was split by time, specifically into two phases. The first phase, used for model derivation, included data from 2002 to 2006. The second phase, used for model validation, included data from 2007 to 2008. This temporal validation approach was chosen for its methodological rigor, especially when dealing with large sample sizes.\n\nIn addition to the temporal split, a combined approach of site-specific and temporal validation was employed. This involved using each site once as a validation sample, with the remaining sites used for model derivation during the first phase. Site-specific estimates of discrimination and calibration were then pooled and tested in the second phase.\n\nThe dataset consisted of 228,438 births across 12 sites. Data for estimated blood loss was available from 10 of these 12 sites, totaling 152,279 births, which represents 66.6% of the total births. The mean estimated blood loss was 445 mL, with a standard deviation of 2,327 mL. The distribution of data points in each split was not explicitly detailed, but the temporal validation ensured that the model was tested on data from different time periods, providing a robust assessment of its performance.",
  "dataset/redundancy": "The dataset was split by time, a method known as temporal validation. This involved using data from the first phase (2002\u20132006) for model derivation and the second phase (2007\u20132008) for model validation. This approach is considered methodologically more rigorous than a simple random split, especially when the sample size is very large.\n\nTo further ensure the robustness of the models, we combined both site-specific and temporal validation. This was achieved by using each site once as a validation sample, with the remaining sites used for model derivation during the first phase. Site-specific estimates of discrimination and calibration were then pooled and tested in the second phase. This method helps to ensure that the training and test sets are independent, as data from different time periods and different sites are used for training and validation.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the medical field. The large size of the dataset, spanning nearly a decade and including data from multiple hospitals across the U.S., enhances its generalizability. However, it is important to note that the dataset is now a decade old, and replication in more contemporary prospective datasets is recommended for future studies. Additionally, the dataset has some limitations, such as missing data and incomplete ascertainment of certain variables, which may affect the general application of these models.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the ensemble learning class. Specifically, we employed Extreme Gradient Boosting and Random Forest models. These are well-established algorithms known for their robustness and high predictive performance in various domains, including healthcare.\n\nThe algorithms used are not new; they have been extensively studied and applied in both machine learning and clinical research. Extreme Gradient Boosting, often referred to as XGBoost, is a popular implementation of gradient boosting that is known for its efficiency and effectiveness in handling large datasets. Random Forest is another widely used ensemble method that combines multiple decision trees to improve the accuracy and control over-fitting.\n\nThe choice to use these established algorithms in a clinical context, rather than a machine-learning journal, is driven by the practical application and validation of these models in a real-world healthcare setting. The focus of this study is on the clinical utility and performance of these models in predicting postpartum hemorrhage, rather than the development of new machine-learning algorithms. By demonstrating the effectiveness of these models in a clinical environment, we aim to highlight their potential to improve patient outcomes and inform clinical decision-making.",
  "optimization/meta": "Not applicable. The models discussed do not use data from other machine-learning algorithms as input. The models developed and validated include Extreme Gradient Boosting, random forest, logistic regression, and lasso regression. Each of these models was trained and validated independently using data from the Eunice Kennedy Shriver National Institute of Child Health and Human Development Consortium on Safe Labor (CSL). The dataset was split temporally and by site to ensure robust validation, but there is no indication that the models themselves are meta-predictors that combine the outputs of other machine-learning algorithms.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithms involved several steps to ensure the models could effectively learn from the dataset. Variable reduction was performed using backward stepwise elimination during logistic regression, with a bootstrap bias-corrected concordance index serving as the stopping criterion. Variables with individual P-values greater than 0.05 were retained if they improved the overall model. For lasso regression, 10-fold cross-validation of the lambda value was used, and the final model included variables that were most predictive within one standard error of the best value.\n\nMissing predictor values were handled using Multiple Imputed Chained Equations (MICE). This method is robust for dealing with missing data and ensures that the models are trained on a complete dataset. The frequency of missing data for each variable did not vary significantly over time between the two periods used in temporal validation (2002\u20132006 and 2007\u20132008).\n\nThe dataset was split temporally, with the first phase (2002\u20132006) used for model derivation and the second phase (2007\u20132008) for model validation. This approach is methodologically rigorous, especially when dealing with large sample sizes. Additionally, site-specific validation was combined with temporal validation by using each site once as a validation sample, with the remaining sites used for model derivation during the first phase. Site-specific estimates of discrimination and calibration were pooled and tested in the second phase.\n\nThe dataset included 55 candidate predictors of postpartum hemorrhage, encompassing socio-demographic, obstetric, clinical, and physiologic variables. These variables were assessed for their association with postpartum hemorrhage and included in the models accordingly. The final models were internally validated using bootstrapping or cross-validation to measure optimism-corrected reliability, ensuring that the models' performance was robust and generalizable.",
  "optimization/parameters": "In our study, the number of input parameters used in the models varied depending on the variable selection process employed. For logistic regression, we started with a full model and used backwards stepwise elimination to reduce the number of variables. The stopping criteria for this process was a bootstrap bias-corrected concordance index, with variables having individual P values greater than 0.05 being retained if they improved the overall model. The removal of each variable was evaluated based on its effect on the adjusted R2, and the process was stopped when the bootstrap concordance index changed by 0.001.\n\nFor lasso regression, variable reduction was performed using 10-fold cross-validation of the lambda value. The final model incorporated the variables that were most predictive within one standard error of the best value. This approach allowed us to select a subset of variables that provided the best predictive performance while minimizing overfitting.\n\nThe specific number of parameters (p) used in each model can be found in the appendices, where the variables included in each model are listed. The Extreme Gradient Boosting model, which was the best performing, included the top 10 variables ranked from most to least important. These variables included pre-pregnancy maternal weight, admission maternal weight, prenatal diagnosis of fetal macrosomia, admission temperature, attempted trial of labor on admission, pre-pregnancy maternal body mass index, admission systolic blood pressure, multiple gestation, anemia diagnosis during pregnancy, and spontaneous labor on admission.",
  "optimization/features": "In our study, we initially considered 55 candidate predictors of postpartum hemorrhage, which were compiled from expert opinion, consensus statements, prior observational cohorts, and data available in the CSL database. These predictors included a mix of socio-demographic, obstetric, clinical, and physiologic variables.\n\nFeature selection was indeed performed to eliminate the number of variables and improve model predictions. For logistic regression, we used backwards stepwise elimination with a bootstrap bias-corrected concordance index as the stopping criteria. Variables with individual P values greater than 0.05 were retained if they contributed to improving the overall model. The removal of each variable was evaluated based on its effect on the adjusted R2, and the process stopped when the bootstrap concordance index changed by 0.001.\n\nFor lasso regression, feature selection was performed using 10-fold cross-validation of the lambda value. The final model incorporated the variables that were most predictive within one standard error of the best value.\n\nAll feature selection processes were conducted using the training set only, ensuring that the validation set remained independent for unbiased performance evaluation. The specific variables included in each model are detailed in the appendix.",
  "optimization/fitting": "In our study, we employed several techniques to address both overfitting and underfitting in our models. Initially, we started with a large number of candidate predictors, which could potentially lead to overfitting. To mitigate this, we performed variable reduction using different methods depending on the model.\n\nFor logistic regression, we used backwards stepwise elimination with a bootstrap bias-corrected concordance index as the stopping criterion. This process ensured that only the most relevant variables were retained, reducing the risk of overfitting. Variables with individual P-values greater than 0.05 were considered for retention if they improved the overall model performance.\n\nIn lasso regression, we utilized 10-fold cross-validation to select the optimal lambda value, which helps in regularizing the model and preventing overfitting by shrinking less important variable coefficients to zero.\n\nTo assess model performance and ensure we were not underfitting, we used three key measures: the C statistic (area under the ROC curve), calibration curves, and decision curves. The C statistic provided an overall measure of the model's discriminative ability. Calibration curves helped us understand how well the predicted probabilities matched the observed outcomes, ensuring the model was well-calibrated and not underfitting. Decision curve analysis quantified the net benefit of each model across a range of threshold probabilities, providing a practical measure of the model's clinical utility.\n\nAdditionally, we internally validated all models using bootstrapping or cross-validation to measure optimism-corrected reliability. This step was crucial in ensuring that our models generalized well to new data and were not overfitting or underfitting.\n\nMissing data were handled using Multiple Imputed Chained Equations (MICE), which helped in maintaining the integrity of the dataset and ensuring that the models were robust to missing values. This approach further aided in preventing underfitting by utilizing all available information.\n\nIn summary, our approach to fitting the models involved careful variable selection, regularization techniques, thorough validation, and the use of multiple performance measures to balance between overfitting and underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and enhance the generalization of our models. For the logistic regression model, we utilized lasso regularization, which is a form of L1 regularization that adds a penalty equal to the absolute value of the magnitude of coefficients. This method helps to shrink some coefficients to zero, effectively performing variable selection and reducing the model complexity. The optimal lambda value for lasso regularization was determined through 10-fold cross-validation, ensuring that the final model included the most predictive variables within one standard error of the best value.\n\nAdditionally, we performed variable reduction during logistic regression using backward stepwise elimination. This process started with the full model and employed a bootstrap bias-corrected concordance index as the stopping criterion. Variables with individual p-values greater than 0.05 were retained if they contributed to improving the overall model. The removal of each variable was evaluated based on its impact on the adjusted R\u00b2, and the process was halted when the bootstrap concordance index changed by less than 0.001.\n\nFor the machine learning models, such as random forest and extreme gradient boosting, we used internal validation techniques like bootstrapping and cross-validation to measure optimism-corrected reliability. These methods help to assess the models' performance on unseen data and ensure that they generalize well to new samples. Furthermore, we imputed missing predictor values using Multiple Imputed Chained Equations (MICE) to handle any potential biases that might arise from incomplete data.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models we employed, particularly the Extreme Gradient Boosting and random forest models, are often considered to be somewhat of a black box due to their complex nature and the numerous interactions between variables. However, we have taken steps to enhance their interpretability.\n\nFor instance, we have provided variable importance plots, which rank the top predictors of postpartum hemorrhage. In the Extreme Gradient Boosting model, the most important variables included pre-pregnancy maternal weight, admission maternal weight, and prenatal diagnosis of fetal macrosomia, among others. These plots help to understand which factors contribute most significantly to the model's predictions.\n\nAdditionally, the calibration curves and decision curve analysis offer insights into how well the models' predicted probabilities align with actual outcomes and the net benefit across different threshold probabilities. These visualizations help clinicians understand the model's performance and reliability in various scenarios.\n\nWhile these models may not be entirely transparent, the use of variable importance and performance metrics allows for a degree of interpretability, enabling clinicians to understand the key drivers of postpartum hemorrhage risk and the models' predictive capabilities.",
  "model/output": "The models developed in this study are primarily classification models. They are designed to predict the binary outcome of postpartum hemorrhage, defined as an estimated blood loss of 1,000 mL or more. The models use various algorithms, including logistic regression, lasso regression, random forest, and Extreme Gradient Boosting, to classify women as either having or not having postpartum hemorrhage based on data available at the time of labor admission.\n\nThe performance of these models was evaluated using metrics such as the C statistic (concordance index), which measures discrimination, and calibration curves, which assess the agreement between predicted and actual probabilities. The Extreme Gradient Boosting model demonstrated the highest discriminative ability with a C statistic of 0.93, followed by the random forest model with a C statistic of 0.92. Logistic regression and lasso regression had slightly lower but still good discriminative abilities, with C statistics of 0.87.\n\nThe models were validated both temporally and by site, ensuring their robustness and generalizability. Decision curve analysis further showed that all models provided a superior net benefit when clinical decision thresholds were between 0 to 80% predicted risk, with the Extreme Gradient Boosting model offering the greatest net benefit across this range.\n\nIn summary, the models are classification models that effectively predict the risk of postpartum hemorrhage using data readily available to obstetric providers at the time of labor admission.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the models involved several rigorous methods to ensure their reliability and generalizability. Temporal validation was employed, where the dataset was split by time into two phases: 2002\u20132006 for model derivation and 2007\u20132008 for validation. This approach is particularly robust when dealing with large sample sizes. Additionally, temporal and site validation were combined, using each site once as a validation sample while the remaining sites were used for model derivation during the first phase. This method helps to account for variations across different clinical settings.\n\nModel performance was assessed using three key measures: the C statistic (area under the receiver operating characteristic curve), calibration curves, and decision curves. The C statistic evaluates the model's ability to discriminate between high- and low-risk patients. Calibration curves were plotted to show the relationship between predicted outcomes and observed outcomes, with a perfectly calibrated model following a 45\u00b0 line. Decision curve analysis (DCA) was used to quantify the net benefit of each model, considering the range of patient preferences for accepting risks of undertreatment and overtreatment. This analysis helps in understanding the clinical value of the models by evaluating the benefits and harms associated with different testing and treatment strategies.\n\nVariable importance was also plotted to understand the contribution of each predictor in the machine learning models. All models were internally validated using bootstrapping or cross-validation to measure optimism-corrected reliability. Missing predictor values were imputed using Multiple Imputed Chained Equations (MICE). The frequency of missing data for each variable did not vary significantly over time between the two time periods used in temporal validation.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the predictive models for postpartum hemorrhage. The primary metric used was the C statistic, also known as the area under the receiver operating characteristic (ROC) curve. This metric assesses the model's overall ability to discriminate between high- and low-risk patients.\n\nAdditionally, we utilized calibration curves to evaluate how well the predicted probabilities align with the actual outcomes. A perfectly calibrated model would follow a 45\u00b0 line on the calibration plot, indicating that the predicted probabilities match the observed outcomes.\n\nDecision curve analysis (DCA) was another key metric employed. DCA quantifies the net benefit of using each model by comparing the expected benefits and harms across a range of threshold probabilities. This analysis helps in understanding the clinical utility of the models by considering the trade-offs between undertreatment and overtreatment.\n\nVariable importance was also plotted to understand the contribution of each predictor in the machine learning models. This metric provides a scaled measure of the importance of each variable, with a maximum value of 100.\n\nThe models were internally validated using bootstrapping or cross-validation to measure optimism-corrected reliability. This ensures that the models' performance is robust and generalizable.\n\nThese metrics are widely recognized in the literature for evaluating clinical prediction models. The C statistic is a standard measure for assessing discriminative ability, while calibration curves and decision curve analysis provide insights into the models' clinical applicability and reliability. The use of variable importance helps in interpreting the models and identifying key predictors. Overall, the chosen metrics provide a thorough evaluation of the models' performance and their potential for clinical application.",
  "evaluation/comparison": "In our study, we did not compare our models to publicly available methods on benchmark datasets. Instead, we focused on validating our models using temporal and site-specific data from our cohort. This approach allowed us to assess the performance of our models in real-world clinical settings.\n\nWe did, however, compare our models to simpler baselines, specifically logistic regression and lasso regression. These statistical models served as benchmarks to evaluate the performance of our machine learning models, which included Extreme Gradient Boosting and random forest. The comparison showed that the machine learning models, particularly Extreme Gradient Boosting, had superior discriminative ability and provided a greater net benefit across a range of threshold probabilities.\n\nThe comparison to simpler baselines was crucial in demonstrating the added value of more complex machine learning models in predicting postpartum hemorrhage. While these models may introduce increased complexity, they also offer improved predictive performance using clinical and physiologic data readily available at the time of labor admission. This comparison highlights the potential benefits of integrating advanced machine learning techniques into clinical practice for better patient outcomes.",
  "evaluation/confidence": "The performance metrics presented in our study include confidence intervals, providing a measure of the uncertainty around the point estimates. For instance, the C statistic, which measures the model's discriminative ability, is reported with 95% confidence intervals for each model. This allows for a more nuanced understanding of the model's performance and the precision of the estimates.\n\nStatistical significance is also considered in our analysis. The decision curve analysis, for example, evaluates the net benefit of each model across a range of threshold probabilities, providing a visual and quantitative comparison of the models' performance. The Extreme Gradient Boosting model demonstrated the greatest net benefit across the range of threshold probabilities, suggesting its superiority in this context.\n\nMoreover, the calibration curves, which show the relationship between predicted and observed outcomes, are plotted with 95% confidence intervals. This helps to assess the model's calibration and the reliability of its predictions. The intercept and slope of the calibration curves, along with their confidence intervals, provide further insight into the model's performance and its potential for overfitting or underfitting.\n\nIn summary, the performance metrics in our study are accompanied by confidence intervals, and the results are statistically significant, allowing for robust claims about the models' performance and their comparative superiority.",
  "evaluation/availability": "Not enough information is available."
}