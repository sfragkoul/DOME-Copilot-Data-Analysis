{
  "publication/title": "Automatic segmentation and classification of breast lesions through identification of informative multiparametric PET/MRI features.",
  "publication/authors": "Vogl WD, Pinker K, Helbich TH, Bickel H, Grabner G, Bogner W, Gruber S, Bago-Horvath Z, Dubsky P, Langs G",
  "publication/journal": "European radiology experimental",
  "publication/year": "2019",
  "publication/pmid": "31030291",
  "publication/pmcid": "PMC6486931",
  "publication/doi": "10.1186/s41747-019-0096-3",
  "publication/tags": "- Breast cancer\n- Imaging\n- Magnetic resonance imaging\n- Positron emission tomography\n- Computer-aided diagnosis\n- Multiparametric imaging\n- Feature selection\n- Random forest\n- Lesion segmentation\n- Classification",
  "dataset/provenance": "The dataset used in this study was sourced from a prospective, single-institution study approved by the institutional review board of the Medical University of Vienna. All patients involved in the study provided written, informed consent. The specific number of data points is not explicitly stated, but the study involved a comprehensive analysis of imaging data, including multiparametric imaging (mpI) features collected across various modalities such as DCE-MRI, DWI, and 18F-FDG PET. The dataset includes both malignant and benign lesions, with a focus on evaluating the contribution of different features to segmentation and classification accuracy.\n\nThe study leveraged previously established methods and features, such as those reported by Woods et al. and Yao et al., which utilized texture features to achieve high ROC-AUC values. Additionally, the study built upon work by Twellmann et al. and Vignati et al., who reported high performance metrics for lesion detection and automated systems. The dataset and methods used in this study are part of a broader effort to improve the clinical exploitation of imaging parameters, enabling the design of feasible imaging paradigms and hypothesis generation related to disease mechanisms.\n\nThe datasets used and/or analyzed during the current study are available from the corresponding author upon reasonable request. This ensures that the community has access to the data for further research and validation of findings. The study was supported by various funding sources, including the Austrian Science Fund, the Austrian Federal Ministry of Economy, Family and Youth, and the H2020 Research and Innovation Framework Programme.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The datasets used and analyzed during the current study are not publicly available. However, they can be obtained from the corresponding author upon reasonable request. This approach ensures that the data is shared responsibly and ethically, adhering to the guidelines and regulations set by the institutional review board and the consent provided by the participants. The data sharing process is managed directly through the corresponding author to maintain control over the distribution and usage of the datasets.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the Random Forest (RF) classifier. This algorithm is well-established and widely used in various fields, including medical imaging, for its robustness and ability to handle high-dimensional data.\n\nThe RF classifier employed in our work is not a new algorithm. It has been extensively studied and applied in numerous research areas. The reason it was not published in a machine-learning journal is that our focus was on applying this established method to a specific problem in medical imaging\u2014namely, the segmentation and classification of breast lesions using multiparametric PET/MRI features. Our contribution lies in the innovative application of RF to this particular domain, rather than in the development of a new algorithm. We aimed to demonstrate the effectiveness of RF in improving the detection and classification of breast lesions, leveraging the unique characteristics of multiparametric imaging data.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. Instead, it relies on features extracted directly from imaging data. The features considered for training the lesion classification model include intensity-based, kinetic, morphological, and textural features derived from DCE-MRI, DWI ADC, and 18F-FDG-PET maps. The model is trained on features extracted from 1000 randomly selected samples per class and patient, ensuring that the training data is independent for each new patient. The trained model is then used to predict the segmentation label for voxels in the breast area of a new patient who was not part of the training dataset. This approach ensures that the model's performance is evaluated on unseen data, maintaining the independence of the training and validation datasets.",
  "optimization/encoding": "All computations were restricted to the breast area, which was segmented using an intensity-based growing region algorithm. MRI intensity values were standardized to zero mean and unit standard deviation, estimated from the breast area on the pre-contrast images. Features were computed from all imaging data, including changes in contrast over time and the summed-up contrast in the DCE-MRI sequence. The data was encoded as features extracted at each voxel location, which were then used to train a machine learning algorithm for lesion segmentation. For lesion classification, intensity-based, kinetic, morphological, and textural features were considered. Intensity-based features were calculated from DCE-MRI, DWI ADC, and the 18F-FDG-PET map. Kinetic features were derived from the contrast enhancement curves, while textural features were obtained using a volumetric texture analysis approach based on the grey-level co-occurrence matrix and Haralick texture features. Morphological features were obtained using shape descriptors. The features were ranked based on their contribution to segmentation and classification using Gini importance and minimum-redundancy-maximum-relevance measures. The top-ranked features were then used to train and validate the models.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the feature selection method employed. We utilized two primary feature selection techniques: Gini Importance (GI) and minimum-Redundancy-Maximum-Relevance (mRMR).\n\nFor the GI method, the optimal number of features was determined by evaluating the model's performance with an increasing number of top-ranked features. The performance peaked at four features for manual annotations and at ten features for automatic segmentations. This indicates that the GI method benefited from a relatively small set of highly informative features.\n\nOn the other hand, the mRMR method showed different optimal feature counts. For manual annotations, the model's performance was highest with just two features. For automatic segmentations, the performance peaked at three features. This suggests that mRMR is effective in selecting a minimal yet highly relevant set of features, reducing redundancy and enhancing the model's efficiency.\n\nThe selection of the number of parameters was driven by the goal of achieving the highest possible performance metrics, such as the Dice Similarity Coefficient (DSC) for segmentation and the Area Under the Curve (AUC) for classification. By systematically increasing the number of features and evaluating the model's performance, we identified the optimal number of parameters for each feature selection method and dataset type.\n\nIn summary, the number of parameters used in the model ranged from two to ten, depending on the feature selection method and whether manual or automatic segmentations were used. This approach ensured that the model was both efficient and effective in differentiating between malignant and benign lesions.",
  "optimization/features": "In our study, we utilized a variety of features extracted from multiparametric and multimodal imaging data. These features included dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI), diffusion-weighted imaging (DWI), and 18F-FDG positron emission tomography (PET) data. The specific features used were ranked based on their importance using two different methods: Random Forest Gini importance (GI) and minimum-redundancy-maximum-relevance (mRMR).\n\nFeature selection was indeed performed to identify the most relevant features for both segmentation and classification tasks. This process involved evaluating the contribution of each feature to the models' performance. The selection was done using the training set only, ensuring that the validation and testing phases remained unbiased.\n\nThe number of features used varied depending on the specific model and the feature selection method. For instance, the GI feature selection model for segmentation peaked at six features, while the mRMR feature selection model peaked at eight features. In the classification task, the mRMR feature selection model achieved the highest area under the curve (AUC) using only two features, whereas the GI feature selection model peaked at four features. This demonstrates the effectiveness of feature selection in improving model performance by focusing on the most informative features.",
  "optimization/fitting": "The fitting method employed in this study involves an asymmetric generalised logistic function, which is used to capture contrast enhancement kinetics. This function is multiplied with an exponential term and fitted to the characteristic kinetic curve. The parameters of this function include G (scaling), \u03b1 (asymmetry), \u03c4 (steepness), t1/2 (time point), \u03b2, and k (terminal slope).\n\nThe number of parameters in the model is not excessively large compared to the number of training points. To ensure that over-fitting is ruled out, the experiments were repeated 20 times, and averages for AUC and sensitivity/specificity were reported. This repetition helps in validating the consistency and robustness of the model. Additionally, the model's performance was evaluated using different feature selection methods, such as Gini importance and minimum-redundancy-maximum-relevance, which help in identifying the most relevant features and reducing the risk of over-fitting.\n\nTo address under-fitting, the model's performance was assessed with an increasing number of top-ranked features. This approach allows for the evaluation of the contribution of each individual feature in a multimodal, multiparametric setup. The performance of the models with different numbers of features indicates that the model is capable of capturing the necessary complexity in the data without being too simplistic.\n\nFurthermore, the use of multiparametric features, including DCE-MRI, DWI, and PET, enhances the model's ability to capture relevant information, thereby reducing the risk of under-fitting. The evaluation of feature contribution through measures like Gini importance and minimum-redundancy-maximum-relevance ensures that the model leverages the most informative features, leading to better generalization and performance.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method involved evaluating the contribution of features using two measures: Random Forest Gini Importance (GI) and minimum-redundancy-maximum-relevance (mRMR). These measures helped us rank the features based on their relevance and redundancy, allowing us to select the most informative features for training and validation. By successively increasing the number of features starting from the top-ranked ones, we could assess the contribution of each individual feature in a multimodal, multiparametric setup.\n\nAdditionally, we performed classification on both manually delineated lesions and automatically segmented lesions to study the impact of segmentation accuracy on classification. This approach allowed us to evaluate classification accuracy independent of the segmentation performance. For the two benign cases where the lesion was not detected, manual segmentation was used instead of the automatic segmentation, further ensuring that our models were not overfitting to the segmentation process.\n\nWe also implemented a post-processing step to remove false-positive blobs by computing connected-components from the segmentations. Only blobs that partially overlapped with the manual annotation were selected, mimicking the manual selection of a suspicious region that a radiologist would investigate further. This step helped in refining the segmentation results and reducing the likelihood of overfitting to noise or artifacts in the data.\n\nFurthermore, all experiments were repeated 20 times, and averages for AUC and sensitivity/specificity were reported. This repetition helped in assessing the stability and generalizability of our models, providing a more reliable estimate of their performance.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters are not explicitly detailed in the publication. However, the datasets used and/or analyzed during the study are available from the corresponding author upon reasonable request. This suggests that while the specific configurations and parameters might not be directly provided in the paper, they could potentially be obtained by contacting the corresponding author. The publication also mentions that the study was supported by various funding sources, which might imply that the methodologies and configurations used are robust and could be shared under certain conditions. The availability of the datasets indicates a willingness to share resources, which could extend to the optimization details upon request.",
  "model/interpretability": "The model employed in this study is not a black box but rather a transparent one, as it utilizes a Random Forest (RF) classifier, which is inherently interpretable. The RF model's interpretability is enhanced through the evaluation of feature contributions using two measures: Gini Importance (GI) and minimum-redundancy-maximum-relevance (mRMR).\n\nGini Importance measures the average amount of information gain using the Gini index splitting criterion during RF training. This allows for the ranking of each feature's contribution as part of a multivariate pattern. If features are redundant but informative, they are all ranked highly. On the other hand, mRMR provides a ranking based on the relevance and redundancy of the features, ensuring that the selected features are not only informative but also diverse.\n\nThe contribution of features collected across multiparametric imaging (mpI) data was evaluated, and their ranking was used to assess the contribution of each individual feature in a multimodal, multiparametric setup. This process involved successively increasing the number of features for training and validation, beginning with the top-ranked feature, and measuring the performance of each model. This approach allowed for a clear understanding of which features were most influential in the classification task.\n\nAdditionally, the benefits of multiparametric and multimodal features were evaluated by training models using only dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) features and combined DCE-MRI, diffusion-weighted imaging (DWI), and/or 18F-fluorodeoxyglucose positron emission tomography (18F-FDG PET) features. This comparative analysis further illuminated the transparency of the model by showcasing the impact of different feature sets on classification performance.",
  "model/output": "The model employed in our study is primarily a classification model. It is designed to perform both lesion segmentation and lesion classification tasks. For lesion segmentation, the model assigns a binary label to each voxel, indicating whether it belongs to a lesion (label 1) or not (label 0). This is achieved through a voxel-wise classification process using a random forest (RF) classifier. The model is trained on features extracted from imaging data, and it predicts segmentation labels for new patients who were not part of the training dataset.\n\nIn the lesion classification phase, the model further categorizes the segmented lesions as either benign or malignant. This classification is based on various features extracted per lesion, including intensity-based, kinetic, morphological, and textural features from different imaging modalities such as DCE-MRI, DWI, and 18F-FDG PET. The model's performance is evaluated using metrics such as the Dice similarity coefficient (DSC) for segmentation accuracy and area under the curve (AUC) for classification performance. The experiments were repeated multiple times to ensure the reliability of the results, and the averages for AUC and sensitivity/specificity are reported.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the lesion segmentation and classification methods was conducted using a leave-one-out cross-validation (LOOCV) approach. This involved training the segmentation algorithm and feature rankings on all but one example, and then applying the trained model to the remaining example not included in the training. This process was repeated for each example in the dataset.\n\nFor lesion segmentation, the quality was assessed on a pixel level by comparing the predicted segmentation with manually annotated data. The Dice similarity coefficient (DSC) was used as a similarity measure, and sensitivity (true-positive rate) was used to describe the probability of detection. The random forest (RF) threshold was determined as the one that maximized the DSC on the training set. The overall performance was obtained by computing the mean of all test DSC scores.\n\nIn the classification of lesions into benign and malignant categories, evaluation was also performed in an LOOCV fashion. This included both ranking the features and determining accuracy. Accuracy was reported as the area under the receiver operating characteristic curve (AUC) and sensitivity/specificity. The RF threshold was chosen within the training set as the one that maximized the F1 score, which is the harmonic mean of precision and sensitivity. All experiments were repeated 20 times, and averages for AUC and sensitivity/specificity were reported.\n\nTo study the impact of segmentation accuracy on classification, experiments were conducted on both manually delineated lesions and automatically segmented lesions. A post-processing step was included to remove false-positive blobs by computing connected-components from the segmentations using a six-neighbourhood. Only blobs that partially overlapped with the manual annotation were selected, mimicking the manual selection of a suspicious region that a radiologist would investigate further. For the two benign cases where the lesion was not detected, manual segmentation was used instead of the automatic segmentation. This post-processing step allowed for the evaluation of classification accuracy independent of the segmentation performance.",
  "evaluation/measure": "In our study, we reported several performance metrics to evaluate the effectiveness of our methods. For lesion detection, we used the Receiver Operating Characteristic - Area Under the Curve (ROC-AUC), which is a common metric in the literature. Our results showed high ROC-AUC values, indicating strong performance. Additionally, we reported the detection rate and sensitivity at a specific number of false-positive cases per breast, providing a practical measure of how well our system performs in a clinical setting.\n\nFor segmentation, we used the Dice Similarity Coefficient (DSC) and sensitivity. The DSC measures the overlap between the automated segmentation and the manual annotation, with higher values indicating better performance. We also reported the sensitivity of our segmentation method, which is the ability to correctly identify true positive voxels.\n\nTo evaluate the contribution of individual features, we used two measures: Random Forest Gini Importance (GI) and minimum-redundancy-maximum-relevance (mRMR). These measures allowed us to rank features based on their importance and relevance, providing insights into which features contribute most to our models' performance.\n\nOur set of metrics is representative of the literature, as we used established measures such as ROC-AUC, DSC, and sensitivity. Furthermore, by evaluating feature contribution, we provided an in-depth understanding of our models, which is essential for clinical exploitation and hypothesis generation.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, we focused on evaluating the performance of our own models using different feature selection techniques and imaging modalities. We compared the performance of models using Gini Importance (GI) and minimum-redundancy-maximum-relevance (mRMR) feature selection methods. Additionally, we assessed the impact of including or excluding specific imaging modalities, such as diffusion-weighted imaging (DWI) and positron emission tomography (PET), on the segmentation and classification performance.\n\nWe also evaluated the performance of our models using both manually delineated lesions and automatically segmented lesions. This allowed us to study the impact of segmentation accuracy on classification performance. For the automatic segmentation, we included a post-processing step to remove false-positive blobs, mimicking the manual selection process of a radiologist.\n\nFurthermore, we compared the performance of our models using an increasing number of top-ranked features, which provided insights into the contribution of individual features in a multimodal, multiparametric setup. This approach helped us understand the importance of different features and their combinations in improving the segmentation and classification performance.\n\nNot applicable.",
  "evaluation/confidence": "The evaluation of our method includes performance metrics with associated confidence intervals. For instance, the Dice similarity coefficient (DSC) and sensitivity are presented with mean and standard deviation, as well as median and interquartile range. This provides a comprehensive view of the performance variability and confidence in the results.\n\nStatistical significance is addressed through repeated experiments and the use of robust feature selection methods. All experiments were repeated 20 times, and averages for AUC and sensitivity/specificity are reported. This repetition helps to ensure that the results are reliable and not due to random chance.\n\nThe feature selection process, which includes Gini importance and minimum-redundancy-maximum-relevance (mRMR), helps in identifying the most contributory features. The successive increase in the number of features for training and validation allows us to assess the contribution of each individual feature in a multimodal, multiparametric setup. This methodical approach enhances the confidence in the feature selection and the overall performance of the model.\n\nAdditionally, the post-processing step, where false-positive blobs are removed, mimics the manual selection process of a radiologist. This step ensures that the classification accuracy is evaluated independently of the segmentation performance, further bolstering the confidence in the results.\n\nThe use of multiparametric and multimodal features, including DCE-MRI, DWI, and 18F-FDG PET, provides a more robust evaluation compared to using a single modality. This comprehensive approach increases the reliability and statistical significance of the findings, making a strong case for the superiority of the method.",
  "evaluation/availability": "The datasets used and/or analyzed during the current study are available from the corresponding author on reasonable request. This means that the raw evaluation files are not publicly released but can be obtained by contacting the corresponding author. The specifics of the license or terms of use for these datasets would need to be discussed directly with the corresponding author."
}