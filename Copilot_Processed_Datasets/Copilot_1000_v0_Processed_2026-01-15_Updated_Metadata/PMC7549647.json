{
  "publication/title": "2lpiRNApred: a two-layered integrated algorithm for identifying piRNAs and their functions based on LFE-GM feature selection.",
  "publication/authors": "Zuo Y, Zuo Y, Zou Q, Lin J, Jiang M, Liu X",
  "publication/journal": "RNA biology",
  "publication/year": "2020",
  "publication/pmid": "32138598",
  "publication/pmcid": "PMC7549647",
  "publication/doi": "10.1080/15476286.2020.1734382",
  "publication/tags": "- piRNAs\n- target mRNA deadenylation\n- feature selection\n- feature extraction strategies\n- a two-layered integrated classifier algorithm\n- LFE-GM\n- SRC\n- SVMMDRBF\n- piRNA prediction\n- machine learning",
  "dataset/provenance": "The datasets utilized in this study were constructed from NONCODE v3.0 and piRBase. From piRBase, sequences from mouse annotated as being piRNAs having the function of inducing target mRNA deadenylation (CPos+) and those without this function (CPos\u2212) were collected. Additionally, from NONCODE v3.0, non-piRNA sequences from mouse (CNeg) were downloaded. The CD-HIT program with a threshold of 0.8 was utilized to remove redundant samples, resulting in 798 CPos+, 1257 CPos\u2212, and 2068 CNeg sequences.\n\nThe benchmark dataset was divided into two subsets: piRNA_tr for training and piRNA_te for testing. The training dataset, piRNA_tr, contained 709 CPos+, 709 CPos\u2212, and 1418 CNeg. The independent testing dataset, piRNA_te, included 109 CPos+ (90 mouse annotated CPos+ and 19 D.melanogaster annotated CPos+), 548 CPos\u2212, and 650 CNeg. The D.melanogaster annotated CPos+ samples were also collected from piRBase and processed with the CD-HIT program to remove redundancies.",
  "dataset/splits": "The dataset was divided into two main subsets: a training dataset and an independent testing dataset. The training dataset, referred to as piRNA_tr, contained 709 sequences annotated as piRNAs with the function of inducing target mRNA deadenylation (CPos+), 709 sequences annotated as piRNAs without this function (CPos-), and 1418 non-piRNA sequences (CNeg). The independent testing dataset, referred to as piRNA_te, included the remaining sequences that were not in the training dataset. Specifically, piRNA_te contained 109 CPos+ sequences (90 mouse annotated and 19 D.melanogaster annotated), 548 CPos- sequences, and 650 CNeg sequences. The independent testing dataset was further augmented with D.melanogaster annotated CPos+ sequences to ensure a sufficient number of samples for testing. The CD-HIT program with a threshold of 0.8 was used to remove redundant samples, ensuring the diversity and representativeness of the datasets.",
  "dataset/redundancy": "The datasets utilized in this study were constructed from NONCODE v3.0 and piRBase. Sequences from mouse annotated as being piRNAs with the function of inducing target mRNA deadenylation (CPos+) and those without this function (CPos\u2212) were collected from piRBase. Additionally, non-piRNA sequences from mouse (CNeg) were downloaded from NONCODE v3.0. The CD-HIT program with a threshold of 0.8 was used to remove redundant samples, resulting in 798 CPos+, 1257 CPos\u2212, and 2068 CNeg sequences.\n\nThe benchmark dataset was divided into two subsets: piRNA_tr for training and piRNA_te for testing. The training dataset, piRNA_tr, contained 709 CPos+, 709 CPos\u2212, and 1418 CNeg sequences. The independent testing dataset, piRNA_te, included the remaining sequences: 90 CPos+, 548 CPos\u2212, and 650 CNeg. To ensure the independence of the testing dataset, none of the sequences in piRNA_te were present in piRNA_tr.\n\nGiven the limited number of CPos+ sequences in the testing dataset, additional sequences from D.melanogaster annotated as being piRNAs with the function of inducing target mRNA deadenylation (CPos+) were collected from piRBase. The CD-HIT program with a threshold of 0.8 was used to remove redundant samples, resulting in 19 additional CPos+ sequences. Therefore, the final independent testing dataset, piRNA_te, included 109 CPos+ (90 mouse annotated CPos+ and 19 D.melanogaster annotated CPos+), 548 CPos\u2212, and 650 CNeg sequences.\n\nThe distribution of the datasets in this study is designed to ensure a comprehensive evaluation of the predictive model. The training dataset is balanced with equal numbers of CPos+ and CPos\u2212 sequences, while the testing dataset includes a diverse set of sequences to assess the model's performance on independent data. This approach aligns with previously published machine learning datasets that emphasize the importance of independent testing sets to validate the generalizability of the model.",
  "dataset/availability": "The datasets utilized in this study were constructed from NONCODE v3.0 and piRBase. Specifically, sequences from mouse annotated as being piRNAs with the function of inducing target mRNA deadenylation (CPos+) and those without this function (CPos\u2212) were collected from piRBase. Additionally, non-piRNA sequences from mouse (CNeg) were downloaded from NONCODE v3.0. The CD-HIT program with a threshold of 0.8 was used to remove redundant samples, resulting in 798 CPos+, 1257 CPos\u2212, and 2068 CNeg sequences.\n\nThe benchmark dataset was divided into two subsets: piRNA_tr for training and piRNA_te for testing. The training dataset contained 709 CPos+, 709 CPos\u2212, and 1418 CNeg sequences, while the testing dataset included 90 CPos+, 548 CPos\u2212, and 650 CNeg sequences. To enhance the testing dataset, all sequences from D.melanogaster annotated as being piRNAs with the function of inducing target mRNA deadenylation (CPos+) from piRBase were also collected and processed with the CD-HIT program, resulting in an additional 19 D.melanogaster annotated CPos+ sequences.\n\nThe datasets and the predictor tool are publicly available. The predictor tool can be accessed at http://lab.malab.cn/soft/pirna/ and the source code is available on GitHub at https://github.com/JianyuanLin/2lpiRNApred. The availability of these resources ensures that other researchers can replicate and build upon the findings presented in this study.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages a combination of well-established machine-learning algorithms, specifically Support Vector Classifier with Minimum Distance to the Rbf kernel (SVMMDRBF) and Sparse Representation-based Classifier (SRC). These algorithms are not new but have been integrated in a novel way to enhance the prediction accuracy of piRNAs and their functional types.\n\nThe integration of SRC and SVMMDRBF classifier algorithms was specifically tailored to address the challenges in identifying piRNAs and determining their functional roles. This integration is not a novel machine-learning algorithm per se, but rather a innovative application of existing algorithms to a specific biological problem. The focus of our study is on the biological significance and the improvement in prediction accuracy rather than the development of a new machine-learning algorithm.\n\nThe reason this work was not published in a machine-learning journal is that the primary contribution lies in the biological application and the improvement in the prediction of piRNAs and their functions. The integration of these algorithms is a means to an end, with the end being the biological insights and improvements in prediction accuracy. The biological context and the specific problem being addressed are the main drivers of this research, making it more suitable for publication in a bioinformatics or computational biology journal.",
  "optimization/meta": "The model described is indeed a meta-predictor, leveraging the outputs of multiple machine-learning algorithms to enhance predictive performance. Specifically, it integrates the results from Support Vector Machine with Radial Basis Function (SVMMRBF) and Stochastic Gradient Descent (SRC) classifiers.\n\nIn the first layer, the optimal integration model combines SVMMRBF and SRC classifiers to identify whether an RNA sequence is a piRNA. The best-performing combination in this layer is SVMMRBF3+SRC7+SRC8, which was selected based on the Matthews correlation coefficient (Mcc) and other performance metrics. This layer uses a feature combination of PC+Kmer.\n\nThe second layer further refines the prediction by determining whether the identified piRNA sequences function in inducing target mRNA deadenylation. This layer employs an SRC integration model, specifically SRC2(72)+SRC13(106)+SRC14(104)+SRC15(80)+SRC16(90), using a more comprehensive feature combination of PC+Kmer+GAC+NMBAC+SC.\n\nA majority voting strategy is used to construct the ensemble models in both layers, ensuring that the final predictions are robust and reliable. This approach helps to mitigate the risk of overfitting and improves the generalizability of the model.\n\nRegarding the independence of the training data, the model utilizes five-fold cross-validation, which is a standard technique to ensure that the training data is independent across different folds. This method helps to validate the model's performance and generalization capabilities by training and testing on different subsets of the data.",
  "optimization/encoding": "In our study, we employed five distinct feature extraction strategies to encode RNA sequences into numerical vectors, which are essential for training machine learning models. These strategies included Kmer, General parallel correlation pseudo-dinucleotide composition (PC), General series correlation pseudo-dinucleotide composition (SC), Normalized Moreau\u2013Broto autocorrelation (NMBAC), and Geary autocorrelation (GAC). Each of these methods converts the variable-length RNA sequences into fixed-length vectors, capturing different aspects of the sequences' properties.\n\nThe Kmer approach, which has been widely used in bioinformatics, involves counting the occurrence frequencies of specific nucleotide strings (k-mers) of lengths 1 to 3. This results in a vector of 84 dimensions, representing the frequencies of all possible 1-mers, 2-mers, and 3-mers.\n\nThe PC, SC, NMBAC, and GAC methods utilize six RNA physicochemical indices: rise, roll, shift, slide, tilt, and twist. These indices provide a more nuanced representation of the RNA sequences by considering the correlations and autocorrelations of these properties along the sequence.\n\nTo preprocess the data, we collected piRNA and non-piRNA sequences from NONCODE v3.0 and piRBase. We removed redundant samples using the CD-HIT program with a threshold of 0.8. The benchmark dataset was divided into training and testing subsets. The training dataset, piRNA_tr, contained balanced numbers of positive and negative samples, while the testing dataset, piRNA_te, included an independent set of sequences, ensuring no overlap with the training data.\n\nThis comprehensive encoding and preprocessing pipeline ensures that the input data for our machine learning models is both informative and robust, facilitating accurate prediction of piRNAs and their functional types.",
  "optimization/parameters": "In the optimization process of our model, two uncertain parameters, \u03bb and \u03c9, were considered for the PC encoding strategy. The range for \u03c9 was set between 0.1 and 0.9 with increments of 0.1, and for \u03bb, the range was from 1 to 9. This resulted in a total of 81 individual prediction models, as each combination of \u03bb and \u03c9 was evaluated.\n\nTo select the optimal parameters, a five-fold cross-validation strategy was employed, and the Matthews correlation coefficient (Mcc) was used as the evaluation measure. The Mcc reflects the correlation between the predicted and observed results, with higher values indicating better model performance. Models with Mcc values above certain thresholds (0.6850 for SVMMRBF and 0.6840 for SRC) were retained for further evaluation.\n\nIn the first layer, three SVMMRBF models and ten SRC models met these criteria. Among all combinations of these models, the optimal integrated model was identified as SVMMRBF1 + SVMMRBF2 + SVMMRBF3 + SRC7 + SRC8, achieving an Mcc of 0.6978. Further refinement using a hybrid feature extraction strategy and the LFE-GM feature selection algorithm led to the final optimal model, SVMMRBF3(60) + SRC7(88) + SRC8(86), with an Mcc of 0.7759.\n\nIn the second layer, the optimal SRC integration model was constructed using the feature combination PC + Kmer + GAC + NMBAC + SC. The selected model, SRC2(72) + SRC13(106) + SRC14(104) + SRC15(80) + SRC16(90), demonstrated improved predictive performance compared to models without LFE-GM feature selection.",
  "optimization/features": "In the optimization process of our model, we initially extracted a large number of features from the RNA sequences. The exact number of features (f) varied depending on the combination of feature extraction strategies used. For instance, in the first layer, we employed five feature extraction strategies: PC, Kmer, GAC, NMBAC, and SC. The specific combination of PC+Kmer was found to be optimal for the first layer, while PC+Kmer+GAC+NMBAC+SC was used in the second layer.\n\nFeature selection was indeed performed to reduce dimensionality and improve predictive performance. We developed a novel feature selection algorithm called LFE-GM, which stands for feature selection based on Luca fuzzy entropy and Gaussian membership function. This algorithm was applied to rearrange and select the most relevant features for our classifiers.\n\nThe feature selection process was conducted using only the training set to ensure that the model's performance on unseen data was not compromised. This approach helps in preventing overfitting and ensures that the selected features are generalizable to new data. The LFE-GM algorithm calculates the mean and standard deviation of each feature for both positive and negative training sequences, and then uses these statistics to evaluate the importance of each feature. Features were then rearranged based on their importance scores, and the top features were selected for the final model.\n\nFor example, in the first layer, the optimal model SVMMRBF3(60)+SRC7(88)+SRC8(86) was trained using the top 60 features for SVMMRBF3, the top 88 features for SRC7, and the top 86 features for SRC8. Similarly, in the second layer, the optimal model SRC2(72)+SRC13(106)+SRC14(104)+SRC15(80)+SRC16(90) used the top features for each sub-classifier as indicated.\n\nThe effectiveness of the LFE-GM feature selection algorithm was validated through five-fold cross-validation, demonstrating improved predictive performance compared to models without feature selection. This confirms that the feature selection process was crucial in enhancing the model's accuracy and reliability.",
  "optimization/fitting": "The fitting method employed in this study involved a comprehensive approach to ensure both over-fitting and under-fitting were adequately addressed.\n\nInitially, five feature extraction strategies were utilized to encode each RNA sequence, and five-fold cross-validation was employed to train the prediction models. This cross-validation technique helps in assessing the model's performance on different subsets of the data, thereby reducing the risk of over-fitting. The Matthews correlation coefficient (Mcc) was used as the evaluation measure, which effectively reflects the correlation between predicted and observed results. A higher Mcc indicates better model performance.\n\nTo further mitigate over-fitting, a feature selection algorithm based on Luca fuzzy entropy and Gaussian membership function (LFE-GM) was proposed. This algorithm rearranges features according to their Luca fuzzy entropy values, removing those with the highest entropy. This step ensures that only the most relevant features are retained, reducing the dimensionality of the data and preventing the model from becoming too complex.\n\nThe optimal integration model in the first layer was constructed using the feature combination PC+Kmer. Additionally, the same procedure was applied to construct the SRC integration model in the second layer using the feature combination PC+Kmer+GAC+NMBAC+SC. The majority voting strategy was used to integrate the predictive results of SRC and SVMMRBF, further enhancing the model's robustness.\n\nThe effectiveness of the LFE-GM feature selection algorithm was verified by comparing the predictive performance of models before and after feature selection. The results showed significant improvements in specificity (Sp), accuracy (Acc), and Mcc values, indicating that the feature selection process successfully reduced over-fitting.\n\nTo address under-fitting, the models were trained using a hybrid feature extraction strategy that combined PC with other encoding strategies such as Kmer, GAC, NMBAC, and SC. This approach ensured that the models had access to a diverse set of features, capturing various aspects of the RNA sequences. The top models with Mcc values greater than 0.7000 were combined, and the combination of SVMMRBF3+SRC7+SRC8 for the PC+Kmer encoding strategy achieved the best predictive performance.\n\nIn summary, the fitting method involved a rigorous process of feature extraction, cross-validation, and feature selection to balance the complexity of the model and ensure it generalizes well to unseen data. This approach effectively ruled out both over-fitting and under-fitting, leading to robust and accurate prediction models.",
  "optimization/regularization": "In the optimization process of our model, several techniques were employed to prevent overfitting. One of the key strategies involved the use of cross-validation, specifically five-fold cross-validation. This method helps to ensure that the model generalizes well to unseen data by training and validating on different subsets of the data.\n\nAdditionally, feature selection played a crucial role in mitigating overfitting. The LFE-GM feature selection algorithm was utilized to identify and retain the most relevant features for each sub-classifier. This process not only improved the predictive performance but also reduced the complexity of the model, making it less prone to overfitting.\n\nFurthermore, the integration of multiple models through a majority voting strategy in both the first and second layers contributed to the robustness of the final predictor. By combining the predictions of several models, the ensemble approach helps to average out the errors and biases of individual models, thereby enhancing the overall performance and generalization capability.\n\nThe use of Matthews correlation coefficient (Mcc) as an evaluation measure also aided in selecting features and models that had a strong correlation between predicted and observed results, further ensuring the model's reliability and reducing the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the publication. Specifically, the ranges considered for the parameters \u03bb and \u03c9 in the PC encoding strategy are explicitly mentioned. For \u03bb, the values ranged from 1 to 9, and for \u03c9, the values ranged from 0.1 to 0.9. These parameters were systematically evaluated to determine their impact on the performance of the SRC and SVMMRBF prediction models.\n\nThe optimization process involved five-fold cross-validation, and the Matthews correlation coefficient (Mcc) was used as the evaluation measure to select the best-performing models. The specific combinations of models that yielded the highest Mcc values are also reported, such as the optimal integrated model (SVMMRBF1+ SVMMRBF2+ SVMMRBF3+ SRC7+ SRC8) with an Mcc of 0.6978.\n\nAdditionally, the feature selection algorithm LFE-GM was employed to further improve the predictive performance of the sub-classifiers. The optimal number of features for each sub-classifier was determined through this algorithm, and the corresponding improvements in performance metrics are provided.\n\nThe detailed results, including the specific configurations and performance metrics, are available in the supplementary material, which includes tables listing the actual results for various models and feature combinations. These tables provide a comprehensive overview of the optimization process and the configurations that led to the best predictive performance.\n\nThe publication and its supplementary materials are available under standard academic publishing licenses, which typically allow for the reproduction and use of the reported methods and results for non-commercial purposes. This ensures that other researchers can replicate and build upon the work presented.",
  "model/interpretability": "The model presented in this publication is not a black-box model. It employs a combination of feature extraction strategies and ensemble learning techniques that contribute to its interpretability. The process of constructing the optimal integration model involves several transparent steps, making it possible to understand how predictions are made.\n\nInitially, five feature extraction strategies were used to encode RNA sequences. These strategies include PC, Kmer, GAC, NMBAC, and SC. The PC encoding had two uncertain parameters, \u03bb and \u03c9, which were systematically varied to create multiple models. This approach allows for a clear understanding of how different parameter settings affect the model's performance.\n\nThe performance of each model was evaluated using the Matthews correlation coefficient (Mcc), which measures the correlation between predicted and observed results. This metric provides a transparent way to compare the effectiveness of different models and feature combinations.\n\nIn the first layer, the optimal integration model was built using the feature combination PC+Kmer. This combination was selected based on its superior performance in identifying piRNA sequences. The model's performance was further improved by combining the top-performing sub-classifiers, specifically SVMMRBF3, SRC7, and SRC8. The LFE-GM feature selection algorithm was used to rearrange and select the most informative features, enhancing the model's predictive accuracy.\n\nThe second layer of the model utilized a majority voting strategy to integrate the predictive results of multiple SRC classifiers. The feature combination PC+Kmer+GAC+NMBAC+SC was used in this layer to predict whether identified piRNA sequences function in inducing target mRNA deadenylation. The detailed results of this process are available in the supplementary material, providing transparency into the model's construction and evaluation.\n\nThe use of majority voting and the combination of multiple feature extraction strategies make the model's decision-making process more interpretable. Each sub-classifier contributes to the final prediction, and the LFE-GM feature selection algorithm ensures that the most relevant features are used. This approach allows researchers to understand how different features and classifiers interact to produce the final prediction.\n\nIn summary, the model is designed to be transparent, with clear steps for feature extraction, model evaluation, and integration. The use of interpretable metrics like Mcc and the combination of multiple feature extraction strategies contribute to the model's transparency, making it a valuable tool for identifying piRNA sequences and their functional types.",
  "model/output": "The model is a classification model. It is designed to solve a classification problem with three classes: piRNA involved in deadenylation, piRNA not involved in deadenylation, and not a piRNA. The model uses binary classifiers to achieve this. The primary output of the model is the prediction of whether a given RNA sequence is a piRNA and, if so, whether it functions in inducing target mRNA deadenylation.\n\nThe model employs a two-layered integrated classifier algorithm, utilizing SRC and SVMMDRBF classifier algorithms along with five feature extraction strategies: Kmer, PC, SC, NMBAC, and GAC. The first layer of the model identifies piRNAs, while the second layer determines whether these piRNAs function in inducing target mRNA deadenylation.\n\nThe performance of the model is evaluated using metrics such as sensitivity (Sn), specificity (Sp), accuracy (Acc), and the Matthews correlation coefficient (Mcc). The model has shown significant improvements in these metrics compared to other existing methods, indicating its effectiveness in both identifying piRNAs and determining their functional role in deadenylation.\n\nThe LFE-GM feature selection algorithm is integral to the model's performance, as it reduces the dimensionality of the features while maintaining high efficiency and accuracy. This feature selection strategy has been proven to enhance the overall predictive performance of the integrated algorithm.\n\nIn summary, the model is a robust classification tool that leverages advanced feature extraction and selection techniques to accurately identify piRNAs and predict their functional involvement in mRNA deadenylation.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the 2lpiRNApred model is publicly available on GitHub. Additionally, a web server has been set up to facilitate the use of the model. The web server can be accessed at http://lab.malab.cn/soft/pirna/. The GitHub repository, which contains the source code, can be found at https://github.com/JianyuanLin/2lpiRNApred. The software is released under a license that allows for its use and modification, promoting open access and collaboration within the scientific community.",
  "evaluation/method": "The evaluation of the proposed method, 2lpiRNApred, involved several rigorous steps to ensure its performance and robustness. Initially, a five-fold cross-validation was conducted on the training dataset to compare 2lpiRNApred with existing tools like 2 L-piRNA. This process was repeated for both the first and second layers of prediction, demonstrating the method's effectiveness in identifying piRNAs and determining their role in inducing target mRNA deadenylation.\n\nAdditionally, an independent test dataset was used to further validate the performance of 2lpiRNApred. This dataset included comparisons with other tools such as piRNN and 2 L-piRNA, showcasing 2lpiRNApred's superior accuracy and Matthews correlation coefficient (Mcc) values. The independent test results highlighted 2lpiRNApred's remarkable improvement in both identifying piRNAs and determining their functional role in mRNA deadenylation.\n\nTo assess the impact of dataset similarity on prediction results, the CD-HIT program was employed to set thresholds ranging from 0.8 to 0.5. The performance metrics, including sensitivity (Sn), specificity (Sp), accuracy (Acc), and MCC, were evaluated at these different thresholds. The results indicated that the performance of 2lpiRNApred was not significantly affected by the similarity of the training samples within this range, demonstrating its robustness.\n\nFurthermore, the method was compared with several other programs using 5/10-fold cross-validation and an independent test. 2lpiRNApred consistently outperformed these tools, achieving higher accuracy and sensitivity in piRNA prediction across various benchmark datasets, including human, mouse, and Drosophila datasets. The comprehensive evaluation underscored the effectiveness and reliability of 2lpiRNApred in piRNA identification and functional prediction.",
  "evaluation/measure": "In our evaluation of the 2lpiRNApred model, we utilized several performance metrics to comprehensively assess its effectiveness. The primary metrics reported include Sensitivity (Sn), Specificity (Sp), Accuracy (Acc), and Matthews Correlation Coefficient (Mcc). These metrics are widely recognized in the literature for evaluating the performance of predictive models, particularly in the context of bioinformatics and machine learning.\n\nSensitivity, or the true positive rate, measures the proportion of actual positives that are correctly identified by the model. Specificity, or the true negative rate, measures the proportion of actual negatives that are correctly identified. Accuracy provides an overall measure of the model's correctness by calculating the proportion of true results (both true positives and true negatives) among the total number of cases examined. The Matthews Correlation Coefficient is a balanced measure that takes into account true and false positives and negatives, providing a value between -1 and 1, where 1 indicates perfect prediction, 0 indicates no better than random prediction, and -1 indicates total disagreement between prediction and observation.\n\nThese metrics are representative of the standards used in the field, ensuring that our evaluation is both rigorous and comparable to other studies. By reporting these metrics, we aim to provide a clear and comprehensive understanding of the model's performance, highlighting its strengths and areas for potential improvement. This approach aligns with best practices in the literature, ensuring that our findings are both reliable and reproducible.",
  "evaluation/comparison": "In the evaluation of our proposed model, 2lpiRNApred, we conducted a thorough comparison with several publicly available methods and simpler baselines to assess its performance. We utilized benchmark datasets and employed 5/10-fold cross-validation to ensure a rigorous evaluation.\n\nFor the first layer of prediction, which focuses on identifying piRNAs, we compared 2lpiRNApred with six other programs: GA-WE, 2 L-piRNA, Piano, k-mer, and accurate piRNA prediction. The results, presented in Table 5, demonstrate that 2lpiRNApred generally led to improvements ranging from 1.29% to 65.15% in accurate piRNA prediction compared to these methods across various datasets. Specifically, in the GA-WE Drosophila balanced benchmark dataset, while the specificity (Sp) obtained by 2lpiRNApred was slightly lower than that of k-mer, accurate piRNA prediction, and GA-WE, the sensitivity (Sn) was notably higher by approximately 1.8% to 4.3%.\n\nAdditionally, we compared 2lpiRNApred with piRNN and 2 L-piRNA on an independent test dataset. The results, shown in Table 7, indicate that 2lpiRNApred significantly outperformed both methods. For instance, the Matthews correlation coefficient (Mcc) for 2lpiRNApred was about 0.4875 and 0.2908 higher than those of piRNN and 2 L-piRNA, respectively, when identifying piRNAs in the first layer. This comparison underscores the superior performance of 2lpiRNApred in piRNA identification.\n\nFor the second layer of prediction, which determines whether piRNAs function in inducing target mRNA deadenylation, we compared 2lpiRNApred with 2 L-piRNA. The results, presented in Table 6 and Table 7, show that 2lpiRNApred achieved a substantially higher accuracy (Acc) by approximately 56.47% compared to 2 L-piRNA. This highlights the effectiveness of 2lpiRNApred in both identifying piRNAs and determining their functional role in mRNA deadenylation.\n\nIn summary, the comprehensive evaluation through benchmark datasets and cross-validation demonstrates that 2lpiRNApred not only outperforms existing methods in piRNA identification but also excels in determining the functional role of piRNAs in mRNA deadenylation.",
  "evaluation/confidence": "Evaluation Confidence\n\nThe evaluation of the proposed predictor, 2lpiRNApred, was conducted using several performance metrics, including Sensitivity (Sn), Specificity (Sp), Accuracy (Acc), and Matthews correlation coefficient (Mcc). These metrics were calculated across various datasets and validation methods, such as five-fold and ten-fold cross-validation.\n\nThe results demonstrate that 2lpiRNApred consistently outperforms other methods, including 2 L-piRNA and piRNN, across multiple datasets. For instance, on the independent test dataset, 2lpiRNApred achieved an accuracy of 87.38% and an Mcc of 0.7507 in the first layer, significantly higher than the 72.07% accuracy and 0.4599 Mcc of 2 L-piRNA. In the second layer, 2lpiRNApred maintained superior performance with an accuracy of 77.02% and an Mcc of 0.4074, compared to 2 L-piRNA's 20.55% accuracy and -0.5015 Mcc.\n\nThe statistical significance of these results is evident in the consistent improvement across multiple metrics and datasets. The proposed LFE-GM feature selection strategy played a crucial role in enhancing the predictive performance, as seen in the optimal integration models. The use of majority voting strategies in the ensemble learning algorithm further contributed to the robustness and accuracy of 2lpiRNApred.\n\nHowever, specific confidence intervals for the performance metrics were not explicitly provided in the results. While the superior performance of 2lpiRNApred is clear, the lack of confidence intervals means that the precise statistical significance and variability of the results are not fully quantified. This information would be beneficial for a more comprehensive evaluation of the method's reliability and generalizability.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being publicly available. The study focuses on the performance of the 2lpiRNApred model, comparing it with other methods using various metrics such as sensitivity, specificity, accuracy, and the Matthews correlation coefficient. The results are presented in tables within the publication, but there is no indication that the raw data or evaluation files used to generate these results have been made publicly accessible. Therefore, it is not clear whether these files are available for download or further analysis by other researchers."
}