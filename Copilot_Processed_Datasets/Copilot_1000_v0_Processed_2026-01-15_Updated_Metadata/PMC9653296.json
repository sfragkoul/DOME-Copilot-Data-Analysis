{
  "publication/title": "Machine Learning Models for Predicting Postoperative Outcomes following Skull Base Meningioma Surgery.",
  "publication/authors": "Jimenez AE, Porras JL, Azad TD, Shah PP, Jackson CM, Gallia G, Bettegowda C, Weingart J, Mukherjee D",
  "publication/journal": "Journal of neurological surgery. Part B, Skull base",
  "publication/year": "2022",
  "publication/pmid": "36393884",
  "publication/pmcid": "PMC9653296",
  "publication/doi": "10.1055/a-1885-1447",
  "publication/tags": "- Machine Learning\n- Skull Base Meningiomas\n- Postoperative Outcomes\n- Predictive Modeling\n- Elastic Net Logistic Regression\n- Hospital Length of Stay\n- Discharge Disposition\n- Hospital Charges\n- Neurosurgery\n- High-Value Healthcare",
  "dataset/provenance": "The dataset used in this study was collected from a single academic medical institution over a restricted time period from 2016 to 2019. The cohort consisted of 265 patients who underwent surgery for skull base meningiomas. This dataset is unique to our study and has not been previously used in other published research or by the community. The data points include various demographic and clinical characteristics of the patients, such as age, sex, race, insurance type, marital status, admission type, tumor grade, tumor volume, tumor location, and surgical approach. Additionally, the dataset includes postoperative outcomes such as hospital length of stay, discharge disposition, and hospital charges. The data was collected using Microsoft Excel and analyzed using R statistical software.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a holdout testing set. The training set comprised 70% of the data, while the holdout testing set contained the remaining 30%. This split was used to train and evaluate the machine learning models.\n\nThe training set was further utilized for hyperparameter tuning through a process involving fivefold cross-validation repeated 10 times. This method ensured that the models were robust and generalizable by evaluating them on different subsets of the training data.\n\nThe holdout testing set, which consisted of 30% of the data, was used to assess the final predictive performance of the models. This approach helped in validating the models' ability to generalize to unseen data, providing a reliable measure of their performance.",
  "dataset/redundancy": "The datasets were split into training and independent holdout testing subsets based on a 70/30 ratio, respectively. This means that 70% of the data was used for training the machine learning models, while the remaining 30% was reserved for testing the models' performance on unseen data. The training and test sets were independent, ensuring that the models were evaluated on data they had not encountered during training. This independence was enforced by the split ratio and the random assignment of data points to either the training or testing set.\n\nThe distribution of the datasets was designed to ensure that the models could generalize well to new, unseen data. This approach is consistent with best practices in machine learning, where the goal is to develop models that perform well not just on the training data but also on independent test data. The use of a holdout set helps to provide an unbiased evaluation of the model's performance, which is crucial for assessing its real-world applicability.\n\nThe specific details of the data distribution, such as the number of samples and the balance between different classes, were not explicitly mentioned. However, the use of a 70/30 split is a common practice that helps to balance the need for sufficient training data with the need for a robust evaluation of the model's performance. This approach is in line with previously published machine learning datasets, where similar split ratios are often used to ensure that models are trained and tested on independent data.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is elastic net logistic regression. This is a well-established technique in the field of machine learning and statistics, known for its ability to handle high-dimensional data and perform variable selection.\n\nThe elastic net logistic regression algorithm is not new. It has been extensively used and studied in various domains, including medical research. The reason it was not published in a machine-learning journal is that our focus was on applying this algorithm to a specific medical problem\u2014prognosticating postoperative outcomes among skull base meningioma patients. Our work contributes to the medical literature by demonstrating the applicability and performance of elastic net logistic regression in this context, rather than introducing a novel machine-learning algorithm.\n\nThe elastic net logistic regression method was chosen for its effectiveness in preventing model overfitting by applying a penalty function to regression coefficients. This allows for better predictive performance compared to ordinary least-squares regression by effectively removing model covariates that do not contribute to optimizing predictive performance. This results in more parsimonious models that perform better when evaluated on out-of-sample testing data.",
  "optimization/meta": "The models developed in this study do not use data from other machine-learning algorithms as input. Instead, they directly utilize preoperative patient demographic and clinical characteristics to predict postoperative outcomes.\n\nThe specific machine-learning method employed is elastic net logistic regression. This technique is used to prevent model overfitting by applying a penalty function to regression coefficients, which helps in creating more parsimonious models that perform better on out-of-sample testing data.\n\nThe training data for these models was separated into training and independent holdout testing subsets based on a 70/30 ratio. This ensures that the training data is independent from the testing data, which is crucial for evaluating the model's predictive performance accurately. Fivefold cross-validation repeated 10 times was used to tune model hyperparameters on the 70% training dataset, and hyperparameter optimization was conducted using a random search. Following training, the predictive abilities of the finalized models were evaluated on the 30% holdout testing dataset. This approach ensures that the models are tested on data that was not used during the training process, providing a robust assessment of their generalization capabilities.",
  "optimization/encoding": "The data was collected using Microsoft Excel and then analyzed using R statistical software. For the machine learning algorithms, elastic net logistic regression was employed. This technique involves applying a penalty function to the regression coefficients to prevent overfitting. The data was split into training and testing subsets using a 70/30 ratio. Fivefold cross-validation, repeated 10 times, was used to tune the model hyperparameters on the training dataset. Hyperparameter optimization was conducted using a random search. The predictive abilities of the finalized models were then evaluated on the holdout testing dataset. The models were compared based on their discrimination and calibration, quantified by the area under the receiver operating characteristic curve (AUROC) and the Brier score, respectively. For the AUROC and Brier score metrics, 95% confidence intervals were obtained using 2,000 bootstrapped replicates. Additionally, Spiegelhalter\u2019s Z-test was used to assess the calibration of the models.",
  "optimization/parameters": "In our study, the number of input parameters (p) used in the model was determined through the application of elastic net logistic regression, a machine learning technique that incorporates both L1 and L2 regularization. This method is particularly useful for variable selection and regularization, helping to prevent overfitting by applying a penalty function to the regression coefficients.\n\nThe elastic net regularization process effectively removes model covariates that do not contribute significantly to optimizing predictive performance. This results in a more parsimonious model that performs better on out-of-sample testing data. The specific number of parameters retained in the final model was data-driven, meaning that the selection of p was automatically determined by the algorithm based on the training data.\n\nThe training process involved a 70/30 split of the patient data into training and holdout testing subsets. Fivefold cross-validation, repeated 10 times, was used to tune the model hyperparameters on the 70% training dataset. Hyperparameter optimization was conducted using a random search, ensuring that the model's performance was robust and generalizable.\n\nThrough this process, the elastic net logistic regression algorithm identified the most relevant predictors for prolonged hospital length of stay, nonroutine discharge disposition, and high hospital charges. The final models included only the variables that contributed most significantly to the predictive performance, thereby optimizing the number of input parameters used.",
  "optimization/features": "The study utilized elastic net logistic regression models to predict postoperative outcomes for skull base meningioma patients. The input features for these models included various preoperative patient demographic and clinical characteristics. These features encompassed age, sex, race, insurance type, marital status, admission type, WHO grade, tumor volume, tumor location, ASA score, mFI-5 score, symptomatic presentation, surgeon years of experience, and surgical approach.\n\nFeature selection was implicitly performed through the use of elastic net regularization. This technique applies a penalty function to the regression coefficients, effectively removing covariates that do not contribute significantly to the model's predictive performance. This process helps in creating more parsimonious models that perform better on out-of-sample testing data. The feature selection was conducted using the training dataset only, ensuring that the model's performance on the holdout testing dataset was not influenced by data leakage.",
  "optimization/fitting": "The study employed elastic net logistic regression, a machine learning algorithm designed to handle datasets where the number of parameters can be large relative to the number of training points. This method incorporates a penalty function to the regression coefficients, which helps in preventing overfitting by effectively removing covariates that do not contribute significantly to the predictive performance. This regularization technique ensures that the model remains parsimonious and generalizes well to out-of-sample data.\n\nTo further mitigate overfitting, the data was split into training and independent holdout testing subsets in a 70/30 ratio. Hyperparameter tuning was performed using fivefold cross-validation repeated 10 times on the training dataset, followed by a random search for hyperparameter optimization. This rigorous validation process helps in ensuring that the model's performance is robust and not merely a result of overfitting to the training data.\n\nAdditionally, the models were evaluated on the holdout testing dataset to assess their discrimination and calibration. The area under the receiver operating characteristic curve (AUROC) and the Brier score were used as metrics to quantify these aspects. An AUROC of 0.70 or higher is generally considered to indicate clinically useful discrimination. The use of these metrics, along with bootstrapped confidence intervals, provides a comprehensive evaluation of the model's performance and helps in ruling out underfitting by ensuring that the models are sufficiently complex to capture the underlying patterns in the data.",
  "optimization/regularization": "In our study, we employed elastic net logistic regression as our machine learning algorithm. This method incorporates a regularization technique that helps prevent overfitting. Regularization works by adding a penalty to the regression coefficients, which effectively shrinks some of the coefficients to zero. This process removes variables that do not contribute significantly to the model's predictive performance, resulting in a more parsimonious model. By doing so, elastic net logistic regression enhances the model's ability to generalize to out-of-sample data, thereby improving its predictive performance and reducing the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available within the text of the publication. Specifically, we utilized fivefold cross-validation repeated 10 times to tune model hyperparameters on the 70% training dataset. Hyperparameter optimization was conducted using a random search. The predictive abilities of the finalized models were evaluated on a 30% holdout testing dataset. Models were compared based on their discrimination and calibration, quantified by the area under the receiver operating characteristic curve (AUROC) and by the Brier score, respectively.\n\nThe elastic net logistic regression models were trained using the Caret package in R statistical software (version 4.0.2). The elastic net regularization method was employed to prevent model overfitting by applying a penalty function to regression coefficients. This technique allows for better predictive performance compared with ordinary least-squares regression by effectively removing model covariates that do not contribute to optimizing predictive performance.\n\nThe specific model files and optimization parameters are not explicitly provided in the publication, as the focus was on the methodology and results rather than the detailed technical implementation. However, the general approach and parameters used for model training and evaluation are thoroughly described, allowing for reproducibility by researchers familiar with the tools and techniques mentioned.\n\nRegarding the availability and licensing of the configurations and schedules, the methods and results are presented in a manner consistent with academic publishing standards. The use of open-source software like R and its packages ensures that the tools are accessible to the research community. The publication itself is subject to the copyright policies of the journal, which typically allow for academic use and citation but may restrict direct reproduction or distribution without permission. For specific details on licensing and access, one would need to refer to the journal's policies or contact the publishers directly.",
  "model/interpretability": "The models developed in this study utilize elastic net logistic regression, which provides a degree of interpretability. Unlike deep neural networks, which often act as black boxes, elastic net logistic regression allows for the calculation of \u03b2-coefficients and odds ratios. This means that we can gain some understanding of how the model inputs influence the outputs. For instance, variables such as Medicare or Medicaid insurance, non-elective admission, greater tumor volume, higher ASA score, and less surgeon years of experience were associated with increased odds of prolonged hospital length of stay. Similarly, older patient age, Medicare or Medicaid insurance, greater tumor volume, higher ASA score, and higher mFI-5 score were linked to higher odds of nonroutine discharge. These associations help in interpreting the model's predictions and understanding the underlying relationships in the data. However, it is important to note that while elastic net logistic regression offers more transparency than some other machine learning methods, it still lacks the probabilistic structure of ordinary least-squares linear or logistic regression, limiting our ability to make strong inferences about statistical relationships among model inputs and outputs.",
  "model/output": "The models developed in this study are classification models. Specifically, we employed elastic net logistic regression, a machine learning algorithm that combines the penalties of Lasso and Ridge regression to perform both variable selection and regularization. This approach was used to predict three binary outcomes: prolonged hospital length of stay (LOS), nonroutine discharge disposition, and high hospital charges. The models were evaluated using metrics such as the area under the receiver operating characteristic curve (AUROC), accuracy, Brier score, and Spiegelhalter\u2019s Z-test to assess their discrimination and calibration. The AUROC values for the models predicting prolonged LOS and nonroutine discharge were above 0.70, indicating clinically useful discrimination, while the model for high hospital charges had an AUROC of 0.592, suggesting inadequate discrimination. The models demonstrated good calibration, as indicated by Spiegelhalter\u2019s Z-test p-values greater than 0.05. Additionally, the models allowed for the calculation of odds ratios for each covariate, providing some insight into the relationships between the input variables and the outcomes. However, due to the lack of an underlying probabilistic structure, the models do not support the calculation of confidence intervals and p-values, limiting the inferences that can be made about the statistical relationships among the model inputs and outputs.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the machine learning models in this study involved a rigorous process to ensure the reliability and generalizability of the results. The patient data was divided into training and independent holdout testing subsets using a 70/30 ratio. This split allowed for the training of the models on a substantial portion of the data while reserving a separate dataset for unbiased evaluation.\n\nTo optimize the hyperparameters of the models, fivefold cross-validation was employed, repeated 10 times. This technique helps in tuning the model parameters effectively by providing a robust estimate of model performance. Hyperparameter optimization was conducted using a random search, which systematically explores different combinations of hyperparameters to find the best-performing model.\n\nFollowing the training and hyperparameter optimization, the predictive abilities of the finalized models were evaluated on the 30% holdout testing dataset. This independent dataset was crucial for assessing the models' performance on unseen data, thereby providing a realistic measure of their predictive accuracy.\n\nThe models were compared based on their discrimination and calibration. Discrimination was quantified using the area under the receiver operating characteristic curve (AUROC), also known as the c-statistic. An AUROC of 0.70 or higher is generally considered to indicate clinically useful discrimination. Calibration was assessed using the Brier score, which measures the accuracy of probabilistic predictions. Additionally, 95% confidence intervals for the AUROC and Brier score metrics were obtained using 2,000 bootstrapped replicates. For accuracy, a 95% confidence interval was calculated as described by Clopper and Pearson. Spiegelhalter\u2019s Z-test was also used to assess the calibration of the models.",
  "evaluation/measure": "In our study, we evaluated the performance of our machine learning models using several key metrics to ensure a comprehensive assessment of their predictive capabilities. The primary metrics reported include the Area Under the Receiver Operating Characteristic Curve (AUROC), accuracy, Brier score, and Spiegelhalter\u2019s Z-test.\n\nThe AUROC, also known as the c-statistic, is a widely used metric in the literature to evaluate the discrimination ability of a model. An AUROC of 0.70 or higher is generally considered to indicate clinically useful discrimination. We reported the AUROC for our models predicting prolonged hospital length of stay (LOS), nonroutine discharge disposition, and high hospital charges. The AUROC values for these models were 0.798, 0.752, and 0.592, respectively, with corresponding 95% confidence intervals obtained using 2,000 bootstrapped replicates.\n\nAccuracy is another important metric that provides the proportion of true results (both true positives and true negatives) among the total number of cases examined. We reported the accuracy percentages for our models, which were 82.1% for prolonged hospital LOS, 89.9% for nonroutine discharge disposition, and 73.4% for high hospital charges. The 95% confidence intervals for accuracy were calculated using the method described by Clopper and Pearson.\n\nThe Brier score is a measure of the mean squared difference between predicted probabilities and the actual binary outcomes. It provides an assessment of the calibration of the model, with lower scores indicating better calibration. Our models had Brier scores of 0.15 for prolonged hospital LOS, 0.084 for nonroutine discharge disposition, and 0.19 for high hospital charges.\n\nSpiegelhalter\u2019s Z-test was used to further assess the calibration of our models. A p-value greater than 0.05 from this test indicates adequate calibration. Our models for prolonged hospital LOS and nonroutine discharge disposition had Spiegelhalter\u2019s Z-test p-values of 0.16 and 0.83, respectively, indicating good calibration. However, the model for high hospital charges had a p-value of 0.64, suggesting adequate calibration but with limited discrimination as indicated by the AUROC.\n\nThese metrics collectively provide a robust evaluation of our models' performance, ensuring that we can assess both their discriminative ability and calibration. The choice of these metrics aligns with established practices in the literature, providing a representative and comprehensive assessment of our models' predictive capabilities.",
  "evaluation/comparison": "A comparison to simpler baselines was performed in this study. Specifically, the machine learning (ML) methods used, particularly elastic net logistic regression, were evaluated against traditional statistical techniques. The study highlights that while ML can achieve reasonable predictive performance, it is crucial to empirically assess the performance of these algorithms. This assessment involves comparing them to linear or logistic regression to determine if they attain acceptable levels of discrimination and calibration for the outcomes of interest.\n\nThe study also mentions that the mixed results of ML predictive performance relative to traditional statistical techniques within the medical literature underscore the need for such comparisons. This approach ensures that the chosen ML methods are not assumed to be superior a priori but are instead validated through empirical performance metrics.\n\nAdditionally, the study acknowledges the limitations of the elastic net regularization method, which lacks an underlying probabilistic structure. This limitation means that while ML can provide good predictive performance, it may not offer the same level of insight into statistical relationships as traditional methods like linear or logistic regression. Therefore, the comparison to simpler baselines is essential for understanding the trade-offs between predictive performance and interpretability.",
  "evaluation/confidence": "The evaluation of our machine learning models included the calculation of confidence intervals for the performance metrics. Specifically, 95% confidence intervals were obtained for the area under the receiver operating characteristic curve (AUROC) and the Brier score using 2,000 bootstrapped replicates. Additionally, a 95% confidence interval for accuracy was calculated using the method described by Clopper and Pearson.\n\nTo assess the statistical significance of our models' performance, Spiegelhalter\u2019s Z-test was employed. This test helps determine whether the calibration of the models is adequate. A p-value greater than 0.05 from Spiegelhalter\u2019s Z-test indicates that the model's calibration is acceptable.\n\nRegarding the superiority of our method compared to others and baselines, it is important to note that while our models demonstrated adequate discrimination and calibration for certain outcomes, such as prolonged hospital length of stay and nonroutine discharge disposition, the model predicting high hospital charges showed inadequate discrimination. Therefore, claims of superiority should be made cautiously and should be supported by empirical evidence from the performance metrics and statistical tests.\n\nIn summary, the performance metrics include confidence intervals, and statistical tests were used to evaluate the significance of the results. However, the superiority of the method over others and baselines should be empirically assessed and supported by the performance metrics and statistical tests.",
  "evaluation/availability": "Not enough information is available."
}