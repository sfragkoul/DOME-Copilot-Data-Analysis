{
  "publication/title": "Traditional Chinese medicine diagnostic prediction model for holistic syndrome differentiation based on deep learning.",
  "publication/authors": "Chen Z, Zhang D, Liu C, Wang H, Jin X, Yang F, Zhang J",
  "publication/journal": "Integrative medicine research",
  "publication/year": "2024",
  "publication/pmid": "38298865",
  "publication/pmcid": "PMC10826311",
  "publication/doi": "10.1016/j.imr.2023.101019",
  "publication/tags": "- Traditional Chinese Medicine (TCM)\n- Deep Learning\n- TCM-BERT-CNN Model\n- Syndrome Differentiation\n- Model Evaluation\n- Cross-Validation\n- Precision\n- Recall\n- F1 Score\n- Data Segmentation\n- Clinical Diagnosis\n- Integrative Medicine\n- Text Classification\n- Machine Learning\n- Syndrome Prediction",
  "dataset/provenance": "The dataset utilized in this study was constructed based on clinical practice guidelines and textbooks in Traditional Chinese Medicine (TCM). The data collection process involved comprehensive searches of both Chinese and English databases, including the China National Knowledge Infrastructure (CNKI), Wanfang Data Knowledge Service Platform, VIP Information, China National Biomedical Literature Service System (Sinomed), PubMed, and Embase. Additionally, TCM textbooks such as \"Internal Medicine in TCM,\" \"Pediatrics in TCM,\" and \"Gynecology in TCM\" were reviewed to supplement the dataset.\n\nThe final dataset comprised a total of 6148 samples. These samples were used to assess the performance of the TCM-BERT-CNN model through ten-fold cross-validation. The dataset included multiple TCM syndromes that could coexist and intersect within a single sample, reflecting the complexity and dynamic nature of TCM syndrome differentiation.\n\nThe dataset was categorized into various TCM syndromes, including exterior syndrome, internal syndrome, deficiency syndrome, excess syndrome, cold syndrome, heat syndrome, and others. This categorization allowed for a detailed analysis of the model's performance across different syndrome types.\n\nThe dataset was not used in previous papers or by the community. It was specifically constructed for this study to address the limitations of existing models and to provide a comprehensive approach to TCM syndrome differentiation. The use of expert knowledge and clinical guidelines ensured the relevance and accuracy of the dataset for the study's objectives.",
  "dataset/splits": "The dataset used in this study was divided into ten subsets for the purpose of cross-validation. This method, known as ten-fold cross-validation, involves using nine of these subsets as training sets and one as a test set. This process is repeated ten times, with each subset serving as the test set once, ensuring that every data point is used for both training and testing. The final results are averaged across these ten iterations to provide a comprehensive evaluation of the model's performance.\n\nThe dataset consisted of a total of 6148 samples. These samples were categorized into various Traditional Chinese Medicine (TCM) syndromes, with some samples containing multiple coexisting syndromes. The distribution of these syndromes included categories such as exterior syndrome, internal syndrome, deficiency syndrome, excess syndrome, cold syndrome, heat syndrome, and others. Each syndrome had a varying number of samples, reflecting the diversity and complexity of the dataset.\n\nThe use of ten-fold cross-validation helped to mitigate the impact of imbalanced data distribution, ensuring that the model's performance was assessed accurately and reliably. This approach provided a robust framework for evaluating the model's understanding of the data and its ability to generalize to new, unseen samples.",
  "dataset/redundancy": "The dataset utilized in this study was constructed based on clinical practice guidelines and textbooks in Traditional Chinese Medicine (TCM). To ensure robust model evaluation, ten-fold cross-validation was employed. This method involves dividing the dataset into ten subsets, using nine subsets for training and one subset for testing in each iteration. This process is repeated ten times, with each subset serving as the test set once. The results from these ten iterations are then averaged to assess the model's accuracy and precision. This approach effectively mitigates the impact of imbalanced data distribution on the results, ensuring that the model's performance is evaluated comprehensively.\n\nThe dataset comprised a total of 6148 samples, with multiple TCM syndromes coexisting and intersecting within a single sample. The classification of dataset distribution for the TCM syndromes included various categories such as exterior syndrome, internal syndrome, deficiency syndrome, excess syndrome, and others. This detailed classification ensures that the model can handle the complexity and overlap of TCM syndromes, providing a more accurate and holistic diagnostic approach.\n\nThe use of ten-fold cross-validation ensures that the training and test sets are independent in each iteration, reducing the risk of data leakage and overfitting. This method is widely recognized in machine learning for its ability to provide a reliable estimate of model performance, especially when dealing with imbalanced datasets. The distribution of the dataset in this study is designed to reflect the real-world complexity of TCM syndrome differentiation, making it a valuable contribution to the field of TCM and machine learning.",
  "dataset/availability": "The data that support the findings of this study are available within the article. Additionally, the data can be obtained from the corresponding author upon reasonable request. This approach ensures that the data is accessible to other researchers while maintaining control over its distribution. There is no mention of a public forum or specific license for the data release. The data availability is enforced through direct requests to the corresponding author, ensuring that the data is shared responsibly and in accordance with ethical guidelines.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is deep learning, specifically a combination of BERT and CNN models. This approach is not entirely new, as both BERT and CNN have been widely used in natural language processing and text classification tasks. However, the integration of these models with traditional Chinese medicine (TCM) characteristics and expert knowledge to form the TCM-BERT-CNN model is innovative.\n\nThe TCM-BERT-CNN model was developed to address the unique challenges of TCM syndrome differentiation, which involves complex and holistic diagnostic methods. This model was not published in a machine-learning journal because the primary focus of this research is on its application in TCM, rather than the development of a new machine-learning algorithm. The study aims to leverage existing deep learning techniques to enhance the diagnostic capabilities of TCM, providing a modern approach to an ancient medical practice.\n\nThe model's performance was evaluated using precision, recall, and F1 scores, which are standard metrics in machine learning for assessing the effectiveness of classification models. The TCM-BERT-CNN model demonstrated superior results compared to other deep learning models, such as BERT, TextCNN, LSTM RNN, and LSTM ATTENTION, in predicting various TCM syndromes. This indicates that the integration of BERT and CNN, tailored to TCM characteristics, is effective in improving the accuracy and reliability of syndrome differentiation.",
  "optimization/meta": "The model described in this publication is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it is a deep learning model that combines BERT and CNN architectures to handle the task of holistic syndrome differentiation in Traditional Chinese Medicine (TCM). The TCM-BERT-CNN model is trained on a dataset constructed from clinical practice guidelines and textbooks in TCM, utilizing ten-fold cross-validation to assess its performance. The model's architecture involves extracting TCM-specific features using BERT embeddings, which are then processed by a CNN with hierarchical connections. This approach allows the model to effectively learn semantic features and identify characteristic symptoms of TCM syndromes. The evaluation metrics used for the model include precision, recall, and F1 score, which are calculated based on true positive, true negative, false positive, and false negative results. The model's performance is compared with other deep learning models such as BERT, TextCNN, LSTM RNN, and LSTM ATTENTION, demonstrating superior results in predictive classification of various TCM syndromes.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithm. The dataset was constructed based on clinical practice guidelines and textbooks in Traditional Chinese Medicine (TCM). The information related to 21 TCM syndromes was categorized into two constraint groups: 'exterior syndrome' and 'internal syndrome'; 'deficiency syndrome' and 'excess syndrome'. The remaining 17 syndromes formed an overall classification group.\n\nThe sentences were tokenized, and special tokens [CLS] and [SEP] were used to accept all words and separate the two sentences. The [CLS] token served as the start token, and the [SEP] token as the end token. These tokens were mapped to multiple categories. A two-dimensional vector was used for category determination in the two constraint groups, while a multidimensional vector was employed in the overall classification group.\n\nThe BERT embedding output was obtained after extracting TCM-specific features, which served as the input for the Convolutional Neural Network (CNN) with hierarchical connections. This process resulted in the structure of the TCM-BERT-CNN model.\n\nA multilayer Perceptron was used for vector space mapping. The SOFTMAX function was used as the activation function for the constrained groups, normalizing probabilities based on the natural exponent 'e', ensuring that the sum of probabilities for binary classification data equals 1. The SIGMOID function was used for the non-constrained groups, defining the value range for multiple classifications and distributing the probability values within the 0\u20131 numerical range.\n\nThe dataset consisted of 6148 samples, with multiple TCM syndromes coexisting and intersecting within a single sample. Ten-fold cross-validation was used to assess the model's performance, involving dividing the dataset into ten subsets, using nine as training sets and one as a test set. This method helped mitigate the impact of an imbalanced data distribution on the results.",
  "optimization/parameters": "In our study, the TCM-BERT-CNN model utilized a combination of BERT embeddings and CNN layers, which inherently involves a large number of parameters. The exact number of parameters, p, can vary based on the specific configuration of the BERT model and the CNN architecture used. Typically, BERT models like BERT-base have around 110 million parameters, while the CNN layers add additional parameters depending on the number of filters, kernel sizes, and other hyperparameters.\n\nThe selection of p was not arbitrary but was determined through a combination of empirical evidence and theoretical considerations. We started with a pre-trained BERT model, which has been extensively validated in various natural language processing tasks. The CNN layers were then added to capture local dependencies and patterns in the text data. The hyperparameters for the CNN, such as the number of filters and kernel sizes, were tuned using a validation set to ensure optimal performance.\n\nAdditionally, we employed ten-fold cross-validation to assess the model's performance and stability. This method involved dividing the dataset into ten subsets, using nine for training and one for testing, and repeating this process ten times. This approach helped in mitigating the impact of data imbalance and ensured that the model's performance was robust and generalizable.\n\nIn summary, the number of parameters in our model is primarily determined by the BERT architecture and the additional CNN layers. The selection of these parameters was guided by empirical validation and cross-validation techniques to ensure the model's effectiveness in classifying TCM syndromes.",
  "optimization/features": "The input features for our model are derived from TCM-specific characteristics, which are extracted and embedded using BERT. The information related to 21 TCM syndromes is categorized into two constraint groups ('exterior syndrome' and 'internal syndrome'; 'deficiency syndrome' and 'excess syndrome') and the remaining 17 syndromes form an overall classification group. The BERT embedding output serves as the input for the CNN with hierarchical connections, resulting in the structure of the TCM-BERT-CNN model.\n\nThe sentences are tokenized, and special tokens [CLS] and [SEP] are used to accept all words and separate the two sentences. [CLS] serves as the start token and [SEP] as the end token, and the vectors are mapped to multiple categories. A two-dimensional vector is used for category determination in the two constraint groups, and a multidimensional vector is employed in the overall classification group.\n\nFeature selection was not explicitly mentioned as a separate step in our methodology. However, the use of BERT embeddings inherently involves a form of feature selection, as BERT focuses on the most relevant tokens and contextual information within the sentences. This process ensures that the input features are optimized for the task at hand.\n\nThe feature extraction and embedding process is performed using the entire dataset, including the training set, to ensure that the model learns the most relevant features from the data. This approach helps in mitigating the impact of an imbalanced data distribution on the results. The use of ten-fold cross-validation further ensures that the model's performance is evaluated consistently across different subsets of the data.",
  "optimization/fitting": "The TCM-BERT-CNN model was designed to handle a substantial dataset comprising 6148 samples, which were used for ten-fold cross-validation. This approach ensures that the model's performance is evaluated across multiple subsets of the data, mitigating the risk of overfitting. Overfitting was further addressed by using cross-validation, which helps in assessing the model's generalization capability by training on different subsets of the data and validating on the remaining subset.\n\nThe model's architecture, which combines BERT for feature extraction and CNN for hierarchical connections, allows it to capture complex patterns in the data without an excessively large number of parameters. This design helps in balancing the model's capacity to learn from the data while avoiding overfitting.\n\nTo ensure that the model does not underfit, various deep learning models were compared, including BERT, TextCNN, LSTM RNN, and LSTM ATTENTION. The TCM-BERT-CNN model demonstrated superior performance in terms of precision, recall, and F1 score across different TCM syndromes. This comparison validates that the model is sufficiently complex to capture the necessary patterns in the data without being too simplistic.\n\nAdditionally, the use of activation functions like SOFTMAX and SIGMOID in the model helps in normalizing probabilities and distributing probability values within the appropriate range, further enhancing the model's ability to make accurate predictions. The experimental environment, which includes PyTorch 1.10 and Python 3.6, provides a robust framework for training and evaluating the model, ensuring that the results are reliable and reproducible.",
  "optimization/regularization": "Not enough information is available.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in this study are not explicitly detailed in the provided information. However, the experimental environment and framework are specified. The study employs PyTorch 1.10 as the framework and Python 3.6 as the experimental environment. This information is crucial for replicating the experiments and understanding the technical setup.\n\nRegarding the availability of model files and optimization parameters, there is no mention of where these can be accessed or under what license they might be provided. Typically, such details would be included in the supplementary materials or a dedicated repository, but this information is not available here.\n\nFor those interested in replicating the study, it would be beneficial to contact the authors directly for access to the specific model files, hyper-parameter configurations, and optimization parameters. This would ensure that the experiments can be reproduced accurately and that the results can be validated independently.",
  "model/interpretability": "The TCM-BERT-CNN model, while leveraging deep learning techniques, is designed with a focus on interpretability, making it more transparent than typical black-box models. This model incorporates semantic feature analysis to visualize symptom correlations within Traditional Chinese Medicine (TCM) syndromes. For instance, in the case of deficiency syndrome, the model highlights key symptoms such as \"dull complexion,\" \"pale lips and nails,\" \"light period,\" \"pale enlarged tongue,\" and \"fine pulse.\" The symptom feature correlation matrix reveals that \"dull complexion\" and \"fine pulse\" are the main symptom features, showing the highest probability of co-occurrence with other symptoms. This visualization helps in understanding the model's decision-making process by illustrating how different symptoms are interrelated.\n\nSimilarly, for cold syndrome, the model identifies symptoms like \"constipation,\" \"cold stomach ache,\" \"cold limbs,\" \"clear abundant urination,\" and \"pale tongue with white coating.\" These visualizations not only make the model's predictions more interpretable but also align with clinical diagnostic criteria, ensuring that the model's outputs are clinically relevant and understandable.\n\nBy projecting symptom data from lower-dimensional spaces to higher-dimensional spaces, the TCM-BERT-CNN model achieves better performance in handling various TCM syndromes. This approach allows for a more comprehensive understanding of the relationships between symptoms, making the model's predictions more transparent and reliable. The semantic feature presentation of the model reveals that information between symptoms in TCM patterns is not independent but exhibits correlations, further enhancing the model's interpretability.",
  "model/output": "The model is designed for classification tasks, specifically for predicting different Traditional Chinese Medicine (TCM) syndromes. It categorizes information related to 21 TCM syndromes into two constraint groups: 'exterior syndrome' and 'internal syndrome'; 'deficiency syndrome' and 'excess syndrome'. The remaining 17 syndromes form an overall classification group.\n\nThe model uses a combination of BERT for embedding extraction and CNN with hierarchical connections for classification. Special tokens [CLS] and [SEP] are used during tokenization to handle sentence structure. The [CLS] token serves as the start token, and the [SEP] token as the end token, with vectors mapped to multiple categories.\n\nFor category determination, a two-dimensional vector is used for the two constraint groups, while a multi-dimensional vector is employed for the overall classification group. A multi-layer Perceptron is utilized for vector space mapping, with SOFTMAX and SIGMOID serving as activation functions for the constrained and non-constrained groups, respectively. The SOFTMAX function normalizes probabilities to ensure the sum equals 1 for binary classification data, while the SIGMOID function defines the value range for multiple classifications, distributing probability values within the 0\u20131 numerical range.\n\nThe model's performance is evaluated using precision, recall, and F1 score, which are more suitable for handling imbalanced datasets compared to accuracy. These metrics provide a comprehensive assessment of the model's prediction performance across various TCM syndromes. The model was tested and compared with other deep learning models, including BERT, TextCNN, LSTM RNN, and LSTM ATTENTION, demonstrating superior performance in terms of precision, recall, and F1 score.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive assessment of model performance using a dataset constructed from clinical practice guidelines and textbooks in Traditional Chinese Medicine (TCM). To ensure robust and reliable results, ten-fold cross-validation was utilized. This technique involved dividing the dataset into ten subsets, where nine subsets were used for training and one subset was reserved for testing. This process was repeated ten times, with each subset serving as the test set once, and the results were averaged to provide a stable estimate of the model's performance.\n\nThe performance of the models was evaluated using three key metrics: precision, recall, and the F1 score. Precision measures the accuracy of the positive predictions made by the model, recall assesses the model's ability to identify all relevant instances, and the F1 score provides a harmonic mean of precision and recall, offering a balanced view of the model's performance. These metrics were calculated for various TCM syndromes, including exterior syndrome, interior syndrome, deficiency syndrome, excess syndrome, cold syndrome, heat syndrome, and others.\n\nThe models compared in this study included BERT, TextCNN, LSTM RNN, LSTM ATTENTION, and TCM-BERT-CNN. The evaluation results indicated that the TCM-BERT-CNN model outperformed the other models in terms of precision, recall, and F1 score across multiple TCM syndromes. This suggests that the TCM-BERT-CNN model has a superior ability to accurately identify and classify TCM syndromes compared to the other models tested. The ten-fold cross-validation results were relatively stable, further confirming the reliability and effectiveness of the evaluation method used.",
  "evaluation/measure": "In our study, we employed several key performance metrics to evaluate the effectiveness of our models in identifying Traditional Chinese Medicine (TCM) syndromes. The primary metrics reported are precision, recall, and the F1 score. These metrics are widely recognized in the field of machine learning and are particularly useful for evaluating the performance of classification models.\n\nPrecision measures the accuracy of the positive predictions made by the model. It is calculated as the ratio of true positive predictions to the total number of positive predictions (true positives plus false positives). High precision indicates that the model has a low false positive rate.\n\nRecall, also known as sensitivity or true positive rate, measures the model's ability to identify all relevant instances within a dataset. It is calculated as the ratio of true positive predictions to the total number of actual positives (true positives plus false negatives). High recall indicates that the model has a low false negative rate.\n\nThe F1 score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. It is particularly useful when dealing with imbalanced datasets, as it gives a more comprehensive view of the model's performance. The F1 score is calculated using the formula:\n\nF1 score = 2 * (Precision * Recall) / (Precision + Recall)\n\nThese metrics are representative of the standards used in the literature for evaluating classification models, especially in the context of medical and healthcare applications. They provide a clear and concise way to assess the model's performance across different TCM syndromes, ensuring that the results are both reliable and comparable to other studies in the field.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the performance of our proposed TCM-BERT-CNN model against several established deep learning models to ensure a comprehensive assessment. The models compared included BERT, TextCNN, LSTM RNN, and LSTM ATTENTION. These models were chosen because they represent a range of architectures commonly used in natural language processing and text classification tasks, providing a robust baseline for comparison.\n\nThe evaluation was conducted using a dataset constructed from clinical practice guidelines and textbooks in Traditional Chinese Medicine (TCM). This dataset was segmented using ten-fold cross-validation, a technique that involves dividing the dataset into ten subsets, using nine for training and one for testing, and repeating this process ten times to ensure the model's performance is consistent and not dependent on a particular split of the data.\n\nThe performance metrics used for comparison were precision, recall, and F1 score. These metrics provide a comprehensive view of the model's ability to correctly identify positive instances (precision), retrieve all relevant instances (recall), and balance these two aspects (F1 score). The results showed that the TCM-BERT-CNN model outperformed the other models in all three metrics, indicating its superior performance in predicting TCM syndromes.\n\nIn addition to the overall comparison, we also analyzed the performance of each model across various TCM syndromes. This detailed analysis revealed that while some models performed well in specific syndromes, the TCM-BERT-CNN model consistently delivered high precision, recall, and F1 scores across all syndromes. This consistency underscores the model's robustness and its potential for practical application in TCM diagnosis.\n\nBy comparing our model to these publicly available and widely used methods, we aimed to demonstrate its effectiveness and reliability. The results of this comparison provide strong evidence that the TCM-BERT-CNN model is a significant advancement in the field of TCM syndrome identification.",
  "evaluation/confidence": "The evaluation of the models included precision, recall, and F1 score metrics, all of which have associated confidence intervals. For instance, the TCM-BERT-CNN model demonstrated precision of 0.926 \u00b1 0.0274, recall of 0.9238 \u00b1 0.0293, and F1 score of 0.9247 \u00b1 0.0239. These intervals indicate the stability and reliability of the model's performance across different TCM syndromes.\n\nThe use of ten-fold cross-validation further enhances the statistical significance of the results. This method involves dividing the dataset into ten subsets, using nine for training and one for testing, and repeating this process ten times. The results from these iterations are averaged, which helps to mitigate the impact of imbalanced data distribution and ensures that the model's performance is robust and generalizable.\n\nThe comparison of multiple models, including BERT, TextCNN, LSTM RNN, and LSTM ATTENTION, provides a comprehensive evaluation framework. The TCM-BERT-CNN model consistently outperformed the other models in terms of precision, recall, and F1 score, suggesting that it is superior in identifying various TCM syndromes. The statistical significance of these results is supported by the consistent performance across different syndromes and the use of cross-validation, which reduces the likelihood of overfitting and ensures that the model's predictions are reliable.",
  "evaluation/availability": "Not enough information is available."
}