{
  "publication/title": "Statistical prediction of immunity to placental malaria based on multi-assay antibody data for malarial antigens.",
  "publication/authors": "Siriwardhana C, Fang R, Salanti A, Leke RGF, Bobbili N, Taylor DW, Chen JJ",
  "publication/journal": "Malaria journal",
  "publication/year": "2017",
  "publication/pmid": "28962616",
  "publication/pmcid": "PMC5622501",
  "publication/doi": "10.1186/s12936-017-2041-3",
  "publication/tags": "- Malaria\n- Placental Malaria\n- Predictive Models\n- Logistic Regression\n- Classification Algorithms\n- Antibody Responses\n- Immunity\n- Pregnancy\n- Demographic Factors\n- Biological Predictors",
  "dataset/provenance": "The dataset used in this study consists of archival plasma samples collected from a cross-sectional study conducted between 1996 and 2001 in Yaound\u00e9, Cameroon. The samples were obtained from 1377 pregnant women, both with and without placental malaria (PM). These samples were screened using 28 serological assays to measure IgG responses to various malarial antigens, including FV2, different lines of the six DBL domains, and eleven other antigens associated with malarial immunity. The study site, Yaound\u00e9, is a malaria-endemic area with an estimated entomological inoculation rate of 13 infectious bites per person per year during the sample collection period. The plasma samples were collected from two hospitals: Biyem-Assi Hospital and the Central Maternity Hospital. This dataset provides a valuable resource for understanding naturally acquired immunity to placental malaria, as the samples were collected before the implementation of intermittent preventive treatment (IPT) in 2004. The use of these samples allows for the assessment of immune responses without the confounding effects of IPT, making it a unique and important dataset for studying placental malaria immunity.",
  "dataset/splits": "The dataset used in this study consisted of 1377 individuals. To evaluate the performance of the classification models and avoid overfitting, a fivefold cross-validation technique was employed. This method involved partitioning the dataset into five approximately equal-sized random subgroups or folds. In each iteration of the cross-validation process, four of these folds were used for model development, while the remaining fold served as a test set to assess the classification performance. This procedure was repeated 500 times with different sets of random partitions to ensure a robust evaluation of the average performance. The distribution of data points in each fold was designed to be as balanced as possible, with each fold containing roughly one-fifth of the total dataset. This approach helped in obtaining unbiased estimates of the model's performance metrics, such as overall accuracy, sensitivity, specificity, and the area under the curve (AUC).",
  "dataset/redundancy": "The dataset used in this study consisted of 1377 individuals. To evaluate the performance of the classification models, a fivefold cross-validation technique was employed. This involved partitioning the dataset into five approximately equal-sized random subgroups or folds. In each iteration of the cross-validation, four of the five folds were used for model development, while the remaining fold served as a test set to assess the classification performance. This procedure was repeated 500 times with different sets of random partitions to evaluate the average performance.\n\nThe use of cross-validation ensures that the training and test sets are independent in each iteration, as the data is randomly partitioned anew for each run. This approach helps to provide an unbiased estimate of the model's performance and mitigates the risk of overfitting. The averaging via many random folds removes the biasedness that could be caused by the selection of specific partitions.\n\nIt is important to note that an independent test set from an entirely different population was not used for validation. Such data with the exact same measurements in a substantially large number of subjects with similar inclusion criteria was not available for an extensive study. However, the cross-validation technique used provides a robust method for evaluating the model's performance on unseen data.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and diverse, reflecting a comprehensive approach to classification problems. The algorithms employed include Recursive Partitioning (RPART), Random Forests (RF), Linear Discriminate Analysis (LDA), Quadratic Discriminant Analysis (QDA), and Support Vector Machine (SVM). These algorithms were chosen for their varied underlying techniques, ensuring a robust evaluation of predictive models.\n\nThe algorithms are not new; they are widely recognized and have been extensively studied in the field of machine learning. RPART, for instance, is a non-parametric multivariable classification technique that develops decision trees based on logical conditions. Random Forests extend this idea by growing multiple decision trees and aggregating their results to improve prediction accuracy. LDA and QDA are classical classification techniques that assume different distributional properties of the data. SVM, on the other hand, is a supervised machine learning algorithm that seeks hyperplanes in a multi-dimensional space to specify decision boundaries.\n\nThe choice of these algorithms was driven by the need to compare different classification techniques and evaluate their performance on the specific dataset used in the study. The algorithms were selected based on their diverse nature and proven effectiveness in handling binary classification problems. The study aimed to identify the most appropriate classifier for predicting the presence or absence of placental malaria (PM) at delivery, using a set of demographic and biological predictors.\n\nThe decision to use these established algorithms rather than proposing a new one was likely influenced by the goal of providing a thorough comparison and validation of existing methods. Publishing in a machine-learning journal was not the primary objective; instead, the focus was on applying these algorithms to a specific medical research problem. The study demonstrates the potential utility of incorporating multiple malarial antigens in developing predictive models for PM states, highlighting the importance of using diverse and well-understood machine-learning techniques to achieve this goal.",
  "optimization/meta": "In our study, we did not employ a meta-predictor approach. Instead, we focused on developing and evaluating individual classifiers to predict placental malaria (PM) status. We considered seven different classification techniques: Logistic Regression (both full and reduced models), Recursive Partitioning, Random Forests, Linear Discriminant Analysis, Quadratic Discriminant Analysis, and Support Vector Machine.\n\nEach of these classifiers was trained and evaluated independently using a five-fold cross-validation technique. This approach allowed us to assess the performance of each classifier across multiple performance measures, including overall accuracy, specificity, sensitivity, and the area under the curve (AUC).\n\nWhile we did not use a meta-predictor that combines the outputs of multiple machine-learning algorithms, we did compare the agreement among the classifiers' predictions. This analysis provided insights into the consensus and divergence among the different classification methods.\n\nThe training data for each classifier was kept independent by using the five-fold cross-validation technique. This method involves partitioning the data into five subsets, training the model on four subsets, and validating it on the remaining subset. This process is repeated five times, with each subset serving as the validation set once. This approach helps to ensure that the performance estimates are unbiased and generalizable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the robustness and accuracy of our machine-learning models. We began by handling the antibody responses data, which typically follow a highly-skewed distribution. To mitigate the effects of extreme data points, we applied a log transformation to the antibody data. This transformation helped in stabilizing the variance and making the data more suitable for analysis.\n\nWe also explored the impact of adding second-order polynomials of antibodies to our models, comparing them with corresponding univariate models. However, this approach did not significantly improve predictive performance.\n\nTo avoid overfitting, we employed the fivefold cross-validation technique. This involved partitioning our dataset of 1377 individuals into five approximately equal-sized random subgroups or folds. Four of these folds were used for model development, while the remaining fold served as a test set to evaluate classification performance. This procedure was repeated 500 times with different sets of random partitions to assess the average performance.\n\nFor model selection, we used the Bayesian Information Criterion (BIC), which helped in identifying the most parsimonious model that balanced model complexity and fit. This criterion ensured that our final model was not overly complex and generalized well to new data.\n\nAdditionally, we determined the optimal probability cut-point for class specification by maximizing Youden\u2019s J statistic. This approach is commonly used in binary classification problems to optimize both sensitivity and specificity.\n\nTo handle multicollinearity, we carefully selected a threshold value for the maximum correlation allowed between pairs of predictors. This step was essential to minimize the impact of highly correlated variables on our models.\n\nIn summary, our data encoding and preprocessing steps included log transformation of antibody data, exploration of polynomial terms, fivefold cross-validation, use of BIC for model selection, optimization of the probability cut-point, and careful management of multicollinearity. These steps collectively ensured that our machine-learning models were robust, accurate, and generalizable.",
  "optimization/parameters": "In our study, we employed a logistic regression model that initially included a set of nineteen potential demographic and biological predictors, along with their pairwise interactions. To determine the optimal set of parameters, we utilized a stepwise approach to refine the model. This process involved adding and dropping variables to achieve a reduced model with a lower Akaike Information Criterion (AIC), which balances model fit and complexity. The final reduced model, which we refer to as LR-Reduced, included a subset of these parameters that demonstrated significant main effects and interactions. Specifically, the final model incorporated six key variables: ID1-ID2a (FCR3 line), AMA-1, CSP, LSA1, MSP1, and gravidity. These variables were selected based on their statistical significance and their ability to improve the model's predictive performance. The selection process ensured that the model was both parsimonious and effective in predicting the outcome of interest.",
  "optimization/features": "In our study, we utilized a comprehensive set of demographic and biological predictors to develop our models. The specific number of features used as input varied depending on the model, but we initially considered a wide range of variables, including antibody responses to various malarial antigens, maternal age, and gravidity.\n\nFeature selection was indeed performed to identify the most relevant predictors. This process involved evaluating the significance of individual predictors and their interactions. For instance, in our final logistic regression model, six variables\u2014ID1-ID2a (FCR3 line), AMA-1, CSP, LSA1, MSP1, and gravidity\u2014were found to have significant main effects. Additionally, we examined pairwise interactions between predictors, such as the interactions between maternal age and other factors.\n\nThe feature selection process was conducted using the training data only, ensuring that the model's performance on the test data remained unbiased. This approach helped us to avoid overfitting and to develop a robust model that generalizes well to new data. The selection of the threshold value for the maximum correlation allowed between pairs of predictors was based on minimizing multicollinearity, which further refined our set of input features.",
  "optimization/fitting": "The fitting method employed in this study involved a logistic regression model, which was initially fitted using all possible two-way interactions among the candidate predictors. To address the potential issue of overfitting, given the number of parameters relative to the training points, a stepwise adding and dropping technique was utilized. This method helped in obtaining a reduced model with a lower Akaike Information Criterion (AIC), which balances model fit and complexity. The model with the lowest AIC was selected as it achieves the highest likelihood while penalizing for the number of parameters, thus mitigating overfitting.\n\nTo further ensure the robustness of the model and avoid overfitting, a fivefold cross-validation technique was applied. This involved partitioning the dataset of 1377 individuals into five approximately equal-sized random subgroups. Four of these folds were used for model development, while the remaining fold served as a test set to evaluate the classification performance. This procedure was repeated 500 times with different sets of random partitions to assess the average performance, providing a reliable estimate of the model's generalizability.\n\nAdditionally, the optimal probability cut-point for class specification was determined by maximizing Youden\u2019s J statistic, which optimizes both sensitivity and specificity. This approach helps in ensuring that the model is not underfitting by providing a balanced trade-off between the two performance measures.\n\nThe use of cross-validation and the selection of the model with the lowest AIC ensured that the final model was neither overfitted nor underfitted, providing a reliable and generalizable predictive model for the binary classification problem at hand.",
  "optimization/regularization": "To prevent overfitting, we employed the fivefold cross-validation technique. This involved partitioning the dataset of 1377 individuals into five approximately equal-sized random subgroups or folds. Four of these folds were used for model development, while the remaining fold served as a test set to evaluate the classification performance. This procedure was repeated 500 times with different sets of random partitions to assess the average performance. Additionally, we determined the optimal probability cut-point for class specification by maximizing Youden\u2019s J statistic, which is a common tool in binary classification problems for optimizing both sensitivity and specificity. This approach helped ensure that our models were robust and generalizable to new data.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters are not explicitly detailed in the provided information. The focus of the publication is on the performance of various classifiers, including logistic regression models, random forests, recursive partitioning, linear discriminant analysis, quadratic discriminant analysis, and support vector machines. The evaluation metrics such as overall accuracy, specificity, sensitivity, and AUC are reported for these classifiers.\n\nThe study emphasizes the use of a five-fold cross-validation technique to assess the classification performance, ensuring that the models are not overfitted. The dataset consisted of 1377 individuals, and the performance was evaluated through multiple random partitions to determine the average performance. The optimal probability cut-point for class specification was determined by maximizing Youden\u2019s J statistic.\n\nThe publication does not provide specific details on the availability of hyper-parameter configurations, optimization schedules, or model files. Therefore, it is not clear whether these resources are available for public access or under what license they might be distributed. The primary focus is on the comparative analysis of different classification techniques and their performance metrics.",
  "model/interpretability": "The model employed in this study is not a blackbox. It is a logistic regression model, which is inherently interpretable. The final logistic regression model, referred to as LR-Reduced, includes both main effects and pairwise interactions of predictors. This transparency allows for a clear understanding of how each predictor and their interactions influence the outcome.\n\nSix variables, namely ID1-ID2a (FCR3 line), AMA-1, CSP, LSA1, MSP1, and gravidity, were identified as having significant main effects. These variables directly contribute to the prediction of the outcome, making their impact straightforward to interpret.\n\nIn addition to main effects, the model also incorporates significant pairwise interactions. For instance, interactions between mothers' age and other factors were observed, indicating complex relationships that contribute to the prediction of placental malaria (PM). These interactions provide insights into how different factors jointly influence the outcome, rather than acting in isolation.\n\nThe graphical representation of the model highlights the significance of these main effects and interactions, offering a visual aid to understand the model's structure. This transparency is crucial for validating the model and ensuring that the predictions are based on meaningful and interpretable relationships between the predictors and the outcome.\n\nMoreover, the model's performance was evaluated using multiple metrics, including overall accuracy, sensitivity, specificity, and the area under the curve (AUC). The LR-Reduced model demonstrated a balanced performance across these metrics, with an AUC of approximately 0.76, indicating its effectiveness in predicting PM status.\n\nThe use of logistic regression ensures that the model remains interpretable, even as it incorporates complex interactions. This interpretability is essential for stakeholders to trust the model's predictions and for researchers to gain insights into the underlying mechanisms of PM.",
  "model/output": "The model discussed in this publication is a classification model. Specifically, it is a logistic regression model designed for binary classification, aiming to predict the presence or absence of placental malaria (PM). The model evaluates various demographic and biological predictors, along with their interactions, to classify individuals into PM+ (presence of placental malaria) or PM\u2212 (absence of placental malaria) categories.\n\nSeveral classification techniques were employed and compared, including Recursive Partitioning (RPART), Random Forests (RF), Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), and Support Vector Machine (SVM). The performance of these classifiers was assessed using metrics such as overall accuracy, specificity, sensitivity, and the Area Under the Curve (AUC). The logistic regression models, particularly the reduced model (LR-Reduced), showed promising results in terms of balanced performance, considering both sensitivity and specificity.\n\nThe final logistic regression model, LR-Reduced, was identified as the most appropriate classifier for predicting PM based on the proposed set of predictors. This model demonstrated reasonably good sensitivity and specificity measures, with an AUC of approximately 0.76. However, it was noted that the univariate logistic regression model for AMA-1 alone also had a high AUC of 0.7, indicating that the multivariate approach did not provide as extensive a performance boost as hypothesized.\n\nThe classification performance was evaluated using a fivefold cross-validation technique to avoid overfitting. This involved partitioning the dataset into five folds, using four folds for model development and the remaining fold for testing. The procedure was repeated 500 times with different random partitions to evaluate the average performance. The optimal probability cut-point for class specification was determined by maximizing Youden\u2019s J statistic, which optimizes both sensitivity and specificity.\n\nIn summary, the model is a classification model designed to predict the presence of placental malaria using a combination of demographic and biological predictors. The logistic regression approach, particularly the reduced model, was found to be the most effective among the compared classifiers.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the predictive models in our study was conducted using a fivefold cross-validation technique. This method involved partitioning the dataset of 1377 individuals into five approximately equal-sized random subgroups or folds. Four of these folds were used for model development, while the remaining fold served as a test set to evaluate the classification performance. This procedure was repeated 500 times with different sets of random partitions to assess the average performance.\n\nTo determine the optimal probability cut-point for class specification, we maximized Youden\u2019s J statistic. This approach is commonly used in binary classification problems to optimize both sensitivity and specificity.\n\nIn addition to logistic regression, we evaluated five other classification techniques: Recursive Partitioning (RPART), Random Forests (RF), Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), and Support Vector Machine (SVM). These classifiers were chosen for their diverse underlying techniques and ability to handle different types of data relationships.\n\nThe performance of each classifier was assessed using multiple measures, including overall prediction accuracy, sensitivity, specificity, and the area under the curve (AUC). These measures provided a comprehensive evaluation of the models' predictive capabilities.\n\nTo avoid overfitting, the classification performance was rigorously assessed through the fivefold cross-validation technique. This method ensures that the models are evaluated on unseen data, providing a more reliable estimate of their performance in real-world scenarios.\n\nOverall, the evaluation process was designed to be thorough and unbiased, ensuring that the best-performing models were identified based on their predictive accuracy and robustness.",
  "evaluation/measure": "In our study, we evaluated the performance of various predictive models using four key metrics: overall accuracy, specificity, sensitivity, and the area under the curve (AUC). These metrics provide a comprehensive view of the models' predictive capabilities.\n\nOverall accuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. It gives a general sense of how well the model performs across all predictions.\n\nSpecificity, also known as the true negative rate, indicates the proportion of actual negatives that are correctly identified by the model. This metric is crucial for understanding how well the model can identify cases that do not have the condition in question.\n\nSensitivity, or the true positive rate, measures the proportion of actual positives that are correctly identified. This is particularly important in medical diagnostics, where identifying true positive cases is often critical.\n\nThe area under the curve (AUC) of the receiver operating characteristic (ROC) curve provides a single scalar value that summarizes the performance of the model across all classification thresholds. An AUC of 1 indicates perfect performance, while an AUC of 0.5 suggests performance no better than random guessing. This metric is especially useful for comparing the performance of different models.\n\nThese four metrics are widely used in the literature for evaluating predictive models, particularly in medical and biological research. They offer a balanced view of model performance, covering both the correctness of positive and negative predictions and the overall reliability of the model. By considering these metrics, we ensure that our evaluation is thorough and representative of the model's true predictive power.",
  "evaluation/comparison": "In our study, we employed a comprehensive approach to evaluate and compare various classification techniques for predicting placental malaria (PM) status. We considered five commonly used classification methods alongside logistic regression to develop predictive models. These methods included Recursive Partitioning (RPART), Random Forests (RF), Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), and Support Vector Machine (SVM). Each of these classifiers was chosen for its unique underlying techniques, providing a diverse set of tools for comparison.\n\nTo ensure a robust evaluation, we utilized a fivefold cross-validation technique. This method involved partitioning the dataset of 1377 individuals into five approximately equal-sized random subgroups. Four of these folds were used for model development, while the remaining fold served as a test set to assess classification performance. This procedure was repeated 500 times with different sets of random partitions to obtain an average performance measure. This approach helped to avoid overfitting and provided an unbiased estimate of the classifiers' performance.\n\nThe performance of each classifier was evaluated using multiple measures, including overall accuracy, sensitivity, specificity, and the area under the curve (AUC). These metrics allowed us to compare the classifiers' abilities to correctly predict PM status, as well as their performance in identifying true positives (sensitivity) and true negatives (specificity).\n\nOur findings indicated that the Random Forests (RF) classifier held the highest overall accuracy, approximately 75%, and demonstrated the best sensitivity, around 90%. The reduced logistic regression model (LR-Reduced) also performed well, with an overall accuracy of approximately 71%. Other classifiers, such as RPART, LDA, QDA, and SVM, showed varying levels of performance, with overall accuracies ranging from 45% to 69%.\n\nIn addition to comparing these classifiers, we also examined the importance of individual predictors using a permutational method. This technique allowed us to rank the predictors based on their contribution to the model's predictive performance. The top five ranked predictors, which were crucial across all performance measures, included antibodies to non-VAR antigens such as AMA-1, MSP2, EBA-175, Pf41, and MSP11.\n\nOverall, our study demonstrated the potential utility of incorporating multiple malarial antigens in developing predictive models for PM states. The comparison of different classification techniques provided valuable insights into their strengths and weaknesses, highlighting the importance of selecting the appropriate method for a given dataset and research question.",
  "evaluation/confidence": "The evaluation of the predictive models in this study was conducted using several performance metrics, including overall accuracy, sensitivity, specificity, and the area under the curve (AUC). These metrics were calculated using a fivefold cross-validation technique, which provides an unbiased estimate of performance measures. The cross-validation approach helps to ensure that the results are robust and not dependent on a single partition of the data.\n\nStatistical significance was considered for a P value of less than 0.05. This threshold was applied to determine the significance of various predictors and their interactions in the logistic regression models. For instance, the significance of main effects and pairwise interactions in the reduced logistic regression model was assessed at this level. This ensures that the findings are not due to random chance but reflect genuine associations.\n\nConfidence intervals for the performance metrics were not explicitly mentioned in the provided information. However, the use of cross-validation and the consideration of statistical significance at the 0.05 level provide a strong foundation for evaluating the confidence in the results. The cross-validation technique helps to mitigate the risk of overfitting and provides a more reliable estimate of the model's performance on unseen data.\n\nThe study compared multiple classification algorithms, including logistic regression, random forests, recursive partitioning, linear discriminant analysis, quadratic discriminant analysis, and support vector machines. The logistic regression approach, particularly the reduced model, was found to have the best overall performance in terms of AUC and specificity. This suggests that the logistic regression model is a reliable and superior method for predicting placental malaria (PM) status based on the proposed set of demographic and biological predictors.\n\nIn summary, the evaluation of the predictive models was rigorous, utilizing cross-validation and statistical significance testing. While confidence intervals for the performance metrics were not provided, the methods used ensure that the results are reliable and that the logistic regression model is a strong predictor of PM status.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The data consists of sensitive medical information from participants, and sharing it publicly would violate privacy and ethical guidelines. However, we have provided detailed descriptions of our methods, models, and results in the publication to ensure reproducibility. Researchers interested in collaborating or accessing the data for specific purposes can contact the corresponding author to discuss potential data-sharing agreements. The study was conducted in accordance with ethical standards, and all participants provided informed consent. The evaluation was performed using standard techniques, such as five-fold cross-validation, to ensure the robustness and generalizability of our findings. The performance measures, including overall accuracy, sensitivity, specificity, and AUC, were calculated and reported for various classifiers to provide a comprehensive evaluation of the predictive models."
}