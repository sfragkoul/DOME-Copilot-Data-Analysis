{
  "publication/title": "Local attention and long-distance interaction of rPPG for deepfake detection.",
  "publication/authors": "Wu J, Zhu Y, Jiang X, Liu Y, Lin J",
  "publication/journal": "The Visual computer",
  "publication/year": "2023",
  "publication/pmid": "37361461",
  "publication/pmcid": "PMC10052279",
  "publication/doi": "10.1007/s00371-023-02833-x",
  "publication/tags": "- Deepfake detection\n- Remote photoplethysmography (rPPG)\n- Face forgery\n- Multi-scale Spatial\u2013Temporal PPG map\n- Mask-Guided Local Attention module (MLA)\n- Temporal Transformer\n- Generative adversarial networks (GANs)\n- Video compression\n- Video clip length\n- Source detection\n- Face forgery categorization\n- Deep learning\n- Computer vision\n- Pattern recognition\n- Medical image processing",
  "dataset/provenance": "The dataset used in our study is the FaceForensics ++ (FF ++) dataset, which is widely recognized and utilized in the community for face forgery detection tasks. This dataset is relatively large and includes 1000 real videos and 4000 fake videos. The fake videos are generated using five different face manipulation methods: Deepfakes (DF), Face2Face (F2F), FaceSwap (FS), NeuralTextures (NT), and FaceShifter (FSH). These methods vary in the regions they modify, with some swapping entire faces and others focusing on smaller areas or specific facial features.\n\nThe FF ++ dataset is particularly valuable because it contains videos with different compression rates, categorized as RAW (c0), HQ (c23), and LQ (c40). This variation in video quality allows for a more robust evaluation of detection methods under different conditions. Additionally, the dataset includes detailed annotations for facial landmarks, which are crucial for generating PPG maps and other relevant features.\n\nThe dataset has been used in previous works, including those by Ciftci et al., FakeCatcher, Boccignone et al., DeepRhythm, and Liang et al. These studies have contributed to the development and validation of various face forgery detection methods, making FF ++ a benchmark dataset in the field. The dataset's comprehensive nature and the diversity of manipulation techniques it includes make it an ideal choice for evaluating the effectiveness of our proposed method.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not applicable",
  "dataset/availability": "The datasets used in our study are publicly available, ensuring reproducibility and accessibility for further research. The primary dataset employed is FaceForensics++, which is widely recognized and used in the field of deepfake detection. This dataset includes a substantial collection of real and manipulated videos, generated using various face manipulation techniques such as Deepfakes, Face2Face, FaceSwap, NeuralTextures, and FaceShifter. The dataset is available in different video quality versions, including RAW, HQ, and LQ, corresponding to different compression rates.\n\nAdditionally, we utilized the Celeb-DF(v2) dataset to demonstrate the extension capability of our proposed method. Celeb-DF(v2) is a more challenging dataset that contains high-quality fake videos generated using advanced synthetic processes. This dataset is also publicly available and includes a significant number of real and manipulated videos.\n\nThe data splits used in our experiments follow standard practices in the literature. For the binary face forgery detection task, the datasets were divided into training, test, and validation sets in the ratio of 8:1:1. For the source detection task, the datasets were split in the ratio of 7:3, consistent with previous works. These splits ensure a fair and comprehensive evaluation of our method's performance.\n\nThe datasets are released under licenses that permit their use for research purposes. The specific licenses and access details can be found on the respective dataset websites. By adhering to these licenses, we ensure that the datasets are used ethically and in accordance with the terms set by their creators.",
  "optimization/algorithm": "The optimization algorithm employed in our work leverages deep learning techniques, specifically focusing on convolutional neural networks (CNNs) and transformers. The core of our approach involves a Mask-Guided Local Attention (MLA) module, which is designed to highlight specific regions in the feature maps of photoplethysmogram (PPG) signals that correspond to modified areas in face images. This module is integrated into a backbone network, which is based on the popular Xception architecture.\n\nThe MLA module itself is not entirely novel but is inspired by previous works and adapted for our specific application. It consists of several steps, including the generation of an attention mask from mid-level feature maps and the application of this mask to weight the features. The attention mask is trained in a supervised manner using an additional L1 loss function to approximate the ground truth manipulation mask.\n\nIn addition to the MLA module, we utilize a Temporal Transformer to capture temporal correlations between adjacent PPG clips. This component is also not entirely new but is adapted from standard Transformer architectures, which have been widely used in natural language processing and other sequential data tasks. The Temporal Transformer helps to exploit global information between adjacent features, improving the overall accuracy of our method.\n\nThe reason these components were not published in a machine-learning journal is that our primary focus is on their application to face forgery detection rather than the development of new machine-learning algorithms. The innovations lie in how these techniques are applied and integrated to solve a specific problem in the domain of media forensics. The effectiveness of our approach is demonstrated through extensive experiments on the FaceForensics++ dataset, showing state-of-the-art performance in both binary face forgery detection and multi-category source detection tasks.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, the data encoding process involved generating Multi-scale Spatial\u2013Temporal PPG maps from video clips. These maps were derived from multiple facial regions to exploit heartbeat signals effectively. The PPG maps were computed for each video clip, and corresponding masks were generated to highlight modified regions in the face images. These masks were used in the Mask-Guided Local Attention (MLA) module to focus on specific areas of the PPG maps that corresponded to manipulated regions.\n\nThe PPG maps were processed through a two-stage network. In the first stage, the MLA module was employed to locate spatial inconsistencies in the modified facial regions reflected on the PPG maps. This module utilized a convolution operation followed by a Sigmoid activation to generate attention masks, which were then used to weight the feature maps. The weighted features were fed into the subsequent layers of the network.\n\nIn the second stage, a Temporal Transformer was adopted to exploit long-distance information between adjacent video clips. This transformer structure allowed for the effective interaction of adjacent feature vectors, enhancing the network's ability to capture temporal correlations in the PPG signals.\n\nThe data was further pre-processed by normalizing the residual maps to a range of [0,1] and resizing them to match the scale of the attention masks. A threshold of 0.1 was applied to convert the residual maps into binary masks, which were used as ground truth for training the MLA module in a supervised manner. An extra L1 loss function was added to approximate the attention masks with the ground truth manipulation masks.\n\nThe experiments were conducted on the FaceForensics++ and Celeb-DF(v2) datasets, which included various face manipulation methods. The proposed method demonstrated superior performance in both face forgery detection and categorization tasks, outperforming other rPPG-based approaches. The extension experiments confirmed the method's generalization capability against newly added manipulation models.",
  "optimization/parameters": "In our work, the number of parameters used in the model is influenced by the architecture of the Temporal Transformer. Specifically, the number of embedded features, denoted as K, is more limited in our approach compared to other scenarios adopting Vision Transformers (ViT). This limitation allows a single layer of the Transformer structure to be sufficient for our purposes.\n\nThe selection of K and the overall architecture was driven by the need to balance model complexity and performance. Adding more layers of the Transformer encoder increases the number of training parameters but does not necessarily improve the outcome. Therefore, we opted for a single layer to maintain efficiency while achieving high accuracy.\n\nThe effectiveness of this choice is supported by our experiments, which showed that increasing the number of layers did not yield significant improvements in categorization accuracy. This indicates that our model is well-optimized for the task at hand, leveraging the strengths of the Transformer architecture without unnecessary complexity.",
  "optimization/features": "The input features for our method are derived from multi-scale spatial-temporal PPG maps. These maps are generated from video clips and represent the rhythmic patterns of facial regions. The number of input channels, denoted as C, corresponds to the number of channels in the PPG maps, which is typically three for RGB channels. The specific number of face sub-regions of interest (ROIs), denoted as n, and the clip length T can vary, but a common setting is T = 64, which has shown to provide a good balance between the amount of training data and the information contained in individual clips.\n\nFeature selection in the traditional sense is not explicitly performed. Instead, the multi-scale approach inherently selects relevant features by combining information from different facial regions at various scales. This method ensures that the most informative rhythmic patterns are captured without the need for manual feature selection.\n\nThe generation of PPG maps and the corresponding masks is done in a data-driven manner, utilizing the entire dataset. However, during the training of the Mask-Guided Local Attention module (MLA), the attention masks are generated in a supervised manner using ground truth manipulation masks derived from the training set. This ensures that the model learns to focus on the relevant modified regions without overfitting to the training data.",
  "optimization/fitting": "Not applicable.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our experiments are detailed within the publication. Specifically, the video clip length T is set to 64, the step size \u03c9 to 16, and the loss balancing hyperparameter \u03bb to 10. The batch size is 32, and the optimizer used is SGD with an initial learning rate of 0.01 over 30 epochs. The model architecture includes EfficientNetV2-M as the backbone, with a Multi-scale PPG Spatial\u2013Temporal map, a Mask-Guided Local Attention module, and a Temporal Transformer. The number of ROI sub-regions n is 6, and both RGB and YUV color spaces are used to generate PPG maps, resulting in C being 6. The adjacent PPG clip number K is 5, the number of self-attention heads is 8, and the embedded features dimension D is 256.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly mentioned as being publicly available. However, the implementation details and framework used (PyTorch) are provided, which allows for reproducibility. The dataset used, FaceForensics++, is widely available and can be accessed for further experimentation. The license under which these configurations and details are shared is not specified, but typical academic publications allow for non-commercial use and reproduction for research purposes.",
  "model/interpretability": "The model presented in this work is not a black box; it incorporates several explainable methods to enhance interpretability. One of the key approaches used is the integration of biological signals, specifically remote Photoplethysmography (rPPG), which provides clear physical meanings. This method estimates heart rate from recorded face videos, and the inconsistencies in heartbeat signals serve as strong evidence for detecting forgery in videos.\n\nAdditionally, the model utilizes explainable AI (XAI) techniques such as Local Interpretable Model-Agnostic Explanations (LIME) and Layer-Wise Relevance Propagation (LRP). These techniques provide clear visualizations of the salient regions of the image that the model focuses on, making the detection process more transparent.\n\nThe model also employs a Mask-Guided Local Attention (MLA) mechanism, which highlights specific regions in the PPG maps that have been modified. This allows for a more detailed examination of the local discrepancies between PPG maps generated by different manipulation sources. The use of heatmaps further aids in visualizing how the model distinguishes between different face manipulation methods.\n\nFurthermore, the model's architecture combines Convolutional Neural Networks (CNN) with Transformers, utilizing both spatial and temporal inconsistencies in Deepfakes. The local attention mechanism and the long-distance self-attention mechanism of the Transformer architecture contribute to the model's ability to detect forgeries effectively while maintaining interpretability.\n\nIn summary, the model is designed to be transparent and explainable, leveraging biological signals, XAI techniques, and attention mechanisms to provide clear insights into the detection process.",
  "model/output": "The model is designed for classification tasks, specifically for face forgery detection and source detection. It categorizes inputs into different classes, such as real vs. fake faces and various types of manipulation sources. The output of the model is the prediction score for the final classification, which is obtained through a Multi-Layer Perceptron (MLP) head applied to the class-token output of the last layer of the Temporal Transformer. The metric used to evaluate the model's performance is average categorization accuracy, expressed as a percentage. The model achieves high accuracy in binary face forgery detection and multi-category source detection tasks, demonstrating its effectiveness in distinguishing between real and manipulated videos, as well as identifying the specific manipulation methods used.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The proposed method was evaluated through a series of comprehensive experiments designed to test its effectiveness in face forgery detection and source detection tasks. The evaluation involved comparisons with six benchmark methods, including both face-based and rPPG-based approaches. The FaceForensics++ (FF++) dataset, which is widely used and contains a large number of real and fake videos generated by various manipulation methods, was employed for these evaluations.\n\nThe experiments included binary face forgery detection and multi-category source detection tasks. For the binary classification task, the dataset was divided into training, test, and validation sets in an 8:1:1 ratio. For the source detection task, the dataset was split into a 7:3 ratio, consistent with previous works. An open-source face detector, OpenFace, was used to detect 68 facial landmarks, and PPG maps were generated using both RGB and YUV color spaces.\n\nThe evaluation also involved detailed ablation studies to demonstrate the impact of each component of the proposed method. These studies included progressively adding the Multi-scale PPG Spatial\u2013Temporal map, the Mask-Guided Local Attention module, and the Temporal Transformer. The results of these ablation experiments were presented in a table, showing the average categorization accuracy for each configuration.\n\nAdditionally, extension experiments were conducted to show the method's ability to handle new manipulation sources. This involved adding a new category from the Celeb-DF dataset and evaluating the method's performance on this extended dataset. Experiments were also performed to explore the balance between the length of a single video clip and the amount of training data, as well as the method's robustness against video compression.\n\nThe evaluation metrics used were accuracy percentages for both binary classification and multi-category source detection tasks. The proposed method achieved the best results in several sub-datasets of the binary face forgery task and demonstrated state-of-the-art performance among all rPPG-based methods on the source detection task. This comprehensive evaluation highlights the effectiveness and robustness of the proposed method in detecting face forgeries and identifying their sources.",
  "evaluation/measure": "In our evaluation, we primarily focus on accuracy as our performance metric. This metric is reported as average categorization accuracy in percentage. We assess our method's performance on both binary face forgery detection (real vs. fake) and multi-category source detection tasks. For the binary classification task, we report accuracy on five sub-datasets: Deepfakes (DF), Face2Face (F2F), FaceSwap (FS), NeuralTextures (NT), and FaceShifter (FSH). For the multi-category source detection, we evaluate the method on five categories (1 real and 4 fakes) and report the average accuracy across these categories.\n\nThis set of metrics is representative of the current literature in the field of deepfake detection. Accuracy is a commonly used metric in similar studies, allowing for direct comparison with other methods. Additionally, evaluating performance on multiple sub-datasets and categories provides a comprehensive assessment of the method's effectiveness across different types of manipulations. The use of average categorization accuracy ensures that the reported performance is not skewed by any single category or sub-dataset.",
  "evaluation/comparison": "In the evaluation of our method, a comprehensive comparison was conducted with both face-based and rPPG-based approaches to ensure a fair and thorough assessment. For face-based methods, we selected the popular Xception model, which is widely recognized in the field. Additionally, we included all relevant rPPG-based methods for comparison, such as those proposed by Ciftci et al., FakeCatcher, Boccignone et al., DeepRhythm, and Liang et al. This comparison was performed on the widely used FaceForensics++ (FF++) dataset, which includes various sub-datasets generated by different face manipulation methods. The dataset contains real videos and fake videos created using techniques like Deepfakes, Face2Face, FaceSwap, NeuralTextures, and FaceShifter. The comparison involved both binary face forgery detection and multi-category source detection tasks, providing a robust evaluation of our method's performance against established benchmarks. The results demonstrated that our method achieved superior accuracy in detecting face forgeries and identifying the sources of manipulation, outperforming all other rPPG-based approaches.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}