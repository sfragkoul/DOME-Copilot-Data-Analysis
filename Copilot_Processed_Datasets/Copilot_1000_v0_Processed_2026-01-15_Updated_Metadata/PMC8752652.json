{
  "publication/title": "Comparative analysis of proficiencies of various textures and geometric features in breast mass classification using k-nearest neighbor.",
  "publication/authors": "Singh H, Sharma V, Singh D",
  "publication/journal": "Visual computing for industry, biomedicine, and art",
  "publication/year": "2022",
  "publication/pmid": "35018506",
  "publication/pmcid": "PMC8752652",
  "publication/doi": "10.1186/s42492-021-00100-1",
  "publication/tags": "- Breast cancer\n- Machine learning\n- Classification\n- k-NN classifier\n- Feature selection\n- Texture features\n- Geometric features\n- INbreast dataset\n- SVM classifier\n- Performance metrics",
  "dataset/provenance": "The dataset used in this study is the INbreast dataset. This dataset is publicly available and can be accessed from the repository at http://medicalresearch.inescporto.pt/breastresearch/index.php/Get_INbreast_Database.\n\nThe INbreast dataset consists of 115 mass lesions, with 52 benign masses and 63 malignant masses. The dataset includes 106 mammographic images, with 98 images containing a single mass, 7 images containing double masses, and 1 image containing three masses.\n\nThe INbreast dataset has been utilized in various studies within the community for the experimentation and evaluation of systems related to breast cancer detection and classification. The dataset provides boundary points in the form of pixel coordinates that inscribe various types of abnormalities in mammographic images of breasts. This detailed annotation is crucial for the extraction of exact shapes of breast masses, which is a stringent requirement for the extraction of geometric features. The dataset includes samples of mammogram images containing both benign and malignant mass lesions, which are essential for training and evaluating machine learning models in the classification of breast masses.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The dataset used in this study is publicly available. The INbreast dataset, which was utilized for experimentation and evaluation of the proposed system, can be accessed through the repository at http://medicalresearch.inescporto.pt/breastresearch/index.php/Get_INbreast_Database. This dataset includes mammogram images with annotations of boundary points in the form of pixel coordinates, inscribing various types of abnormalities. The images were obtained using MammoNovation Siemens FFDM equipment at Centro de Mama - Hospital de S. Jo\u00e3o (CHSJ), breast center Porto. The dataset contains a total of 115 mass lesions, with 52 benign and 63 malignant masses. The images include samples with single and multiple mass lesions, both benign and malignant. The dataset is licensed under terms that permit use, sharing, adaptation, distribution, and reproduction, as long as appropriate credit is given to the original authors and the source. The specific license details can be found at http://creativecommons.org/licenses/by/4.0/.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the k-nearest neighbors (k-NN) algorithm. This is a conventional machine learning algorithm that has been widely used in various classification tasks.\n\nThe k-NN algorithm employed in our study is not new. It is a well-established method in the field of machine learning and has been extensively studied and applied in numerous research works. The k-NN algorithm operates based on the principle that similar instances are close to each other in the feature space. It classifies an instance by finding the k closest instances in the training set and assigning the most common class label among these neighbors.\n\nThe reason the k-NN algorithm was not published in a machine-learning journal in the context of this study is that our focus was on applying and evaluating this algorithm for a specific medical diagnosis task, rather than introducing a new algorithm. Our study aimed to investigate the discriminative capabilities of different features in classifying breast masses using the k-NN algorithm, along with other conventional machine learning algorithms. The novelty of our work lies in the application of these algorithms to the INbreast dataset and the selection of the most discriminative features using the Relief-F method, rather than the development of a new machine-learning algorithm.",
  "optimization/meta": "The study does not employ a meta-predictor. Instead, it focuses on using conventional machine learning algorithms for feature selection and classification of breast masses. The primary classifier used in this research is the k-Nearest Neighbors (k-NN) algorithm. The k-NN classifier's performance was evaluated using various combinations of features, including texture and geometric features, extracted from the INbreast dataset.\n\nThe k-NN classifier's performance was compared with other state-of-the-art classifiers such as Support Vector Machines (SVM), Decision Trees (DT), Naive Bayes (NB), Random Forest (RF), and Ensemble Tree (ET). The experiments were conducted using ten-fold cross-validation, and the results were averaged over ten repetitions to ensure robustness.\n\nThe study does not combine the predictions of multiple machine learning models to form a meta-predictor. Instead, it focuses on optimizing the performance of the k-NN classifier by selecting the most discriminative features using the Relief-F method. The classification accuracy was further improved by experimentally selecting the top nine features out of the twenty features identified by the Relief-F method.\n\nThe training data used in this study is the INbreast dataset, which is publicly available. The dataset was used to train and evaluate the performance of the k-NN classifier and other machine learning algorithms. The study does not mention the use of any additional datasets or the combination of data from other machine-learning algorithms as input. Therefore, it is clear that the training data is independent and specifically designed for this research.",
  "optimization/encoding": "In our study, we utilized a comprehensive set of texture and geometric features extracted from breast mass lesions in fully field digital mammographic (FFDM) images. A total of 125 texture and geometric measures were derived for each of the 115 mass lesions from 106 FFDM images obtained from the INbreast dataset. These features were encoded and pre-processed to ensure compatibility with the machine-learning algorithms employed.\n\nThe texture features were extracted using various models, including Gray-Level Co-occurrence Matrix (GLCM), Gray-Level Difference Statistics (GLDS), Gray-Level Run-Length Matrix (GLRLM), Neighborhood Gray-Tone Difference Matrix (NGTDM), Statistical Gray-Level Difference Matrix (SGLDM), and Fourier Power Spectrum (FPS). These models capture different aspects of the texture information present in the breast mass lesions.\n\nGeometric features were derived using shape descriptors, Zernike moments, and Fourier descriptors. These features provide information about the shape and structure of the breast masses, which are crucial for distinguishing between malignant and benign categories.\n\nTo reduce the dimensionality of the feature set, we applied four state-of-the-art feature selection algorithms: Relief-F, Pearson correlation coefficient, neighborhood component analysis, and term variance. These algorithms helped in identifying the top 20 most discriminating features. Further experimentation revealed that a set of nine features, including Fourier descriptors, Euler number, solidity, mean, and others, yielded the highest classification accuracy.\n\nThe selected features were then used to train and evaluate various machine-learning classifiers, including k-Nearest Neighbors (k-NN), Support Vector Machine (SVM), Decision Tree (DT), Naive Bayes (NB), Random Forest (RF), and Ensemble Tree (ET). The k-NN classifier, with k=5 and squared inverse distance weight, outperformed all other algorithms, achieving an accuracy of 90.4%, sensitivity of 92.0%, and specificity of 88.0% for the classification of benign and malignant breast masses.",
  "optimization/parameters": "In the optimization process, the k-Nearest Neighbors (k-NN) classifier was employed, and its performance was found to depend on the selection of two key parameters: the number of nearest neighbors (k) and the distance metric.\n\nThe value of k was chosen to maximize classification accuracy. Specifically, k was set to 5 for the cosine variant of the k-NN classifier. This selection was based on experimental results that indicated this value yielded the best performance.\n\nThe distance metric used was the cosine distance, which was selected for its effectiveness in capturing the similarity between feature vectors. This choice was part of a broader investigation into various distance metrics and their impact on classification performance.\n\nIn summary, the model utilized two primary parameters: the number of nearest neighbors (k = 5) and the cosine distance metric. These parameters were selected through a systematic evaluation of their impact on classification accuracy.",
  "optimization/features": "In our study, we initially computed a total of 125 texture and geometric features. To enhance the performance of our classification models and mitigate the impact of redundant and irrelevant features, we employed a filter-based feature selection technique. This process was crucial for identifying the most discriminative features from the larger pool.\n\nWe utilized four state-of-the-art filter-based feature selection techniques: Relief-F, Pearson correlation coefficient, neighborhood component analysis, and term variance. After a thorough analysis, Relief-F was determined to be the most effective method for selecting the top 20 most discriminative features. This selection was performed using the training set only, ensuring that the feature selection process did not introduce any bias from the test set.\n\nThe top 20 features selected by Relief-F included a mix of texture and geometric features, belonging to various models such as the SGLCM model, FOS, SFM, fractal texture analysis, shape descriptors, Zernike moments, and Fourier descriptors. This subset of features was then used in further experiments to evaluate the classification performance of different state-of-the-art classifiers.",
  "optimization/fitting": "In our study, we employed a k-Nearest Neighbors (k-NN) classifier for the classification of breast mass lesions into malignant and benign categories. The k-NN algorithm is an instance-based learning method that does not explicitly fit a model to the data. Instead, it relies on the similarity between instances in the feature space to make predictions. This approach inherently avoids the issue of overfitting that can occur with more complex models, especially when the number of parameters is much larger than the number of training points.\n\nTo ensure robust performance, we used a 10-fold cross-validation strategy. This technique involves dividing the dataset into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. By averaging the results of these 10 experiments, we obtained a more reliable estimate of the model's performance, which helps in ruling out both overfitting and underfitting.\n\nAdditionally, we conducted extensive feature selection to identify the most discriminative features. We initially computed 125 texture and geometric features but used filter-based feature selection techniques to reduce this set to the top 20 most discriminative features. This step is crucial for preventing overfitting by ensuring that only the most relevant features are used for classification.\n\nFurthermore, we experimented with various combinations of features and different variants of the k-NN classifier, including fine k-NN, medium k-NN, cosine k-NN, cubic k-NN, and weighted k-NN. Each variant differs in terms of the distance metric and distance weight, allowing us to explore different ways of measuring similarity between instances. This comprehensive approach helped us to fine-tune the model and avoid underfitting by ensuring that the model captures the underlying patterns in the data.\n\nIn summary, our use of k-NN, cross-validation, and feature selection techniques ensured that our model was neither overfitted nor underfitted, providing reliable and accurate classification of breast mass lesions.",
  "optimization/regularization": "In our study, we employed feature selection as a regularization method to prevent overfitting. Feature selection is a crucial preprocessing step that helps in selecting an optimal and relevant set of features from a larger pool, thereby reducing the dimensionality of the data and mitigating the risk of overfitting.\n\nWe utilized a filter-based feature selection technique, specifically the Relief-F algorithm, to identify the top 20 most discriminative features out of a total of 125 texture and geometric features. The Relief-F algorithm is known for its effectiveness in handling noisy data and multi-class problems, making it suitable for our study.\n\nBy focusing on the most relevant features, we ensured that our machine learning models, particularly the k-NN classifier, were trained on a more concise and informative dataset. This approach not only improved the classification performance but also enhanced the generalization capability of the models, reducing the likelihood of overfitting to the training data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the publication. Specifically, we employed various variants of the k-NN classifier, including fine k-NN, medium k-NN, cosine k-NN, cubic k-NN, and weighted k-NN. The performance of these classifiers was evaluated using ten-fold cross-validation, and the results were averaged over ten repetitions. The experiments were conducted using MATLAB R2018a on a Windows 10 operating system with an Intel(R) Core(TM) i5-8250U CPU and 8 GB of RAM.\n\nThe dataset used for experimentation and evaluation, the INbreast dataset, is publicly available in the repository at http://medicalresearch.inescporto.pt/breastresearch/index.php/Get_INbreast_Database. This dataset was utilized to assess the performance of our proposed system.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly provided in the publication. However, the methods and configurations described are sufficient for replication of the experiments. The specific hyper-parameter settings and optimization schedules are discussed in the context of the k-NN classifier variants and the feature selection techniques employed.\n\nNot applicable.",
  "model/interpretability": "The model employed in this study is not a black-box but rather transparent, as it relies on interpretable features and a well-understood classifier. The k-Nearest Neighbors (k-NN) classifier, which was used for evaluation, is inherently interpretable. It classifies data points based on the majority vote of their k nearest neighbors, making it straightforward to understand how predictions are made.\n\nThe features used in the model are also interpretable. They include texture features, such as those derived from the Statistical Gray-Level Co-occurrence Matrix (SGLCM), Fractal Texture Analysis, and others, as well as geometric features like Fourier Descriptors and Zernike Moments. These features have clear physical meanings and are well-established in the field of image analysis. For instance, SGLCM features like sum squares, sum average, and entropy provide information about the texture patterns in the images, while Fourier Descriptors capture the shape of the objects.\n\nFurthermore, feature selection techniques like Relief-F were employed to rank and select the most discriminative features. This process not only improves the model's performance but also enhances its interpretability by focusing on the most relevant features. The top features selected by Relief-F include both texture and geometric features, providing a comprehensive view of the important characteristics for classification.\n\nThe use of these interpretable features and a transparent classifier allows for a clear understanding of the model's decision-making process. This is crucial in medical applications, where interpretability is essential for gaining the trust of healthcare professionals and ensuring the safe and effective use of the model.",
  "model/output": "The model employed in our study is a classification model. Specifically, we utilized the k-Nearest Neighbors (k-NN) classifier to evaluate the classification performance of various feature sets. The k-NN algorithm is a non-parametric, instance-based learning algorithm used for classification tasks. It classifies data points based on the majority vote of their k-nearest neighbors. In our experiments, we explored different variants of k-NN, including fine k-NN, medium k-NN, cosine k-NN, cubic k-NN, and weighted k-NN, to determine which configuration yielded the best performance metrics such as accuracy, sensitivity, and specificity.\n\nWe conducted extensive experiments using different feature sets, including texture and geometric features, to assess their discriminative capabilities in classifying breast masses. The performance of the k-NN classifier was evaluated using ten-fold cross-validation, and the results were averaged over ten repetitions to ensure robustness. The classification results were presented in tables, highlighting the performance of individual feature models and their combinations.\n\nIn summary, the model is designed for classification tasks, focusing on the accurate identification of breast masses using various feature sets and k-NN classifier variants.",
  "model/duration": "The experiments for this study were conducted using MATLAB R2018a on a Windows 10 operating system. The system specifications included an Intel(R) Core(TM) i5-8250U CPU running at 1.60 GHz with 1.80 GHz turbo boost and 8 GB of RAM. The k-NN classifier was evaluated using ten-fold cross-validation, and the performance was observed after ten repetitions. However, the exact execution time for the model to run is not specified.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the proposed system involved a comprehensive analysis using the INbreast dataset. The k-NN classifier was employed for assessing classification performance, utilizing ten-fold cross-validation. This process was repeated ten times, and the final results were averaged to ensure robustness. The performance metrics considered included mean accuracy, sensitivity, and specificity.\n\nInitially, all 125 features were used to evaluate six different state-of-the-art classifiers: k-NN, SVM, Decision Tree (DT), Naive Bayes (NB), Random Forest (RF), and Ensemble Tree (ET). The SVM classifier initially showed the best performance with an accuracy of 80.0%, sensitivity of 78.5%, and specificity of 82.0%. However, subsequent experiments revealed that the k-NN classifier outperformed others for various combinations of texture and geometric features.\n\nTo further investigate the discriminative capabilities of individual and combined features, numerous experiments were conducted. These included evaluating seven different texture models and three geometric feature models individually, as well as combinations of these features. Five variants of the k-NN classifier were used, differing in distance metrics and distance weights.\n\nThe Relief-F method was employed to select the top 20 most discriminative features, which improved classification accuracy from 76.0% to 85.2%. Further refinement using a set of nine experimentally selected features from the top 20 increased accuracy to 90.4%. This set included features like Fourier descriptors, shape descriptors, and texture features, which were found to be highly discriminative.\n\nThe experiments were conducted using MATLAB R2018a on a Windows 10 operating system with an Intel Core i5-8250U CPU and 8 GB of RAM. The results demonstrated that the k-NN classifier, when used with the selected features, provided the best classification performance for breast mass classification.",
  "evaluation/measure": "In our study, we evaluated the performance of various classifiers using a comprehensive set of metrics to ensure a thorough assessment. The primary metrics reported are accuracy, sensitivity, and specificity. Accuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, also known as recall, indicates the proportion of actual positives that are correctly identified by the model. Specificity, on the other hand, represents the proportion of actual negatives that are correctly identified.\n\nThese metrics are widely used in the literature and are considered representative for evaluating classification performance, particularly in medical imaging and diagnostic tasks. Accuracy provides an overall measure of the model's performance, while sensitivity and specificity offer insights into the model's ability to correctly identify positive and negative cases, respectively. This combination of metrics allows for a balanced evaluation of the classifiers' effectiveness in distinguishing between different classes.\n\nAdditionally, we employed ten-fold cross-validation to ensure the robustness of our results. This technique involves dividing the dataset into ten subsets, training the model on nine subsets, and testing it on the remaining subset. This process is repeated ten times, with each subset serving as the test set once. The final performance metrics are averaged over these ten repetitions, providing a more reliable estimate of the model's performance.\n\nIn summary, the reported performance metrics\u2014accuracy, sensitivity, and specificity\u2014are standard and representative in the field. They offer a comprehensive evaluation of the classifiers' performance, ensuring that our findings are both reliable and comparable to other studies in the literature.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison with existing works to evaluate the performance of our proposed method. We specifically compared our results with those from a recently published study by Hans et al., which also utilized conventional machine learning algorithms for feature selection and breast mass classification on the INbreast dataset. This comparison allowed us to benchmark our approach against established methods, ensuring that our findings are robust and reproducible.\n\nAdditionally, we evaluated our method against simpler baselines to understand its relative effectiveness. We tested six different state-of-the-art classifiers, including k-NN, SVM, DT, Naive Bayes, random forest, and ensemble tree, using all 125 features initially extracted. This initial experiment helped us identify the baseline performance of these classifiers. Subsequently, we focused on the k-NN classifier, as it demonstrated superior performance for various combinations of textures and geometric features, including a reduced set of nine most discriminative features.\n\nOur experiments were designed to investigate the discriminative capabilities of individual and combined features. We conducted multiple experiments using different feature models and classifier variants to ensure a comprehensive evaluation. The k-NN classifier was used with ten-fold cross-validation, and the performance was assessed through mean accuracy, sensitivity, and specificity over ten repetitions. This rigorous evaluation process ensured that our conclusions are based on a solid foundation of empirical evidence.",
  "evaluation/confidence": "The evaluation of our study involved a rigorous process to ensure the reliability and significance of our results. We employed ten-fold cross-validation for our experiments, which helps in providing a more robust estimate of model performance by averaging results over multiple train-test splits. This method reduces the variance and provides a more reliable estimate of the model's performance on unseen data.\n\nWe reported performance metrics such as accuracy, sensitivity, and specificity for various classifiers and feature sets. While specific confidence intervals for these metrics were not explicitly stated in the results, the use of ten-fold cross-validation inherently provides a measure of confidence through the variability observed across the different folds.\n\nStatistical significance was not explicitly tested using methods like p-values or confidence intervals in the provided results. However, the consistent performance improvements observed with different feature sets and classifiers, particularly the k-NN classifier with the top nine most discriminative features, suggest a strong indication of the method's superiority. The k-NN classifier achieved an accuracy of 90.4%, sensitivity of 92.0%, and specificity of 88.0% with these features, outperforming other classifiers and previous studies.\n\nThe comparison with previous work, such as the study by Hans et al., further supports the robustness of our findings. Our method using the Relief-F feature selection technique and k-NN classifier achieved an accuracy of 90.4%, which is significantly higher than the 78.8% reported by Hans et al. This comparison provides additional confidence in the superiority of our approach.\n\nIn summary, while explicit confidence intervals and statistical significance tests were not detailed, the use of ten-fold cross-validation and the consistent performance improvements across different experiments and comparisons with previous work provide a strong basis for claiming the superiority of our method.",
  "evaluation/availability": "The raw evaluation files used in this study are not directly available. However, the dataset employed for experimentation and evaluation, known as the INbreast dataset, is publicly accessible. This dataset can be found in the repository at the following URL: http://medicalresearch.inescporto.pt/breastresearch/index.php/Get_INbreast_Database. The dataset is provided under terms that allow for use, sharing, adaptation, distribution, and reproduction, as long as appropriate credit is given to the original authors and the source. A link to the Creative Commons license should also be provided, and any changes made should be indicated. For more detailed information regarding the license, one can visit http://creativecommons.org/licenses/by/4.0/."
}