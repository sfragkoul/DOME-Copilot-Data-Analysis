{
  "publication/title": "Artificial Intelligence in Plasma Cell Myeloma: Neural Networks and Support Vector Machines in the Classification of Plasma Cell Myeloma Data at Diagnosis.",
  "publication/authors": "Yenamandra AK, Hughes C, Maris AS",
  "publication/journal": "Journal of pathology informatics",
  "publication/year": "2021",
  "publication/pmid": "34760332",
  "publication/pmcid": "PMC8529344",
  "publication/doi": "10.4103/jpi.jpi_26_21",
  "publication/tags": "- Artificial neural network\n- Cytogenetics\n- Fluorescence in situ hybridization\n- Machine learning\n- Microarray\n- National Comprehensive Cancer Network\n- Next Generation Sequencing\n- Plasma cell myeloma\n- Support vector machines kernel trick",
  "dataset/provenance": "The dataset used in this study was collected from patients diagnosed with plasma cell myeloma (PCM). The data consisted of two cohorts, collected at different time points. The first cohort, labeled as cohort 1, included 301 cases collected from July 20, 2017, to August 31, 2018. The second cohort, labeled as cohort 2, consisted of 176 cases collected from September 1, 2018, to May 31, 2019. The variables included in the dataset were age, gender, percentage of bone marrow plasma cells, white blood cell count, and results of PCM fluorescence in situ hybridization (FISH) analysis.\n\nThe data was de-identified and collected retrospectively after approval by the Institutional Review Board (IRB). The first cohort was qualitative, with binary numerical values representing the results of various variables and risk status. The risk status was represented as 1 for normal and 2 for abnormal. The second cohort was quantitative, with the percentage of abnormal cells for different variables and a binary numerical value for risk status, where 0 represented normal and 1 represented abnormal.\n\nThe dataset was preprocessed and converted into a Comma Separated Value (CSV) file for import into Python and R studio. Subsets of the data were also created for deep learning analysis and visualization using Tableau. The subsets included combinations of variables, with some focusing on high-risk variables. The data was used to design predictive algorithms for identifying PCM risk status and to evaluate the performance of artificial neural network (ANN) models.",
  "dataset/splits": "The dataset was divided into two main cohorts, each with its own data splits. The first cohort consisted of 301 cases, while the second cohort had 176 cases. For the first cohort, various training and testing ratios were used, including 90/10, 80/20, 75/25, 70/30, 60/40, and 40/60. The 40/60 ratio was notably effective, achieving high accuracy in predictive models. This ratio was also used for the second cohort. The data splits were designed to build and evaluate models, with the training set used to develop the models and the testing set used to assess their performance. The distribution of data points in each split varied according to the chosen ratio, ensuring a comprehensive evaluation of the models' predictive capabilities.",
  "dataset/redundancy": "The datasets were split into training and testing sets using a 40/60 ratio, respectively. This split was chosen because it was found to be the most effective for this type of study. The training set was used to build the models, while the testing set was used for evaluation. The independence of the training and test sets was enforced by ensuring that the data used for training was not used in the testing phase. This was crucial to prevent data leakage and to ensure that the model's performance on the test set was a true indication of its generalizability.\n\nThe distribution of the datasets compared favorably to previously published machine learning datasets in the context of plasma cell myeloma (PCM) risk classification. The datasets included variables such as age, gender, white blood cell count, bone marrow plasma cell percentage, and results of fluorescence in situ hybridization (FISH) analysis. The datasets were collected retrospectively after approval by the Institutional Review Board (IRB) and consisted of two cohorts collected at different time points. Cohort 1 consisted of 301 cases, while Cohort 2 consisted of 176 cases. Both cohorts had similar types of variables, but Cohort 1 was qualitative with binary numerical values for results of various variables and risk status, whereas Cohort 2 was quantitative with percentages of abnormal cells for different variables.\n\nThe datasets underwent preprocessing steps, including conversion of categorical variables into numerical values and the creation of subsets with combinations of variables. This preprocessing was essential to prepare the data for analysis using artificial neural networks (ANNs) and other machine learning techniques. The subsets were designed to focus on high-risk variables, which were of particular interest in the study. The datasets were also visualized using Tableau, which provided valuable insights into the data distribution and the performance of the ANN models. The use of Tableau visualizations helped to identify patterns and trends in the data, which were crucial for the development and validation of the predictive algorithms.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is artificial neural networks (ANNs). Specifically, we employed a type of ANN known as a feedforward neural network, which is a popular and well-established class of algorithms in the field of machine learning.\n\nThe algorithm used is not new; it has been extensively studied and applied in various domains. The choice of using ANNs was driven by their ability to handle complex, nonlinear relationships in data, as well as their robustness to noisy data. These characteristics make ANNs particularly suitable for the type of risk classification tasks we were addressing in our research.\n\nRegarding the publication venue, our focus was on the application of these algorithms to pathology informatics, specifically for predicting risk status in plasma cell myeloma (PCM) cases. The Journal of Pathology Informatics was chosen because it is a specialized journal that caters to the intersection of pathology and informatics, making it the most appropriate platform to disseminate our findings to the relevant audience. The emphasis of our paper was on the practical application and the insights gained from using ANNs in this specific medical context, rather than on the development of new machine-learning algorithms.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It is a standalone artificial neural network (ANN) designed for risk classification. The ANN was built using a three-layer network with an input layer corresponding to selected variables, two hidden layers with varying numbers of nodes, and an output layer with a sigmoid activation function to ensure binary risk outcomes.\n\nThe ANN was developed using Python with the TensorFlow 2 backend and the Keras library. The model was trained using a two-phase procedure involving backpropagation and gradient descent, with optimization performed using the Adam algorithm. The dataset was divided into training and testing sets in various ratios, such as 90/10, 80/20, 75/25, 70/30, 60/40, and 40/60, to evaluate the model's performance.\n\nThe model's accuracy was assessed using metrics such as precision, recall, and F1 score. For instance, the MM411 dataset with a 40/60 training/testing ratio achieved high accuracy, demonstrating the model's predictive ability. The dataset was also visualized using Tableau to gain insights into how the ANN performed with different subsets of data.\n\nIn summary, the model is an ANN that does not rely on data from other machine-learning algorithms as input. It is a standalone predictive model designed to classify risk status based on selected variables from the dataset. The training data was independently divided into subsets to ensure the model's robustness and generalizability.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for preparing the datasets for the machine-learning algorithms. Initially, categorical variables such as gender were converted into numerical values, with male encoded as 1 and female as 2. Similarly, scores for each probe and case result were transformed into numerical values, where normal results were encoded as 1 and abnormal results as 2. This standardization ensured that the data could be effectively processed by the algorithms.\n\nThe Excel files containing the datasets were converted into Comma Separated Value (CSV) files to facilitate importation into Python and R Studio. The datasets included various columns with 20 variables, such as case number, gender, age, white blood cell count (WBC), and the percentage of bone marrow (BM) plasma cells.\n\nSubsets of the original dataset were created to focus on combinations of variables, particularly those deemed high-risk. These subsets were used to build and test different models. For instance, subsets like MM1, MM2, MM31, and MM411 were created with different combinations of variables to identify the most effective predictors.\n\nIn the case of cohort 2, the data preprocessing steps were similar to those of cohort 1, but with a slight modification. The normal result for a specific FISH probe was left empty, while the abnormal result was represented as a numerical percentage of abnormal cells. The last result column was converted into numerical values of 0 or 1, where normal was encoded as 0 and abnormal as 1.\n\nThese preprocessing steps ensured that the data was in a suitable format for training and evaluating the machine-learning models, including artificial neural networks (ANN) and support vector machines (SVM). The encoded data allowed for efficient computation and improved the accuracy of the predictive algorithms.",
  "optimization/parameters": "In our study, the number of input parameters, p, varied depending on the specific model and dataset used. Initially, we worked with a dataset containing 20 variables, including case number, gender, age, white blood cell count (WBC), and bone marrow (BM) percentage of plasma cells, among others. However, we employed a stepwise variable selection procedure to identify the most relevant features for building accurate models.\n\nFor instance, in the MM1 dataset, all 20 variables were included. In contrast, the MM10 dataset excluded gender, age, WBC, and BM plasma cell percentage, focusing on other variables. Further refinements led to datasets like MM2, which included 13 columns, and MM31, which used only 6 columns. The most successful models, MM411 and PCMQ3, utilized just five columns: WBC, BM plasma cell percentage, TP53 deletion, and FGFR3/IGH.\n\nThe selection of input parameters was driven by the goal of creating smaller, more efficient models that fit the data well while eliminating noisy or correlated variables. This process involved creating subsets of the original dataset, training models with different combinations of variables, and evaluating their performance. The final selection of parameters was based on the models that achieved the highest accuracy and predictive ability.",
  "optimization/features": "In our study, the number of input features varied across different models, ranging from 5 to 20. Initially, we started with a dataset containing 20 variables, including case number, gender, age, WBC, and percentage of bone marrow plasma cells, among others. However, we recognized that not all features were equally informative or beneficial for model performance. Therefore, we employed a stepwise variable selection procedure to identify the most relevant features.\n\nThis feature selection process involved creating subsets of the data by adding or removing variables that were noisy or reduced the model's accuracy. The goal was to build smaller models that fit the data well while eliminating correlations between predictor variables that could negatively impact the models. We created numerous subsets, each with a different combination of variables, and labeled them accordingly (e.g., MM1, MM2, MM31, MM411 for cohort 1, and PCMQ3 for a subset of cohort 2).\n\nThe feature selection was performed using the training set only, ensuring that the test set remained unseen during this process. This approach helped us to avoid overfitting and to build models that could generalize well to new, unseen data. Through this iterative process, we identified that subsets with fewer, more relevant features often led to higher accuracy in our models. For instance, the MM411 subset, which included only five columns (WBC, bone marrow plasma cell percentage, TP53 deletion, FGFR3/IGH, and results), achieved high accuracy in predicting risk status.",
  "optimization/fitting": "The fitting method employed in our study involved the use of artificial neural networks (ANNs) to build predictive models for risk classification. The ANN architecture consisted of three layers: an input layer, two hidden layers, and an output layer. The input layer corresponded to the selected variables, with the first hidden layer varying from 4 to 19 nodes and the second hidden layer from 4 to 8 nodes. The output layer had one node with a sigmoid activation function to ensure binary risk outcomes.\n\nTo address the potential issue of overfitting, given that the number of parameters in the ANN was indeed larger than the number of training points, several strategies were implemented. Firstly, a stepwise variable selection procedure was used to create subsets of the data, focusing on variables that improved model accuracy and eliminating noisy or correlated predictors. This helped in reducing the complexity of the model and preventing it from learning noise in the training data.\n\nAdditionally, the models were trained using a two-phase procedure. The first phase involved backpropagation with a moderate training rate, while the second phase utilized gradient descent. The optimization algorithm Adam was used for stochastic gradient descent, which is known for its efficiency in training deep learning models and its ability to adapt the learning rate for each parameter.\n\nTo further mitigate overfitting, the data was divided into training and testing sets in various ratios (90/10, 80/20, 75/25, 70/30, 60/40, and 40/60). The model's performance was evaluated on the training dataset using the evaluate function, and predictions were generated for each input and output accuracy. This approach ensured that the model's performance was assessed on unseen data, providing a more reliable estimate of its generalization capability.\n\nUnderfitting was addressed by experimenting with different activation functions such as ReLU, Leaky ReLU, and Tanh, and by adjusting the number of nodes in the hidden layers. The use of a cross-entropy error function to adjust weights also helped in improving the model's ability to learn from the data. The models were trained over 100\u20131000 epochs with each epoch split into batches, allowing the network to learn complex patterns in the data.\n\nIn summary, the fitting method involved careful selection of variables, use of advanced optimization algorithms, and thorough evaluation on unseen data to ensure that the models neither overfit nor underfit the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our artificial neural network models. One of the primary methods used was a stepwise variable selection procedure. This involved creating subsets of the data by adding or removing independent variables that were noisy or reduced the accuracy of the model. By focusing on variables that fit the data best, we aimed to build smaller, more efficient models that minimized the correlation between predictor variables, which can have undesirable effects on model performance.\n\nAdditionally, we utilized a two-phase training procedure for our neural networks. The first phase involved backpropagation with a moderate training rate, while the second phase employed gradient descent. This approach helped in fine-tuning the weights and biases of the network, ensuring that the model generalized well to unseen data.\n\nWe also experimented with different training and testing ratios, including 90/10, 80/20, 75/25, 70/30, 60/40, and 40/60. Through this process, we identified that a 40/60 training/test ratio was the most effective for our specific dataset, as it provided a good balance between model complexity and generalization.\n\nFurthermore, we used the Adam optimization algorithm for stochastic gradient descent, which is known for its efficiency and effectiveness in training neural networks. This algorithm helped in adjusting the weights of the network to minimize the cross-entropy error function, ensuring that the model output was interpreted as probabilities and comparable to the results of logistic regression.\n\nBy implementing these regularization techniques, we aimed to build models that were not only accurate but also robust and generalizable to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. We employed a three-layer neural network architecture, with the first hidden layer varying from 4 to 19 nodes and the second hidden layer from 4 to 8 nodes. The activation functions used were ReLU for the hidden layers and sigmoid for the output layer. The network was trained using a two-phase procedure: the first phase involved backpropagation with a moderate training rate, and the second phase utilized gradient descent. The optimization algorithm Adam was used for stochastic gradient descent. The model was trained over 100 to 1000 epochs, with each epoch split into batches.\n\nThe specific details of the models, including the number of nodes, activation functions, and training ratios, are discussed in the context of different datasets (e.g., MM1, MM10, MM2, MM31, MM411, and PCMQ3). For instance, the MM411 dataset with a 60/40 training/test ratio achieved high accuracy, but issues like the accuracy paradox due to imbalanced data were noted. The 40/60 training/test ratio was found to be the most effective for our type of study.\n\nRegarding the availability of model files and optimization parameters, the publication does not explicitly mention where these can be accessed or under what license. However, the methods and configurations are thoroughly described, allowing replication of the experiments by other researchers. The use of Python with TensorFlow 2 and the Keras library is specified, which are open-source tools, facilitating reproducibility.\n\nFor those interested in the specific implementations, the code and datasets used in this study are not directly provided in the publication. However, the detailed descriptions of the models and training procedures should enable researchers to implement similar configurations in their own work. Additional studies and visualizations, such as those created using Tableau, provide further insights into the data and model performance.",
  "model/interpretability": "The model we developed is an artificial neural network (ANN), which is inherently a black-box model. This means that while the ANN can provide accurate predictions, the internal workings and the specific reasoning behind these predictions are not easily interpretable. The model consists of interconnected layers of neurons, each applying activation functions to inputs, making it challenging to trace the decision-making process.\n\nHowever, we employed several strategies to enhance the interpretability of our model. One key approach was the use of a stepwise variable selection procedure. This involved creating subsets of data by adding or removing independent variables that were noisy or reduced the model's accuracy. By focusing on the variables that best fit the data, we aimed to build smaller, more interpretable models. For instance, we found that variables like age and gender were noisy and reduced the model's performance, leading us to exclude them in later iterations.\n\nAdditionally, we used Tableau for data visualization, which helped in uncovering valuable insights into how the ANN interacted with the dataset. Visualizations, such as those showing the distribution of high-risk categories, provided a clearer understanding of the data patterns that the model was learning from.\n\nWe also utilized logistic regression as a complementary model, which is more interpretable than ANNs. The logistic regression model achieved an accuracy of 1.0 with a 40/60 training and testing ratio, providing a transparent view of the relationships between the input variables and the output predictions. This helped in validating the ANN's predictions and understanding the key factors influencing the risk classification.\n\nIn summary, while the ANN model itself is a black box, our use of variable selection, data visualization, and complementary models like logistic regression helped in making the overall predictive process more interpretable. This approach allowed us to gain insights into the data patterns and the factors driving the model's predictions.",
  "model/output": "The model developed is a classification model. It is designed to predict binary risk outcomes, specifically to classify data points into categories based on risk status. The output layer of the neural network has one node with a sigmoid activation function, ensuring that the network output is between 0 and 1. This binary output is used to determine the risk classification, making it a classification model rather than a regression model. The model's performance is evaluated using metrics such as precision, recall, and the F1 score, which are commonly used in classification tasks. Additionally, the use of a cross-entropy error function further supports the classification nature of the model, as this function is typically employed in classification problems to measure the performance of a classification model's predictions.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method involved several steps to ensure the robustness and accuracy of the artificial neural network (ANN) models. Initially, the data was divided into training and testing sets using various ratios, including 90/10, 80/20, 75/25, 70/30, 60/40, and 40/60. This division allowed for a comprehensive assessment of how different training sizes affected model performance.\n\nThe models were built using a three-layer network architecture, with the input layer corresponding to selected variables. The first hidden layer varied from 4 to 19 nodes, depending on the number of input variables, and used the ReLU activation function. The second hidden layer had 4\u20138 nodes with the same activation function. The output layer had one node with a sigmoid activation function to ensure binary risk outcomes.\n\nThe models were trained using a two-phase procedure: the first phase involved backpropagation with a moderate training rate, and the second phase used gradient descent. The optimization algorithm Adam was employed for stochastic gradient descent to train the classifier. The models were trained over 100\u20131000 epochs, with each epoch split into batches.\n\nEvaluation metrics included precision, recall, F1 score, and accuracy. For instance, the MM411 dataset with a 40/60 training/testing ratio achieved 94% accuracy. The classification summary for MM411 and PCMQ3 datasets revealed precision of 0.97, recall of 0.76, and an F1 score of 0.83. Logistic regression accuracy was 1.0 for the 40/60 ratio model, indicating strong predictive ability.\n\nVisualizations using Tableau provided insights into how the ANN performed with the dataset, particularly highlighting the increase in abnormal cases with CD138 enrichment and the distribution of high-risk categories among males and females.\n\nThe evaluation also addressed the issue of imbalanced data distribution, which was attributed to the infrequent occurrence of high-risk categories in the dataset. This imbalance was not due to data collection errors but rather a reflection of real-world data characteristics. Additional studies and a better-weighted average could help clarify this aspect in future research.",
  "evaluation/measure": "In our evaluation of the artificial neural network (ANN) model, we reported several key performance metrics to assess the effectiveness of our classification algorithm. These metrics include precision, recall, F1 score, and accuracy.\n\nPrecision, also known as the positive predictive value (PPV), measures the proportion of true positive predictions among all positive predictions made by the model. A high precision indicates that when the model predicts a positive outcome, it is likely to be correct.\n\nRecall, or sensitivity, evaluates the model's ability to identify all relevant instances within a dataset. It is the proportion of true positives correctly identified by the model out of all actual positives.\n\nThe F1 score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. It is particularly useful when dealing with imbalanced datasets, as it considers both false positives and false negatives.\n\nAccuracy measures the overall correctness of the model's predictions, representing the proportion of true results (both true positives and true negatives) among the total number of cases examined.\n\nFor our datasets, MM411 and PCMQ3, with a training and testing ratio of 40/60, we achieved a precision of 0.97, a recall of 0.76, and an F1 score of 0.83. The accuracy of the logistic regression model was 1.0. These metrics collectively indicate that our model has a strong predictive ability, particularly in terms of precision and accuracy.\n\nThe set of metrics reported is representative of standard practices in the literature for evaluating classification models, especially in the context of medical and biological data analysis. Precision, recall, F1 score, and accuracy are commonly used to provide a comprehensive view of a model's performance, ensuring that both the correctness of positive predictions and the ability to identify all relevant instances are considered.",
  "evaluation/comparison": "In our study, we primarily focused on developing and evaluating artificial neural network (ANN) models for predicting risk status in plasma cell myeloma (PCM) cases. We did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, our approach involved creating and testing various ANN models using different subsets of our dataset, which included variables such as white blood cell count, bone marrow plasma cell percentage, TP53 deletion, and FGFR3/IGH.\n\nWe did, however, compare different configurations of our ANN models, including variations in the number of nodes, layers, and activation functions. This internal comparison helped us identify the most effective model architecture for our specific dataset. For instance, we found that a 40/60 training/test ratio generally yielded high accuracy, particularly for the MM411 and PCMQ3 datasets.\n\nRegarding simpler baselines, we did not explicitly compare our ANN models to simpler machine learning algorithms like logistic regression or decision trees. However, we did evaluate the performance of logistic regression within our study, achieving an accuracy of 1.0 for the 40/60 ratio model. This high accuracy suggests that logistic regression performed exceptionally well on our dataset, but it does not necessarily indicate that it is a simpler or more effective baseline compared to our ANN models.\n\nOur evaluation also included metrics such as precision, recall, and F1 score, which provided a comprehensive view of our models' performance. For the MM411 and PCMQ3 datasets, the 40/60 ratio model showed a precision of 0.97, recall of 0.76, and an F1 score of 0.83. These metrics, along with the high accuracy of logistic regression, indicate that our models are robust and capable of making accurate predictions.\n\nIn summary, while we did not conduct a direct comparison with publicly available methods or simpler baselines, our internal evaluations and the use of multiple performance metrics demonstrate the effectiveness of our ANN models in predicting PCM risk status.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The data consists of sensitive patient information, including details on plasma cell myeloma cases, which requires strict confidentiality and ethical considerations. Therefore, the datasets are not released to the public to protect patient privacy and comply with institutional review board (IRB) regulations.\n\nAccess to the data is restricted to authorized researchers involved in the study. The datasets were collected retrospectively after approval by the IRB, ensuring that all data handling procedures adhered to ethical guidelines. The data includes variables such as age, gender, percentage of bone marrow plasma cells, white blood cell count, and results of fluorescence in situ hybridization (FISH) analysis. These variables were crucial for developing and evaluating our predictive models.\n\nFor those interested in replicating or building upon our work, we provide detailed descriptions of the data preprocessing steps, model architectures, and evaluation metrics used in our study. This information is intended to facilitate future research while maintaining the confidentiality of the patient data. Researchers who wish to collaborate or gain access to similar datasets should contact the corresponding author for further information and potential data-sharing agreements."
}