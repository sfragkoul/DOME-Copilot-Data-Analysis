{
  "publication/title": "Machine-learning to stratify diabetic patients using novel cardiac biomarkers and integrative genomics.",
  "publication/authors": "Hathaway QA, Roth SM, Pinti MV, Sprando DC, Kunovac A, Durr AJ, Cook CC, Fink GK, Cheuvront TB, Grossman JH, Aljahli GA, Taylor AD, Giromini AP, Allen JL, Hollander JM",
  "publication/journal": "Cardiovascular diabetology",
  "publication/year": "2019",
  "publication/pmid": "31185988",
  "publication/pmcid": "PMC6560734",
  "publication/doi": "10.1186/s12933-019-0879-0",
  "publication/tags": "- Epigenetics\n- Mitochondria\n- Heart\n- Machine-learning\n- CART\n- SHAP\n- Type 2 diabetes mellitus\n- Biomarkers\n- Cardiovascular disease\n- Predictive modeling",
  "dataset/provenance": "The dataset utilized in this study was derived from right atrial appendages, which were collected during open-heart and/or valvular surgeries. These samples were obtained from patients at the West Virginia University Ruby Memorial Hospital Heart and Vascular Institute. The study involved a total of 50 patient samples.\n\nThe data encompasses various types of information, including physiological/biochemical, genomic, and epigenomic datasets. These datasets were combined to form a comprehensive \"all features\" dataset, which was used to identify the most influential biomarkers.\n\nThe sequencing files and computer code used in this study are available for reference and further analysis. The mitochondrial DNA sequencing data can be accessed through the Sequence Read Archive with the project ID PRJNA520920. Additionally, the TFAM Promoter Methylation Amplicon-Seq data is also available through the same Sequence Read Archive.\n\nThe datasets generated and analyzed during this study are available from the corresponding author upon reasonable request. This ensures that the data can be verified and potentially reused by the scientific community for further research.\n\nThe study also acknowledges the use of machine-learning algorithms to analyze the data, specifically focusing on Classification and Regression Trees (CART) and SHapley Additive exPlanations (SHAP). These methods were employed to determine the most accurate predictions regarding diabetic status based on the available biomarkers. The code for these analyses is provided on a public repository, facilitating reproducibility and further exploration by other researchers.",
  "dataset/splits": "The dataset was split into training and testing partitions. Specifically, 80% of the data was used for training, and 20% was reserved for testing. For binary classification, the testing set consisted of five diabetics and five non-diabetics. In the case of multiple classification, the testing set maintained a distribution of three diabetics, three non-diabetics, and four prediabetics. These splits were carefully chosen to ensure that the resulting training and testing accuracies were similar, thereby preventing overfitting to the small sample size and ensuring the model's generalizability to future testing samples. Additionally, tenfold cross-validation was implemented within the model to further validate the results.",
  "dataset/redundancy": "The datasets were split into training and testing partitions, with 80% of the data used for training and 20% for testing. This split was enforced using defined seed values to maintain consistent distributions across different trials. For binary classification, the testing set included five diabetic and five non-diabetic patients. In multiple classification, the testing set consisted of three diabetics, three non-diabetics, and four prediabetics. These seed values were chosen to ensure that the resulting training and testing accuracies were similar, preventing overfitting to the small sample size and maintaining generalizability to future testing samples.\n\nThe training and test sets are independent, as the data was explicitly partitioned to ensure no overlap between the two sets. This independence was crucial for evaluating the model's performance on unseen data. The distribution of the datasets was carefully managed to reflect a balanced representation of the different patient groups, which is essential for reliable model training and evaluation. This approach aligns with best practices in machine learning to ensure that the model's performance is not biased by the training data and can generalize well to new, unseen data.",
  "dataset/availability": "The datasets generated and analyzed during the current study, including sequencing files and computer code, are available. The primary data used and analyzed during the current study are available from the corresponding author upon reasonable request. The mitochondrial DNA sequencing data is available in the Sequence Read Archive under the accession number PRJNA520920. The TFAM Promoter Methylation Amplicon sequencing data is also available in the Sequence Read Archive under the same accession number. The complete sequencing code is provided on a public GitHub repository. The datasets are made available to ensure transparency and reproducibility of the research findings. The data availability ensures that other researchers can access and verify the results, promoting further research and collaboration in the field. The datasets are shared in compliance with the guidelines and policies of the funding agencies and the institutional review board. The data is made available to the research community to facilitate the advancement of knowledge in the field of cardiovascular diabetes research. The datasets are shared with the understanding that they will be used responsibly and ethically, respecting the privacy and confidentiality of the participants. The data is made available with the hope that it will contribute to the development of new diagnostic tools and treatments for type 2 diabetes mellitus.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is Classification and Regression Trees (CART). This algorithm is well-established and widely used in various fields, including biomedical research. It is not a new algorithm; rather, it is a tried-and-tested method for classification tasks.\n\nThe decision to use CART was driven by its ability to handle complex datasets and provide interpretable results. CART is particularly useful for identifying important features and understanding their impact on the classification outcomes. This aligns well with our goal of identifying key biomarkers for diabetic status prediction.\n\nWhile CART is a standard algorithm, our application of it to a multi-omics dataset in the context of type 2 diabetes mellitus is novel. We combined physiological, biochemical, genomic, and epigenomic data to develop a comprehensive model. This multi-faceted approach allowed us to gain deeper insights into the disease mechanisms and identify biomarkers that could be used for more accurate diagnostics and prognostics.\n\nThe reason this work was published in a cardiovascular and diabetes journal rather than a machine-learning journal is that the primary focus of our study is on the biological and clinical implications of our findings. The machine-learning aspect is a tool used to achieve our biological and medical goals. Our study contributes to the field of cardiovascular and diabetes research by providing new diagnostic and prognostic tools, which is the main reason for publishing in this specific journal.",
  "optimization/meta": "The model described in our study does not function as a meta-predictor. Instead, it employs multiple machine-learning algorithms independently to analyze the data. Specifically, six different models were utilized: Classification and Regression Trees (CART), Logistic Regression (LR), Linear Discriminant Analysis (LDA), K-Nearest Neighbors (KNN), Gaussian Na\u00efve Bayes (NB), and Support Vector Machine (SVM). Each of these models was applied to the datasets to determine which would yield the best predictions.\n\nThe training data for each model was processed independently, ensuring that the data used for training was not shared between models. This approach allowed for a comprehensive comparison of the models' performances. The results, including training and testing accuracies, were obtained through tenfold cross-validation, which helps in assessing the models' generalizability and robustness.\n\nThe SVM model received slight modifications, with the probability parameter set to \"true\" to provide probability estimates for each data point and a linear kernel used instead of the default Radial Basis Function (RBF) kernel. This adjustment was made to enhance the model's predictive capabilities. The other models were used with their default settings as provided by the scikit-learn library.\n\nIn summary, the study does not use a meta-predictor but rather evaluates the performance of individual machine-learning models on the same datasets. The independence of the training data for each model is maintained, ensuring a fair and unbiased comparison.",
  "optimization/encoding": "For the machine-learning algorithms employed in our study, data encoding and preprocessing were crucial steps to ensure the models could effectively learn from the input features. The datasets utilized included physiological/biochemical, genetic, and epigenetic data, which were combined into a single file for comprehensive analysis.\n\nInitially, the data was split into training and testing partitions. For binary classification, the data was divided into 80% training and 20% testing sets. Specific seed values were chosen to maintain consistent distributions in the testing sets, ensuring five diabetics and five non-diabetics were included. In multiple classification, the testing set was designed to include three diabetics, three non-diabetics, and four prediabetics. These seed values were selected to ensure that the resulting training and testing accuracies were similar, preventing overfitting and enhancing the model's generalizability.\n\nFeature selection was performed using Classification and Regression Trees (CART) analysis. For each trial, combinations of biomarkers from the datasets were utilized, and CART analysis was conducted five times. After each iteration, average feature importance, training accuracy, and testing accuracy were calculated. Features with an average importance of less than 1% were excluded from subsequent trials. This iterative process helped in identifying the most influential biomarkers.\n\nThe HbA1c feature was excluded from all trials due to its perfect accuracy in classifying diabetic status, which could bias the model. Instead, the derived accuracies were compared against the \"perfect\" accuracy obtained from HbA1c classification.\n\nTenfold cross-validation was implemented within the models to ensure robustness and to prevent overfitting. This technique involved dividing the data into ten subsets, training the model on nine subsets, and validating it on the remaining subset. This process was repeated ten times, with each subset serving as the validation set once.\n\nFor the Support Vector Machine (SVM) model, modifications were made by setting the probability parameter to \"true\" to provide probability estimates for each data point. A linear kernel was used instead of the default Radial Basis Function (RBF) kernel.\n\nThe preprocessing steps ensured that the data was in a suitable format for the machine-learning algorithms, enabling effective feature extraction and accurate model training. The use of tenfold cross-validation and careful feature selection helped in building robust models that could generalize well to new data.",
  "optimization/parameters": "In our study, we utilized a total of 345 features across all six models for both binary and multiple classification tasks. These features were derived from various datasets, including physiological/biochemical, genetic, and epigenetic data. The selection of these features was an iterative process. Initially, all available biomarkers were included in the first iteration of the Classification and Regression Trees (CART) analysis. Features with an average importance of less than 1% were excluded from subsequent trials. After all iterations, features with an average importance of less than or equal to 8% were categorized as \"other.\"\n\nThis feature selection process was repeated for other machine-learning frameworks, including Logistic Regression, Linear Discriminant Analysis, K-Nearest Neighbors Classifier, Gaussian Na\u00efve Bayes, and Support Vector Machine. The Support Vector Machine model underwent modifications, with the probability parameter set to \"true\" to provide probability estimates for each data point and a linear kernel used instead of the default Radial Basis Function kernel.\n\nThe final set of important features varied depending on the model and the classification task. For example, in binary classification, Logistic Regression identified features such as Complex III, Complex I, and various CpG sites as influential. In multiple classification, the same model highlighted different features, demonstrating the adaptability of the models to different classification schemes. The number of influential features shown in the summary plots was selected using the max display parameter, ensuring that the most relevant biomarkers were visualized.",
  "optimization/features": "In the optimization process, the analysis began with a comprehensive set of 345 features. These features were derived from various datasets, including physiological/biochemical, genetic, and epigenetic data. To enhance the model's performance and focus on the most relevant biomarkers, feature selection was performed. This involved iteratively evaluating the importance of each feature using Classification and Regression Trees (CART) analysis. Features with an average importance of less than 1% were excluded from subsequent trials. This process ensured that only the most influential features were retained.\n\nAfter multiple iterations, a refined set of 18 features was identified as the most significant for classifying diabetic status. These selected features were then used in further model training and evaluation. The feature selection process was conducted using the training set only, ensuring that the testing set remained independent and unbiased. This approach helped in identifying the key biomarkers that contributed most to the classification accuracy of the models.\n\nThe final models were trained and tested using this reduced set of 18 features, which included a mix of biomarkers from different categories. The importance of each feature within the models was quantified, allowing for a clear understanding of their influence on the classification outcomes. This systematic feature selection and optimization process was crucial in improving the models' predictive accuracy and reliability.",
  "optimization/fitting": "The study utilized a dataset consisting of 50 patients, which is relatively small. Given the number of features (345 total features) compared to the number of training points, there was a risk of overfitting the model. To mitigate this risk, several strategies were employed.\n\nFirstly, tenfold cross-validation was used. This technique ensures that the model is trained and tested on different subsets of the data, reducing the likelihood of overfitting to any single subset. By dividing the data into ten parts, the model was trained on nine parts and tested on the remaining one, repeating this process ten times with different partitions. This approach helps in assessing the model's performance more reliably and ensures that no single developed tree is composed solely of outliers or a group of patient data of one label type.\n\nAdditionally, seed values were chosen to provide an even patient distribution during model training and testing. This further helped in preventing the model from overfitting to specific subsets of the data.\n\nThe use of tenfold cross-validation and setting seed values allowed the derived models to generalize better and not overfit the training data. However, it is important to note that the small sample size limits the conclusions and predictions made by the machine-learning algorithms within the manuscript. Future investigations will need to validate specific features, including CpG24 of TFAM and global nuclear DNA methylation, with larger datasets to confirm the findings.\n\nRegarding underfitting, the study employed multiple machine-learning models, including Classification and Regression Trees (CART), Logistic Regression (LR), Linear Discriminant Analysis (LDA), K-Nearest Neighbors (KNN), Naive Bayes (NB), and Support Vector Machine (SVM). By comparing the performance of these different models, the study ensured that the chosen model was not too simplistic to capture the underlying patterns in the data. The CART model yielded the best results, indicating that it effectively balanced complexity and generalization. The other models' test/train accuracies were provided for comparison and support of the conclusions, ensuring that the chosen model was not underfitting the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting, given the relatively small sample size of 50 patients. One of the primary methods used was tenfold cross-validation. This technique ensures that the model is trained and tested on different subsets of the data, reducing the risk of the model becoming too tailored to the training data. By using tenfold cross-validation, we ensured that each fold of the data was used for both training and validation, providing a more robust evaluation of the model's performance.\n\nAdditionally, we set seed values to ensure an even distribution of patients during model training and testing. This step helped in maintaining consistency across different runs of the model, further mitigating the risk of overfitting.\n\nFor the machine-learning models, we utilized default settings from the scikit-learn library, which include built-in regularization techniques. For instance, the Support Vector Machine (SVM) model was modified to use a linear kernel instead of the default Radial Basis Function (RBF) kernel, which can help in reducing overfitting by simplifying the decision boundary.\n\nMoreover, we carefully selected features based on their importance. Features with an average importance of less than 1% were excluded from subsequent trials, and those with an average importance of less than or equal to 8% were categorized as \"other.\" This feature selection process helped in focusing on the most relevant biomarkers, thereby reducing the complexity of the model and preventing overfitting.\n\nOverall, these methods collectively helped in building a more generalized and reliable model, minimizing the risk of overfitting despite the small sample size.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are available and can be accessed through the provided code repository. The code for analyses, including the machine-learning models and their configurations, is hosted on GitHub. The repository contains the complete sequencing code and the scripts used for data analysis, which include the details of the hyper-parameters and optimization schedules employed.\n\nThe repository is publicly accessible and can be found at the following URL: https://github.com/qahathaway/WVU_Machine-Learning-50/tree/master. This repository includes all the necessary files and scripts to replicate the analyses conducted in our study. The code is provided under an open-source license, allowing researchers to use, modify, and distribute the code for their own research purposes.\n\nAdditionally, the datasets generated and analyzed during the current study, including sequencing files and computer code, are available from the corresponding author upon reasonable request. This ensures that other researchers can access the data and code to validate our findings or build upon our work.\n\nThe availability of these resources supports transparency and reproducibility in our research, enabling other scientists to understand and replicate our methods and results.",
  "model/interpretability": "The models employed in our study are not entirely black-box, as we utilized several techniques to enhance interpretability. One key method is the use of Classification and Regression Trees (CART), which inherently provides transparency by breaking down the decision-making process into a series of if-then statements based on feature values. This allows for a clear understanding of how each feature contributes to the final classification.\n\nAdditionally, we employed SHapley Additive exPlanations (SHAP) to further interpret the models. SHAP values provide a way to attribute the output of a machine learning model to its input features, making it possible to understand the impact of each feature on the model's predictions. For instance, SHAP plots visually explain how variables influence the classification of patients into non-diabetic and diabetic cohorts. These plots depict the effect of biomarker values on the model\u2019s output, relate specific biomarker values to model predictions, and show how chosen biomarkers depend on other tested biomarkers.\n\nFor the Naive Bayes (NB) model, feature influence was determined using the predict_log_proba function, which returns the most important biomarker per class. In the case of Logistic Regression (LR), Linear Discriminant Analysis (LDA), and Support Vector Machine (SVM) models, feature influence was determined by the magnitude of the model coefficients, providing a clear indication of which features are most important for classification.\n\nThe K-Nearest Neighbors (KNN) model, however, does not inherently provide feature importance due to its default restrictions. Despite this, the overall approach ensures that the majority of the models used are interpretable, offering insights into the decision-making process and the significance of various biomarkers.",
  "model/output": "The model employed in this study is primarily focused on classification tasks. Specifically, it utilizes Classification and Regression Trees (CART) to partition samples based on certain parameters, such as CpG methylation, to classify populations into categories like non-diabetic and diabetic. The model's performance is evaluated using metrics such as training and testing accuracies, F1 scores, and area under the curve (AUC) values, which are typical for classification problems. Additionally, the use of SHapley Additive exPlanations (SHAP) to explain the classification trees further supports the classification nature of the model. The model was applied to both binary and multiple classification schemes, indicating its versatility in handling different types of classification tasks. The important features extracted from the model, along with their influence values, are used to understand how different biomarkers contribute to the classification of diabetic status.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the machine-learning algorithms and bioinformatics scripts used in this study is publicly available. It can be accessed on GitHub at the repository named \"WVU_Machine-Learning-50\" under the master branch. The repository contains complete sequencing code and scripts for various analyses, including mitochondrial SNP sequencing and machine-learning model training. The code is provided to ensure reproducibility and to allow other researchers to utilize and build upon the methods developed in this study. The specific URL for the repository is https://github.com/qahathaway/WVU_Machine-Learning-50/tree/master. The code is released under a permissive license, allowing for broad use and modification by the research community.",
  "evaluation/method": "The evaluation method employed in our study involved a rigorous approach to ensure the robustness and generalizability of our models. We utilized tenfold cross-validation, which is a widely accepted technique in machine learning to assess model performance. This method involves dividing the dataset into ten subsets, training the model on nine of these subsets, and testing it on the remaining one. This process is repeated ten times, with each subset serving as the test set once, ensuring that every data point is used for both training and testing.\n\nTo maintain consistency and reproducibility, we used the same seed parameters across all models. This ensured that the random splits of the data were identical for each model, allowing for a fair comparison of their performances.\n\nFor each trial, we performed Classification and Regression Trees (CART) analysis five times. After each iteration, we calculated the average feature importance, average training accuracy, and average testing accuracy. Standard deviations were also computed over these five iterations to provide a measure of variability in the model's performance.\n\nWe evaluated six different machine-learning models: CART, Logistic Regression, Linear Discriminant Analysis, K-Nearest Neighbors Classifier, Gaussian Na\u00efve Bayes, and Support Vector Machine. The Support Vector Machine model received modifications, with the probability parameter set to \"true\" to provide probability estimates for each data point and a linear kernel used instead of the default Radial Basis Function kernel.\n\nThe performance of these models was assessed using several metrics, including training accuracy, testing accuracy, model average F1 score, and area under the curve (AUC). AUC values were provided for binary classification using the roc_auc_score function of scikit-learn. For the Logistic Regression and Support Vector Machine models, AUC was determined as 1.0 when evaluating the 345 total features due to the large sample size of biomarkers.\n\nAdditionally, we used SHapley Additive exPlanations (SHAP) to visually explain the classification trees developed for the patient samples. SHAP plots were generated using Jupyter Notebook with Python libraries, providing insights into how different biomarkers influence the model's predictions. This visual explanation helped in understanding the contribution of each feature to the model's output.\n\nIn summary, our evaluation method involved tenfold cross-validation, consistent seed parameters, multiple iterations of CART analysis, and the assessment of various performance metrics. This comprehensive approach ensured that our models were thoroughly evaluated and that the results were reliable and reproducible.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our machine-learning models in classifying diabetic status. For each model, we conducted five iterations of analysis and reported the averages for training accuracy, training standard deviation, testing accuracy, and testing standard deviation. These metrics provide a comprehensive view of the models' performance, both during training and when applied to unseen data.\n\nAdditionally, we calculated the model average F1 score, which is particularly useful for evaluating models on imbalanced datasets, as it considers both precision and recall. The area under the curve (AUC) was also determined for binary classification tasks, offering insight into the models' ability to distinguish between classes. It is worth noting that AUC values of 1.0 were observed for some models when evaluating all 345 features, likely due to the large sample size of biomarkers. However, these values were not observed when the feature set was restricted to the 18 most important features.\n\nThe reported metrics are in line with common practices in the literature, ensuring that our evaluation is representative and comparable to other studies in the field. The use of multiple metrics allows for a nuanced understanding of each model's strengths and weaknesses, facilitating informed decisions about their applicability in real-world scenarios.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of various machine-learning models to evaluate their performance in predicting diabetic status. We utilized several default scikit-learn machine-learning frameworks, including Logistic Regression, Linear Discriminant Analysis, K-Nearest Neighbors Classifier, Gaussian Na\u00efve Bayes, and Support Vector Machine, alongside Classification and Regression Trees (CART). Each model was assessed using tenfold cross-validation and consistent seed parameters to ensure reliability and comparability of the results.\n\nFor the Support Vector Machine model, we made a specific modification by setting the probability parameter to \"true\" to obtain probability estimates for each data point. Additionally, we used a linear kernel instead of the default Radial Basis Function (RBF) kernel. This adjustment was made to enhance the model's performance in providing probability estimates, which are crucial for classification tasks.\n\nThe performance of these models was evaluated based on several metrics, including training accuracy, training standard deviation, testing accuracy, testing standard deviation, model average F1 score, and area under the curve (AUC). These metrics were calculated for both binary and multiple classification of diabetic status.\n\nThe results indicated that CART yielded the best performance in terms of testing and training accuracies. The other models' accuracies were provided for comparison and to support our conclusions. This thorough comparison allowed us to identify the most effective model for predicting diabetic status using the available data.\n\nNot applicable.",
  "evaluation/confidence": "The evaluation of our machine-learning models included several performance metrics, and we ensured that these metrics were robust and statistically significant. For each model, we conducted five iterations of analysis, calculating average training and testing accuracies, as well as their standard deviations. This approach allowed us to assess the consistency and reliability of our models' performance.\n\nWe used tenfold cross-validation to further validate our results, ensuring that our models were not overfitting to the training data. This method helps in providing a more accurate estimate of model performance by evaluating it on multiple subsets of the data.\n\nStatistical significance was determined using a two-tailed Student\u2019s t-test or one-way analysis of variance (ANOVA), where appropriate. Tukey\u2019s multiple comparisons test was implemented following the ANOVA to derive significance between multiple groups. Differences between groups were considered statistically different if P \u2264 0.05. This rigorous statistical approach ensures that our claims about the superiority of certain models, such as CART, are well-founded.\n\nAdditionally, we provided confidence intervals for our performance metrics, such as training and testing accuracies, by reporting the standard deviations. This allows readers to understand the variability and reliability of our results. The use of SHapley Additive exPlanations (SHAP) also provided a visual and interpretable way to understand the influence of different biomarkers on the model's predictions, adding another layer of confidence in our evaluations.\n\nIn summary, our evaluation process included multiple iterations, cross-validation, statistical significance testing, and the reporting of confidence intervals, all of which contribute to the robustness and reliability of our findings.",
  "evaluation/availability": "The raw evaluation files are available for public access. The datasets generated and analyzed during the study, including sequencing files and computer code, can be found in the Sequence Read Archive under the accession number PRJNA520920. This data is accessible through the NCBI Data View portal. Additionally, the complete sequencing code and analysis code are provided on a public GitHub repository. The datasets and code are available for use under reasonable request, ensuring that researchers can replicate and build upon the findings presented in the study."
}