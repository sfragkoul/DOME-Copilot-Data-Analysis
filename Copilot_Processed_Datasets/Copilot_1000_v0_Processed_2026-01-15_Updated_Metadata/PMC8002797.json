{
  "publication/title": "Pre-stroke disability and stroke severity as predictors of discharge destination from an acute stroke ward.",
  "publication/authors": "de Berker H, de Berker A, Aung H, Duarte P, Mohammed S, Shetty H, Hughes T",
  "publication/journal": "Clinical medicine (London, England)",
  "publication/year": "2021",
  "publication/pmid": "33762385",
  "publication/pmcid": "PMC8002797",
  "publication/doi": "10.7861/clinmed.2020-0834",
  "publication/tags": "- Machine Learning\n- Stroke\n- Discharge Destination\n- Predictive Modeling\n- Clinical Decision-Making\n- Pre-stroke Disability\n- Stroke Severity\n- Random Forest Models\n- Healthcare Data Analysis\n- Patient Outcomes",
  "dataset/provenance": "The dataset used in this study was collected prospectively for the Sentinel Stroke National Audit Programme (SSNAP). It consists of data from 1,142 consecutive patients who were admitted to the acute stroke unit at the University Hospital of Wales between January 1, 2015, and December 31, 2016.\n\nThe data collected included stroke severity, measured using the National Institutes of Health Stroke Scale (NIHSS), pre-stroke disability, assessed using the modified Rankin Scale (mRS), and patient demographics such as age and gender. These variables were collected in the emergency department by health practitioners with specialist stroke training.\n\nThe dataset was divided into a training set of 1,016 patients and a test set of 115 patients. Both sets were matched for age, gender, mRS, and NIHSS to ensure comparability. The training set was used to develop the predictive models, while the test set was used to validate the models' accuracy.\n\nThe dataset has not been used in previous publications by the community, as this study is the first to utilize it for predicting stroke discharge destinations using machine learning models. The focus on pre-stroke disability and stroke severity, along with demographic factors, provides a comprehensive approach to understanding and predicting discharge outcomes.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a test set. The training set consisted of 1,016 data points, while the test set contained 115 data points. These splits were matched for key variables such as age, gender, modified Rankin Scale (mRS), and National Institutes of Health Stroke Scale (NIHSS) to ensure comparability.\n\nThe distribution of discharge destinations in both sets was similar. In the training set, 15.1% of patients were discharged home, 26.5% to a community hospital, 49.1% to inpatient rehabilitation, and 9.4% died. The test set had comparable percentages: 15.7% home, 26.1% community hospital, 48.7% inpatient rehabilitation, and 9.6% death. This matching ensured that the model's performance could be reliably validated on the test set.",
  "dataset/redundancy": "The dataset used in this study was collected prospectively from 1,142 consecutive patients admitted to the acute stroke unit at the University Hospital of Wales between January 1, 2015, and December 31, 2016. The data included stroke severity (baseline NIHSS), pre-stroke disability (pre-morbid mRS), and patient demographics such as age and gender.\n\nThe dataset was split into a training set and a test set. The training set consisted of 1,016 patients, while the test set consisted of 115 patients. These sets were matched for age, gender, mRS, and NIHSS to ensure that they were independent and comparable. This matching process helped to enforce the independence of the training and test sets, ensuring that the model's performance could be accurately validated.\n\nThe distribution of the training and test sets was carefully compared to ensure that there were no significant differences in patient characteristics or outcomes. This comparison showed that there was no difference between the train and test sets for any of the patient characteristics or outcomes, with a p-value of less than 0.05. This indicates that the datasets were well-balanced and that the model's performance on the test set could be considered a reliable indicator of its generalizability to new, unseen data.\n\nIn terms of comparison to previously published machine learning datasets, this study's approach to dataset splitting and matching is consistent with best practices in the field. The use of a matched training and test set helps to ensure that the model's performance is not overly optimistic and that it can be expected to generalize well to new data. This is particularly important in clinical settings, where the reliability and validity of predictive models are crucial for informing decision-making and improving patient outcomes.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is the random forest model. This is a well-established ensemble learning method that operates by constructing multiple decision trees during training and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n\nThe random forest algorithm is not new; it has been extensively used and studied in the machine learning community for many years. It was chosen for this study due to its robustness, ability to handle large datasets, and effectiveness in capturing complex relationships between predictors and outcomes.\n\nThe decision to use a random forest model in this clinical context, rather than publishing it in a machine-learning journal, is driven by the specific goals of the research. The primary aim was to develop a predictive model for stroke discharge destinations that clinicians can understand and trust. Random forests are known for their interpretability, which makes them suitable for clinical applications where transparency is crucial. Additionally, the focus of this study is on the clinical implications and practical applications of the model, rather than the novelty of the machine-learning algorithm itself.",
  "optimization/meta": "The model employed in this study is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on a single type of machine-learning method: random forest models. These models were chosen after an initial exploration that showed they provided the best performance on the validation set.\n\nThe random forest models were trained using a combination of input variables, including age, gender, pre-morbid modified Rankin Scale (mRS), and National Institutes of Health Stroke Scale (NIHSS). The data used for training and testing was collected prospectively from consecutive patients admitted to an acute stroke unit. The training set consisted of 1,016 patients, while the test set consisted of 115 patients. These sets were matched for age, gender, mRS, and NIHSS to ensure independence and avoid overfitting.\n\nThe performance of the models was validated against the test set, and the best-performing model achieved an accuracy of 70.4% on the test set. The independence of the training and test data is clear, as they were matched for key variables and derived from a prospective collection of patient data. This approach ensures that the model's predictions are based on independent and representative data, enhancing its reliability and generalizability.",
  "optimization/encoding": "The data used for the machine-learning algorithm was collected prospectively from consecutive patients admitted to an acute stroke unit. The key predictors gathered included stroke severity, measured using the National Institutes of Health Stroke Scale (NIHSS), and pre-stroke disability, assessed with the modified Rankin Scale (mRS). Additionally, patient demographics such as age and gender were recorded.\n\nThe data was split into a training set and a test set, both matched for age, gender, mRS, and NIHSS to ensure comparability. This splitting was crucial for validating the model's performance on unseen data.\n\nFor the machine-learning models, particularly the random forest models, the data was encoded in a way that allowed the algorithm to process and learn from the relationships between the predictors and the discharge destinations. The discharge destinations were categorized into four main outcomes: home, community hospital, inpatient rehabilitation, and death.\n\nThe models were trained using various combinations of input variables, including age, gender, mRS, and NIHSS. The performance of these models was evaluated using metrics such as accuracy, positive predictive value (PPV), and sensitivity. The best-performing model utilized all four predictors together, achieving an accuracy of 70.4% on the test set.\n\nTo avoid overfitting, hyperparameters were optimized through cross-validation within the training set. This ensured that the model generalized well to the test set and provided reliable predictions. The analyses were performed using Python, with scikit-learn for model specification and training, and eli5 for model analysis.",
  "optimization/parameters": "The model utilized four input parameters: age, gender, pre-stroke disability (measured by the modified Rankin Scale, mRS), and stroke severity (assessed by the National Institutes of Health Stroke Scale, NIHSS). These parameters were chosen based on their clinical relevance and availability at the time of admission.\n\nThe selection of these parameters was informed by initial exploratory analyses that indicated their potential to predict discharge destinations. Specifically, the mRS was found to be the strongest single predictor, while the combination of all four parameters yielded the best model performance. The choice of these parameters was also supported by previous studies that highlighted the importance of pre-stroke disability, stroke severity, age, and gender in predicting stroke outcomes.\n\nThe model's performance was optimized by training various random forest models with different combinations of these input parameters. The best-performing model, which included all four parameters, achieved an accuracy of 70.4% on the test set. This model was validated against an independent dataset, demonstrating its robustness and generalizability.",
  "optimization/features": "The model utilized four input features to predict discharge destinations for stroke patients. These features were pre-stroke disability, stroke severity on admission, age, and gender. Pre-stroke disability was measured using the modified Rankin Scale (mRS), while stroke severity was assessed using the National Institutes of Health Stroke Scale (NIHSS). The inclusion of age and gender was found to enhance the model's accuracy.\n\nFeature selection was implicitly performed by choosing these four predictors based on their clinical relevance and initial exploratory analysis. The selection process involved assessing the relationship between each predictor and the discharge destinations using statistical tests, such as the Kruskal-Wallis test. This ensured that the selected features had a significant and independent relationship with the outcomes.\n\nThe feature selection process was conducted using the training set only, adhering to best practices to avoid data leakage and ensure the model's generalizability. The training set consisted of data from 1,016 patients, while the test set, used for validation, included 115 patients. This approach helped in optimizing the model's performance and reliability.",
  "optimization/fitting": "The fitting method employed in this study utilized random forest models to predict stroke discharge destinations. The number of parameters in these models was indeed larger than the number of training points, which is a common scenario in machine learning to capture complex relationships in the data.\n\nTo rule out over-fitting, several strategies were implemented. Firstly, hyperparameters were optimized through cross-validation within the training set. This process involved training multiple models with different hyperparameter settings and selecting the best-performing model based on its performance on a validation subset of the training data. This ensured that the model generalized well to unseen data rather than memorizing the training set.\n\nAdditionally, the models were validated against an independent test set that was matched for key patient characteristics such as age, gender, pre-stroke disability (mRS), and stroke severity (NIHSS). The test set was not used during the training process, providing an unbiased evaluation of the model's performance. The final model achieved an accuracy of 70.4% on this test set, indicating good generalization.\n\nTo address under-fitting, various combinations of input variables were tested. Single predictors like age and NIHSS alone did not perform better than a dummy model, but combining pairs or triplets of predictors significantly improved performance. The best-performing model included all four predictors: age, gender, mRS, and NIHSS. This comprehensive approach ensured that the model captured the complex interactions between these variables, reducing the risk of under-fitting.\n\nFurthermore, the use of random forest models inherently helps in mitigating both over-fitting and under-fitting. Random forests are an ensemble of decision trees, each trained on a different subset of the data and using a different subset of features. This diversity in the ensemble helps in capturing a wide range of patterns in the data while averaging out the noise, leading to a more robust and generalizable model.",
  "optimization/regularization": "To prevent over-fitting, hyperparameters were optimized through cross-validation within the training set. This process involved evaluating multiple models to ensure that the best-performing model was selected based on its performance on the training data, rather than being overly tailored to the test set. By doing so, the risk of the model performing well on the training data but poorly on unseen data was minimized. Additionally, the use of random forest models inherently provides some regularization due to the ensemble nature of the method, which helps to reduce over-fitting by averaging the predictions of multiple decision trees.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are not explicitly detailed in the publication. However, the methods section outlines the general approach taken for model optimization. We utilized cross-validation within the training set to optimize hyperparameters, ensuring that the model was not overfitted to the test set. This process involved training a variety of random forest models with different combinations of input variables and selecting the best-performing model based on validation accuracy.\n\nThe specific model files and optimization parameters are not provided in the publication. The focus was on presenting the overall methodology and results rather than the detailed technical specifications of the models. The analyses were performed using Python, with scikit-learn for model specification and training, and eli5 for model analysis. These tools are widely available and can be accessed under open-source licenses, allowing for reproducibility of the methods described.\n\nFor those interested in replicating or building upon our work, the general approach and tools used are well-documented in the scientific community. The use of random forest models and the inclusion of variables such as age, gender, pre-stroke disability (mRS), and stroke severity (NIHSS) are clearly outlined, providing a foundation for further research and development in this area.",
  "model/interpretability": "The model developed in this study is not a black-box model. While random forest models can be complex, efforts have been made to enhance their interpretability. An interactive version of the analysis has been deployed online, allowing users to explore how different input values affect the model's outputs. This interactive tool helps users visualize where an individual patient lies in relation to the training data, thereby providing insight into the model's decision-making process. This approach enables clinicians to understand the rationale behind the model's predictions, fostering trust and confidence in its use. Additionally, the model's performance metrics, such as positive predictive value and sensitivity, are provided for each discharge destination, offering further transparency into the model's capabilities and limitations.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the discharge destination of stroke patients, which is a categorical outcome. The possible discharge destinations include home, community hospital, inpatient rehabilitation, and death. The model uses a random forest algorithm to classify patients into one of these categories based on input variables such as pre-stroke disability, stroke severity, age, and gender.\n\nThe performance of the model is evaluated using metrics such as positive predictive value (PPV) and sensitivity for each discharge destination. For instance, the model has a PPV of 0.88 for predicting discharge to home and a PPV of 0.68 for predicting further inpatient rehabilitation. These metrics indicate the proportion of patients predicted to have a certain outcome who actually have that outcome.\n\nThe model's accuracy is assessed on a test set, and it achieves an overall accuracy of 70.4% in predicting discharge destinations. This accuracy is significantly better than a dummy model that predicts the most common discharge destination for all patients, which achieves an accuracy of 50%.\n\nThe model's predictions provide additional information that can inform the discharge planning process. For example, the high PPV for discharge to home or further inpatient rehabilitation can help clinicians and the multidisciplinary team make more informed decisions about patient care. However, the model does not account for all situational and social determinants of discharge destination, and it should be used as an adjunct to clinical observations and knowledge.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the model is not publicly released. However, an interactive version of the analysis has been deployed online. This allows users to explore the effect of different input values on the model's outputs. The interactive tool can be accessed at a specific URL, enabling users to visualize where an individual patient lies in relation to the training data and thus understand the confidence in the predictions made by the model for that patient. This tool serves as a practical way to engage with the model's decision-making process without direct access to the underlying code.",
  "evaluation/method": "The evaluation method employed for this study involved a combination of cross-validation and testing on an independent dataset. Initially, a variety of random forest models were trained to predict patient discharge destinations using factors such as age, gender, pre-stroke disability (measured by the modified Rankin Scale, mRS), and stroke severity (measured by the National Institutes of Health Stroke Scale, NIHSS). These models were tested against a dummy model, which predicted the most common destination for all patients\u2014rehabilitation hospital. The dummy model achieved roughly 50% accuracy, serving as a baseline for comparison.\n\nTo ensure the robustness of the models, 100-fold cross-validation was performed. This process involved dividing the data into 100 subsets, training the model on 99 subsets, and validating it on the remaining subset. This procedure was repeated 100 times, each time using a different subset as the validation set. The resulting distribution of test accuracies was then analyzed using t-tests to determine statistical significance.\n\nHyperparameters were optimized through cross-validation within the training set to prevent overfitting. This step was crucial to ensure that the model generalized well to new, unseen data. The best-performing model, identified through this optimization process, was then evaluated on an independent test set. This test set was matched for age, gender, mRS, and NIHSS to the training set, ensuring a fair comparison.\n\nThe performance of the best model was assessed using a confusion matrix, which illustrated when the model was correct and which categories it tended to confuse. Additionally, the positive predictive value (PPV) and sensitivity were computed for each outcome. PPV indicates the proportion of patients predicted to have a particular outcome who actually had that outcome, while sensitivity measures the proportion of patients who truly had the outcome and were correctly predicted by the model.\n\nThe model's accuracy was found to be 70.4% on the test set, significantly outperforming the dummy model and other models that used fewer predictors. This evaluation method provided a comprehensive assessment of the model's performance, ensuring that it was both accurate and reliable for predicting stroke discharge destinations.",
  "evaluation/measure": "For the evaluation of our predictive model, we focused on several key performance metrics to ensure a comprehensive assessment of its effectiveness. The primary metrics reported include positive predictive value (PPV) and sensitivity for each discharge destination category. PPV, also known as precision in machine learning, indicates the proportion of patients predicted to have a particular outcome who actually have that outcome. Sensitivity, or recall, measures the proportion of patients who truly have a specific outcome and are correctly predicted by the model.\n\nThe PPV and sensitivity values for different discharge destinations were as follows: for death, PPV was 0.5 and sensitivity was 0.18; for community hospital, PPV was 0.70 and sensitivity was 0.53; for inpatient care, PPV was 0.68 and sensitivity was 0.88; and for home, PPV was 0.88 and sensitivity was 0.78. These metrics provide a clear picture of how well the model performs in predicting each specific outcome.\n\nAdditionally, we utilized a confusion matrix to illustrate the model's performance on the test set. This matrix shows when the model's predictions were correct (diagonal entries) and which categories it tends to confuse (off-diagonal entries). The confusion matrix reflects the ordering of the outcomes, highlighting that patients discharged to community hospital are never assigned to the death category, and vice versa. This detailed analysis helps in understanding the model's strengths and areas where it may need improvement.\n\nThe set of metrics reported is representative of standard practices in the literature, ensuring that our evaluation is both rigorous and comparable to other studies in the field. By focusing on PPV and sensitivity, we provide a thorough assessment of the model's predictive accuracy and reliability, which are crucial for clinical decision-making.",
  "evaluation/comparison": "In our evaluation, we compared our model's performance to a simpler baseline, referred to as the dummy model. This dummy model predicted the most common discharge destination for all patients, which was inpatient rehabilitation. This approach achieved an accuracy of 50%, as nearly half of the patients were discharged to inpatient rehabilitation. This baseline served as a reference point to assess the improvement provided by our models.\n\nWe also compared the performance of various machine learning models, including those utilizing single predictors and combinations of predictors. For instance, models using only the modified Rankin Scale (mRS) as a predictor showed significant improvement over the dummy model. However, the best performance was achieved by a random forest model that combined all four predictors: age, gender, mRS, and the National Institutes of Health Stroke Scale (NIHSS). This model outperformed all other models, including those with fewer predictors, demonstrating the value of integrating multiple factors in predicting discharge destinations.\n\nAdditionally, we validated our model using a confusion matrix, which illustrated the model's accuracy and the types of errors it made. This matrix helped us understand how often the model correctly predicted each discharge destination and which categories were most frequently confused. For example, outcomes that were similar in severity, such as home and community hospital, were more likely to be confused by the model.\n\nOverall, our comparison to simpler baselines and various model configurations highlighted the effectiveness of our chosen approach in predicting stroke discharge destinations.",
  "evaluation/confidence": "The evaluation of our model's performance included several statistical analyses to ensure the results were robust and reliable. We employed 100-fold cross-validation to assess the accuracy of our models, which helps in understanding the variability and stability of the model's performance. This method provides a more comprehensive evaluation compared to a single train-test split.\n\nTo determine statistical significance, we performed t-tests on the distribution of test accuracies. A p-value threshold of less than 0.05 was considered statistically significant. This approach allowed us to compare the performance of our models against a dummy model, which predicted the most common discharge destination for all patients. The dummy model achieved roughly 50% accuracy, serving as a baseline for comparison.\n\nOur best-performing model, which utilized all four predictors (age, gender, modified Rankin Scale, and National Institutes of Health Stroke Scale), achieved a test set accuracy of 70.4%. This model's performance was significantly better than any other model, with p-values ranging from 0.021 to 2.1 \u00d7 10^-26. These results indicate that the model's superior performance is statistically significant and not due to random chance.\n\nAdditionally, we computed positive predictive values (PPV) and sensitivity for each outcome category. For instance, the PPV for predicting discharge to home was 0.88, and for inpatient rehabilitation, it was 0.68. These metrics provide insight into the precision and recall of the model's predictions, further supporting its reliability.\n\nTo avoid overfitting, we optimized hyperparameters through cross-validation within the training set. This ensured that the model generalized well to unseen data, as evidenced by its performance on the test set.\n\nIn summary, the performance metrics of our model are supported by rigorous statistical analyses, including cross-validation and t-tests, which demonstrate the model's superiority over baselines and other models. The results are statistically significant, providing confidence in the model's predictive capabilities.",
  "evaluation/availability": "Not applicable."
}