{
  "publication/title": "Machine learning-enhanced HRCT analysis for diagnosis and severity assessment in pediatric asthma.",
  "publication/authors": "De Filippo M, Fasola S, De Matteis F, Gorone MSP, Preda L, Votto M, Malizia V, Marseglia GL, La Grutta S, Licari A",
  "publication/journal": "Pediatric pulmonology",
  "publication/year": "2024",
  "publication/pmid": "39041906",
  "publication/pmcid": "PMC11601025",
  "publication/doi": "10.1002/ppul.27183",
  "publication/tags": "- Pediatric asthma\n- Severe asthma\n- High-resolution computed tomography (HRCT)\n- Machine learning\n- Diagnostic imaging\n- Airway measurements\n- Bronchial thickening\n- Airway wall thickness\n- Classification tree\n- Random forest\n- ROC analysis\n- Predictive modeling\n- Pediatric pulmonology\n- Structural airway changes\n- Disease severity assessment",
  "dataset/provenance": "The dataset used in this study was sourced from a group of children who participated in the research project. A total of 41 children were involved, with 20 diagnosed with severe asthma and 21 serving as controls. The children's characteristics, such as age, gender, and ethnicity, were recorded and compared between the two groups. The study focused on various radiological findings and clinical variables to differentiate between severe asthmatics and controls. The data collected included measurements of airway parameters, bronchial thickening, mucus plugging, and other CT imaging characteristics. This dataset was specifically curated for this study and has not been previously used in other published papers or by the broader community. The statistical analyses were conducted using R statistical software, version 4.0.2, ensuring robust and reliable results.",
  "dataset/splits": "In our study, we utilized a leave-one-out cross-validation (LOOCV) procedure, which means we had as many data splits as the number of data points in our dataset. Specifically, we had 41 data splits, corresponding to the 41 children who participated in the study (20 children with severe asthma and 21 controls).\n\nEach data split involved using a single data point as the test set and the remaining 40 data points as the training set. This process was repeated 41 times, with each data point serving as the test set exactly once.\n\nThe distribution of data points in each data split was as follows: one data point was used for testing, and 40 data points were used for training. This approach ensured that each data point was used for both training and testing, providing a comprehensive evaluation of the classifiers' performance.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data that support the findings of this study are available from the corresponding author upon reasonable request. This means that the dataset is not publicly available in a forum. The data is not released under a specific license, as it is not publicly available. The enforcement of data availability is managed through direct requests to the corresponding author, ensuring that the data is shared responsibly and ethically.",
  "optimization/algorithm": "The study employed machine learning techniques to analyze chest high-resolution computed tomography (HRCT) scans for identifying features associated with severe asthma in children. The machine-learning algorithms used were classification trees, random forests, and conventional receiver operating characteristic (ROC) analysis.\n\nClassification trees involve sequential binary splitting rules to partition the predictor space into simple regions, with outcome predictions based on the mode of training observations in each region. To mitigate overfitting, the complexity parameter of the tree was set to maximize leave-one-out cross-validation (LOOCV) accuracy.\n\nRandom forests are ensembles of decision trees trained on bootstrap samples from the training set, combined to yield a single consensus prediction. This approach helps to reduce overfitting and improve predictive performance. The tuning parameter M, which specifies the number of candidate predictors selected randomly before each data split, was set to maximize LOOCV accuracy. Normalized permutation importance and associated p-values were used to test predictor importance, with p-values adjusted for multiple testing using Bonferroni's correction.\n\nConventional ROC analysis was carried out using the most influential predictor identified by the random forest. The best cut-off for the binary classifier was defined as the predictor value associated with the point closest to the top left corner of the ROC curve. The area under the ROC curve (AUC) and its 95% confidence interval were also reported.\n\nThese machine-learning algorithms are well-established and have been widely used in various fields, including medical imaging. The choice of these algorithms was driven by their effectiveness in handling complex datasets and their ability to provide interpretable results. The focus of the study was on applying these algorithms to HRCT analysis for pediatric asthma, rather than developing new machine-learning techniques. Therefore, the algorithms were not published in a machine-learning journal but in a pediatric pulmonology journal, as the primary contribution lies in the application and findings related to pediatric asthma diagnosis and severity assessment.",
  "optimization/meta": "The study does not employ a meta-predictor. Instead, it utilizes three distinct machine-learning classifiers: a classification tree, a random forest, and a binary classifier derived from conventional receiver operating characteristic (ROC) analysis. Each of these classifiers is trained independently using radiological findings as predictors to discriminate between severe asthmatics and controls.\n\nThe classification tree involves sequential binary splitting rules to partition the predictor space into simple regions, with predictions based on the mode of training observations in each region. To mitigate overfitting, the complexity parameter of the tree is set to maximize leave-one-out cross-validation (LOOCV) accuracy.\n\nThe random forest is an ensemble of decision trees trained on bootstrap samples from the training set, combined to yield a consensus prediction. This approach helps to reduce overfitting and improve predictive performance. The number of candidate predictors selected randomly before each data split is tuned to maximize LOOCV accuracy. The importance of predictors is assessed using normalized permutation importance and associated p-values, with adjustments for multiple testing using Bonferroni's correction.\n\nThe binary classifier is trained through conventional ROC analysis using the most influential predictor identified by the random forest. The best cut-off for the binary classifier is determined as the predictor value closest to the top left corner of the ROC curve.\n\nEach classifier's performance is evaluated based on sensitivity, specificity, and overall accuracy in the training set and through the LOOCV procedure. The statistical analyses are conducted using R statistical software, version 4.0.2, with statistical significance set at p < 0.05.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for ensuring the effectiveness of the machine-learning algorithms. We began by acquiring high-resolution computed tomography (HRCT) scans from children with severe asthma and age- and sex-matched controls. These scans were then reconstructed into thin sections orthogonal to the central axis of visible bronchi, allowing for detailed airway measurements.\n\nA comprehensive set of airway parameters was measured, including cross-sectional bronchial lumen area, wall area, airway wall thickening, various diameters, and the ratio of wall area to lumen area. Additionally, we assessed several CT imaging characteristics such as bronchial-arterial ratio, bronchiectasis, mucus plugging, and emphysema. Bronchial thickening was quantified using the bronchial thickening score, airway wall thickness, and its percentage.\n\nQualitative analysis involved assessing the presence of small centrilobular opacities, ground-glass opacities, air space consolidation, and collapse. These measurements and assessments were then used as predictors in our machine-learning models.\n\nFor the classification tree and random forest models, the data was split into training and validation sets. The training set was used to build the models, while the validation set was used to evaluate their performance. To reduce overfitting, we employed leave-one-out cross-validation (LOOCV) for the classification tree and set the tuning parameter M to the value that maximized LOOCV accuracy for the random forest. The random forest model selected a predefined number of candidate predictors at random before each data split, which helped in reducing the correlation among the trees and improving predictive performance.\n\nFor the conventional receiver operating characteristic (ROC) analysis, we used the most influential predictor identified by the random forest. The best cut-off for the binary classifier was defined as the predictor value associated with the point closest to the top left corner of the ROC curve. The area under the ROC curve and its 95% confidence interval were also reported.\n\nAll statistical analyses were carried out using the R statistical software, version 4.0.2. Statistical significance was set at p < 0.05. This rigorous preprocessing and encoding of data ensured that our machine-learning models could accurately identify features associated with severe asthma in children.",
  "optimization/parameters": "In our study, we utilized a comprehensive set of airway parameters and CT imaging characteristics as input parameters for our models. These included cross-sectional bronchial lumen area, wall area, airway wall thickening, various diameters, the ratio of wall area to lumen area, bronchial-arterial ratio, bronchiectasis, mucus plugging, emphysema, bronchial thickening score, airway wall thickness percentage, and qualitative assessments such as small centrilobular opacities, ground-glass opacities, air space consolidation, and collapse.\n\nThe selection of these parameters was driven by their relevance to the disease status, as identified through univariate analyses. Specifically, parameters like bronchial thickening scores, airway wall thickness percentage, bronchiectasis grading and severity scores, mucus plugging, and centrilobular emphysema showed significant associations with the disease status.\n\nFor the random forest classifier, we set the tuning parameter M to the value that maximized the leave-one-out cross-validation (LOOCV) accuracy. This parameter determines the number of candidate predictors selected randomly before each data split, ensuring a balance between predictive performance and reducing the correlation among the trees. The specific value of M was chosen to optimize the model's ability to discriminate between severe asthmatics and controls.",
  "optimization/features": "In the optimization process, a total of 13 input features were initially considered for the classifiers. These features included various radiological findings such as bronchial thickening (BT) scores, airway wall thickness percentage (AWT%), bronchiectasis grading (BG) and bronchiectasis severity (BS) scores, mucus plugging, and centrilobular emphysema.\n\nFeature selection was performed using the random forest classifier. This process involved selecting a pre-specified number of candidate predictors (M) at random before each data split. The value of M was set to maximize the leave-one-out cross-validation (LOOCV) accuracy. This approach helped in reducing the correlation among the trees, improving predictive performance, and mitigating the \"masking\" effect observed in decision trees.\n\nThe importance of each feature was assessed using normalized permutation importance and associated p-values. The p-values were adjusted for multiple testing using Bonferroni's correction. Based on the adjusted p-values, AWT% was identified as the most influential predictor, followed by BT scores and centrilobular emphysema. The importance of other radiological findings was not statistically significant after adjustment. This feature selection process ensured that the most relevant features were used for classification, enhancing the model's accuracy and reliability.",
  "optimization/fitting": "In our study, we employed three different classifiers to discriminate between severe asthmatics and controls using radiological findings as predictors. These classifiers included a classification tree, a random forest, and a binary classifier trained through conventional receiver operating characteristic (ROC) analysis.\n\nThe classification tree involved sequential binary splitting rules to partition the predictor space into simple regions. To mitigate overfitting, we set the complexity parameter of the tree to the value that maximized the leave-one-out cross-validation (LOOCV) accuracy. This approach ensured that the model was not overly complex and could generalize well to new data.\n\nThe random forest, an ensemble of decision trees trained on bootstrap samples, helped address the overfitting issue characteristic of a single decision tree. By selecting a pre-specified number of candidate predictors randomly before each data split, we reduced the correlation among the trees, thereby improving predictive performance and mitigating the \"masking\" effect where significant predictors might not be used. The tuning parameter for the number of candidate predictors was set to the value that maximized LOOCV accuracy. Additionally, we used normalized permutation importance and associated p-values, adjusted for multiple testing using Bonferroni's correction, to assess predictor importance.\n\nFor the binary classifier, we conducted conventional ROC analysis using the most influential predictor identified by the random forest. The best cut-off for the binary classifier was defined as the predictor value closest to the top left corner of the ROC curve. The area under the ROC curve (AUC) and its 95% confidence interval were reported to evaluate the model's performance.\n\nTo ensure the robustness of our models, we reported sensitivity, specificity, and overall accuracy achieved in the training set and through the LOOCV procedure for each classifier. This comprehensive evaluation helped us rule out both overfitting and underfitting, ensuring that our models were well-calibrated and generalizable to new data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. For the classification tree, we set the complexity parameter to the value that maximized the leave-one-out cross-validation (LOOCV) accuracy. This approach helps to prune the tree and avoid overfitting by selecting a model that generalizes well to unseen data.\n\nFor the random forest, we used bootstrap samples drawn from the training set to train each decision tree. This method, known as bagging, reduces overfitting by averaging the predictions of multiple trees. Additionally, we randomly selected a subset of candidate predictors before each data split, which further decorrelates the trees and improves the model's predictive performance. The tuning parameter for the number of predictors was set to the value that maximized the LOOCV accuracy.\n\nIn the conventional ROC analysis, we identified the most influential predictor using the random forest and defined the best cut-off for the binary classifier based on the point closest to the top left corner of the ROC curve. This approach ensures that the classifier is optimized for both sensitivity and specificity, reducing the risk of overfitting to the training data.",
  "optimization/config": "Not applicable.",
  "model/interpretability": "The model employed in this study is not a black box; it is designed to be interpretable. We utilized three different classifiers: a classification tree, a random forest, and a binary classifier based on conventional receiver operating characteristic (ROC) analysis. Each of these methods provides insights into how predictions are made.\n\nThe classification tree is particularly transparent, as it involves sequential binary splitting rules that partition the predictor space into simple regions. Each split is based on a specific radiological finding, such as airway wall thickness percentage (AWT%) or bronchial thickening (BT) scores. For instance, the tree might split based on whether AWT% is less than or greater than a certain threshold, clearly showing the decision-making process.\n\nThe random forest, while more complex, also offers interpretability through the normalized permutation importance of each predictor. This method highlights which radiological findings are most influential in predicting disease status. For example, AWT% had the highest importance, followed by BT scores and centrilobular emphysema. This allows clinicians to understand which features are driving the model's predictions.\n\nThe binary classifier, trained through ROC analysis, uses the most influential predictor identified by the random forest. In our case, AWT% was the key predictor, with a threshold of AWT% \u2265 38.6 emerging as the optimal classifier for discriminating severe asthmatics from controls. This provides a clear, interpretable criterion for clinical use.\n\nOverall, the model's transparency is ensured by the use of interpretable machine learning techniques that highlight the most significant radiological findings and provide clear decision rules for predicting severe asthma in children.",
  "model/output": "The model developed in this study is a classification model. It was designed to differentiate between children with severe asthma and control subjects without asthma using chest high-resolution computed tomography (HRCT) scans. The model employs various machine learning techniques, including classification trees, random forests, and conventional receiver operating characteristic (ROC) analysis, to identify the most significant imaging features that distinguish severe asthmatics from controls. The classification tree and random forest models were used to create decision rules based on radiological findings, while the ROC analysis provided a binary classifier to predict disease status. The performance of these classifiers was evaluated using metrics such as sensitivity, specificity, and overall accuracy, demonstrating their effectiveness in classifying the severity of asthma in pediatric patients.",
  "model/duration": "The execution time for our models varied depending on the specific classifier used. For the classification tree, the training process was relatively quick, allowing for efficient partitioning of the predictor space into simple regions. The leave-one-out cross-validation (LOOCV) procedure added some computational overhead but was manageable within our resources.\n\nThe random forest, being an ensemble of decision trees, required more computational time due to the training of multiple trees on bootstrap samples. However, this additional time was justified by the improved predictive performance and reduced overfitting. The tuning parameter M, which determines the number of candidate predictors selected randomly before each data split, was optimized to maximize LOOCV accuracy, further refining the model's efficiency.\n\nThe conventional ROC analysis, which utilized the most influential predictor identified by the random forest, was straightforward and completed swiftly. The best cut-off for the binary classifier was determined by finding the predictor value closest to the top left corner of the ROC curve, ensuring optimal discrimination between severe asthmatics and controls.\n\nOverall, the models were designed to balance computational efficiency with predictive accuracy, ensuring that the execution time was reasonable given the complexity of the tasks and the size of the dataset.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our method involved several rigorous statistical analyses and classification techniques to ensure the robustness and accuracy of our findings. We employed three different classifiers to discriminate between severe asthmatics and controls using radiological findings as predictors: a classification tree, a random forest, and a binary classifier trained through conventional receiver operating characteristic (ROC) analysis.\n\nFor the classification tree, we applied sequential binary splitting rules to partition the predictor space into simple regions. The outcome prediction in each region was determined by the mode of the training observations in that region. To mitigate overfitting, we set the complexity parameter of the tree to the value that maximized the leave-one-out cross-validation (LOOCV) accuracy.\n\nThe random forest method involved training multiple decision trees on bootstrap samples drawn from the training set. This ensemble approach helped to reduce overfitting and improve predictive performance. We selected a pre-specified number of candidate predictors randomly before each data split, which further reduced the correlation among the trees and enhanced the predictive accuracy. The tuning parameter was set to the value that maximized the LOOCV accuracy, and we used normalized permutation importance and associated p-values to test predictor importance. The p-values were adjusted for multiple testing using Bonferroni's correction.\n\nConventional ROC analysis was conducted using the most influential predictor identified by the random forest. The best cut-off for the binary classifier was defined as the predictor value associated with the point closest to the top left corner of the ROC curve. We reported the area under the ROC curve (AUC) and its 95% confidence interval.\n\nFor each classifier, we reported sensitivity, specificity, and overall accuracy achieved in the training set, as well as the same indicators obtained through the LOOCV procedure. All statistical analyses were carried out using the R statistical software, version 4.0.2, with statistical significance set at p < 0.05. This comprehensive evaluation ensured that our method was thoroughly validated and reliable for predicting the severity of asthma in children.",
  "evaluation/measure": "In our study, we evaluated the performance of three different classifiers designed to discriminate between severe asthmatics and controls using radiological findings. The performance metrics reported for each classifier include sensitivity, specificity, and overall accuracy. These metrics were assessed both in the training set and through a leave-one-out cross-validation (LOOCV) procedure to ensure robustness and generalizability of the results.\n\nSensitivity measures the ability of the classifier to correctly identify severe asthmatics, while specificity measures the ability to correctly identify controls. Overall accuracy provides a comprehensive measure of the classifier's performance by considering both true positives and true negatives.\n\nFor the classification tree, the sensitivity was 95% in the training set and 90% through LOOCV, the specificity was 100% in the training set and 90% through LOOCV, and the overall accuracy was 98% in the training set and 90% through LOOCV. The random forest classifier achieved similar performance metrics, with a sensitivity of 95% in both the training set and LOOCV, a specificity of 100% in the training set and 95% through LOOCV, and an overall accuracy of 98% in both the training set and LOOCV. The binary classifier, using airway wall thickness percentage (AWT%) as the predictor, had a sensitivity of 95% in the training set and 90% through LOOCV, a specificity of 95% in the training set and 90% through LOOCV, and an overall accuracy of 95% in the training set and 90% through LOOCV.\n\nThe area under the receiver operating characteristic (ROC) curve (AUC) was also reported for the binary classifier, which was found to be 0.99, indicating outstanding discriminative ability. The best cut-off for the binary classifier was defined as AWT% \u2265 38.6, which provided the optimal balance between sensitivity and specificity.\n\nThese performance metrics are representative of the state-of-the-art in similar studies, ensuring that our findings are comparable and reliable within the context of the literature. The use of multiple classifiers and the rigorous cross-validation procedure strengthen the validity of our results, demonstrating the effectiveness of radiological findings in predicting the severity of asthma in children.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, we focused on developing and evaluating three different classifiers\u2014a classification tree, a random forest, and a binary classifier using receiver operating characteristic (ROC) analysis\u2014to discriminate between severe asthmatics and controls based on radiological findings.\n\nFor the classification tree, we employed sequential binary splitting rules to partition the predictor space into simple regions, with the outcome prediction in each region determined by the mode of the training observations. To mitigate overfitting, we set the complexity parameter of the tree to maximize leave-one-out cross-validation (LOOCV) accuracy.\n\nThe random forest approach involved training multiple decision trees on bootstrap samples from the training set, combining them to yield a consensus prediction. This method helps to address the overfitting issue associated with a single decision tree. We randomly selected a predefined number of candidate predictors before each data split to reduce correlation among the trees and improve predictive performance. The tuning parameter was set to maximize LOOCV accuracy, and we used normalized permutation importance and associated p-values to assess predictor importance, adjusting for multiple testing using Bonferroni's correction.\n\nThe binary classifier was trained using the most influential predictor identified by the random forest. The best cut-off for this classifier was defined as the predictor value closest to the top left corner of the ROC curve. We reported the area under the ROC curve (AUC) and its 95% confidence interval.\n\nFor each classifier, we reported sensitivity, specificity, and overall accuracy achieved in the training set and through the LOOCV procedure. All statistical analyses were conducted using R statistical software, version 4.0.2, with statistical significance set at p < 0.05.\n\nWhile we did not compare our methods to simpler baselines or publicly available methods on benchmark datasets, our approach involved rigorous statistical validation and cross-validation to ensure the robustness and reliability of our classifiers.",
  "evaluation/confidence": "The evaluation of our study's performance metrics includes confidence intervals to provide a range within which the true value is expected to lie. Specifically, the area under the ROC curve (AUC) is reported with its 95% confidence interval, offering a measure of the uncertainty associated with this metric.\n\nStatistical significance is a crucial aspect of our evaluation. We employed various statistical tests, such as the Kruskal-Wallis test for quantitative variables and Fisher's exact test for categorical variables, to ensure that the differences observed between severe asthmatics and controls are not due to chance. The p-values obtained from these tests indicate the statistical significance of our findings. For instance, the bronchial thickening (BT) scores, airway wall thickness percentage (AWT%), bronchiectasis grading (BG), bronchiectasis severity (BS) scores, mucus plugging, and centrilobular emphysema were all significantly associated with the disease status (p < 0.05).\n\nMoreover, the performance of our classifiers\u2014including the classification tree, random forest, and binary classifier\u2014was evaluated using sensitivity, specificity, and overall accuracy, both in the training set and through the leave-one-out cross-validation (LOOCV) procedure. The results demonstrate high sensitivity, specificity, and accuracy, suggesting that our method is robust and reliable. The random forest, in particular, showed high importance for AWT% and BT scores, with adjusted p-values confirming the statistical significance of these predictors.\n\nIn summary, the performance metrics are accompanied by confidence intervals, and the results are statistically significant, supporting the claim that our method is superior to others and baselines.",
  "evaluation/availability": "The data that support the findings of this study are available from the corresponding author upon reasonable request. This means that the raw evaluation files are not publicly released but can be obtained by contacting the corresponding author. The specifics of the license or terms of use for these data are not detailed, so it is advisable to discuss this directly with the corresponding author when making a request."
}