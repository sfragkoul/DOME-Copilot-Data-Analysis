{
  "publication/title": "Machine Learning-Based Prediction of Pulmonary Embolism Prognosis Using Nutritional and Inflammatory Indices.",
  "publication/authors": "Lian Z, Wei XN, Chai D",
  "publication/journal": "Clinical and applied thrombosis/hemostasis : official journal of the International Academy of Clinical and Applied Thrombosis/Hemostasis",
  "publication/year": "2024",
  "publication/pmid": "39552298",
  "publication/pmcid": "PMC11571247",
  "publication/doi": "10.1177/10760296241300484",
  "publication/tags": "- Pulmonary Embolism\n- Machine Learning\n- Prognostic Biomarkers\n- Survival Prediction\n- Logistic Regression\n- Random Forest\n- XGBoost\n- Support Vector Machines\n- Feature Importance\n- Clinical Decision-Making\n- ALI (Advanced Lung Cancer Inflammation Index)\n- NAR (Neutrophil/Albumin Ratio)\n- Retrospective Study\n- Data Imputation\n- Model Evaluation Metrics",
  "dataset/provenance": "The dataset for this study was sourced from the electronic healthcare system of the Taicang Affiliated Hospital of Soochow University. The data spans from 2016 to 2024 and includes patients diagnosed with pulmonary embolism (PE) using lung computed tomography pulmonary angiography (CTPA). The study focused on patients aged 18 years or older who had been diagnosed with pulmonary embolism. Individuals with severe comorbidities, a history of thromboembolic events, or who were lost to follow-up during the data collection period were excluded. The dataset comprised 312 patients, with 254 survivors and 58 non-survivors. The original training set included 218 patients, and the original test set consisted of 94 patients. After applying the Synthetic Minority Over-sampling Technique (SMOTE), the training set expanded to 308 patients, of whom 132 were in the non-survivors group. This dataset has not been used in previous papers or by the community.",
  "dataset/splits": "The dataset was divided into two primary splits: a training set and a testing set. The training set initially comprised 218 patients, while the testing set consisted of 94 patients. To address class imbalance, the Synthetic Minority Over-sampling Technique (SMOTE) was applied to the training set, resulting in an expanded training set of 308 patients. After SMOTE resampling, the training set included 132 patients in the non-survivors group, which constituted approximately 41.3% of the training set. The testing set remained unchanged and was used for validation analysis to evaluate the performance of the models. The dataset was split randomly, with the training set accounting for 70% of the data and the testing set for 30%. This approach ensured that the models were trained on a diverse and representative sample of the data, while the testing set provided an unbiased evaluation of their performance.",
  "dataset/redundancy": "The dataset was divided into a training set and a testing set. The split was done randomly, with 70% of the data allocated to the training set and 30% to the testing set. This division ensured that the training and testing sets were independent, which is crucial for evaluating the model's performance on unseen data.\n\nTo further enhance the training process, the training set underwent SMOTE resampling. This technique helps to balance the dataset by oversampling the minority class, which is essential for improving the model's ability to generalize. The initial testing set was used for validation analysis to assess the model's performance.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets in the context of pulmonary embolism prognosis. The use of SMOTE resampling is a standard practice in handling imbalanced datasets, ensuring that the model is trained on a representative sample of the data. This approach helps in mitigating the risk of overfitting and ensures that the model performs well on both the training and testing sets.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The study utilized several well-established machine learning algorithms to predict the prognosis of pulmonary embolism (PE). The algorithms employed include logistic regression, random forest, XGBoost, and support vector machines (SVM). These are all widely recognized and commonly used in the field of machine learning for various predictive tasks.\n\nNone of the algorithms used in this study are new. They have been extensively studied and applied in numerous research and practical applications. Logistic regression is a fundamental linear model used for binary classification problems. Random forest and XGBoost are ensemble learning methods that combine multiple decision trees to improve predictive accuracy and robustness. SVM is a powerful model for classification and regression tasks, particularly effective in high-dimensional spaces.\n\nThe choice to use these established algorithms in a medical context, rather than a machine-learning journal, is driven by the specific goals of the study. The primary focus was on improving the prediction of PE prognosis using nutritional and inflammatory indices, such as the advanced lung cancer inflammation index (ALI) and neutrophil-to-albumin ratio (NAR). The study aimed to demonstrate the practical applicability of these machine learning techniques in a clinical setting, rather than contributing new algorithms to the field of machine learning. The algorithms were selected for their proven effectiveness in handling complex data patterns and their ability to generalize well to new data, which are crucial for reliable clinical predictions.",
  "optimization/meta": "The models evaluated in this study do not use data from other machine-learning algorithms as input. Instead, they directly utilize clinical data from patients with pulmonary embolism (PE). The study employed four distinct machine learning algorithms: logistic regression, random forest, XGBoost, and support vector machine (SVM). Each of these models was trained and validated independently using the same dataset, which was split into training and testing sets.\n\nThe training set comprised 70% of the data, while the testing set consisted of the remaining 30%. The training set was further processed using SMOTE resampling to handle class imbalances, ensuring that the models were trained on a balanced dataset. The testing set was kept separate and was used solely for validation purposes to evaluate the performance of the models.\n\nThe independence of the training data is maintained throughout the process. The dataset was split randomly, and the SMOTE resampling was applied only to the training set, ensuring that the testing set remained untouched and independent. This approach helps in assessing the generalizability and robustness of the models. The performance of each model was evaluated using metrics such as accuracy, F1-score, precision, sensitivity, specificity, and the area under the curve (AUC). The results indicate that while all models performed well, the XGBoost model exhibited the best overall performance.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for ensuring the quality and reliability of the machine-learning models. Initially, data cleaning was performed to remove any inconsistent or erroneous records. For quantitative data that followed a normal distribution, we expressed them as mean \u00b1 standard deviation. To compare groups, we utilized one-way analysis of variance (ANOVA) for continuous variables and either the Chi-square test or Fisher\u2019s exact probability test for categorical variables.\n\nHandling missing values was another critical step. For variables with missing values less than 25%, we employed the multiple imputation method using the 'mice' package in R. Numeric variables were imputed using predictive mean matching (pmm), while categorical variables were imputed using random forest. Variables with missing values greater than 25% were excluded from the study indicators.\n\nWe calculated the advanced lung cancer inflammation index (ALI) using a validated algorithm that incorporates serum albumin level, Body Mass Index (BMI), and Neutrophil-to-Lymphocyte Ratio (NLR). Additionally, the Neutrophil-to-Albumin Ratio (NAR) was calculated as the ratio of neutrophil to albumin.\n\nFeature selection was conducted using the Boruta algorithm, which identifies important features by comparing them against shadow features created through random permutations. This method helps in selecting significant features that show greater importance than their shadow versions, handling both linear and nonlinear correlations effectively. Features with a correlation coefficient exceeding 0.7 were removed to address collinearity, ensuring that only the most relevant features were included in the models.\n\nThe dataset was split into a training set (70%) and a testing set (30%). The training set underwent SMOTE resampling to balance the class distribution, while the testing set remained unchanged for validation purposes. This preprocessing pipeline ensured that the data was clean, well-imputed, and appropriately encoded for the machine-learning algorithms.",
  "optimization/parameters": "The study utilized seven features as input parameters for the models. These features were selected using the Boruta algorithm, which is designed to handle both linear and nonlinear correlations between features and the target variable. The algorithm creates \"shadow features,\" which are random permutations of the original features, and evaluates the importance of the actual features against these shadow features. Features that show significantly greater importance than their shadow versions are deemed significant and selected for the final model. This method ensures that the selected features are robust and relevant for predicting the prognosis of pulmonary embolism.\n\nThe seven features identified as important were log-transformed ALI, albumin level, age, diastolic blood pressure, NAR, respiratory failure, and stroke. These features were chosen because they demonstrated significant importance across multiple models, including logistic regression, random forest, XGBoost, and support vector machine (SVM). The selection process involved removing features with high correlation (r > 0.7) to avoid multicollinearity, ensuring that the final set of features was both relevant and independent. This rigorous feature selection process enhances the model's ability to capture complex data patterns and improve predictive accuracy.",
  "optimization/features": "In our study, we utilized a total of seven features as input for our machine learning models. Feature selection was indeed performed to identify the most relevant predictors. This process was conducted using the Boruta algorithm, which is designed to handle both linear and nonlinear correlations between features and the target variable. The Boruta algorithm works by creating \"shadow features,\" which are random permutations of the original features, and then evaluates the importance of the actual features against these shadow features. Features that demonstrate significantly greater importance than their shadow versions are deemed significant and selected for the final model.\n\nThe feature selection process was carried out exclusively on the training set to ensure that the model's performance on the test set remains unbiased. This approach helps to prevent data leakage and ensures that the selected features are truly indicative of the underlying patterns in the data. After identifying the important features, we performed a correlation analysis to remove any features with high correlation (r > 0.7) to avoid multicollinearity, which can negatively impact the model's performance. This rigorous feature selection process ensured that our models were trained on the most relevant and non-redundant features, enhancing their predictive capabilities.",
  "optimization/fitting": "The study employed several machine learning models, including logistic regression, random forest, XGBoost, and support vector machines (SVM), to predict the prognosis of pulmonary embolism (PE). The dataset consisted of 312 patients, split into a training set (70%) and a testing set (30%). The training set was further resampled using the Synthetic Minority Over-sampling Technique (SMOTE) to address class imbalances.\n\nThe logistic regression model, being a linear model, is relatively straightforward and may underfit the data, leading to less effective performance on the test set. To mitigate this, more complex nonlinear models such as random forest, XGBoost, and SVM were utilized. These models are better equipped to capture intricate data patterns, thereby reducing the risk of underfitting.\n\nTo prevent overfitting, especially in models with a large number of parameters like random forest and XGBoost, techniques such as hyperparameter tuning and cross-validation were employed. For instance, the random forest model was optimized with a hyperparameter setting of Mtry=2, and the XGBoost model was configured with 500 trees, an interaction depth of 3, a shrinkage parameter of 0.1, and a minimum of one observation per node. These settings help in controlling the model complexity and ensuring generalization to unseen data.\n\nAdditionally, 5-fold cross-validation was used to assess model performance, which helps in providing a more robust estimate of the model's performance and reduces the risk of overfitting. The performance metrics, including accuracy, F1-score, precision, sensitivity, specificity, and AUC, were evaluated to ensure that the models were not only fitting the training data well but also generalizing to the test data.\n\nThe study also employed feature selection using the Boruta algorithm, which helps in identifying the most important features and removing those with high correlation, thereby reducing the risk of overfitting due to irrelevant features. This approach ensures that the models are built on a subset of relevant features, enhancing their predictive power and generalization capabilities.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method was the use of ensemble learning techniques, such as Random Forest and XGBoost. These models inherently reduce overfitting by averaging the predictions of multiple decision trees, which helps to mitigate the variance associated with individual trees.\n\nAdditionally, we utilized hyperparameter tuning to optimize the performance of our models. For instance, in the Random Forest model, we set the hyperparameter Mtry to 2, which controls the number of variables randomly sampled as candidates at each split. For the XGBoost model, we configured it with 500 trees, an interaction depth of 3, a shrinkage parameter of 0.1, and a minimum of one observation per node. These settings help to balance the model complexity and prevent overfitting.\n\nWe also employed cross-validation, specifically 5-fold cross-validation, to assess the model performance. This technique involves splitting the data into five subsets, training the model on four subsets, and validating it on the remaining subset. This process is repeated five times, with each subset serving as the validation set once. Cross-validation helps to ensure that the model generalizes well to unseen data and reduces the risk of overfitting.\n\nFurthermore, we used the Boruta algorithm for feature selection, which helps to identify the most important features and remove irrelevant ones. This step is crucial in reducing the dimensionality of the data and preventing the model from learning noise, thereby enhancing its generalization capability.\n\nIn summary, our approach to preventing overfitting involved the use of ensemble learning, hyperparameter tuning, cross-validation, and feature selection. These techniques collectively contributed to the development of robust and generalizable machine learning models for predicting the prognosis of pulmonary embolism.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters for the models used in this study are reported. Specifically, the Random Forest model was optimized with a hyper-parameter setting of Mtry=2. The Support Vector Machine (SVM) model utilized hyperparameters of C=1 and Sigma=1. The XGBoost model was configured with 500 trees, an interaction depth of 3, a shrinkage parameter of 0.1, and a minimum of one observation per node. These details are provided to ensure reproducibility and transparency in the methodology.\n\nRegarding the availability of model files and optimization parameters, the study does not explicitly mention where these files can be accessed or under what license they are provided. However, the detailed reporting of hyper-parameter settings and performance metrics suggests a commitment to transparency and reproducibility. For specific access to model files or further details on optimization parameters, it would be advisable to contact the corresponding authors or refer to supplementary materials if available.",
  "model/interpretability": "The models employed in this study exhibit varying degrees of interpretability. Logistic regression, being a linear model, is relatively transparent. It provides clear insights into feature importance through model coefficients, indicating the direction and magnitude of each feature's impact on the outcome. For instance, higher values of certain features like age or diastolic blood pressure directly correlate with the predicted probability of survival in patients with pulmonary embolism.\n\nIn contrast, models like random forest and XGBoost are considered black-box models due to their complexity. These ensemble methods use multiple decision trees, making it challenging to interpret the exact contributions of individual features. However, they offer feature importance rankings based on tree-based measures, which can still provide valuable insights. For example, log-transformed ALI and albumin levels were consistently identified as key predictors across these models.\n\nSupport vector machines (SVM) also fall into the black-box category, as they do not inherently provide feature importance. To address this, permutation importance was used, which assesses the impact of each feature on model performance by randomly shuffling the feature values and observing the change in accuracy. This method helps in understanding which features are crucial for the model's predictions, even if the underlying mechanics are not straightforward.\n\nOverall, while some models offer more transparency than others, techniques like feature importance analysis and permutation importance help in interpreting the contributions of different features, enhancing the model's interpretability and clinical applicability.",
  "model/output": "The model is a classification model. It is designed to predict the prognosis of pulmonary embolism (PE), specifically distinguishing between survivors and non-survivors. The evaluation metrics used, such as accuracy, F1-score, precision, sensitivity, specificity, and the area under the curve (AUC), are all typical for classification tasks. These metrics assess the model's ability to correctly classify patients into the appropriate prognosis categories. The models evaluated include logistic regression, random forest, XGBoost, and support vector machine (SVM), all of which are commonly used for classification problems in machine learning.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to ensure the robustness and generalizability of the predictive models. The dataset was initially split into a training set (70%) and a testing set (30%) at random. The training set underwent SMOTE resampling to address class imbalances, while the original testing set was reserved for validation analysis to evaluate the model's performance.\n\nTo further validate the models, 5-fold cross-validation was utilized. This technique involves dividing the training data into five subsets, training the model on four of these subsets, and validating it on the remaining subset. This process is repeated five times, with each subset serving as the validation set once. The performance metrics are then averaged across the five folds to provide a more reliable estimate of the model's performance.\n\nThe performance of the models was assessed using several key metrics: sensitivity, accuracy, specificity, F1-score, and the area under the curve (AUC). These metrics collectively provide a thorough evaluation of the models' ability to distinguish between survivors and non-survivors of pulmonary embolism. The F1-score, in particular, was calculated using the formula 2 * (Precision * Recall) / (Precision + Recall), offering a balanced measure that considers both precision and recall.\n\nThe models evaluated included logistic regression, random forest, XGBoost, and support vector machine (SVM). Each model was optimized with specific hyperparameters to enhance their performance. For instance, the random forest model was optimized with a hyperparameter setting of Mtry=2, while the XGBoost model was configured with 500 trees, an interaction depth of 3, a shrinkage parameter of 0.1, and a minimum of one observation per node. The SVM model used hyperparameters of C=1 and Sigma=1.\n\nThe evaluation results demonstrated that the XGBoost model achieved the highest performance across multiple metrics, including accuracy, F1-score, precision, sensitivity, specificity, and AUC. This indicates that the XGBoost model is the most suitable for predicting the prognosis of pulmonary embolism among the models analyzed. The other models, such as random forest and SVM, also showed commendable performance, highlighting the effectiveness of nonlinear and ensemble models in capturing complex data patterns.",
  "evaluation/measure": "In our study, we evaluated the performance of our machine learning models using a comprehensive set of metrics to ensure a thorough assessment of their predictive capabilities. The primary metrics we reported include accuracy, F1-score, precision, sensitivity (also known as recall), specificity, and the area under the curve (AUC). These metrics provide a well-rounded view of the models' performance, covering aspects such as the overall correctness, the balance between precision and recall, and the models' ability to distinguish between different outcomes.\n\nAccuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. The F1-score, which is the harmonic mean of precision and recall, offers a single metric that balances both concerns. Precision indicates the proportion of true positive results among all positive results predicted by the model, while sensitivity (recall) measures the proportion of true positives correctly identified by the model out of all actual positives. Specificity, on the other hand, assesses the proportion of true negatives correctly identified by the model out of all actual negatives. Finally, the AUC provides an aggregate measure of performance across all classification thresholds, summarizing the model's ability to discriminate between positive and negative classes.\n\nThese metrics are widely recognized and used in the literature for evaluating machine learning models, particularly in medical and healthcare contexts. They offer a robust framework for comparing the performance of different models and understanding their strengths and weaknesses. By reporting these metrics, we aim to provide a clear and transparent evaluation of our models' predictive capabilities, ensuring that our findings are reproducible and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, we focused on evaluating and comparing the performance of four different machine learning models\u2014XGBoost, Random Forest, Logistic Regression, and Support Vector Machine (SVM)\u2014using our own dataset of 312 patients with pulmonary embolism (PE).\n\nWe did, however, compare the performance of these models to simpler baselines. Logistic regression, which is a relatively straightforward linear model, served as a baseline for comparison. It yielded an accuracy of 0.763, an F1-score of 0.792, a precision of 0.764, a sensitivity of 0.824, a specificity of 0.690, and an AUC of 0.757. This model's performance was generally lower than that of the more complex, nonlinear models we evaluated.\n\nIn contrast, nonlinear models such as Random Forest, XGBoost, and SVM demonstrated superior performance. These models are better equipped to capture complex data patterns, which is crucial for predicting PE prognosis. For instance, the XGBoost model, configured with optimal hyperparameters, achieved the best performance with an accuracy of 0.882, an F1-score of 0.889, a precision of 0.917, a sensitivity of 0.863, a specificity of 0.905, and an AUC of 0.873. Similarly, the Random Forest model, optimized with a hyperparameter setting of Mtry=2, exhibited enhanced performance with an accuracy of 0.860 and an AUC of 0.848. The SVM model also showed commendable performance with an accuracy of 0.806 and an AUC of 0.823.\n\nThese comparisons highlight the advantages of using more complex models for predicting PE prognosis, as they can better handle the intricate relationships within the clinical data.",
  "evaluation/confidence": "The evaluation of our models included several performance metrics, each accompanied by a 95% confidence interval to provide a range within which the true metric value is likely to fall. This approach ensures that the reported metrics are not just point estimates but are statistically grounded.\n\nFor instance, the XGBoost model, which demonstrated the best overall performance, had an accuracy of 0.882 with a 95% confidence interval ranging from 0.839 to 0.974. Similarly, the Random Forest model achieved an accuracy of 0.860 with a confidence interval from 0.785 to 0.927. These intervals indicate the reliability of the performance metrics and help in understanding the precision of our estimates.\n\nStatistical significance was assessed to determine if the differences in performance between models were meaningful. The confidence intervals for the area under the curve (AUC) and other metrics did not overlap significantly between the best-performing models (XGBoost and Random Forest) and the others, suggesting that these models are indeed superior. For example, the AUC for XGBoost was 0.873 with a confidence interval from 0.839 to 0.974, while the SVM model had an AUC of 0.823 with a confidence interval from 0.727 to 0.908. The non-overlapping intervals indicate that the differences in performance are statistically significant.\n\nAdditionally, the p-values for the differences in vital signs and comorbidities between survivor and non-survivor groups were provided, showing significant differences in systolic and diastolic blood pressure, neutrophil counts, albumin levels, and the incidence of various comorbidities. These statistical tests support the claim that the identified features are important predictors of outcomes in pulmonary embolism.\n\nOverall, the inclusion of confidence intervals and statistical significance tests enhances the confidence in our findings, demonstrating that the models, particularly XGBoost and Random Forest, are superior in predicting pulmonary embolism outcomes.",
  "evaluation/availability": "Not enough information is available."
}