{
  "publication/title": "A Deep Batch Normalized Convolution Approach for Improving COVID-19 Detection from Chest X-ray Images.",
  "publication/authors": "Al-Shourbaji I, Kachare PH, Abualigah L, Abdelhag ME, Elnaim B, Anter AM, Gandomi AH",
  "publication/journal": "Pathogens (Basel, Switzerland)",
  "publication/year": "2022",
  "publication/pmid": "36678365",
  "publication/pmcid": "PMC9860560",
  "publication/doi": "10.3390/pathogens12010017",
  "publication/tags": "- Chest X-ray\n- COVID-19\n- Deep learning\n- Batch normalized convolutional neural network (BNCNN)\n- Classification\n- Machine learning\n- Medical imaging\n- Neural networks\n- Computational complexity\n- Model performance",
  "dataset/provenance": "The dataset used in our study consists of chest X-ray images collected by researchers from the University of Doha and the University of Dhaka. This dataset is publicly available on Kaggle, along with its metadata. It includes three classes of images: COVID-19, Normal, and Viral Pneumonia. The dataset is partitioned into three subsets: training (80%), validation (10%), and testing (10%). Specifically, the training subset contains 2,892 COVID-19 images, 8,153 Normal images, and 1,076 Viral Pneumonia images, totaling 12,121 images. The validation subset includes 362 COVID-19 images, 1,019 Normal images, and 134 Viral Pneumonia images, summing up to 1,514 images. The testing subset comprises 362 COVID-19 images, 1,020 Normal images, and 135 Viral Pneumonia images, making a total of 1,518 images. Overall, the dataset contains 3,616 COVID-19 images, 10,192 Normal images, and 1,345 Viral Pneumonia images, amounting to 15,153 images in total. This dataset has been utilized in previous research and by the community for developing and evaluating models for COVID-19 detection from X-ray images.",
  "dataset/splits": "The dataset used in this study is divided into three distinct splits: training, validation, and testing. Each split contains a specific number of data points distributed across three classes: COVID-19, Normal, and Viral Pneumonia.\n\nThe training split comprises 2,892 COVID-19 images, 8,153 Normal images, and 1,076 Viral Pneumonia images, totaling 12,121 images. The validation split includes 362 COVID-19 images, 1,019 Normal images, and 134 Viral Pneumonia images, summing up to 1,514 images. The testing split consists of 362 COVID-19 images, 1,020 Normal images, and 135 Viral Pneumonia images, making a total of 1,517 images.\n\nIn summary, the dataset is evenly distributed across the three splits, with each split containing a representative number of images from each class. This distribution ensures that the models are trained, validated, and tested on a balanced and diverse set of images, which is crucial for evaluating their performance accurately.",
  "dataset/redundancy": "The dataset used in this study comprises chest X-ray images categorized into three classes: COVID-19, Normal, and Viral Pneumonia. To ensure robust evaluation, the dataset was partitioned into three mutually exclusive and exhaustive subsets: training, validation, and testing. The training set constitutes 80% of the dataset, the validation set 10%, and the testing set the remaining 10%. This partitioning ensures that the training and test sets are independent, which is crucial for unbiased evaluation of the model's performance.\n\nTo enforce the independence of the training and test sets, the dataset was carefully split such that no image appears in more than one subset. This approach prevents data leakage, where information from the test set might inadvertently influence the training process, leading to overoptimistic performance estimates.\n\nThe distribution of the dataset across the three classes is as follows: the training set includes 2892 COVID-19 images, 8153 Normal images, and 1076 Viral Pneumonia images. The validation set comprises 362 COVID-19 images, 1019 Normal images, and 134 Viral Pneumonia images. The testing set contains 362 COVID-19 images, 1020 Normal images, and 135 Viral Pneumonia images. This distribution ensures a balanced representation of each class in all subsets, which is essential for training a model that generalizes well to unseen data.\n\nCompared to previously published machine learning datasets for COVID-19 detection, this dataset is notable for its comprehensive coverage of different chest conditions. The inclusion of Viral Pneumonia images, in addition to COVID-19 and Normal images, provides a more challenging and realistic scenario for model evaluation. This is particularly important given the similarities in symptoms and radiographic appearances between COVID-19 and other forms of pneumonia. The dataset's size and diversity make it a valuable resource for developing and validating machine learning models for COVID-19 detection.",
  "dataset/availability": "The dataset used in this study is publicly available. It was collected by researchers from the University of Doha and the University of Dhaka and can be accessed on the Kaggle platform. This dataset includes chest X-ray images categorized into three classes: COVID-19, Normal, and Viral Pneumonia. The dataset is partitioned into three subsets: training (80%), validation (10%), and testing (10%). These subsets are mutually exclusive and exhaustive, ensuring that each image is used only in one of the subsets. The dataset is available with metadata, which provides additional information about the images. The public availability of the dataset allows for reproducibility and further research by other scientists and practitioners in the field.",
  "optimization/algorithm": "The optimization algorithm used in our study is the Adaptive Moment Estimation (Adam) optimizer. This optimizer is widely recognized for its efficiency in training deep learning models. It combines the advantages of two other extensions of stochastic gradient descent. Specifically, Adam computes adaptive learning rates for each parameter, which helps in faster convergence and better performance.\n\nThe machine-learning algorithm class used is a convolutional neural network (CNN), specifically a novel model named BNCNN. This model incorporates batch normalization and dropout layers to enhance generalization and prevent overfitting. The BNCNN model is designed to classify chest X-ray images into categories such as COVID-19, Normal, and Viral Pneumonia.\n\nThe BNCNN model is indeed a new algorithm. It was developed to address the limitations of existing pre-trained models like VGG-16, VGG-19, Inception-V3, and ResNet-50, which often require large-scale datasets and have a high computational complexity. The BNCNN model aims to reduce these requirements while maintaining high accuracy in COVID-19 detection.\n\nThe reason the BNCNN model was not published in a machine-learning journal is that the primary focus of this research is on its application in medical imaging, particularly in the context of COVID-19 detection. The model's development and evaluation are presented in a journal that specializes in pathogens and infectious diseases, where the practical implications of the model for healthcare are of primary interest. The model's design and performance are thoroughly discussed in the context of medical diagnosis, highlighting its potential to assist radiologists and healthcare professionals in detecting COVID-19 from X-ray images.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure optimal performance. The dataset consisted of chest X-ray images categorized into three classes: COVID-19, Normal, and Viral Pneumonia. Each X-ray image was resized to 150 \u00d7 150 pixels, which reduced the input dimension and the number of trainable and non-trainable weights in the model. This resizing differed from the 224 \u00d7 224 pixel size used in VGG models, aiming to decrease computational complexity.\n\nNormalization was applied by dividing each pixel value by 255, resulting in images with values ranging from 0 to 1. This normalization helped in facilitating weight learning by avoiding issues like vanishing and exploding gradients.\n\nData augmentation techniques were employed to simulate real-life scenarios and prevent overfitting. These techniques included rotations ranging from -10 to 10 degrees, zooming from 0 to 10%, shearing from 0 to 10%, horizontal and vertical strides from 0 to 10%, and horizontal flipping. Vertical flipping was avoided to maintain the natural orientation of chest X-ray images, which is easily identifiable by users. Any pixel values that became unavailable due to augmentation were replaced by the nearest available pixel values.\n\nClass labels were encoded using one-hot encoding, converting each label into a 3D vector with all zeros except for the position corresponding to the image class. This encoding method ensured that the order of classes in the dimensions did not affect the classifier's performance. The dataset was partitioned into training, validation, and testing subsets, with 80%, 10%, and 10% of the data respectively. This partitioning allowed for comprehensive training, validation, and testing of the model.",
  "optimization/parameters": "In the proposed BNCNN model, the total number of parameters is approximately 2,787,683 for three-way classification and 2,787,554 for two-way classification. These parameters are distributed across various layers, including convolutional layers, batch normalization layers, dense layers, and dropout layers.\n\nThe selection of the number of parameters was guided by the need to balance model complexity and performance. The convolutional layers, which are responsible for feature extraction, have a varying number of parameters depending on the output shape and the number of filters used. For instance, the first convolutional layer has 448 trainable parameters, while the last one before the flattening layer has 73,856 trainable parameters. This increase in parameters allows the model to capture more complex features as the data progresses through the network.\n\nBatch normalization layers, which help in stabilizing and accelerating the training process, contribute a small number of trainable and non-trainable parameters. Each batch normalization layer adds 32 trainable parameters and 32 non-trainable parameters for the first convolutional layer, increasing to 256 trainable and 256 non-trainable parameters for the last convolutional layer before flattening.\n\nThe dense layers, which are part of the classifier, have a significant number of parameters. The first dense layer has 2,654,464 trainable parameters, and the second dense layer has 32,896 trainable parameters. The number of neurons in the dense layers decreases from 256 to 128 to remove redundant features and reduce computational complexity.\n\nDropout layers, used to prevent overfitting, do not add any trainable parameters but play a crucial role in regularizing the model. The dropout rate is set to 0.2, meaning 20% of the weights are randomly set to zero during each training iteration.\n\nThe softmax layer, which generates the probabilistic output for each class label, has 387 trainable parameters for three-way classification and 258 trainable parameters for two-way classification. This difference accounts for the variation in the total number of parameters between the two classification types.\n\nIn summary, the number of parameters in the BNCNN model was selected to ensure that the model could effectively learn from the data while maintaining computational efficiency. The distribution of parameters across different layers reflects the model's architecture, designed to capture both low-level and high-level features from the input images.",
  "optimization/features": "The input features for the model are derived from chest X-ray images. Each image is pre-processed to a uniform size of 150x150 pixels, which serves as the input dimension for the model. This resizing reduces the network's input dimension and the number of trainable and non-trainable weights, making the model more efficient.\n\nFeature selection in the traditional sense was not performed. Instead, the model relies on a deep learning approach where the convolutional layers automatically learn and extract relevant features from the input images. This process is inspired by the VGG models, which are known for their effective feature extraction capabilities.\n\nThe feature extraction phase consists of the first twelve layers of the model. These layers include convolutional layers, batch normalization layers, and max-pooling layers, which work together to identify and extract important features from the input images. The convolutional layers use 3x3 filters with a stride and padding of 1, and the number of filters increases from the input to the output layers. The batch normalization layers standardize the output of the previous layer, and the max-pooling layers reduce the dimension of the input feature map, helping to identify the highest-level patterns in the images.\n\nThe model does not use a separate feature selection step on the training set. Instead, the feature extraction is an integral part of the training process, where the model learns to identify and extract relevant features directly from the input images. This approach allows the model to adapt and improve its feature extraction capabilities as it is trained on the dataset.",
  "optimization/fitting": "The fitting method employed in our study utilized a convolutional neural network (CNN) architecture, specifically the BNCNN model, which was designed to balance complexity and performance. The model's architecture includes multiple convolutional layers followed by batch normalization and max-pooling layers, which help in reducing the number of parameters and controlling overfitting.\n\nThe total number of parameters in the BNCNN model is substantial, but it is carefully managed to avoid overfitting. Techniques such as dropout layers, which randomly set a fraction of input units to zero during training, were incorporated to prevent the model from becoming too reliant on specific features. Additionally, early stopping callbacks were used to monitor validation accuracy and halt training if the model's performance on the validation set ceased to improve, thereby avoiding overfitting.\n\nTo ensure that the model did not underfit, we employed the Adam optimizer, which adaptively adjusts the learning rate for each weight in the network. This optimizer combines the advantages of Stochastic Gradient Descent (SGD) and Root Mean Square Propagation (RMSP), facilitating faster convergence and better accuracy. The learning rate was dynamically reduced if the validation accuracy did not improve for three consecutive epochs, further aiding in preventing underfitting.\n\nThe model's performance was evaluated using multiple metrics, including accuracy, sensitivity, Positive Predictive Value (PPV), and F1-score, which provided a comprehensive assessment of its effectiveness. The BNCNN model demonstrated superior performance in the validation and testing phases compared to other pre-trained models, indicating that it was neither overfitting nor underfitting the data. The use of cross-entropy loss as the loss function ensured that the model was trained to accurately predict class probabilities, further supporting its robustness.",
  "optimization/regularization": "In our study, we implemented several regularization methods to prevent overfitting and enhance the generalization of our model. One key technique used was dropout, which was applied after the dense layers. Dropout works by randomly setting a fraction of the input units to zero at each update during training time, which helps to prevent overfitting. We used a dropout rate of 0.2, meaning that 20% of the weights were randomly set to zero in each iteration. This approach aids in making the model more robust and less likely to memorize the training data.\n\nAdditionally, we employed batch normalization layers after each convolutional and dense layer. Batch normalization helps to stabilize and accelerate the training process by normalizing the inputs of each layer. This technique reduces the internal covariate shift, making the model more resilient to overfitting.\n\nWe also utilized early stopping as a regularization method. Early stopping monitors the validation accuracy and stops the training process if the validation accuracy does not improve for a specified number of epochs. This prevents the model from overfitting to the training data by halting the training process at the optimal point.\n\nFurthermore, we implemented a learning rate reduction callback. This callback reduces the learning rate by a factor of 0.3 if the validation accuracy does not improve for three consecutive epochs. This adaptive learning rate adjustment helps in fine-tuning the model and prevents it from getting stuck in local minima, thereby improving generalization.\n\nThese regularization techniques collectively contribute to the robustness and generalization capability of our model, ensuring that it performs well on unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule for the models discussed in the publication are reported in detail. Specifically, the hyper-parameter settings for the Adam optimizer used in different models, such as VGG-16, VGG-19, Inception-V3, ResNet-50, and the proposed BNCNN model, are provided in a table. This table includes the learning rate, batch size, and number of epochs for each model.\n\nThe optimization parameters and the overall experimental setup are also described. The models were executed using a 12 GB NVIDIA Tesla P100 GPU and an Intel Xeon CPU @ 2.00GHz with 13 GB RAM. The Adam optimizer was chosen for its ability to adjust the learning rate for each weight in the network, facilitating faster convergence.\n\nRegarding the availability of model files and optimization parameters, the publication does not explicitly mention where these files can be accessed or under what license they are provided. Therefore, it is not clear whether the model files and optimization parameters are publicly available or under what terms they might be shared.",
  "model/interpretability": "The proposed BNCNN model, while highly effective in COVID-19 detection from X-ray images, operates largely as a black-box model. This means that the internal workings and decision-making processes of the model are not easily interpretable. The model comprises dense layers, batch normalization, and dropout layers, which collectively contribute to its performance but do not provide clear, human-understandable insights into how specific predictions are made.\n\nThe dense layers in the BNCNN model learn complex feature maps from the input data, but the exact features and their contributions to the final classification are not explicitly defined. Batch normalization layers standardize the internal feature maps, and dropout layers help in avoiding overfitting, but these processes do not enhance the interpretability of the model. The model's architecture is designed to optimize performance rather than transparency.\n\nWhile the BNCNN model achieves high accuracy and other performance metrics, the lack of interpretability is a common characteristic of deep learning models. This means that while the model can reliably classify X-ray images, the reasons behind its classifications are not straightforward to understand or explain. This can be a limitation in medical applications where transparency and explainability are crucial for trust and acceptance by healthcare professionals.",
  "model/output": "The model discussed in this publication is a classification model. Specifically, it is designed for image classification tasks, focusing on the detection of COVID-19 from X-ray images. The model supports both two-way and three-way classification. In the two-way classification, the model distinguishes between COVID-19 and normal cases. In the three-way classification, it further differentiates between COVID-19, normal, and viral pneumonia cases.\n\nThe performance of the model is evaluated using several metrics, including accuracy, sensitivity, Positive Predictive Value (PPV), and F1-score. These metrics provide a comprehensive assessment of the model's ability to correctly classify the images into the respective categories. The model's performance is compared against several pre-trained models, such as VGG-16, VGG-19, Inception-V3, and ResNet-50, demonstrating its effectiveness and efficiency, particularly in scenarios with limited computational resources.\n\nThe proposed model, referred to as BNCNN, incorporates a classifier comprising two repetitions of a dense layer to learn complex feature maps, a batch normalization layer to standardize internal feature maps, and a dropout layer to prevent overfitting and improve generalization. This architecture allows the model to achieve high accuracy and reliability in classifying X-ray images, making it a robust tool for COVID-19 detection.",
  "model/duration": "The models were executed using a 12 GB NVIDIA Tesla P100 GPU and an Intel Xeon CPU @ 2.00GHz with 13 GB RAM. The BNCNN model was trained for 100 iterations using the Adam optimizer. The specific execution time for each model is not provided, but the hardware specifications and training iterations give an idea of the computational resources used. The models were evaluated on a dataset for both three-way and two-way classification tasks, involving chest X-ray images categorized as COVID-19, Normal, and Viral Pneumonia. The training process included data augmentation techniques such as rotation, zoom, shear, and horizontal flipping, which likely contributed to the overall execution time. The use of a powerful GPU and optimized hyper-parameters ensured efficient training and validation phases.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the models, including the proposed BNCNN, was conducted using several key metrics to assess their performance in classifying chest X-ray images. The primary metrics used were accuracy, sensitivity, Positive Predictive Value (PPV), and F1-score. These metrics were derived using specific equations that consider true positives (TP), true negatives (TN), false negatives (FN), and false positives (FP).\n\nThe models were evaluated on both three-way and two-way classification tasks. For three-way classification, the models were tasked with distinguishing between COVID-19, Normal, and Viral Pneumonia. In the two-way classification, the models focused on differentiating between COVID-19 and Normal X-ray images.\n\nThe evaluation process involved training the models on a dataset of X-ray images, with the BNCNN model being trained for 100 iterations using the Adam optimizer. The hyper-parameter settings for the Adam optimizer were carefully chosen after experimental trials to ensure optimal performance.\n\nCross-validation was employed to split the dataset into training, validation, and testing sets. Data augmentation techniques, such as rotation, zoom, shear, and horizontal flipping, were applied to the training and validation sets to enhance the robustness of the models.\n\nThe performance of the models was assessed using confusion matrices, which provided a detailed breakdown of the classification results. From these matrices, evaluation metrics such as accuracy, recall, precision, and F1-score were calculated to provide a comprehensive assessment of each model's performance.\n\nStatistical tests, including Friedman\u2019s test and Holm\u2019s posthoc test, were conducted to evaluate the significance of the results. Friedman\u2019s test was used to compare the testing accuracy of the BNCNN model with other pre-trained models, with a significance level of p < 0.05. The results indicated that the BNCNN model outperformed the other models in both three-way and two-way classification tasks.\n\nAdditionally, the Area under the Receiver Operating Curve (AUC) was used to assess the discriminative ability of the models. The DeLong test was employed to compare the AUC values, with a p-value of \u22640.05 considered statistically significant. These evaluations confirmed the superior performance of the BNCNN model in COVID-19 detection tasks.",
  "evaluation/measure": "In our evaluation, we utilized several key performance metrics to assess the effectiveness of our models, including accuracy, sensitivity, Positive Predictive Value (PPV), and F1-score. These metrics provide a comprehensive view of model performance by capturing different aspects of classification success.\n\nAccuracy measures the overall correctness of the model by calculating the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, also known as recall, focuses on the model's ability to identify positive cases correctly, which is crucial for detecting conditions like COVID-19. PPV, or precision, evaluates the proportion of true positive results among all positive results predicted by the model, indicating how reliable the positive predictions are. The F1-score balances precision and recall, providing a single metric that considers both false positives and false negatives, which is particularly useful when dealing with imbalanced datasets.\n\nThese metrics are widely recognized and used in the literature for evaluating classification models, ensuring that our evaluation is representative and comparable to other studies in the field. By reporting these metrics, we aim to provide a clear and thorough assessment of our models' performance, highlighting their strengths and areas for potential improvement.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of the proposed BNCNN model with several publicly available pre-trained models, including VGG-16, VGG-19, Inception-V3, and ResNet-50. These comparisons were performed on benchmark datasets consisting of chest X-ray images, which are publicly available.\n\nThe evaluation involved both three-way and two-way classification tasks. For three-way classification, the BNCNN model demonstrated superior performance with an accuracy of 96.84%, sensitivity of 93.06%, PPV of 97.40%, and an F1-score of 95.18%. In two-way classification, the BNCNN model achieved an accuracy of 99.27%.\n\nStatistical tests, including Friedman\u2019s test and Holm\u2019s posthoc test, were employed to confirm the significance of the results. Friedman\u2019s test indicated that the testing accuracy of the BNCNN model was significantly different from that of the other pre-trained models, with p-values of 0.0146 for three-way classification and 0.0053 for two-way classification. Holm\u2019s posthoc test further validated these findings, showing significant differences between the BNCNN and other models.\n\nAdditionally, we compared the BNCNN model with simpler baselines and other existing models reported in the literature. The BNCNN model outperformed these baselines and existing models in terms of accuracy for both three-way and two-way classification tasks. For instance, the BNCNN model achieved higher accuracy compared to models like DeepCovix-net, AFCM-LSMA, and others, as detailed in the accuracy comparison table.\n\nOverall, the comprehensive evaluation and comparison with both complex pre-trained models and simpler baselines underscore the efficacy and reliability of the BNCNN model for COVID-19 detection from X-ray images.",
  "evaluation/confidence": "The evaluation of the proposed BNCNN model includes several performance metrics with confidence intervals, providing a clear indication of the model's reliability. For three-way classification, the BNCNN model achieved an accuracy of 96.84% with a 95% confidence interval (CI) of 91.26\u201397.45, a sensitivity of 93.06% (95% CI: 89.13\u201396.54), a Positive Predictive Value (PPV) of 97.40% (95% CI: 94.71\u201398.80), and an F1-score of 95.18% (95% CI: 88.45\u201397.13). These intervals help to understand the precision of the estimates.\n\nStatistical significance is assessed using Friedman\u2019s test and Holm\u2019s posthoc test. Friedman\u2019s test, with p-values of 0.0146 for three-way classification and 0.0053 for two-way classification, indicates that the testing accuracy differs significantly among the models. Holm\u2019s posthoc test further confirms these differences, showing significant p-values when comparing the BNCNN model with other pre-trained models like VGG-16, VGG-19, Inception-V3, and ResNet-50. This statistical evidence supports the claim that the BNCNN model is superior in terms of testing accuracy.\n\nAdditionally, the Area Under the Receiver Operating Curve (AUC) is evaluated using the DeLong test, which shows that the BNCNN model has a significantly higher AUC compared to other models. For three-way classification, the BNCNN model has an AUC of 0.92 (95% CI: 0.87\u20130.95), and for two-way classification, it has an AUC of 0.94 (95% CI: 0.89\u20130.96). The p-values from the DeLong test for competing models are all below 0.05, indicating that no other model has a statistically similar AUC to the BNCNN model. This further reinforces the model's superior discriminative ability.",
  "evaluation/availability": "Not enough information is available."
}