{
  "publication/title": "Classification of multiple sclerosis clinical profiles using machine learning and grey matter connectome.",
  "publication/authors": "Barile B, Ashtari P, Stamile C, Marzullo A, Maes F, Durand-Dubief F, Van Huffel S, Sappey-Marinier D",
  "publication/journal": "Frontiers in robotics and AI",
  "publication/year": "2022",
  "publication/pmid": "36313252",
  "publication/pmcid": "PMC9608344",
  "publication/doi": "10.3389/frobt.2022.926255",
  "publication/tags": "- Multiple Sclerosis\n- Machine Learning\n- Classification\n- Logistic Regression\n- Random Forest\n- Support Vector Machine\n- AdaBoost\n- Statistical Analysis\n- Graph Metrics\n- Ensemble Learning\n- Morphological Connectivity\n- Longitudinal Data\n- Cross-Validation\n- Hyperparameter Optimization\n- Deep Learning\n- Connectome Analysis\n- Brain Network\n- MRI Biomarkers\n- Data Fusion\n- Clinical Profiles",
  "dataset/provenance": "The dataset used in this study is derived from MRI acquisitions, focusing on brain structural connectivity in the context of multiple sclerosis (MS). The study involved a relatively small number of patients, with a particular note on the reduced number of patients in the Clinically Isolated Syndrome (CIS) group, which may introduce some uncertainty in the generalization of the results. This imbalance led to an imbalanced dataset, which is a known limitation of the study.\n\nThe dataset includes various global graph metrics calculated on binarized graphs, such as Betweenness Centrality (BC), Assortativity (r), Transitivity (T), Efficiency (Eg), Modularity (Q), and Density (D). These metrics were computed using two different atlases: FSAverage and Glasser 2016. The mean values and standard deviations of these metrics were reported for different MS clinical profiles, including CIS, Relapsing-Remitting (RR), Primary Progressive (PP), and Secondary Progressive (SP) MS.\n\nThe dataset has been used in previous studies and by the community, as indicated by the references to related work in the field of MS imaging and artificial intelligence. The study also mentions the use of data augmentation techniques, such as generative adversarial neural networks, to enhance the dataset and improve the robustness of the findings.\n\nThe dataset was analyzed using a machine learning pipeline that included four different models: Logistic Regression, Random Forest, Support Vector Machine, and Adaptive Boosting. These models were chosen for their wide usage, ease of training, and minimal hyperparameter tuning requirements. The predictive performances of these models were evaluated using a stratified 10-fold cross-validation strategy, with metrics such as accuracy, precision, recall, F1 score, and Area Under the Receiver Operating Characteristics (ROC-AUC) curve.\n\nThe study acknowledges the limitations of the dataset, particularly the small number of patients and the imbalance in the CIS group. These limitations may affect the generalizability of the results and highlight the need for further research with larger and more balanced datasets.",
  "dataset/splits": "The dataset was divided into three splits: training, validation, and test sets. The training set comprised 80% of the data, while the validation set consisted of 20% of the data. The test set was a hold-out set, meaning it was not used during the training or validation phases. This split was done to avoid overfitting and to ensure that the model's performance could be evaluated on unseen data.\n\nThe entire procedure, including the data splitting, was repeated 10 times to ensure the robustness of the results. This repetition helped in averaging out any variability that might have occurred due to the randomness in the data splitting process. Additionally, to prevent data leakage, if a specific patient was assigned to the test set, all corresponding longitudinal scans for that patient were also included in the test set. This approach ensured that the test set was independent of the training and validation sets, providing a more accurate evaluation of the model's performance.",
  "dataset/redundancy": "The dataset was split using a stratified 10-fold cross-validation strategy. In each iteration, one fold was set aside as the test set. From the remaining instances, 80% were used as the training set, and 20% as the validation set. This process was repeated 10 times, and the results were averaged to ensure robustness.\n\nTo avoid data leakage, a crucial step was taken to ensure the independence of the training and test sets. If a specific patient was assigned to the test set, all corresponding longitudinal scans for that patient were also included in the test set. This approach prevented any information from the test set from influencing the training process, thereby maintaining the integrity and independence of the datasets.\n\nThe distribution of the dataset was not explicitly compared to previously published machine learning datasets. However, the use of stratified cross-validation helps in maintaining a balanced distribution of classes in each fold, which is a common practice in machine learning to handle class imbalances and ensure that the model generalizes well to unseen data.",
  "dataset/availability": "The raw data supporting the conclusions of this article will be made available by the authors upon reasonable request. This approach ensures that the data is accessible to other researchers while maintaining control over its distribution. By providing the data upon request, we can ensure that it is used responsibly and in accordance with ethical guidelines. This method also allows us to track how the data is being utilized and to address any potential misuse or misinterpretation. The data availability statement reflects our commitment to transparency and reproducibility in scientific research.",
  "optimization/algorithm": "The optimization algorithm employed in this study utilized well-established machine-learning models, rather than introducing a new algorithm. The models used were Logistic Regression (LR), Random Forest (RF), Support Vector Machine (SVM), and Adaptive Boosting (AdaBoost). These models were chosen for their widespread use, ease of training, and minimal hyperparameter tuning requirements, which are particularly advantageous when working with smaller datasets to prevent overfitting.\n\nThe decision to use these specific models was driven by their proven effectiveness in various classification tasks and their ability to provide built-in feature importance, except for SVM when using a non-linear kernel. The models were optimized using a grid search strategy, where hyperparameters were tuned within specified ranges to enhance performance. For instance, the regularization parameter for LR was optimized between 1 and 100, while the number of trees and max depth for RF were adjusted within the ranges of 10 to 500 and 1 to 10, respectively. Similarly, the SVM model used a radial basis kernel with a regularization parameter and kernel coefficient optimized between 1 and 100. For AdaBoost, the number of trees and learning rate were tuned between 10 and 500, and 0.1 and 3, respectively.\n\nThe thresholding value for graph binarization was also considered as an additional hyperparameter, optimized across the entire percentile range. This thorough optimization process ensured that the models were finely tuned to the specific dataset and task at hand.\n\nThe models were validated using a stratified 10-fold cross-validation strategy, where 80% of the remaining instances after leaving one fold out were used as the training set, and the remaining 20% as the validation set. This procedure was repeated 10 times, and the results were averaged to provide robust performance metrics. The final results reported refer to the performance obtained on the hold-out test sets, ensuring that the models' generalizability was assessed independently of the training and validation processes.",
  "optimization/meta": "The model employed in this study utilizes a meta-predictor approach, which combines the predictions of multiple machine learning algorithms to enhance overall performance. This ensemble model leverages the individual strengths of four different classifiers: Logistic Regression, Random Forest, Support Vector Machine, and Adaptive Boosting. By integrating these models, the ensemble aims to improve predictive accuracy and robustness.\n\nThe ensemble model is constructed through majority voting, where each of the four base models contributes to the final prediction. This method ensures that the ensemble benefits from the diverse underlying assumptions and strengths of each classifier. The use of an ensemble approach is particularly advantageous in scenarios with small datasets, as it helps to mitigate the risk of overfitting.\n\nTo ensure the independence of the training data, a stratified 10-fold cross-validation strategy was employed. In this process, the dataset is divided into 10 folds, with one fold serving as the test set in each iteration. The remaining data is further split into 80% for training and 20% for validation. This procedure is repeated 10 times, with the results averaged to provide a comprehensive evaluation of the model's performance. Additionally, to avoid data leakage, the search for optimal hyperparameters and threshold values was conducted solely on the training set, ensuring that the validation and test sets remain independent.",
  "optimization/encoding": "The data encoding process began with the acquisition of 3D T1-weighted MRI scans from 90 MS patients, collected at various time points over several years. These scans were preprocessed using the Freesurfer image analysis suite to ensure consistency and comparability. The preprocessing steps included skull removal, motion correction, registration, intensity normalization, and resampling into the Talairach space. This process reduced noise and artifacts, making the MRI scans suitable for further analysis.\n\nFollowing preprocessing, the anatomical tissue was segmented into different classes, including white matter (WM), cortical gray matter (GM), subcortical GM, and cerebrospinal fluid (CSF). The cortical GM thickness was used as a key morphometric feature for characterizing MS patients. This feature was then used to generate GM connectivity maps, which were represented as adjacency matrices. Six global graph metrics\u2014Betweenness Centrality, Assortativity, Transitivity, Efficiency, Modularity, and Density\u2014were calculated from these matrices. These metrics described the topological behavior of the brain connectome and were used as input features for the machine-learning models.\n\nThe dataset was split into training, validation, and test sets, with 80% of the data used for training and parameter tuning, and 20% reserved for validation to prevent overfitting. The entire procedure was repeated 10 times, and the results were averaged to ensure robustness. To avoid data leakage, if a patient was assigned to the test set, all corresponding longitudinal scans were also included in the test set. This approach ensured that the models were evaluated on independent data, providing a more reliable assessment of their performance.",
  "optimization/parameters": "In our study, we optimized several hyperparameters for different machine learning models to enhance their performance. For the Logistic Regression model, we tuned a regularization parameter within the range of 1 to 100. The Random Forest model involved optimizing the number of trees, which varied between 10 and 500, and the maximum depth of each tree, set between 1 and 10. For the Support Vector Machine model, we used a radial basis function kernel and adjusted both the regularization parameter and the kernel coefficient within the range of 1 to 100. The AdaBoost model's hyperparameters included the number of trees, tuned between 10 and 500, and the learning rate, which ranged from 0.1 to 3. Additionally, we considered the thresholding value used for graph binarization as an extra hyperparameter, exploring the entire percentile range. This thorough optimization process ensured that each model was finely tuned to achieve the best possible performance.",
  "optimization/features": "Six global graph metrics were used as input features for the machine learning models. These metrics were calculated from the adjacency matrix representation of the brain connectome for each patient. The metrics included Betweenness Centrality, Assortativity, Transitivity, Efficiency, Modularity, and Density. Feature selection was not explicitly mentioned as a separate step in the process. However, the thresholding value used for graph binarization was considered as an additional hyperparameter and was optimized using only the training set to avoid data leakage. This approach ensures that the selection of the thresholding value does not inadvertently use information from the validation or test sets, maintaining the integrity of the model evaluation process.",
  "optimization/fitting": "In our study, we employed a robust strategy to ensure that our models neither overfit nor underfit the data. To address the potential issue of overfitting, given that the number of parameters in some models could be large, we utilized a stratified 10-fold cross-validation strategy. This approach involved dividing the dataset into 10 folds, where one fold was used as the test set and the remaining folds were split into 80% for training and 20% for validation. This process was repeated 10 times, ensuring that each fold served as the test set once. Additionally, we repeated the entire procedure 10 times and averaged the results to further mitigate overfitting.\n\nTo optimize the models, we used a grid search strategy to fine-tune hyperparameters. For example, for the Logistic Regression model, we optimized the regularization parameter within a specified range. For the Random Forest model, we tuned the number of trees and the maximum depth of each tree. Similarly, for the Support Vector Machine model, we optimized the regularization parameter and kernel coefficient using a radial basis function kernel. For the AdaBoost model, we adjusted the number of trees and the learning rate. This systematic hyperparameter tuning helped in finding the best model configurations that generalized well to unseen data, thereby reducing the risk of overfitting.\n\nTo avoid data leakage, we ensured that if a specific patient was assigned to the test set, all corresponding longitudinal scans were also included in the test set. This approach maintained the integrity of the patient data across different folds. Furthermore, the thresholding value used for graph binarization was optimized solely on the training set to prevent any information from the test set from influencing the model training process.\n\nIn terms of underfitting, we selected models that are known for their ability to capture complex patterns in the data, such as Random Forest and Support Vector Machine with a radial basis function kernel. These models, along with Logistic Regression and AdaBoost, were chosen for their effectiveness in handling small datasets and providing built-in feature importance, which helped in understanding the contribution of each feature to the prediction task. By combining these models into an ensemble through majority voting, we leveraged their individual strengths to improve overall predictive performance, thereby reducing the risk of underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the primary methods used was cross-validation. Specifically, we utilized a stratified 10-fold cross-validation strategy. This involved dividing the dataset into 10 folds, where one fold was used as the test set in each iteration, and the remaining folds were split into 80% for training and 20% for validation. This process was repeated 10 times, and the results were averaged to provide a more reliable estimate of model performance.\n\nAdditionally, we implemented regularization techniques specific to each model. For the Logistic Regression (LR) model, we optimized a regularization parameter within the interval between 1 and 100. For the Support Vector Machine (SVM) model, which used a radial basis function (RBF) kernel, we tuned both the regularization parameter and the kernel coefficient within the same range. These regularization parameters helped to prevent the models from becoming too complex and overfitting the training data.\n\nFurthermore, we ensured that the search for the optimal threshold for graph binarization was performed only on the training set to avoid data leakage. This step was crucial in maintaining the integrity of the validation and test sets, ensuring that the models were evaluated on unseen data.\n\nBy combining these techniques, we aimed to build models that generalize well to new, unseen data, thereby reducing the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, for the Logistic Regression (LR) model, the regularization parameter was optimized within the interval between 1 and 100. For the Random Forest (RF) model, the number of trees was tuned between 10 and 500, and the maximum depth of each tree was optimized between 1 and 10. The Support Vector Machine (SVM) model utilized a radial basis kernel (RBF) with a regularization parameter and kernel coefficient both ranging from 1 to 100. The AdaBoost model had its number of trees adjusted between 10 and 500, with a learning rate between 0.1 and 3. Additionally, the thresholding value for graph binarization was considered as a hyperparameter across the entire percentile range, but its optimization was restricted to the training set to prevent data leakage.\n\nThe model files and specific optimization parameters are not explicitly provided in the publication. However, the methods and strategies for optimization are thoroughly described, allowing for reproducibility. The raw data supporting the conclusions of this article will be made available by the authors upon reasonable request. This ensures that interested researchers can access the necessary information to replicate the study's findings and optimization processes.\n\nThe publication does not specify the licensing terms for the data or models, but it adheres to ethical standards and approvals from relevant authorities, ensuring compliance with research integrity and participant consent.",
  "model/interpretability": "The models employed in this study are not entirely black-box, as several of them provide built-in feature importance mechanisms. This transparency allows for a better understanding of the factors contributing to the predictions.\n\nLogistic Regression (LR) is inherently interpretable, as it provides coefficients that indicate the direction and strength of the relationship between each feature and the outcome. These coefficients can be directly interpreted to understand the impact of each global graph metric on the classification of MS clinical profiles.\n\nRandom Forest (RF) also offers interpretability through feature importance scores. These scores indicate the relative importance of each feature in making predictions. By examining these scores, one can identify which global graph metrics are most influential in the classification task.\n\nAdaBoost, similarly, provides feature importance scores that can be used to interpret the model. These scores help in understanding which features are most critical for the model's predictions.\n\nSupport Vector Machine (SVM), when using a linear kernel, also provides weights for each feature, which can be interpreted similarly to the coefficients in Logistic Regression. However, when using a non-linear kernel like the Radial Basis Function (RBF), the model becomes less interpretable.\n\nIn summary, while some models like SVM with non-linear kernels can be considered more black-box, others like LR, RF, and AdaBoost offer clear mechanisms for interpreting feature importance, making the overall modeling process more transparent.",
  "model/output": "The model employed in this study is designed for classification tasks. Specifically, it focuses on the classification of multiple sclerosis (MS) clinical profiles. Four different machine learning models were utilized for this purpose: Logistic Regression, Random Forest, Support Vector Machine, and Adaptive Boosting. These models were chosen for their wide usage, ease of training, and minimal hyperparameter tuning requirements, which helps in avoiding overfitting, especially with small datasets. The models were combined into an ensemble through majority voting to enhance predictive performance. The evaluation metrics used for assessing the models' performance include accuracy, precision, recall, F1 score, and the Area Under the Receiver Operating Characteristics (ROC-AUC) curve. The validation method involved a stratified 10-fold cross-validation strategy, ensuring robust and reliable performance evaluation.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models and algorithms used in this study is not publicly released. The statistical programming was conducted using Stata16, and the machine learning models were implemented in Python 3.6 with the scikit-learn package. The networkx package was used for calculating global graph metrics. However, the specific implementations and scripts are not available for public access.\n\nFor running the algorithm, no executable, web server, virtual machine, or container instance has been made publicly available. The methodology and results are detailed in the publication, but the practical tools to replicate the analysis directly are not provided.",
  "evaluation/method": "The evaluation method employed in this study is a stratified 10-fold cross-validation strategy. This approach ensures that each fold represents a balanced subset of the data, maintaining the distribution of the different classes. During each iteration, one fold is held out as the test set, while the remaining instances are split into an 80% training set and a 20% validation set. The training set is used for parameter tuning, and the model is validated on the validation set to prevent overfitting. This entire procedure is repeated 10 times, with the results averaged to provide a robust estimate of the model's performance.\n\nTo avoid data leakage, if a specific patient's data is assigned to the test set, all corresponding longitudinal scans for that patient are also included in the test set. This ensures that the model's performance is evaluated on completely unseen data, simulating a real-world scenario where the model would need to generalize to new patients.\n\nAdditionally, a grid search strategy was used for model optimization. Hyperparameters for each model were tuned within specified ranges to find the optimal configuration. For example, the regularization parameter for the Logistic Regression model was optimized within the interval between 1 and 100, while the number of trees and maximum depth for the Random Forest model were tuned within specified ranges. This systematic approach to hyperparameter tuning helps in achieving the best possible performance for each model.\n\nThe predictive performances were compared using the non-parametric Wilcoxon matched-pairs signed-rank test, which is suitable for comparing paired samples and does not assume a normal distribution. This statistical test helps in determining whether there are significant differences in the performance metrics across different models or configurations.\n\nIn summary, the evaluation method is rigorous and designed to ensure that the models are thoroughly tested and optimized, providing reliable and generalizable results.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our machine learning models. These metrics include accuracy, precision, recall, F1 score, and the Area Under the Receiver Operating Characteristics (ROC-AUC) curve.\n\nAccuracy is a fundamental metric that represents the ratio of correctly predicted instances to the total number of instances. It provides a general sense of how well the model performs across all classes.\n\nPrecision, on the other hand, focuses on the correctness of positive predictions. It is defined as the ratio of true positive predictions to the total number of positive predictions made by the model. This metric is crucial when the cost of false positives is high.\n\nRecall, also known as sensitivity, measures the model's ability to identify all relevant instances. It is the ratio of true positive predictions to the total number of actual positives. This metric is particularly important when the cost of false negatives is high.\n\nThe F1 score is a harmonic mean of precision and recall, providing a single metric that balances both concerns. It is especially useful when dealing with imbalanced datasets, as it gives a more nuanced view of the model's performance.\n\nFinally, the ROC-AUC curve provides a comprehensive evaluation of the model's performance across all classification thresholds. The area under this curve (AUC) indicates the model's ability to distinguish between the positive and negative classes.\n\nThese metrics are widely used in the literature and provide a robust evaluation framework for our models. They allow us to assess the models' strengths and weaknesses, ensuring that our findings are reliable and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, our focus was on evaluating the performance of different machine learning models and atlases for classifying multiple sclerosis (MS) clinical profiles.\n\nWe compared four different machine learning models: Logistic Regression, Random Forest, Support Vector Machine, and Adaptive Boosting. These models were chosen for their wide usage, ease of training, and minimal hyperparameter tuning requirements. Additionally, we combined these models into an ensemble model through majority voting to boost performance and exploit the individual properties of each classifier.\n\nWe also compared the performance of two different atlases for parcellation: FSAverage and Glasser 2016. The results showed high variability between the folds for both atlases, making it difficult to draw firm conclusions when comparing the two parcellation strategies. However, the lower performances observed for the FSAverage atlas were mainly due to two out of ten folds, indicating potential issues with specific data subsets.\n\nIn terms of simpler baselines, we did not explicitly compare our models to simpler baselines such as majority voting or random guessing. Our evaluation focused on the comparative performance of the selected machine learning models and the impact of different atlases on classification accuracy. The predictive performances were compared using the non-parametric Wilcoxon matched-pairs signed-rank test, ensuring a robust statistical evaluation of the models' effectiveness.",
  "evaluation/confidence": "The evaluation of the predictive performances of the models was conducted using a stratified 10-fold cross-validation strategy. This method ensures that the results are robust and generalizable. For each fold, one subset was used as the test set, while the remaining data was split into training and validation sets, with 80% used for training and 20% for validation. This process was repeated 10 times, and the results were averaged to provide a comprehensive assessment of the models' performance.\n\nThe performance metrics reported include mean values along with standard deviations, which serve as confidence intervals. These intervals provide a measure of the variability and reliability of the results. For instance, the F1 score, precision, accuracy, and AUC (Area Under the Curve) for different groups were presented with their respective standard deviations, indicating the range within which the true performance metrics are likely to fall.\n\nStatistical significance was evaluated using the non-parametric Wilcoxon matched-pairs signed-rank test. This test was employed to compare the predictive performances of the models, ensuring that any observed differences were not due to random chance. The tests were conducted with a significance level of 5%, which is a standard threshold in statistical analysis. This rigorous approach helps to confirm that the methods and models discussed are indeed superior to others and baselines, providing a strong foundation for the conclusions drawn.",
  "evaluation/availability": "Not enough information is available."
}