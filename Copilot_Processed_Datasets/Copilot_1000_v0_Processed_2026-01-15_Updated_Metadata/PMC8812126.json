{
  "publication/title": "An efficient hardware architecture based on an ensemble of deep learning models for COVID -19 prediction.",
  "publication/authors": "R S, Thaseen IS, M V, M D, M A, R M, Mahendran A, Alnumay W, Chatterjee P",
  "publication/journal": "Sustainable cities and society",
  "publication/year": "2022",
  "publication/pmid": "35136715",
  "publication/pmcid": "PMC8812126",
  "publication/doi": "10.1016/j.scs.2022.103713",
  "publication/tags": "- Artificial Intelligence\n- Deep Learning\n- Convolutional Neural Network\n- COVID-19\n- Chest X-rays\n- Ensemble Learning\n- Hardware Architecture\n- Model Optimization\n- Medical Imaging\n- Diagnostic Systems\n- Sustainable Development Goals\n- Computational Efficiency\n- Cloud Computing\n- Verilog HDL\n- Model Simulation\n- Dataflow Optimization\n- Performance Metrics\n- Healthcare Monitoring\n- Smart Healthcare\n- Ensemble Models",
  "dataset/provenance": "The dataset utilized in this study is sourced from the COVID-19 Radiography Database. This database was created by a research team from various countries and includes chest X-ray images for COVID-19 positive cases, as well as images of normal and viral pneumonia. The dataset has been released in multiple phases. In the first phase, it included 219 COVID-19, 1341 normal, and 1345 viral pneumonia chest X-ray images. The second phase increased the number of COVID-19 class images to 1200. In the third phase, the dataset expanded to include 3616 COVID-19 positive cases, along with 10,192 normal images and 1345 viral pneumonia images. All images are in Portable Network Graphics (PNG) file format with a resolution of 299 \u00d7 299 pixels.\n\nThis dataset has been widely used in the community for training and validating models related to COVID-19 detection. The images were collected to provide a comprehensive resource for researchers working on diagnostic tools for COVID-19 using chest X-rays. The dataset's diversity and size make it a valuable asset for developing and testing machine learning models aimed at identifying COVID-19 from radiological images.",
  "dataset/splits": "The dataset used in this study is the COVID-19 Radiography Database, which contains chest X-ray images categorized into COVID-19, normal, and viral pneumonia cases. The dataset was released in multiple phases, with the final phase including 3,616 COVID-19 positive cases, 10,192 normal images, and 1,345 viral pneumonia images. All images are in PNG format with a resolution of 299 \u00d7 299 pixels.\n\nFor the training process, the dataset was split into three parts: training, validation, and testing. Initially, 90% of the dataset was allocated for training, and 10% for testing. From the training dataset, 10% was randomly allocated for validation. Additionally, a stratified K-fold cross-validation with K = 5 was performed to ensure robust model evaluation.\n\nThe dataset was also balanced and imbalanced. The imbalanced dataset had a significant disparity with 10,192 normal images, 3,616 COVID-19 images, and 1,345 pneumonia images. To address this imbalance, random undersampling was done on the majority class (normal images) during the preprocessing phase. A class weighting mechanism was also implemented to penalize the model for misclassifying positive samples, thereby reducing overfitting and improving the model's performance on imbalanced data.",
  "dataset/redundancy": "The dataset used in our study was collected from the COVID-19 Radiography Database, which includes chest X-ray images for COVID-19 positive cases, as well as images of normal and viral pneumonia. The dataset was released in multiple phases, with the final phase containing 3,616 COVID-19 positive cases, 10,192 normal images, and 1,345 viral pneumonia images. All images are in PNG format with a resolution of 299 \u00d7 299 pixels.\n\nTo ensure the independence of training and test sets, we employed a stratified K-fold cross-validation approach with K = 5. This method helps to maintain the distribution of classes in each fold, ensuring that the model is trained and tested on representative samples of the dataset. During the training phase, 90% of the dataset was allocated for training, and 10% was reserved for testing. Additionally, 10% of the training dataset was randomly allocated for validation purposes.\n\nThe initial dataset had a significant imbalance, with a much higher number of normal images compared to COVID-19 and pneumonia images. To address this imbalance, we performed random undersampling on the majority class (normal images) during the preprocessing phase. This step helped to reduce the imbalance ratio, making the dataset more balanced and preventing the model from being biased towards the majority class.\n\nThe distribution of our dataset compares favorably to previously published machine learning datasets for COVID-19 prediction. By addressing the class imbalance and ensuring the independence of training and test sets, we aimed to improve the generalizability and robustness of our models. The use of stratified K-fold cross-validation further ensures that our results are reliable and not dependent on a specific split of the data.",
  "dataset/availability": "The dataset used in this study is publicly available and was collected from the COVID-19 Radiography Database. This database includes chest X-ray images for COVID-19 positive cases, as well as images of normal and viral pneumonia cases. The dataset was released in multiple phases, with the final phase containing 3,616 COVID-19 positive cases, 10,192 normal images, and 1,345 viral pneumonia images. All images are in PNG format with a resolution of 299 \u00d7 299 pixels.\n\nThe dataset is freely accessible and can be used for research purposes. The images were used to train and test various deep learning models, with 90% of the dataset allocated for training and 10% for testing. From the training dataset, 10% was randomly allocated for validation. A stratified K-fold cross-validation with K = 5 was performed to ensure the robustness of the models.\n\nThe dataset's imbalance was addressed through random undersampling of the majority class and the implementation of a class weighting mechanism. This approach helps to penalize the model when a positive sample is misclassified, thereby improving the model's performance on imbalanced data.\n\nThe dataset and the models trained on it are part of a broader effort to make COVID-19-related research freely available. This effort is in line with the permissions granted by Elsevier for COVID-19-related research, which allows for unrestricted research re-use and analyses with proper acknowledgment of the original source. The dataset and the models can be accessed and used by researchers worldwide to advance the understanding and detection of COVID-19.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages ensemble learning techniques, which are a class of machine-learning algorithms that combine multiple models to improve overall performance. Specifically, we utilized three ensemble methods: Majority Voting, Simple Averaging, and Weighted Averaging. These methods are well-established in the field of machine learning and are known for their effectiveness in enhancing predictive accuracy and robustness.\n\nThe algorithms used are not new; they are widely recognized and have been extensively studied and applied in various domains. The choice to include them in this work was driven by their proven ability to handle complex classification tasks, such as distinguishing between normal, pneumonia, and COVID-19 cases in chest X-ray images. The ensemble approaches were selected for their capacity to mitigate overfitting and improve generalization, which are critical considerations in medical imaging where data can be imbalanced and noisy.\n\nThe decision to publish these findings in a journal focused on sustainable cities and society, rather than a machine-learning journal, was strategic. Our primary goal was to demonstrate the practical application of these machine-learning techniques in a real-world scenario, specifically in the context of public health and urban sustainability. The integration of advanced machine-learning models into healthcare systems can significantly enhance diagnostic capabilities, leading to better public health outcomes and more efficient use of resources. This aligns with the broader themes of sustainability and societal impact, which are central to the journal's focus.",
  "optimization/meta": "The model employs a meta-predictor approach, utilizing data from multiple machine-learning algorithms as input. Specifically, it integrates five deep learning models: ResNet, FitNet, IRCNN, EfficientNet, and MobileNet. These models are fine-tuned to enhance their performance in predicting COVID-19 from chest X-ray images.\n\nThe ensemble methods used include majority voting, simple averaging, and weighted averaging. Each of these methods combines the predictions from the individual models to produce a final output. The weighted averaging method, in particular, dynamically calculates weights based on the performance of the previous classifiers, aiming to improve the overall accuracy and reliability of the predictions.\n\nRegarding the independence of the training data, the models are initially pre-trained using a large dataset like ImageNet. Subsequently, they undergo fine-tuning on a specific dataset of chest X-ray images. The dataset is split into training, validation, and testing sets to ensure that the models are evaluated on independent data. This process helps in assessing the generalizability and performance of the models accurately. The training data for each model is independent, and the ensemble methods are designed to leverage the strengths of individual models to enhance predictive performance.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for enhancing the performance of our machine-learning models. The images used were collected from the COVID-19 Radiography Database, which includes chest X-ray images for COVID-19 positive cases, as well as images of normal and viral pneumonia. These images were in Portable Network Graphics (PNG) format with a resolution of 299 \u00d7 299 pixels.\n\nTo prepare the data for training, we employed several augmentation techniques. These included random reflections along both axes, rescaling randomly between 0.5 and 1.50, and rotating randomly between -40 degrees and 40 degrees. These augmentations helped to increase the diversity of the training dataset, making the models more robust and less prone to overfitting.\n\nThe dataset was split into training, testing, and validation sets. Specifically, 80% of the data was used for training, 20% for testing, and a random allocation of 10% of the training data was used for validation. This split ensured that the models were trained on a sufficient amount of data while also having a separate set for evaluating their performance.\n\nTo address the class imbalance in the dataset, we implemented a class weighting mechanism. This mechanism penalized the model whenever a positive sample was misclassified, thereby reducing the bias towards the majority class. Additionally, random undersampling was performed on the majority class to further balance the dataset.\n\nThe preprocessing steps also included padding the trimmed models with zeros. This was followed by classification using a convolutional layer with 3 \u00d7 3 filters and 1024 feature maps. A dropout layer with a 0.5 dropout ratio was added to prevent overfitting, and a Global Average Pooling (GAP) layer was used to reduce the dimensionality of the feature maps. The final dense layer employed a softmax activation function to calculate the prediction probabilities.\n\nThese encoding and preprocessing steps were essential for improving the performance and generalizability of our models in classifying chest X-ray images as normal, pneumonia, or COVID-19 affected.",
  "optimization/parameters": "In our study, the optimization of hyperparameters was a critical step in enhancing the performance of our deep learning models. We utilized a 5-fold cross-validation approach to determine the optimal settings for various parameters.\n\nThe key hyperparameters that were optimized include the batch size, maximum number of epochs, global learning rate, validation frequency, dropout rate, and learning rate factor. These parameters were fine-tuned to achieve the best possible accuracy and performance for our models.\n\nThe batch size was set to 10, and the maximum number of epochs was varied between 100 and 200 depending on the model. The global learning rate was set to 4, and the validation frequency was 68. The dropout rate, which helps in preventing overfitting, was set to 0.5 for some models and 0.8 for others. The learning rate factor was consistently set to 10 across all models.\n\nAdditionally, the classification layer weight vector for the input, hidden, and output layers was optimized based on the cross-validation accuracy. This vector was found to be [0.75, 0.15, 1.18] for the models that were fine-tuned.\n\nThe optimization process involved iterative adjustments of these parameters to maximize the performance metrics such as accuracy, precision, recall, and F1 score. The final set of hyperparameters was selected based on the model's performance on the validation dataset.\n\nNot sure about the exact number of parameters (p) used in the model, as this information is not explicitly stated. However, the optimization process involved tuning multiple hyperparameters to improve the model's performance.",
  "optimization/features": "The input features for the models used in this study are derived from chest X-ray images. The images are in Portable Network Graphics (PNG) file format with a resolution of 299 \u00d7 299 pixels. This resolution indicates that each image has 299 \u00d7 299 = 89,401 pixels, which serve as the initial input features.\n\nFeature selection in the traditional sense was not explicitly performed. Instead, the models utilized pre-trained convolutional neural networks (CNNs) that automatically learn and select relevant features from the input images. These pre-trained models include ResNet-18, MobileNet-V2, FitNet, IRCNN, and EfficientNet. The feature extraction process is integrated into the training pipeline, where the models are fine-tuned on the specific dataset of chest X-ray images.\n\nThe dataset used for training, validation, and testing consists of images from the COVID-19 Radiography Database, which includes categories for COVID-19, normal, and viral pneumonia cases. The training process involves splitting the dataset into training, validation, and testing sets, with 90% of the data used for training and 10% for testing. From the training dataset, 10% is randomly allocated for validation. This splitting ensures that the feature learning and model optimization are performed using the training set only, maintaining the integrity of the validation and testing phases.\n\nThe models are trained using a stratified K-fold cross-validation with K = 5, which helps in assessing the model's performance and generalization capability. The hyperparameters, such as batch size, max epochs, global learning rate, validation frequency, dropout rate, and learn rate factor, are optimized based on the cross-validation accuracy obtained from the training set. This approach ensures that the feature learning and model optimization are robust and reliable.",
  "optimization/fitting": "In our study, we employed several deep learning models, including ResNet-18, MobileNet-V2, FitNet, IRCNN, and EfficientNet, to ensure a robust and diverse representation power. The number of parameters in these models is indeed significantly larger than the number of training points, which is a common scenario in deep learning. To address potential overfitting, we implemented several strategies.\n\nFirstly, we used data augmentation techniques such as random reflection, rescaling, and rotation. These techniques help to artificially increase the size of the training dataset and make the model more generalizable. Secondly, we employed dropout layers with rates optimized through cross-validation, which randomly set a fraction of input units to zero during training, preventing the model from becoming too reliant on any single feature.\n\nAdditionally, we utilized early stopping based on validation performance, halting training when the model's performance on a validation set ceased to improve. This approach ensures that the model does not overfit to the training data. We also performed k-fold cross-validation with k=5, which helps in assessing the model's performance across different subsets of the data, further mitigating the risk of overfitting.\n\nTo rule out underfitting, we fine-tuned the models iteratively, adjusting hyperparameters such as batch size, learning rate, and the number of epochs. We also used an ensemble of models, including majority voting, simple averaging, and weighted averaging, which combines the strengths of multiple models to improve overall performance. The ensemble models showed better performance metrics, indicating that they were not underfitting the data.\n\nFurthermore, we evaluated the models using various performance metrics such as accuracy, precision, recall, F1-score, and AUC, ensuring that the models were not only fitting the training data but also generalizing well to unseen data. The use of these metrics provided a comprehensive evaluation of the models' performance, helping to confirm that neither overfitting nor underfitting was occurring.",
  "optimization/regularization": "In our study, several regularization methods were employed to prevent overfitting and improve the generalization of our models. One key technique used was dropout, which was applied with a dropout rate of 0.5 and 0.8 in different models. Dropout helps to prevent overfitting by randomly setting a fraction of input units to 0 at each update during training time, which helps to prevent over-reliance on any single neuron.\n\nAdditionally, we utilized data augmentation techniques to artificially increase the diversity of our training dataset. These techniques included random axis side reflection, rescaling randomly between 0.5 and 1.50, and rotating randomly between -40 degrees and 40 degrees. Data augmentation helps to make the model more robust and less likely to overfit to the training data.\n\nAnother important regularization technique we employed was class weighting. This method assigns higher weights to minority classes, which helps to balance the impact of imbalanced datasets and reduces the bias towards the majority class. By penalizing the model more heavily for misclassifying minority samples, class weighting helps to improve the model's performance on underrepresented classes.\n\nFurthermore, we used ensemble methods, such as majority voting, simple averaging, and weighted averaging, to combine the predictions of multiple models. Ensemble methods can improve the overall performance and robustness of the models by reducing the variance and bias of individual models.\n\nLastly, we performed random undersampling on the majority class to address class imbalance. This technique helps to create a more balanced dataset, which in turn helps to prevent the model from becoming biased towards the majority class.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are detailed in the appendix of our publication. Specifically, Table C.2 in the appendix provides a comprehensive overview of the hyper-parameter settings for various deep learning models, including ResNet, FitNet, IRCNN, EfficientNet, and different ensemble methods such as Majority Voting, Simple Averaging, and Weighted Averaging.\n\nThe optimization parameters, such as batch size, maximum epochs, global learning rate, validation frequency, dropout rate, and learn rate factor, are also reported in this table. These parameters were initialized after achieving optimal 5-fold cross-validation accuracy for the models.\n\nRegarding the availability of model files and optimization parameters, our publication is part of the COVID-19 resource center hosted on Elsevier Connect. Elsevier has granted permission to make all its COVID-19-related research immediately available in PubMed Central and other publicly funded repositories. This includes the right for unrestricted research re-use and analysis in any form or by any means, with proper acknowledgment of the original source. Therefore, the configurations and parameters discussed are accessible under these terms, facilitating further research and validation by the scientific community.",
  "model/interpretability": "The models employed in our study are primarily deep learning-based convolutional neural networks (CNNs), which are often considered black-box models due to their complex architectures and the difficulty in interpreting their internal workings. These models include MobileNet, EfficientNet, FitNet, IRCNN, and ResNet-18. While these models are highly effective in classifying medical images, such as chest X-rays, their decision-making processes are not easily interpretable.\n\nHowever, we have integrated ensemble techniques to improve the overall performance and robustness of our models. These ensemble methods, including majority voting, simple averaging, and weighted averaging, provide a way to combine the predictions of multiple models, which can enhance interpretability to some extent. By aggregating the outputs of different models, we can gain insights into which features are consistently important across multiple models.\n\nAdditionally, we have used pre-trained models on large datasets like ImageNet, which helps in learning varied feature representations. This pre-training phase allows the models to generalize better to medical images, even though the specific features they learn may not be immediately transparent.\n\nTo address the interpretability challenge, we have also considered hybrid models that combine machine learning techniques with more interpretable methods. For instance, a hybrid relevance vector machine and logistic regression (RVM-L) model has been proposed, which can achieve high prediction accuracy while providing more interpretable results. This hybrid approach can help in understanding the key factors contributing to the model's predictions, thereby improving transparency.\n\nIn summary, while our primary models are black-box CNNs, we have employed ensemble techniques and hybrid models to enhance interpretability. These methods allow us to leverage the strengths of deep learning while also providing some level of transparency in the decision-making process.",
  "model/output": "The model discussed in this publication is primarily focused on classification tasks. Specifically, it is designed for the classification of chest X-ray images into categories such as Normal, Pneumonia, and COVID-19. The performance of the model is evaluated using metrics typical for classification problems, such as precision, recall, and F1-score. These metrics are used to assess the model's ability to correctly identify and differentiate between the various classes of images.\n\nThe model employs ensemble techniques, integrating different convolutional neural networks (CNNs) like ResNet, FitNet, IRCNN, MobileNet, and EfficientNet. These ensembles are fine-tuned to improve classification accuracy. The use of ensemble methods helps in combining the predictions of multiple models, which can reduce the variance and improve the overall performance of the classification task.\n\nAdditionally, the model's performance is analyzed using ANOVA tests on mean squared error (MSE) cross-validation replicates. This statistical analysis provides insights into the variance in MSE influenced by different learning approaches and attribute mapping methods. The results indicate that the ensemble models, particularly the fine-tuned weighted averaging ensemble model, show significant improvements in classification performance compared to individual models.\n\nIn summary, the model is a classification model designed to predict the presence of COVID-19, Pneumonia, or Normal conditions from chest X-ray images. The use of ensemble techniques and fine-tuning processes enhances its classification accuracy and robustness.",
  "model/duration": "The execution time of our proposed model varies depending on the ensemble method used. For the majority voting ensemble, the computation time ranges from 5.64 seconds to 9.7 seconds, depending on the number of top models considered. Simple averaging ensemble methods take between 4.54 seconds and 6.75 seconds. The weighted averaging ensemble results in the minimum computational time, ranging from 3.72 seconds to 8.78 seconds. These times reflect the efficiency of our model in processing and analyzing data, contributing to its feasibility for real-time applications in COVID-19 prediction and diagnosis.\n\nAdditionally, our hardware architecture, designed with pipeline and parallel processing capabilities, has shown a significant reduction in latency and clock cycles. The synthesized results indicate a 40% reduction in computation time and a 17% reduction in power consumption. This optimization is crucial for developing a robust system with high speed and low power consumption, aligning with the goals of sustainable and efficient healthcare solutions. The reduced memory access and latency further support the development of a green environment, upholding international sustainable development goals.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the proposed models involved a comprehensive approach to ensure robustness and accuracy. The experiments were conducted on a Windows system equipped with an Intel Xeon CPU E3-1275 v6 processor and an NVIDIA GeForce 1050 Ti GPU. The TensorFlow backend with the Keras deep learning framework was utilized, and CUDA and cuDNN libraries were employed to accelerate GPU performance.\n\nThe dataset used for evaluation consisted of chest X-ray images from the COVID-19 Radiography Database, which includes images of COVID-19 positive cases, normal cases, and viral pneumonia. The dataset was released in multiple phases, with the final phase containing 3616 COVID-19 positive cases, 10,192 normal images, and 1345 viral pneumonia images. All images were in PNG format with a resolution of 299 \u00d7 299 pixels.\n\nA stepwise training approach was adopted, where pre-trained models like ImageNet and custom Convolutional Neural Networks (CNNs) were retrained using a large collection of images. The dataset was split into 90% for training and 10% for testing. From the training dataset, 10% was randomly allocated for validation. A stratified K-fold cross-validation with K = 5 was performed to ensure the models' generalizability.\n\nSeveral deep learning models were evaluated, including ResNet-18, MobileNet-V2, FitNet, IRCNN, and EfficientNet. These models were chosen for their architectural diversity and representation power when integrated into ensemble learning. The performance of these models was assessed using various metrics such as accuracy, sensitivity, specificity, precision, F1 score, Matthews correlation coefficient, Diagnostic Odds Ratio, Kappa, and Area Under the Curve (AUC).\n\nThe evaluation also included fine-tuning the models iteratively to improve their performance. Ensemble approaches such as Majority Voting, Simple Averaging, and Weighted Averaging were employed to enhance the classification results. The top-1 weighted averaging method was found to perform better than top-2 and top-4 methods based on multiple evaluation metrics.\n\nIn summary, the evaluation method involved a rigorous process of dataset preparation, model training, and performance assessment using multiple metrics and ensemble techniques to ensure the reliability and accuracy of the proposed models.",
  "evaluation/measure": "In our evaluation, we employed a comprehensive set of performance metrics to thoroughly assess the effectiveness of our models. These metrics include accuracy, sensitivity, specificity, precision, F1 score, Matthews correlation coefficient (MCC), Diagnostic Odds Ratio (DOR), Kappa, and Area Under the Curve (AUC). These metrics are widely recognized and used in the literature, ensuring that our evaluation is representative and comparable to other studies in the field.\n\nAccuracy measures the overall correctness of the model's predictions. Sensitivity, also known as recall, indicates the model's ability to correctly identify positive cases. Specificity measures the model's ability to correctly identify negative cases. Precision reflects the proportion of true positive predictions among all positive predictions made by the model. The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both concerns.\n\nThe Matthews correlation coefficient (MCC) is a balanced measure that takes into account true and false positives and negatives, providing a value between -1 and 1, where 1 indicates a perfect prediction. The Diagnostic Odds Ratio (DOR) is the ratio of the odds of positivity in diseased relative to the odds of positivity in non-diseased. Kappa measures the agreement between the predicted and actual classifications, adjusted for the agreement occurring by chance.\n\nThe Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve provides an aggregate measure of performance across all classification thresholds. This metric is particularly useful for evaluating models in scenarios where the class distribution is imbalanced.\n\nBy reporting these metrics, we aim to provide a holistic view of our models' performance, covering various aspects of predictive accuracy and reliability. This approach ensures that our evaluation is robust and aligned with established practices in the scientific community.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the performance of our proposed models against several publicly available deep learning methods using benchmark datasets. The comparison included well-known architectures such as AlexNet, ResNet-50, VGG variants, LeNet-5, Inception V3, and others. These models were assessed on metrics like accuracy, precision, recall, and F1 score to ensure a comprehensive evaluation.\n\nWe also compared our models to simpler baselines, including individual models like IRCNN, MobileNet, FitNet, and ResNet-18, both in their baseline and fine-tuned states. This comparison helped us understand the incremental improvements achieved through fine-tuning and ensemble learning techniques.\n\nOur ensemble models, which included majority voting, simple averaging, and weighted averaging, were evaluated against these baselines. The results showed that our top-1 weighted averaging method outperformed other models in terms of diagnostic odds ratio (DOR), area under the curve (AUC), accuracy, F1 score, Matthews correlation coefficient (MCC), specificity, precision, and Kappa.\n\nAdditionally, we compared our proposed model with state-of-the-art deep learning models, demonstrating superior performance across various metrics. This thorough comparison ensures that our models are not only effective but also competitive with the latest advancements in the field.",
  "evaluation/confidence": "In our study, we have employed several metrics to evaluate the performance of our models, and we have taken steps to ensure the statistical significance of our results. The performance metrics we used include accuracy, sensitivity, specificity, precision, F1 score, Matthews correlation coefficient (MCC), Diagnostic Odds Ratio (DOR), Kappa, and Area Under the Curve (AUC). For some of these metrics, such as AUC, we have provided confidence intervals to indicate the reliability of our estimates.\n\nTo assess the statistical significance of our findings, we conducted an ANOVA test on the top-1, top-2, and top-4 fine-tuned ensemble models. This analysis helped us determine whether the differences in performance between our proposed models and existing ones, such as FitNet, were statistically significant. The results of this test are presented in a table, demonstrating that our ensemble models, particularly the top-1 weighted averaging model, show significant improvements in performance.\n\nAdditionally, we addressed the issue of class imbalance in our dataset, which can lead to overfitting and unreliable predictions. We employed random undersampling of the majority class and implemented a class weighting mechanism to penalize misclassifications of positive samples. These steps helped ensure that our models' performance metrics are robust and not merely artifacts of an imbalanced dataset.\n\nFurthermore, we compared our models' performance with state-of-the-art deep learning models, showing that our ensemble approaches, particularly the top-1 weighted averaging model, achieve superior results. The statistical significance of these comparisons is supported by the ANOVA test results and the confidence intervals provided for key metrics like AUC.\n\nIn summary, our evaluation includes confidence intervals for key metrics and statistical tests to ensure the significance of our results. These steps provide a strong basis for claiming that our proposed ensemble models are superior to other methods and baselines.",
  "evaluation/availability": "The raw evaluation files are not directly available. However, the research content, including evaluation details, is freely accessible on the COVID-19 resource centre hosted on Elsevier Connect. This resource centre provides unrestricted access for research re-use and analyses, with proper acknowledgement of the original source. The permissions granted by Elsevier allow for the use of this content in publicly funded repositories such as PubMed Central and the WHO COVID database. This ensures that the evaluation methods and results can be reviewed and utilized by the research community."
}