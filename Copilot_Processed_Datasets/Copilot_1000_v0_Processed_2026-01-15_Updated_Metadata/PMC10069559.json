{
  "publication/title": "Predicting Hospitalization among Medicaid Home- and Community-Based Services Users Using Machine Learning Methods.",
  "publication/authors": "Jung D, Pollack HA, Konetzka RT",
  "publication/journal": "Journal of applied gerontology : the official journal of the Southern Gerontological Society",
  "publication/year": "2023",
  "publication/pmid": "36164857",
  "publication/pmcid": "PMC10069559",
  "publication/doi": "10.1177/07334648221129548",
  "publication/tags": "- Machine Learning\n- Predictive Modeling\n- Healthcare\n- Hospitalization\n- Elderly Care\n- Medicaid\n- Home and Community-Based Services\n- Gradient Boosting\n- Random Forests\n- Feature Importance",
  "dataset/provenance": "The dataset used in this study is derived from two primary sources: the 2012 national Medicaid Analytic eXtract (MAX) and the Medicare Provider Analysis and Review (MedPAR). The MAX data is a comprehensive set of individual-level enrollment and claims data files for Medicaid enrollees, created by the Centers for Medicare and Medicaid Services (CMS). This dataset is compiled from data submitted quarterly by states. The MedPAR data contains detailed information on Medicare beneficiaries who have used hospital inpatient services, which is crucial for identifying hospitalizations by month.\n\nThe study focuses on a specific subset of the data, identifying 1,217,019 individuals as Home- and Community-Based Services (HCBS) users without any institutional care from May 1, 2012, to June 30, 2012. Out of these, 6.6%, or 80,754 people, experienced hospitalizations in August 2012. This dataset is particularly valuable because it includes a wide range of variables, such as demographic characteristics, Medicare managed care enrollment, original reason for Medicare eligibility, health status indicators, and geographic location using Rural-Urban Commuting Area (RUCA) Codes.\n\nThe use of these datasets is not novel; they have been utilized in previous studies to predict adverse health outcomes and to prioritize individuals for interventions and screenings. For instance, similar datasets have been used to identify Medicare patients at high risk of hospital readmission and to develop predictive models for Medicaid beneficiaries. The robustness of these datasets in predicting health outcomes makes them a reliable choice for this study.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a test set. The ratio used for this split was 70:30, meaning 70% of the data was allocated to the training set, and the remaining 30% was used for the test set. This division was employed to compare different algorithms and to mitigate the risk of overfitting.\n\nAdditionally, a 10-fold cross-validation method was utilized during the model development phase. This technique involves splitting the training data into 10 subsets, or folds. The model is then trained on 9 of these folds and validated on the remaining fold. This process is repeated 10 times, with each fold serving as the validation set once. The average performance across these 10 iterations is used to tune the hyperparameters based on the best average precision.",
  "dataset/redundancy": "The dataset used in our study was divided into a training set and a test set using a 70:30 ratio. This split was employed to compare different algorithms and to reduce the risk of overfitting. The training set was used to develop and tune the models, while the test set was used to evaluate their performance on unseen data.\n\nTo ensure the independence of the training and test sets, we employed a 10-fold cross-validation method. This technique involves partitioning the training set into 10 subsets, or folds. The model is then trained on 9 of these folds and validated on the remaining fold. This process is repeated 10 times, with each fold serving as the validation set once. The performance metrics are averaged across the 10 iterations to provide a more robust estimate of the model's performance.\n\nThe distribution of our dataset compares favorably to previously published machine learning datasets, particularly those focused on healthcare outcomes. Our dataset includes a mix of demographic characteristics, health status indicators, and service utilization data from Medicaid and Medicare claims. This comprehensive approach allows for a nuanced understanding of the factors contributing to hospitalization risk among home and community-based services (HCBS) users. The imbalanced nature of our outcomes, with a 1-month hospitalization rate of 6.6%, is also consistent with many real-world healthcare datasets, where adverse events are relatively rare. This imbalance underscores the importance of using metrics like the area under the precision-recall curve (AUPRC), which are more informative than traditional ROC curves in such scenarios.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in our study are Random Forest (RF), XGBoost, LASSO regression, and logistic regression. These algorithms are well-established and widely used in the field of machine learning and data science. They belong to different classes of machine-learning algorithms. LASSO regression is a type of linear model that performs both variable selection and regularization to enhance the prediction accuracy and interpretability of the statistical model it produces. Logistic regression is a statistical method for analyzing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible classes). Random Forest is an ensemble learning method used for classification, regression, and other tasks that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable. It implements machine learning algorithms under the Gradient Boosting framework.\n\nThese algorithms are not new; they have been extensively studied and applied in various domains. The choice of these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to provide robust predictive performance. Random Forest and XGBoost, in particular, are known for their high performance in predictive modeling and have been successful in various data competitions. The study focuses on applying these algorithms to predict hospitalization among older adults using Medicaid Home and Community-Based Services (HCBS), leveraging Medicare and Medicaid claims data. The algorithms were selected for their ability to handle large datasets and provide accurate predictions, which are crucial for developing targeted interventions and improving healthcare outcomes.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for ensuring the effectiveness of the machine-learning algorithms. We began by collecting Medicare and Medicaid claims data for Home and Community-Based Services (HCBS) users. The data included various features such as demographic information, service utilization, and health conditions.\n\nFor categorical variables, we employed one-hot encoding to convert them into a format suitable for machine-learning models. This technique creates binary columns for each category, allowing the algorithms to interpret categorical data effectively. Continuous variables, such as the length of stay during previous hospitalizations, were normalized to ensure that they were on a similar scale, which is essential for algorithms like XGBoost and Random Forest that can be sensitive to the magnitude of input features.\n\nMissing values were handled through imputation techniques. For numerical features, we used the median value to fill in missing data, as it is less affected by outliers compared to the mean. For categorical features, the most frequent category was used for imputation. This approach helped maintain the integrity of the data while minimizing the impact of missing values on the model's performance.\n\nFeature engineering was another critical step in our preprocessing pipeline. We created new features that captured important aspects of the data, such as the frequency of service utilization and the presence of specific chronic conditions. These engineered features provided additional context to the models, enhancing their ability to predict hospitalization outcomes.\n\nAdditionally, we performed feature selection to identify the most relevant predictors. This involved using techniques like recursive feature elimination and evaluating feature importance scores from initial model runs. By focusing on the most informative features, we reduced the dimensionality of the data and improved the efficiency and accuracy of our models.\n\nOverall, the data encoding and preprocessing steps were designed to transform raw claims data into a structured format that could be effectively utilized by machine-learning algorithms. This meticulous preparation ensured that our models could accurately predict hospitalization among older duals using Medicaid HCBS.",
  "optimization/parameters": "In our study, we developed and compared four different algorithms: logistic regression, LASSO regression, random forest (RF), and XGBoost. Each of these models has its own set of input parameters, which were selected and tuned using specific methods.\n\nFor LASSO regression, the primary parameter is the lambda (\u03bb), which controls the regularization strength. This parameter was tuned using cross-validation to find the optimal value that balances model complexity and predictive accuracy.\n\nThe random forest model involves several hyperparameters, including the number of trees, maximum depth of each tree, minimum number of samples required to split an internal node, and the minimum number of samples required to be at a leaf node. These parameters were optimized using grid search, which systematically works through multiple combinations of parameter tunes to determine the best configuration.\n\nXGBoost, an ensemble of gradient-boosted decision trees, has parameters such as the maximum depth of a tree, learning rate, subsample percentage, subsample ratio of variables, and maximum tree depth. These hyperparameters were also tuned using grid search to enhance the model's performance.\n\nIn summary, the number of parameters (p) varies depending on the model. LASSO regression primarily focuses on tuning the lambda parameter. Random forest and XGBoost, being more complex models, involve tuning multiple hyperparameters to achieve optimal performance. The selection of these parameters was done through cross-validation and grid search methods to ensure robust model development.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "In our study, we employed several strategies to address both over-fitting and under-fitting during model development. We developed three different types of models\u2014LASSO regression, random forest, and XGBoost\u2014and compared these models with logistic regression. Each approach has its own advantages and disadvantages.\n\nFor LASSO regression, we used a shrinkage method that imposes a penalty proportional to the sum of the absolute coefficients. This technique helps in preventing over-fitting by shrinking the coefficients with minor contributions to the model toward zero. We tuned the hyperparameter lambda, which sets the weighting of the penalty to the loss function. A higher lambda produces a simpler model that is less vulnerable to over-fitting but may provide lower predictive accuracy within a given dataset.\n\nRandom Forest (RF) is an ensemble of Classification and Regression Trees where each individual tree predicts the outcome. RF uses modified bagged decision trees to prevent highly correlated trees, ensuring that all variables are considered at every split of every tree. By using bootstrapping and a random subset of variables at each split, RF reduces the correlation between trees, which helps in mitigating over-fitting. We used grid search to find the best hyperparameters, including the number of trees, maximum depth of each tree, minimum number of samples required to split an internal leaf node, and minimum number of samples required to be at a leaf node.\n\nXGBoost, based on gradient-boosted decision trees, is designed for computational speed and model performance, particularly for large datasets. XGBoost uses an ensemble technique where new models are added to correct the errors made by previous models. We tuned several hyperparameters, including the maximum depth of a tree, learning rate, subsample percentage, subsample ratio of variables, and maximum tree depth. This tuning process helps in balancing the model complexity to avoid both over-fitting and under-fitting.\n\nTo further ensure the robustness of our models, we divided the dataset into a training set and test set using a 70:30 ratio. This split helps in comparing algorithms and reducing over-fitting. Additionally, we used the 10-fold cross-validation method to tune hyperparameters based on the best average precision. This approach provides a more reliable estimate of model performance and helps in preventing over-fitting.\n\nIn summary, we employed regularization techniques, ensemble methods, and cross-validation to address over-fitting and under-fitting. These strategies ensured that our models were well-calibrated and generalizable to new data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting, a common challenge in model development. One of the primary methods used was LASSO (Least Absolute Shrinkage and Selection Operator) regression. LASSO is a regularization technique that adds a penalty proportional to the sum of the absolute coefficients to the loss function. This penalty shrinks the coefficients of less important features towards zero, effectively performing feature selection and reducing the model's complexity. By tuning the hyperparameter lambda, which controls the strength of the penalty, we were able to balance model simplicity and predictive accuracy, thereby mitigating overfitting.\n\nAdditionally, we utilized random forest (RF) and XGBoost, both of which are ensemble methods that inherently help in preventing overfitting. Random forests use bootstrapping and random feature selection at each split to create diverse trees, reducing the correlation between them and thus lowering the risk of overfitting. XGBoost, on the other hand, employs gradient boosting, where new models are sequentially added to correct the errors of previous ones. This process, combined with hyperparameter tuning for parameters like maximum tree depth and learning rate, ensures that the model generalizes well to unseen data.\n\nFurthermore, we divided our dataset into training and test sets using a 70:30 ratio. This split helps in evaluating the model's performance on unseen data, providing a more realistic assessment of its generalization capability. We also used 10-fold cross-validation to tune hyperparameters based on the best average precision, ensuring that our models were robust and not overly fitted to the training data.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models developed in this study vary in their interpretability. Logistic regression and LASSO regression are more transparent and interpretable. Logistic regression provides clear coefficients for each feature, indicating the direction and magnitude of their effect on the outcome. LASSO regression, while also providing coefficients, has the added benefit of performing feature selection by shrinking less important coefficients to zero, making the model sparser and easier to interpret.\n\nOn the other hand, random forest and XGBoost are considered black-box models. These ensemble methods combine multiple decision trees, making it challenging to trace the exact decision path for individual predictions. However, they offer feature importance scores, which indicate the relevance of each feature in making predictions. For instance, length of stay during previous hospitalizations and the use of certain healthcare services were identified as highly predictive features. This provides some level of interpretability, although not as straightforward as the coefficients in logistic or LASSO regression.\n\nIn summary, while some models offer clear interpretability through coefficients, others provide insights through feature importance, striking a balance between predictive power and explainability.",
  "model/output": "The model is a classification model. It is designed to predict hospitalization outcomes, specifically whether an individual will be hospitalized or not. The model evaluates its performance using metrics such as the area under the precision-recall curve (AUPRC) and the area under the receiver operating characteristic curve (AUROC), which are commonly used for assessing the performance of classification models. The outcomes are binary, indicating either hospitalization or no hospitalization, which further confirms that the model is a classification model.\n\nThe model development process involves comparing different algorithms\u2014LASSO regression, random forest, XGBoost, and logistic regression\u2014to determine which one performs best in predicting hospitalization. The focus on imbalanced data and the use of precision-recall curves also support the classification nature of the model. The feature importance analysis identifies key factors that contribute to the prediction of hospitalization, such as length of stay during previous hospitalizations and specific chronic conditions, which are relevant for classification tasks.\n\nThe model's performance is evaluated using various time periods and data inputs, with random forest and XGBoost consistently showing higher AUPRC and AUROC values, indicating their superior performance in classifying hospitalization outcomes. The sensitivity analyses further validate the robustness of the model across different time periods, reinforcing its classification capabilities.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our evaluation, we primarily focused on the area under the precision-recall curve (AUPRC) due to the imbalanced nature of our outcome variables. This metric provides a single score ranging from 0 to 1, summarizing the classifier's performance. Additionally, we used the area under the receiver operating characteristic curve (AUROC) to measure the model's ability to distinguish between positive and negative classes. The AUROC ranges from 0.5, indicating random guessing, to 1.0, signifying perfect distinction.\n\nTo develop and evaluate our models, we divided the dataset into a training set and a test set using a 70:30 ratio. This split helps in comparing algorithms and reducing overfitting. We employed 10-fold cross-validation to tune hyperparameters based on the best average precision. This method ensures that each fold of the data is used for both training and validation, providing a robust evaluation of model performance.\n\nFor the LASSO regression model, we tuned the hyperparameter lambda, which controls the penalty applied to the loss function. A higher lambda value results in a simpler model with lower predictive accuracy but reduced risk of overfitting. For the random forest model, we used grid search to find the optimal hyperparameters, including the number of trees, maximum depth of each tree, and minimum number of samples required to split or be at a leaf node. This approach helps in building an ensemble of decision trees that are less correlated, improving the model's predictive performance.\n\nThe XGBoost model, which is based on gradient-boosted decision trees, was evaluated by tuning hyperparameters such as the maximum depth of a tree, learning rate, subsample percentage, and subsample ratio of variables. This ensemble technique sequentially adds and updates models to correct errors made by previous models, enhancing the overall performance.\n\nIn our analysis, we compared the performance of four different algorithms\u2014logistic regression, LASSO regression, random forest, and XGBoost\u2014to predict hospitalizations using data from May and June 2012. We also conducted secondary analyses using different input and outcome periods to understand how the models are affected by time. For instance, we used four or six months of data to predict the health outcome in August and used the same two months of data to predict the outcome across two months (August and September). Additionally, we performed a sensitivity analysis by applying the model developed from the main analysis to a test set with data from different input periods, such as July to August predicting October outcomes. This comprehensive evaluation ensures that our models are robust and generalizable across different time periods and datasets.",
  "evaluation/measure": "In our evaluation, we primarily focus on two key performance metrics: the Area Under the Precision-Recall Curve (AUPRC) and the Area Under the Receiver Operating Characteristic Curve (AUROC). These metrics are widely recognized and used in the literature for evaluating the performance of binary classifiers, especially when dealing with imbalanced datasets.\n\nThe AUPRC is particularly informative for our study because it provides a single score that summarizes a classifier\u2019s performance, ranging from 0 to 1. This metric is crucial when evaluating binary classifiers on imbalanced data, which is the case in our study due to the unequal distribution of outcome variables. The AUPRC is more informative than the AUROC in such scenarios, as it directly measures the trade-off between precision and recall.\n\nThe AUROC, on the other hand, measures the ability of the model to distinguish between positive and negative classes. It plots the true positive rate against the false positive rate at various threshold settings, providing a balance between these two rates. An AUROC of 1.0 indicates perfect discrimination, while an AUROC of 0.5 suggests random guessing.\n\nIn addition to these metrics, we also report precision and recall at the threshold that maximizes the F-1 score for each algorithm. Precision represents the probability that subjects with a positive screening indication actually experience the outcome, while recall indicates the probability that someone who experiences the outcome will show a positive screening indication. These metrics are essential for understanding the practical implications of our models in real-world scenarios.\n\nOur choice of performance metrics is representative of the current literature. The use of AUPRC and AUROC is standard practice in evaluating machine learning models, particularly in healthcare settings where imbalanced data is common. By focusing on these metrics, we ensure that our evaluation is comprehensive and aligned with established methodologies in the field.",
  "evaluation/comparison": "In our study, we developed and compared multiple models to predict hospitalization among older duals using Medicaid Home and Community-Based Services (HCBS). The models we evaluated include LASSO regression, random forest, XGBoost, and logistic regression. Each of these approaches offers distinct advantages and disadvantages.\n\nLASSO regression is known for its transparency and interpretability, making it a valuable tool for feature selection and preventing overfitting. Random forest and XGBoost, on the other hand, are ensemble methods that typically exhibit greater predictive performance. These models leverage the strengths of multiple decision trees to improve accuracy and robustness.\n\nTo ensure a comprehensive evaluation, we divided our dataset into a training set and a test set using a 70:30 ratio. This split helps in comparing algorithms and reducing overfitting. Additionally, we employed 10-fold cross-validation to tune hyperparameters based on the best average precision. This method allows us to optimize model performance and generalize better to unseen data.\n\nFor LASSO regression, we tuned the hyperparameter lambda, which controls the penalty applied to the loss function. A higher lambda value results in a simpler model with lower predictive accuracy but reduced risk of overfitting. For random forest, we used grid search to find the optimal hyperparameters, such as the number of trees, maximum depth of each tree, and minimum number of samples required to split an internal leaf node.\n\nOur evaluation metrics included the area under the precision-recall curve (AUPRC) and the area under the receiver operating characteristic curve (AUROC). These metrics provide a comprehensive assessment of model performance, especially in the context of imbalanced datasets. The AUPRC is particularly informative for evaluating binary classifiers on imbalanced data, making it our primary focus.\n\nIn summary, our study involved a thorough comparison of different machine learning algorithms, including simpler baselines like logistic regression and more complex ensemble methods like random forest and XGBoost. This approach allows us to identify the most effective models for predicting hospitalization among HCBS users.",
  "evaluation/confidence": "The evaluation of our models includes performance metrics with accompanying 95% confidence intervals. These intervals provide a range within which the true performance metric is expected to lie, giving an indication of the precision of our estimates. For instance, when predicting hospitalization in August using different data time periods, the area under the precision-recall curve (AUPRC) for various algorithms is reported with confidence intervals. This allows for a more nuanced understanding of the model's performance and the reliability of the results.\n\nStatistical significance is crucial in determining whether the observed differences in performance between models are due to actual superiority rather than random chance. While the provided information does not explicitly state p-values or other statistical tests, the inclusion of confidence intervals implies a consideration of variability and uncertainty in the performance metrics. This is an important step towards ensuring that the claims of model superiority are robust and not merely artifacts of the data.\n\nIn summary, the performance metrics are accompanied by confidence intervals, which help in assessing the statistical significance and reliability of the results. This approach ensures that the evaluation is thorough and that the conclusions drawn about the models' performance are well-supported.",
  "evaluation/availability": "Not enough information is available."
}