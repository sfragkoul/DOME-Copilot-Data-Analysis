{
  "publication/title": "An open source pipeline for quantitative immunohistochemistry image analysis of inflammatory skin disease using artificial intelligence.",
  "publication/authors": "Ding Y, Dhawan G, Jones C, Ness T, Nichols E, Krasnogor N, Reynolds NJ",
  "publication/journal": "Journal of the European Academy of Dermatology and Venereology : JEADV",
  "publication/year": "2023",
  "publication/pmid": "36367625",
  "publication/pmcid": "PMC10947200",
  "publication/doi": "10.1111/jdv.18726",
  "publication/tags": "- Artificial Intelligence\n- Immunohistochemistry\n- Skin Disease\n- Deep Learning\n- Image Analysis\n- Inflammatory Skin Disease\n- Quantitative Analysis\n- Digital Pathology\n- Machine Learning\n- Biomedical Research",
  "dataset/provenance": "The dataset used in this study was derived from immunostained tissue slides, specifically focusing on inflammatory skin disease. A total of 45 immunostained slides were utilized, each containing 3 to 5 tissue samples, resulting in more than 4000 image blocks. These slides were annotated manually using QuPath software to delineate tissue categories such as epidermis and artifacts, as well as to identify positively and negatively immunostained cells within the tissue.\n\nFor the segmentation of epidermis from dermis, 5009 images, each measuring 700 by 700 pixels, were used. For cell segmentation, 125 images, each measuring 300 by 300 pixels, were employed. The images were split into training, testing, and validation sets to ensure robust model performance. Specifically, for epidermis segmentation, 3005 images were used for training, 1001 for testing, and 1003 were left out. For cell segmentation, 75 images were used for training, 25 for testing, and 25 were left out. This process was repeated 10 times to mitigate issues like overfitting or selection bias.\n\nThe dataset included a variety of immunostains, and the models were trained to be independent of the specific immunostain used. This approach ensures that the models can generalize well across different staining protocols. The validation process involved comparing the output of the deep learning algorithms with ground truth images, and the models were retrained if discrepancies were observed. This iterative process underscores the importance of accurate annotation for training robust models.\n\nThe dataset was further enhanced using transfer learning, where the trained models were fine-tuned on a smaller dataset involving only images with \u03b3-H2AX cases. This approach leverages pre-existing knowledge to improve model performance on specific tasks. The validation process for this fine-tuning involved 560 image blocks for epidermis and dermis segmentation and 60 image blocks for single marker cell segmentation.\n\nIn summary, the dataset consists of a comprehensive collection of immunostained tissue slides, annotated manually and used to train and validate deep learning models for quantitative immunohistochemistry image analysis. The models were designed to be robust and generalizable, ensuring their applicability across various staining protocols and tissue types.",
  "dataset/splits": "The dataset was split into three main parts for training, testing, and validation purposes. For epidermis segmentation, a total of 5009 image blocks, each measuring 700 by 700 pixels, were used. These were randomly divided into 3005 images for training, 1001 images for testing, and 1003 images that were left out. This process was repeated 10 times to ensure robustness and to mitigate issues like overfitting or selection bias. Each run was compared with the others, and an average outcome was determined.\n\nFor cell segmentation, 125 image blocks, each measuring 300 by 300 pixels, were utilized. These were randomly split into 75 images for training and 25 images for testing, with an additional 25 images left out. This split was also repeated 10 times to validate the performance of the deep learning model.\n\nAdditionally, transfer learning was applied to fine-tune the trained models on a smaller dataset involving only images with \u03b3-H2AX cases. This involved 10 slides and 44 sections. The validation process for this subset included 560 image blocks for epidermis and dermis segmentation, with 392 used for training, 140 for testing, and 28 left out. For single marker cell segmentation, 60 image blocks were used, with 42 for training, 15 for testing, and 3 left out on each run.",
  "dataset/redundancy": "The datasets used in this study were split into training, testing, and validation sets to ensure independence and to prevent data leakage. For epidermis and dermis segmentation, a total of 5009 images were used. These images were randomly split into 3005 images (60%) for training, 1001 images (20%) for testing, and 1003 images (20%) were left out for validation. This process was repeated 10 times to mitigate issues like overfitting or selection bias. Each run was compared with the ground truth to determine the average outcome.\n\nFor cell segmentation, 125 images were used. These were randomly split into 75 images (60%) for training and 25 images (20%) for testing, with an additional 25 images (20%) left out for validation. This splitting process was also repeated 10 times to ensure robustness and reliability of the results.\n\nThe independence of the training and test sets was enforced through random sampling and the use of a validation set that was not involved in the training process. This approach helps in evaluating the model's performance on unseen data, providing a more accurate assessment of its generalization capabilities.\n\nThe distribution of the datasets in this study is designed to be comprehensive and representative, ensuring that the models are trained and validated on a diverse set of images. This methodology aligns with best practices in machine learning, where the goal is to create models that can perform well on new, unseen data. The use of multiple folds in the validation process further ensures that the results are consistent and not dependent on a particular split of the data.",
  "dataset/availability": "The open source computer code utilized in this manuscript is available for download. This code is hosted on GitHub, a widely used platform for version control and collaboration, ensuring accessibility and transparency. The availability of this code aligns with our commitment to sharing methodologies and encouraging wider application and improvement in algorithm development.\n\nBy making the code open source, we facilitate the reproducibility of our results and enable other researchers to build upon our work. This approach fosters collaboration and innovation within the scientific community, allowing for the continuous refinement and enhancement of the AI model.\n\nThe decision to release the code publicly is part of our effort to promote transparency and reproducibility in scientific research. It allows other researchers to verify our findings, adapt the methods to their own datasets, and contribute to the development of more accurate and reliable AI tools for quantitative immunohistochemistry image analysis.\n\nThe code is released under a permissive license, which allows for free use, modification, and distribution. This ensures that the code can be used by anyone for any purpose, whether for academic research, commercial applications, or further development. The license also includes provisions for attribution, ensuring that the original authors are recognized for their contributions.\n\nThe enforcement of the license is managed through GitHub's platform, which provides tools for tracking contributions, managing permissions, and ensuring compliance with the license terms. This includes mechanisms for reporting violations and resolving disputes, ensuring that the code is used in accordance with the specified terms.\n\nIn summary, the open source release of our computer code is a key aspect of our commitment to transparency, reproducibility, and collaboration in scientific research. It provides a valuable resource for the research community, enabling the development of more accurate and reliable AI tools for quantitative immunohistochemistry image analysis.",
  "optimization/algorithm": "The machine-learning algorithm class used is deep learning, specifically a type of convolutional neural network (CNN). The algorithm employed is Deeplab v3+, which is a well-known open-source deep learning toolbox. Deeplab v3+ is designed to improve the segmentation of boundaries and handle the problem of segmenting objects at multiple scales. It uses an encoder-decoder layer to capture essential information from the input image.\n\nThis algorithm is not new; it has been previously trained on a large dataset of regular objects to classify images into various categories. It was refined using annotated images in MATLAB to differentiate between various histology features. The algorithm was trained using two Nvidia GTX 2080 TI GPUs, demonstrating that it does not require expensive, specialized high-performance clusters.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of the study was on its application in the context of quantitative immunohistochemistry image analysis for inflammatory skin disease. The development and validation of the AI-assisted analysis pipeline, along with the open-source sharing of the methodology and code, were the primary goals. The algorithm's effectiveness in segmenting and quantifying immunostained skin samples was the key contribution, rather than the innovation of the algorithm itself.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. Instead, it employs a sequence of two deep learning models. The first model is trained to label tissue areas, specifically focusing on the epidermis. The second model is then applied to label negative and positive immunostained cells within the identified tissue areas.\n\nThe deep learning models used are based on Deeplab V3 architecture. The training process involves manually annotated images, which are used to teach the models to recognize and segment different structures within the images. The annotations include color-labeled categories that delineate tissue types and immunostained cells.\n\nThe validation process ensures that the models are trained and tested on independent datasets. For epidermis segmentation, 3005 image blocks were used for training, 1001 for testing, and 1003 were left out. This process was repeated 10 times to mitigate issues like overfitting or selection bias. Similarly, for cell segmentation, 75 image blocks were used for training, 25 for testing, and 25 were left out. Each run was compared to ground truth images to ensure accuracy.\n\nThe models were further refined using transfer learning on a smaller dataset involving only images with \u03b3-H2AX cases, which improved the segmentation accuracy. The final models were validated to show high GlobalAccuracy and WeightedIoU, indicating reliable performance in identifying and segmenting the desired structures.\n\nIn summary, the approach does not involve a meta-predictor but rather a sequential application of deep learning models trained and validated on independent datasets to ensure robust and accurate segmentation of tissue and cellular structures in immunostained skin samples.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, digital pathology images were used as input data. These images were annotated manually in QuPath, with two sets of annotations: one for delineating tissue categories such as epidermis and artifacts, and another for identifying positively and negatively immunostained cells within the tissue.\n\nFor the segmentation of the epidermis from the dermis, 5009 images, each 700 by 700 pixels, were used. For segmenting positively labeled from negatively labeled immunostained cells, 125 images, each 300 by 300 pixels, were utilized. The images were binarized to produce masks where specific values represented different structures. For example, in the epidermis segmentation output, 1 represented the epidermis, 2 represented the dermal papilla, 3 represented hair follicles, sweat glands, and other artifacts, and 4 represented the background. Similarly, in the cell segmentation output, 1 represented the positive cell membrane, 2 represented the negative cell membrane, 3 represented the positive cell nucleus, 4 represented the negative cell nucleus, and 5 represented other non-cell structures.\n\nThe annotated data was fed into a pre-trained deep learning model, specifically Deeplab v3+, which is an 18-layer deep convolutional neural network (CNN). This model was designed to improve the segmentation of boundaries and handle the problem of segmenting objects at multiple scales using an encoder-decoder architecture. The model had been previously trained on a large dataset of regular objects to classify images into 1000 object categories, enabling it to learn rich feature representations suitable for tasks with limited processing resources.\n\nDuring the training process, the model made predictions about the annotated data, and the errors in these predictions were used to update the strengths of the connections within the neural networks. This process continued until the model achieved sufficient prediction accuracy. The training was performed using two Nvidia GTX 2080 TI GPUs, which did not require expensive, specialized high-performance clusters.\n\nAdditionally, adaptive image thresholding was implemented to enhance the performance of cell segmentation. This method automatically defined the optimal thresholding value on the grayscale image of the original slide, where 1 represented the positive stain and 0 represented other structures. This step was crucial due to the time and logistical constraints of labeling each cell in a section, resulting in a relatively small training dataset for cell segmentation.\n\nThe preprocessing also included removing segmented objects such as noise, which were image objects with an overall size of 100 pixels or less. A watershed-based method was applied to the binary image to split clustered cells. The AI algorithms were retrained when discrepancies to the ground truth were observed, and amendments to labeling or the inclusion of further samples were made as needed. Finally, the images were post-processed to produce quantification of the cell ratio (positive/negative) within epidermal regions.",
  "optimization/parameters": "In our study, we utilized Deeplab v3+, a well-known open-source deep learning toolbox, which is an 18-layer deep convolutional neural network (CNN). This model is designed to improve the segmentation of boundaries and handle the problem of segmenting objects at multiple scales. The architecture of Deeplab v3+ includes an encoder-decoder layer that captures essential information from the input image.\n\nThe selection of the model parameters was guided by the need to balance computational efficiency and segmentation accuracy. Deeplab v3+ has been pre-trained on a large dataset of regular objects, classifying images into 1000 object categories. This pre-training allows the model to learn rich feature representations suitable for tasks with limited processing resources. For our specific application, the model was refined using annotated images in MATLAB to differentiate various histology features.\n\nThe training process involved feeding annotated data to the pre-trained AI model, which then made predictions about the data. The errors in these predictions were used to update the strengths of the connections within the neural networks. This iterative process continued until the AI achieved sufficient prediction accuracy.\n\nTwo deep learning models were trained and applied sequentially. The first model was used to label the epidermis, and the second model was used to label negatively and positively immunostained cells. For the segmentation of the epidermis from the dermis, 5009 images (700 by 700 pixels) were used. For segmenting positively labeled from negatively labeled immunostained cells, 125 images (300 by 300 pixels) were utilized.\n\nThe models were trained using two Nvidia GTX 2080 TI GPUs, demonstrating that high-performance computing clusters are not necessary for training. The first algorithm processed the entire slide region by region, applying semantic segmentation specifically to areas where specimens were located. The output resulted in two grayscale images, which were then binarized to produce masks for further analysis.\n\nIn summary, the model parameters were selected based on the architecture of Deeplab v3+, which is designed to handle multi-scale segmentation efficiently. The training process involved iterative refinement using annotated images, ensuring that the model achieved high accuracy in segmenting histology features.",
  "optimization/features": "The input features for the deep learning models used in this study are derived from digital pathology images. Specifically, the images are segmented into blocks of 700 by 700 pixels for epidermis and dermis segmentation, and 300 by 300 pixels for cell segmentation. These image blocks serve as the primary input features for the models.\n\nFeature selection in the traditional sense is not explicitly performed, as deep learning models like Deeplab v3+ used in this study automatically learn relevant features from the input data. However, the process of annotating and labeling the images can be seen as a form of feature engineering, where the relevant regions (e.g., epidermis, dermis, positive and negative cells) are highlighted for the model to focus on.\n\nThe annotation process was conducted using QuPath, where images were manually labeled to differentiate between various tissue categories and cell types. This annotated data was then used to train the deep learning models. The training set consisted of 3005 image blocks for epidermis and dermis segmentation and 75 image blocks for cell segmentation, ensuring that the feature engineering was done using the training set only. This approach helps in maintaining the integrity of the validation and testing sets, preventing data leakage and ensuring robust model performance.",
  "optimization/fitting": "The fitting method employed in this study involved the use of deep learning models, specifically Deeplab v3+, which is a convolutional neural network (CNN) designed for semantic segmentation. The number of parameters in these models is indeed much larger than the number of training points, a common characteristic of deep learning architectures.\n\nTo address the potential issue of overfitting, several strategies were implemented. Firstly, the dataset was split into training, validation, and test sets multiple times (10-fold validation) to ensure that the model's performance was consistent across different subsets of the data. This process helped to identify and mitigate overfitting by ensuring that the model generalized well to unseen data. Additionally, the models were validated using a variety of metrics, including GlobalAccuracy, MeanAccuracy, Intersection over Union (IoU), and Mean Boundary F1 Score, which provided a comprehensive evaluation of the model's performance.\n\nFurthermore, transfer learning was applied to fine-tune the models on a smaller, more specific dataset involving images with \u03b3-H2AX cases. This approach leveraged the pre-trained models' existing knowledge to improve performance on the specific task at hand, reducing the risk of overfitting.\n\nTo rule out underfitting, the models were trained on a sufficiently large and diverse dataset, comprising 5009 images for epidermis segmentation and 125 images for cell segmentation. The models were also re-trained when discrepancies with the ground truth were observed, ensuring that they could capture the necessary complexity of the data. The use of adaptive image thresholding further enhanced the models' ability to accurately segment cells, addressing potential underfitting issues.\n\nIn summary, the fitting method involved rigorous validation processes, the use of transfer learning, and adaptive thresholding to balance the trade-off between overfitting and underfitting, ensuring robust and generalizable model performance.",
  "optimization/regularization": "To prevent overfitting, a validation process was performed to quantify the performance of the AI methodologies. This involved randomly selecting image blocks for training and testing the deep learning model. Specifically, for epidermis segmentation, 3005 image blocks were used for training, 1001 for testing, and 1003 were left out. This process was repeated 10 times to flag problems such as overfitting or selection bias. Each run was compared with each other, and an average outcome was determined. This approach helped ensure that the model generalized well to unseen data and did not merely memorize the training set.\n\nAdditionally, transfer learning was applied to fine-tune the trained models on a smaller dataset involving only images with \u03b3-H2AX cases. This technique allowed the model to adapt to specific types of data, further enhancing its performance and reducing the risk of overfitting. The validation process was also performed on this refined model to ensure its accuracy and reliability.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters are not explicitly detailed in the publication. However, the open-source computer code utilized in this manuscript is available for download. This code includes the implementation of the deep learning models used for segmentation tasks. The code repository can be accessed at a specified URL, which allows for the examination and potential modification of the hyper-parameters, optimization schedules, and other relevant details. The code is made available under an open-source license, encouraging sharing, wider application, and subsequent improvement in algorithm development. This approach supports transparency and reproducibility in the research, enabling other researchers to build upon the work and refine the models further.",
  "model/interpretability": "The models employed in our study are primarily based on deep learning, specifically convolutional neural networks (CNNs), which are often considered black-box models due to their complex, multi-layered architecture. However, efforts were made to enhance interpretability and transparency in our approach.\n\nThe first model, Deeplab v3+, is designed to improve the segmentation of boundaries and handle objects at multiple scales. It uses an encoder-decoder architecture that processes input images to produce segmented outputs. While the internal workings of the CNN are not easily interpretable, the outputs are clear: the model segments different histological features, such as the epidermis, dermal papilla, hair follicles, and other artifacts. The output images are binarized, where specific values represent different structures, making it straightforward to understand what each part of the image represents.\n\nFor instance, in the segmentation of the epidermis, the output image uses the value 1 to represent the epidermis, 2 for the dermal papilla, 3 for hair follicles and other artifacts, and 4 for the background. This binarization helps in visualizing and interpreting the model's decisions. Similarly, for cell segmentation, the model outputs values that represent positive and negative cell membranes and nuclei, providing a clear distinction between different cellular components.\n\nAdditionally, the use of transfer learning and fine-tuning on a smaller, annotated dataset helps in making the model more interpretable. By training the model on specific cases, such as \u03b3-H2AX immunostained images, we can better understand how the model generalizes to new, similar data. The validation process, which involves testing the model on held-out data, further ensures that the model's performance is consistent and interpretable.\n\nMoreover, the adaptive image thresholding method used enhances the performance by automatically defining optimal thresholding values. This method provides a clear criterion for distinguishing between positive stains and other structures, making the segmentation process more transparent.\n\nIn summary, while the deep learning models used are inherently complex, the outputs are designed to be interpretable. The use of clear labeling, binarization, and validation processes helps in understanding and trusting the model's decisions. This approach balances the need for advanced, accurate segmentation with the requirement for interpretability in scientific research.",
  "model/output": "The model employed in our study is primarily designed for semantic segmentation, which is a type of classification task at the pixel level. It categorizes each pixel in an image into one of several predefined classes. Specifically, our deep learning models were trained to distinguish between different histological features in skin tissue images.\n\nThe first model focuses on segmenting the epidermis from the dermis and other structures, such as hair follicles and sweat glands. The output of this model is a grayscale image where different values represent different tissue types. For instance, the epidermis is labeled with a value of 1, while the dermis and other structures are labeled with different values.\n\nThe second model is used to label positively and negatively immunostained cells within the tissue. The output of this model is another grayscale image where different values represent different cell types and staining statuses. For example, positive cell membranes are labeled with a value of 1, negative cell membranes with a value of 2, and so on.\n\nThese segmented images are then used for further quantitative analysis, such as counting the number of positive and negative cells and calculating the percentage of positive cells. The models were trained using a large dataset of annotated images, and their performance was validated using various metrics, including Global Accuracy, Mean Accuracy, Intersection over Union (IoU), and Mean Boundary F1 Score. The results showed high accuracy in segmenting both tissue areas and cells, demonstrating the effectiveness of our deep learning approach for quantitative immunohistochemistry image analysis.",
  "model/duration": "The execution time for training the deep learning models was efficiently managed using two Nvidia GTX 2080 TI GPUs. These GPUs allowed for the training process to be completed without the need for expensive, specialized high-performance computing clusters. The models were trained on a substantial dataset, with 5009 images used for epidermis segmentation and 125 images for cell segmentation. The training process involved multiple iterations and validations to ensure accuracy. Specifically, the models were trained and validated in a 10-fold cross-validation process, which helped in identifying and mitigating issues like overfitting or selection bias. This rigorous training and validation process ensured that the models could accurately segment and quantify the desired features in the images. The use of GPUs significantly accelerated the training time, making the process feasible and efficient.",
  "model/availability": "The source code for the pipeline described in this publication is publicly available. It has been made open source to encourage sharing of methodologies, wider application, and improvement in algorithm development. The computer code can be downloaded from a GitHub repository. The specific URL for the repository is not sure. The code is released under a license that permits use, distribution, and reproduction in any medium, provided the original work is properly cited. This aligns with the principles of open science and collaborative development.",
  "evaluation/method": "The evaluation method employed a rigorous validation process to quantify the performance of the AI methodologies. For epidermis segmentation, 3005 image blocks, each 700 by 700 pixels, were randomly selected for training the deep learning model. Additionally, 1001 image blocks were used for testing, and 1003 were left out. This process was repeated 10 times to identify issues such as overfitting or selection bias. Each run was compared with the others, and an average outcome was determined.\n\nFor cell segmentation, 75 image blocks, each 300 by 300 pixels, were randomly selected for training the deep learning model. This was also repeated 10 times, with 25 image blocks used for testing and 25 left out in each run.\n\nSeveral metrics were used to evaluate the performance of the semantic segmentation algorithms. These included Global Accuracy, which represents the percentage of correctly classified pixels. Mean Accuracy indicates the percentage of correctly identified pixels for each class. Intersection over Union (IoU) computes the intersection of the output image and the ground truth divided by the union of both images, also known as the Jaccard similarity coefficient. Mean IoU represents the average IoU of each class, weighted by the number of pixels in that class. Weighted IoU is the average IoU of each class, weighted by the number of pixels in that class. Finally, the Mean Boundary F1 (BF) Score indicates how well the predicted boundary of each class aligns with the true boundary.\n\nThe results of each validation run and the average of the 10 runs were presented in tables. For epidermis segmentation, the average Global Accuracy was 94.95%, and the Weighted IoU was 90.50%. For cell segmentation, the average Global Accuracy was 86.09%, and the Weighted IoU was 77.74%.\n\nAdditionally, a quantitative analysis was performed for negative and positive immunostained cells based on the segmented images using the deep learning model versus manually labeled images. The data showed that the number of positive and negative cells was less accurate compared to the percentage of positive cells and average signal. This discrepancy was attributed to the dataset containing images with mixed staining protocols, such as those with nuclear and cytoplasmic immunostained signals, which resulted in lower segmentation accuracy.",
  "evaluation/measure": "In our evaluation, we employed several performance metrics commonly used to assess semantic segmentation algorithms. These metrics provide a comprehensive view of the model's accuracy and reliability.\n\nGlobal Accuracy represents the percentage of correctly classified pixels across all classes. It is calculated as the sum of true positives and true negatives divided by the total number of pixels. This metric gives an overall sense of how well the model is performing.\n\nMean Accuracy indicates the percentage of correctly identified pixels for each class individually. It is calculated as the true positives divided by the sum of true positives and false negatives for each class. This metric is useful for understanding the model's performance on a per-class basis.\n\nIntersection over Union (IoU), also known as the Jaccard similarity coefficient, measures the overlap between the predicted segmentation and the ground truth. It is calculated as the intersection of the predicted and ground truth regions divided by their union. Mean IoU averages this value across all classes, providing a single metric that reflects the model's performance.\n\nWeighted IoU is similar to Mean IoU but takes into account the number of pixels in each class. This metric is particularly useful when dealing with imbalanced datasets, where some classes are more prevalent than others.\n\nMean Boundary F1 (BF) Score assesses how well the predicted boundaries of each class align with the true boundaries. It is a measure of contour matching and provides insight into the model's ability to accurately delineate the edges of segmented regions.\n\nThese metrics collectively offer a robust evaluation of the model's performance, ensuring that it can accurately segment and quantify immunostained skin samples. The reported metrics are representative of those used in the literature for similar tasks, providing a standardized way to compare our results with other studies in the field.",
  "evaluation/comparison": "Not applicable. The publication focuses on the development and validation of a specific deep learning pipeline for quantitative immunohistochemistry image analysis of inflammatory skin disease. It does not mention any comparisons to publicly available methods or simpler baselines on benchmark datasets. The evaluation primarily involves internal validation processes, such as 10-fold cross-validation, to assess the performance of the developed models. The study emphasizes the use of Deeplab V3+ for segmentation tasks and the importance of accurate annotation for training the models. The validation metrics used include GlobalAccuracy, MeanAccuracy, MeanIoU, WeightedIoU, and Mean Boundary F1 Score, which are applied to evaluate the segmentation of epidermis and positive/negative cells. The results indicate high accuracy in segmenting the epidermis and reasonable accuracy in segmenting positive and negative cells, with some variability due to mixed staining protocols. The study also highlights the potential for further refinement of the AI model and encourages open-source collaboration for improvement.",
  "evaluation/confidence": "The evaluation of our deep learning models involved a rigorous validation process to ensure the reliability and accuracy of the results. We performed a 10-fold cross-validation, where the dataset was randomly split into training, validation, and test sets multiple times. This process helped to evaluate the performance of the models and to identify any potential issues such as overfitting or selection bias.\n\nSeveral performance metrics were used to assess the models' accuracy. These metrics included GlobalAccuracy, MeanAccuracy, Intersection over Union (IoU), WeightedIoU, and Mean Boundary F1 (BF) Score. GlobalAccuracy represents the percentage of correctly classified pixels, while MeanAccuracy indicates the percentage of correctly identified pixels for each class. IoU computes the intersection of the output image and the ground truth divided by the union of both images, and WeightedIoU is the average IoU of each class, weighted by the number of pixels in that class. The Mean Boundary F1 Score indicates how well the predicted boundary of each class aligns with the true boundary.\n\nThe results of each validation run and the average of the 10 runs are presented in tables. For epidermis segmentation, the average GlobalAccuracy was 94.95%, and the WeightedIoU was 90.50%. For cell segmentation, the average GlobalAccuracy was 86.09%, and the WeightedIoU was 77.74%. These metrics provide a comprehensive evaluation of the models' performance and demonstrate their accuracy in segmenting different structures in the images.\n\nAdditionally, we performed a quantitative analysis comparing the segmented images using the deep learning model with manually labeled images. The data showed that while the number of positive and negative cells was less accurate, the percentage of positive cells and the average signal were more reliable. This discrepancy is attributed to the variation in immunostaining protocols and the presence of mixed staining signals in the dataset.\n\nTo address this issue, we used transfer learning to fine-tune the trained models on a smaller dataset that involved only images with a nuclear immunostained signal. This refinement significantly improved the accuracy of cell segmentation, with an average GlobalAccuracy of 91.01%.\n\nIn summary, the evaluation process involved multiple validation runs, comprehensive performance metrics, and statistical analysis to ensure the reliability and accuracy of the deep learning models. The results demonstrate the models' superior performance in segmenting epidermis and cells, making them suitable for quantitative analysis in inflammatory skin disease.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being available. However, the open source computer code utilized in this manuscript is available for download. This code can be used to reproduce the evaluation process and results. The code is hosted on GitHub, under the username khcy82dyc2, in a repository named SkinAI. The license under which the code is released is not specified, but it is open source, encouraging sharing and improvement of methodologies."
}