{
  "publication/title": "3D brain glioma segmentation in MRI through integrating multiple densely connected 2D convolutional neural networks.",
  "publication/authors": "Zhang X, Hu Y, Chen W, Huang G, Nie S",
  "publication/journal": "Journal of Zhejiang University. Science. B",
  "publication/year": "2021",
  "publication/pmid": "34128370",
  "publication/pmcid": "PMC8214948",
  "publication/doi": "10.1631/jzus.b2000381",
  "publication/tags": "- Brain Tumor Segmentation\n- Glioma\n- Convolutional Neural Networks\n- MRI\n- Deep Learning\n- Medical Imaging\n- Cross-Entropy Loss\n- Densely Connected Networks\n- Brain Tumor Classification\n- Image Segmentation\n- Neural Networks\n- Medical Image Analysis\n- Brain Tumor Detection\n- 2D-CNN\n- Brain Tumor Segmentation Accuracy",
  "dataset/provenance": "The dataset used in our study is the BraTS (Brain Tumor Segmentation) dataset, which is a widely recognized benchmark in the field of brain tumor segmentation. This dataset has been utilized in various challenges, including BraTS 2013 and BraTS 2018, to evaluate the performance of different segmentation methods.\n\nFor our experiments, we utilized the BraTS 2013 training dataset, which includes 30 cases comprising 20 cases of high-grade gliomas (HGG) and 10 cases of low-grade gliomas (LGG). This dataset provides multi-modal MRI scans, including T1-weighted, T1-weighted with contrast enhancement, T2-weighted, and FLAIR images. These modalities are essential for accurately segmenting different tumor regions, such as the enhancing tumor, tumor core, and whole tumor.\n\nAdditionally, we conducted preliminary tests using 20% (57 cases) of the BraTS 2018 dataset. The BraTS 2018 dataset is known for its comprehensive and diverse set of brain tumor cases, making it a robust choice for validating the generalizability of our segmentation method.\n\nThe BraTS dataset has been extensively used by the research community for developing and evaluating brain tumor segmentation algorithms. Our work builds upon this foundation, leveraging the rich data provided by BraTS to train and validate our densely connected 2D-CNN models. The dataset's multi-modal nature allows for a more comprehensive analysis and segmentation of brain tumors, which is crucial for improving diagnostic and treatment planning accuracy.",
  "dataset/splits": "The dataset used in our study is from the 2018 global multi-modal brain tumor segmentation challenge (BraTS 2018). We utilized two main splits for our experiments: a training set and a test set.\n\nThe training set consisted of 210 cases of high-grade glioma (HGG) and 75 cases of low-grade glioma (LGG), making a total of 285 cases. From this training set, 80% of the data was used for training the models, while the remaining 20% (57 cases) was reserved for preliminary testing.\n\nThe validation dataset consisted of 66 cases with unknown grades. This dataset was used for online evaluation since it included four MRI sequences per patient but lacked ground truth labels.\n\nAdditionally, to further test the generalization ability of our segmentation model, we also used the BraTS 2013 training dataset. This dataset included 20 cases of HGG and 10 cases of LGG, totaling 30 cases.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The datasets used in our study are publicly available through the BraTS (Brain Tumor Segmentation) challenges. Specifically, we utilized the BraTS 2018 and BraTS 2013 datasets for training and testing our segmentation models. These datasets are provided by the global multi-modal brain tumor segmentation challenge and include various MRI sequences for each patient, along with ground truth labels manually determined by experts.\n\nThe BraTS 2018 training dataset consists of 285 cases, including 210 high-grade glioma (HGG) cases and 75 low-grade glioma (LGG) cases. Each case includes four MRI sequences: FLAIR, T1, T1c, and T2. The validation dataset for BraTS 2018 comprises 66 cases with unknown grades, and the test dataset includes an additional set of cases for evaluation. Similarly, the BraTS 2013 training dataset includes 30 cases, with 20 HGG cases and 10 LGG cases.\n\nThese datasets are accessible to the research community, and the specific splits used for training, validation, and testing are clearly defined in the challenge guidelines. The data is released under a license that allows for academic and research use, ensuring that the datasets can be utilized by other researchers for similar studies. The enforcement of data usage is typically managed through the challenge organizers, who provide access to the datasets and monitor their usage to ensure compliance with the licensing agreements.",
  "optimization/algorithm": "The machine-learning algorithm class used is convolutional neural networks (CNNs), specifically densely connected 2D-CNNs. This approach is not entirely new, as CNNs have been widely used in various applications, including image segmentation. However, the specific implementation and modifications applied to the loss function are novel contributions of this work.\n\nThe reason this study was not published in a machine-learning journal is that the primary focus is on the application of these models to brain tumor segmentation in medical imaging. The improvements made to the loss function and the evaluation of the model's performance on the BraTS 2018 dataset are more aligned with the biomedical and biotechnology fields. The modifications to the loss function are designed to address specific challenges in medical image segmentation, such as overfitting and data imbalance, which are critical issues in this domain.\n\nThe use of densely connected 2D-CNNs allows for effective feature extraction from different views of MRI scans, including axial, coronal, and sagittal views. The backpropagation (BP) algorithm and the stochastic batch gradient descent (SGD) algorithm are employed to supervise the loss function and optimize the network parameters. These optimization techniques are standard in training deep learning models and ensure that the model learns effectively from the training data.\n\nThe improved loss function incorporates a uniform distribution component, which helps to mitigate overconfidence in the softmax predictions and prevents overfitting. This modification is crucial for enhancing the segmentation accuracy and reliability of the model, particularly in medical applications where precise segmentation is essential for diagnosis and treatment planning.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, we employed a meticulous data encoding and preprocessing strategy to enhance the performance of our machine-learning algorithm. Initially, we extracted forty patches from each slice, ensuring an equal number of patches for different classes to mitigate data imbalance issues. This approach helped in maintaining a balanced dataset, which is crucial for training robust models.\n\nThe preprocessing involved several steps. First, we combined the FLAIR and T2 modalities using algebraic superposition fusion to create an enhanced image of the edema region. This step was essential for highlighting the differences in transverse relaxation between tissues, making it easier to observe diseased areas. The formula used for this enhancement was:\n\nI_enhance = (FLAIR + T2) / (FLAIR \u00d7 T2)\n\nNext, we normalized the FLAIR, I_enhance, T1c, and T1 data to have zero mean and unit variances. This normalization process preserved the original information of the images to the greatest extent possible. Additionally, we normalized the patches with respect to their mean and variance to ensure consistency across the dataset.\n\nThe input size for our network was set to 33 \u00d7 33 \u00d7 4, where the four channels corresponded to the patches extracted from the preprocessed FLAIR, T1c, I_enhance, and T1 modalities, respectively. This multi-modal approach allowed the model to leverage diverse information from different MRI sequences, improving the overall segmentation accuracy.\n\nTo further enhance the model's performance, we used small convolution kernels of size 3 \u00d7 3. This design choice was inspired by research indicating that smaller kernels can reduce the number of parameters in the network, making it more efficient while maintaining performance. We also incorporated densely connected blocks into our network architecture. These blocks allowed each layer to accept the features of all previous layers as input, promoting better feature reuse and utilization. The densely connected block was formulated as:\n\ny^(m+1) = f([y^(0), y^(1), ..., y^(m)])\n\nwhere f(y^(n)) = W * \u03b4(B(y^(n))) and [y^(0), y^(1), ..., y^(m)] represents the concatenations of all outputs of previous layers before the layer of m+1.\n\nIn summary, our data encoding and preprocessing strategy involved extracting balanced patches, combining modalities to enhance edema regions, normalizing data, and using efficient network architectures with small convolution kernels and densely connected blocks. These steps collectively contributed to the improved performance of our machine-learning algorithm in segmenting brain gliomas.",
  "optimization/parameters": "The model utilizes an input size of 33 \u00d7 33 \u00d7 4, where the four channels correspond to different MRI modalities: FLAIR, T1c, T1-enhanced, and T1. The initial convolution layer employs 64 kernels, each with a size of 3 \u00d7 3. The architecture includes two densely connected blocks, with specific configurations detailed in accompanying tables.\n\nThe training batch size is set to 64, and the initial learning rate is 0.0001. The learning rate is adjusted by dividing it by 10 after every 30 epochs. Convolution weights are regularized using L1 regularization with a coefficient of 0.0001. The model employs a cross-entropy loss function, enhanced with a uniform distribution component to mitigate overfitting.\n\nThe backpropagation (BP) algorithm and stochastic batch gradient descent (SGD) are used to supervise the loss function and optimize the network parameters. This process results in three optimized densely connected 2D-CNN models for axial, coronal, and sagittal views.",
  "optimization/features": "The input features for the model consist of image patches extracted from four different MRI modalities: FLAIR, T1, T1c, and T2. Each input patch has a size of 33 \u00d7 33 pixels, and there are four channels corresponding to the four MRI modalities. Therefore, the input size is 33 \u00d7 33 \u00d7 4.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the patches were extracted equally from different classes to alleviate data imbalance. The training image patches and their labels from axial, coronal, and sagittal views were used directly as inputs to the densely connected 2D-CNN model. This approach ensures that the model receives a balanced and representative set of features for training.",
  "optimization/fitting": "In our study, we initially employed the cross-entropy loss function for training our densely connected 2D-CNN models. However, we observed over-fitting issues as the training epochs increased. To address this, we introduced a new loss function that combines the traditional cross-entropy loss with a uniform distribution term. This modification helps to prevent the model from becoming overly confident in its predictions, which is a common issue with the softmax function.\n\nThe new loss function is defined as:\n\nL o s s = - ( 1 - \u03b5 ) \u2211\ni\nq\ni\nl n p\ni\n- \u03b5 \u2211\ni = 1\nn\n1\nn\nq\ni\nl n p\ni\n\nwhere p\ni\nis the predicted distribution, q\ni\nis the true distribution, n represents the number of categories, and \u03b5 is a constant. This approach ensures that the model does not simply fit the one-hot distribution but also considers a uniform distribution, thereby reducing over-fitting.\n\nTo further validate the effectiveness of our new loss function, we conducted experiments on the BraTS 2018 dataset. The results demonstrated that the improved loss function significantly increased the segmentation accuracy from 80% to 86%. Throughout the training process, there was no evidence of over-fitting or under-fitting, as the loss continuously decreased and the network was optimized.\n\nAdditionally, we used techniques such as data augmentation and regularization to prevent over-fitting. The number of extracted patches for different classes was kept equal, which helped in alleviating the problem of data imbalance. The training image patches and their labels from axial, coronal, and sagittal views were fed into the densely connected 2D-CNN model, and the backpropagation (BP) algorithm along with the stochastic batch gradient descent (SGD) algorithm were used to supervise the loss function and optimize the network parameters.\n\nIn summary, our approach effectively addressed both over-fitting and under-fitting issues by introducing a new loss function and employing robust training strategies. This ensured that our model generalized well to unseen data and achieved high segmentation accuracy.",
  "optimization/regularization": "In our study, we encountered over-fitting issues during the training of our network using the cross-entropy loss function. To address this, we proposed a new loss function designed to mitigate over-fitting. This loss function combines the traditional cross-entropy loss with a uniform distribution component. The formula for our loss function is:\n\nL o s s = - ( 1 - \u03b5 ) \u2211\ni\nq\ni\nl n p\ni\n- \u03b5 \u2211\ni = 1\nn\n1\nn\nq\ni\nl n p\ni\n\nwhere p\ni\nis the predicted distribution, q\ni\nis the true distribution, n represents the number of categories, and \u03b5 is a constant. The first part of this function is the standard cross-entropy loss used in classification tasks, while the second part introduces a uniform distribution component. This addition helps to prevent the model from becoming overly confident in its predictions, which is a common issue with the softmax function. By including the uniform distribution, we ensure that the model does not simply fit the one-hot distribution but also considers a more balanced approach, thereby reducing over-fitting.\n\nThe effectiveness of this new loss function was demonstrated in the BraTS 2018 dataset, where it significantly improved segmentation accuracy from 80% to 86%. Throughout the training process, there was no evidence of over-fitting or under-fitting, and the loss continuously decreased, indicating ongoing optimization of the network. This approach highlights the importance of incorporating a uniform distribution into the cross-entropy loss function to enhance model performance and stability.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our experiments are detailed within the publication. Specifically, for the axial densely connected 2D-CNNs, the input size is 33 \u00d7 33 \u00d7 4, with patches extracted from pre-processed FLAIR, T1c, T1-enhanced, and T1 modalities. The training batch size is set to 64, and the initial learning rate is 0.0001, which is divided by 10 after each 30 epochs. The convolution weight is regularized using L1 regularization with a coefficient of 0.0001.\n\nThe backpropagation (BP) algorithm and the stochastic batch gradient descent (SGD) algorithm are employed to supervise the loss function and optimize the network parameters. An improved cross-entropy loss function is utilized to enhance segmentation accuracy and prevent overfitting. This loss function combines the traditional cross-entropy with a uniform distribution component, which helps in avoiding overconfidence in the predictions.\n\nThe model files and specific optimization parameters are not explicitly provided in the text, but the methods and configurations are thoroughly described, allowing for replication of the experiments. The datasets used, including the BraTS 2018 and BraTS 2013 training datasets, are publicly available and can be accessed for further research. The implementation details mention the use of Keras and TensorFlow as backends, which are open-source libraries, facilitating the reproducibility of the models.\n\nRegarding the license, the publication does not specify the licensing terms for the datasets or the code used. However, Keras and TensorFlow are open-source and available under the Apache 2.0 license, which permits free use, modification, and distribution. For the datasets, it is common practice to refer to the original dataset providers for licensing information, typically available on their respective websites or repositories.",
  "model/interpretability": "The model employed in our study is a densely connected 2D-CNN, which inherently offers more interpretability compared to traditional black-box models. This architecture allows for a clearer understanding of the feature extraction process. The model processes input patches from different MRI modalities, specifically FLAIR, T1c, T1-enhanced, and T1, to classify and segment various regions of interest such as edema, necrosis/non-enhancing tumor, and enhancing tumor.\n\nThe use of densely connected blocks in our CNN ensures that each layer receives inputs from all preceding layers, facilitating better gradient flow and feature reuse. This design choice enhances the model's ability to learn and represent complex patterns in the data, making it more interpretable. The concatenation of low-level and high-level features further aids in understanding how different levels of abstraction contribute to the final segmentation results.\n\nAdditionally, the model's training process involves a clear and structured approach. The backpropagation algorithm and stochastic batch gradient descent are used to optimize the network parameters, and the loss function is carefully designed to prevent overfitting. The improved cross-entropy loss function, which includes a uniform distribution component, helps in reducing overconfidence in the predictions, thereby making the model's decisions more reliable and interpretable.\n\nThe evaluation metrics used, such as dice score, sensitivity, and specificity, provide a quantitative measure of the model's performance. These metrics help in understanding the model's strengths and weaknesses in segmenting different regions of the brain tumor. The dice score, for instance, measures the overlap between the predicted and ground truth regions, providing a clear indication of the model's accuracy.\n\nIn summary, the densely connected 2D-CNN model used in our study is not a black-box model. Its architecture and training process are designed to enhance interpretability, allowing for a better understanding of how the model makes its predictions. The use of clear evaluation metrics further aids in interpreting the model's performance and reliability.",
  "model/output": "The model is a classification model. It is designed to segment brain tumors from MRI images, which involves classifying each pixel in the image into one of four categories: background, edema region, necrosis/non-enhancing tumor, and enhancing tumor. The model uses a softmax classifier to output the predicted distribution over these classes. The final output is a segmentation map that indicates the presence and type of tumor in the input MRI image.\n\nThe model employs densely connected 2D convolutional neural networks (2D-CNNs) to extract features from the input MRI patches. These patches are extracted from different views (axial, coronal, and sagittal) of the MRI images. The model is trained using a combination of cross-entropy loss and a uniform distribution function to prevent overfitting and overconfidence in the predictions. The training process involves optimizing the network parameters using backpropagation and stochastic batch gradient descent algorithms.\n\nThe output of the model is evaluated using metrics such as dice score, sensitivity, and specificity. These metrics measure the overlap between the predicted lesion regions and the ground truth, as well as the accuracy of tumor and normal region classification. The model's performance is assessed on various datasets, including the BraTS 2018 and BraTS 2013 datasets, to ensure its generalization ability. The final segmentation results are obtained by combining the classification results of each pixel in the image.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our method involved several key steps and metrics to ensure its effectiveness and robustness. We utilized three primary metrics to assess the experimental results: dice score, sensitivity, and specificity. These metrics were calculated using the formulas for true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n\nThe dice score measures the overlap area between the predicted lesion region and the ground truth. Sensitivity assesses the proportion of tumor pixels that were correctly classified, while specificity evaluates the proportion of normal regions that were correctly classified. These metrics were applied to different regions of the tumor, including the complete region, core region, and enhancing region.\n\nIn the test phase, the segmentation task was transformed into a classification task, where each image patch was classified to achieve segmentation. To ensure that every pixel in the image was predictable, we expanded the image size by filling zeros around the original image. We then captured 33x33 image patches column by column and sent them to the segmentation model to obtain the classification results for the central pixels. The segmentation result for the entire slice was obtained by combining the classification results of each pixel.\n\nFor our preliminary tests, we used 20% of the data from the BraTS 2018 dataset, which consisted of 57 cases. The evaluation results for axial, coronal, and sagittal densely connected 2D-CNNs, as well as fusion processing and post-processing, were documented. The results indicated that pre-processing significantly improved the segmentation accuracy, particularly for the edema area of brain gliomas. The fusion of segmentation results from different views also markedly enhanced the segmentation accuracy.\n\nAdditionally, we evaluated our method on the BraTS 2018 validation dataset, with the results provided by the BraTS 2018 organizers. Our method demonstrated superior sensitivity scores for the enhancing tumor, whole tumor, and tumor core regions compared to other methods participating in the BraTS 2018 challenge. This evaluation highlighted the effectiveness and robustness of our approach in brain tumor segmentation.",
  "evaluation/measure": "In our study, we evaluated the performance of our brain tumor segmentation method using three key metrics: dice score, sensitivity, and specificity. These metrics are widely recognized and used in the literature for assessing the effectiveness of segmentation algorithms.\n\nThe dice score measures the overlap between the predicted lesion region and the ground truth. It is calculated as 2 times the true positives divided by 2 times the true positives plus the false positives and false negatives. This metric provides a measure of the accuracy of the segmentation, with higher values indicating better performance.\n\nSensitivity, also known as recall, measures the proportion of actual positive cases that are correctly identified by the model. It is calculated as the true positives divided by the true positives plus the false negatives. Sensitivity is crucial for evaluating the model's ability to detect tumor pixels accurately.\n\nSpecificity, on the other hand, measures the proportion of actual negative cases that are correctly identified. It is calculated as the true negatives divided by the true negatives plus the false positives. Specificity is important for assessing the model's ability to correctly classify normal regions.\n\nThese metrics were chosen because they provide a comprehensive evaluation of the segmentation performance, covering aspects of accuracy, detection capability, and classification of normal regions. The use of these metrics aligns with standard practices in the field, ensuring that our results are comparable to other studies in the literature.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of our proposed method with other publicly available methods using benchmark datasets. Specifically, we evaluated our approach on the BraTS 2018 validation dataset, which is a well-known benchmark for brain tumor segmentation. The evaluation results, including dice scores, sensitivity, and specificity, were provided by the BraTS 2018 organizers. Our method was compared against several other methods, such as those introduced by McKinley et al. (2018) and Zhou et al. (2018), which utilized different deep learning architectures. McKinley et al. employed a DeepSCAN architecture with densely connected blocks of dilated convolutions within a U-Net-style structure. Zhou et al. designed multiple deep architectures to learn contextual and attentive information, integrating predictions from various models to enhance robustness.\n\nOur method demonstrated significant advantages in sensitivity, achieving scores of 0.8441 for the enhancing tumor, 0.9511 for the whole tumor, and 0.9228 for the tumor core. These results highlight the effectiveness of our approach in accurately identifying tumor regions.\n\nAdditionally, we evaluated our method on the BraTS 2013 dataset, comparing it with other state-of-the-art techniques. For instance, Hussain et al. (2017) developed a brain tumor segmentation method based on cascaded deep CNNs, while Li et al. (2016) proposed a probabilistic model combining sparse representation and Markov random field. Our proposed model outperformed these methods in terms of dice score and sensitivity, further validating its robustness and accuracy.\n\nIn summary, our comparisons with publicly available methods on benchmark datasets demonstrate the superior performance of our brain tumor segmentation approach. The use of densely connected 2D-CNNs and an improved loss function contributed to achieving high accuracy and stability in segmentation results.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files for the BraTS 2018 validation dataset are not directly available to the public. The evaluation scores of our method for this dataset were provided by the BraTS 2018 organizers. These scores can be accessed through the official BraTS 2018 website, where participants upload their segmentation results for online evaluation. The validation dataset consists of 66 cases with unknown grades, and it includes four MRI sequences (FLAIR, T1, T1c, and T2) without ground truth labels. This setup ensures that the evaluation is conducted in a standardized and unbiased manner.\n\nFor the BraTS 2013 training dataset, which includes 20 cases of high-grade glioma (HGG) and 10 cases of low-grade glioma (LGG), the dataset is publicly available. However, the specific evaluation files generated during our experiments are not publicly released. Researchers interested in replicating or building upon our work can access the BraTS 2013 dataset through the official BraTS website, where detailed information about the dataset and its usage is provided. The dataset is released under a license that allows for academic and research purposes, ensuring that it can be used by the scientific community for further advancements in brain tumor segmentation."
}