{
  "publication/title": "Imaging-Based Biomarkers Predict Programmed Death-Ligand 1 and Survival Outcomes in Advanced NSCLC Treated With Nivolumab and Pembrolizumab: A Multi-Institutional Study.",
  "publication/authors": "Yolchuyeva S, Giacomazzi E, Tonneau M, Lamaze F, Orain M, Coulombe F, Malo J, Belkaid W, Routy B, Joubert P, Manem VSK",
  "publication/journal": "JTO clinical and research reports",
  "publication/year": "2023",
  "publication/pmid": "38124790",
  "publication/pmcid": "PMC10730368",
  "publication/doi": "10.1016/j.jtocrr.2023.100602",
  "publication/tags": "- Radiomics\n- NSCLC\n- Survival Analysis\n- CT Imaging\n- Machine Learning\n- Predictive Biomarkers\n- Immunotherapy\n- XGBoost\n- Feature Selection\n- Clinical Endpoints",
  "dataset/provenance": "The dataset used in this study was sourced from two distinct cohorts: the CHUM discovery cohort and the IUCPQ validation cohort. The CHUM cohort consisted of 174 patients for overall survival (OS) and progression-free survival (PFS) analysis, and 146 patients for PD-L1 expression analysis. The IUCPQ cohort included 149 patients for OS and PFS, and 121 patients for PD-L1 expression. These patients were retrospectively selected from those who had been treated with at least one cycle of nivolumab or pembrolizumab at the CHUM and IUCPQ centers.\n\nThe data points included clinical variables such as age, sex, ECOG status, smoking status, subtype group, and first-line treatment. Additionally, radiomics features were extracted from pretreatment CT scans using the PyRadiomics platform, resulting in a total of 851 radiomics features. These features, along with the clinical variables, were used to develop predictive models for OS, PFS, and PD-L1 expression.\n\nTo the best of our knowledge, no previous study has published radiomics biomarkers predicting these three clinical endpoints in patients with NSCLC treated with nivolumab and pembrolizumab using a similarly large cohort. The significant number of patients treated with these compounds and trained/evaluated in two institutions (n = 323) distinguishes our study from prior research. This dataset has not been widely used by the community previously, making our findings novel and potentially impactful for future research and clinical applications.",
  "dataset/splits": "The study utilized data from two distinct cohorts: the CHUM discovery cohort and the IUCPQ validation cohort. For the overall survival (OS) and progression-free survival (PFS) tasks, the CHUM discovery cohort consisted of 174 patients, while the IUCPQ validation cohort included 149 patients. For the PD-L1 task, the CHUM discovery cohort had 146 patients, and the IUCPQ validation cohort had 121 patients.\n\nThe data was split into discovery and validation sets to develop and evaluate predictive models. The discovery set, primarily from the CHUM cohort, was used for feature selection and model training. The validation set, from the IUCPQ cohort, was used to assess the performance of the models.\n\nIn addition to these primary splits, the study employed a fivefold cross-validation with 10 repetitions during the model development phase. This cross-validation approach was used to determine the best hyperparameters and to ensure the robustness of the models. The cross-validation was applied to the entire training cohort, which included the CHUM discovery cohort.\n\nThe distribution of data points in each split was as follows:\n\n* For OS and PFS:\n    * CHUM discovery cohort: 174 patients\n    * IUCPQ validation cohort: 149 patients\n\n* For PD-L1:\n    * CHUM discovery cohort: 146 patients\n    * IUCPQ validation cohort: 121 patients\n\nThe study also mentioned the use of a prevalidation phase within the discovery data set, which involved fivefold cross-validation with 10 repetitions. This phase was crucial for feature selection and hyperparameter tuning. The final models were then tested on the validation data set to evaluate their performance.",
  "dataset/redundancy": "The datasets used in this study were sourced from two distinct academic institutions, ensuring diversity and robustness in the model training and validation processes. The CHUM cohort served as the discovery set, while the IUCPQ cohort was used for validation. This split ensures that the training and test sets are independent, reducing the risk of overfitting and providing a more reliable assessment of the model's generalizability.\n\nTo enforce independence between the training and validation sets, we performed several steps. Initially, empty rows were removed from both cohorts to ensure data integrity. Data standardization was applied to all variables, requiring them to have a mean of zero and a standard deviation of one. This process helps in normalizing the data, making it comparable across different cohorts.\n\nThe CHUM cohort, consisting of 174 patients for overall survival (OS) and progression-free survival (PFS), and 146 patients for programmed death-ligand 1 (PD-L1) expression, was used for feature selection and model training. The IUCPQ cohort, with 149 patients for OS and PFS, and 121 patients for PD-L1, was used for model validation. This division ensures that the models are tested on entirely new data, which is crucial for evaluating their performance in real-world scenarios.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the field. Most prior studies have relied on smaller, single-institution datasets, which can introduce biases and limit the generalizability of the models. For instance, a study by Zerunian et al. used only 21 patients from a single institution, while Liu et al. worked with 46 patients. In contrast, our study includes a larger and more diverse cohort of 323 patients from two institutions, providing a more comprehensive and robust dataset for model development and validation. This approach helps in mitigating the limitations observed in earlier studies, such as sample size constraints and potential biases from using texture features alone.",
  "dataset/availability": "The data presented in this study are not publicly available at this time. However, they may be obtained from the corresponding author, Venkata Manem, upon reasonable request. This approach ensures that the data can be accessed for verification or further research while maintaining control over its distribution. There is no public forum or license associated with the data release, as it is shared directly with interested parties through the corresponding author.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is gradient boosting, specifically the XGBoost algorithm. XGBoost, which stands for \"eXtreme Gradient Boosting,\" is not a new algorithm. It was first introduced by Chen and Guestrin and is an ensemble approach based on the gradient boosting algorithm. The gradient boosting algorithm was improved to enhance the predictive model's performance and reduce computational time by enabling parallel processing. The term \"extreme\" in XGBoost refers to the significant improvements made over the original gradient boosting algorithm.\n\nXGBoost is widely recognized for its efficiency and effectiveness in handling large datasets and delivering high predictive accuracy. It works by sequentially adding new models to correct errors in existing models, thereby improving overall performance. The algorithm is known for its ease of use, low computational costs, and strong predictive capabilities.\n\nThe choice of XGBoost for our study was driven by its proven track record in various machine learning applications, particularly in the field of bioinformatics and medical imaging. Its ability to handle large datasets with numerous features, as well as its robustness to noise and missing values, made it a suitable choice for our radiomics and radiomics-clinical model development.",
  "optimization/meta": "The model developed in this study does not function as a meta-predictor. Instead, it leverages a combination of radiomics features extracted from CT imaging data and clinical variables to build predictive models. The primary machine learning method used is XGBoost, an ensemble approach based on gradient boosting. This method was applied to the entire training cohort after feature selection using Lasso regression and ReliefF.\n\nThe feature selection process involved using Lasso regression for overall survival (OS) and progression-free survival (PFS) tasks, and Lasso Logistic regression for the PD-L1 challenge. Additionally, the ReliefF feature selection method was employed to identify the most relevant features. These selected features were then used to train the XGBoost models.\n\nThe training data for the models consisted of two cohorts: the CHUM discovery cohort and the IUCPQ validation cohort. The CHUM cohort was used for the discovery phase, which included a prevalidation phase with fivefold cross-validation repeated 10 times. The IUCPQ cohort was used for validation to assess the model's performance on independent data.\n\nIn summary, the model does not use data from other machine-learning algorithms as input in a meta-predictor sense. Instead, it integrates radiomics features and clinical variables, utilizing XGBoost as the primary machine learning method. The training data from the CHUM cohort is independent of the validation data from the IUCPQ cohort, ensuring that the model's performance is evaluated on unseen data.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for preparing the dataset for the machine-learning algorithm. Initially, empty rows were removed from both the CHUM and IUCPQ cohorts to ensure data integrity. Data standardization was performed to normalize all variables, ensuring that they had a mean of zero and a standard deviation of one. This step was essential for both cohorts to maintain consistency across the datasets.\n\nFor feature selection, Lasso regression was employed for overall survival (OS) and progression-free survival (PFS) tasks, while Lasso Logistic regression with an L1 penalty was used for the PD-L1 challenge. These methods were applied to the entire training cohort to identify and retain the most relevant features. Grid search with fivefold cross-validation was utilized to determine the optimal alpha for Lasso regression and the best C value for Logistic regression. After training Lasso on the entire CHUM cohort, features with zero coefficients were eliminated, further refining the dataset.\n\nThe ReliefF feature selection method was then used to incrementally choose features from the generated dataset, including those retained after Lasso regression. This process was carried out during the prevalidation phase of the discovery dataset, which involved fivefold cross-validation with 10 repetitions. For each fold, the ReliefF feature selection method was applied to identify the most relevant features.\n\nThe best number of features was determined based on the highest concordance index (C-index) for OS and PFS, and the area under the receiver operating characteristic curve (AUC) for PD-L1. GridSearchCV from SciKit-Learn was employed for hyperparameter tuning of the XGBoost algorithm, ensuring optimal model performance.\n\nFinally, the model with the best features and hyperparameters was tested on the validation dataset to evaluate its performance. This comprehensive preprocessing and encoding pipeline ensured that the data was well-prepared for the machine-learning algorithm, leading to robust and reliable predictive models.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the specific task and the feature selection process. Initially, we extracted 851 radiomics features using the PyRadiomics platform. To reduce dimensionality, we employed the LASSO feature selection strategy followed by the ReliefF feature selection method. This process helped us identify the most relevant features for each task.\n\nFor the overall survival (OS) task, we ended up with 21 features after applying the LASSO feature reduction method and removing features with zero coefficients. For the progression-free survival (PFS) task, 22 features were selected. For the PD-L1 expression task, 35 features were retained.\n\nThe selection of these features was done through a rigorous process involving grid search with fivefold cross-validation to determine the best alpha for LASSO regression and the best C for Logistic regression. Additionally, we used ReliefF feature selection and XGBoost machine learning methods to incrementally choose features from the generated features, including those maintained after the LASSO. This was carried out in the discovery data set\u2019s prevalidation phase, which consisted of fivefold cross-validation with 10-times repetitions.\n\nThe final set of features for each task was determined based on the highest C-index for OS and PFS, and the highest AUC for PD-L1. This ensured that the selected features were the most predictive for the respective clinical endpoints.",
  "optimization/features": "In the optimization process, a total of 851 radiomics features were initially extracted from segmented tumor areas of pretreatment CT scans. These features encompassed various types, including intensity-based, shape-based, texture-based, and wavelet-based characteristics.\n\nFeature selection was indeed performed to identify the most relevant features for model development. This process involved several steps. Initially, Lasso regression and Lasso Logistic regression were applied to the entire training cohort to reduce the number of features. Features with zero coefficients were eliminated, resulting in a reduced set of features for each task: 21 features for overall survival (OS), 22 features for progression-free survival (PFS), and 35 features for the PD-L1 task.\n\nSubsequently, the ReliefF-based feature selection method was employed. This method is known for its efficiency in handling large datasets and its sensitivity to both linear and nonlinear interactions between features. The ReliefF feature selection was conducted within the prevalidation phase of the discovery dataset, which consisted of 10 iterations of fivefold cross-validation. This ensured that the feature selection was performed using the training set only, maintaining the integrity of the validation process.\n\nThe final set of features selected for each task was determined based on the highest concordance index (C-index) for OS and PFS, and the area under the receiver operating characteristic curve (AUC) for PD-L1. This rigorous feature selection process aimed to enhance the predictive performance of the models by focusing on the most informative features.",
  "optimization/fitting": "In our study, we addressed the challenge of having a large number of features relative to the number of training samples through a systematic approach to feature selection and model validation.\n\nWe began by extracting 851 radiomics features from pre-treatment CT scans using the PyRadiomics platform. To mitigate the risk of overfitting, we employed a multi-step feature selection process. Initially, we used Lasso regression for overall survival (OS) and progression-free survival (PFS) tasks, and Lasso Logistic regression for the PD-L1 challenge. This step helped in reducing the dimensionality by eliminating features with zero coefficients. Following this, we applied the ReliefF feature selection method, which is known for its efficiency in handling large datasets and its sensitivity to both linear and nonlinear interactions between features. This method further refined the feature set by estimating the quality of features using a feature-weighting scheme.\n\nTo ensure the robustness of our models, we utilized grid search with fivefold cross-validation to determine the optimal hyperparameters for Lasso regression and Logistic regression. This process was repeated 10 times to enhance the stability and reliability of our feature selection. Additionally, we conducted a cross-validation for XGBoost's hyperparameter tuning using GridSearchCV from SciKit-Learn. This rigorous validation strategy helped in preventing overfitting by ensuring that the model generalizes well to unseen data.\n\nFor the final model, we selected the best features and hyperparameters based on the highest concordance index (C-index) for OS and PFS, and the area under the receiver operating characteristic curve (AUC) for PD-L1. The final models were then tested on a separate validation dataset to assess their performance, further ensuring that the models were not overfitted to the training data.\n\nTo address underfitting, we employed the XGBoost machine learning algorithm, which is known for its ability to build strong predictive models by combining multiple weak models. The ensemble nature of XGBoost helps in capturing complex patterns in the data, thereby reducing the risk of underfitting. Moreover, the use of clinically significant features such as Eastern Cooperative Oncology Group (ECOG) status, sex, and first-line treatment in the radiomics-clinical models ensured that the models were clinically relevant and not overly simplified.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and enhance the generalization of our predictive models. One of the primary methods used was Lasso regression, which incorporates L1 regularization. This technique adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function, effectively shrinking some coefficients to zero and thus performing feature selection. This helped in reducing the complexity of the model and preventing overfitting by eliminating less important features.\n\nAdditionally, we utilized Lasso Logistic regression with an L1 penalty for the PD-L1 challenge. This approach is particularly useful for binary classification tasks, as it helps in selecting the most relevant features while penalizing the magnitude of the coefficients.\n\nTo further ensure the robustness of our models, we employed grid search with fivefold cross-validation to determine the optimal hyperparameters for Lasso regression (alpha) and Logistic regression (C). This process involved systematically working through multiple combinations of parameter tunes to find the best-performing model configuration, thereby reducing the risk of overfitting to the training data.\n\nMoreover, we used the ReliefF feature selection method, which is known for its efficiency in handling large datasets with numerous features. ReliefF estimates the quality of features by considering their ability to distinguish between instances and their neighbors, making it robust to noise and missing values. This method was applied in conjunction with the XGBoost machine learning approach, which is an ensemble technique based on gradient boosting. XGBoost improves the predictive performance by iteratively adding new models to correct the errors of existing ones, thus enhancing the overall model accuracy and reducing overfitting.\n\nIn summary, our approach to regularization involved a combination of Lasso regression, Lasso Logistic regression, grid search with cross-validation, and the ReliefF feature selection method. These techniques collectively helped in building robust and generalizable predictive models for our study.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are not explicitly detailed in the provided information. However, it is mentioned that grid search with fivefold cross-validation was used to determine the best alpha for Lasso regression and C for Logistic regression. Additionally, GridSearchCV from SciKit-Learn was employed for hyperparameter tuning of the XGBoost model. The specific parameters and schedules for these optimizations are not reported.\n\nThe model files and optimization parameters themselves are not made available in the provided information. The data used in this study are not publicly available at this time but may be obtained from the corresponding author on reasonable request. This suggests that while the methods and general approaches are described, the exact configurations and files are not openly accessible.\n\nRegarding licensing, there is no information provided about the licensing terms for any available data or models. Typically, such details would be included in a data availability or supplementary materials section, but this information is not present in the provided context.",
  "model/interpretability": "The models developed in this study are not entirely black-box, as they incorporate interpretable features and methods that contribute to their transparency. The use of radiomics features, which are quantitative measures extracted from medical images, provides a level of interpretability. These features can be categorized into intensity-based, shape-based, texture-based, and wavelet-based groups, each offering insights into different aspects of the tumor characteristics.\n\nIntensity-based features, for instance, quantify tumor intensity characteristics using a histogram of voxel intensity values. Shape-based features describe the tumor's geometry, such as sphericity or compactness. Texture-based features capture the texture changes within the tumor volume using matrices like the gray-level co-occurrence matrix. Wavelet-based features are derived from the intensity and textural data after applying a Laplace of Gaussian transformation to the image.\n\nThe feature selection process, particularly the use of the ReliefF method, further enhances the interpretability of the models. ReliefF identifies the most relevant features by estimating their quality using a feature-weighting scheme. This method is sensitive to both linear and nonlinear interactions between features, allowing for the identification of feature dependencies that might not be captured by other methods.\n\nAdditionally, the integration of clinical features, such as sex, ECOG status, and first-line treatment, into the radiomics-clinical models adds another layer of interpretability. These clinical variables are well-understood and widely used in medical practice, making the models more transparent and clinically relevant.\n\nThe use of XGBoost, an ensemble learning method based on gradient boosting, also contributes to the interpretability of the models. XGBoost builds the model by sequentially adding weak learners (decision trees) to correct the errors of the previous ones. This process can be visualized and understood, providing insights into how the model makes predictions.\n\nIn summary, while the models developed in this study are complex and involve advanced machine learning techniques, they are not entirely black-box. The use of interpretable radiomics features, the ReliefF feature selection method, and the integration of clinical variables all contribute to the transparency of the models. This makes the models more interpretable and clinically relevant, allowing for better understanding and potential implementation in clinical workflows.",
  "model/output": "The models developed in this study are primarily regression models, with the exception of the model predicting PD-L1 expression, which is a classification model. For overall survival (OS) and progression-free survival (PFS), the models predict continuous values, making them regression tasks. The performance of these models is evaluated using the concordance index (C-index). In contrast, the model for predicting PD-L1 expression is a classification task, as it predicts binary outcomes. The performance of this model is evaluated using the area under the receiver operating characteristic curve (AUC).\n\nThe regression models for OS and PFS use Lasso regression and XGBoost, while the classification model for PD-L1 expression uses Lasso Logistic regression and XGBoost. The final models were selected based on their performance metrics, with the best features and hyperparameters determined through cross-validation.\n\nThe output of the regression models provides a continuous prediction of survival times, which can be used to stratify patients into different risk groups. The classification model for PD-L1 expression provides a binary prediction, indicating whether a patient is likely to express PD-L1 or not. These predictions can aid clinicians in making more informed treatment decisions.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models and algorithms used in this study is not publicly available. However, the specific libraries and tools utilized, such as PyRadiomics, XGBoost, and Scikit-learn, are open-source and can be accessed through their respective repositories. The implementation details provided in the publication allow for reproducibility by researchers who wish to replicate the methods described.\n\nFor those interested in accessing the data or specific details of the models, requests can be directed to the corresponding author. This approach ensures that the data is handled responsibly and in accordance with ethical guidelines, particularly given the sensitive nature of patient information.",
  "evaluation/method": "The evaluation of the predictive models involved several rigorous steps to ensure robustness and generalizability. Initially, data standardization was performed to ensure all variables had a mean of zero and a standard deviation of one. This was crucial for maintaining consistency across the datasets.\n\nFeature selection was conducted using Lasso regression for overall survival (OS) and progression-free survival (PFS) tasks, and Lasso Logistic regression for the PD-L1 challenge. Grid search with fivefold cross-validation was employed to determine the optimal hyperparameters for these models. This process helped in identifying the most relevant features by eliminating those with zero coefficients.\n\nFor further refinement, the ReliefF feature selection method and XGBoost machine learning approach were used independently. This was carried out in the discovery dataset's pre-validation phase, which included fivefold cross-validation with 10 repetitions. The ReliefF method was applied at each fold to incrementally select the best features.\n\nThe performance of the models was evaluated using the C-index for OS and PFS, and the area under the curve (AUC) for PD-L1 expression. The highest C-index and AUC values were used to determine the optimal number of features. GridSearchCV from SciKit-Learn was utilized for hyperparameter tuning of the XGBoost models.\n\nThe final models, incorporating the best features and hyperparameters, were tested on a validation dataset to assess their performance. This validation step was essential for evaluating the models' ability to generalize to new, unseen data.\n\nAdditionally, the prognostic value of the radiomics-only signatures was assessed using a log-rank test for Kaplan-Meier survival curves. The data was split into two groups based on the median value, with overall survival (OS) and relapse-free survival as the endpoints.\n\nIn summary, the evaluation method involved a combination of cross-validation, feature selection, and independent dataset testing to ensure the models' reliability and effectiveness in predicting clinical outcomes.",
  "evaluation/measure": "In our study, we primarily reported the C-index and the area under the curve (AUC) as our performance metrics. The C-index, also known as the concordance index, measures the probability that two randomly selected samples will be ranked correctly by the model. It is particularly useful for survival analysis and is a generalization of the AUC. A C-index of 0.5 indicates a random predictor, while a C-index of 1 indicates a perfect predictor.\n\nFor the overall survival (OS) and progression-free survival (PFS) tasks, we reported the C-index values. For the PD-L1 expression task, we reported the AUC values. These metrics were chosen because they are widely accepted and used in the literature for evaluating the performance of predictive models in similar contexts.\n\nThe C-index was used to evaluate the performance of our models for OS and PFS. We reported the C-index values for both the discovery and validation cohorts. For OS, the radiomics model achieved a C-index of approximately 0.599 in the discovery set and 0.554 in the validation cohort. The ensemble radiomics-clinical model improved these values to 0.651 and 0.591, respectively. For PFS, the radiomics model achieved a C-index of 0.608 in the discovery cohort and 0.563 in the validation cohort. The ensemble radiomics-clinical model improved these values to 0.63 and 0.595, respectively.\n\nFor the PD-L1 expression task, we used the AUC as the performance metric. The radiomics model achieved an AUC of 0.62 in the discovery cohort and 0.61 in the validation cohort. The ensemble radiomics-clinical model significantly improved these values to 0.78 and 0.696, respectively.\n\nThese metrics provide a comprehensive evaluation of our models' performance. The C-index is particularly useful for survival analysis, as it takes into account the censoring of data, which is common in survival studies. The AUC is a well-established metric for evaluating the performance of binary classifiers, making it suitable for the PD-L1 expression task. Together, these metrics provide a robust evaluation of our models' predictive performance.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. However, we did employ a rigorous feature selection process using Lasso regression and ReliefF, which are well-established methods in the field of machine learning and radiomics. These methods helped us to identify the most relevant features from the extensive set of radiomics features extracted using the PyRadiomics platform.\n\nWe also utilized XGBoost, a powerful and widely-used machine learning algorithm, to build our predictive models. XGBoost is known for its efficiency and performance in handling large datasets and has been successfully applied in various biomedical research studies.\n\nWhile we did not compare our approach to simpler baselines explicitly, the use of Lasso regression for feature selection can be seen as a form of baseline comparison. Lasso regression helps in identifying the most important features by penalizing the absolute size of the regression coefficients, effectively performing both variable selection and regularization. This approach ensures that our models are not overfitting and are generalizable to new data.\n\nAdditionally, we evaluated the performance of our models using cross-validation techniques, which provide a robust estimate of model performance and help in comparing different modeling approaches. The use of five-fold cross-validation with 10 repetitions ensured that our models were thoroughly validated and that the results were not dependent on a particular split of the data.\n\nIn summary, although we did not perform a direct comparison to publicly available methods or simpler baselines on benchmark datasets, our study employed well-established feature selection and machine learning techniques. These methods, combined with rigorous validation procedures, ensured the robustness and generalizability of our predictive models.",
  "evaluation/confidence": "The performance metrics presented in this study include confidence intervals and statistical significance tests to ensure the robustness of the findings. For instance, the C-index values for the radiomics and radiomics-clinical models were evaluated using fivefold cross-validation with 10 repetitions, providing a comprehensive assessment of model performance. The improvement in C-index from the radiomics model to the radiomics-clinical model was statistically significant for both overall survival (OS) and progression-free survival (PFS). Specifically, the C-index for OS improved from 0.599 to 0.651 in the discovery cohort and from 0.554 to 0.591 in the validation cohort, with similar significant improvements observed for PFS.\n\nAdditionally, the prognostic value of the radiomics signatures was assessed using a log-rank test for Kaplan-Meier survival curves, yielding significant p-values of 0.00022 for OS and 0.013 for PFS. These results indicate that the developed models can effectively stratify patients into low- and high-survival groups, demonstrating their clinical relevance.\n\nThe AUC for predicting PD-L1 expression also showed significant improvement with the ensemble radiomics-clinical model, increasing from 0.62 to 0.78 in the discovery cohort and from 0.61 to 0.696 in the validation cohort. These enhancements underscore the superior predictive ability of the radiomics-clinical model compared to the radiomics model alone.\n\nOverall, the statistical significance and confidence intervals associated with the performance metrics provide strong evidence supporting the superiority of the radiomics-clinical model over the radiomics model alone. This confidence in the results is further bolstered by the use of rigorous validation techniques and the integration of clinically relevant features, ensuring the models' reliability and potential for clinical translation.",
  "evaluation/availability": "The data presented in this study are not publicly available at this time. However, they may be obtained from the corresponding author, Venkata Manem, upon reasonable request. This approach ensures that the data can be accessed for verification or further research while maintaining control over its distribution."
}