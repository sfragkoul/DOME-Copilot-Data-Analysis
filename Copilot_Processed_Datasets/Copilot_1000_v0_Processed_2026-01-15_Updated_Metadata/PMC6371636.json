{
  "publication/title": "A Novel LSSVM Based Algorithm to Increase Accuracy of Bacterial Growth Modeling.",
  "publication/authors": "Borujeni MS, Ghaderi-Zefrehei M, Ghanegolmohammadi F, Ansari-Mahyari S",
  "publication/journal": "Iranian journal of biotechnology",
  "publication/year": "2018",
  "publication/pmid": "30805384",
  "publication/pmcid": "PMC6371636",
  "publication/doi": "10.21859/ijb.1542",
  "publication/tags": "- Predictive microbiology\n- Bacterial growth modeling\n- Least square support vector machine (LSSVM)\n- Genetic algorithm (GA)\n- Non-dominated Sorting Genetic Algorithm-II (NSGA-II)\n- Machine learning in microbiology\n- Time series analysis\n- Growth curve prediction\n- Listeria monocytogenes\n- Escherichia coli\n- Computational biology\n- Sigmoid functions\n- Logistic function\n- Gompertz function\n- Cross-validation\n- Mean absolute error (MAE)\n- Mean absolute percentage error (MAPE)",
  "dataset/provenance": "The dataset used in this study originates from two bacterial strains: Listeria monocytogenes and Escherichia coli. The growth data for L. monocytogenes was obtained from a previous study by Augustin et al., which provided datasets with different initial population sizes over a period of 552 hours. This data is detailed in Supplementary Table 1a and 1b. For E. coli, the growth data was collected by profiling the optical density (OD) using spectrophotometry at 610 nm. The E. coli dataset was collected over an experimental period of 13 hours, with measurements taken every 30 minutes until the death phase. This data is presented in Supplementary Table 1c. The specific conditions for the E. coli dataset involved batch culturing in a liquid LB (Luria-Bertani) medium at 37\u00b0C. The number of data points varies depending on the dataset, with L. monocytogenes data spanning 552 hours and E. coli data spanning 13 hours with frequent measurements. These datasets have been used to model bacterial growth curves using various algorithms, including LSSVM-based methods and sigmoid functions.",
  "dataset/splits": "The dataset was divided into two primary splits: the modeling dataset and the test dataset. The modeling dataset constituted 80% of the total data, while the test dataset made up the remaining 20%.\n\nThe modeling dataset was further subdivided into multiple groups for cross-validation purposes. Specifically, it was divided into five groups. In each iteration of the cross-validation process, one of these groups was designated as the validation dataset, and the remaining four groups were used as the training dataset. This approach ensured that the model was trained and validated on different subsets of the data, enhancing its robustness and generalizability.\n\nThe test dataset was used to evaluate the final performance of the trained model, providing an unbiased assessment of its accuracy and reliability. This multi-step splitting process was crucial for optimizing the model's parameters and ensuring that it could accurately predict bacterial growth curves.",
  "dataset/redundancy": "The datasets were divided into two main parts: a modeling dataset and a test dataset. The modeling dataset constituted 80% of the total data, while the test dataset made up the remaining 20%. This split was done to ensure that the model's performance could be evaluated on unseen data, thereby providing a more reliable assessment of its generalization capabilities.\n\nThe modeling dataset was further divided into training and validation subsets. This was done to fine-tune the model parameters and prevent overfitting. In each iteration of the training process, one of the five groups within the modeling dataset was used as the validation set, while the remaining groups served as the training set. This cross-validation approach helped in optimizing the model's parameters, specifically the 'c' and '\u03c3' coefficients, which are crucial for the Least Square Support Vector Machine (LSSVM) model.\n\nThe test dataset remained independent of the modeling dataset throughout the process. This independence was enforced by ensuring that the test dataset was only used after the model had been fully trained and validated. This approach guarantees that the test results are a true reflection of the model's performance on new, unseen data.\n\nRegarding the distribution of the datasets, it is not explicitly compared to previously published machine learning datasets. However, the use of a 80-20 split for modeling and testing is a common practice in machine learning, ensuring a robust evaluation of the model's performance. The additional step of dividing the modeling dataset into training and validation subsets through cross-validation is also a standard technique to enhance the model's reliability and generalization.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study is a hybrid approach that combines the Least Square Support Vector Machine (LSSVM) with the Non-dominated Sorting Genetic Algorithm-II (NSGA-II). LSSVM is a type of Support Vector Machine (SVM) that offers computational advantages by solving linear systems instead of quadratic programming. This makes it particularly suitable for function estimation tasks, such as predicting bacterial growth curves.\n\nThe NSGA-II is a multi-objective optimization algorithm that is used to optimize the parameters 'c' and '\u03c3' of the LSSVM. These parameters are crucial for the performance of the LSSVM model, as they determine the trade-off between training error minimization and model smoothness. By using NSGA-II, we aim to achieve a more accurate and smooth modeling of bacterial growth curves.\n\nThe hybrid NSGA-II-LSSVM algorithm is not entirely new, as both LSSVM and NSGA-II have been used individually in various studies. However, the specific combination and application of these algorithms to bacterial growth modeling is novel. This hybrid approach was developed to address the limitations of traditional methods and to improve the predictive accuracy of bacterial growth curves.\n\nThe reason this hybrid algorithm was not published in a machine-learning journal is that the primary focus of our study is on bacterial growth modeling rather than the development of new machine-learning algorithms. The application of NSGA-II-LSSVM to bacterial growth modeling is a significant contribution to the field of biotechnology, as it provides a more accurate and reliable method for predicting bacterial growth under various conditions. This study demonstrates the potential of hybrid algorithms in improving the predictive performance of machine-learning models in biological research.",
  "optimization/meta": "The model described in this publication employs a hybrid approach that leverages multiple machine-learning algorithms to enhance the accuracy of bacterial growth curve prediction. Specifically, the NSGA-II-LSSVM method integrates the Non-dominated Sorting Genetic Algorithm II (NSGA-II) with the Least Square Support Vector Machine (LSSVM).\n\nThe NSGA-II-LSSVM model uses data from other machine-learning algorithms as input. The NSGA-II algorithm is utilized to optimize the coefficients 'c' and '\u03c3' for the LSSVM, which are crucial for the training process. This optimization involves dividing the dataset into modeling and test datasets, further splitting the modeling dataset into training and validation subsets. The LSSVM is then trained using the training dataset and tested with the validation data. The training error is considered as the first fitness function, and the validation error as the second fitness function. The minimization of these two fitness functions results in the calculation of the optimized coefficients, which are then used to train the LSSVM model.\n\nThe hybrid model combines the strengths of both NSGA-II and LSSVM. NSGA-II is a multi-objective optimization algorithm that handles problems with multiple fitness functions, ensuring a more accurate and smooth modeling of the bacterial growth curve. LSSVM, on the other hand, provides computational advantages and is effective in handling nonlinear time series data, which is essential for modeling bacterial growth.\n\nIt is clear that the training data is independent. The dataset is divided into modeling and test datasets, with the modeling dataset further split into training and validation subsets. This ensures that the data used for training the LSSVM is separate from the data used for validation and testing, maintaining the independence of the training data. This independence is crucial for evaluating the model's performance accurately and avoiding overfitting.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for the effective application of the machine-learning algorithms, particularly the Least Square Support Vector Machine (LSSVM). The bacterial growth data was measured as log colony-forming units per milliliter (log cfu.mL\u207b\u00b9) for Listeria monocytogenes and optical density (OD) at 610 nm for Escherichia coli. This logarithmic transformation was essential for normalizing the data and making it suitable for modeling.\n\nThe datasets were divided into modeling and test sets, with 80% of the data used for modeling and 20% reserved for testing. The modeling dataset was further split into training and validation subsets using a cross-validation method. This approach ensured that the model's performance was evaluated on unseen data, providing a more robust assessment of its generalization capabilities.\n\nFor the LSSVM-based algorithms, the data was encoded in a manner that allowed the machine-learning model to learn the underlying patterns in the bacterial growth curves. The input features included time points and corresponding growth measurements, while the output was the predicted growth at those time points. The LSSVM model was trained using these encoded features to minimize the error between the predicted and actual growth values.\n\nThe preprocessing steps also involved the calculation of 'c' and '\u03c3' coefficients, which are critical parameters in the LSSVM model. These coefficients were optimized using genetic algorithms (GA) and simplex algorithms, as well as a novel hybrid method called NSGA-II-LSSVM. The optimization process aimed to find the best combination of 'c' and '\u03c3' that minimized the fitness functions, thereby improving the model's accuracy and smoothness.\n\nIn summary, the data encoding and preprocessing involved logarithmic transformation of growth data, division of datasets into modeling and test sets, and optimization of LSSVM parameters using advanced algorithms. These steps were essential for ensuring the reliability and accuracy of the predictive models developed in our study.",
  "optimization/parameters": "In our study, two key parameters, 'c' and '\u03c3', were used in the model. These parameters are crucial for the Least Squares Support Vector Machine (LSSVM) training procedure.\n\nThe parameter 'c' serves as the regularization parameter, which determines the trade-off between achieving a low training error and maintaining the model's smoothness. On the other hand, '\u03c3' is related to the width of the radial basis function (RBF) kernel, influencing the model's ability to fit the data.\n\nTo select these parameters, we employed a multi-objective optimization approach using the Non-dominated Sorting Genetic Algorithm II (NSGA-II). This method allowed us to simultaneously minimize two fitness functions: the training error and the validation error. By optimizing these parameters, we aimed to enhance the model's accuracy and smoothness in predicting bacterial growth curves. The NSGA-II algorithm was chosen for its effectiveness in handling optimization problems with multiple objectives, ensuring that the selected parameters provide a balanced and accurate model.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "In our study, we employed Least Squares Support Vector Machines (LSSVM) for modeling bacterial growth curves. LSSVM is particularly advantageous because it solves linear systems instead of quadratic programming, making it computationally efficient.\n\nTo address the potential issue of overfitting, which can occur when the number of parameters is much larger than the number of training points, we utilized a robust cross-validation method. Specifically, we implemented N-fold cross-validation, where the dataset was divided into modeling (80%) and test (20%) datasets. The modeling dataset was further split into training and validation subsets. This approach ensures that the model's performance is evaluated on unseen data, helping to mitigate overfitting.\n\nAdditionally, we optimized the regularization parameter 'c' and the kernel parameter '\u03c3' using genetic algorithms (GA) and simplex algorithms. These optimization techniques help in finding the best parameters that balance the trade-off between training error minimization and model smoothness, further reducing the risk of overfitting.\n\nTo prevent underfitting, we ensured that our model had sufficient complexity to capture the underlying patterns in the data. The use of the radial basis function (RBF) kernel in LSSVM allows for nonlinear mapping of samples into a higher-dimensional space, enabling the model to handle complex relationships between inputs and outputs. The RBF kernel is particularly effective in regression problems due to its ability to capture nonlinearities in the data.\n\nMoreover, we calculated the modeling error and test error to assess the model's performance. The modeling error was determined using the training dataset, while the test error was evaluated using the test dataset. This dual evaluation ensures that the model generalizes well to new, unseen data, thereby avoiding underfitting.\n\nIn summary, our approach combines N-fold cross-validation, parameter optimization, and the use of the RBF kernel to effectively manage both overfitting and underfitting, resulting in a robust and accurate LSSVM model for predicting bacterial growth curves.",
  "optimization/regularization": "In our study, we employed regularization techniques to prevent overfitting in our model. Specifically, we used the Least Squares Support Vector Machine (LSSVM) approach, which inherently includes a regularization parameter 'c'. This parameter plays a crucial role in balancing the trade-off between achieving a low training error and maintaining a smooth decision boundary, thereby helping to prevent overfitting.\n\nThe regularization parameter 'c' is optimized using either Genetic Algorithms (GA) or simplex algorithms. These optimization methods ensure that the model generalizes well to unseen data by tuning 'c' to an optimal value. Additionally, we utilized N-fold cross-validation to further validate the model's performance and to fine-tune the 'c' and '\u03c3' coefficients, which are essential for the model's accuracy and smoothness.\n\nMoreover, our novel hybrid method, NSGA-II-LSSVM, incorporates a multi-objective optimization approach. This method minimizes two fitness functions simultaneously: the training error and the validation error. By doing so, it helps in finding a balance that reduces the risk of overfitting while improving the model's learning accuracy.\n\nIn summary, our regularization methods, including the use of the 'c' parameter, optimization algorithms, cross-validation, and the NSGA-II-LSSVM approach, collectively work to prevent overfitting and enhance the model's predictive performance.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the publication. Specifically, the coefficients 'c' and '\u03c3' were optimized using both Genetic Algorithm (GA) and Non-dominated Sorting Genetic Algorithm-II (NSGA-II). The process involved dividing the dataset into modeling (80%) and test (20%) datasets. The modeling dataset was further split into training and validation sets to estimate and optimize these coefficients.\n\nThe optimization schedule is outlined in the methodology section, where the steps for training and validating the Least Squares Support Vector Machine (LSSVM) model are described. This includes the use of cross-validation to ensure the robustness of the model. The fitness functions used for optimization are also provided, with equations detailing the training and validation errors.\n\nModel files and specific optimization parameters are not explicitly provided in the text, but the methods and algorithms used are thoroughly described. The results of the optimization, including the mean absolute error (MAE) and mean absolute percentage error (MAPE), are presented in tables and figures, demonstrating the effectiveness of the NSGA-II-LSSVM hybrid model compared to other methods.\n\nFor access to supplementary files, including additional data and detailed results, readers can visit the provided link. The publication is open access, allowing for free access to the full text and supplementary materials. This ensures that the methods and results can be replicated and verified by other researchers.",
  "model/interpretability": "The model employed in this study, NSGA-II-LSSVM, is a hybrid approach that combines the Least Squares Support Vector Machine (LSSVM) with the Non-dominated Sorting Genetic Algorithm II (NSGA-II). This combination leverages the strengths of both methods to enhance the accuracy and robustness of bacterial growth curve prediction.\n\nThe LSSVM component of the model is inherently a black-box model, meaning that the relationships between inputs and outputs are not explicitly interpretable. This is because LSSVM operates by finding a hyperplane that best separates the data in a high-dimensional feature space, which is not straightforward to interpret.\n\nHowever, the integration of NSGA-II introduces a level of transparency. NSGA-II is an optimization algorithm that works by evolving a population of solutions over generations, using genetic operators like selection, crossover, and mutation. This process can be observed and analyzed, providing insights into how the model parameters ('c' and '\u03c3') are optimized. The fitness functions used in NSGA-II, which are based on training and validation errors, offer a clear metric for evaluating the model's performance at each stage of the optimization process.\n\nMoreover, the use of mean absolute error (MAE) and mean absolute percentage error (MAPE) as evaluation metrics adds another layer of interpretability. These metrics provide a quantitative measure of the model's accuracy, allowing for a clear comparison between different models and datasets. The coefficients of the Logistic and Gompertz functions, as well as the 'c' and '\u03c3' coefficients of the LSSVM-based models, are explicitly presented, further enhancing the transparency of the model.\n\nIn summary, while the LSSVM component of the model is a black-box, the integration of NSGA-II and the use of clear evaluation metrics introduce a level of transparency. This allows for a better understanding of the model's optimization process and its performance.",
  "model/output": "The model employed in this study is a regression model. Specifically, it utilizes the Least Squares Support Vector Machine (LSSVM) approach, which is designed for regression tasks. The LSSVM model is trained to predict bacterial growth curves by fitting appropriate coefficients, 'ai' and 'b', which are calculated using linear equations derived from the optimization process. The model's performance is evaluated using metrics such as Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE), which are common in regression analysis. The use of a radial basis function (RBF) kernel further enhances the model's ability to handle nonlinear relationships between input and output variables, making it well-suited for regression tasks.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the implementation of the LSSVM using the optimized 'c' and '\u03c3' coefficients obtained from NSGA-II is available. The MATLAB codes for the E. coli growth dataset are provided as supplementary material. This allows other researchers to replicate and build upon the work presented in the study. The supplementary material includes detailed information and code snippets that can be used to run the algorithm. However, specific details about the licensing terms under which the code is released are not provided.",
  "evaluation/method": "The evaluation of the proposed method involved a comprehensive comparison of different algorithms using specific error metrics. The datasets were initially divided into modeling (80%) and test (20%) sets. The modeling dataset was further split into training and validation groups through a cross-validation method. The Least Squares Support Vector Machine (LSSVM) was trained using various 'c' and '\u03c3' coefficients, which were optimized via different algorithms, including Genetic Algorithm (GA) and Simplex. The performance of each trained LSSVM model was evaluated using the validation data, and a fitness function was calculated to assess the accuracy.\n\nThe evaluation metrics used were the mean absolute error (MAE) and the mean absolute percentage error (MAPE). These metrics provided a quantitative measure of the prediction accuracy of the models. The MAE was calculated as the average of the absolute differences between the predicted and actual logarithmic bacterial growth values. The MAPE was computed as the average of the absolute percentage differences between the predicted and actual values, expressed as a percentage.\n\nThe results indicated that the LSSVM-based models, particularly those optimized using the NSGA-II algorithm, outperformed traditional sigmoid functions such as Logistic and Gompertz. The NSGA-II-LSSVM method showed superior accuracy in predicting bacterial growth curves, as evidenced by lower MAE and MAPE values. This hybrid approach leveraged the strengths of both NSGA-II for parameter optimization and LSSVM for predictive modeling, resulting in more precise and reliable growth predictions. The comparison was conducted across multiple datasets, including those for Listeria monocytogenes and Escherichia coli, demonstrating the robustness and general applicability of the NSGA-II-LSSVM method.",
  "evaluation/measure": "In our study, we employed two primary performance metrics to evaluate the accuracy of the algorithms used for modeling bacterial growth curves: Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE). These metrics are widely recognized and used in the literature for assessing the performance of predictive models.\n\nMAE provides a straightforward measure of the average magnitude of errors in a set of predictions, without considering their direction. It is calculated as the average of the absolute differences between the predicted and actual values. This metric is useful for understanding the average error size, but it does not account for the relative size of the errors.\n\nMAPE, on the other hand, expresses the accuracy as a percentage, making it easier to interpret the error relative to the actual values. It is calculated as the average of the absolute percentage differences between the predicted and actual values. MAPE is particularly useful when comparing the performance of different models across various datasets, as it normalizes the errors by the actual values.\n\nBoth MAE and MAPE were used to compare the accuracy of different algorithms, including sigmoid functions (Logistic and Gompertz) and various LSSVM-based models (Simplex-LSSVM, GA-LSSVM, and NSGA-II-LSSVM). The results indicated that the NSGA-II-LSSVM model outperformed the other models in terms of both MAE and MAPE, demonstrating its superiority in predicting bacterial growth curves.\n\nThe use of these metrics is representative of the literature, as they are commonly employed in studies evaluating predictive models. The combination of MAE and MAPE provides a comprehensive assessment of model performance, considering both the absolute and relative errors. This approach ensures that our evaluation is robust and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of various methods to evaluate the performance of our proposed NSGA-II-LSSVM algorithm for modeling bacterial growth curves. We compared our approach with several publicly available methods, including sigmoid functions such as Logistic and Gompertz, as well as other LSSVM-based algorithms like simplex-LSSVM and GA-LSSVM.\n\nTo ensure a fair and comprehensive evaluation, we used benchmark datasets that included growth data for two important bacterial strains: Listeria monocytogenes and Escherichia coli. These datasets were obtained from previous studies and covered different initial population sizes and experimental conditions.\n\nWe employed two primary metrics for comparison: mean absolute error (MAE) and mean absolute percentage error (MAPE). These metrics provided a quantitative measure of the accuracy of each method in predicting bacterial growth.\n\nOur results demonstrated that the NSGA-II-LSSVM algorithm outperformed the other methods in terms of both MAE and MAPE. Specifically, the NSGA-II-LSSVM showed lower error values compared to the Logistic and Gompertz functions, indicating higher accuracy in modeling bacterial growth. Additionally, when compared to simpler baselines like simplex-LSSVM and GA-LSSVM, the NSGA-II-LSSVM consistently achieved better performance, highlighting its superiority in handling the complexity of bacterial growth data.\n\nThe comparison was further supported by visual representations, such as figures showing the proximity between predicted and observed values, which visually confirmed the accuracy of the NSGA-II-LSSVM algorithm. Overall, our evaluation underscored the effectiveness of the NSGA-II-LSSVM in providing more accurate and reliable predictions of bacterial growth curves.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}