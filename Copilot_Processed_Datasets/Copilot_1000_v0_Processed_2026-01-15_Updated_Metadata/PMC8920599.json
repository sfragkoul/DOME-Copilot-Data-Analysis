{
  "publication/title": "Automatic detection of tumor vessels in indeterminate biliary strictures in digital single-operator cholangioscopy.",
  "publication/authors": "Pereira P, Mascarenhas M, Ribeiro T, Afonso J, Ferreira JPS, Vilas-Boas F, Parente MPL, Jorge RN, Macedo G",
  "publication/journal": "Endoscopy international open",
  "publication/year": "2022",
  "publication/pmid": "35295246",
  "publication/pmcid": "PMC8920599",
  "publication/doi": "10.1055/a-1723-3369",
  "publication/tags": "- Digital Single-Operator Cholangioscopy\n- Convolutional Neural Network\n- Tumor Vessels\n- Biliary Strictures\n- Malignant Lesions\n- Diagnostic Accuracy\n- Endoscopic Imaging\n- Machine Learning in Medicine\n- Image Processing\n- Diagnostic Tools",
  "dataset/provenance": "The dataset used in this study was sourced from digital single-operator cholangioscopy (D-SOC) procedures performed between August 2017 and January 2021. A total of 6475 frames were extracted for the construction of the convolutional neural network (CNN). Of these, 4415 frames showed tumor vessels (TVs) associated with malignancy, while 2060 frames depicted benign findings. The dataset was divided into a training set, which comprised 80% of the images (5180 frames), and a validation set, which included the remaining 20% (1295 frames). The validation dataset consisted of 829 images with TVs and 466 images showing benign findings.\n\nThe images were obtained using the Spyglass DS and DSII systems, and the procedures were conducted by two experienced endoscopists who had performed over 2000 endoscopic retrograde cholangiopancreatographies (ERCPs) and 100 cholangioscopies each. The classification of the images as either benign or showing TVs was based on histological evidence of malignancy and required consensus between the two endoscopists. Images that did not achieve consensus were excluded from the dataset.\n\nThe dataset has not been used in previous publications by the community, as this study represents the first application of a deep learning CNN for the automatic identification of TVs in D-SOC images. The images were collected and classified specifically for this research to develop and validate the CNN model.",
  "dataset/splits": "The dataset was divided into two splits: a training dataset and a validation dataset. The training dataset comprised 80% of the total images, amounting to 5180 images. The remaining 20% of the images, totaling 1295, were used as the validation dataset. Within the validation dataset, 829 images showed tumor vessels, while 466 images depicted benign findings. This division was crucial for training and evaluating the performance of the convolutional neural network.",
  "dataset/redundancy": "The dataset used in this study consisted of 6475 images, which were split into training and validation sets. The training dataset comprised 80% of the total images, amounting to 5180 images. The remaining 20%, or 1295 images, were allocated to the validation dataset. This split ensures that the training and test sets are independent, which is crucial for evaluating the model's performance accurately.\n\nTo enforce the independence of the datasets, the images were divided in a way that the validation set was not used during the training phase. This approach helps in assessing the model's generalization capability on unseen data.\n\nThe distribution of the images in the datasets reflects a common practice in machine learning, where a significant portion of the data is used for training to allow the model to learn patterns, and a smaller portion is reserved for validation to test the model's performance. This split is designed to mimic real-world scenarios where the model will encounter new, unseen data.\n\nThe images were classified into two categories: those showing tumor vessels (TVs) associated with malignancy and those showing benign findings. In the training set, 4415 images showed TVs, while 2060 images depicted benign findings. The validation set followed a similar distribution, with 829 images of TVs and 466 images of benign findings. This distribution ensures that the model is trained and validated on a representative sample of the data, enhancing its robustness and reliability.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is a convolutional neural network (CNN). This type of deep learning model is particularly well-suited for image analysis tasks, making it ideal for our goal of automatically detecting tumor vessels (TVs) in digital single-operator cholangioscopy (D-SOC) images.\n\nThe specific CNN model employed is the Xception architecture, which is pre-trained on the ImageNet dataset. We leveraged transfer learning by retaining the convolutional layers of the Xception model and fine-tuning it on our dataset of D-SOC images. This approach allows the model to benefit from the extensive feature learning performed on ImageNet while adapting to the specific characteristics of our medical imaging data.\n\nThe CNN was developed using TensorFlow 2.3 and Keras libraries, which are widely used in the machine learning community for building and training neural networks. The model was trained and validated on a dataset comprising 6475 images, with 4415 images showing TVs and 2060 images showing benign findings. The training dataset consisted of 80% of the images, while the remaining 20% were used for validation.\n\nThe choice to use a CNN and the Xception model was driven by their proven effectiveness in image classification tasks. While the algorithm itself is not entirely new, its application to the specific problem of detecting TVs in D-SOC images is novel. This specialization is why the work was published in an endoscopic journal rather than a machine-learning journal. The focus is on the medical application and the potential impact on diagnostic accuracy in endoscopy, rather than the development of a new machine-learning algorithm.",
  "optimization/meta": "Not applicable. The model described is a convolutional neural network (CNN) specifically designed for the automatic identification of tumor vessels (TVs) in digital single-operator cholangioscopy (D-SOC) images. It does not incorporate data from other machine-learning algorithms as input. The CNN was developed using the Xception model, which was pre-trained on ImageNet. The training and validation datasets were carefully divided to ensure independence, with 80% of the images used for training and the remaining 20% for validation. The performance of the CNN was evaluated based on its ability to detect TVs associated with malignancy, achieving high sensitivity, specificity, and accuracy. The model's output provides a probability for each finding, with the highest probability determining the final classification.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the images were suitable for training and validation of the convolutional neural network (CNN). Initially, a total of 6475 frames were extracted from the digital single-operator cholangioscopy (D-SOC) images. These frames were classified into two categories: those showing tumor vessels (TVs) associated with malignancy and those showing benign findings. The dataset was then split into training and validation sets, with 80% of the images (5180 frames) used for training and the remaining 20% (1295 frames) reserved for validation.\n\nThe CNN was developed using the Xception model, which had its weights pre-trained on the ImageNet dataset. This transfer learning approach allowed the model to leverage existing knowledge from a large and diverse image dataset. The convolutional layers of the Xception model were retained to capture essential features from the input images. TensorFlow 2.3 and Keras libraries were utilized to prepare the data and run the model. The preprocessing steps included resizing the images to a consistent dimension and normalizing the pixel values to ensure uniformity across the dataset.\n\nThe images were labeled based on the consensus of two experienced endoscopists, who independently identified TVs and benign findings. The final classification required agreement between both researchers, ensuring high-quality labeled data for training the CNN. This meticulous labeling process was crucial for the model's ability to accurately distinguish between malignant and benign conditions in the cholangioscopy images. The preprocessing and encoding steps were performed using a computer equipped with a 2.1 GHz Intel Xeon Gold 6130 processor and a double NVIDIA Quadro RTX 4000 graphic processing unit, which provided the necessary computational power for efficient model training and validation.",
  "optimization/parameters": "The convolutional neural network (CNN) developed for this study utilized the Xception model, which is a deep learning architecture known for its efficiency and performance in image classification tasks. The Xception model has a total of 22,910,480 parameters. These parameters were initially trained on the ImageNet dataset, a large-scale visual database designed for use in visual object recognition software research.\n\nThe selection of the Xception model and its parameters was based on its proven effectiveness in transferring learning to new datasets. By keeping the convolutional layers of the model and fine-tuning them on our specific dataset of digital single-operator cholangioscopy (D-SOC) images, we aimed to leverage the pre-trained weights to improve the model's performance on our task of identifying tumor vessels (TVs) associated with malignancy.\n\nThe choice of the Xception model was also influenced by its architectural design, which includes depthwise separable convolutions. This design allows the model to achieve high accuracy with fewer parameters compared to traditional convolutional neural networks, making it suitable for our computational resources and dataset size. The model was implemented using TensorFlow 2.3 and Keras libraries, and the analyses were performed on a high-performance computing setup equipped with a 2.1 GHz Intel Xeon Gold 6130 processor and dual NVIDIA Quadro RTX 4000 graphic processing units.",
  "optimization/features": "The convolutional neural network (CNN) utilized in this study was designed to automatically identify tumor vessels (TVs) in digital single-operator cholangioscopy (D-SOC) images. The input features for the CNN consisted of individual image frames extracted from the cholangioscopy videos. A total of 6475 frames were collected, with 4415 frames showing TVs and 2060 frames showing benign findings. These frames were used to train and validate the CNN.\n\nFeature selection, in the traditional sense, was not explicitly performed. Instead, the CNN was trained using the entire set of image frames, leveraging the deep learning architecture to automatically learn and extract relevant features from the images. The CNN model used was based on the Xception architecture, which is pre-trained on the ImageNet dataset. The convolutional layers of this pre-trained model were retained to transfer learning to our specific dataset of cholangioscopy images.\n\nThe dataset was divided into training and validation sets. The training dataset comprised 80% of the extracted images (5180 images), while the remaining 20% (1295 images) were used as the validation dataset. This division ensured that the model's performance was evaluated on a separate set of data that it had not seen during training, providing a more robust assessment of its generalization capabilities.\n\nThe classification provided by the CNN was compared to the gold standard classification provided by experienced endoscopists, which integrated data from visual impressions, histopathology, and clinical follow-up. This approach ensured that the CNN's performance was rigorously evaluated against a reliable benchmark.",
  "optimization/fitting": "The convolutional neural network (CNN) developed for this study utilized the Xception model, which has a large number of parameters due to its deep architecture. The number of parameters in the model is indeed much larger than the number of training points, which is a common scenario in deep learning to capture complex patterns in data.\n\nTo address the risk of overfitting, several strategies were employed. First, the model used transfer learning by initializing the convolutional layers with weights pre-trained on the ImageNet dataset. This approach leverages knowledge from a large and diverse dataset, helping the model to generalize better to the specific task of identifying tumor vessels (TVs) in digital single-operator cholangioscopy (D-SOC) images. Second, the dataset was divided into training and validation sets, with 80% of the images used for training and 20% reserved for validation. This separation allows for the evaluation of the model's performance on unseen data, providing a more reliable estimate of its generalization capability. Additionally, the model's performance was assessed using multiple metrics, including sensitivity, specificity, positive and negative predictive values, accuracy, and the area under the receiver operating characteristic curve (AUROC). The high performance across these metrics suggests that the model generalizes well to the validation dataset.\n\nTo mitigate underfitting, the model was trained with a sufficient number of epochs, allowing it to learn the complex patterns in the data. The accuracy of the CNN increased as data were repeatedly inputted into its multi-layer architecture, indicating that the model was able to capture the necessary features for the task. Furthermore, the use of a deep architecture with many parameters enables the model to learn intricate representations of the input images, reducing the risk of underfitting.\n\nIn summary, the combination of transfer learning, a large and diverse training dataset, and thorough validation procedures helped to ensure that the model neither overfits nor underfits the data. The high performance metrics on the validation dataset further support the effectiveness of the chosen fitting method.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are not explicitly detailed in the provided information. However, the model files and optimization parameters are implicitly described through the mention of the Xception model with weights trained on ImageNet. The convolutional layers of this pre-trained model were retained for transfer learning, and the implementation was done using TensorFlow 2.3 and Keras libraries.\n\nThe computational environment is specified, including the use of a 2.1 GHz Intel Xeon Gold 6130 processor and double NVIDIA QuadroRTX 4000 graphic processing units. This setup provides a clear picture of the hardware used for training and validating the convolutional neural network (CNN).\n\nRegarding the availability and licensing of the model files and optimization parameters, specific details are not provided. However, the use of open-source libraries like TensorFlow and Keras suggests that the implementation can be replicated using these widely available tools. The Xception model, being a well-known architecture, is also accessible through standard deep learning frameworks.\n\nFor those interested in replicating the study, the general approach and tools used are well-documented in the field of deep learning. The specific hyper-parameters and optimization schedules would typically be part of the supplementary materials or code repositories associated with the publication, but these details are not included in the provided information.",
  "model/interpretability": "The model developed in this study is a convolutional neural network (CNN) based on the Xception architecture, which is inherently a black-box model. This means that the internal workings of the model are not easily interpretable by humans. The CNN processes images through multiple layers of convolutions and pooling operations, extracting features that are used to make predictions. However, the specific features that the model focuses on and the reasoning behind its predictions are not directly accessible or understandable.\n\nThe model's predictions are based on probabilities assigned to each image, indicating the likelihood of the presence of tumor vessels (TVs) associated with malignancy or benign findings. The category with the highest probability is outputted as the model's classification. While this probability gives an indication of the model's confidence in its prediction, it does not provide insights into the specific visual features that led to that prediction.\n\nTo enhance interpretability, future work could involve techniques such as Grad-CAM (Gradient-weighted Class Activation Mapping) or LIME (Local Interpretable Model-agnostic Explanations). These methods can highlight the regions of the image that most influenced the model's decision, providing some level of transparency. However, such techniques were not applied in this study, and thus, the current model remains a black-box in terms of interpretability.",
  "model/output": "The model developed is a classification model. It is designed to automatically identify tumor vessels (TVs) in digital single-operator cholangioscopy (D-SOC) images, distinguishing between images showing TVs associated with malignancy and those showing benign findings. The model outputs a classification for each image, indicating whether it contains TVs or benign findings. This classification is based on the probability assigned by the convolutional neural network (CNN) to each category, with the highest probability determining the final output. The performance of the model was evaluated using metrics such as sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), accuracy, and the area under the receiver operating characteristic curve (AUROC), all of which are typical for classification tasks.",
  "model/duration": "The convolutional neural network (CNN) demonstrated efficient computational performance. It completed the analysis of the entire validation dataset in just 27 seconds. This translates to an approximate processing speed of 20 milliseconds per image, indicating the model's capability to handle image data swiftly. This rapid processing time is crucial for real-time applications, such as during diagnostic procedures, where quick and accurate results are essential. The model's efficiency in processing images highlights its potential for integration into clinical workflows, where timely decision-making is vital.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method for the convolutional neural network (CNN) involved a comprehensive assessment using a validation dataset. The CNN was trained on 80% of the collected images, totaling 5180 images, while the remaining 20%, comprising 1295 images, were used for validation. This validation dataset included 829 images showing tumor vessels (TVs) and 466 images depicting benign findings.\n\nThe performance of the CNN was evaluated based on several key metrics: sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), accuracy, and the area under the receiver operating characteristic curve (AUROC). These metrics provided a robust assessment of the model's ability to accurately detect TVs associated with malignancy.\n\nAdditionally, the computational performance of the CNN was evaluated by measuring the time required to process the validation dataset. The CNN completed reading the validation dataset in 27 seconds, translating to an approximate processing speed of 20 milliseconds per image. This efficiency is crucial for real-time applications in clinical settings.\n\nStatistical analysis was performed using Sci-Kit learn v0.22.2, ensuring the reliability and validity of the results. The classification provided by the CNN was compared to the gold standard classification provided by experienced endoscopists, which integrated data from visual impressions, histopathology, and clinical follow-up. This comparison allowed for a thorough evaluation of the CNN's performance in detecting TVs associated with malignancy.",
  "evaluation/measure": "The performance of the convolutional neural network (CNN) was evaluated using several key metrics to ensure a comprehensive assessment of its diagnostic capabilities. The primary outcome measures included sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), accuracy, and the area under the receiver operating characteristic curve (AUROC). These metrics are widely recognized in the literature for evaluating the performance of diagnostic tools, particularly in medical imaging.\n\nSensitivity and specificity were reported at 99.3% and 99.4%, respectively, indicating the model's high true positive and true negative rates. The PPV and NPV were 99.6% and 98.7%, respectively, demonstrating the model's reliability in correctly identifying both the presence and absence of tumor vessels (TVs) associated with malignancy. The overall accuracy of the network was 99.3%, reflecting its high precision in classifying images.\n\nThe AUROC for detecting TVs was 1.00, which is the highest possible score, signifying perfect discrimination between positive and negative cases. This metric is crucial as it provides a single measure of the model's ability to distinguish between the two classes across all threshold levels.\n\nAdditionally, the computational performance of the CNN was assessed by measuring the time required to process the validation dataset. The CNN completed reading the validation dataset in 27 seconds, translating to an approximate processing speed of 20 milliseconds per image. This efficiency is important for real-time diagnostic applications.\n\nThese performance metrics collectively indicate that the CNN is highly effective and efficient in detecting TVs associated with malignancy in digital single-operator cholangioscopy (D-SOC) images. The reported metrics are representative of the standards used in the literature for evaluating similar diagnostic tools, ensuring that the results are comparable and reliable.",
  "evaluation/comparison": "Not applicable. The publication does not provide information about comparisons to publicly available methods or simpler baselines. The focus is on the development and performance of a convolutional neural network (CNN) for detecting tumor vessels (TVs) in direct visualization cholangioscopy (D-SOC) images. The evaluation primarily involves assessing the CNN's sensitivity, specificity, positive and negative predictive values, accuracy, and area under the receiver operating characteristic curve (AUROC) using a validation dataset. The study does not mention benchmark datasets or simpler baselines for comparison.",
  "evaluation/confidence": "The evaluation of the convolutional neural network (CNN) focused on several key performance metrics, including sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), accuracy, and the area under the receiver operating characteristic curve (AUROC). These metrics were calculated to assess the network's ability to detect tumor vessels (TVs) associated with malignancy.\n\nThe sensitivity and specificity of the CNN were both exceptionally high, at 99.3% and 99.4%, respectively. The PPV and NPV were 99.6% and 98.7%, respectively, indicating a very low rate of false positives and false negatives. The overall accuracy of the network was 99.3%, and the AUROC for detecting TVs was 1.00, which is the highest possible score, signifying perfect discrimination between positive and negative cases.\n\nStatistical significance was assessed using appropriate tests. Categorical variables were compared using the chi-square test, while continuous variables were compared using the Mann-Whitney U test. These tests helped to determine the significance of differences observed in the data.\n\nThe study also considered the computational performance of the CNN, which completed reading the validation dataset in 27 seconds, translating to an approximate processing speed of 20 milliseconds per image. This efficiency is crucial for real-time applications in clinical settings.\n\nWhile confidence intervals for the performance metrics were not explicitly mentioned, the high values and statistical significance of the metrics suggest a robust and reliable performance of the CNN. The results indicate that the method is superior in detecting TVs associated with malignancy compared to traditional diagnostic tools, which often have poor performance and fair to moderate interobserver agreement.\n\nIn summary, the evaluation demonstrates that the CNN is highly accurate and efficient in detecting TVs, with statistically significant results supporting its superiority over existing methods. The high performance metrics and computational efficiency make it a promising tool for improving the diagnostic yield in patients with indeterminate biliary strictures.",
  "evaluation/availability": "Not enough information is available."
}