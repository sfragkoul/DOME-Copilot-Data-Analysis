{
  "publication/title": "\"Application and accuracy of artificial intelligence-derived large language models in patients with age related macular degeneration\".",
  "publication/authors": "Ferro Desideri L, Roth J, Zinkernagel M, Anguita R",
  "publication/journal": "International journal of retina and vitreous",
  "publication/year": "2023",
  "publication/pmid": "37980501",
  "publication/pmcid": "PMC10657493",
  "publication/doi": "10.1186/s40942-023-00511-7",
  "publication/tags": "- LLMs\n- Large language models\n- Artificial Intelligence\n- Artificial intelligence in ophthalmology\n- Macular edema\n- Wet macular degeneration\n- Dry macular degeneration\n- Age-related macular degeneration\n- ChatGPT\n- Patient education",
  "dataset/provenance": "Not enough information is available.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "All data are available and kept in a protected database at Inselspital. The data is not released in a public forum. The data is kept in a protected database at Inselspital, Bern, Switzerland. The data is not publicly available, and there is no specific license associated with it. Access to the data is restricted and enforced through the institutional policies and procedures of Inselspital.",
  "optimization/algorithm": "Not applicable.",
  "optimization/meta": "Not applicable",
  "optimization/encoding": "Not enough information is available.",
  "optimization/parameters": "Not applicable",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "Not applicable",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "Not applicable.",
  "model/interpretability": "The models evaluated in our study, specifically ChatGPT 3.5, Bing AI, and Google Bard, are largely considered black-box models. This means that while they can generate human-like responses based on extensive training data, the internal workings of how they arrive at these responses are not transparent or easily interpretable. These large language models (LLMs) use complex neural networks and deep learning techniques to process and generate text, making it difficult to trace the exact reasoning or data points that contribute to a particular output.\n\nHowever, there are aspects where these models demonstrate a degree of transparency in their outputs. For instance, the responses generated by these LLMs can be evaluated for accuracy and sufficiency by domain experts. In our study, three experienced retina specialists assessed the responses, categorizing them as 'Accurate and Sufficient,' 'Partially Accurate and Sufficient,' or 'Inaccurate.' This evaluation process provides a level of interpretability by allowing experts to judge the reliability and comprehensiveness of the information provided by the models.\n\nAdditionally, the models' ability to simplify medical jargon and provide more comprehensible responses can be seen as a form of transparency. For example, ChatGPT 3.5 was found to generate responses with lower Flesch Kincaid Grade Level scores and fewer complex words, making the information more accessible to the average reader. This indicates that while the internal mechanisms of the models are not transparent, the outputs can be made understandable and interpretable by end-users, particularly when evaluated by experts.\n\nIn summary, while the LLMs used in our study are primarily black-box models, their outputs can be interpreted and evaluated for accuracy and comprehensibility by domain experts. This evaluation process adds a layer of transparency to the otherwise opaque internal workings of these models.",
  "model/output": "The model employed in this study is primarily a classification model. It was used to evaluate and categorize the responses generated by large language models (LLMs) into three distinct classes: \"Accurate and Sufficient,\" \"Partially Accurate and Sufficient,\" and \"Inaccurate and Not Sufficient.\" These classifications were based on the accuracy and sufficiency of the information provided by the LLMs in response to patient queries related to age-related macular degeneration (AMD).\n\nThe classification process involved a detailed evaluation by three experienced retina specialists, who assessed the responses for correctness and comprehensiveness. The results were then analyzed using statistical methods, including non-parametric tests and reliability tests, to compare the performance of different LLMs.\n\nThe study found that ChatGPT 3.5 consistently provided the most accurate and satisfactory responses, particularly for technical queries. While all three LLMs\u2014ChatGPT 3.5, Bing AI, and Google Bard\u2014showed promise in delivering precise information about AMD, there is still room for improvement, especially in handling more technical questions. The integration of such LLMs into clinical practice could offer significant benefits for both patients and ophthalmologists, enhancing accessibility and patient education.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not applicable",
  "evaluation/method": "The evaluation method involved assessing the responses generated by three large language models (LLMs): ChatGPT 3.5, Bing AI, and Google Bard. These responses were evaluated by three experienced retina specialists, each with at least eight years of clinical experience. The specialists categorized the responses based on their accuracy and sufficiency. The classifications were as follows:\n\n* 'Accurate and Sufficient' for responses that were both correct and comprehensive.\n* 'Partially Accurate and Sufficient' for responses that contained minor inaccuracies but still provided substantial and understandable information.\n* 'Inaccurate' for responses that were entirely incorrect or contained critical errors, rendering them unreliable.\n\nStatistical analysis was conducted using the SPSS program (IBM SPSS Statistics, version 25). Descriptive analysis, including frequency, means, and standard deviation, was performed. A normality distribution test (Shapiro\u2013Wilk) was also conducted. Given the abnormal distribution of the data, a non-parametric Kruskal\u2013Wallis test was performed to compare average scores across the three LLMs. Additionally, a reliability test was performed by measuring the Cronbach \u03b1 coefficient. A p-value of less than 0.05 was considered statistically significant.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the responses generated by three large language models (LLMs)\u2014ChatGPT 3.5, Bing AI, and Google Bard\u2014on questions related to age-related macular degeneration (AMD). The primary metrics used were accuracy and sufficiency, which were assessed by three experienced retina specialists. Responses were categorized into three groups: 'Accurate and Sufficient,' 'Partially Accurate and Sufficient,' and 'Inaccurate.'\n\nTo ensure statistical rigor, we conducted descriptive analysis, including frequency, means, and standard deviation. We also performed a normality distribution test using the Shapiro\u2013Wilk method. Given the abnormal distribution of the data, a non-parametric Kruskal\u2013Wallis test was used to compare average scores across the three LLMs. Additionally, a reliability test was performed by measuring the Cronbach \u03b1 coefficient to assess the consistency of the evaluations.\n\nThe average scores for the LLMs were reported, showing variations in performance across different types of questions. For general medical advice questions, the average scores were 1.20 for ChatGPT 3.5, 1.60 for Bing AI, and 1.60 for Google Bard, with no significant differences among the groups. However, for pre- and post-intravitreal injections advice questions, the average scores were 1.07 for ChatGPT 3.5, 1.69 for Bing AI, and 1.38 for Google Bard, indicating a significant difference among the groups (p = 0.0042).\n\nThe Cronbach \u03b1 coefficient of 0.237 suggested a relatively low agreement between the three LLMs, highlighting the variability in their performance. This set of metrics is representative of the current literature, which often focuses on accuracy, reliability, and statistical significance in evaluating the performance of LLMs in healthcare settings. Our approach ensures a thorough and unbiased assessment of the LLMs' capabilities in providing accurate and sufficient information to patients with AMD.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, we focused on evaluating the performance of three specific large language models (LLMs)\u2014ChatGPT 3.5, Bing AI, and Google Bard\u2014in responding to questions related to age-related macular degeneration (AMD). These LLMs were chosen for their prominence and accessibility via the internet.\n\nThe evaluation was conducted by three experienced retina specialists who categorized the responses based on accuracy and sufficiency. The responses were classified into three categories: 'Accurate and Sufficient,' 'Partially Accurate and Sufficient,' and 'Inaccurate.' This categorization allowed us to assess the reliability and effectiveness of each LLM in providing medical information to patients with AMD.\n\nStatistical analysis was performed using SPSS, including descriptive analysis, normality distribution tests, and non-parametric Kruskal\u2013Wallis tests to compare average scores across the three LLMs. The reliability of the evaluations was measured using Cronbach's alpha coefficient.\n\nWhile we did not compare the LLMs to simpler baselines, the study provides a comprehensive assessment of their performance in a real-world healthcare context. The results highlight the potential of LLMs in bridging communication gaps and providing understandable medical information, particularly in addressing specific patient queries within the context of AMD. However, the study also acknowledges limitations, such as the relatively low sample size and the need for further investigation into other advanced LLMs and larger-scale studies.",
  "evaluation/confidence": "The evaluation of the large language models (LLMs) involved a detailed statistical analysis to ensure the reliability and significance of the results. The responses generated by the LLMs were categorized based on their accuracy and sufficiency by experienced retina specialists. These categories included 'Accurate and Sufficient', 'Partially Accurate and Sufficient', and 'Inaccurate'.\n\nStatistical analysis was conducted using the SPSS program, version 25. Descriptive analysis, including frequency, means, and standard deviation, was performed. A normality distribution test (Shapiro\u2013Wilk) was also carried out. Given the abnormal distribution of the data, a non-parametric Kruskal\u2013Wallis test was used to compare the average scores across the three LLMs. This test helped in determining whether there were significant differences in the performance of the LLMs.\n\nThe reliability of the evaluations was assessed using Cronbach's alpha coefficient. A p-value of less than 0.05 was considered statistically significant. In the group of medical advice general questions, the average scores were 1.20 (\u00b1 0.41) for ChatGPT 3.5, 1.60 (\u00b1 0.63) for Bing AI, and 1.60 (\u00b1 0.73) for Google Bard, showing no significant differences among the three groups (p = 0.129). However, in the second group of questions (pre- and post-intravitreal injections advice questions), the average scores were 1.07 (\u00b1 0.27) for ChatGPT 3.5, 1.69 (\u00b1 0.63) for Bing AI, and 1.38 (\u00b1 0.63) for Google Bard, showing a significant difference among the three groups (p = 0.0042). The reliability statistics showed a Cronbach's alpha of 0.237, indicating a relatively low agreement between the three LLMs.\n\nThese statistical measures provide confidence in the evaluation results, particularly highlighting the superior performance of ChatGPT 3.5 in technical queries. The significant p-value in the second group of questions underscores the statistical significance of the differences observed among the LLMs.",
  "evaluation/availability": "All data generated and analyzed during this study are available and kept in a protected database at Inselspital. Unfortunately, the raw evaluation files are not publicly released. This decision was made to ensure the privacy and security of the data, as it contains sensitive medical information. The data is available upon reasonable request and with permission from the corresponding author. The study adheres to the Creative Commons Public Domain Dedication waiver, which applies to the data made available, unless otherwise stated. For more details on the license, one can visit the Creative Commons website."
}