{
  "publication/title": "Assessment of tumour viability in human lung cancer xenografts with texture-based image analysis.",
  "publication/authors": "Turkki R, Linder N, Holopainen T, Wang Y, Grote A, Lundin M, Alitalo K, Lundin J",
  "publication/journal": "Journal of clinical pathology",
  "publication/year": "2015",
  "publication/pmid": "26021331",
  "publication/pmcid": "PMC4518739",
  "publication/doi": "10.1136/jclinpath-2015-202888",
  "publication/tags": "- Tumor Viability\n- Automated Assessment\n- Texture Analysis\n- Machine Learning\n- Support Vector Machines\n- Image Processing\n- Histopathology\n- Mouse Xenografts\n- Non-Small Cell Lung Cancer\n- Digital Pathology\n- Viable Tumor Tissue\n- Non-Viable Tumor Tissue\n- Whole-Slide Imaging\n- Computer-Assisted Diagnosis\n- Preclinical Testing",
  "dataset/provenance": "The dataset used in this study consists of microscopy images of NSCLC xenograft tumors from mice treated with a set of antiangiogenic compounds. A total of 72 whole-slide images (WSIs) were initially scanned, but after a quality check to exclude out-of-focus areas, 56 WSIs were selected for further analysis. These WSIs were digitized using an automated whole-slide scanner with a resolution of 0.22 \u03bcm/pixel and compressed into a wavelet file format.\n\nFrom these WSIs, two sets of single-tissue entity images (STEIs) were created. The training set included 177 STEIs, with 57 from viable tumor tissue, 52 from non-viable tumor tissue, and 68 from mouse host tissue such as stroma, muscle, and adipose tissue. These STEIs were extracted from four WSIs that were not used in the testing phase. The test set consisted of 494 STEIs, with 242 from viable tumor regions and 252 from non-viable tumor regions, extracted from 23 WSIs.\n\nAdditionally, two researchers manually annotated regions of viable and non-viable tumor in the WSIs. These annotations were used to evaluate the automated assessment on a sample level. The dataset was specifically curated for this study and has not been used in previous publications or by the community. The focus was on creating a controlled subset of data representing different tissue categories to minimize the problem of wrongly labeled examples in the training set.",
  "dataset/splits": "The dataset was divided into two main sets: a training set and a test set. The training set consisted of images extracted from four whole-slide images (WSIs), which were not used in the testing phase. This training set was further divided into windows, resulting in a total of 6372 individual training windows. For the final model training, 5616 windows were randomly selected from each tissue category.\n\nThe test set comprised 494 single-tissue entity images (STEIs), which were selected from 23 WSIs. These STEIs were evenly distributed between viable tumour regions (242 images) and non-viable tumour regions (252 images). Additionally, the test set included 52 WSIs, which were used to evaluate the method on a sample level. These WSIs were manually annotated by two researchers to identify regions of viable and non-viable tumour tissue.\n\nThe dataset splits were designed to ensure a balanced representation of different tissue categories and to facilitate both training and evaluation of the proposed method. The training set was used to optimize the support vector machine (SVM) classifier, while the test set was used to assess the performance of the classifier in discriminating between viable and non-viable tumour tissue.",
  "dataset/redundancy": "The datasets used in this study were carefully split to ensure independence between training and test sets. The training set was extracted from four whole-slide images (WSIs) that were not used in the testing phase. This approach helps to prevent data leakage and ensures that the model's performance is evaluated on unseen data.\n\nThe test set consisted of 494 single-tissue entity images (STEIs), selected from viable tumour (n=242) and non-viable tumour regions (n=252). These STEIs were extracted from 23 WSIs, which were different from those used in the training set. This separation ensures that the test set is independent of the training set, providing a robust evaluation of the model's generalization capabilities.\n\nThe distribution of the datasets in this study is designed to represent the tissue categories of interest accurately. The STEI test set contains homogeneous areas of viable and non-viable tumour tissue, making it a precise reference for evaluating the method's ability to discriminate between these categories. In contrast, the WSI test set includes digitized whole tumours, which are more challenging to annotate with high precision. This dual approach allows for a comprehensive assessment of the method's performance at both the single-entity and whole-slide levels.\n\nCompared to previously published machine learning datasets in similar domains, the approach taken here emphasizes the importance of controlled and pure subsets of data. The STEI sets were created to minimize the problem of wrongly labeled examples in the training set, which is a common issue in more heterogeneous datasets. However, it is noted that the STEI set covers more limited sample areas compared to WSIs, which might affect its representativeness of the entire samples. This trade-off is carefully managed to balance precision and representativeness in the evaluation process.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used is a multiclass linear Support Vector Machine (SVM). This algorithm is not new; it is a well-established method in the field of machine learning. The choice to use this specific algorithm was driven by its effectiveness in handling large-scale learning tasks and its ability to model complex non-linear relationships through the combination of linear SVM and sparse kernel approximation.\n\nThe decision to use this algorithm was based on its proven track record and the need for efficient large-scale learning and complex non-linear modeling. The SVM was optimized using a threefold cross-validation process to determine the best cost parameter (C). The parameter values tested ranged from 2^-10 to 2^12, and the value that resulted in the highest average accuracy was selected. The final selected parameter value was C=8, achieving an average cross-validation accuracy of 97.5%.\n\nThe use of an open-source library for computer vision algorithms facilitated the feature mapping, approximating the \u03c72 kernel. This combination allowed for efficient large-scale learning and complex non-linear modeling, which was crucial for the classification of windows into the three target categories based on the obtained LBP/VAR descriptors.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The model is a support vector machine (SVM) classifier that uses texture descriptors, specifically Local Binary Patterns (LBP) and Variance (VAR) descriptors, extracted from image windows. These descriptors are computed directly from the image data and do not rely on predictions from other machine-learning models. The SVM classifier is trained using these descriptors to distinguish between different tissue categories, such as viable and non-viable tumour tissue.\n\nThe training process involves dividing images into windows, extracting the LBP/VAR descriptors, and then using these descriptors to train the SVM model. The model's performance is evaluated using cross-validation, where the data is split into folds to ensure that the training and validation sets are independent. This process helps to optimize the model's parameters and assess its generalization capability.\n\nThe SVM classifier used is a multiclass linear SVM with an L2-regularized L2-loss function. To enhance the model's discrimination power, a sparse feature mapping is applied to approximate the \u03c72 kernel. This combination allows for efficient large-scale learning and complex non-linear modeling. The final model is trained using a subset of the training windows, ensuring that the training data is independent and representative of the different tissue categories.",
  "optimization/encoding": "The images were initially downscaled to a pixel size of 0.44 micrometers and converted into grayscale. This conversion was achieved by weighting the color channels with a specific vector. The images were then divided into tiles, with a maximum size of 3968\u00d73968 pixels, and these tiles did not overlap. From these tiles, windows of 128\u00d7128 pixels were extracted, with an overlap of 50% for each subsequent window, achieved by reading every 64th pixel.\n\nTwo types of joint distributions were obtained from these grayscale-converted windows using Local Binary Patterns (LBP) and Variance (VAR) descriptors. The first distribution used parameters LBPriu23;8=VAR3;8, and the second used LBPriu24;16=VAR4;16. These parameters define the perimeter and radius pairs used, with the superscript riu2 indicating the rotation-invariant 2-uniform code mapping. The parameter values were chosen based on previous experience and preliminary tests.\n\nThe windows were classified into three target categories using a multiclass linear Support Vector Machine (SVM) combined with a sparse feature mapping. The SVM used was an L2-regularized L2-loss multiclass (one vs. rest) classifier. To enhance the discrimination of the linear model, a sparse feature mapping was applied to the features, approximating the \u03c72 kernel. This combination was chosen to enable efficient large-scale learning and complex non-linear modeling.\n\nAfter classifying all the windows of a whole-slide image (WSI), the classification results were merged into a result image. This was done by averaging the overlapping decision scores and applying a Gaussian filtering for final refinement. In the result image, the tumor region was first identified by majority voting, selecting the strongest decision value. A heat map was then drawn on top of the detected tumor, visually representing the classifier's classification confidence. Blue indicated viable tumor tissue, while red indicated non-viable tissue.",
  "optimization/parameters": "The model utilized two sets of parameters for the LBP/VAR joint distributions. The first set used parameters LBP_riu2_3;8=VAR_3;8, and the second set used parameters LBP_riu2_4;16=VAR_4;16. The subscripts (3,8) and (4,16) define the perimeter (P) and radius (R) pairs, respectively, while the superscript riu2 indicates the rotation invariant 2-uniform code mapping. These parameter values were chosen based on previous experience and preliminary tests.\n\nAdditionally, the cost parameter C of the SVM classifier was optimized using threefold cross-validation. The parameter values tested ranged from 2^-10 to 2^12, and the value that resulted in the highest average accuracy was selected. The chosen parameter value was C=8, achieving an average cross-validation accuracy of 97.5%. This value was then used to train the final SVM model.",
  "optimization/features": "The input features used in the model are derived from Local Binary Patterns (LBP) and Variance (VAR) descriptors. Two joint distributions were obtained from the grey-scale converted windows: the first with parameters LBPriu23;8=VAR3;8 and the second with parameters LBPriu24;16=VAR4;16. These parameters define the used perimeter (P) and radius (R) pairs: (P,R), and the superscript riu2 defines the applied code mapping, which is rotation invariant 2-uniform.\n\nThe features were extracted from windows of size 128\u00d7128 pixels, with an overlap of 50% for every subsequent window. This means that the windows were read every 64th pixel, creating a dense set of overlapping windows across the image.\n\nFeature selection was not explicitly mentioned as a separate step in the process. However, the parameters for the LBP and VAR descriptors were selected based on previous experience and preliminary tests, which could be considered a form of feature selection. The final features used in the model were the LBP/VAR joint distributions with the specified parameters.\n\nThe model optimization process involved a threefold cross-validation to select the best cost parameter (C) for the Support Vector Machine (SVM) classifier. This cross-validation was performed using the training set only, ensuring that the feature selection and model optimization were done in a way that prevents data leakage and maintains the integrity of the evaluation process.",
  "optimization/fitting": "The fitting method employed in this study utilized a multiclass linear Support Vector Machine (SVM) with a sparse feature mapping to classify windows into three target categories. The SVM classifier was regularized using an L2-regularized L2-loss function, which helps in preventing overfitting by penalizing large weights.\n\nTo optimize the cost parameter C of the SVM classifier, a threefold cross-validation was performed. This process involved dividing the 177 training images into windows, resulting in a total of 6372 individual training windows. The parameter values tested ranged from 2^-10 to 2^12, and the value that yielded the highest average accuracy was selected. The chosen parameter value was C=8, with an average cross-validation accuracy of 97.5%. This high accuracy indicates that the model generalizes well to unseen data, suggesting that overfitting was effectively mitigated.\n\nTo ensure that the model was not underfitting, a sufficient number of training windows were used. In total, 5616 windows were selected from each tissue category for training the final SVM model. This large number of training samples helps in capturing the complexity of the data, reducing the risk of underfitting.\n\nAdditionally, the use of sparse feature mapping approximating the \u03c72 kernel enabled efficient large-scale learning and complex non-linear modeling. This approach allowed the model to capture intricate patterns in the data, further reducing the likelihood of underfitting.\n\nIn summary, the fitting method involved a balanced approach to prevent both overfitting and underfitting. The use of cross-validation, regularization, and a large number of training samples ensured that the model was robust and generalizable.",
  "optimization/regularization": "In our study, we employed regularization techniques to prevent overfitting and enhance the generalization of our model. Specifically, we used an L2-regularized L2-loss Support Vector Machine (SVM) for classification. This approach helps to penalize large weights in the model, thereby reducing the risk of overfitting to the training data. Additionally, we utilized a sparse feature mapping to approximate the \u03c72 kernel, which further aids in managing the complexity of the model and improving its performance on unseen data. These methods collectively ensure that our model remains robust and reliable when applied to new datasets.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we employed a threefold cross-validation to optimize the cost parameter C of the SVM classifier. The parameter values tested ranged from 2^-10 to 2^12, and the value that yielded the highest average accuracy was selected for training the final SVM model. The chosen parameter value was C=8, resulting in an average cross-validation accuracy of 97.5%.\n\nThe model was trained using 5616 windows, which were randomly selected from each tissue category to ensure balanced representation. These details are provided to ensure reproducibility and transparency in our methodology.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly provided in the publication. However, the methods and configurations described are sufficient for researchers to replicate the experiments. The use of open-source libraries and standard image-processing frameworks, such as MATLAB, further facilitates this replication.\n\nThe publication does not specify the licensing terms for the data or code used, but it is implied that the methods and configurations are intended for academic and research purposes. For specific inquiries about licensing or access to additional resources, interested parties may need to contact the authors directly.",
  "model/interpretability": "The model employed in this study is not entirely a black box, as it utilizes interpretable features and a clear decision-making process. The approach relies on texture descriptors, specifically Local Binary Patterns (LBP) and Variance (VAR), which are well-understood and interpretable features in the field of image analysis. These descriptors capture the local texture patterns in the images, making them intuitive to understand and analyze.\n\nThe model uses a combination of a multiclass linear Support Vector Machine (SVM) and sparse feature mapping. The linear SVM is inherently more interpretable than complex, non-linear models because the decision boundaries are linear and can be visualized in the feature space. The sparse feature mapping approximates the \u03c72 kernel, which is a common and interpretable kernel function used in text classification and other domains.\n\nThe decision-making process involves classifying windows of images into different tissue categories based on the LBP/VAR descriptors. The results are then merged to create a heat map that visually represents the classifier's confidence in identifying viable and non-viable tumor regions. This heat map provides a clear and interpretable visualization of the model's decisions, making it easier to understand how the model arrives at its conclusions.\n\nAdditionally, the model's performance is evaluated using metrics such as percent agreement, kappa statistics, and the area under the receiver operating characteristic curve (AUC), which are standard and interpretable measures in the field of machine learning and medical imaging. These metrics provide a clear indication of the model's accuracy and reliability.\n\nIn summary, while the model involves complex computations, the use of interpretable features, a linear SVM, and clear visualization techniques makes it more transparent and understandable. This transparency is crucial for gaining trust in the model's predictions, especially in medical applications where interpretability is essential for clinical decision-making.",
  "model/output": "The model developed in this study is a classification model. It is designed to categorize tissue windows into three target categories based on the obtained Local Binary Patterns (LBP) and Variance (VAR) descriptors. Specifically, the model uses a multiclass linear Support Vector Machine (SVM) with a sparse feature mapping to classify the windows into viable tumour, non-viable tumour, and non-tumourous categories. The output of the model is a heat map that visually represents the classifier's confidence in identifying viable and non-viable tumour regions, with blue indicating viable tumour tissue and red indicating non-viable tissue. This heat map is overlaid on the detected tumour region in the result image, providing a clear visualization of the tumour viability assessment. The model's performance was evaluated using metrics such as percent agreement, kappa-statistics, and the area under the receiver operating characteristic curve (AUC), demonstrating high accuracy in discriminating between viable and non-viable tumour tissue.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The method was evaluated using two separate image sets: the STEI test set and the WSI test set. The STEI test set consisted of single-entity regions, which are images representing homogeneous areas of viable and non-viable tumour tissue. These images were divided into windows and classified according to the dominant tissue category. The WSI test set, on the other hand, was a collection of digitized whole tumours.\n\nThe agreement of the method was evaluated by comparing the obtained results with expert annotations. This comparison was done using several metrics, including percent agreement, kappa statistics, and the area under the receiver operating characteristic curve (AUC) for the STEI test set. Additionally, the results were evaluated on the WSIs based on percent agreement on the pixel level (agreement in segmentation) and by Pearson\u2019s product-moment correlation on a sample level (agreement in tumour viability assessment).\n\nThe proposed approach was also compared to a standard classification method: nearest neighbour (NN). This comparison was done to assess the performance of the proposed method relative to an established technique.\n\nIn summary, the evaluation involved a comprehensive comparison with expert annotations and a benchmarking against a standard classification method, ensuring a robust assessment of the method's performance.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our method in assessing tumour viability. For the single-tissue entity images (STEIs), we reported the agreement percentage, which was 94.5%. Additionally, we calculated the area under the receiver operating characteristic curve (AUC), which was 0.995, and the kappa statistic, which was 0.90 with a 95% confidence interval of 0.86 to 0.97. These metrics provided a comprehensive evaluation of our method's ability to discriminate between viable and non-viable tumour tissue.\n\nWe also reported sensitivity and specificity. The sensitivity was 90.5%, indicating the proportion of actual viable tumour cases correctly identified by our method. The specificity was 99.2%, showing the proportion of actual non-viable tumour cases correctly identified.\n\nFor the whole-slide images (WSIs), we evaluated the agreement on a pixel level, achieving an average agreement of 83.3% in segmentation. We also assessed the correlation between our automated tumour viability assessment and expert readouts using Pearson\u2019s product-moment correlation, which was r=0.79 with a 95% confidence interval of 0.66 to 0.87 and a p-value of less than 0.0001. This correlation was calculated on a sample level across 52 WSIs.\n\nThese performance metrics are representative of those commonly used in the literature for evaluating image classification and segmentation tasks, particularly in the context of digital pathology. The use of agreement percentage, AUC, kappa statistic, sensitivity, specificity, and correlation coefficients ensures a thorough assessment of our method's accuracy and reliability.",
  "evaluation/comparison": "In our evaluation, we compared the proposed approach to a simpler baseline method to assess its performance. Specifically, we evaluated our method against a nearest neighbour (NN) classifier using the same features. This comparison was conducted to provide a benchmark and demonstrate the effectiveness of our approach.\n\nHowever, a comparison to publicly available methods on benchmark datasets was not performed. The lack of publicly available validation data makes cross-comparison with other published works uncertain. This limitation highlights the need for standardized datasets and benchmarks in the field to facilitate more comprehensive evaluations and comparisons.",
  "evaluation/confidence": "The evaluation of the proposed method includes several performance metrics with associated confidence intervals, ensuring the reliability of the results. For instance, the agreement in classifying single-tissue entity images (STEIs) into viable or non-viable categories is reported with a kappa value of 0.90, accompanied by a 95% confidence interval (CI) ranging from 0.86 to 0.97. This indicates a high level of agreement and statistical significance.\n\nAdditionally, the area under the receiver operating characteristic curve (AUC) for the STEI test set is 0.995, demonstrating excellent discriminative power. The statistical significance of the results is further supported by the p-value, which is less than 0.0001 for the Pearson\u2019s product-moment correlation on the sample level in the whole-slide images (WSIs) test set. This low p-value confirms that the observed correlation is highly unlikely to have occurred by chance, reinforcing the robustness of the method.\n\nThe sensitivity and specificity of the method are also provided, with values of 90.5% and 99.2%, respectively. These metrics, along with the confidence intervals and statistical significance, collectively indicate that the proposed approach is superior to the baseline methods, such as the nearest neighbour (NN) classifier. The high agreement rates and statistically significant results across different evaluation metrics underscore the method's effectiveness and reliability in assessing tumour viability.",
  "evaluation/availability": "Not enough information is available."
}