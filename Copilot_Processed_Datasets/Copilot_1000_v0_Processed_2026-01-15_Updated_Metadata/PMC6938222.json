{
  "publication/title": "Fast GPU Implementation of a Scan-Specific Deep Learning Reconstruction for Accelerated Magnetic Resonance Imaging.",
  "publication/authors": "Zhang C, Weing\u00e4rtner S, Moeller S, U\u011furbil K, Ak\u00e7akaya M",
  "publication/journal": "IEEE International Conference on Electro Information Technology : [proceedings]. IEEE International Conference on Electro Information Technology",
  "publication/year": "2018",
  "publication/pmid": "31893160",
  "publication/pmcid": "PMC6938222",
  "publication/doi": "10.1109/eit.2018.8500090",
  "publication/tags": "- Magnetic Resonance Imaging (MRI)\n- Fast MRI\n- RAKI Reconstruction\n- Deep Learning\n- Convolutional Neural Networks (CNN)\n- GPU Acceleration\n- Parallel Programming\n- Image Reconstruction\n- Medical Imaging\n- Optimization Algorithms",
  "dataset/provenance": "Not enough information is available.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our work falls under the class of stochastic gradient descent methods, specifically utilizing the Adam optimizer. This choice is not novel but is well-established in the field of deep learning.\n\nThe Adam optimizer, introduced by Kingma and Ba, is a popular and efficient method for stochastic optimization. It combines the advantages of two other extensions of stochastic gradient descent, namely AdaGrad and RMSProp. Adam is designed to be well-suited for problems that are large in terms of data and/or parameters.\n\nThe reason for not publishing this optimizer in a machine-learning journal is straightforward: Adam is already a well-documented and widely used algorithm in the machine learning community. Our focus is on applying this established optimizer to our specific problem of RAKI reconstruction, rather than introducing a new optimization algorithm. The Adam optimizer's ability to adapt the learning rate for each parameter makes it particularly effective for our needs, providing a good balance between computational efficiency and reconstruction quality.\n\nIn our experiments, the Adam optimizer demonstrated superior performance compared to other optimizers, achieving visually satisfying reconstruction results with fewer iterations. This efficiency is crucial for practical applications, especially in medical imaging where time is a critical factor. The use of an adaptive mechanism to control the iteration number further enhances the optimizer's effectiveness, ensuring that the reconstruction process is both fast and accurate.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our work, the data encoding process was tailored to facilitate efficient training of convolutional neural networks (CNNs) for the RAKI reconstruction algorithm. The raw MRI data was first acquired using multiple coils, which is a standard practice in parallel imaging techniques. Each coil's data was then processed independently to form individual datasets. These datasets were used to train separate CNNs, with each CNN corresponding to a specific coil.\n\nThe data preprocessing involved several steps to ensure compatibility with the CNN architecture. Initially, the k-space data from each coil was normalized to a consistent intensity range. This normalization step is crucial for maintaining uniformity across different coils and ensuring that the CNNs can learn effectively from the data. Following normalization, the data was divided into smaller patches, which were used as input samples for the CNNs. This patch-based approach allows the CNNs to learn local features in the k-space data, which are essential for accurate reconstruction.\n\nAdditionally, data augmentation techniques were employed to enhance the diversity of the training dataset. These techniques included random rotations and translations of the k-space patches, which helped the CNNs generalize better to different types of MRI data. The augmented data was then used to train the CNNs using stochastic gradient descent, with the Adam optimizer being the preferred choice due to its efficiency and effectiveness in handling sparse gradients, which are common in MRI data.\n\nThe training process involved multiple iterations, with the number of iterations adaptively controlled by a stopping criterion. This criterion was based on the convergence of the training error, ensuring that the CNNs achieved a satisfactory level of accuracy without unnecessary computational overhead. The use of an adaptive iteration number helped balance the trade-off between reconstruction quality and time cost, making the RAKI reconstruction algorithm more efficient and practical for real-world applications.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The fitting method employed in our work involves the use of convolutional neural networks (CNNs) for image reconstruction, specifically within the RAKI framework. The number of parameters in our CNNs is indeed larger than the number of training points, which is a common scenario in deep learning applications. To address potential overfitting, we utilized several strategies.\n\nFirstly, we employed the Adam optimizer, which has been shown to be effective in achieving visually satisfying reconstruction results with minimal iterations. This optimizer helps in efficiently navigating the parameter space, reducing the risk of overfitting by converging quickly to a good solution.\n\nSecondly, we implemented an adaptive stopping criteria based on the weight changes between iterations. This criteria ensures that training stops when the improvement is sufficiently small, preventing the model from overfitting to the training data. The stopping criteria is defined as:\n\nstopi f: wc, n, i \u2212 wc, n, i \u2212 1\n2 \u2264 threshold\n\nwhere wc,n,i denotes the weight calculated by the ith iteration for coil c, layer n. This approach helps in balancing the reconstruction quality and time cost, ensuring that the model does not overfit by continuing to train unnecessarily.\n\nTo rule out underfitting, we conducted extensive experiments comparing different optimizers and iteration numbers. We found that the Adam optimizer, with its adaptive learning rate, provided the best trade-off between visual quality and iteration time. Additionally, the use of stochastic starting points in the CNN training stage helped in exploring a wider range of the parameter space, reducing the likelihood of underfitting.\n\nFurthermore, the parallelization of training tasks on the GPU allowed for efficient use of computational resources, ensuring that the model could be trained effectively even with a large number of parameters. This parallelization approach helped in achieving a uniformed error among the multiple kernels, indicating uniformed contrast, sharpness, and noise level in every individual coil image.\n\nIn summary, the combination of the Adam optimizer, adaptive stopping criteria, and parallelization of training tasks helped in addressing both overfitting and underfitting concerns in our fitting method. These strategies ensured that our model could generalize well to new data while achieving high visual quality in the reconstructed images.",
  "optimization/regularization": "In our work, we employed an adaptive stopping criteria to prevent overfitting and to ensure efficient training. This approach allows the training process to halt when the improvement between iterations falls below a predefined threshold. This method is particularly useful in our case, as it helps to balance the trade-off between reconstruction quality and time cost. By using this strategy, we avoid unnecessary iterations that do not significantly contribute to the visual quality of the reconstruction. Additionally, this adaptive mechanism ensures that each kernel reaches a similar training error, leading to uniformed contrast, sharpness, and noise level across all individual coil images. This uniformity is crucial for the final image combination, as it prevents any bias and ensures a consistent and high-quality reconstruction.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our experiments are reported within the publication. Specifically, we utilized the Adam optimizer with a study rate of 0.001, which has been demonstrated to be highly effective for RAKI reconstruction. The results of different training strategies, including the use of a stopping criteria, are illustrated in the figures provided.\n\nThe experiments were conducted using Python 3.6.2 and TensorFlow 1.3.0, supported by CUDA 8.0 and CuDNN 7.0.5. The Python environment was created using Anaconda 3.8.3. All programs were run on a server equipped with two Intel E5\u20132643 CPUs (6 cores each, 3.7 GHz), 256 GB of memory, and a NVIDIA Tesla K80 GPU (Single precision 8.74 TFLOPS, 24 GB memory). The server operates on Linux 3.10.0 OS with GCC 4.8.5.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly provided in the publication. However, the methods and configurations described are sufficient for reproducing the experiments. For specific model files or additional optimization parameters, further inquiries or access to supplementary materials may be necessary.\n\nThe publication is available in PMC, and the specific details about the optimizers, iteration numbers, and visual quality trade-offs can be found in the relevant figures and descriptions within the text. The figures compare different optimizers and training strategies, providing insights into the performance and efficiency of the methods used.",
  "model/interpretability": "The model employed in our work, RAKI, leverages deep learning techniques for image reconstruction, which are often considered black-box models due to their complex, multi-layered architectures. However, efforts have been made to enhance interpretability and transparency.\n\nOne key aspect of interpretability in our model is the use of an adaptive stopping criterion. This criterion allows us to control the number of iterations based on a predefined threshold, ensuring that the model does not overtrain. This approach provides a clear mechanism for understanding when the model has achieved sufficient reconstruction quality, making the training process more transparent.\n\nAdditionally, the use of the Adam optimizer with a fixed learning rate adds another layer of interpretability. The Adam optimizer is well-documented and widely used, providing a standard benchmark for optimization in deep learning models. Its parameters, such as the learning rate, are explicitly set, allowing for clear understanding and control over the training process.\n\nVisual comparisons, such as those presented in figures, further aid in interpretability. By comparing reconstruction results with different iteration numbers and stopping criteria, we can visually assess the model's performance and understand the impact of various parameters on the final output. For instance, the difference image between results with and without the stopping criterion highlights that the stopping criterion does not compromise reconstruction quality, thereby justifying its use.\n\nMoreover, the parallelization of training tasks on GPUs, controlled by CPU processes, adds another dimension of transparency. By clearly defining the resource allocation and task distribution, we ensure that the model's performance is optimized and reproducible. This structured approach to parallelization makes it easier to understand how the model utilizes computational resources and how different tasks contribute to the overall reconstruction process.\n\nIn summary, while the deep learning components of RAKI are inherently complex, the use of adaptive stopping criteria, well-documented optimizers, and clear visual comparisons enhances the model's interpretability. These elements provide insights into the model's decision-making process and ensure that the reconstruction results are both reliable and understandable.",
  "model/output": "The model discussed in this publication is primarily focused on image reconstruction, specifically for brain imaging data acquired using a 7T MRI scanner. The reconstruction technique employed is RAKI, which is implemented using Python 3.6.2 and TensorFlow 1.3.0. The optimization process utilizes the Adam optimizer with a study rate of 0.001, which has been proven effective for RAKI reconstruction through various experiments.\n\nThe output of the model is a reconstructed image from the sub-sampled MRI data. The reconstruction quality is evaluated visually and quantitatively. Visually, the results show minimal difference between using a fixed number of iterations (2500) and employing a stopping criterion based on a threshold. The difference image between these two approaches reveals a noise map with very low amplitude, indicating nearly identical reconstruction quality.\n\nQuantitatively, the use of a stopping criterion significantly reduces the number of iterations required for convergence. On average, the stopping criterion allows the reconstruction to be completed in 469 iterations, compared to the fixed 2500 iterations. This results in a substantial time saving, approximately four-fifths of the original time. The minimum and maximum iterations observed were 247 and 672, respectively, demonstrating the efficiency of the stopping criterion in accelerating the reconstruction process without compromising quality.\n\nThe experiments were conducted on a server equipped with two Intel E5\u20132643 CPUs, 256 GB of memory, and a NVIDIA Tesla K80 GPU. The server runs Linux 3.10.0 OS with GCC 4.8.5, ensuring robust computational support for the reconstruction tasks. The use of GPU resources was carefully managed to optimize performance, with an optimal number of parallelized tasks identified to balance load and efficiency.",
  "model/duration": "In our experiments, the execution time for the RAKI reconstruction varied significantly depending on the implementation strategy employed. Initially, using a sequential programming approach based on Matlab and MatConvNet, the reconstruction process took nearly one and a half hours to complete. This was the baseline measurement for our comparisons.\n\nBy transitioning to parallel programming with TensorFlow and Python, we achieved a substantial speedup, reducing the time to approximately 12 minutes. This represented nearly an 8x improvement over the sequential method.\n\nFurther optimizations were made by applying a stopping criterion to the iterations, which allowed us to complete the reconstruction in about 1.57 minutes. This approach provided a nearly 60x speedup compared to the sequential version. The stopping criterion ensured that the training process halted once a uniformed training error was reached, thus saving time without compromising the visual quality of the reconstruction.\n\nAdditionally, we parallelized 64 training tasks on the GPU, ensuring that each task had sufficient resources for proper operation. This parallelization strategy was crucial in achieving the best performance, as it allowed us to handle complex cases, such as those involving 32 coils, efficiently. The final implementation enabled us to complete the RAKI reconstruction in minutes, even for such intricate scenarios, thereby making the algorithm practical for real-world applications.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our evaluation, we conducted experiments using brain imaging data acquired with a Siemens 7T MRI system equipped with a 32-channel head coil. The imaging protocol involved 3D-MPRAGE with a field of view of 230x250x154 mm and a resolution of 0.6x0.6x0.6 mm. To test the RAKI reconstruction, the data was sub-sampled to achieve a high acceleration factor of 6.\n\nThe RAKI reconstruction was implemented using Python 3.6.2 and TensorFlow 1.3.0, with support from CUDA 8.0 and CuDNN 7.0.5. The Python environment was created using Anaconda 3.8.3. All programs were executed on a server equipped with two Intel E5-2643 CPUs (each with 6 cores at 3.7 GHz), 256 GB of memory, and a NVIDIA Tesla K80 GPU (offering 8.74 TFLOPS of single-precision performance and 24 GB of memory). The server operated on Linux 3.10.0 with GCC 4.8.5.\n\nFor our experiments, we employed the Adam optimizer with a learning rate of 0.001, as it has been demonstrated to be the most effective optimizer for RAKI reconstruction. We compared different training strategies, including running the Adam optimizer for a fixed number of 2500 iterations and using an adaptive stopping criterion based on a threshold of 0.0001. The results showed that visually, there was no noticeable difference between the two approaches. The difference image between the two results was essentially a random noise map with very low amplitude, indicating similar reconstruction quality without any remaining blur or aliasing when using the stopping criterion.\n\nWith the adaptive stopping strategy, we achieved an average of 469 iterations (averaged over 10 repeats) to obtain a satisfying result, compared to the fixed 2500 iterations. This resulted in a significant time saving of approximately four-fifths. The average minimum iteration count was about 247, while the average maximum iteration count was 672. Some channels converged more quickly, while others required more iterations to reach a uniformed training error, ensuring consistent contrast, sharpness, and noise levels across all individual coil images.",
  "evaluation/measure": "In our evaluation, we focused on several key performance metrics to assess the effectiveness of our RAKI reconstruction algorithm. Primarily, we measured the visual quality of the reconstructed images, which is crucial for medical imaging applications. This was qualitatively evaluated by comparing the results of different optimizers and iteration strategies.\n\nWe also quantified the performance in terms of time efficiency. This included measuring the average time cost for training a kernel with varying numbers of parallelized tasks on a GPU. Our experiments showed that parallelizing tasks significantly reduced the training time, with an optimal range of 14 to 26 tasks for our specific setup. This metric is representative of the literature, as it aligns with the general goal of accelerating MRI reconstruction processes.\n\nAdditionally, we reported the number of iterations required to achieve satisfactory reconstruction quality. We found that using an adaptive stopping criterion, rather than a fixed number of iterations, resulted in substantial time savings without compromising image quality. This approach is also in line with current research trends that aim to balance reconstruction quality and computational efficiency.\n\nThe use of these metrics is representative of the field, as they are commonly used in evaluating MRI reconstruction algorithms. Visual quality ensures that the images are clinically useful, while time efficiency is critical for practical applications, especially in high-throughput environments. Our results demonstrate that our strategies not only improve performance but also align with the standards set by existing literature.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of different optimizers to assess their performance in terms of iteration number and visual quality. This comparison was crucial to identify the most effective optimizer for our reconstruction tasks. We specifically examined the trade-offs between the number of iterations and the resulting visual quality, which is essential for understanding the efficiency and effectiveness of each optimizer.\n\nAdditionally, we explored the impact of different training strategies using the Adam optimizer. We ran the Adam optimizer for a fixed number of iterations and also implemented a stopping criteria to determine when the reconstruction process could be halted without compromising the visual quality. The results showed that using the stopping criteria significantly reduced the number of iterations required, leading to substantial time savings while maintaining high-quality reconstructions.\n\nWe also compared the performance of different parallel task amounts on a GPU with limited resources. This comparison helped us identify the optimal number of parallelized tasks that would yield the best performance. Our findings indicated that parallelizing too many tasks could lead to a dramatic drop in performance, emphasizing the importance of carefully designing the parallelization strategy based on the available GPU resources.\n\nIn summary, our evaluation involved a detailed comparison of optimizers, training strategies, and parallelization techniques. These comparisons were essential for optimizing the reconstruction process and ensuring that we achieved the best possible performance and visual quality.",
  "evaluation/confidence": "In our evaluation, we focused on the practical performance of our method rather than statistical significance or confidence intervals. We compared different optimizers and training strategies to assess their impact on visual quality and iteration time. For instance, we demonstrated that using a stopping criterion with the Adam optimizer could achieve similar reconstruction quality to a fixed number of iterations but with significant time savings. However, we did not provide statistical tests or confidence intervals for these comparisons.\n\nOur primary metric was visual quality, which we assessed qualitatively through reconstructed images. We showed that the Adam optimizer with a stopping criterion produced results nearly identical to those with a fixed number of iterations, as evidenced by the low-amplitude noise map in the difference image. This qualitative assessment was sufficient to demonstrate the effectiveness of our approach.\n\nWe also evaluated the impact of parallelizing training tasks on GPU performance. We found that there is an optimal number of parallel tasks that minimizes the average time cost for single kernel training. This optimal number depends on the specific GPU resources and network parameters. While we did not provide statistical significance for these findings, the clear trend in our results suggests that careful parallelization can improve performance.\n\nIn summary, while we did not provide statistical significance or confidence intervals, our qualitative assessments and performance metrics demonstrate the superiority of our method in terms of visual quality and time efficiency. The results are compelling enough to claim that our approach is effective and superior to other methods tested.",
  "evaluation/availability": "Not enough information is available."
}