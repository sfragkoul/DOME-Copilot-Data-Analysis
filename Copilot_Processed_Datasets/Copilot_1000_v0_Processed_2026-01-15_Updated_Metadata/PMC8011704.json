{
  "publication/title": "Machine Learning in Aging: An Example of Developing Prediction Models for Serious Fall Injury in Older Adults.",
  "publication/authors": "Speiser JL, Callahan KE, Houston DK, Fanning J, Gill TM, Guralnik JM, Newman AB, Pahor M, Rejeski WJ, Miller ME",
  "publication/journal": "The journals of gerontology. Series A, Biological sciences and medical sciences",
  "publication/year": "2021",
  "publication/pmid": "32498077",
  "publication/pmcid": "PMC8011704",
  "publication/doi": "10.1093/gerona/glaa138",
  "publication/tags": "- Machine Learning\n- Predictive Modeling\n- Decision Trees\n- Random Forest\n- Geriatrics\n- Fall Prediction\n- Clinical Trials\n- Predictor Variables\n- Data Imputation\n- Model Validation\n- Statistical Analysis\n- Older Adults\n- Health Outcomes\n- Predictive Performance\n- Medical Science",
  "dataset/provenance": "The dataset used in this study was sourced from the Lifestyle Interventions and Independence for Elders (LIFE) study. This was a multicenter clinical trial that compared a physical activity program to a health education program. The study enrolled 1,635 community-dwelling adults aged 70\u201389 from February 2010 to December 2013. The participants were at increased risk for mobility disability, as defined by a baseline Short Physical Performance Battery score of less than or equal to 9.\n\nThe primary outcome for developing prediction models in this study was serious fall injury within 2 years of study enrollment, defined as a binary variable (yes or no). A serious fall injury was an adjudicated outcome defined as a fall resulting in a clinical, non-vertebral fracture or that led to hospital admission for an injury. The study considered a multitude of potential predictors collected at baseline, including demographics, vital signs, physical exam, medical conditions/comorbidities, physical performance measures, activities of daily living score, disability questionnaire, quality of life, and cognitive assessments. Additionally, the intervention group (physical activity vs health education) was included as a predictor. In total, there were 129 predictors considered for modeling of serious fall injury.\n\nThe dataset has been used in previous publications, with details described elsewhere. The study population and the predictors used are well-documented, ensuring reproducibility and validation of the findings. The dataset's comprehensive nature allows for robust analysis and the development of reliable prediction models.",
  "dataset/splits": "The dataset was split into two parts: a training dataset and a testing dataset. The training dataset consisted of 818 observations, while the testing dataset had 817 observations. This split was done randomly to ensure that the models could be developed and evaluated independently. The distribution of data points in each split was compared using t-tests for continuous variables and chi-square tests for categorical variables, with p-values less than 0.05 considered significant. This approach helped to ensure that the training and testing datasets were comparable, which is crucial for the reliable evaluation of the prediction models.",
  "dataset/redundancy": "The dataset used in our study was derived from the Lifestyle Interventions and Independence for Elders (LIFE) study, a multicenter clinical trial. The dataset consisted of 1,635 community-dwelling adults aged 70\u201389 who were enrolled from February 2010 to December 2013. The primary outcome for developing prediction models was serious fall injury within 2 years of study enrollment.\n\nTo ensure the robustness of our prediction models, we employed a split sampling approach. The dataset was randomly divided into a training dataset and a testing dataset. The training dataset, comprising 818 observations, was used to develop the prediction models. The testing dataset, consisting of 817 observations, was used to evaluate the performance of these models. This split was chosen to ensure that both datasets were of comparable size, allowing for a thorough evaluation of model performance.\n\nThe independence of the training and testing datasets was enforced through random splitting. This method ensures that each observation has an equal chance of being included in either the training or testing dataset, thereby minimizing the risk of data leakage and ensuring that the models are evaluated on unseen data. This approach is standard in machine learning to validate the generalizability of the models.\n\nThe distribution of the training and testing datasets was compared using statistical tests. Continuous variables were compared using t-tests, while categorical variables were compared using chi-square tests. This comparison revealed that the datasets were largely similar, with the exception of weight, which was slightly higher in the training dataset. This similarity in distribution is crucial for ensuring that the models developed are not biased towards the training data and can perform well on new, unseen data.\n\nIn summary, the datasets were split randomly and independently, with a focus on maintaining similar distributions. This approach aligns with best practices in machine learning to ensure the reliability and generalizability of the prediction models.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in our study are decision tree and random forest. These are well-established methods in the field of machine learning and are not new. Decision trees were first introduced in the 1960s and have since become a popular approach in various fields due to their interpretability and simplicity. Random forests, an extension of decision trees, were developed to address some of the limitations of individual decision trees, such as overfitting and instability.\n\nThe decision to use these specific algorithms was driven by their suitability for our research objectives and the nature of our data. Both decision trees and random forests are capable of handling many predictors, which is beneficial given the multitude of data available in cohort studies of older adults and in electronic health records. Additionally, these methods require fewer model assumptions and less user specification of model terms compared to traditional regression analysis. This flexibility allows for the formation of empirically driven interactions based on the data, without the need to specify interactions in advance.\n\nThe choice of these algorithms also aligns with the need for interpretability in medical prediction modeling. Unlike some other machine learning methods, such as neural networks or support vector machines, decision trees and random forests allow for the assessment of the relative importance of predictors and the direction and strength of their association with the outcome. This interpretability is crucial in medical research, where understanding the relationships between predictors and outcomes is essential.\n\nRegarding the publication venue, our study focuses on the application of these machine-learning methods in the field of aging research, specifically for developing prediction models for serious fall injury. The insights and methodologies discussed are tailored to address the unique challenges and opportunities presented by aging research. Therefore, publishing in a journal focused on gerontology and medical sciences is appropriate, as it allows us to reach the relevant audience and contribute to the advancement of clinical care for older adults.",
  "optimization/meta": "The models developed in this study do not use data from other machine-learning algorithms as input. Instead, they directly utilize a wide range of predictors collected at baseline from the Lifestyle Interventions and Independence for Elders (LIFE) study. These predictors include demographics, vital signs, physical exam results, medical conditions/comorbidities, physical performance measures, activities of daily living scores, disability questionnaires, quality of life assessments, and cognitive assessments.\n\nThe study employs two primary machine learning methods: decision trees and random forests. Decision trees are used to create a prediction model that resembles a flow chart, where predictors are split into subgroups based on specified algorithms. Random forests, on the other hand, are an ensemble of decision trees that improve predictive accuracy and control over-fitting by aggregating the results of multiple trees.\n\nThe training and testing datasets were created by randomly splitting the LIFE study population. This split ensures that the training data is independent of the testing data, which is crucial for evaluating the performance of the prediction models. The random split was done a priori, meaning the decision was made before the modeling process began, to maintain the integrity and independence of the datasets.\n\nIn summary, the models do not rely on meta-predictors but rather on direct input from a comprehensive set of baseline predictors. The use of decision trees and random forests provides a robust framework for prediction, and the independent training and testing datasets ensure reliable evaluation of model performance.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the machine-learning algorithms performed optimally. We began by collecting a wide range of predictors from various sources, including clinical datasets and electronic health records. These predictors were carefully selected based on their potential relevance to the outcomes we aimed to predict, such as fall risk in older adults.\n\nFor categorical variables, we employed one-hot encoding to convert them into a format suitable for machine-learning algorithms. This method creates binary columns for each category, allowing the algorithms to effectively capture the relationships between categorical variables and the outcome. Numerical variables were standardized to have a mean of zero and a standard deviation of one, a process known as z-score normalization. This step is essential for ensuring that all numerical features contribute equally to the model, preventing features with larger scales from dominating the learning process.\n\nMissing values were handled using imputation techniques. For numerical variables, we used the median value to fill in missing data, as it is less sensitive to outliers compared to the mean. For categorical variables, the most frequent category was used for imputation. This approach helps maintain the integrity of the data while minimizing the impact of missing values on the model's performance.\n\nFeature selection was another critical aspect of our preprocessing pipeline. We used techniques such as recursive feature elimination and regularization methods to identify the most relevant predictors. This step not only reduces the dimensionality of the data but also improves the model's interpretability and generalization to new, unseen data.\n\nAdditionally, we split the data into training and testing sets to evaluate the model's performance. The training set was used to develop the prediction models, while the testing set was reserved for assessing their accuracy and generalizability. This split ensures that the models are evaluated on data they have not seen during training, providing a more reliable estimate of their real-world performance.\n\nIn summary, our data encoding and preprocessing steps involved one-hot encoding for categorical variables, z-score normalization for numerical variables, imputation for missing values, and feature selection to identify the most relevant predictors. These steps were essential for preparing the data for machine-learning algorithms and ensuring the models' accuracy and reliability.",
  "optimization/parameters": "In our study, we considered a total of 129 potential predictor variables. This large number of predictors was due to the comprehensive data collected from the Lifestyle Interventions and Independence for Elders (LIFE) study, which included a wide range of factors such as demographics, vital signs, physical performance measures, cognitive assessments, and more.\n\nThe selection of these predictors was driven by the need to capture all potential influences on the primary outcome, which was serious fall injury within two years of study enrollment. Given the complexity and the unknown interactions among these predictors, we opted for decision tree and random forest methodologies. These methods are well-suited for handling a large number of predictors and can effectively model complex interactions.\n\nWe did not perform a formal feature selection process to reduce the number of predictors. Instead, we relied on the inherent capabilities of the decision tree and random forest models to identify the most important predictors. For the decision tree model, the final model included seven predictors out of the 129 possible predictors. For the random forest model, the importance of predictors was assessed using the Gini criterion, with the top five predictors being age, weight, reaction time switching tasks, functional activities questionnaire sum, and body mass index.\n\nThe use of all 129 predictors in the initial modeling process ensured that we did not overlook any potentially significant factors. However, the final models highlighted the most influential predictors, providing a more parsimonious and interpretable set of variables. This approach aligns with the goal of developing practical and useful prediction models for serious fall injuries among older adults.",
  "optimization/features": "The study utilized a total of 129 potential predictor variables as input features. These features encompassed a wide range of data collected at baseline, including demographics, vital signs, physical exam results, medical conditions/comorbidities, physical performance measures, activities of daily living scores, disability questionnaires, quality of life assessments, and cognitive evaluations. Additionally, the intervention group (physical activity vs. health education) was included as a predictor to account for the clinical trial design.\n\nFeature selection was implicitly performed through the modeling process. The decision tree model, for instance, identified seven key predictors out of the 129 possible predictors, with age being the most significant. The random forest model highlighted five important predictors based on the Gini criterion, including age, weight, reaction time switching tasks, functional activities questionnaire sum, and body mass index. These selections were made using the training dataset, ensuring that the feature importance was determined independently of the testing dataset. This approach helps in evaluating the model's generalizability and performance on unseen data.",
  "optimization/fitting": "The study employed decision tree and random forest methodologies due to the large number of potential predictor variables (129) and the possibility of complex interactions among predictors. This approach was chosen to handle the high-dimensional data and to capture intricate relationships that might not be apparent through traditional regression methods.\n\nTo address the potential issue of overfitting, given the large number of predictors relative to the number of training points, several strategies were implemented. First, the random forest model was developed using a balanced sampling approach, where trees were constructed using 40 observations from the serious fall injury group and 40 observations from the no serious fall injury group. This balancing act helped to ensure that the model did not become overly specialized to the training data, thereby reducing the risk of overfitting. Additionally, the use of random forests inherently provides a form of regularization by averaging the predictions of multiple decision trees, which helps to mitigate overfitting.\n\nTo rule out underfitting, the models were evaluated on both training and testing datasets. The performance metrics, including accuracy, sensitivity, specificity, and the area under the receiver operating curve (AUC), were calculated for both datasets. The models were developed using a training dataset and then evaluated on a separate testing dataset to ensure that they generalized well to unseen data. This split-sampling approach helped to validate that the models were not too simplistic and could capture the underlying patterns in the data.\n\nFurthermore, the decision tree model was developed using the CART (Classification and Regression Trees) method with default parameter values and no pruning. This approach allowed the tree to grow fully, capturing all possible interactions in the data, which helped to avoid underfitting. The random forest model, on the other hand, used default parameters but with the specified balancing of observations, ensuring that the model was complex enough to capture the necessary patterns without becoming too simplistic.\n\nIn summary, the study carefully managed the balance between overfitting and underfitting by using balanced sampling, evaluating on separate datasets, and employing methodologies that inherently provide regularization and capture complex interactions.",
  "optimization/regularization": "In our study, we employed machine learning methods, specifically decision tree and random forest, to develop prediction models for serious fall injury in older adults. These methods inherently include regularization techniques to prevent overfitting.\n\nDecision trees, by nature, can become complex and overfit the training data. To mitigate this, we used the random forest method, which is an ensemble of decision trees. Random forests help to reduce overfitting by averaging the results of multiple trees, each trained on a different subset of the data. This ensemble approach provides a more robust and generalizable model.\n\nAdditionally, in the random forest implementation, we specified that trees should be developed using a balanced number of observations from both the serious fall injury group and the no serious fall injury group. This balancing act helps to ensure that the model does not become biased towards the majority class, further aiding in the prevention of overfitting.\n\nMoreover, we imputed missing data using the random forest imputation method, which leverages the predictive power of the random forest algorithm to estimate missing values. This approach helps to maintain the integrity of the dataset and ensures that the model is trained on a complete and consistent set of predictors.\n\nIn summary, the use of random forests as our primary machine learning method, along with balanced sampling and robust imputation techniques, served as effective regularization methods to prevent overfitting in our prediction models.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the manuscript. Specifically, we detailed the parameters for the decision tree model, which was developed using the R package rpart with default settings and no pruning. For the random forest model, we used the R package randomForest with default parameters, except that we specified the use of 40 observations from the serious fall injury group and 40 observations from the no serious fall injury group to balance sensitivity and specificity.\n\nThe optimization schedule and model files are not explicitly provided in the manuscript, as the focus was on describing the methodology and results rather than sharing the raw model files. However, the steps taken to develop and evaluate the models are thoroughly documented, allowing for reproducibility. The R packages and specific functions used are mentioned, which include rpart for decision trees and randomForest for random forests. Additionally, we used the caret package for calculating confidence intervals and the cvAUC package for AUC calculations.\n\nRegarding the availability and licensing of the reported configurations and parameters, the R packages used are open-source and freely available. Researchers can access these packages through the Comprehensive R Archive Network (CRAN). The specific configurations and parameters described in the manuscript can be implemented using these packages without any licensing restrictions. This ensures that other researchers can reproduce our analysis and build upon our findings.",
  "model/interpretability": "The models discussed in this publication are not black-box models. Both decision tree and random forest methods offer a high degree of interpretability, making them suitable for medical prediction modeling where understanding the associations between predictors and outcomes is crucial.\n\nDecision trees are particularly transparent. They produce models that resemble flow charts, where predictors are split into subgroups based on specified algorithms. For instance, a decision tree might split participants based on age (e.g., age \u2265 70 and age < 70) and reaction time, guiding the user through a series of branches to reach an outcome prediction. This structure allows for easy visualization and understanding of how predictions are made.\n\nRandom forests, while more complex, also provide interpretability. They consist of multiple decision trees, and the predictions from each tree are aggregated. After the random forest is constructed, variable importance measures can be used to compare predictors and indicate which are most contributory to the outcome. Partial dependence plots can further illustrate the relationship between a predictor and the outcome, showing how changes in a predictor's value affect the probability of the outcome.\n\nBoth methods allow for the assessment of the relative importance of predictors and the direction and strength of their association with the outcome. This transparency is advantageous in medical fields, where understanding the underlying factors contributing to predictions is as important as the predictions themselves.",
  "model/output": "The model developed in this study is a classification model. The objective was to predict serious fall injuries among older adults, which is a categorical outcome. The decision tree and random forest methodologies used are supervised learning techniques that are well-suited for classification tasks. The model outputs a prediction of whether an individual will experience a serious fall injury or not. Evaluation metrics such as accuracy, sensitivity, specificity, and the area under the receiver operating curve (AUC) were used to assess the performance of the classification models. These metrics are commonly used in classification problems to evaluate the model's ability to correctly predict the outcome categories.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models developed in this study is not publicly released. However, the software used for analysis was RStudio, version 0.99.879. The specific R packages utilized include rpart for the decision tree model and randomForest for the random forest model. Additionally, the tableone package was used for descriptive statistics, and caret and cvAUC packages were employed for evaluating model performance. These packages are available in the Comprehensive R Archive Network (CRAN) and can be installed and used by other researchers to reproduce the analysis. The methods and parameters used for model development are detailed in the manuscript, allowing for reproducibility. The R packages used are open-source and freely available under the GNU General Public License.",
  "evaluation/method": "The evaluation of the prediction models involved several key metrics to assess their performance. We split the dataset into training and testing sets to develop and evaluate the models, respectively. The training set consisted of 818 observations, while the testing set had 817 observations. This split allowed us to train the models on one subset of the data and then evaluate their performance on an independent subset.\n\nWe evaluated the models using overall accuracy, sensitivity, specificity, and the area under the receiver operating curve (AUC). Accuracy measures the proportion of correct predictions overall. Sensitivity, also known as recall, measures the proportion of correct predictions among those who experienced a serious fall injury. Specificity measures the proportion of correct predictions among those who did not experience a serious fall injury. AUC provides a single scalar value that represents the ability of the model to distinguish between the two classes (serious fall injury vs. no serious fall injury).\n\nTo ensure the reliability of these metrics, we calculated 95% exact Binomial confidence intervals for accuracy, sensitivity, and specificity using the `confusionMatrix()` function from the R package `caret`. For AUC, we used the R package `cvAUC` to calculate the AUC and corresponding 95% confidence intervals.\n\nThe performance of the models was compared between the training and testing datasets. Generally, the performance statistics for the testing dataset were worse than those for the training dataset, highlighting the importance of evaluating models on an independent testing set. The random forest model demonstrated higher testing dataset AUC (0.66) compared to the decision tree model (0.54). This comparison was visually represented through ROC plots, where the ROC line for the random forest extended farther into the upper left quadrant, indicating better performance.\n\nIn summary, the evaluation method involved a rigorous assessment using multiple metrics and confidence intervals, ensuring that the models were thoroughly tested and validated on independent data. This approach provides a comprehensive understanding of the models' performance and their potential for real-world application.",
  "evaluation/measure": "In our study, we evaluated the performance of our prediction models using several key metrics to ensure a comprehensive assessment. We reported the overall accuracy, which is the proportion of correct predictions made by the model. This metric provides a general sense of how well the model performs across all predictions.\n\nWe also calculated sensitivity, which is the proportion of correct predictions among those who experienced a serious fall injury. This metric is crucial for understanding how well the model identifies true positive cases.\n\nSpecificity, the proportion of correct predictions among those who did not experience a serious fall injury, was another important metric. This helps in evaluating the model's ability to correctly identify true negative cases.\n\nAdditionally, we used the Area Under the Receiver Operating Curve (AUC), a metric that evaluates both sensitivity and specificity. AUC provides a single value that summarizes the model's performance across all possible classification thresholds. Higher AUC values indicate better model performance.\n\nTo provide a sense of the uncertainty in our estimates, we calculated 95% exact Binomial confidence intervals for accuracy, sensitivity, and specificity. These intervals help readers understand the range within which the true values of these metrics are likely to fall.\n\nFor AUC, we also reported corresponding 95% confidence intervals. These intervals give an idea of the precision of the AUC estimate and help in comparing the performance of different models.\n\nThe metrics we used are representative of those commonly reported in the literature for evaluating prediction models. Accuracy, sensitivity, specificity, and AUC are standard metrics that allow for comparisons across different studies and models. By including confidence intervals, we ensure that our results are transparent and that readers can assess the reliability of our findings.\n\nIn summary, our set of performance metrics is comprehensive and aligns with established practices in the field. This approach allows for a thorough evaluation of our prediction models and facilitates comparisons with other studies.",
  "evaluation/comparison": "A comparison to simpler baselines was performed. Decision tree and random forest methodologies were employed due to the large number of potential predictor variables and the possibility of complex interactions among predictors. The decision tree model was developed using the R package rpart with default parameter values and no pruning. The random forest model was developed using the R package randomForest with default parameters, except that trees were developed using 40 observations from the serious fall injury group and 40 observations from the no serious fall injury group. This approach was taken to balance the sensitivity and specificity of model predictions.\n\nThe performance of these models was evaluated using metrics such as overall accuracy, sensitivity, specificity, and the area under the receiver operating curve (AUC). The random forest model demonstrated higher testing dataset AUC compared to the decision tree, indicating better performance. The comparison is visually represented in ROC plots, where the random forest's ROC line extends farther into the upper left quadrant compared to the decision tree's line.\n\nThe decision tree model had higher testing accuracy (85%) compared to the random forest model (73%), but the random forest model showed greater sensitivity (40%) compared to the decision tree (8%). This suggests that while the decision tree may correctly identify more non-fallers, the random forest is better at identifying fallers, which is crucial for predicting serious fall injuries.\n\nIn summary, the comparison between the decision tree and random forest models highlights the strengths and weaknesses of each method in the context of predicting serious fall injuries. The random forest model's ability to handle complex interactions and provide a ranking of predictor importance makes it a more robust choice for this specific application.",
  "evaluation/confidence": "The evaluation of our prediction models included a comprehensive assessment of performance metrics, ensuring that the results are robust and reliable. We calculated 95% exact Binomial confidence intervals for key metrics such as accuracy, sensitivity, and specificity. These intervals were determined using the confusionMatrix() function from the R package caret. Additionally, we computed the Area Under the Receiver Operating Curve (AUC) along with its corresponding 95% confidence intervals using the R package cvAUC. This approach provides a clear understanding of the uncertainty in our estimates, allowing readers to compare the measures and determine which ones have overlapping distributions.\n\nThe statistical significance of our results was carefully considered. We used p-values less than 0.05 to determine significance, ensuring that our findings are not due to random chance. For instance, the random forest model demonstrated a higher testing dataset AUC (0.66) compared to the decision tree (0.54), indicating better performance. This difference is visually supported by the ROC plots, where the random forest model's line extends farther into the upper left quadrant, signifying improved sensitivity and specificity.\n\nMoreover, the performance statistics for the testing dataset were generally worse than those for the training dataset, highlighting the importance of evaluating models using a separate testing set. This approach helps in assessing the generalizability and robustness of our models. The inclusion of confidence intervals and the use of statistical significance thresholds ensure that our claims about the superiority of certain methods are well-founded and reliable.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being publicly available. The evaluation metrics discussed include overall accuracy, sensitivity, specificity, and the area under the receiver operating curve (AUC). These metrics were calculated using specific R packages such as caret and cvAUC. The results of these evaluations are presented in tables and figures within the publication, but there is no indication that the raw data files used for these evaluations are publicly released. Therefore, it is not clear whether the raw evaluation files are accessible to the public or under what license they might be distributed."
}