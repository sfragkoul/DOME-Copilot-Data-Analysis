{
  "publication/title": "Prediction of Prostate Cancer From Routine Laboratory Markers With Automated Machine Learning.",
  "publication/authors": "Sat\u0131r A, \u00dcst\u00fcnda\u011f Y, Ye\u015fil MR, Huysal K",
  "publication/journal": "Journal of clinical laboratory analysis",
  "publication/year": "2025",
  "publication/pmid": "39828871",
  "publication/pmcid": "PMC11821719",
  "publication/doi": "10.1002/jcla.25143",
  "publication/tags": "- Prostate Cancer\n- Machine Learning\n- Prostate Biopsy\n- Gradient Boosting Machines\n- AutoML\n- Predictive Modeling\n- Laboratory Parameters\n- Diagnostic Accuracy\n- SHAP Analysis\n- Clinical Decision Support",
  "dataset/provenance": "The dataset used in this study was sourced from a single-center retrospective collection. The data was gathered from a hospital, focusing on patients who underwent prostate biopsies. The total number of data points included 737 patients, with 532 patients in the training group and 205 patients in the testing group. The dataset comprised routine laboratory data, including parameters such as PSA levels, free PSA, the ratio of free to total PSA, hemoglobin, neutrophils, age, platelets, the neutrophil-to-lymphocyte ratio, glucose, the platelet-to-lymphocyte ratio, and lymphocytes. These parameters were selected to build and evaluate machine learning models for predicting the necessity of prostate biopsies.\n\nThe dataset did not include additional clinical variables such as family history, pro-PSA levels, lower urinary tract symptoms, or prostate volume, which could potentially increase the accuracy of the models. The study aimed to use only routine laboratory data to make the model more applicable in standard clinical settings. The retrospective nature of the data collection means that the data were not pre-collected according to the specific requirements of the study, leading to common issues with missing data that can affect the final prediction results of machine learning studies.",
  "dataset/splits": "The dataset was divided into two primary splits: a training set and a testing set. The training set consisted of 532 patients, which accounted for 70% of the total dataset. The testing set included 205 patients, making up the remaining 30%. This split was done randomly to ensure that the models could be trained and evaluated effectively. Additionally, during the model training process, a 10-fold cross-validation technique was employed. This involved further dividing the training data into 10 subsets, where the model was trained on 9 subsets and validated on the remaining 1 subset. This process was repeated 10 times, with each subset serving as the validation set once, ensuring a comprehensive evaluation of the model's performance.",
  "dataset/redundancy": "The dataset used in this study was split into training and testing groups. Specifically, the data was randomly divided, with 70% allocated to the training group and 30% to the testing group. This resulted in 532 patients in the training group and 205 patients in the testing group.\n\nThe training and testing groups were collected retrospectively from the same hospital. However, due to privacy issues, there was no external validation with an independent dataset from outside the institution. This lack of external validation raises concerns about the generalizability of the findings to other populations and clinical settings.\n\nTo ensure the robustness of the model, ten-fold cross-validation was employed during the training phase. This technique helps in assessing the model's performance and generalizability by dividing the training data into ten subsets, training the model on nine subsets, and validating it on the remaining subset. This process is repeated ten times, with each subset serving as the validation set once.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets in the context of prostate cancer risk assessment. The use of routine laboratory data and the inclusion of a diverse set of parameters, such as PSA, fPSA, and various hematological markers, aligns with the methodologies used in other studies. This approach ensures that the model can be applied in routine clinical settings without the need for additional specialized tests or biomarkers.",
  "dataset/availability": "Not applicable.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is AutoML, specifically implemented through the H2O package on the H2O.ai platform. This approach leverages multiple common machine learning algorithms, including gradient boosting machines (GBMs), XGBoost GBMs, generalized linear models, deep neural networks, extreme randomized trees, and distributed random forests. The H2O AutoML algorithm fine-tunes various models simultaneously across these categories, automatically selecting applicable algorithms and integrating them into multiple ensemble models.\n\nThe AutoML approach is not entirely new but represents a relatively recent advancement in machine learning technology. It is particularly valuable in clinical settings where the interpretation and applicability of models are crucial. The choice to publish in a clinical laboratory analysis journal rather than a machine-learning journal is driven by the focus on the clinical applicability and impact of the model. The study aims to demonstrate how machine learning can optimize prostate cancer risk assessments using routine laboratory data, thereby enhancing clinical diagnostic precision. This focus aligns more closely with the interests and needs of the clinical community, making it a suitable fit for a journal that specializes in clinical laboratory analysis.",
  "optimization/meta": "The model utilized in this study does not function as a meta-predictor. Instead, it relies on a single machine-learning algorithm, specifically the Gradient Boosting Machine (GBM). This algorithm was selected based on its superior performance in distinguishing classes within the dataset, as evidenced by its highest AUC score.\n\nThe GBM technique constructs multiple decision trees sequentially, with each tree aiming to correct the errors of its predecessors. This approach effectively manages complex data and was applied to identify the most critical features impacting model performance.\n\nThe features identified as most influential were PSA, fPSA, and the fPSA/PSA ratio. These features were determined using the Boruta algorithm in the initial phase of the study. Subsequent phases involved the stepwise addition of other important features to evaluate changes in prediction accuracy.\n\nThe training and testing datasets were derived from the same hospital's retrospective data, with a 70% training and 30% testing split. This ensures that the training data is independent of the testing data, maintaining the integrity of the model's evaluation process. However, it is important to note that the lack of external validation from an independent dataset may raise concerns about the generalizability of the findings to other populations and clinical settings.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for optimizing the performance of the machine-learning algorithms. Prostate biopsy results were encoded as a categorical variable, with malignant cases coded as 1 and benign cases as 0. This binary encoding facilitated the classification tasks in our models.\n\nContinuous variables in the dataset were expressed using medians and 25th\u201375th quartiles or means and standard deviations, depending on their distribution. For comparisons between the prostate cancer (PCa) and benign groups, unpaired t-tests or nonparametric Mann\u2013Whitney U tests were employed, with a significance level set at p < 0.05.\n\nMissing values in the dataset were handled by imputing them with the mean values of the relevant columns. This approach ensured that the dataset was complete and ready for analysis. All numeric variables were then scaled to a range of 0\u20131 using the Min\u2013Max normalization method. This scaling was essential for enhancing the model's performance by eliminating biases caused by features with varying scales.\n\nThe dataset was randomly divided into training and testing groups, with 70% of the data used for training and 30% reserved for testing. This split resulted in 532 patients in the training group and 205 patients in the testing group. The training process involved ten-fold cross-validation to ensure the robustness and generalizability of the models.\n\nDuring the model training, a time constraint of 100 seconds was set, and the maximum number of models was limited to 20. This constraint was sufficient for building efficient machine-learning models. The H2O AutoML algorithm was used to fine-tune various models simultaneously across multiple categories, including gradient boosting machines (GBMs), XGBoost GBMs, generalized linear models, deep neural networks, extreme randomized trees, and distributed random forests. The algorithm automatically selected applicable algorithms and integrated them into multiple ensemble models.\n\nThe performance of the models was evaluated using metrics such as AUC, AUCPR, log loss, mean error per class, RMSE, and MSE. The best-performing model was selected based on its performance on the independent test set, and the 95% confidence intervals of each metric were calculated to evaluate the reliability of the model's performance.",
  "optimization/parameters": "In our study, we utilized a gradient boosting machine (GBM) model that incorporated 11 parameters to predict prostate biopsy results. These parameters included total prostate-specific antigen (PSA), free PSA, the ratio of free to total PSA, hemoglobin, neutrophils, age, platelets, the neutrophil-to-lymphocyte ratio, glucose, the platelet-to-lymphocyte ratio, and lymphocytes.\n\nThe selection of these parameters was a systematic process. Initially, we employed the Boruta algorithm to identify the most influential features. This algorithm highlighted PSA, free PSA, and the free-to-total PSA ratio as the three most critical features. Subsequently, we applied a stepwise feature addition method, gradually incorporating other important features into the model. This approach allowed us to evaluate the impact of each additional parameter on the model's prediction accuracy. Through this iterative process, we determined that the inclusion of the aforementioned 11 parameters yielded the best performance metrics, including an area under the curve (AUC) of 0.72 in the test dataset. This model demonstrated superior specificity, positive predictive value, and accuracy compared to models using fewer parameters.",
  "optimization/features": "In the optimization process of our study, we utilized a total of 11 features as input for our final model. These features were selected through a rigorous feature selection process using the Boruta algorithm. The Boruta algorithm is designed to identify the most relevant features that significantly impact the model's performance.\n\nThe feature selection was performed exclusively using the training set to ensure that the model's performance on the test set remained unbiased. This approach helps in preventing data leakage and ensures that the model generalizes well to new, unseen data. The selected features included total PSA, free PSA, the ratio of free to total PSA, hemoglobin, neutrophils, age, platelets, the neutrophil-to-lymphocyte ratio, glucose, the platelet-to-lymphocyte ratio, and lymphocytes. These features were chosen because they demonstrated the highest importance in distinguishing between positive and negative biopsy results.",
  "optimization/fitting": "The dataset used in this study consisted of 737 patients, with 532 in the training group and 205 in the testing group. The number of parameters considered for the model was 11, which is relatively small compared to the number of training points. This helps to mitigate the risk of overfitting, as having more training samples than parameters generally leads to better generalization.\n\nTo further ensure that overfitting was not an issue, several techniques were employed. First, the data was randomly divided into training and testing sets, which helps in evaluating the model's performance on unseen data. Second, 10-fold cross-validation was used during the model's training. This technique involves splitting the training data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. This method provides a robust estimate of the model's performance and helps in detecting overfitting.\n\nAdditionally, the H2O AutoML algorithm was used, which automatically selects applicable algorithms and integrates them into multiple ensemble models. This approach helps in reducing overfitting by combining the strengths of different models. The algorithm also performs hyperparameter tuning using the grid search method, which helps in finding the optimal parameters for the models.\n\nTo rule out underfitting, the model's performance was evaluated using several metrics, including AUC, AUCPR, log loss, accuracy, F1 score, sensitivity, specificity, PPV, and NPV. The best-performing model, the GBM 11-parameter model, showed good performance on these metrics, indicating that the model was able to capture the underlying patterns in the data.\n\nFurthermore, the Shapley Additive Explanations (SHAP) analysis was used to interpret the model's predictions. This method allows for an understanding of the contributions of each feature to the model's predictions, providing insights into the model's decision-making process. The SHAP analysis showed that many parameters had high SHAP values, indicating that these parameters were effective in distinguishing between negative and positive biopsy results.\n\nIn summary, the study employed several techniques to ensure that the model was neither overfitting nor underfitting. The use of a large training dataset, cross-validation, ensemble modeling, and hyperparameter tuning helped in building a robust model. The evaluation of the model's performance using multiple metrics and the interpretation of the model's predictions using SHAP analysis provided further confidence in the model's validity.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our machine learning models. One of the primary methods used was cross-validation, specifically 10-fold cross-validation. This technique involves dividing the dataset into 10 subsets, training the model on 9 of these subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. This approach helps to ensure that the model generalizes well to unseen data by providing a more comprehensive evaluation of its performance.\n\nAdditionally, we utilized the H2O AutoML framework, which automatically selects and tunes multiple machine learning algorithms. This framework includes built-in mechanisms for regularization, such as L1 and L2 regularization, which help to prevent overfitting by penalizing large coefficients in the model. By incorporating these regularization techniques, we were able to build models that are less likely to overfit the training data and more likely to perform well on new, unseen data.\n\nFurthermore, we implemented feature scaling using the Min-Max normalization method. This technique scales all numeric variables to a range of 0\u20131, which helps to eliminate biases caused by features with varying scales. By standardizing the features, we ensured that the model's performance was not unduly influenced by the magnitude of the input variables, thereby reducing the risk of overfitting.\n\nAnother important aspect of our approach was the use of a time constraint of 100 seconds for training the models. This constraint limited the complexity of the models, preventing them from becoming too intricate and overfitting the training data. By setting this time limit, we ensured that the models were trained efficiently and were less likely to capture noise in the data.\n\nIn summary, our study employed several overfitting prevention techniques, including 10-fold cross-validation, regularization methods within the H2O AutoML framework, feature scaling using Min-Max normalization, and time constraints for model training. These techniques collectively helped to build robust and generalizable machine learning models for predicting prostate biopsy results.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available through the H2O AutoML platform. This platform automatically selects and tunes various machine learning algorithms, including gradient boosting machines, XGBoost, generalized linear models, deep neural networks, extreme randomized trees, and distributed random forests. The specific configurations and schedules are determined by the platform's internal processes, which include grid search methods for hyperparameter optimization.\n\nThe model files generated during the optimization process are not explicitly detailed in our publication, but the performance metrics and configurations of the top-performing models are summarized in tables. These tables provide insights into the AUC, log loss, AUCPR, mean error per class, RMSE, and MSE for the top models, which can be used to replicate or build upon our findings.\n\nRegarding the availability and licensing of the tools and data used, the H2O.ai platform is open-source software, which means it is freely available for use and modification under the terms of its open-source license. This allows other researchers to access the same tools and methodologies we employed, facilitating reproducibility and further research.\n\nIn summary, while the exact model files are not provided, the hyper-parameter configurations, optimization schedules, and performance metrics are documented. The use of open-source software ensures that the methods and tools are accessible to the research community.",
  "model/interpretability": "The model developed in this study is not a black-box model. To ensure interpretability, we employed the Shapley Additive Explanations (SHAP) analysis. This method allows for a feature-based interpretation of the machine learning model's predictions, providing insights into how each feature contributes to the final output. By using SHAP, we can understand the impact of individual features such as PSA, fPSA, and other laboratory parameters on the model's decisions.\n\nThe SHAP values indicate the importance of various parameters in distinguishing between negative and positive biopsy results. For instance, many parameters showed high SHAP values, suggesting their effectiveness in predicting biopsy outcomes. This interpretability is crucial for clinicians, as it helps them understand the rationale behind the model's predictions and increases the clinical applicability of the model.\n\nAdditionally, the Gradient Boosting Machine (GBM) ensemble technique used in this study constructs multiple decision trees sequentially, each aiming to correct the errors of its predecessors. This approach effectively manages complex data and provides a transparent view of how different features interact to influence the model's predictions. The use of routine hematological and laboratory parameters further enhances the interpretability, as these are familiar and commonly used in clinical settings.\n\nIn summary, the model's interpretability is achieved through the use of SHAP analysis and the transparent nature of the GBM ensemble technique. This ensures that the model's predictions are not only accurate but also understandable and applicable in clinical practice.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict whether a patient requires a prostate biopsy by classifying the results as malignant or benign. The dataset used for training and testing the model includes prostate biopsy results coded as a categorical variable, with 1 indicating malignant cases and 0 indicating benign cases. This coding facilitates the model's classification task.\n\nThe model's performance was evaluated using various metrics suitable for classification problems, such as the area under the curve (AUC), sensitivity, specificity, accuracy, positive predictive value (PPV), and negative predictive value (NPV). These metrics provide a comprehensive assessment of the model's ability to distinguish between the two classes.\n\nThe gradient boosting machine (GBM) model, which achieved the highest AUC score, was used for the explainability analysis. This model was trained using a combination of 11 parameters, including total PSA, free PSA, the ratio of free to total PSA, hemoglobin, neutrophils, age, platelets, the neutrophil-to-lymphocyte ratio, glucose, the platelet-to-lymphocyte ratio, and lymphocytes. The inclusion of these parameters improved the model's performance and interpretability, making it a reliable tool for predicting the need for a prostate biopsy.",
  "model/duration": "The model training process was constrained by a time limit of 100 seconds. This duration was deemed sufficient after testing various time settings, ensuring that efficient machine learning models could be built within this constraint. The AutoML approach trained and tested 20 machine learning algorithms simultaneously, completing the process within the specified time frame. This time limit was set to accommodate 100 repeated iterations, allowing for thorough model evaluation and selection. The use of a time constraint helped in managing computational resources effectively while still achieving robust model performance.",
  "model/availability": "The software used in this study is the H2O package, which is installed on the H2O.ai platform. This platform is open-source software that includes many common machine learning algorithms. The H2O AutoML algorithm was employed to fine-tune various models simultaneously across multiple categories, including gradient boosting machines (GBMs), XGBoost GBMs, generalized linear models, deep neural networks, extreme randomized trees, and distributed random forests. The H2O package automatically selects applicable algorithms and integrates them into multiple ensemble models.\n\nThe H2O.ai platform is publicly available and can be accessed via its website. The software is released under an open-source license, allowing users to utilize it for their own machine learning tasks. This platform provides a method to run the algorithms through its interface, enabling users to train and test models efficiently. The open-source nature of the H2O package ensures that it is accessible to a wide range of users, promoting reproducibility and collaboration in the scientific community.",
  "evaluation/method": "The evaluation of the method involved several rigorous steps to ensure the robustness and reliability of the models. Initially, the dataset was randomly divided into training and testing groups, with 70% of the data used for training and 30% reserved for testing. This split resulted in 532 patients in the training group and 205 patients in the testing group.\n\nDuring the training phase, ten-fold cross-validation was employed to assess the model's performance. This technique involved dividing the training data into ten subsets, training the model on nine of these subsets, and validating it on the remaining subset. This process was repeated ten times, with each subset serving as the validation set once. The results from these iterations were then averaged to provide a comprehensive evaluation of the model's performance.\n\nThe models were trained using a variety of algorithms, including gradient boosting machines, XGBoost, generalized linear models, deep neural networks, extreme randomized trees, and distributed random forests. The H2O AutoML platform was utilized to fine-tune these models simultaneously, with a time constraint of 100 seconds and a maximum of 20 models trained and tested. This approach allowed for the selection of the most accurate method.\n\nThe performance of the models was measured using several metrics, including the area under the curve (AUC), area under the precision-recall curve (AUCPR), log loss, mean error per class, root mean squared error (RMSE), and mean squared error (MSE). These metrics were calculated on the test dataset to provide an unbiased evaluation of the models' predictive capabilities.\n\nAdditionally, the Shapley Additive Explanations (SHAP) analysis was applied to interpret the predictions of the machine learning models. This method helped in understanding the contribution of each feature to the model's predictions, thereby enhancing the interpretability and clinical applicability of the models.\n\nThe best-performing model was selected based on its performance on the independent test set. The 95% confidence intervals of each metric were calculated using the performance measurements on the independent test dataset, which helped in evaluating the reliability of the model's performance.\n\nIn summary, the evaluation method involved a combination of cross-validation, independent testing, and interpretability analysis to ensure that the models were robust, reliable, and clinically applicable.",
  "evaluation/measure": "In our study, we evaluated the performance of our machine learning models using a comprehensive set of metrics to ensure a thorough assessment. The primary metric used for ranking models was the Area Under the Curve (AUC), which is a critical measure for evaluating the performance of classification models. This metric provides a single scalar value that represents the ability of the model to distinguish between the positive and negative classes.\n\nIn addition to AUC, we reported several other key performance metrics. These include sensitivity, specificity, accuracy, precision-recall area under the curve (AUCPR), F1 score, log loss, positive predictive value (PPV), and negative predictive value (NPV). Sensitivity, also known as recall, measures the proportion of actual positives that are correctly identified by the model. Specificity, on the other hand, measures the proportion of actual negatives that are correctly identified. Accuracy provides an overall measure of the model's correctness, while the F1 score is the harmonic mean of precision and recall, offering a balance between the two.\n\nThe AUCPR metric is particularly useful in imbalanced datasets, as it focuses on the performance of the model in terms of precision and recall. Log loss, or binary cross-entropy, measures the performance of a classification model where the prediction input is a probability value between 0 and 1. PPV and NPV provide insights into the likelihood of a positive or negative prediction being correct, respectively.\n\nThese metrics collectively offer a robust evaluation framework, ensuring that our models are assessed from multiple angles. The inclusion of these metrics aligns with common practices in the literature, providing a representative and comprehensive evaluation of our models' performance.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on developing and evaluating models using a clinical laboratory dataset coupled with AutoML-based algorithms to discern patients requiring a prostate biopsy. We utilized the H2O AutoML platform, which fine-tunes various models simultaneously across multiple categories, including gradient boosting machines (GBMs), XGBoost GBMs, generalized linear models, deep neural networks, extreme randomized trees, and distributed random forests.\n\nWe ranked models according to their AUC scores, the main metric for evaluating their performance. The GBM model achieved the highest AUC score, making it the best-performing model for distinguishing the classes in our dataset. We also compared the performance of models built with different parameters, including PSA alone, fPSA/PSA alone, and a combination of 11 parameters. The 11-parameter model, which included total PSA, free PSA, fPSA/tPSA, Hb, neutrophil, platelet, NLR, glucose, PLR, lymphocytes, and age, showed superior performance with an AUC of 0.72 in the test dataset.\n\nRegarding simpler baselines, we evaluated the performance of models using individual features such as PSA alone and fPSA/PSA alone. The GBM model using PSA alone had an AUC of 0.64, while the model using fPSA/PSA alone had an AUC of 0.68. These comparisons highlighted the incremental improvement in performance when additional parameters were included. The 11-parameter model demonstrated significantly better specificity and accuracy compared to using PSA alone, indicating the value of incorporating multiple routine hematological and laboratory parameters.",
  "evaluation/confidence": "The evaluation of our models included a thorough assessment of performance metrics, with a focus on ensuring statistical significance and reliability. To evaluate the confidence in our results, we calculated the 95% confidence intervals for each metric using the performance measurements on the independent test dataset. These confidence intervals are crucial for understanding the reliability of the model's performance on unseen data. They provide a range within which the true performance metric is expected to lie, with 95% confidence.\n\nThe statistical significance of our findings was determined using established thresholds. For instance, a p-value of less than 0.05 was considered statistically significant in our comparisons. This ensures that the observed differences in performance metrics are unlikely to have occurred by chance.\n\nIn our study, we compared the performance of various models, including those using single parameters like PSA and fPSA/PSA, as well as more complex models incorporating multiple parameters. The gradient boosting machine (GBM) model, which utilized 11 parameters, demonstrated superior performance with an AUC of 0.72 in the test dataset. This model's performance was significantly better than that of the PSA alone model, which had an AUC of 0.64. The confidence intervals for these metrics further support the statistical significance of these results, indicating that the improvements observed are reliable and not due to random variation.\n\nAdditionally, we employed cross-validation techniques, specifically 10-fold cross-validation, to ensure that our models were robust and generalizable. This method involves dividing the data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. The results are then averaged to provide a more accurate estimate of the model's performance.\n\nIn summary, the performance metrics in our study are accompanied by confidence intervals, and the results are statistically significant. This provides a strong basis for claiming that our method is superior to others and baselines, ensuring that the findings are reliable and generalizable.",
  "evaluation/availability": "Not applicable"
}