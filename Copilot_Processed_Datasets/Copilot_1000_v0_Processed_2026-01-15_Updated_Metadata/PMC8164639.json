{
  "publication/title": "Predicting Posttraumatic Stress Disorder Among Survivors of Recent Interpersonal Violence.",
  "publication/authors": "Morris MC, Sanchez-S\u00e1ez F, Bailey B, Hellman N, Williams A, Schumacher JA, Rao U",
  "publication/journal": "Journal of interpersonal violence",
  "publication/year": "2022",
  "publication/pmid": "33256508",
  "publication/pmcid": "PMC8164639",
  "publication/doi": "10.1177/0886260520978195",
  "publication/tags": "- PTSD\n- Machine Learning\n- Interpersonal Violence\n- Trauma Survivors\n- Predictive Modeling\n- Cognitive Behavioral Interventions\n- Posttraumatic Stress Symptom Severity\n- Biopsychosocial Factors\n- Multilevel Modeling\n- Coping Strategies",
  "dataset/provenance": "The dataset used in this study was sourced from young adult women who had experienced interpersonal violence within the past three months. The participants were recruited through various means, including online advertisements, research participant registries, local agencies coordinating services for survivors of domestic violence and sexual assault, and a team of nurse practitioners providing medical legal exams to rape survivors in a local hospital. The study included 58 participants, aged 18 to 30.\n\nScreening for interpersonal violence was conducted over the phone and confirmed at baseline using the Life Events Checklist (LEC). The screening and assessment measures were based on the Diagnostic and Statistical Manual of Mental Disorders, 4th Edition (DSM-IV), as data collection began before the introduction of DSM-5 criteria.\n\nExclusion criteria for the study included current or past bipolar or psychotic disorder, current substance use disorder, current major depressive disorder (MDD) or PTSD resulting from a traumatic event that occurred prior to the three-month window, current generalized anxiety disorder (GAD), current Panic Disorder, current Agoraphobia, stress and psychiatric symptoms, trauma characteristics, and disability.\n\nThe dataset included a wide range of predictors, such as sociodemographic factors, mental health symptoms, medical conditions, psychosocial factors, and trauma characteristics. These predictors were used in both multilevel modeling and machine learning approaches to evaluate biopsychosocial factors contributing to the risk of PTSD following interpersonal violence.\n\nThe study aimed to identify interpersonal trauma survivors at high risk for developing PTSD using a broad array of theory-driven cognitive and neurobiological factors assessed at baseline. The dataset was used to explore the most relevant predictors for developing PTSD over a 6-month follow-up period. The findings from this study contribute to the existing literature on PTSD prediction and highlight the importance of considering multiple factors in assessing risk.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is Gradient Boosting Machine (GBM). This method is not new; it is a well-established technique in the field of machine learning. GBM is a type of ensemble learning algorithm that builds prediction models by assembling small decision trees. It is particularly effective for both regression and classification problems.\n\nThe reason this algorithm was not published in a machine-learning journal is that our focus was on applying it to predict posttraumatic stress symptom severity and PTSD onset, rather than on developing a new machine-learning technique. Our primary goal was to evaluate biopsychosocial factors following interpersonal violence that contribute to the risk for PTSD. We used GBM because it is well-suited to handle large, complex data structures with heterogeneous distributions, making it better-suited to PTSD prediction than general linear models.\n\nWe performed two GBM algorithms: one model predicting PTSD onset and another predicting PTSD symptom severity at 6-month follow-up. Both models included baseline posttraumatic stress symptom severity as a predictor. The performance of these GBM models was compared to a logistic regression model that included only baseline posttraumatic stress symptom severity. The results showed that the GBM models, which included a broad array of theory-driven features, performed somewhat better than the logistic regression model. However, due to the small sample size, these GBM algorithms were considered exploratory.",
  "optimization/meta": "The models employed in this study do not utilize data from other machine-learning algorithms as input. Instead, they rely on a comprehensive set of theory-driven features and baseline posttraumatic stress symptom severity. The primary machine learning method used is Gradient Boosting Machine (GBM), which assembles small decision trees to build prediction models for regression and classification problems. Two distinct GBM algorithms were developed: one for predicting PTSD onset and another for predicting PTSD symptom severity at the 6-month follow-up.\n\nThe study does not explicitly describe a meta-predictor approach, where the output of one machine-learning model serves as input to another. Instead, the focus is on using GBM to enhance predictive accuracy for posttraumatic stress symptom severity and PTSD diagnosis. The performance of these GBM models was compared to a logistic regression model that included only baseline posttraumatic stress symptom severity.\n\nRegarding the independence of training data, the study acknowledges the challenges posed by small sample sizes. To address potential overfitting, an optimism correction method was employed. This method uses bootstrapping models to calculate an optimism value, which is then subtracted from the original AUC. This approach helps to ensure that the models are not overly optimized to the specific dataset and can generalize better to future data. However, the study notes that the optimism-corrected AUCs should be interpreted with caution due to ongoing debates about their effectiveness in smaller samples.",
  "optimization/encoding": "For the machine-learning algorithms employed in this study, data encoding and preprocessing were crucial steps to ensure the models could effectively learn from the input features. The dataset included a wide range of predictors, encompassing demographic information, psychiatric symptoms, trauma characteristics, and disability measures. These features were carefully selected and encoded to facilitate the machine-learning processes.\n\nCategorical variables, such as the presence of current mental health conditions like major depressive disorder, generalized anxiety disorder, panic disorder, and agoraphobia, were encoded using one-hot encoding. This method converts categorical data into a binary format, allowing the machine-learning algorithms to interpret these variables accurately. Continuous variables, including stress levels, depressive symptoms, and trauma exposure, were standardized to have a mean of zero and a standard deviation of one. Standardization helps in ensuring that all features contribute equally to the model, preventing any single feature from dominating the learning process due to its scale.\n\nMissing values were handled through imputation techniques. For continuous variables, missing values were imputed using the mean of the respective variable. For categorical variables, the mode was used for imputation. This approach ensures that the dataset is complete and that the machine-learning algorithms can process it without encountering errors due to missing data.\n\nFeature selection was performed to identify the most relevant predictors for the machine-learning models. This involved using statistical methods to assess the importance of each feature in predicting PTSD onset and symptom severity. Features that showed significant predictive power were retained, while those with minimal impact were excluded to simplify the models and improve their performance.\n\nThe data was split into training and testing sets to evaluate the performance of the machine-learning models. Due to the small sample size, an optimism correction method was employed to address potential overfitting. This method uses bootstrapping to calculate an optimism value, which is then subtracted from the original area under the curve (AUC) to provide a more accurate estimate of the model's performance on unseen data.\n\nIn summary, the data encoding and preprocessing steps involved one-hot encoding for categorical variables, standardization for continuous variables, imputation for missing values, and feature selection to identify the most relevant predictors. These steps were essential in preparing the data for the machine-learning algorithms and ensuring robust and reliable model performance.",
  "optimization/parameters": "In our study, we utilized a comprehensive set of predictors to enhance the accuracy of our models. Specifically, we included a large number of features, totaling 137, in our machine learning models. This extensive array of predictors was chosen to capture a wide range of biopsychosocial factors that could potentially influence the development of PTSD.\n\nThe selection of these parameters was driven by theoretical considerations and previous research findings. We aimed to include variables that have been identified as relevant in the literature on PTSD prediction, such as demographic characteristics, mental health symptoms, trauma history, and coping strategies. By incorporating a broad spectrum of predictors, we sought to build robust models that could accurately identify individuals at high risk for developing PTSD.\n\nHowever, it is important to note that including a large number of features in machine learning models can lead to overfitting, where the model performs well on the current dataset but may not generalize well to new data. To address this issue, we employed optimism correction, a method that uses bootstrapping to calculate an optimism value, which is then subtracted from the original AUC. This approach helps to provide a more realistic estimate of the model's performance and mitigates the risk of overfitting.\n\nGiven the exploratory nature of our study and the relatively small sample size, the selection of parameters was considered preliminary. Future research with larger samples and more diverse populations is needed to validate these findings and refine the set of predictors used in PTSD prediction models.",
  "optimization/features": "The study utilized a comprehensive set of features to predict posttraumatic stress disorder (PTSD) onset and symptom severity. A total of 137 features were considered as input for the machine learning models. These features encompassed a wide range of biopsychosocial factors, including demographic information, psychiatric symptoms, trauma characteristics, stress levels, and various biological measures.\n\nFeature selection was performed to identify the most relevant predictors. The Gradient Boosting Machine (GBM) models were used to determine the key features that contributed significantly to the prediction of PTSD outcomes. The selection process involved assessing the influence of each feature on the model's predictive performance. For instance, features such as the age of first major depressive disorder (MDD) onset, household income, pre-stress cortisol levels, and baseline PTSD severity were found to be influential in predicting PTSD onsets.\n\nThe feature selection process was conducted using the available data, ensuring that the models were trained and validated on the same dataset. This approach helped in identifying the most relevant predictors while mitigating the risk of overfitting. The optimism correction method was employed to address potential overfitting issues, particularly given the small sample size. This method involved bootstrapping models to calculate an optimism value, which was then subtracted from the original Area Under the Curve (AUC) to provide a more accurate estimate of the model's performance.",
  "optimization/fitting": "The study employed Gradient Boosting Machine (GBM) methods to predict posttraumatic stress symptom severity and PTSD onset. These methods assemble small decision trees to build prediction models for regression and classification problems. Two GBM algorithms were performed: one predicting PTSD onset and another predicting PTSD symptom severity at 6-month follow-up. Both models included baseline posttraumatic stress symptom severity as a predictor.\n\nThe number of features (n = 137) in the machine learning models was indeed much larger than the number of training points (58 participants). This high ratio of features to participants could lead to overfitting, where the model performs well on the current dataset but poorly on future data. To address this, an optimism correction method was used. This method involves bootstrapping models to calculate an optimism value, which is then subtracted from the original AUC (Area Under the Curve). This approach helps to provide a more realistic estimate of the model's performance and mitigates the risk of overfitting.\n\nTo rule out under-fitting, the performance of the GBM models was compared to a logistic regression model that included only baseline posttraumatic stress symptom severity. The GBM models, which included a broader range of predictors, performed somewhat better than the logistic regression model. This comparison suggests that the GBM models were able to capture more of the underlying patterns in the data, indicating that under-fitting was not a significant issue.\n\nAdditionally, the study highlighted that the predictive accuracy of the CAPS-IV interview for identifying interpersonal violence survivors who will develop PTSD is high. This further supports the robustness of the models used, as they were able to leverage a comprehensive set of predictors to enhance predictive performance.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting, particularly given the small sample size and the large number of features used in our machine learning models. One of the primary methods we used was optimism correction. This technique involves using bootstrapping to calculate an optimism value, which is then subtracted from the original Area Under the Curve (AUC) value. This adjustment helps to provide a more realistic estimate of the model's performance on new, unseen data.\n\nAdditionally, we compared the performance of our Gradient Boosting Machine (GBM) models to a benchmark logistic regression model that included only baseline posttraumatic stress symptom severity. This comparison helped to ensure that the improvements in predictive accuracy were not merely due to overfitting but reflected genuine enhancements in model performance.\n\nWe also acknowledged the limitations of our sample size and the potential for overfitting when using a large number of features. To mitigate this, we considered our GBM algorithms as exploratory and emphasized the need for replication in larger samples. This cautious approach underscores the importance of validating our findings in more extensive datasets to confirm the robustness of our models.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models used in this study include both machine learning (ML) approaches and multilevel modeling, each offering different levels of interpretability.\n\nThe ML approach, specifically Gradient Boosting Machine (GBM) models, is somewhat of a blackbox. These models are designed to maximize predictive accuracy by identifying complex patterns in the data. While they can handle large, heterogeneous datasets and provide high predictive performance, they do not offer straightforward interpretations of how individual predictors contribute to the outcomes. The GBM models in this study identified relevant variables such as age of onset of first major depressive disorder (MDD) episode, baseline PTSD symptom severity, pain interference, and functional impairment. However, the exact nature of these contributions is not easily discernible from the model itself.\n\nIn contrast, multilevel models provide a more transparent approach. These models facilitate mechanistic interpretations by examining the relationships between independent predictors and posttraumatic stress symptom severity over time. For instance, they identified that greater use of primary control coping strategies, such as problem-solving and emotional expression, was associated with lower posttraumatic stress symptom severity. Additionally, multilevel models revealed that survivors with high and stable posttraumatic stress symptoms were characterized by greater self-blame, generalized anxiety disorder (GAD) severity, and childhood trauma exposure. These findings offer clear insights into how specific factors are associated with different patterns of posttraumatic stress symptom trajectories.\n\nIn summary, while the GBM models excel in predictive accuracy, they lack transparency in explaining the contributions of individual predictors. Multilevel models, on the other hand, provide a clearer understanding of how various factors influence posttraumatic stress symptom trajectories, making them more interpretable.",
  "model/output": "The model employed in this study utilized Gradient Boosting Machine (GBM) methods, which are versatile and can be used for both classification and regression tasks. Specifically, two GBM algorithms were developed: one for predicting PTSD onset, which is a classification problem, and another for predicting PTSD symptom severity at 6-month follow-up, which is a regression problem. Both models incorporated baseline posttraumatic stress symptom severity as a predictor and were compared against a benchmark model that predicted PTSD status using only baseline posttraumatic stress symptom severity.\n\nThe classification model, aimed at predicting PTSD onset, identified 27 relevant variables and achieved an optimism-corrected Area Under the Curve (AUC) of 0.96. This indicates a high level of predictive accuracy for identifying individuals who will develop PTSD. The regression model, focused on predicting PTSD symptom severity, explained 34% of the variability in CAPS-IV severity scores at the 6-month follow-up and identified 18 relevant variables. In comparison, baseline CAPS-IV severity scores alone explained 27% of the variability in PTSD symptom severity at follow-up.\n\nThe performance of these models was evaluated using optimism-corrected AUCs, which account for potential overfitting in smaller samples. The results suggest that the inclusion of a broad array of cognitive and neurobiological predictors can enhance the predictive accuracy of PTSD onset and symptom severity. However, it is important to note that while these models show promise, the findings should be interpreted with caution due to the small sample size and the need for replication in larger and more diverse populations.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the methods employed in this study involved a combination of statistical approaches to ensure both interpretability and predictive accuracy. Multilevel modeling was used to examine the relationships between baseline predictors and the trajectories of posttraumatic stress symptom severity over time. This approach allowed for a mechanistic interpretation of how various factors contribute to PTSD development. The multilevel models were specified using hierarchical linear models, which accounted for both within-person and between-person variability.\n\nIn parallel, machine learning (ML) techniques, specifically Gradient Boosting Machine (GBM) methods, were utilized to enhance predictive accuracy. Two GBM algorithms were developed: one to predict PTSD onset and another to predict PTSD symptom severity at the 6-month follow-up. These models included a wide array of predictors, aiming to capture the complexity of PTSD development. The performance of these ML models was compared to a benchmark model that used only baseline posttraumatic stress symptom severity.\n\nTo address the risk of overfitting, particularly given the small sample size, an optimism correction method was applied. This involved bootstrapping models to calculate an optimism value, which was then subtracted from the original Area Under the Curve (AUC) to provide a more realistic estimate of model performance. This approach is particularly useful in smaller studies where creating separate training and testing subsamples may not be feasible.\n\nThe performance of the ML models was also compared to a logistic regression model that included only baseline posttraumatic stress symptom severity. This comparison helped to assess whether the inclusion of additional cognitive and neurobiological predictors significantly improved predictive accuracy.\n\nOverall, the evaluation methods struck a balance between interpretability and accuracy, providing insights into both the mechanisms underlying PTSD development and the predictive power of various models.",
  "evaluation/measure": "In our study, we primarily reported the Area Under the Curve (AUC) as the key performance metric for evaluating the predictive accuracy of our models. The AUC is a widely used metric in the literature for assessing the performance of machine learning models, particularly in the context of PTSD prediction. It provides a single scalar value that represents the ability of the model to distinguish between those who will develop PTSD and those who will not.\n\nWe compared the AUC of our Gradient Boosting Machine (GBM) models, which included a broad array of theory-driven features, to a logistic regression model that included only baseline posttraumatic stress symptom severity. The GBM model achieved an optimism-corrected AUC of 0.96 for predicting new PTSD onsets over a 6-month follow-up period. This performance is notably higher than those reported in other studies predicting PTSD screening status or symptom trajectories, which ranged from 0.82 to 0.88.\n\nAdditionally, we reported the percentage of variability explained by our models in terms of R-squared values. The full GBM model explained 34% of the variability in CAPS-IV severity scores at the 6-month follow-up, while baseline CAPS-IV severity scores alone explained 27%. These metrics provide insight into how well our models capture the underlying patterns in the data and their potential generalizability to new datasets.\n\nWhile the AUC is a robust metric, it is important to note that it may not fully capture the complexity of PTSD prediction, especially in smaller samples. Therefore, we also discussed the limitations and cautions associated with interpreting these metrics, particularly in the context of overfitting and the ongoing debate regarding optimism-corrected AUCs.\n\nIn summary, our use of AUC and R-squared values aligns with common practices in the literature, providing a comprehensive evaluation of our models' predictive performance. These metrics, along with our discussions on their limitations, offer a balanced view of the strengths and potential challenges of our approach.",
  "evaluation/comparison": "In our study, we employed both multilevel modeling and machine learning (ML) approaches to predict posttraumatic stress disorder (PTSD) outcomes. The multilevel models were used to examine trajectories of posttraumatic stress symptom severity over time, while the ML methods, specifically Gradient Boosting Machine (GBM) algorithms, were used to predict PTSD onset and symptom severity at 6-month follow-up.\n\nFor the ML models, we included a large number of features (137 in total) to enhance predictive accuracy. These models were compared to a benchmark logistic regression model that included only baseline posttraumatic stress symptom severity. The performance of the GBM models was evaluated using the Area Under the Curve (AUC) metric, and the results were compared to those from other ML studies predicting PTSD outcomes.\n\nThe GBM models demonstrated strong predictive performance, with an AUC of 0.96 for predicting new PTSD onsets over follow-up. This performance was favorable when compared to other ML studies that predicted PTSD screening status and symptom trajectories in different populations, such as emergency room patients and soldiers.\n\nHowever, it is important to note that the optimism-corrected AUCs for the GBM models should be interpreted with caution due to the ongoing debate regarding their ability to address overfitting in smaller samples. Additionally, statistical tests appropriate for contrasting AUC values between optimism-corrected and logistic regression models, or for contrasting R2 values between non-nested models, are not available.\n\nIn summary, while our ML models showed promising results in predicting PTSD outcomes, the comparison to simpler baselines, such as logistic regression, indicates that the predictive accuracy of the CAPS-IV interview for identifying interpersonal violence survivors who will develop PTSD is high and is not substantially improved by including additional cognitive and neurobiological risk factors. Future studies are needed to assess the feasibility of including the CAPS interview in routine risk screening with trauma survivors.",
  "evaluation/confidence": "The evaluation of our models involved a careful consideration of performance metrics and their confidence intervals. For the Gradient Boosting Machine (GBM) models, we employed bootstrap-based optimism-corrected AUCs to address potential overfitting, especially given the relatively small sample size. This method provides a more conservative estimate of model performance by adjusting for optimism, which is crucial in smaller datasets. However, it is important to note that there are no statistical tests appropriate for contrasting AUC values between optimism-corrected and logistic regression models, nor for contrasting R2 values between non-nested models. This limitation means that while we can report the optimism-corrected AUCs, direct statistical comparisons with other models are not feasible.\n\nThe performance of our GBM model predicting new PTSD onsets over follow-up (AUC=0.96) was compared favorably to other machine learning studies. However, the optimism-corrected AUCs should be interpreted with caution due to the ongoing debate regarding their ability to address overfitting in smaller samples. This caution is essential because the high AUC value might reflect overfitting to the specific dataset rather than generalizable predictive accuracy.\n\nIn terms of statistical significance, our findings indicate that the predictive accuracy of the CAPS-IV interview for identifying interpersonal violence survivors who will develop PTSD is high. This accuracy is not substantially improved by including additional cognitive and neurobiological risk factors. The logistic regression model predicting endpoint PTSD from baseline CAPS-IV scores performed comparably to a full model with sociodemographic and traumatic event characteristics. This suggests that the baseline PTSD symptom severity is a strong predictor on its own.\n\nOverall, while our models show promising results, the lack of direct statistical comparisons and the need for caution in interpreting optimism-corrected AUCs mean that further validation in larger and more diverse samples is necessary to confirm the superiority of our methods over baselines.",
  "evaluation/availability": "Not enough information is available."
}