{
  "publication/title": "Establishment of prognostic models of adrenocortical carcinoma using machine learning and big data.",
  "publication/authors": "Tang J, Fang Y, Xu Z",
  "publication/journal": "Frontiers in surgery",
  "publication/year": "2022",
  "publication/pmid": "36684185",
  "publication/pmcid": "PMC9857757",
  "publication/doi": "10.3389/fsurg.2022.966307",
  "publication/tags": "- Adrenocortical carcinoma\n- Machine learning\n- Prognostic models\n- Survival analysis\n- Big data\n- SEER database\n- Artificial intelligence in medicine\n- Backpropagation artificial neural network\n- Random forest\n- Support vector machine\n- Naive Bayes classifier\n- Cancer prognosis\n- Medical data analysis\n- Predictive modeling\n- Oncology",
  "dataset/provenance": "The dataset used in this study was sourced from the Surveillance, Epidemiology, and End Results (SEER) database. This database provides comprehensive information on cancer statistics, aiming to reduce the cancer burden among the US population.\n\nInitially, 6,206 patients in the subcategory of C74 or C74.9 were identified, along with 8,370 patients from the ICD-O-3 morphology code. After applying specific screening criteria, 825 patients were selected for the study. The dataset includes patients diagnosed with adrenocortical carcinoma (ACC) between 1975 and 2018. The inclusion criteria required patients to have the location code of C74 or C74.9 and the ICD-O-3 morphology code 8370, and to be diagnosed with ACC histologically.\n\nSeveral exclusion criteria were applied to ensure the quality of the data. These included excluding patients diagnosed based on symptoms, imaging, exfoliative cytological, or gross pathological evidence alone, those with incomplete follow-up data, unknown T or N stage, and patients who died from causes other than ACC or had other tumors simultaneously.\n\nThe dataset has been used in previous research, with several papers published on ACC utilizing data from SEER. These studies have applied similar clinical factors to establish Cox\u2019s proportional hazards regression models and fabricate nomograms. For instance, Kong et al. used over 700 case records from SEER to build a Cox model, with external validation from the Cancer Genome Atlas (TCGA) database and multiple medical centers. However, the current study is the first to combine machine learning algorithms with the SEER database to create a survival condition model for ACC patients.",
  "dataset/splits": "The dataset was split into two primary parts: a training set and a test set. The split ratio used was 80% for training and 20% for testing. This means that 80% of the data was used to train the machine learning models, while the remaining 20% was used to evaluate their predictive performance.\n\nAdditionally, a 10-fold cross-validation method was employed, repeated 10 times. In this process, the data was divided into 10 approximately equal-sized groups. Each group was used once as a test set, while the remaining 9 groups formed the training set. This procedure was repeated 10 times, ensuring that each group served as the test set exactly once. This method generated 100 AUROC values for each model at each time point, providing a robust evaluation of the models' performance.",
  "dataset/redundancy": "The datasets used in this study were sourced from the SEER database. To ensure robust model training and validation, the filtered patient data were partitioned into training and test sets at an 8:2 ratio. This means 80% of the data was used for training the machine learning models, while the remaining 20% was reserved for testing their predictive power. This split was designed to maintain independence between the training and test sets, ensuring that the models were evaluated on data they had not seen during training.\n\nTo further validate the models and mitigate the risk of sampling errors, a 10-fold cross-validation method was employed. This method involved dividing all samples into 10 approximately equal-sized groups. Each group was used as a test set once, while the remaining 9 groups formed the training set. This process was repeated 10 times, generating 100 AUROC values for each model at each time point. This rigorous validation approach helped to confirm the models' performance and reliability.\n\nThe distribution of the data in this study is comparable to previously published machine learning datasets in the medical field, particularly those involving rare cancers like adrenocortical carcinoma (ACC). The SEER database provided a comprehensive set of clinical indicators, which were used to train and test the models. The exclusion criteria ensured that only high-quality data with complete follow-up information were included, enhancing the reliability of the results. The use of the SEER database, combined with the stringent validation methods, ensures that the findings are robust and generalizable to similar patient populations.",
  "dataset/availability": "The datasets utilized in this study are publicly available in online repositories. The specific details regarding the repository and accession numbers can be found within the article or the supplementary material. The data is shared under the terms of the Creative Commons Attribution License (CC BY), which permits the use, distribution, or reproduction in other forums, provided that the original authors and the copyright owner are credited, and the original publication in this journal is cited. This ensures that the data is accessible for further research and validation by other researchers while maintaining proper attribution.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and commonly used in the field. The algorithms employed include backpropagation artificial neural network (BP-ANN), random forest (RF), support vector machine (SVM), and naive Bayes classifier (NBC). These algorithms are part of the broader classes of neural networks, ensemble learning, and probabilistic classifiers, respectively.\n\nThese algorithms are not new; they have been extensively studied and applied in various domains, including medical research. The choice of these algorithms was driven by their proven effectiveness in handling complex, nonlinear data and their ability to select important features from large datasets. This makes them suitable for predicting tumor biological behavior and disease progression.\n\nThe decision to use these established algorithms in a medical context, rather than a machine-learning journal, is due to the specific focus of the study. The primary goal was to apply machine learning to predict the survival status of patients with adrenocortical carcinoma (ACC) using routine clinical indicators. The study aims to contribute to the medical field by demonstrating the practical application of these algorithms in a clinical setting, rather than introducing a new algorithm. The results show that these algorithms can provide valuable insights into patient outcomes, which is crucial for medical research and practice.",
  "optimization/meta": "The model described in this study does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it directly utilizes routine clinical indicators from the SEER database to predict the survival status of adrenocortical carcinoma (ACC) patients at important time points.\n\nThe study employs four common machine learning algorithms: backpropagation artificial neural network (BP-ANN), random forest (RF), support vector machine (SVM), and naive Bayes classifier (NBC). These algorithms were tested independently to predict the 1-, 3-, and 5-year survival status of ACC patients.\n\nThe training and testing procedures were conducted using an 80:20 split of the SEER dataset. Additionally, a 10-fold cross-validation method repeated 10 times was performed to ensure the robustness of the models. This method involved dividing the samples into 10 groups, using each group as a test set once while the remaining groups formed the training set. This process was repeated 10 times to generate 100 AUROC values for each model at each time point, ensuring that the training data was independent for each validation fold.\n\nThe results indicated that BP-ANN showed the highest mean AUROCs at all time points among the four models, establishing its superiority in predicting the survival status of ACC patients.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithms involved several steps to ensure the data was suitable for model training and testing. Initially, variables that could influence the survival of ACC patients were selected, including gender, race, age, T stage, N stage, surgery, tumor size, and metastasis to the liver, lung, and bone. Continuous variables such as age and tumor size were grouped using X-tile software to determine the best cutoff values, converting them into discrete types.\n\nThe dataset was then split into training and testing sets at an 8:2 ratio for internal validation. This division allowed for the training of machine learning models using 80% of the ACC patient data from the SEER registry, with the remaining 20% used to test the predictive power of the trained models. Additionally, a 10-fold cross-validation method repeated 10 times was employed to ensure the robustness of the models, especially in cases where no single algorithm performed best statistically.\n\nFour common machine learning algorithms were tested: backpropagation artificial neural network (BP-ANN), random forest (RF), support vector machine (SVM), and naive Bayes classifier (NBC). These algorithms were chosen to predict the 1-, 3-, and 5-year survival status of patients with ACC. Five-fold cross-validation was used for parameter adjustment of all four machine learning algorithms. The area under the receiver operating characteristic (AUROC) curve was used as a metric to evaluate model performance at each time point. This comprehensive approach ensured that the data was appropriately encoded and preprocessed, enabling effective model training and validation.",
  "optimization/parameters": "In our study, the input parameters for the machine learning models were selected based on variables that could potentially influence the survival of patients with adrenocortical carcinoma (ACC). These variables included gender, race, age, T stage, N stage, surgery, tumor size, and metastasis to the liver, lung, and bone.\n\nTo handle continuous variables like age and tumor size, we used X-tile software to calculate the best cutoff values, converting these variables into discrete types. This approach ensured that all predictive factors included in our models were categorical, which is suitable for the algorithms we employed.\n\nThe selection of these parameters was guided by their clinical relevance and availability in the SEER database. We aimed to include variables that are routinely collected in clinical practice, making our models practical for real-world applications.\n\nThe specific number of parameters (p) used in the model varied slightly depending on the algorithm and the time point being predicted. However, the core set of parameters remained consistent across different models and time points. This consistency allowed us to compare the performance of different machine learning algorithms fairly.\n\nNot applicable",
  "optimization/features": "The study utilized several variables that might influence the survival of ACC patients as input features for the machine learning models. These variables included gender, race, age, T stage, N stage, surgery, tumor size, and liver, lung, and bone metastasis. The continuous variables, age and tumor size, were grouped using X-tile software to calculate the best cutoff values, ensuring that all predictive factors included in the research were discrete types.\n\nFeature selection was implicitly performed by choosing variables that were deemed relevant based on clinical expertise and available data. This selection process was conducted using the entire dataset before splitting it into training and test sets. The training set, which comprised 80% of the data, was used to train the machine learning models, while the remaining 20% served as the test set to evaluate the predictive power of the trained models. The models were further validated using a 10-fold cross-validation method repeated 10 times to ensure robustness and reliability.",
  "optimization/fitting": "The fitting method employed in this study involved the use of machine learning algorithms to predict the survival status of patients with adrenocortical carcinoma (ACC) at various time points. Four common machine learning algorithms were tested: backpropagation artificial neural network (BP-ANN), random forest (RF), support vector machine (SVM), and naive Bayes classifier (NBC). These algorithms were chosen for their ability to handle complex, nonlinear data and select important features from large datasets.\n\nTo address the potential issue of overfitting, where the model performs well on training data but poorly on unseen data, a 10-fold cross-validation method repeated 10 times was performed. This technique involves dividing the dataset into 10 groups, using 9 groups for training and 1 group for testing, and repeating this process 10 times with different groups as the test set. This approach ensures that each data point is used for both training and testing, providing a robust estimate of the model's performance and helping to mitigate overfitting.\n\nUnderfitting, where the model is too simple to capture the underlying patterns in the data, was addressed by using multiple machine learning algorithms with different complexities. The comparison of these algorithms allowed for the selection of the most appropriate model for the given data. Additionally, the use of a comprehensive set of predictive factors, including gender, race, age, T stage, N stage, surgery, tumor size, and metastasis to various organs, ensured that the models had sufficient information to make accurate predictions.\n\nThe models were trained and tested using the \"caret\" package in R, which provides tools for parameter adjustment and model tuning. This package includes functions to characterize the differences between models and to optimize their performance. The area under the receiver operating characteristic curve (AUROC) was used as the primary metric to evaluate the predictive efficiency of the models. The BP-ANN model showed the highest mean AUROCs at all time points among the four machine learning-based models, indicating its superior performance in predicting the survival status of ACC patients.\n\nIn summary, the fitting method involved the use of multiple machine learning algorithms, 10-fold cross-validation to prevent overfitting, and a comprehensive set of predictive factors to ensure accurate and reliable predictions. The models were optimized using the \"caret\" package in R, and their performance was evaluated using the AUROC metric.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One of the primary methods used was cross-validation. Specifically, we utilized a 10-fold cross-validation method repeated 10 times. This involved dividing the dataset into 10 approximately equal-sized groups, using each group as a test set once while the remaining groups formed the training set. This process was repeated 10 times, generating 100 area under the receiver operating characteristic (AUROC) values for each model at each time point. This rigorous approach helped to mitigate the risk of overfitting by ensuring that the model's performance was evaluated across multiple subsets of the data.\n\nAdditionally, we partitioned the filtered patients into training and test sets at an 8:2 ratio. This split allowed us to train our models on a substantial portion of the data while reserving a separate set for unbiased evaluation. By doing so, we could assess the models' generalizability and performance on unseen data, further reducing the likelihood of overfitting.\n\nMoreover, we employed parameter adjustment techniques as guided by the official website of the R package \"caret.\" This involved customizing the tuning process to optimize the models' performance and prevent overfitting. The use of these established methods ensured that our models were well-calibrated and less prone to overfitting, thereby enhancing their reliability and predictive accuracy.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are available through the official website page of the R package \"caret\". This resource provides detailed approaches for parameter adjustment and the construction of modeling methods and codes. The specific URL for this information is http://topepo.github.io/caret/model-training-and-tuning.html#customizing-the-tuning-process.\n\nThe models were trained and tested using the \"caret\" package in R, and the statistical analyses were performed using R software version 4.0.3. The dataset used for this study was obtained from the SEER database, and the exclusion criteria, as well as the definition of forecasting variables, are clearly outlined in the methodology section of the publication.\n\nThe results of the 10-fold cross-validation method repeated 10 times are presented in Table 4, which includes the averages and medians of the area under the receiver operating characteristic (AUROC) curves for different machine learning algorithms at 1-, 3-, and 5-year time points. This table provides a comprehensive overview of the model performance at various time points.\n\nThe datasets used in this study can be found in online repositories, and the names of the repositories and accession numbers are provided in the article or supplementary material. All procedures involving human cases were conducted in accordance with the 1964 Helsinki declaration and later amendments or comparable ethical standards.\n\nThe authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. The datasets and the methods used for optimization are available for further research and validation, ensuring transparency and reproducibility of the results.",
  "model/interpretability": "The models employed in this study, specifically the backpropagation artificial neural network (BP-ANN), random forest (RF), support vector machine (SVM), and naive Bayes classifier (NBC), are generally considered black-box models. This means that while they are highly effective in making predictions, the internal workings and decision-making processes are not easily interpretable.\n\nThe BP-ANN, for instance, involves complex layers of neurons and weights that are adjusted during training, making it difficult to trace how a specific prediction is made. Similarly, the RF model, which aggregates the results of multiple decision trees, can be challenging to interpret due to the sheer number of trees and the interactions between them.\n\nThe SVM, while somewhat more interpretable than neural networks, still relies on finding a hyperplane in high-dimensional space, which can be abstract and hard to visualize. The NBC, on the other hand, is based on probabilistic relationships between features and outcomes, which can be more straightforward but still lacks transparency in how it combines these probabilities to make final predictions.\n\nIn summary, while these models are powerful tools for predicting survival status in patients with adrenocortical carcinoma, they do not provide clear, transparent insights into the decision-making process. This lack of interpretability is a common trade-off in machine learning, where models often sacrifice transparency for improved predictive performance.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the survival status of patients with adrenocortical carcinoma (ACC) at specific time points\u20141 year, 3 years, and 5 years. The model categorizes patients into two classes: \"alive\" and \"dead.\" This classification is achieved using various machine learning algorithms, including backpropagation artificial neural network (BP-ANN), random forest (RF), support vector machine (SVM), and naive Bayes classifier (NBC). The performance of these models is evaluated using metrics such as the area under the receiver operating characteristic curve (AUROC) and area under the curve (AUC). The results indicate that the BP-ANN model, in particular, shows strong predictive power, especially when validated through a 10-fold cross-validation method repeated 10 times. This validation process helps ensure the robustness and reliability of the model's predictions. The study concludes that the BP-ANN model, combined with big data, can precisely predict the survival status of ACC patients, making it a valuable tool for clinical applications.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to ensure the robustness and reliability of the machine learning models used to predict the survival status of patients with adrenocortical carcinoma (ACC). Initially, the dataset was split into training and test sets at an 8:2 ratio. This division allowed for the training of the models using 80% of the data and the evaluation of their predictive power using the remaining 20%.\n\nTo mitigate the risk of sampling errors and ensure the statistical significance of the results, a 10-fold cross-validation method repeated 10 times was performed. This process involved dividing all samples into 10 groups of approximately equal size. Each group was used as a test set in turn, while the other 9 groups formed the training sets. This cycle was repeated 10 times, generating 100 AUROC values for each model at each time node. The models evaluated included backpropagation artificial neural network (BP-ANN), random forest (RF), support vector machine (SVM), and naive Bayes classifier (NBC).\n\nThe performance of these models was assessed at 1-, 3-, and 5-year time points. The BP-ANN model demonstrated the highest mean AUROCs at all time points, indicating its superior predictive efficiency. Pairwise comparisons of the models' efficiencies showed that the averages of 1-, 3-, and 5-year AUROCs of BP-ANN statistically exceeded those of the other three models, establishing the superiority of BP-ANN with a significance level of P < 0.008. This rigorous evaluation method ensured that the findings were reliable and that the BP-ANN model was the most effective among the tested algorithms for predicting the survival status of ACC patients.",
  "evaluation/measure": "The performance of the machine learning models in this study was primarily evaluated using the area under the receiver operating characteristic curve (AUROC). This metric was chosen because it provides a comprehensive measure of model performance at various time points, offering insights into the models' ability to discriminate between different survival outcomes.\n\nThe AUROC values were reported for three key time points: 1-year, 3-year, and 5-year survival statuses. These time points are critical for assessing the long-term prognosis of patients with adrenocortical carcinoma (ACC). The models evaluated included backpropagation artificial neural network (BP-ANN), random forest (RF), support vector machine (SVM), and naive Bayes classifier (NBC). The AUROC values for these models were compared both in the training and test sets, ensuring a robust evaluation of their predictive capabilities.\n\nIn addition to the AUROC, the study employed a 10-fold cross-validation method repeated 10 times to further validate the models' performance. This approach helped to mitigate the risk of overfitting and provided a more reliable estimate of the models' generalizability. The results of this cross-validation process were used to compare the models pairwise, establishing the statistical significance of the differences in their performance.\n\nThe reported metrics are representative of standard practices in the literature for evaluating machine learning models in medical prognosis. The use of AUROC is particularly common in survival analysis and provides a clear and interpretable measure of model performance. The inclusion of cross-validation further strengthens the reliability of the findings, aligning with best practices in machine learning research.",
  "evaluation/comparison": "In our study, we compared the performance of four machine learning (ML) models to predict the survival status of patients with adrenocortical carcinoma (ACC) at 1-, 3-, and 5-year points. The models included Backpropagation Artificial Neural Network (BP-ANN), Random Forest (RF), Support Vector Machine (SVM), and Naive Bayes Classifier (NBC). The comparison was performed using the area under the receiver operating characteristic curve (AUROC) as the primary metric.\n\nInitially, the models were trained and tested using a 1:1 split of the dataset. The AUROCs for the test set at 1-year were 0.899 for BP-ANN, 0.875 for RF, 0.886 for SVM, and 0.862 for NBC. At 3-years, the AUROCs were 0.871 for BP-ANN, 0.858 for RF, 0.853 for SVM, and 0.869 for NBC. At 5-years, the AUROCs were 0.841 for BP-ANN, 0.783 for RF, 0.836 for SVM, and 0.867 for NBC.\n\nTo ensure the robustness of our findings, we employed a 10-fold cross-validation method repeated 10 times. This process involved dividing the samples into 10 groups, using each group as a test set once and the remaining groups as training sets. Each model generated 100 AUROC values at each time node. The mean AUROCs for BP-ANN were 0.890 at 1-year, 0.847 at 3-years, and 0.854 at 5-years, which were statistically superior to the other models (P < 0.008).\n\nThe comparison to simpler baselines, such as RF, SVM, and NBC, demonstrated that BP-ANN consistently outperformed these models across all time points. This indicates that BP-ANN is a more effective choice for predicting the survival status of ACC patients using routine clinical indicators from the SEER database. The use of a rigorous cross-validation method further validates the superiority of BP-ANN in this context.",
  "evaluation/confidence": "The evaluation of our models involved a rigorous process to ensure the reliability and statistical significance of our results. We employed a 10-fold cross-validation method repeated 10 times to mitigate sampling errors and to provide a robust assessment of model performance. This approach generated 100 AUROC values for each model at each time node, allowing for a comprehensive evaluation.\n\nTo determine the statistical significance of our findings, we performed pairwise comparisons of the models' efficiencies. The averages of the 1-, 3-, and 5-year AUROCs for the BP-ANN model statistically exceeded those of the other three models. The superiority of BP-ANN was established with a p-value of less than 0.008, indicating strong statistical significance.\n\nThe use of the 10-fold cross-validation method repeated 10 times ensured that our results were not due to random chance. This method provided a thorough evaluation of the models' performance and confirmed that BP-ANN was indeed the most effective among the four ML-based models tested. The statistical significance of our results gives us high confidence in claiming that BP-ANN is superior to the other models and baselines.",
  "evaluation/availability": "The datasets used in this study are available in online repositories. The specific names of the repositories and accession numbers can be found in the article or the supplementary material. This ensures that the data is accessible for further research and validation by other researchers. The availability of these datasets supports the reproducibility and transparency of the study's findings."
}