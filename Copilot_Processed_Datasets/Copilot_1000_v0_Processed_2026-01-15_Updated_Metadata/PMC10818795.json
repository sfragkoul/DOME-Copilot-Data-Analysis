{
  "publication/title": "Anti-Dengue: A Machine Learning-Assisted Prediction of Small Molecule Antivirals against Dengue Virus and Implications in Drug Repurposing.",
  "publication/authors": "Gautam S, Thakur A, Rajput A, Kumar M",
  "publication/journal": "Viruses",
  "publication/year": "2023",
  "publication/pmid": "38257744",
  "publication/pmcid": "PMC10818795",
  "publication/doi": "10.3390/v16010045",
  "publication/tags": "- Machine Learning\n- QSAR Models\n- Dengue Virus\n- Predictive Modeling\n- Cross-Validation\n- Model Performance\n- Applicability Domain\n- Drug Repurposing\n- Chemical Clustering\n- Web Server Development",
  "dataset/provenance": "The dataset used in this study was sourced from the \"DrugRepV\" database, which is a comprehensive collection of chemicals and repurposed drugs designed to target epidemic and pandemic viruses. This database contains a total of 8485 entries, providing detailed information on antiviral names, drug types, primary and secondary indications, viral strains, pathways, assay details, clinical status, and more.\n\nFor the development of the anti-dengue predictor, 900 antiviral entries specifically targeting dengue virus (DENV) were initially obtained from the \"DrugRepV\" database. These entries were then filtered based on criteria such as IC50/EC50 values, SMILES (Simplified Molecular-Input Line-Entry System), and molecular weight to acquire only the most relevant candidates. This filtering process resulted in a refined dataset of 238 antiviral entries.\n\nThis refined dataset was used to create the model, ensuring that only the most pertinent data points were included in the analysis. The dataset's comprehensive nature and the rigorous filtering process contribute to the robustness and reliability of the predictive model developed in this study.",
  "dataset/splits": "In our study, we employed a ten-fold cross-validation method to assess the performance of our machine learning predictive models. This technique involved splitting the training/testing dataset into ten equal parts. During each iteration, nine parts were combined for training, while the remaining part was used for testing to assess the model\u2019s performance. All ten parts were used as testing data at least once, and the overall model performance was evaluated based on the average performance of all the testing parts.\n\nAdditionally, to create independent validation datasets, we used a random selection process to choose approximately 10% of the available data, while the remaining 90% was utilized for training and testing purposes of the models. We repeated this procedure five times, resulting in five sets of training/testing and independent validation data, each containing 238 molecules. This approach ensured that our models were thoroughly validated and their performance was robust across different datasets.",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and widely recognized in the field. These include Support Vector Machine (SVM), Artificial Neural Network (ANN), k-Nearest Neighbor (kNN), and Random Forest (RF). These algorithms are not new but are chosen for their effectiveness in handling both regression and classification tasks, which are crucial for developing predictive models in drug discovery.\n\nThe SVM algorithm is utilized for its ability to create hyperplanes that best separate data points, making it effective for both linear and nonlinear data through the use of kernel functions. The ANN mimics the human brain's neural network, comprising input, hidden, and output layers to transform input data into meaningful outputs. The kNN algorithm is a memory-based, instance-based learning method that classifies data points based on the proximity of their neighbors, calculated using various distance metrics. The RF algorithm builds multiple decision trees and averages their predictions to improve accuracy and robustness.\n\nThese algorithms were selected for their proven track record in various predictive modeling tasks, particularly in bioinformatics and drug discovery. They have been extensively used in similar studies, such as predicting antiviral compounds, identifying inhibitors, and developing predictive models for different viruses. The choice of these algorithms ensures that the models developed are reliable and can effectively predict the effectiveness of anti-dengue chemicals.\n\nThe decision to use these established algorithms rather than developing a new one is driven by the need for reliability and validation. These algorithms have been thoroughly tested and validated in numerous studies, providing a solid foundation for developing accurate predictive models. Additionally, publishing a new machine-learning algorithm in a specialized machine-learning journal would be appropriate if the focus were on algorithm development rather than application in a specific domain like drug discovery. In this context, the emphasis is on applying proven methods to address a critical health issue, specifically the development of anti-dengue inhibitors.",
  "optimization/meta": "The models developed in this study do not function as meta-predictors. Instead, they are individual machine learning models trained and evaluated using specific techniques. The machine learning methods utilized include Support Vector Machine (SVM), Random Forest (RF), k-Nearest Neighbors (kNN), and Artificial Neural Network (ANN). Each of these models was trained and tested independently using a ten-fold cross-validation method, ensuring that the training data was split into ten equal parts, with nine parts used for training and one part for testing in each iteration.\n\nThe performance of these models was assessed using various metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Coefficient of Determination (R2), and Pearson\u2019s Correlation Coefficient (PCC). Additionally, an independent/external dataset was used to validate the performance of the developed models, ensuring that the training and testing data were independent.\n\nThe applicability domain analysis was conducted to define the boundary of the developed models for their reliability. This analysis involved using a William\u2019s plot based on the distance-based leverage approach to assess the reliability of the models. The leverage threshold was calculated using the formula \\( h* = 3(p + 1)/n \\), where \\( p \\) is the number of descriptors and \\( n \\) is the number of compounds in the training dataset.\n\nIn summary, the models developed in this study are individual machine learning models trained and evaluated using specific techniques, with independent training and testing data. They do not rely on data from other machine-learning algorithms as input.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms. We began by computing a comprehensive set of molecular descriptors and fingerprints using the PaDEL software. This involved calculating 1D, 2D, and 3D molecular descriptors from 3D-SDF structures, resulting in a total of 17,968 descriptors. These descriptors encompassed various properties, including substructural molecular fragments, topological and electronic information, and geometrical and spatial information of the molecules.\n\nOne-dimensional descriptors focused on substructural molecular fragments, such as H-bond acceptors/donors, fingerprints, and fragment counts. Two-dimensional descriptors provided structural and physicochemical properties, including topological and electronic information, as well as connectivity indices. Three-dimensional descriptors were derived from the 3D conformation of the molecules, offering geometrical and spatial insights, such as solvent accessible areas and surface areas.\n\nMolecular fingerprints were also generated, using binary digits to represent specific substructures within the molecules. These fingerprints and descriptors were essential for studying drugs or chemicals to determine their quantitative structure-activity relationships (QSAR).\n\nFollowing the computation of descriptors, feature selection was performed to identify and eliminate redundant and irrelevant features. This process involved using perceptron, support vector regression (SVR), and decision tree (DT) methods within the recursive feature elimination (RFE) module of the scikit-learn library. The goal was to obtain significant features that could improve the accuracy of the developed models. Among the selected features, the top 100 features from the perceptron method were used as input for implementing the machine-learning algorithms.\n\nThe datasets were then split into training, testing, and independent validation sets. Approximately 10% of the available data was randomly selected for independent validation, while the remaining 90% was used for training and testing. This procedure was repeated five times to create five sets of training/testing and independent validation data, each containing 238 molecules.\n\nTo assess the performance of the machine-learning predictive models, we employed the ten-fold cross-validation method. This technique involved splitting the training/testing dataset into ten equal parts, using nine parts for training and one part for testing in each iteration. All ten parts were used as testing data at least once, and the overall model performance was evaluated based on the average performance of all the testing parts.\n\nAdditionally, we used an independent/external dataset that was not utilized during the model\u2019s training and testing to validate the performance of the developed model. Various statistical measures, including mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), coefficient of determination (R2), and Pearson\u2019s correlation coefficient (PCC), were calculated to evaluate the model\u2019s performance. These metrics provided insights into the accuracy and reliability of the predictive models.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "In the optimization process, feature selection was indeed performed to enhance the accuracy of the developed models. The initial dataset comprised 17,968 molecular descriptors and fingerprints, which were computed using 3D-SDF structures with PaDEL software. To identify and eliminate redundant and irrelevant features, recursive feature elimination (RFE) was employed using three different methods: perceptron, support vector regression (SVR), and decision tree (DT). This process was conducted within the scikit-learn library.\n\nThe RFE module was used to find the top 50, 100, 150, and 200 relevant features. Among these, the top 100 features selected by the perceptron method were chosen as the input for implementing the machine learning algorithms. This selection was based on the perceptron method's effectiveness in identifying the most significant features for model development.\n\nThe feature selection process was performed using the training set only, ensuring that the selected features were representative of the data used to train the models. This approach helps in avoiding overfitting and ensures that the models generalize well to new, unseen data. The top 100 features selected by the perceptron method were then used as input for the machine learning algorithms, including SVM, ANN, kNN, and RF, to develop robust predictive models for identifying inhibitors.",
  "optimization/fitting": "In our study, we employed several machine learning algorithms, including Support Vector Machine (SVM), Artificial Neural Network (ANN), k-Nearest Neighbors (kNN), and Random Forest (RF), to develop predictive models. The number of parameters in these models was carefully managed to avoid both overfitting and underfitting.\n\nTo address the potential issue of overfitting, where the model might perform well on training data but poorly on unseen data, we utilized ten-fold cross-validation. This technique involved splitting the dataset into ten equal parts, using nine parts for training and one part for testing in each iteration. This process ensured that all parts were used for testing at least once, providing a robust evaluation of the model's performance. Additionally, we validated the models using an independent/external dataset that was not used during training or testing, further confirming the models' generalizability.\n\nFeature selection was another crucial step in preventing overfitting. We employed recursive feature elimination (RFE) with methods like perceptron, Support Vector Regression (SVR), and Decision Trees (DT) to identify and eliminate redundant and irrelevant features. This process helped in retaining only the most significant features, thereby reducing the complexity of the models and mitigating the risk of overfitting.\n\nTo avoid underfitting, where the model is too simple to capture the underlying patterns in the data, we ensured that the models were sufficiently complex. For instance, in the case of Random Forest, we tuned parameters such as the number of trees, maximum depth, and minimum samples per leaf. Similarly, for kNN, we selected an appropriate number of neighbors to balance bias and variance. For ANN, we adjusted the number of hidden layers and neurons to ensure the model could learn the necessary patterns without becoming too simplistic.\n\nMoreover, the applicability domain analysis using William\u2019s plot helped in assessing the reliability of the models. This analysis ensured that the chemical properties of new compounds fell within the domain of the training data, thereby enhancing the models' predictive accuracy and reliability.\n\nIn summary, through careful feature selection, cross-validation, and parameter tuning, we managed to develop models that neither overfit nor underfit the data, ensuring robust and reliable predictions.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the primary methods used was ten-fold cross-validation. This technique involved splitting the dataset into ten equal parts, using nine parts for training and one part for testing in each iteration. This process was repeated ten times, ensuring that each part was used as testing data at least once. This approach helps to provide a more accurate estimate of model performance and reduces the risk of overfitting.\n\nAdditionally, we performed feature selection to identify and eliminate redundant and irrelevant features. This step is crucial in improving model accuracy by focusing on the most significant features. We used recursive feature elimination (RFE) with methods like perceptron, support vector regression (SVR), and decision trees (DT) to select the top 100 relevant features. This reduction in dimensionality helps in building more generalized models that are less likely to overfit.\n\nFurthermore, we validated our models using an independent/external dataset that was not used during the training and testing phases. This external validation provides an unbiased evaluation of the model's performance and ensures that the model generalizes well to new, unseen data.\n\nWe also conducted an applicability domain analysis to define the boundary of the developed models for their reliability. This analysis helps in assessing the reliability of predictions for new compounds by ensuring that their chemical properties fall within the applicability domain of the training compounds.\n\nLastly, we generated decoy sets to confirm the predictive model's reliability. Decoys are molecules that cannot bind to their target, and comparing their inhibitory activity with active molecules helps in validating the model's performance. This approach provides an additional layer of validation, ensuring that the models are robust and reliable.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in the tables provided in the publication. Specifically, for the k-nearest neighbors (kNN) and random forest (RF) machine learning techniques, the model parameters and their corresponding performance metrics are detailed in Tables 3 and 4, respectively. These tables include information such as the number of neighbors (k) for kNN, the number of trees, maximum depth, minimum samples split, and minimum samples leaf for RF, among other parameters.\n\nThe optimization schedule and model files are not explicitly detailed in the main text, but the performance of the models is thoroughly evaluated using metrics such as mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), coefficient of determination (R2), and Pearson\u2019s correlation coefficient (PCC). These metrics provide a comprehensive overview of the model's performance and the effectiveness of the optimization process.\n\nRegarding the availability and licensing of the model files and optimization parameters, this information is not explicitly stated in the provided context. However, it is common practice in scientific publications to make datasets and model files available upon request or through supplementary materials. For specific details on accessing these resources, readers would typically refer to the supplementary information or contact the authors directly.",
  "model/interpretability": "The models developed in this study are primarily black-box models, meaning their internal workings are not easily interpretable. This is characteristic of machine learning techniques such as Support Vector Machines (SVM), Random Forests (RF), k-Nearest Neighbors (kNN), and Artificial Neural Networks (ANN), which are used here. These models are designed to make predictions based on complex patterns in the data, but the specific reasoning behind these predictions is not straightforward to extract.\n\nFor instance, the SVM model, which was found to be reliable through applicability domain analysis, operates by finding a hyperplane that best separates the data into different classes. While this method is effective for classification and regression tasks, the decision boundaries it creates are not easily interpretable. Similarly, the Random Forest model, which combines multiple decision trees, aggregates the results to make a final prediction. Although individual decision trees can be interpreted, the ensemble nature of Random Forests makes the overall model opaque.\n\nThe k-Nearest Neighbors (kNN) model is another example of a black-box approach. It makes predictions based on the similarity of new data points to existing ones in the dataset. While the nearest neighbors can be examined to understand the basis of a prediction, the model itself does not provide a clear, rule-based explanation of its decisions.\n\nArtificial Neural Networks (ANN) are particularly notorious for being black-box models. They consist of layers of interconnected nodes that process input data through weighted connections and activation functions. The relationships and weights within these layers are determined through training, but the exact process by which a neural network arrives at a prediction is not easily decipherable.\n\nIn summary, while these models are powerful tools for predictive tasks, their lack of transparency means that the specific factors contributing to their predictions are not readily apparent. This is a common trade-off in machine learning, where model complexity and predictive accuracy often come at the cost of interpretability.",
  "model/output": "The model developed in this study is a regression model. It predicts the pIC50 values, which are a measure of the inhibitory activity of compounds against the Dengue virus. The performance of the model is evaluated using regression metrics such as mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), coefficient of determination (R2), and Pearson\u2019s correlation coefficient (PCC). These metrics are used to assess how well the predicted pIC50 values match the actual values, indicating the model's effectiveness in regression tasks rather than classification.\n\nThe models were trained and validated using techniques like ten-fold cross-validation and independent dataset validation. The results show that the models have varying levels of performance, with some achieving high PCC values, indicating a strong correlation between the predicted and actual pIC50 values. This further confirms that the models are designed for regression tasks, focusing on predicting continuous values rather than categorizing data into discrete classes.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the predictive models developed in this study is not publicly released. However, the best-performing model, based on the support vector machine (SVM) algorithm, has been implemented on a web server named \"Anti-Dengue.\" This web server is freely accessible at https://bioinfo.imtech.res.in/manojk/antidengue/. Users can input chemical structures in SDF format to predict the inhibition efficiency of these chemicals against the dengue virus, as indicated by the pIC50 and IC50 values. The web server provides a user-friendly interface with dedicated pages such as \"Help\" and \"Frequently Asked Questions\" to assist users. The computation time for unknown chemicals typically ranges between 2 and 5 minutes. Users can track their job status using a job ID and retrieve the results at any time. The web server is constructed using LAMP software, incorporating Linux, Apache, MySQL, and PHP (Perl or Python) as the object-oriented scripting language. The front end is developed using HTML, CSS, and PHP, while the back end utilizes Python, Perl, and JavaScript.",
  "evaluation/method": "The evaluation of the predictive models involved several rigorous methods to ensure their reliability and performance. Ten-fold cross-validation was employed, where the dataset was split into ten equal parts. During each iteration, nine parts were used for training, and the remaining part was used for testing. This process was repeated ten times, with each part serving as the test set once. The overall model performance was then evaluated based on the average performance across all testing parts.\n\nAdditionally, an independent dataset, which was not used during the training and testing phases, was utilized to validate the model's performance. This external validation step is crucial for assessing the model's generalizability and robustness.\n\nTo quantify the model performance, several metrics were calculated, including mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), coefficient of determination (R2), and Pearson\u2019s correlation coefficient (PCC). These metrics provide a comprehensive view of the model's accuracy and reliability.\n\nThe applicability domain analysis was conducted to define the boundary within which the model's predictions are reliable. This involved using a William\u2019s plot, which depicts the relationship between leverage and standardized residuals. The leverage threshold was calculated to ensure that most data points fell within this threshold, indicating the model's reliability.\n\nFurthermore, decoy sets were generated to confirm the predictive model\u2019s reliability. The inhibitory activity in terms of pIC50 was calculated for these decoy sets and compared with the corresponding active molecules. This step helped in validating the model's ability to distinguish between active and inactive compounds.\n\nThe chemical diversity analysis was also performed to check the structural heterogeneity of the anti-dengue chemical compounds. This analysis involved binning clustering and multidimensional scaling plots to illustrate the dissimilarity of the compounds in chemical space.\n\nOverall, the evaluation methods employed ensured a thorough assessment of the model's performance, reliability, and generalizability.",
  "evaluation/measure": "In the evaluation of our predictive models, we employed several performance metrics to comprehensively assess their effectiveness. These metrics include the mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), coefficient of determination (R2), and Pearson\u2019s correlation coefficient (PCC). These metrics are widely recognized and used in the literature for evaluating the performance of machine learning models, particularly in the context of regression tasks.\n\nThe MAE provides an average measure of the absolute errors between the predicted and actual values, giving a straightforward indication of the model's accuracy. The MSE, on the other hand, quantifies the average of the squared differences between predicted and actual values, which is particularly useful for penalizing larger errors more heavily. The RMSE is the square root of the MSE, providing a measure in the same units as the original data, making it more interpretable.\n\nThe R2 value, or coefficient of determination, indicates how well the model's predictions match the actual data, with a value of 1 indicating perfect fit and 0 indicating no fit. The PCC measures the linear correlation between the predicted and actual values, ranging from -1 to +1, where +1 indicates a perfect positive correlation, -1 indicates a perfect negative correlation, and 0 indicates no correlation.\n\nThese metrics collectively provide a robust evaluation framework, ensuring that our models are assessed from multiple angles. The use of these metrics is consistent with standard practices in the field, making our evaluation process representative and reliable. Additionally, we validated our models using an independent dataset, further ensuring the generalizability and robustness of our findings.",
  "evaluation/comparison": "In our evaluation, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on assessing the performance of our developed models using internal validation techniques such as ten-fold cross-validation and an independent validation dataset. This approach allowed us to thoroughly evaluate the robustness and reliability of our models within the context of our specific research objectives.\n\nRegarding simpler baselines, we did not explicitly compare our models to simpler baselines. Our evaluation primarily centered on the performance metrics of our machine learning models, including support vector machines (SVM), random forests (RF), k-nearest neighbors (kNN), and artificial neural networks (ANN). We assessed these models using various performance metrics such as mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), coefficient of determination (R2), and Pearson\u2019s correlation coefficient (PCC). These metrics provided a comprehensive understanding of how well our models performed in predicting the inhibitory activity (pIC50) of anti-dengue chemical compounds.\n\nThe ten-fold cross-validation method involved splitting the dataset into ten equal parts, using nine parts for training and one part for testing in each iteration. This process was repeated ten times, ensuring that each part was used as testing data at least once. The overall model performance was evaluated based on the average performance across all testing parts. Additionally, we used an independent dataset that was not utilized during the model\u2019s training and testing to further validate the performance of our developed models.\n\nIn summary, while we did not compare our methods to publicly available benchmarks or simpler baselines, our evaluation was rigorous and focused on internal validation techniques and performance metrics to ensure the reliability and robustness of our predictive models.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The evaluation process involved using specific datasets and metrics that are detailed within the publication. However, supplementary tables and figures provide insights into the evaluation results, such as the actual and predicted pIC50 values, and the performance metrics for various models. These supplementary materials can be accessed through the publication's supporting information. The evaluation datasets used, including the training, testing, and independent validation datasets, are not released publicly. The models were assessed using metrics like MAE, MSE, RMSE, R2, and PCC, which are thoroughly explained in the text. For further details on the evaluation process and results, readers are encouraged to refer to the supplementary tables and figures provided."
}