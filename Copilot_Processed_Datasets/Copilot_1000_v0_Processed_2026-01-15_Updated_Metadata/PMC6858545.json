{
  "publication/title": "Positive-unlabeled convolutional neural networks for particle picking in cryo-electron micrographs.",
  "publication/authors": "Bepler T, Morin A, Rapp M, Brasch J, Shapiro L, Noble AJ, Berger B",
  "publication/journal": "Nature methods",
  "publication/year": "2019",
  "publication/pmid": "31591578",
  "publication/pmcid": "PMC6858545",
  "publication/doi": "10.1038/s41592-019-0575-8",
  "publication/tags": "- Cryo-electron microscopy\n- Particle picking\n- Convolutional neural networks\n- Positive-unlabeled learning\n- Protein structure determination\n- Machine learning in biology\n- Computational biology\n- Neural network training\n- Micrograph analysis\n- Automated particle detection",
  "dataset/provenance": "The datasets used in our study were sourced from the Electron Microscopy Public Image Archive (EMPIAR) and from specific research labs. We utilized several datasets, including EMPIAR-10025, EMPIAR-10028, and EMPIAR-10096, which were retrieved directly from EMPIAR. These datasets contain aligned and summed micrographs along with star files containing published particle sets.\n\nFor the EMPIAR-10234 dataset, which focuses on clustered protocadherin, the aligned and summed micrographs and hand-labeled particle coordinates were provided by the Shapiro lab. This dataset involved collecting single particle micrographs on a Titan Krios equipped with a K2 counting camera, operated at 300 kV with a calibrated pixel size of 1.061 \u00c5. A total of 896 micrographs were collected using Leginon, and frames were aligned using MotionCor2.\n\nThe EMPIAR-10215 dataset, which includes rabbit muscle aldolase, was collected on a Titan Krios in super-resolution mode. The microscope was operated at 300 kV with a calibrated super-resolution pixel size of 0.416 \u00c5. A total of 1,052 micrographs were collected using Leginon, and frames were aligned, Fourier binned by a factor of 2, and dose compensated using MotionCor2.\n\nThe Toll receptor dataset was also collected on a Titan Krios equipped with a K2 counting camera, operated at 300 kV with a calibrated pixel size of 0.832 \u00c5. A total of 9,323 micrographs were collected using Leginon, and frames were aligned using MotionCor2.\n\nEach dataset was downsampled and split into training and test sets at the whole micrograph level. The number of micrographs and particles varied across datasets, with some datasets having a larger number of micrographs and particles than others. For example, the EMPIAR-10028 dataset, which focuses on the 80S ribosome, had 831 training micrographs and 250 test micrographs, with a total of 80,701 training particles and 24,546 test particles. In contrast, the EMPIAR-10234 dataset had 67 training micrographs and 20 test micrographs, with a total of 1,167 training particles and 373 test particles.\n\nThese datasets have been used in previous studies and by the community, providing a robust foundation for our research. The use of publicly available datasets ensures reproducibility and allows for comparisons with other studies in the field.",
  "dataset/splits": "Each dataset was split into two main sets: training and testing. The splits were done at the micrograph level, ensuring that each micrograph was entirely within one set or the other.\n\nThe training set consists of a specific number of micrographs and particles, while the test set has a different number of micrographs and particles. The exact numbers vary by dataset. For instance, for the EMPIAR-10025 dataset, the training set includes 156 micrographs and 39,653 particles, whereas the test set contains 40 micrographs and 10,301 particles. Similarly, for the EMPIAR-10028 dataset, the training set has 831 micrographs and 80,701 particles, and the test set includes 250 micrographs and 24,546 particles.\n\nThe distribution of data points in each split is detailed in a table, which provides the number of micrographs and particles for both the training and test sets across various datasets. This table includes information for datasets like EMPIAR-10096, EMPIAR-10215, EMPIAR-10234, and others, specifying the number of micrographs and particles in each split.\n\nNot applicable",
  "dataset/redundancy": "The datasets used in our study were split into training and test sets at the micrograph level. This means that each micrograph was entirely assigned to either the training set or the test set, ensuring that the training and test sets are independent. This approach helps to prevent data leakage and ensures that the model's performance on the test set is a true reflection of its ability to generalize to unseen data.\n\nThe number of micrographs and labeled particles in each split is reported in a table. This table provides a clear overview of the distribution of data between the training and test sets for each dataset. The datasets were downsampled to specific resolutions and normalized using a per-image scaled two-component Gaussian mixture model. This normalization process helps to standardize the data and improve the performance of the classifier.\n\nThe distribution of the datasets used in our study is comparable to previously published machine learning datasets in the field of cryo-electron microscopy. The datasets include a variety of proteins and particles, ranging from well-defined structures like the T20S proteasome and 80S ribosome to more challenging particles like the clustered protocadherin. This diversity ensures that our findings are robust and applicable to a wide range of cryo-EM datasets.",
  "dataset/availability": "The data used in our study, including the data splits, have been made publicly available. Specifically, single particle half maps, full sharpened maps, and masks for several proteins, such as T20S proteasome, 80S ribosome, rabbit muscle aldolase, and the Toll receptor, have been deposited in the Electron Microscopy Data Bank (EMDB). The accession codes for these datasets are EMD-9194, EMD-9201, EMD-9202, EMD-9206, EMD-9207, EMD-9208, EMD-9209, EMD-9210, EMD-9211, EMD-20529, EMD-20531, and EMD-20532. Additionally, the full rabbit muscle aldolase dataset is available in the Electron Microscopy Pilot Image Archive (EMPIAR) with the accession code EMPIAR-10215.\n\nThe availability of these datasets ensures that other researchers can access and use the data for their own studies, promoting reproducibility and further advancements in the field. The data is released under standard licenses typically associated with these databases, which allow for academic use and further research. The enforcement of data availability was managed through the submission and review processes of these databases, ensuring that the data is correctly formatted, annotated, and accessible to the scientific community.",
  "optimization/algorithm": "The machine-learning algorithm class used in our work is convolutional neural networks (CNNs), specifically designed for positive-unlabeled (PU) learning. The core of our approach involves generalized expectation (GE) criteria, which is not entirely new but has been adapted and optimized for our specific application in particle picking for cryo-electron microscopy (cryoEM).\n\nOur method introduces two novel GE criteria: GE-KL and GE-binomial. These criteria are designed to address the challenges of PU learning, where only positive and unlabeled data are available, and the goal is to train a classifier that can accurately identify positive instances. The GE-KL criterion uses KL-divergence to match the expectation of the classifier over the unlabeled data to the known fraction of positives. The GE-binomial criterion, on the other hand, minimizes the difference between the distribution over the number of positives in the minibatch and the binomial distribution parameterized by the known fraction of positives.\n\nThe reason these algorithms were not published in a machine-learning journal is that our primary focus is on the application of these methods to cryoEM, rather than the development of the algorithms themselves. The GE criteria are a general class of methods that have been previously proposed, but our contributions lie in adapting these criteria for PU learning in the context of particle picking and demonstrating their effectiveness on real-world cryoEM datasets. Our work showcases how these adapted algorithms can significantly improve the efficiency and accuracy of particle picking, which is a critical step in the cryoEM workflow.\n\nAdditionally, we explore the integration of autoencoder components with our classifier to further enhance performance, especially when few labeled data points are available. This hybrid approach acts as a form of regularization, improving the generalizability of the classifier by requiring that the feature vectors be descriptive of the input. The autoencoder structure includes a series of transpose convolutional layers followed by batch normalization and leaky ReLU activations, culminating in the reconstruction of the input image.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It primarily relies on convolutional neural networks (CNNs) for particle picking in cryo-electron microscopy (cryoEM) images. The core of the model involves training classifiers using different objective functions, such as the naive Positive-Negative (PN), Kiryo et al.\u2019s non-negative risk estimator (PU), Generalized Expectation (GE)-KL, and GE-binomial. These classifiers are evaluated based on their average-precision scores on test set micrographs.\n\nThe training process involves simulating hand-labeling by randomly sampling varying numbers of particles from the training sets to treat as positive examples, with all other particles considered unlabeled. The classifiers are then trained with these positives and evaluated on the test set micrographs. This process is repeated with multiple independent samples of particles for each number of positives to ensure robustness.\n\nAdditionally, the model incorporates an autoencoder component to improve classifier performance, especially when few labeled data points are available. The autoencoder acts as a regularization technique, requiring that the feature vectors given by the encoder network be descriptive of the input. This hybrid classifier+autoencoder network is trained with different settings of the autoencoder weight (\u03b3) and varying numbers of labeled data points.\n\nThe evaluation of the classifiers is done using the average-precision score, which measures how well the micrograph regions are ranked by the predicted probability of containing a particle. This score corresponds to the area under the precision-recall curve.\n\nIn summary, the model does not use data from other machine-learning algorithms as input in a meta-predictor sense. Instead, it focuses on improving particle picking in cryoEM images through the use of CNNs and autoencoders, with a emphasis on handling scenarios with few labeled positives.",
  "optimization/encoding": "In our work, the data encoding and preprocessing for the machine-learning algorithm involved several key steps. We utilized micrograph regions as input data, where each region was represented as an image. These images were processed through a convolutional neural network (CNN) classifier. The classifier consisted of three convolutional layers with striding, batch normalization, and parametric rectified linear units (PReLU). The architecture was designed as follows: the first layer had 32 filters with a 7x7 kernel size, the second layer had 64 filters with a 5x5 kernel size, and the third layer had 128 filters with a 5x5 kernel size. Each convolutional layer was followed by batch normalization and PReLU activation, with a stride of 2 to downsample the feature maps.\n\nThe final layer of the classifier was a fully connected layer with a single output, which used a sigmoid activation function to produce the predicted probability of a region containing a particle. This output was interpreted as the log-likelihood ratio between the positive and negative classes.\n\nAdditionally, when augmenting the classifier with an autoencoder, we introduced a decoder network. This decoder took the output of the feature extractor network and reconstructed the input image. The decoder structure consisted of transpose convolutions with batch normalization and leaky ReLU activations. The process involved projecting the d-dimensional representation output by the final convolutional layer of the classifier into a small spatial dimension but large feature dimension representation. This representation was then repeatedly projected into larger spatial dimensions and smaller feature dimensions until the final output matched the original input image size. Specifically, the decoder structure was z -> transpose conv4\u00d74 128-d, batch normalization, leaky ReLU -> transpose conv4\u00d74 64-d, stride 2, batch normalization, leaky ReLU -> transpose conv4\u00d74 32-d, stride 2, batch normalization, leaky ReLU -> transpose conv3\u00d73 1-d, stride 2 -> X\u2019.\n\nThis encoding and preprocessing pipeline ensured that the input data was appropriately transformed and ready for training the machine-learning models.",
  "optimization/parameters": "In our model, the primary hyperparameters introduced by our objective function are the unknown positive class prior, \u03c0, and the constraint strength, \u03bb. The positive class prior, \u03c0, could be chosen through cross-validation, but we observed that our results were relatively insensitive to its choice. Therefore, we did not extensively tune this parameter. The constraint strength, \u03bb, was set to a default value of 10 for the GE-KL method, as recommended by previous work, and to 1 for the GE-binomial method. We did not find it necessary to change \u03bb from these default settings.\n\nAdditionally, our model incorporates a simple convolutional neural network (CNN) architecture with reasonable default hyperparameters. While we did not extensively tune these hyperparameters, we note that further tuning, such as adjusting L2 regularization or dropout rates, could potentially improve model performance. The specific architecture of our CNN includes several transpose convolutional layers with batch normalization and leaky ReLU activations, which contribute to the overall number of parameters in the model. However, the exact number of parameters, p, depends on the specific configuration and dimensions of these layers, which were not explicitly detailed in our description.\n\nIn summary, while our model has several hyperparameters and architectural choices that could be tuned, we found that the default settings for \u03bb and \u03c0, along with a simple CNN architecture, were sufficient to achieve strong performance on the datasets we considered. Further exploration of these parameters and architecture could potentially yield additional improvements.",
  "optimization/features": "Not applicable.",
  "optimization/fitting": "The fitting method employed in our work involves training convolutional neural network (CNN) classifiers using a novel objective function, GE-binomial, which is designed to handle the challenges of positive-unlabeled (PU) learning. This approach is particularly useful in scenarios where the number of labeled positive examples is very small compared to the number of unlabeled data points, a common situation in cryo-electron microscopy (cryoEM) particle picking.\n\nTo address the potential issue of overfitting, given the small number of labeled positives, we incorporated several regularization techniques. Firstly, we used an autoencoder component in our hybrid classifier+autoencoder network. This reconstruction error term in the objective function acts as a form of regularization, ensuring that the feature vectors produced by the encoder network are descriptive of the input. This helps in improving the generalizability of the classifier when few labeled data points are available.\n\nAdditionally, we evaluated the performance of the classifier with different settings of the autoencoder weight, \u03b3, and varying numbers of labeled data points, N. We found that including the decoder network with a reconstruction error term improved classifier performance in the regime of few labeled data points. However, as the number of data points increased, the benefit of using the autoencoder decreased, and it eventually hurt classifier performance due to over-regularization. Our results suggest that using the autoencoder with \u03b3=10N gives the best results when N \u2264 250, and not using the autoencoder is best for N > 250.\n\nTo further mitigate overfitting, we also considered additional hyperparameter tuning, such as L2 regularization or dropout, which can improve model performance. The only hyperparameters introduced by our objective function are the unknown positive class prior, \u03c0, and the constraint strength, \u03bb. We observed that our results were relatively insensitive to the choice of \u03c0 and that \u03bb did not need to be changed from the default setting.\n\nUnderfitting was addressed by ensuring that our model architecture was capable of learning from the data. We used a simple CNN architecture with reasonable default hyperparameters and demonstrated that it performed well on the datasets. The GE-binomial objective function explicitly models the sampling statistics of minibatch training to regularize the classifier\u2019s posterior over the unlabeled data, which helps in effectively learning from the limited labeled positives.\n\nIn summary, our fitting method combines GE-binomial PU learning with autoencoder-based regularization to handle the challenges of few labeled positives. This approach helps in preventing overfitting and underfitting, ensuring that the classifier performs well even with a small number of labeled examples.",
  "optimization/regularization": "In our work, we employed several regularization techniques to prevent overfitting and improve the generalizability of our classifier, especially when dealing with a limited number of labeled data points.\n\nOne of the key methods we used is the incorporation of an autoencoder component into our classifier network. This approach involves breaking the classifier network into two parts: an encoder network and a linear classifier layer. The encoder network processes the input image and produces a feature representation, which is then passed to the linear classifier layer. Additionally, we introduced a deconvolutional decoder network that takes the output of the encoder and attempts to reconstruct the input image. The objective function was modified to include a term that penalizes the reconstruction error, weighted by a factor \u03b3. This regularization technique encourages the encoder to produce feature representations that are descriptive of the input, thereby improving the classifier's performance when labeled data is scarce.\n\nAnother regularization technique we utilized is the generalized expectation (GE) criteria, specifically the GE-binomial objective function. This method constrains the classifier to match the expectation over the unlabeled data, effectively regularizing the classifier's predictions. The GE-binomial objective function minimizes the difference between the distribution over the number of positives in the minibatch and the binomial distribution parameterized by the known fraction of positives, \u03c0. This approach helps to reduce overfitting by ensuring that the classifier's predictions are consistent with the known prior probability of the positive class.\n\nAdditionally, we used a simple three-layer convolutional neural network with striding, batch normalization, and parametric rectified linear units (PReLU) as the classifier. Batch normalization is known to act as a regularizer by normalizing the inputs of each layer, which can help to stabilize and accelerate training. Furthermore, the use of PReLU activations can provide a slight regularizing effect by allowing the network to learn sparse representations.\n\nIn summary, our regularization methods include the use of an autoencoder component with a reconstruction error term, the GE-binomial objective function, batch normalization, and PReLU activations. These techniques work together to prevent overfitting and improve the performance of our classifier, particularly in scenarios with few labeled data points.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the publication. Specifically, the values of \u03c0 used for training are specified in a table, and the GE criteria weight, \u03bb, is set according to recommended values. For GE-KL, \u03bb is set to 10, while for GE-binomial, it is set to 1. Additionally, the reconstruction loss weights (\u03b3) are varied and tested at different values, including \u03b3 = 0, \u03b3 = 1, and \u03b3 = 10/N.\n\nThe source code for Topaz, which includes the implementation of our models and the optimization procedures, is publicly available. It can be accessed via Code Ocean and on GitHub at the provided links. The code is licensed under the GNU General Public License v3.0, ensuring that it is freely available for use, modification, and distribution under the terms of this license.\n\nThe datasets used for training and testing, including the EMPIAR datasets, are also publicly available. The aligned and summed micrographs and star files for datasets such as EMPIAR-10025, EMPIAR-10028, and EMPIAR-10096 can be retrieved from EMPIAR. Additionally, the hand-labeled particle coordinates for the EMPIAR-10234 dataset were provided by the Shapiro lab.\n\nFor those interested in reproducing our results or using our models, all necessary information, including hyper-parameter settings, optimization parameters, and access to the code and datasets, is provided in the publication and through the specified online repositories. This ensures transparency and reproducibility of our findings.",
  "model/interpretability": "The model described in our work is primarily a convolutional neural network (CNN)-based classifier, which is inherently a black-box model. This means that the internal workings of the model are not easily interpretable, and it is challenging to understand how the model arrives at its predictions.\n\nHowever, there are aspects of our approach that can be considered more transparent. For instance, the use of an autoencoder component in our model provides some level of interpretability. The autoencoder attempts to reconstruct the input images from the learned features, which can give insights into what the model has learned. By examining the reconstructions, one can infer which features of the input images are considered important by the model.\n\nAdditionally, the use of positive-unlabeled (PU) learning with the GE-binomial objective function introduces a level of interpretability. The GE-binomial method estimates the distribution of positive examples in the unlabeled data, which can provide insights into the model's understanding of the positive class. This estimation process is more transparent than the typical black-box nature of neural networks.\n\nFurthermore, the model's architecture is relatively simple and well-documented. We use a three-layer convolutional neural network with striding, batch normalization, and parametric rectified linear units (PReLU). This architecture is standard and widely understood in the machine learning community, making it easier to interpret the general behavior of the model.\n\nIn summary, while the core of our model is a black-box CNN, the inclusion of an autoencoder and the use of the GE-binomial objective function provide some level of interpretability. The model's architecture is also well-documented and standard, which aids in understanding its general behavior.",
  "model/output": "The model described is a classification model. It is designed to predict the probability of a region in a micrograph containing a particle, effectively distinguishing between positive (particle-containing) and negative (non-particle-containing) regions. The final layer of the classifier network uses a sigmoid activation function to output this probability, which is interpreted as the log-likelihood ratio between the positive and negative classes. This output is crucial for evaluating the model's performance using metrics like the average-precision score, which measures how well the model ranks micrograph regions by their predicted probability of containing a particle.\n\nThe classifier is trained using various objective functions, including those that incorporate positive-unlabeled (PU) learning and autoencoder components. These objective functions help the model generalize better, especially when the number of labeled positive examples is limited. The model's architecture includes convolutional layers with striding, batch normalization, and parametric rectified linear units (PReLU), followed by a fully connected layer that produces the final classification output. When augmented with an autoencoder, the model includes a decoder network that reconstructs the input image from the feature representation, adding a regularization term to the objective function. This regularization helps improve the model's performance in scenarios with few labeled data points by ensuring that the feature vectors are descriptive of the input.",
  "model/duration": "Our pipeline is designed to be computationally efficient. Training the model typically takes a few hours on a single GPU. Once trained, the model can produce predictions for hundreds of micrographs in just minutes. This efficiency allows for rapid processing and analysis of cryoEM data, significantly expediting the structure determination workflow. The model's ability to be applied to new imaging runs of the same particle further enhances its practical utility, reducing the need for repeated manual effort and increasing the overall efficiency of cryoEM workflows.",
  "model/availability": "The source code for Topaz is publicly available via Code Ocean and on GitHub at https://github.com/tbepler/topaz. Updates to Topaz will be posted at http://topaz.csail.mit.edu. Topaz is licensed under the GNU General Public License v3.0. The Topaz GUI is based on VGG Image Annotator (VIA), which is developed and maintained with the support of EPSRC programme grant Seebibyte: Visual Search for the Era of Big Data (EP/M013774/1).",
  "evaluation/method": "The evaluation of our method involved several key steps and experiments to ensure its robustness and effectiveness. We compared classifiers trained with different objective functions by simulating hand-labeling with varying amounts of effort. This was done by randomly sampling different numbers of particles from the training sets to treat as positive examples, with all other particles considered unlabeled. We used cross-entropy loss for the labeled particles and evaluated the classifiers using the average-precision score on the test set micrographs. This process was repeated with 10 independent samples of particles for each number of positives to ensure statistical significance.\n\nWe also evaluated classifiers trained with autoencoder components and varying numbers of labeled data points. The input reconstruction weight, \u03b3, was set to different values (0, 1, and 10N) to assess its impact on classifier performance. For each setting of \u03b3 and N, we trained 10 models with different sets of N randomly sampled positives and calculated the average-precision score for each model on the test split of each dataset.\n\nStatistical significance of performance differences between methods at each number of labeled positive examples was assessed using a two-sided t-test. This rigorous evaluation process allowed us to compare the effectiveness of different objective functions and the impact of autoencoder components on classifier performance. The results demonstrated that our GE-based PU learning approaches dramatically outperformed previous PU learning methods, enabling particle picking despite few labeled positives on challenging datasets.",
  "evaluation/measure": "In our evaluation, we primarily report the average-precision score as the key performance metric for our classifiers. This score is a measure of how well the micrograph regions are ranked when ordered by the predicted probability of containing a particle. It corresponds to the area under the precision-recall curve and is calculated as the sum over the ranked micrograph regions of the precision at k elements times the change in recall.\n\nPrecision is defined as the fraction of predictions that are correct, while recall is the fraction of labeled particles that are retrieved in the top k predictions. This metric is widely used in the literature for evaluating object detection and classification tasks, making it a representative and comparable measure.\n\nThe average-precision score provides a comprehensive evaluation of the classifier's performance, especially in scenarios with imbalanced datasets, which is common in particle picking tasks. By using this metric, we can effectively assess the trade-off between precision and recall, ensuring that our models are robust and generalizable.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated our generalized expectation (GE) criteria-based positive-unlabeled (PU) learning methods against other general-purpose PU learning approaches. Specifically, we benchmarked our methods against the non-negative risk estimator approach of Kiryu et al. and a naive approach where unlabeled data are considered as negative for classifier training.\n\nWe conducted these comparisons on two additional cryo-electron microscopy (cryoEM) datasets: EMPIAR-10096, which contains influenza hemagglutinin trimer particles, and EMPIAR-10234, a challenging dataset provided by the Shapiro lab containing stick-like particles with low signal-to-noise ratio (SNR).\n\nTo ensure a fair comparison, we simulated positively labeled datasets of varying sizes by randomly subsampling the set of all positive examples within the training set of each dataset. This allowed us to assess the performance of each method under different labeling efforts.\n\nOur results demonstrated that classifiers trained with our GE criteria-based objective functions, namely GE-KL and GE-binomial, dramatically outperformed those trained with the non-negative risk estimator or the naive approach. Generally, GE-binomial and GE-KL classifiers displayed similar performance, with a few notable exceptions where GE-binomial provided better results. For instance, on the dataset with more compact particles (EMPIAR-10096), GE-binomial achieved significantly higher average-precision scores than GE-KL when the number of labeled positives was very small (10 positive examples). On the challenging EMPIAR-10234 dataset, GE-binomial significantly outperformed GE-KL at 1,000 labeled examples, while GE-KL gave better results within the 50\u2013250 range of labeled examples.\n\nThese findings indicate that our GE-based PU learning approaches are highly effective, enabling particle picking even with few labeled positives on challenging datasets and substantially improving picking quality on easier datasets.",
  "evaluation/confidence": "The evaluation of our methods includes statistical significance assessments to ensure the robustness of our claims. For the PU learning benchmarking, we repeated the experiments with 10 independent samples of particles for each number of positives. Statistical significance of performance differences between methods at each number of labeled positive examples was assessed using a two-sided t-test. This approach helps to determine if the observed differences in performance are likely due to the methods themselves rather than random chance.\n\nIn our comparisons of different objective functions, we used a two-sided dependent t-test to identify statistically significant differences in average-precision scores. For instance, in the experiments with the EMPIAR-10096 and EMPIAR-10234 datasets, we marked cases where one method significantly outperformed another with p-values less than 0.05. This rigorous statistical testing provides confidence in the superiority of our GE-based PU learning approaches over previous methods.\n\nAdditionally, when evaluating classifiers trained with autoencoder components, we calculated the average-precision score for each model on the test split of each dataset and reported the mean and standard deviation. This allows for an understanding of the variability in performance and the confidence in the reported average-precision scores. The inclusion of standard deviations in our plots further supports the reliability of our findings.\n\nOverall, the use of statistical tests and the reporting of variability in performance metrics ensure that our claims about the superiority of our methods are well-founded and statistically significant.",
  "evaluation/availability": "The raw evaluation files for our study are not directly available. However, the data used for evaluation, such as single particle half maps, full sharpened maps, and masks for various proteins like T20S proteasome, 80S ribosome, rabbit muscle aldolase, and the Toll receptor, have been deposited in the Electron Microscopy Data Bank (EMDB) under specific accession codes. Additionally, the full rabbit muscle aldolase dataset is available in the Electron Microscopy Pilot Image Archive (EMPIAR) with the accession code EMPIAR-10215.\n\nThe source code for Topaz, the tool used in our evaluations, is publicly available via Code Ocean and on GitHub. Updates to Topaz will be posted on the official Topaz website. Topaz is licensed under the GNU General Public License v3.0, which allows for free use, modification, and distribution of the software, provided that the original license and copyright notice are included in all copies or substantial portions of the software."
}