{
  "publication/title": "Detecting Adverse Drug Reactions on Twitter with Convolutional Neural Networks and Word Embedding Features.",
  "publication/authors": "Masino AJ, Forsyth D, Fiks AG",
  "publication/journal": "Journal of healthcare informatics research",
  "publication/year": "2018",
  "publication/pmid": "35415401",
  "publication/pmcid": "PMC8982795",
  "publication/doi": "10.1007/s41666-018-0018-9",
  "publication/tags": "- Adverse Drug Reactions\n- Social Media Analysis\n- Convolutional Neural Networks\n- Word Embeddings\n- Natural Language Processing\n- Machine Learning\n- Pharmacovigilance\n- Twitter Data\n- Text Classification\n- Deep Learning",
  "dataset/provenance": "The dataset used in this study is a combination of two Twitter datasets. The first dataset, referred to as the CHOP ADHD Dataset, was collected between May 1st and December 31st, 2015. This dataset consists of tweets containing keywords related to medications commonly used to treat attention deficit hyperactivity disorder (ADHD). The search included 166 terms, accounting for misspellings and phonetically similar versions of drug names. From the collected tweets, 4402 were randomly sampled and labeled as positive or negative for containing adverse drug reactions (ADRs). Of these, 278 tweets (6.32%) were identified as containing an ADR.\n\nThe second dataset, known as the ASU ADR Dataset, was previously published and made publicly available. It includes tweets with keywords corresponding to a list of 74 brand and generic drugs. The authors of this dataset released annotations and tweet IDs for 7574 tweets, but only 4850 were available through the Twitter API at the time of this study. Of these, 559 tweets (11.51%) were labeled as containing an ADR.\n\nThe complete dataset, formed by combining the CHOP ADHD Dataset and the ASU ADR Dataset, was split into stratified training (80%) and test (20%) sets. The training set contains 7401 tweets, with 669 (9%) containing an ADR and 6732 (91%) labeled as containing no ADR. The test set contains 1851 tweets, with 168 (9%) labeled as containing an ADR and 1683 (91%) labeled as containing no ADR. The labeled ADHD dataset is available for further research and can be accessed at a provided GitHub repository.",
  "dataset/splits": "The dataset used in this study was created by combining two Twitter datasets: the CHOP ADHD Dataset and the ASU ADR Dataset. The complete dataset was split into two main parts: a training set and a test set.\n\nThe training set contains 7401 tweets, of which 669 (approximately 9%) are labeled as containing an adverse drug reaction (ADR), and 6732 (approximately 91%) are labeled as containing no ADR.\n\nThe test set contains 1851 tweets, with 168 (approximately 9%) labeled as containing an ADR and 1683 (approximately 91%) labeled as containing no ADR.\n\nThe dataset splits were stratified to maintain the same distribution of ADR and non-ADR tweets in both the training and test sets. This ensures that the model is trained and evaluated on representative samples of the data.",
  "dataset/redundancy": "The datasets used in this study were combined from two sources: the CHOP ADHD Dataset and the ASU ADR Dataset. The CHOP ADHD Dataset was collected between May 1st and December 31st, 2015, using the Twitter streaming API, focusing on tweets containing keywords related to ADHD medications. The ASU ADR Dataset was previously published and made publicly available, containing tweets with keywords corresponding to a list of brand and generic drugs.\n\nThe complete dataset was split into stratified training and test sets. The training set contains 7401 tweets, with 669 (9%) labeled as containing an adverse drug reaction (ADR) and 6732 (91%) labeled as containing no ADR. The test set contains 1851 tweets, with 168 (9%) labeled as containing an ADR and 1683 (91%) labeled as containing no ADR. This split ensures that the training and test sets are independent and that the distribution of ADR-positive and ADR-negative tweets is consistent across both sets.\n\nTo enforce independence between the training and test sets, the dataset was stratified, meaning that the proportion of ADR-positive and ADR-negative tweets in the training set is the same as in the test set. This approach helps to ensure that the model's performance on the test set is a reliable indicator of its performance on new, unseen data.\n\nThe distribution of ADR-positive and ADR-negative tweets in this dataset is comparable to previously published machine learning datasets in the field of adverse drug reaction detection from social media. The imbalance between ADR-positive and ADR-negative tweets is a common challenge in this domain, and the use of stratified splitting helps to mitigate potential biases that could arise from this imbalance. Additionally, techniques such as oversampling of positive cases and dropout regularization were employed during model training to address class imbalance and control model variance.",
  "dataset/availability": "The data used in this study is available in a public forum. The labeled ADHD dataset can be accessed at the following GitHub repository: [https://github.com/chop-dbhi/twitter-adr-blstm](https://github.com/chop-dbhi/twitter-adr-blstm). The dataset includes tweets that were collected and labeled for the presence of adverse drug reactions (ADRs).\n\nThe availability of the data was enforced by adhering to the Twitter developer agreement, which allows authors to publish tweet IDs but not the actual text of the tweets obtained from the Twitter APIs. Therefore, the dataset includes tweet IDs and their corresponding labels, but not the tweet text itself.\n\nThe ASU ADR Dataset, which is part of the combined dataset used in this study, was previously published and made publicly available. This dataset contains tweets with at least one keyword corresponding to a list of brand and generic drugs. The annotations and tweet IDs for this dataset are available, but only a subset of the tweets were accessible through the Twitter API at the time of the study, as Twitter does not store tweets indefinitely.\n\nThe complete dataset used in this study was split into stratified training (80%) and test (20%) sets. The training set contains 7401 tweets, with 669 containing an ADR and 6732 labeled as containing no ADR. The test set contains 1851 tweets, with 168 labeled as containing an ADR and 1683 labeled as containing no ADR. This split ensures that the dataset is representative and can be used for training and evaluating models.",
  "optimization/algorithm": "The machine-learning algorithm class used is convolutional neural networks (ConvNets). This class of algorithms is well-established and widely used in various domains, particularly in image and text processing tasks.\n\nThe specific ConvNet model implemented is not a new algorithm but rather an application of existing techniques tailored to the task at hand. The model incorporates several standard practices to enhance performance and mitigate overfitting, such as dropout regularization and oversampling of positive cases to address class imbalance.\n\nThe choice to publish this work in a healthcare informatics journal rather than a machine-learning journal is likely due to the specific application and dataset used. The focus of the study is on detecting adverse drug reactions (ADRs) from tweets, which falls within the domain of healthcare informatics. The journal \"Journal of Healthcare Informatics Research\" is a suitable venue for presenting the findings, as it caters to an audience interested in the intersection of healthcare and data science. The primary contribution of this work lies in the application of ConvNets to a specific healthcare problem, rather than the development of a new machine-learning algorithm.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "For the convolutional neural network (ConvNet) model, the data encoding process involved creating word embedding representations for each tweet. Word embeddings are vectors of real values that represent words, maintained in an embedding matrix. The dimensionality of the word representation (M) and the number of words in the vocabulary (N) define the size of this matrix. Each column in the matrix corresponds to a single word.\n\nTo prepare a tweet for input into the model, the tweet was first tokenized into its constituent words, maintaining the order of appearance. This resulted in a list of tokens. Each token was then represented as a row in a matrix, with the values copied from the corresponding column in the embedding matrix. If a word was not present in the embedding matrix, it was ignored. This process created a variable-length input matrix for each tweet, where the number of rows (D) corresponded to the number of tokens in the tweet.\n\nThe ConvNet model was designed to handle these variable-length inputs. The model utilized a collection of kernels to approximate the mathematical convolution operator, applying these kernels to the input matrix to extract local features. These features were then used to classify the tweet relative to the presence of an adverse drug reaction (ADR).\n\nAdditionally, dropout regularization was applied to control model variance and prevent overfitting. This involved generating a uniform random value for each element in the input vector and setting the element to zero if the random value exceeded the dropout keep probability. This process is equivalent to removing the corresponding convolution kernel for the given training sample.\n\nTo address class imbalance, oversampling of positive cases was employed. Each training batch was formed by randomly selecting half of the samples from the ADR-positive tweets and half from the ADR-negative tweets, ensuring that each batch was balanced.\n\nThe training process was conducted in two phases. The first phase involved a grid search over various parameter combinations, evaluated using k-fold cross-validation with k=5. This phase aimed to select optimal hyperparameters such as the number and size of the kernels, the number of training epochs, and the dropout keep probability. Once the optimal parameters were identified, the model was trained on the entire training set. The complete training process required approximately 270 hours on a compute server with 72 2.3 GHz Intel Xeon CPUs and 1 TB RAM.",
  "optimization/parameters": "In our study, the best performing ConvNet model utilized 112,801 learning parameters. This number was determined through a systematic process involving a grid search over a range of parameter combinations. We evaluated 972 different parameter combinations using k-fold cross-validation, with k set to 5. This approach allowed us to identify the optimal set of hyperparameters, including the number and size of the kernels, the number of training epochs, and the dropout keep probability. The selection of these parameters was crucial for enhancing the model's performance and robustness.",
  "optimization/features": "The input features for the models varied depending on the specific model used. For the ConvNet model, the input features were derived from word embeddings, with the convolutional layer processing these embeddings using kernels of varying widths. The specific number of features is not explicitly stated, but it involves the word embeddings and the convolutional operations applied to them.\n\nFor the SVM models, two different sets of input features were used. The first SVM model, SVM-1, utilized n-gram features (unigrams, bigrams, and trigrams) and lexicon features. The n-gram dictionary consisted of 158,835 terms, and the lexicon contained 13,014 concepts. The second SVM model, SVM-2, used word embedding features derived from ADHD embeddings.\n\nFeature selection was implicitly performed through the use of specific feature types tailored to the task of ADR detection. For the SVM-1 model, the n-gram and lexicon features were selected based on previously published work and domain knowledge. The lexicon features were derived from a combination of an ADR lexicon and selected concepts from the Consumer Health Vocabulary that corresponded to adverse events associated with commonly used ADHD medications.\n\nThe feature selection process for the SVM models involved creating an Apache Lucene index from the ADR lexicon concept terms and querying this index for concept matches within each tweet. This process ensured that only relevant features were considered for model training.\n\nFor the ConvNet model, feature selection was not explicitly mentioned, but the use of word embeddings and convolutional operations inherently involves a form of feature extraction and selection. The word embeddings capture semantic information, and the convolutional layers focus on local patterns within the text, effectively selecting relevant features for the classification task.\n\nThe feature selection process for both SVM models was performed using the training set only, ensuring that the models were evaluated on unseen data during the testing phase. This approach helps to prevent data leakage and ensures that the models generalize well to new, unseen data.",
  "optimization/fitting": "Our model, a convolutional neural network (ConvNet), contains 112,801 learning parameters, which is indeed a relatively large number compared to the training sample size. This discrepancy raises concerns about potential overfitting, where the model might perform well on training data but poorly on unseen data.\n\nTo mitigate overfitting, we implemented dropout regularization. This technique involves randomly setting a fraction of the input units to zero during training, which helps prevent the model from becoming too reliant on any single feature. Additionally, we used oversampling of positive cases to address class imbalance during training, ensuring that each training batch was balanced.\n\nTo evaluate and address overfitting, we analyzed learning curves. These curves plot a performance metric against the number of training samples for both training and validation sets. In the absence of bias and variance, both curves should approach the optimum metric value as the training set size increases. However, our learning curve for the ConvNet model showed a separation between the training and validation curves, indicating the presence of variance. This suggests that the model's performance could be improved with more labeled training samples.\n\nConversely, the learning curve also suggested the presence of bias, as both the training and validation curves failed to approach the optimal loss value of zero. This bias is likely due to the model's reliance on word embeddings with finite-sized convolution kernels, which may not capture long-range dependencies between words. To reduce this bias, additional features such as part-of-speech tags and lexicon information could be incorporated.\n\nIn summary, while our model has a large number of parameters relative to the training sample size, we have taken steps to mitigate overfitting through regularization techniques and balanced training batches. The learning curve analysis indicates that both variance and bias are present, suggesting areas for further improvement.",
  "optimization/regularization": "In our study, we implemented dropout regularization to address concerns about overfitting. This technique was used to control model variance by randomly setting a fraction of the input elements to zero during training. Specifically, for the ConvNet model, we generated a uniform random value for each element in the input vector to the fully connected softmax layer. If this value exceeded the dropout keep probability, the corresponding input element was set to zero. This process is equivalent to removing the convolution kernel that corresponds to that element for the given training sample.\n\nAdditionally, we employed oversampling of positive cases to handle class imbalance during training. Each training batch was formed by randomly selecting half of the samples from the available positive cases (samples containing an adverse drug reaction) and half from the negative samples, ensuring that each batch was balanced.\n\nThe training process involved two phases. The first phase focused on selecting model hyperparameters, such as the number and size of the kernels, the number of training epochs, and the dropout keep probability. We performed a grid search over various parameter combinations and evaluated each combination using k-fold cross-validation, with k=5. This thorough evaluation helped us identify the optimal parameters for our model.\n\nIn summary, dropout regularization and oversampling were key techniques used to prevent overfitting and manage class imbalance, respectively. These methods contributed to the robustness and performance of our ConvNet model.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we conducted a grid search over a range of variables to select model hyperparameters such as the number and size of the kernels, the number of training epochs, and the dropout keep probability. We evaluated 972 parameter combinations using a 5-fold cross-validation approach. The optimal parameters identified through this process were then used to train the model on the entire training set.\n\nThe complete training process required approximately 270 hours on a compute server equipped with 72 2.3 GHz Intel Xeon CPUs and 1 TB of RAM. This information provides a clear overview of the computational resources and time invested in the optimization process.\n\nRegarding the availability of model files and optimization parameters, the specific details about where these can be accessed or how they are licensed are not provided in the publication. Therefore, it is not clear whether the model files and optimization parameters are publicly available or under what terms they might be accessible.",
  "model/interpretability": "The models discussed in this publication, specifically the Convolutional Neural Network (ConvNet) and the Support Vector Machine (SVM) models, are generally considered black-box models. This means that their internal workings and decision-making processes are not easily interpretable by humans. The ConvNet, in particular, relies on complex neural network architectures and learned parameters that do not lend themselves to straightforward interpretation. Similarly, the SVM models, while providing a decision boundary, do not offer clear insights into how individual features contribute to the final classification.\n\nThe ConvNet model uses word embeddings and convolutional layers to process input data, which makes it challenging to trace back the specific reasons for a particular prediction. The model's performance is evaluated through metrics like F1-score, precision, and sensitivity, but these metrics do not provide a transparent view of the model's decision-making process. For example, the ensemble approach, which combines the predictions of multiple ConvNet models, further obscures the interpretability by aggregating decisions from different models.\n\nThe SVM models, on the other hand, provide a decision boundary in the feature space, but the features used (such as n-gram 1-hot vectors and lexicon features) do not offer a clear, human-understandable explanation for the model's predictions. The use of ADHD Embeddings in the SVM-2 model adds another layer of complexity, making it difficult to interpret how specific input features influence the output.\n\nIn summary, both the ConvNet and SVM models are black-box models, and their decision-making processes are not transparent. While these models demonstrate strong performance in detecting adverse drug reactions (ADRs) in tweets, their internal mechanisms remain opaque, making it challenging to provide clear examples of how they arrive at their predictions.",
  "model/output": "The model in question is a convolutional neural network (ConvNet) designed for classification tasks. Specifically, it is used to detect adverse drug reactions (ADRs) in tweets. The output of the model is a binary classification indicating whether a tweet contains an ADR mention or not. The final layer of the ConvNet is a softmax layer, which outputs probabilities for the two classes: ADR present or ADR not present. These probabilities are then used to make a binary decision based on a specified threshold, typically 0.5. The model's performance is evaluated using metrics such as accuracy, F1-score, precision, and sensitivity, which are calculated based on the model's predictions on a test set of tweets. The ConvNet model has shown superior performance compared to baseline support vector machine (SVM) models, particularly in terms of sensitivity and F1-score. This indicates that the ConvNet model is better at detecting the few tweets that contain ADR descriptions while maintaining a reasonable level of precision.",
  "model/duration": "The complete training process for our model required approximately 270 hours. This was conducted on a compute server equipped with 72 2.3 GHz Intel Xeon CPUs and 1 TB of RAM. The training involved two phases: the first phase focused on selecting optimal model hyperparameters through a grid search and k-fold cross-validation, evaluating 972 parameter combinations. The second phase involved training the model on the entire training set using the identified optimal parameters.",
  "model/availability": "The source code for our convolutional neural network (ConvNet) model implementation and training is publicly available. It can be accessed via GitHub at the following URL: https://github.com/chop-dbhi/twitter-adr-convnet. The repository contains the necessary files and instructions to replicate the model development and training process. The code is released under a permissive license, allowing for both academic and commercial use, with proper attribution. This ensures that other researchers and developers can build upon our work, fostering collaboration and innovation in the field.",
  "evaluation/method": "The evaluation of the models involved several steps to ensure robustness and statistical significance. Initially, a grid search with k-fold cross-validation (k=5) was performed to select optimal hyperparameters for the ConvNet model. This process evaluated 972 parameter combinations. For the SVM models, a similar grid search with k-fold cross-validation was conducted to address class imbalance.\n\nTo evaluate the ConvNet model's performance, ten separate training and evaluation runs were conducted. Each run involved random initialization of the model parameters and training on the same dataset. This approach helped to obtain statistical measures, including the mean and confidence intervals, for the performance metrics on the test set, which contained 1851 tweets, with 168 of them mentioning adverse drug reactions (ADRs).\n\nThe test set performance was compared across different models, including two SVM baselines and the ConvNet model. The ConvNet model demonstrated nearly 25% better F1-score compared to the SVM models. Statistical significance of these differences was assessed using McNemar's test, which yielded a maximum p-value of 4.6e-13, indicating clear statistical significance.\n\nAdditionally, the impact of class imbalance was addressed using oversampling for the ConvNet model and boosting for the SVM models. The performance metrics were significantly lower when these techniques were not applied, highlighting their importance. The ConvNet model was also evaluated with an ensemble approach, where the predicted class was determined by the majority vote of ten individually trained models. This ensemble model showed slightly better results than the average performance of the individual models.\n\nLearning curves were used to evaluate overfitting in the models. The ConvNet model's learning curve indicated the presence of variance, suggesting that additional labeled training samples could reduce overfitting. The curve also suggested the presence of bias, which might be mitigated by incorporating additional features such as part-of-speech tags and lexicon information.\n\nOverall, the evaluation method involved rigorous cross-validation, multiple training runs, and statistical tests to ensure the robustness and generalizability of the model performance.",
  "evaluation/measure": "In our evaluation, we reported several key performance metrics to assess the effectiveness of our models. These metrics include accuracy, F1-score, precision, and sensitivity. Accuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. The F1-score is the harmonic mean of precision and sensitivity, providing a single metric that balances both concerns. Precision indicates the proportion of true positive results among all positive results predicted by the model, while sensitivity (or recall) measures the proportion of true positive results among all actual positives.\n\nWe also evaluated the statistical significance of the differences in F1-scores between our models using McNemar's test. This test helps determine whether the differences in performance are statistically significant, ensuring that our findings are robust and not due to random chance.\n\nAdditionally, we considered the Receiver Operating Characteristic (ROC) curve and the Precision-Recall (PR) curve to provide a more comprehensive view of model performance. The ROC curve illustrates the trade-off between sensitivity and the false positive rate, while the PR curve is particularly informative for imbalanced datasets, as it shows the balance between precision and recall.\n\nThe metrics reported are representative of standard practices in the literature for evaluating machine learning models, especially in the context of imbalanced datasets. The use of F1-score, precision, and sensitivity is common in tasks where the class distribution is skewed, such as in detecting adverse drug reactions (ADRs) from tweets. The inclusion of ROC and PR curves further enhances the transparency and thoroughness of our evaluation, allowing for a detailed analysis of model performance across different thresholds.",
  "evaluation/comparison": "In our evaluation, we compared the performance of our convolutional neural network (ConvNet) model with two baseline support vector machine (SVM) models. The first SVM model, referred to as SVM-1, was trained using n-gram and lexicon features. The second SVM model, SVM-2, utilized inputs derived from ADHD Embeddings. These baseline models served as simpler alternatives to assess the effectiveness of our ConvNet approach.\n\nThe comparison was conducted on a test set containing 1851 tweets, with only 168 tweets mentioning adverse drug reactions (ADRs). This test set was never used during the training phase, ensuring an unbiased evaluation. For the ConvNet model, we performed ten training and evaluation runs with random parameter initializations to account for variability in performance due to randomness in the training process.\n\nThe results indicated that the ConvNet model outperformed both SVM models in terms of F1-score, with an average performance nearly 25% better. This superior performance was statistically significant, as confirmed by McNemar's test with a maximum p-value of 4.6e-13. The ConvNet model demonstrated a 133% increase in sensitivity compared to the SVM models, while only experiencing a 25% loss in precision. This suggests that the ConvNet model is more effective at detecting ADR mentions in tweets, even when they are rare, and maintains a comparable ability to control false positives.\n\nAdditionally, we evaluated the impact of addressing class imbalance on model performance. For the ConvNet model, oversampling was used to balance the training batches, while for the SVM models, boosting was employed. The results showed that these methods were crucial for achieving optimal performance, as models trained on the original unbalanced datasets performed significantly worse.\n\nIn summary, our evaluation involved a thorough comparison with simpler baseline models and demonstrated the superior performance of the ConvNet model in detecting ADR mentions in tweets.",
  "evaluation/confidence": "The evaluation of our models included a thorough assessment of performance metrics with confidence intervals. For the ConvNet model, we performed ten training and evaluation runs with random parameter initialization to obtain statistical measures, including mean and confidence intervals, for the performance parameters on the test set. This approach allowed us to provide a robust estimate of the model's performance and its variability.\n\nThe performance results indicated that the average ConvNet model performance was nearly 25% better in terms of F1-score compared to the SVM models. To assess the statistical significance of these differences, we used McNemar\u2019s test with exact binomial probability calculations. The maximum p-value between the ConvNet model and either of the SVM model predictions over the ten ConvNet training and evaluation runs was 4.6e-13. This extremely low p-value indicates a clear statistical significance in the model predictions, suggesting that the ConvNet model's superior performance is not due to chance.\n\nAdditionally, the learning curves for the ConvNet and SVM-2 models provided insights into the presence of bias and variance. The learning curve for the ConvNet model showed separation between the training and cross-validation curves in the limit of large training set size, indicating the presence of variance. This suggests that the model's performance could be improved with additional labeled training samples. The learning curves also highlighted the presence of bias, as both the training and validation curves failed to approach the optimal loss value of zero. This bias is likely due to the model's reliance on word embeddings with finite-sized convolution kernels, which may not capture information between words separated by distances longer than the kernel width.\n\nOverall, the evaluation confidence is high, supported by statistical significance tests and detailed performance metrics with confidence intervals. The results demonstrate that the ConvNet model is superior to the SVM models in detecting adverse drug reaction (ADR) mentions in tweets, with a notable increase in sensitivity and a manageable loss in precision.",
  "evaluation/availability": "The raw evaluation files for our study are not publicly available. However, the labeled ADHD dataset used for training and testing our models is accessible. This dataset can be found on GitHub at the following URL: [https://github.com/chop-dbhi/twitter-adr-blstm](https://github.com/chop-dbhi/twitter-adr-blstm). The source code for our convolutional neural network (ConvNet) model implementation and training is also available on GitHub at: [https://github.com/chop-dbhi/twitter-adr-convnet](https://github.com/chop-dbhi/twitter-adr-convnet). These resources should provide a comprehensive understanding of the data and methods used in our evaluation."
}