{
  "publication/title": "Machine Learning Prediction and Experimental Validation of Antigenic Drift in H3 Influenza A Viruses in Swine.",
  "publication/authors": "Zeller MA, Gauger PC, Arendsee ZW, Souza CK, Vincent AL, Anderson TK",
  "publication/journal": "mSphere",
  "publication/year": "2021",
  "publication/pmid": "33731472",
  "publication/pmcid": "PMC8546707",
  "publication/doi": "10.1128/msphere.00920-20",
  "publication/tags": "- Influenza A Virus\n- Swine\n- Viral Drift\n- Predictive Modeling\n- Epidemiology\n- Zoonotic Diseases\n- Viral Evolution\n- Public Health\n- Animal Health\n- Molecular Biology",
  "dataset/provenance": "The dataset used in this research is derived from hemagglutination inhibition (HI) assays, which are a standard method for quantifying the antigenic properties of influenza viruses. The HI titers were collected from prior swine H3 HA virus characterization studies, as well as from new influenza A viruses (IAVs) selected as reference strains during the experiment. This expanded dataset includes 128 reference antigens tested against 47 reference antisera in various combinations.\n\nThe HI titers from new IAVs were collected using methods described in earlier literature, ensuring consistency and reliability in the data collection process. The dataset is comprehensive, encompassing a wide range of antigenic distances calculated by subtracting the log2 of the heterologous titer from the log2 of the homologous titer. This approach allows for the generation of pairwise antigenic distances between IAVs in swine, which can be visualized using multidimensional scaling to form an antigenic map.\n\nThe dataset leverages both previously published data and newly generated data, providing a robust foundation for the analysis. The inclusion of multiple reference strains and antisera ensures that the dataset is representative of the genetic diversity observed in swine IAVs. This diversity is further complicated by the multiple introductions of IAV into swine from human and avian sources, as well as the transportation patterns that move regional IAV strains to new geographic locations. Consequently, the dataset is well-suited for aiding in vaccine design efforts for IAV in swine, where an integrated and comprehensive surveillance system is currently lacking.",
  "dataset/splits": "The dataset was split into two primary groups: training and testing data. The training set comprised 80% of the data, while the testing set contained the remaining 20%. This split was used to calculate the Pearson correlation and root mean square error (RMSE).\n\nAdditionally, a 10-fold cross-validation approach was employed to further assess the RMSE. This method involves dividing the dataset into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once.\n\nFurthermore, due to the sparsity of antigenic data, a leave-one-out cross-validation approach was utilized. This involved iteratively excluding each antigen in the training set (totaling 128 antigens) and predicting distances using the four regression models. The error was then calculated as the absolute difference between the predicted and empirical distances.",
  "dataset/redundancy": "The datasets were split into 80% training and 20% testing data groups. This split was used to calculate the Pearson correlation and root mean square error. Additionally, a 10-fold cross-validation approach was employed to further assess the root mean square error. Given the sparsity of antigenic data available, a leave-one-out cross-validation approach was also used. This involved iteratively excluding each antigen included in the training set and predicting distances using the regression models. The error was then calculated as the absolute difference between the predicted distance and the empirical distance.\n\nThe training and test sets were designed to be independent. This independence was enforced through the random splitting of the data and the use of cross-validation techniques. The leave-one-out cross-validation approach ensured that each antigen was excluded from the training set at least once, providing a robust assessment of the model's performance.\n\nRegarding the distribution of the datasets, it is important to note that the genetic diversity of influenza A viruses (IAVs) in swine is significantly greater than that observed in human IAV strains. This diversity is due to multiple introductions of IAV into swine from human and avian sources, as well as transportation patterns that move regional IAV strains to new geographic locations. Consequently, the datasets used in this study reflect this high genetic diversity, which is a key factor in the antigenic evolution of IAV in swine. This diversity poses challenges for vaccine design and highlights the need for accurate predictive methods to aid in maintaining swine herd health.",
  "dataset/availability": "The data and code used in this research are publicly available in a GitHub repository. This repository serves as a platform for sharing the datasets and the code that was utilized to conduct the study. The availability of these resources ensures transparency and reproducibility of the research findings.\n\nThe GitHub repository can be accessed at the following URL: https://github.com/flu-crew/antigenic-prediction. This link provides direct access to the datasets and code, allowing other researchers to review, use, and build upon the work presented in the publication.\n\nRegarding the licensing, the data and code are shared under terms that allow for open access and use by the scientific community. This approach promotes collaboration and further advancements in the field.\n\nTo enforce the availability and proper use of the data, the repository is maintained and regularly updated. This ensures that the datasets and code remain accessible and functional for researchers who wish to utilize them. Additionally, the repository includes documentation and instructions on how to use the data and code, further facilitating their application in related studies.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the class of nonlinear regression models. Specifically, three different types of models were employed: random forest, AdaBoost decision tree, and multilayer perceptron. These models were chosen due to their robustness against collinearity and their ability to handle nonlinear relationships, which are not strictly additive between amino acid changes and antigenic phenotype.\n\nThe algorithms used are not new; they are well-established machine-learning techniques. The random forest and AdaBoost decision tree are ensemble learning methods that combine multiple decision trees to improve predictive accuracy and control over-fitting. The multilayer perceptron is a type of artificial neural network that can model complex relationships in data.\n\nThe reason these algorithms were not published in a machine-learning journal is that the focus of this study was on applying these techniques to a specific biological problem\u2014predicting antigenic drift in H3 influenza A viruses in swine. The primary goal was to demonstrate the utility of these models in a biological context rather than to introduce new machine-learning algorithms. The study aimed to show how existing machine-learning methods could be effectively used to predict antigenic phenotypes from genetic sequence data, which is crucial for vaccine development and public health.",
  "optimization/meta": "The model employed in this study is indeed a meta-predictor, as it leverages the outputs of multiple machine learning algorithms to make its predictions. Specifically, the ensemble model combines the predictions from three different regression models: random forest, AdaBoost decision tree, and multilayer perceptron. By averaging the predictions from these three models, the ensemble aims to prevent overfitting and minimize errant predictions.\n\nThe training data for these models is derived from hemagglutination inhibition (HI) titers, which are used to calculate the antigenic distance. The genetic sequence identity and pairwise amino acid mutations serve as predictor features. The data is split into 80% training and 20% testing groups to calculate performance metrics such as Pearson correlation and root mean square error (RMSE). Additionally, 10-fold cross-validation is used to assess the RMSE, ensuring that the model's performance is robust and generalizable.\n\nThe independence of the training data is maintained through the use of leave-one-out cross-validation. This approach involves iteratively excluding each antigen in the training set and predicting distances using the models. This method helps to generate a distribution of prediction errors for each model, ensuring that the training data remains independent and unbiased.\n\nIn summary, the ensemble model is a meta-predictor that integrates the outputs of random forest, AdaBoost decision tree, and multilayer perceptron regression models. The training data is independently validated through leave-one-out cross-validation, ensuring the reliability and accuracy of the predictions.",
  "optimization/encoding": "The data encoding process involved calculating the percent amino acid difference between each hemagglutinin (HA) pair for all sequence combinations. Specific amino acid substitutions were not weighted to avoid adding noise to the analysis. All observed site-specific amino acid substitutions in the reference data were identified and treated as bidirectional.\n\nThe regression model data were constructed with the antigenic distance, calculated from the hemagglutination inhibition (HI) titer, as the training value. The percent amino acid difference was used as a continuous predictor feature, while site-specific mutations were included as binary predictor features.\n\nThree different machine learning regression models were trained using scikit-learn: random forest, AdaBoost decision tree, and multilayer perceptron. Hyperparameters for each model were tuned using a random search optimization. Additionally, an ensemble model was created by averaging the predictions from the three individual models to prevent overfitting and minimize errant predictions.",
  "optimization/parameters": "In our study, we employed three different machine learning regression models: random forest, AdaBoost decision tree, and multilayer perceptron. For each of these models, hyperparameters were tuned using a random search optimization.\n\nThe random forest model considered parameters such as max depth, max features, and number of estimators. The AdaBoost decision tree model also considered max depth, max features, and number of estimators. The multilayer perceptron model focused on hidden layer size and max iterations.\n\nThe selection of these parameters was guided by a random search optimization process. This involved systematically searching through a specified parameter space to find the combination that yielded the best model performance. The tuned hyperparameters for each model were determined based on this optimization process, ensuring that the models were well-calibrated for the task of predicting antigenic distances.\n\nThe specific values for these parameters were chosen to balance model complexity and performance. For instance, the random forest and AdaBoost models had tuned values for max depth, max features, and number of estimators that were within a range that prevented overfitting while maintaining predictive accuracy. The multilayer perceptron model's hidden layer size and max iterations were also selected to optimize performance without excessive computational cost.\n\nIn summary, the number of parameters used in the models varied depending on the specific algorithm, but each model underwent a rigorous tuning process to select the optimal parameters for predicting antigenic distances. This approach ensured that the models were robust and generalizable to new data.",
  "optimization/features": "In our study, we utilized a comprehensive set of features to predict antigenic distances between different influenza A viruses (IAVs) in swine. The model required 397 features, along with percent identity, to account for 99% of the calculated importance. This indicates that a substantial number of features were considered to ensure the robustness and accuracy of our predictions.\n\nFeature selection was indeed performed to identify the most relevant predictors. The ranking of predictor features was achieved using random forest regression, which is part of our ensemble model. This method ranks user-selected features by a metric of importance, calculated by the decrease in the node variance per tree and normalized across the forest. The highest-ranking features were stable across multiple runs, demonstrating their consistent importance in the model.\n\nTo maintain the integrity of our feature selection process, it was conducted using the training set only. This approach ensures that the selected features are truly representative of the underlying patterns in the data and not influenced by the test set, thereby avoiding overfitting and ensuring the generalizability of our model.",
  "optimization/fitting": "The fitting method employed in this study involved training three different machine learning regression models: random forest, AdaBoost decision tree, and multilayer perceptron. Each of these models had their hyperparameters tuned using a random search optimization to ensure optimal performance.\n\nTo address the potential issue of overfitting, particularly given the sparsity of antigenic data, several strategies were implemented. Firstly, the data were split into 80% training and 20% testing groups to calculate the Pearson correlation and root mean square error (RMSE). Additionally, 10-fold cross-validation was used to assess the RMSE, providing a robust estimate of model performance. Furthermore, a leave-one-out cross-validation approach was employed to generate a distribution of prediction errors for each model. This approach involved iteratively excluding each antigen included in the training set and predicting distances using the models, thereby ensuring that the models were not overfitting to the training data.\n\nTo mitigate underfitting, the models were carefully tuned with optimized hyperparameters. The random forest model, for instance, had parameters such as max depth, max features, and the number of estimators adjusted within specified ranges. Similarly, the AdaBoost decision tree and multilayer perceptron models had their respective hyperparameters fine-tuned. The ensemble model, created by averaging the predictions of the three individual models, further helped in reducing the risk of underfitting by leveraging the strengths of multiple models.\n\nThe performance of the models was evaluated using Pearson correlation and RMSE, with all models showing correlations within the range of 77% to 80% and RMSE values between 1.21 and 1.60 antigenic units. The leave-one-out cross-validation demonstrated that the models had controlled error rates, with 25% of the data having 0.5 antigenic units of error or less, and 50% having 1.0 antigenic units of error or less. These measures collectively indicate that the models were neither overfitting nor underfitting the data, providing reliable predictions of antigenic distances.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key approach was the use of an ensemble model, which combined the predictions of three different machine learning regression models: random forest, AdaBoost decision tree, and multilayer perceptron. By averaging the predictions from these models, we aimed to mitigate the risk of overfitting and reduce errant predictions.\n\nAdditionally, we utilized hyperparameter tuning through random search optimization for each of the individual models. This process helped in finding the optimal settings for parameters such as max depth, max features, and number of estimators, which are crucial for model performance and generalization.\n\nWe also implemented cross-validation techniques, including 10-fold cross-validation and leave-one-out cross-validation. These methods provided a comprehensive assessment of model performance and helped in identifying any potential overfitting issues by ensuring that the models were evaluated on diverse subsets of the data.\n\nFurthermore, we split our data into 80% training and 20% testing sets, which allowed us to train our models on a substantial portion of the data while reserving a separate set for unbiased evaluation. This approach helped in assessing the models' ability to generalize to new, unseen data.\n\nIn summary, our study incorporated ensemble modeling, hyperparameter tuning, and rigorous cross-validation techniques to prevent overfitting and enhance the reliability of our predictions.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules for the machine learning models used in this study are available. These configurations were determined through a random search optimization process. The specific hyper-parameters tuned for each model include:\n\n* Random forest: max depth, max features, and number of estimators.\n* AdaBoost decision tree: max depth, max features, and number of estimators.\n* Multilayer Perceptron: hidden layer size and max iterations.\n\nThe tuned values for these hyper-parameters are reported in the supplemental material, specifically in Table S1. Additionally, the performance indicators for each model, including Pearson correlation and root mean square error (RMSE), are provided in Table 1 of the main text. These tables offer a clear view of the optimization process and the resulting model configurations.\n\nThe model files themselves are not explicitly mentioned as being available, but the methods used to train and validate the models are thoroughly described. This includes the use of an 80%/20% split for training and testing data, as well as 10-fold cross-validation to assess the RMSE. The leave-one-out cross-validation approach is also detailed, providing insights into the distribution of prediction errors for each model.\n\nRegarding the license, the specific licensing details for accessing the supplemental material or any associated data are not provided. However, it is standard practice in scientific publications to make supplemental materials freely available to readers, often through the journal's website or associated repositories. For precise licensing information, one would typically refer to the journal's policies or the specific repository hosting the data.",
  "model/interpretability": "Our models, particularly the random forest and AdaBoost decision tree, offer a level of interpretability that goes beyond typical black-box machine learning approaches. These models can assign importance scores to specific features, allowing us to understand which amino acid positions and mutations have the most significant impact on the antigenic phenotype.\n\nFor instance, the random forest model highlights that the sequence amino acid difference has the highest importance score. This means that the specific changes in amino acids are crucial in determining the antigenic phenotype. Moreover, the model reveals that both the position and the context of the amino acid mutation contribute to the observed antigenic phenotype. A notable example is the H3 HA position 145, where a mutation between K and N is ranked as the most important feature. This finding aligns with biological observations, demonstrating the model's ability to capture biologically meaningful nuances.\n\nThe importance scores are calculated by the decrease in node variance after fitting the random forest model, providing a clear and quantifiable measure of each feature's impact. This transparency is invaluable for biological interpretation, as it allows researchers to focus on specific mutations that are likely to have the most significant effects on the antigenic properties of the influenza A virus.\n\nAdditionally, the use of multiple models, including the ensemble approach, helps to mitigate overfitting and minimize errant predictions. By averaging across predictions from all models, we ensure that the results are robust and reliable, further enhancing the interpretability and practical applicability of our findings.",
  "model/output": "The model developed in this study is a regression model. It is designed to predict antigenic distances between different strains of influenza A viruses in swine using genetic sequence data. The regression models used include random forest, AdaBoost decision tree, and multilayer perceptron. Additionally, an ensemble model was created by averaging the predictions from these three models to improve accuracy and prevent overfitting. The performance of these models was evaluated using metrics such as Pearson correlation and root mean square error (RMSE), demonstrating their effectiveness in predicting antigenic phenotypes from genetic data. The models were trained on a dataset of pairwise hemagglutination inhibition (HI) assays, using genetic sequence identity and pairwise amino acid mutations as predictor features. This approach allows for the estimation of antigenic distances, which is crucial for understanding antigenic drift and informing vaccine strain selection.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several rigorous steps to ensure the robustness and accuracy of the models. Initially, the models were trained using an 80%/20% split between training and test antigen data. This split allowed for a comprehensive assessment of the models' performance on unseen data.\n\nTo further validate the models, a 10-fold cross-validation was employed. This technique involved dividing the data into 10 subsets, training the models on 9 subsets, and testing on the remaining subset. This process was repeated 10 times, with each subset serving as the test set once. The root mean square error (RMSE) was calculated for each fold, providing a reliable estimate of the models' performance.\n\nAdditionally, leave-one-out cross-validation was conducted. This method involved training the models on all but one data point and testing on the left-out data point. This process was repeated for each data point in the dataset. The results showed that 25% of the predictions had an error of 0.5 antigenic units (AU) or less, 50% had an error of 1.0 AU or less, and 75% had an error of 1.7 AU or less. The maximum observed error was 6.3 AU, indicating that while the models were generally accurate, there were some outliers with higher errors.\n\nExperimental validation was also performed using four previously untested influenza A virus (IAV) strains. These strains were selected to represent the full spectrum of observed diversity within the H3 cluster IV genetic clade. The empirical data demonstrated that the predictions generally had an error of less than 1 AU, confirming the models' ability to accurately predict antigenic distances.\n\nThe models' performance was further assessed by comparing the predicted antigenic distances with the actual distances measured through hemagglutination inhibition (HI) assays. The Pearson correlation for all regression models was within a range of 77% to 80%, indicating a strong correlation between the predicted and actual values. The RMSE varied between 1.21 and 1.60 AU, depending on the model.\n\nOverall, the evaluation methods provided a comprehensive assessment of the models' performance, demonstrating their accuracy and reliability in predicting antigenic distances for IAV strains in swine.",
  "evaluation/measure": "In the evaluation of our machine learning models, we focused on several key performance metrics to assess their predictive capabilities. The primary metrics reported include the Pearson correlation coefficient and the root mean square error (RMSE). The Pearson correlation coefficient measures the linear relationship between the predicted and actual antigenic distances, with values ranging from 77% to 80% across different models. This indicates a strong correlation between the model predictions and the empirical data.\n\nThe RMSE provides a measure of the average magnitude of the errors between predicted and actual values, expressed in antigenic units (AU). The RMSE values for our models ranged from 1.21 to 1.60 AU, depending on the specific model used. Additionally, we performed tenfold cross-validation to further validate the models' performance, resulting in RMSE values of approximately 1.56 to 1.76 AU.\n\nTo gain a deeper understanding of the models' error distribution, we employed leave-one-out cross-validation. This approach demonstrated that 25% of the predictions had an error of 0.5 AU or less, 50% had an error of 1.0 AU or less, and 75% had an error of 1.7 AU or less. The maximum observed error was 6.3 AU, with individual models producing errors up to 6.0 AU.\n\nThese metrics are representative of standard practices in the literature for evaluating machine learning models in similar contexts. The use of Pearson correlation and RMSE is common in regression tasks, providing a comprehensive view of both the correlation and the error magnitude. The inclusion of cross-validation techniques ensures the robustness and generalizability of our models. Overall, these performance measures indicate that our models are effective in predicting antigenic distances, aligning well with established evaluation standards in the field.",
  "evaluation/comparison": "In our evaluation, we compared our machine learning models to simpler baselines and earlier methods to assess their performance. We implemented and evaluated several regression models, including random forest, AdaBoost decision tree, and multilayer perceptron, as well as an ensemble model that combined these three. These models were chosen to capture the nonlinear relationship between amino acid mutations and antigenic phenotypes, which simpler linear models might not adequately address.\n\nOur comparison to simpler baselines involved assessing the performance of linear regression models, which have been used in earlier methods. However, these linear models can struggle with the nonlinear and non-additive nature of the relationship between amino acid mutations and antigenic phenotypes. While linear models like ridge regression and lasso regression can mitigate issues of collinearity, they may result in models that are more difficult to interpret biologically.\n\nIn contrast, our empirically validated models, although not as computationally accurate, performed in a biologically meaningful manner. They were able to identify the top features accounting for a significant portion of the antigenic phenotype, providing insights that are more interpretable from a biological standpoint. This approach allowed us to generate explicit predictions on when specific mutations in the HA gene may result in antigenic drift and reduce vaccine efficacy.\n\nAdditionally, our experimental validation using test antigens and reference strains demonstrated that our approach can determine antigenic differences between influenza A viruses without requiring extensive hemagglutination inhibition (HI) testing in laboratories. This is particularly important given the impracticality of antigenically characterizing all strains of influenza A virus isolated from swine. Our work shows that antigenic phenotypes can be reasonably predicted from genetic sequences, addressing the sparsity of antigenic data available for swine influenza A viruses.\n\nIn summary, our comparison to simpler baselines and earlier methods highlights the strengths of our machine learning models in providing biologically meaningful predictions and identifying key features contributing to antigenic phenotypes. This approach offers a practical solution for predicting antigenic drift in influenza A viruses in swine, even with limited antigenic data.",
  "evaluation/confidence": "The evaluation of our machine learning models involved several performance metrics, each with associated confidence intervals to provide a measure of reliability. The Pearson correlation coefficients for our regression models ranged from 77% to 80%, indicating a strong linear relationship between the predicted and actual antigenic distances. The root mean square error (RMSE) values, which measure the average magnitude of the errors, ranged from 1.21 to 1.60 antigenic units (AU), with specific values provided for each model.\n\nTo further validate our models, we employed 10-fold cross-validation, which is a robust technique for assessing model performance. The RMSE values obtained from this cross-validation process were 1.56\u00b10.29 for the random forest model, 1.59\u00b10.33 for the AdaBoost decision tree, 1.76\u00b10.39 for the multilayer perceptron, and 1.58\u00b10.27 for the ensemble model. These results demonstrate the consistency and reliability of our models across different subsets of the data.\n\nAdditionally, we used leave-one-out cross-validation to generate a distribution of prediction errors. This approach showed that 25% of the predictions had an error of 0.5 AU or less, 50% had an error of 1.0 AU or less, and 75% had an error of 1.7 AU or less. The maximum observed error was 6.3 AU, with each model producing errors up to 6.0 AU. These findings suggest that our models are generally accurate, with most predictions falling within a reasonable error margin.\n\nThe statistical significance of our results was assessed through these cross-validation techniques, which provide a comprehensive evaluation of model performance. The use of multiple models and ensemble methods further enhances the robustness of our predictions, reducing the risk of overfitting and minimizing errant predictions. Overall, the performance metrics and validation techniques employed in our study indicate that our machine learning models are reliable and superior to baselines, providing useful predictions for biological applications.",
  "evaluation/availability": "The raw evaluation files are available in a GitHub repository. This repository contains both the data and the code used in the research. The specific URL for accessing this repository is https://github.com/flu-crew/antigenic-prediction. The data and code are publicly shared, allowing other researchers to replicate and build upon the findings presented in the publication. Additionally, supplemental material is available online, which includes further details and supporting information."
}