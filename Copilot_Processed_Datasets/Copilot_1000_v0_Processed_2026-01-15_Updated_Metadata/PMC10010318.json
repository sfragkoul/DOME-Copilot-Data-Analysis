{
  "publication/title": "Brain Neural Progenitors are New Predictive Biomarkers for Breast Cancer Hormonotherapy.",
  "publication/authors": "Basseville A, Cordier C, Ben Azzouz F, Gouraud W, Lasla H, Panloup F, Campone M, J\u00e9z\u00e9quel P",
  "publication/journal": "Cancer research communications",
  "publication/year": "2022",
  "publication/pmid": "36923306",
  "publication/pmcid": "PMC10010318",
  "publication/doi": "10.1158/2767-9764.crc-21-0090",
  "publication/tags": "- Breast Cancer\n- Hormonotherapy\n- Machine Learning\n- Biomarkers\n- Tumor Microenvironment\n- Neural Progenitors\n- Transcriptomics\n- Predictive Signatures\n- Treatment Resistance\n- Prognostic Factors",
  "dataset/provenance": "The datasets used in this study were sourced from publicly available repositories, specifically the Gene Expression Omnibus (GEO) and cBioPortal. These datasets included transcriptomic analyses of tumors performed prior to any treatment, with known relapse statuses. The primary focus was on microarray-derived transcriptomic data due to the lack of publicly available RNA sequencing datasets associated with relapse response information.\n\nThe study utilized a total of 21 datasets comprising 3,940 patients. These datasets were filtered to include only those with more than 10,000 quantified genes, at least 50 patients per cohort, and individual relapse information. This filtering process resulted in a subset of 1,480 breast cancer patients treated with hormonotherapy, with relapse information available for all of them. Further refinement led to a balanced dataset of 568 patients treated with hormonotherapy, equally divided between those who relapsed and those who did not.\n\nSeveral datasets were used for external validation, including the Buffa cohort (Illumina microarray), Chanrion cohort (noncommercial microarray), and Saal cohort (Illumina RNA-seq) from GEO, as well as the METABRIC dataset (Illumina) from cBioPortal. Additionally, the Juin cohort (Affymetrix) from the Institut de Canc\u00e9rologie de l\u2019Ouest (ICO) and the ICO triple-negative breast cancer (TNBC) cohort were included. The Guedj cohort (Affymetrix) was initially used for normalization but was not included in the training cohort due to the lack of individual clinical information.\n\nThe datasets were normalized using the RMA method and cross-platform normalization algorithms, including remove batch effect (limma), ComBat (sva), and quantile normalization (preprocessCore). The best-performing algorithm was selected based on performance metrics such as gPCA, batch correlation with principal component analysis, and the Kolmogorov-Smirnov test on gene distribution. This rigorous selection and normalization process ensured the reliability and comparability of the datasets used in the study.",
  "dataset/splits": "In our study, we utilized multiple data splits to ensure robust validation of our models. Initially, we gathered a total of 3,940 patients from 21 datasets. From this combined cohort, we focused on patients treated by hormonotherapy with known relapse status, resulting in 1,480 patients. Among these, 350 had experienced relapse, while 1,130 were relapse-free.\n\nTo address the class imbalance, we employed resampling techniques. This led to the creation of two balanced datasets: one using ADASYN oversampling and another using random undersampling. The undersampled dataset consisted of 568 patients, with an equal number of relapse and relapse-free cases (285 and 283, respectively).\n\nFor external validation, we used seven datasets. These included the Buffa cohort, Chanrion cohort, Saal cohort, Guedj cohort, Juin cohort, METABRIC dataset, and the ICO TNBC cohort. Each of these cohorts had varying numbers of patients and was used to assess the prognostic value of our gene expression signatures.\n\nIn summary, our dataset splits included an initial combined cohort of 3,940 patients, a focused cohort of 1,480 hormonotherapy-treated patients, a balanced undersampled dataset of 568 patients, and seven external validation cohorts.",
  "dataset/redundancy": "The datasets were split into training and external validation sets to ensure independence. The training set consisted of 3,940 patients from 21 studies, focusing on those treated with hormonotherapy and having known relapse status. This set was used to develop and internally validate the models. To mitigate batch effects, cross-platform normalization using ComBat was applied.\n\nExternal validation was performed on seven independent cohorts, including the Buffa, Chanrion, Saal, Guedj, Juin, METABRIC, and ICO TNBC cohorts. These cohorts were chosen to assess the prognostic value of the gene expression signatures (GES) developed. The external validation cohorts were selected based on their availability and the presence of transcriptomic data and relapse status information. The Juin cohort, for instance, included patients treated at the Institut de Canc\u00e9rologie de l\u2019Ouest (ICO) with follow-up data until 2021.\n\nThe distribution of the datasets compares favorably with previously published machine learning datasets in breast cancer research. The use of multiple external validation cohorts helps to ensure that the models are generalizable and not overfitted to the training data. The selection criteria for the datasets, including the requirement for more than 10,000 quantified genes and at least 50 patients per cohort, helped to maintain a high standard of data quality. Additionally, the focus on patients treated with hormonotherapy and the known relapse status ensured that the models were developed on clinically relevant data.",
  "dataset/availability": "The datasets used in this study were created using publicly available data. The transcriptomic analysis of the tumors was performed prior to any treatment, and the relapse status of the patients is known. The bc-GenExMiner v4.5 online tool was used to select clinically annotated datasets that were merged. The patient characteristics and inclusion criteria are detailed in Supplementary Table S2.\n\nThe datasets were obtained from various sources, including Gene Expression Omnibus (GEO) and cBioPortal. Specifically, the Buffa cohort, Chanrion cohort, and Saal cohort were downloaded from GEO. The METABRIC dataset was downloaded from cBioPortal. The Juin cohort consists of patients with breast cancer treated at the Institut de Canc\u00e9rologie de l\u2019Ouest (ICO), and the transcriptomic data were previously published and deposited on GEO. The ICO triple-negative breast cancer (TNBC) cohort includes patients treated at the ICO for whom transcriptomic analysis of the tumors was performed prior to treatment.\n\nThe data splits used in the study were determined by the availability of public datasets. Seven external validation cohorts were used to assess the prognostic value of the gene expression signature (GES). These cohorts included the Buffa cohort, Chanrion cohort, Saal cohort, Guedj cohort, Juin cohort, METABRIC dataset, and the ICO TNBC cohort. The Guedj cohort was initially included in the merged Affymetrix cohort for normalization but was not included in the training cohort during the machine learning (ML) analysis. Instead, the non-normalized dataset was used as an external validation.\n\nThe data, including the data splits, are not explicitly released in a public forum as part of this publication. However, the sources from which the data were obtained are publicly available, and the specific datasets used can be accessed through GEO and cBioPortal. The use of these public datasets ensures that the data is accessible to other researchers for validation and further study. The inclusion criteria and patient characteristics are provided in the supplementary materials, allowing for transparency and reproducibility.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and widely recognized in the field. The algorithms employed include elastic net, k-nearest neighbors, logistic regression, neural networks, random forest, support vector machines, variable selection using random forests, and extreme gradient boosting. These algorithms are part of the broader supervised learning framework, which is commonly used for classification tasks in various domains, including biomedical research.\n\nNone of the algorithms used are new; they have been extensively studied and applied in numerous research and practical applications. The choice of these algorithms was driven by their proven effectiveness in handling high-dimensional data and their ability to capture complex patterns in the data.\n\nThe decision to use these established algorithms in a biomedical context, rather than a machine-learning journal, is due to the specific focus of the study. The primary goal was to develop predictive models for treatment response in breast cancer patients, particularly those undergoing hormonotherapy. The study leverages these algorithms to identify important genes linked to relapse after hormonotherapy, which is a critical aspect of personalized medicine and cancer treatment.\n\nThe algorithms were optimized using various parameters to enhance their performance. For instance, elastic net was tested with different alpha values and lambda parameters, while k-nearest neighbors were evaluated with varying numbers of neighbors. Logistic regression, neural networks, random forest, support vector machines, variable selection using random forests, and extreme gradient boosting were also fine-tuned with specific parameters to achieve the best predictive performance.\n\nThe study's findings indicate that the models developed using these algorithms showed promising results, with median internal validation Matthews correlation coefficients (MCC) ranging from 0.15 to 0.29. The external validation performance was comparable or better than the internal cohort prediction for the undersampled dataset, suggesting that the models were not overfitted. This highlights the robustness and generalizability of the models in predicting treatment response in breast cancer patients.",
  "optimization/meta": "The models developed for response to treatment prediction did not use data from other machine-learning algorithms as input. Instead, they were built using various machine-learning algorithms independently. These algorithms included elastic net, k-nearest neighbors, logistic regression, neural network, random forest, support vector machine, variable selection using random forest, and extreme gradient boosting.\n\nEach of these algorithms was tested with different configurations, including various numbers of variables and specific parameters tailored to each method. For instance, the elastic net algorithm was tested with different alpha values and lambda parameters, while the k-nearest neighbors algorithm varied the number of neighbors (k) and the number of variables. Similarly, other algorithms had their unique sets of parameters optimized for performance.\n\nThe performance of these models was evaluated using internal cross-validation and external validation on different cohorts. The best-performing models for each algorithm were selected based on their median internal validation Matthews correlation coefficient (MCC) and were further analyzed for their predictive capabilities.\n\nThe focus was on ensuring that the models were not overfitted, particularly noting that undersampling methods generally performed better in external validation compared to oversampling methods. This suggests that the models were robust and generalizable to new data.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithms involved several steps to ensure the quality and compatibility of the datasets. Initially, we focused on selecting datasets with at least 10,000 quantified genes and a minimum of 50 patients per cohort. We specifically chose datasets from the Affymetrix platform to minimize bias due to different manufacturers. The raw data underwent normalization using the Robust Multi-array Average (RMA) method from the simpleaffy package.\n\nTo combine datasets, probes were transformed into HUGO gene symbols and aggregated by median. This process reduced the data to 12,659 genes that were shared across all Affymetrix platforms. Three cross-platform normalization algorithms were tested: remove batch effect (limma), ComBat (sva), and quantile normalization (preprocessCore). The best performer was selected based on performance metrics estimated by gPCA, batch correlation with principal component analysis (FactoMineR), and the Kolmogorov-Smirnov test (BEclear) on gene distribution.\n\nAfter normalization, the machine learning analysis was performed on transcriptomic data for patients who received hormonotherapy and for whom relapse status was known. Survival analysis was conducted on patients with known relapse-free survival (RFS). Due to the lack of publicly available RNA sequencing datasets associated with relapse response information, only microarray-derived transcriptomic data were used. The final dataset included 3,940 patients from 21 studies, with a focus on 1,480 patients treated by hormonotherapy, of which 350 had relapse and 1,130 were relapse-free.",
  "optimization/parameters": "In our study, we explored various algorithms with different numbers of variables, specifically 50, 400, 1000, 2000, or 5000. The selection of the optimal number of variables was a crucial step in our model optimization process. For each algorithm, we tested these different variable counts to determine which provided the best performance. The chosen number of variables for each algorithm was based on the median performance across internal and external validations, ensuring robustness and generalizability of our models. This approach allowed us to balance between capturing sufficient information and avoiding overfitting. For instance, the elastic net algorithm performed best with 5000 variables, while the k-nearest neighbors algorithm showed optimal results with 1000 variables. This systematic evaluation ensured that our models were well-tuned and capable of making accurate predictions.",
  "optimization/features": "In our study, we employed a range of feature selection options to minimize the dimensionality of the high-dimensional dataset. Specifically, we tested five different numbers of variables: 50, 400, 1000, 2000, and 5000. This approach allowed us to evaluate the impact of varying the number of input features on model performance.\n\nFeature selection was indeed performed, and it was conducted using the training set only. This ensures that the selection process does not introduce any bias from the validation or test sets. The selection of important genes linked to relapse after hormonotherapy was carried out for all models except the Support Vector Machine (SVM) model, due to the lack of accessibility to important variables in that specific algorithm.\n\nFor each retained algorithm model, candidate genes were selected if they were among the top 200 ranked important variables in at least three out of five cross-validation folds. This rigorous selection process helps in identifying robust and reliable features that contribute significantly to the predictive performance of the models.",
  "optimization/fitting": "In our study, we addressed the challenge of high-dimensional data where the number of parameters is much larger than the number of training points. To mitigate overfitting, we employed several strategies. Firstly, we used resampling methods to balance the class distribution, specifically focusing on undersampling the majority class. This approach helped in reducing the bias towards the majority class and improved the model's generalization capability. We tested two undersampling methods: randomly picking samples while conserving batch proportions and using cluster centroids based on k-means clustering. The efficiency of these methods was evaluated by comparing the prediction performance of a random forest algorithm on both the majority and minority classes using 5-fold cross-validation.\n\nAdditionally, we performed internal validation using cross-validation and external validation on separate cohorts. The external validation results were within the same range or better than the internal cohort predictions for the undersampled dataset, indicating that our models were not overfitted. In contrast, oversampling methods like ADASYN increased the overfitting bias, which was evident from the validation results.\n\nTo ensure that our models were not underfitted, we selected the best performer for each algorithm in the undersampled dataset and extracted the important genes linked to relapse after hormonotherapy. This process involved testing different options, including two balanced datasets (ADASYN oversampling and random undersampling), eight predictive algorithms, and five choices of feature selection numbers. By optimizing these parameters, we aimed to achieve a balance between bias and variance, thereby avoiding underfitting.\n\nFurthermore, we compared our results with those obtained by the MAQC2 consortium for event-free survival as an endpoint. Our models had median internal validation Matthews correlation coefficients (MCC) between 0.15 and 0.29, which were within the same range as the MAQC2 consortium's results. This comparison provided additional confidence in the robustness of our models.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was resampling, which involved both oversampling and undersampling techniques. Oversampling methods like ADASYN and SMOTE were tested to balance the class distribution by generating synthetic samples for the minority class. However, we found that oversampling tended to increase overfitting bias. On the other hand, undersampling methods, such as random undersampling, were more effective in maintaining model performance without overfitting. We specifically selected the best-performing models from the undersampled dataset, which included 568 patients, and observed that the external validation performance was in the same range or better than the internal cohort prediction. This suggests that our models were not overfitted.\n\nAdditionally, we utilized cross-validation as an internal validation technique to assess the performance of our models. This involved splitting the data into multiple folds and training the model on different subsets while validating on the remaining data. This process helped in evaluating the model's generalization capability and reducing the risk of overfitting.\n\nFurthermore, we optimized the number of variables selected for each algorithm, which is a form of regularization. By testing different variable selection options, we aimed to minimize the feature set and improve the model's performance. This approach helped in reducing the complexity of the models and preventing overfitting.\n\nIn summary, our study incorporated resampling techniques, cross-validation, and variable selection optimization to prevent overfitting and ensure the reliability of our predictive models.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters for various algorithms are reported in detail. These configurations include specific settings for algorithms such as elastic net, k-nearest neighbors, logistic regression, neural networks, random forests, support vector machines, variable selection using random forests, and extreme gradient boosting. For each algorithm, the tested parameters and the selected parameters for the undersampled dataset are clearly outlined. For instance, for the elastic net algorithm, the alpha parameter ranges from 0 to 1 in increments of 0.1, and the lambda parameter is set to lambda.min. The variable selection options include 50, 400, 1000, 2000, or 5000 variables. Similar detailed configurations are provided for other algorithms, ensuring reproducibility.\n\nThe optimization schedule and model files are not explicitly mentioned as being available for download. However, the performance metrics and validation results are thoroughly documented. The best models were selected based on median internal validation Matthew's correlation coefficient (MCC) and were further validated using external cohorts. The performance of these models was found to be within a comparable range to other studies, indicating robust optimization.\n\nRegarding the availability and licensing of these configurations, it is not specified in the provided information. However, the detailed reporting of hyper-parameter settings and optimization parameters suggests a commitment to transparency and reproducibility. Researchers interested in replicating or building upon this work would have sufficient information to do so, assuming the necessary datasets and tools are accessible.",
  "model/interpretability": "The models developed for response to treatment prediction in breast cancer are not entirely black-box systems. While some algorithms, like neural networks and support vector machines, are inherently more complex and less interpretable, efforts were made to enhance the transparency of the models.\n\nFor several algorithms, the important genes linked to relapse after hormonotherapy were extracted. This was possible for all models except the support vector machine (SVM) model, due to the lack of accessibility to important variables in that specific case. The genes were selected based on their representation among the top 200 ranked important variables in at least three of five cross-validation folds. This approach allows for a degree of interpretability, as it identifies specific genes that are significant in predicting treatment response.\n\nAdditionally, enrichment analysis was performed using ToppFun from the ToppGene suite on the candidate genes from the selected models. This analysis helped to identify significant enriched pathways, which were then classified into subcategories using keyword searches. This process provides insights into the biological pathways and processes that are associated with the predictive models.\n\nFurthermore, transcription factor analysis was conducted to understand the regulatory mechanisms underlying the identified genes. Genes were ranked based on a t-test between patients with relapse versus those without relapse, and gene set enrichment analysis (GSEA) was performed for transcription factor enrichment. This analysis helps to elucidate the transcriptional regulatory networks that are involved in treatment response.\n\nIn summary, while the models incorporate complex algorithms, steps were taken to enhance their interpretability by identifying important genes, performing enrichment analysis, and conducting transcription factor analysis. These efforts provide a clearer understanding of the biological mechanisms underlying the predictive models.",
  "model/output": "The model developed is primarily a classification model designed to predict treatment response, specifically relapse in patients undergoing hormone therapy for breast cancer. The classification task involves determining whether a patient will experience relapse or remain relapse-free. The performance of the model was evaluated using the Matthews correlation coefficient (MCC), which is a measure suitable for classification problems, particularly when dealing with imbalanced datasets.\n\nSeveral machine learning algorithms were employed, including elastic net, k-nearest neighbors, logistic regression, neural networks, random forest, support vector machines, variable selection using random forest, and extreme gradient boosting. Each algorithm was tested under different conditions, such as varying the number of variables selected and tuning hyperparameters to optimize performance.\n\nThe model's performance was assessed through internal validation using cross-validation and external validation on three separate cohorts. The best-performing models had median internal validation MCC values ranging from 0.15 to 0.29, depending on the algorithm used. These results were comparable to those obtained by the MAQC2 consortium for event-free survival endpoints in other cancer types.\n\nImportant genes linked to relapse after hormone therapy were identified from the best-performing models in the undersampled dataset. These genes were further analyzed for enrichment in various biological pathways, particularly those related to the nervous system. The final output of the model includes a gene expression signature (GES) that can be used to predict relapse in patients undergoing hormone therapy for breast cancer. This signature was derived from the combined analysis of multiple models and validated through both internal and external datasets.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the algorithms used in this study is not explicitly released as part of this publication. However, several R packages were utilized, which are publicly available and can be accessed through their respective repositories. These packages include tools for differential expression analysis, variable selection, machine learning algorithms, and data preprocessing. For instance, the `limma` package for differential expression analysis and the `randomForest` package for classification and regression are both available on CRAN. Similarly, other packages like `xgboost` for gradient boosting and `caret` for building predictive models are also publicly accessible.\n\nWhile the specific implementations and configurations used in this study are not provided as a standalone executable or web server, the underlying methods and algorithms are well-documented and can be replicated using the mentioned R packages. These packages are licensed under open-source licenses, allowing for free use, modification, and distribution. Researchers interested in replicating or extending the work can refer to the documentation and examples provided with these packages.\n\nFor those looking to run the algorithms without delving into the code, it may be possible to use virtual machines or container instances that include the necessary R packages and configurations. However, this would require additional setup and is not provided as part of this publication.",
  "evaluation/method": "The evaluation of the method involved a comprehensive approach to ensure the robustness and generalizability of the models. Initially, internal validation was performed using cross-validation to assess the models' performance within the training dataset. This step was crucial for identifying the best-performing models and understanding their behavior on unseen data within the same cohort.\n\nIn addition to internal validation, external validation was conducted using several independent cohorts. These cohorts included the Buffa, Chanrion, Saal, Guedj, Juin, METABRIC, and ICO TNBC datasets. The external validation cohorts provided a real-world assessment of the models' predictive performance, ensuring that they were not overfitted to the training data. The performance metrics, particularly the Matthews correlation coefficient (MCC), were evaluated for both internal and external validation to compare the models' consistency and reliability.\n\nThe evaluation also considered the impact of different resampling techniques on model performance. Both oversampling and undersampling methods were tested to address class imbalance in the dataset. It was observed that undersampling methods generally led to better performance and reduced overfitting bias compared to oversampling techniques. This insight was critical in selecting the most appropriate resampling strategy for the final models.\n\nFurthermore, the evaluation included a comparison with the results obtained by the MAQC2 consortium for event-free survival as an endpoint. The models developed in this study showed performance metrics within the same range as those reported by the consortium, validating the effectiveness of the approach.\n\nOverall, the evaluation method combined internal and external validation, along with a thorough assessment of resampling techniques, to ensure the models' reliability and generalizability in predicting relapse after hormonotherapy in breast cancer patients.",
  "evaluation/measure": "In our study, we primarily used the Matthews Correlation Coefficient (MCC) to evaluate the performance of our models. MCC is a balanced measure that takes into account true and false positives and negatives, making it particularly useful for imbalanced datasets, which are common in medical research. The MCC values for our best models ranged from 0.15 to 0.29 during internal validation, which is comparable to results obtained by the MAQC2 consortium for event-free survival endpoints in multiple myeloma and neuroblastoma.\n\nWe also noted that the performance in external validation was within the same range or even better than in the internal cohort for the undersampled dataset, suggesting that our models were not overfitted. In contrast, oversampling methods tended to increase overfitting bias. This comprehensive evaluation approach ensures that our findings are robust and generalizable to new, unseen data.",
  "evaluation/comparison": "In our study, we compared the performance of various machine learning algorithms to predict treatment response in breast cancer patients. We evaluated eight different classifier algorithms, including elastic net, K nearest neighbors, logistic regression, neural network, random forest, support vector machine, VSURF, and XGBoost. To address class imbalance, we tested six resampling methods: SMOTE, ADASYN, Borderline-SMOTE, relocating safe-level SMOTE, random undersampling, and cluster centroids undersampling. The efficiency of these methods was assessed using random forest prediction performance on both majority and minority classes, with sensitivity and specificity evaluated through 5-fold cross-validation.\n\nWe performed internal validation using cross-validation and external validation on three separate cohorts (Juin, Buffa, and Guedj) to ensure the robustness of our models. The performance was calculated using the Matthews correlation coefficient (MCC), which provides a balanced measure even when the classes are of very different sizes. The best models showed median internal validation MCC values ranging from 0.15 to 0.29, depending on the algorithm. These results were comparable to those obtained by the MAQC2 consortium for event-free survival endpoints in multiple myeloma and neuroblastoma.\n\nAdditionally, we noted that external validation prediction performance was within the same range or better than the internal cohort prediction for the undersampled dataset, suggesting that our models were not overfitted. In contrast, oversampling methods increased the risk of overfitting bias. We selected the best-performing model for each algorithm in the undersampled dataset and extracted the important genes linked to relapse after hormonotherapy, making these genes available for further analysis, except for the support vector machine model.\n\nNot applicable",
  "evaluation/confidence": "The evaluation of our models included a thorough assessment of performance metrics, with a focus on ensuring statistical significance and reliability. We utilized the Matthews correlation coefficient (MCC) as our primary performance metric, which provides a balanced measure even when the classes are of very different sizes. The best models demonstrated median internal validation MCC values ranging from 0.15 to 0.29, depending on the algorithm used. These results were comparable to those obtained by the MAQC2 consortium for event-free survival endpoints in multiple myeloma and neuroblastoma, indicating robust performance.\n\nTo ensure the reliability of our findings, we conducted both internal and external validation. The external validation performance was within the same range or better than the internal cohort prediction for the undersampled dataset, which included 568 patients. This suggests that our models were not overfitted. In contrast, oversampling methods increased the overfitting bias, highlighting the importance of careful resampling strategies.\n\nStatistical significance was assessed using univariate Cox models, which showed that high scores for our gene expression signatures (GES) were significantly associated with unfavorable relapse-free survival (RFS) in the merged Affymetrix hormonotherapy-treated cohort. The hazard ratios (HR) for NervSign24 and NervSign97 were 2.72 and 2.9, respectively, both with highly significant P-values (P = 5.36e-19 and P = 3.4e-17). These results were further validated across multiple independent cohorts, demonstrating the robustness and generalizability of our findings.\n\nConfidence intervals were provided for the hazard ratios, ensuring that the statistical significance of our results could be clearly interpreted. The wide confidence intervals in smaller cohorts, such as the Juin cohort, indicated less reliable results, which were appropriately excluded from the final performance comparison. Overall, our evaluation process included rigorous statistical analyses and validation steps, providing strong confidence in the superiority and reliability of our method.",
  "evaluation/availability": "The data analyzed in this study were obtained from publicly available sources, including Gene Expression Omnibus (GEO), European Nucleotide Archive (ENA), and cBioPortal. The specific datasets used are identified by their accession numbers, such as GSE42568, GSE7390, GSE26971, and others. These datasets are freely accessible to the research community, allowing for reproducibility and further analysis.\n\nThe raw data from these sources were normalized using the RMA method from the simpleaffy package, and cross-platform normalization was performed using algorithms like ComBat from the sva package. This ensures that the data from different sources can be combined and analyzed together, reducing bias due to differences in manufacturing platforms.\n\nThe processed data, including the normalized transcriptomic profiles and the associated clinical information, are not explicitly mentioned as being publicly released in a specific repository. However, the methods and tools used for data processing are well-documented and publicly available, which facilitates the replication of the analysis.\n\nFor those interested in accessing the raw data, it can be obtained from the aforementioned public repositories. The specific licenses and terms of use for these datasets would be governed by the policies of GEO, ENA, and cBioPortal. Researchers are encouraged to review these policies to ensure compliance with data usage guidelines."
}