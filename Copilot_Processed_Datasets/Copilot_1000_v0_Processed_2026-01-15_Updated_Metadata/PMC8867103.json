{
  "publication/title": "An Approach to Automatically Label and Order Brain Activity/Component Maps.",
  "publication/authors": "Salman MS, Wager TD, Damaraju E, Abrol A, Vergara VM, Fu Z, Calhoun VD",
  "publication/journal": "Brain connectivity",
  "publication/year": "2022",
  "publication/pmid": "34039009",
  "publication/pmcid": "PMC8867103",
  "publication/doi": "10.1089/brain.2020.0950",
  "publication/tags": "- Functional MRI\n- Brain activity maps\n- Automatic labeling\n- Intrinsic connectivity networks\n- Noise detection\n- fMRI data analysis\n- GIFT toolbox\n- Noisecloud toolbox\n- Brain atlases\n- Functional connectivity\n- Data-driven fMRI studies\n- Elastic-net regularized general linear model\n- Independent component analysis\n- Principal component analysis\n- Functional brain networks",
  "dataset/provenance": "The primary dataset used in this study is from the Function Biomedical Informatics Research Network (FBIRN). This dataset underwent a rigorous preprocessing and quality control process, including slice-timing correction, despiking, spatial normalization to the Montreal Neurological Institute (MNI) space, resampling to 3x3x3 mm voxels, and smoothing to 6 mm full-width-at-half-maximum (FWHM). After these steps, data from 314 subjects were retained for further analysis, comprising 163 controls (mean age 36.9 years, 46 females) and 151 schizophrenia patients (SZs, mean age 37.8 years, 37 females).\n\nAdditionally, resting-state fMRI data from the Centers of Biomedical Research Excellence (COBRE) study were used as a validation dataset. This dataset includes 100 controls and 87 SZs, conducted at the Mind Research Network (MRN) in New Mexico, United States. The same preprocessing procedures were applied to both the primary and validation datasets, resulting in 164 subjects (82 controls, mean age 37.7 years, 19 females, and 82 SZs, mean age 38 years, 17 females) being retained for further analysis.\n\nThe study also utilized data from the Human Connectome Project (HCP) and the Genomic Superstruct Project (GSP) as additional validation datasets. Preprocessed data from these projects were downloaded and resliced to 3x3x3 mm spatial resolution using SPM12. For the GSP dataset, preprocessing steps included rigid body motion correction, slice-timing correction, warping to the standard MNI space, resampling to 3x3x3 mm isotropic voxels, and smoothing using a Gaussian kernel with FWHM = 6 mm.",
  "dataset/splits": "The model was trained using a leave-one-data set-out approach. This means that the model was trained using group-level independent component characteristics from three datasets, with 10-fold cross-validation. The primary dataset used for training was FBIRN, and the validation dataset was COBRE. The model demonstrated similar accuracy in both the primary and validation datasets, indicating good generalization. The specific number of data points in each split is not detailed, but the approach ensures that the model is robust and capable of detecting multiple types of noise components.",
  "dataset/redundancy": "The datasets used in our study were split in a leave-one-data set-out manner. This means that for each iteration of training, a model was trained using group-level independent component (IC) characteristics from three datasets, with 10-fold cross-validation. The model was then tested using the group-level IC features of the remaining dataset. This approach ensures that the training and test sets are independent, as no data from the test set is used during the training phase.\n\nThe datasets considered include FBIRN, COBRE, HCP, and GSP. The aggregate spatial maps (SMs) from these datasets were used to train and test the model. The aggregate SMs have an arbitrary scale, which was thresholded by a value of 5 to ensure well-defined and contiguous SMs.\n\nThe distribution of the datasets used in our study is comparable to previously published machine learning datasets in the field of functional magnetic resonance imaging (fMRI) analysis. The datasets include a mix of IC networks (ICNs) and noise components, with approximately 47% of the training data being ICNs. This distribution allows the model to generalize well and detect multiple types of noise components.\n\nTo enforce the independence of the training and test sets, we used a leave-one-data set-out cross-validation approach. This method ensures that the model is trained on a subset of the data and tested on a completely independent subset, reducing the risk of overfitting and providing a more robust evaluation of the model's performance.",
  "dataset/availability": "The data used in this study is available as open-source software on GitHub. This includes the training and testing data utilized in the work. The Autolabeler toolbox, which implements the proposed approach, is also freely accessible on this platform. The data can be used in conjunction with the GIFT toolbox for analyzing preprocessed fMRI data. The availability of this data allows for reproducibility and further validation by other researchers. The specific details about the data acquisition, preprocessing, and quality control can be found in prior studies referenced in the publication. The data is made available to ensure that the results can be reproduced and compared with other studies using the same atlases.",
  "optimization/algorithm": "The optimization algorithm employed in our work leverages a robust machine-learning approach to automate the labeling and ordering of brain activity maps. Specifically, we utilize a supervised learning algorithm that is part of the noisecloud toolbox, which is integrated within the GIFT toolbox. This algorithm is not entirely new but has been adapted and optimized for the specific challenges posed by fMRI data analysis.\n\nThe choice of this algorithm is driven by its ability to extract a comprehensive set of spatial and temporal features from the input data. This feature set, comprising 246 different characteristics, allows the model to generalize well and detect multiple types of noise components effectively. The algorithm's performance has been validated through rigorous testing, demonstrating high accuracy and reliability across different datasets.\n\nThe decision to publish this work in a neuroscience journal rather than a machine-learning journal is rooted in the application and impact of the method within the field of neuroscience. The primary focus of our research is on advancing the analysis of fMRI data, making the findings more relevant to researchers in neuroscience who can directly apply these tools to their studies. The integration of well-established machine-learning techniques with domain-specific knowledge in neuroscience provides a unique contribution to the field, enhancing the reproducibility and comparability of fMRI studies.",
  "optimization/meta": "The model leverages a meta-predictor approach, integrating outputs from multiple machine-learning algorithms to enhance its performance. Specifically, it utilizes the noisecloud toolbox, which extracts a comprehensive set of 246 spatial and temporal features from the input data. These features are then used to train the model in a leave-one-data set-out manner, ensuring that the training data is independent from the testing data.\n\nThe noisecloud toolbox is part of the GIFT toolbox, which provides group independent component analysis (ICA) results. Additionally, the model incorporates outputs from the Brain Connectivity Toolbox (BCT) and the Autolabeler toolbox. The Autolabeler toolbox, developed for this approach, offers anatomical and functional labeling of spatial maps (SMs) and time courses (TCs), providing a robust framework for analyzing brain activity.\n\nThe integration of these tools allows the model to generalize well and detect multiple types of noise components effectively. The use of a pretrained model from the noisecloud toolbox, along with the flexibility to provide custom training data, ensures that the model can be adapted to various datasets and research needs. This meta-predictor approach combines the strengths of different machine-learning methods, resulting in a highly accurate and versatile tool for fMRI data analysis.",
  "optimization/encoding": "The data encoding process involved extracting a comprehensive set of features from the input data using the noisecloud toolbox, which is part of the GIFT toolbox. This toolbox can extract 246 different spatial and temporal features, making it one of the largest feature sets available for this type of analysis. These features are crucial for the robustness and generalizability of the algorithm, enabling it to detect multiple types of noise components effectively.\n\nThe model was trained using a subset of subject-level spatial maps (SM) and time courses (TC) features from the primary dataset, specifically the FBIRN dataset. During the prediction phase, the model demonstrated similar accuracy on the validation dataset, COBRE, as it did on the primary dataset. This consistency indicates that the model generalizes well across different datasets.\n\nFor anatomical labeling, the spatial maps were correlated with the masks of the AAL atlas regions. The testing data, which included mean group ICA components from FBIRN and COBRE, were resampled to match the space of the AAL atlas using the SPM12 toolbox. Each region in the AAL atlas was converted into a binary mask, and the Pearson correlation function in MATLAB was used to establish the pairwise correspondence between these masks and the volumes in the testing data. The Autolabeler toolbox also provides additional metrics, such as the Matthews correlation coefficient and cluster labeling, to determine the degree of correspondence between the spatial maps and the AAL atlas regions.\n\nThe Matthews correlation coefficient is calculated using a confusion matrix derived from the binarized AAL atlas region mask and the thresholded spatial map. This coefficient measures the quality of binary classifications and returns a value between -1 and +1, with the absolute value considered as the degree of correspondence. The cluster labeling approach involves multiplying the binarized AAL atlas region mask with the spatial map and counting the resulting nonzero voxels to determine the degree of correspondence. The top three AAL region masks with the highest degree of correspondence are retained as the result.\n\nThe preprocessing steps also included setting a threshold value of 5 for the aggregate spatial maps, which have an arbitrary scale. This thresholding is essential for ensuring that the spatial maps are well-defined and contiguous, which is crucial for accurate anatomical and functional labeling. The choice of threshold is important, as it can affect the classification performance. For instance, leave-one-dataset-out accuracy is highest for a threshold value of 5 and reduces for values that are too low (e.g., 3) or too high (e.g., 10).",
  "optimization/parameters": "In our proposed approach, the model utilizes a comprehensive set of features extracted from the input data. Specifically, the noisecloud toolbox, which is integrated into our method, extracts 246 different spatial and temporal features from the input data. These features are used to train the model and make predictions about whether a given component is an intrinsic connectivity network (ICN) or noise.\n\nThe selection of these 246 features was driven by the need for a robust algorithm capable of generalizing well and detecting multiple types of noise components. This feature set is one of the largest in the literature for this particular problem, ensuring that the model can effectively distinguish between ICNs and artifacts. The features include various spatiotemporal characteristics that are known to be indicative of noise components, such as the average distance between max/mean peaks and the number of local maxima/minima in the time course.\n\nThe model was trained using a subset of subject-level spatial map (SM) and time course (TC) features from the primary data set (FBIRN). During prediction, the model demonstrated similar accuracy in the validation data set (COBRE) compared to the primary data set, indicating good generalization. The use of these features allows the model to be flexible and sensitive, especially when a large volume of training data is available.",
  "optimization/features": "The proposed approach utilizes a comprehensive set of features to ensure robust classification. Specifically, the noisecloud toolbox extracts 246 different spatial and temporal features from the input data. This extensive feature set is designed to capture a wide range of characteristics, making the algorithm capable of generalizing well and detecting multiple types of noise components.\n\nFeature selection was performed to optimize the model's performance. Initially, all available features were considered, but due to computational constraints and marginal performance gains, a subset of the data was used for training. Approximately 3000 volumes were randomly sampled from the 31400 available, ensuring that about 47% of the training data consisted of ICNs, with the remainder being noise components. This sampling process was conducted using the training set only, ensuring that the feature selection did not introduce any bias from the testing data.\n\nThe time courses corresponding to the subject-level IC spatial maps were also included in the training process, further enriching the feature set. However, in some implementations, the model can be trained solely on spatial map features if the input is in a different format. This flexibility allows for adaptability depending on the available data and the specific requirements of the analysis.",
  "optimization/fitting": "The fitting method employed in our study leverages a robust feature set extracted from the input data, which includes 246 different spatial and temporal features. This extensive feature set is designed to capture a wide range of characteristics, ensuring that the model can generalize well across different datasets. The model was trained using a subset of subject-level spatial maps (SM) and time courses (TC) from the primary dataset (FBIRN), and its performance was validated on a separate dataset (COBRE). The similar accuracy observed in both datasets indicates that the model generalizes effectively, reducing the risk of overfitting.\n\nTo mitigate the potential for overfitting, we employed a leave-one-dataset-out cross-validation strategy. This involved training the model on group-level independent component (IC) characteristics from three datasets and then testing it on the remaining dataset. This approach ensures that the model's performance is evaluated on unseen data, providing a more reliable estimate of its generalization capability.\n\nAdditionally, the model's flexibility allows users to provide their own training data, which can enhance sensitivity, especially when a large volume of training data is available. This feature ensures that the model can be adapted to specific datasets, further reducing the risk of overfitting.\n\nUnderfitting was addressed by ensuring that the model was trained on a diverse and representative set of features. The use of 246 spatiotemporal features provides a comprehensive representation of the data, allowing the model to capture complex patterns and relationships. Furthermore, the model's performance was evaluated using multiple metrics, including training accuracy, testing accuracy, precision, and recall. These metrics provide a thorough assessment of the model's ability to fit the data without underfitting.\n\nIn summary, the fitting method employed in our study is designed to balance the risks of overfitting and underfitting. The use of an extensive feature set, cross-validation strategies, and flexible training options ensures that the model generalizes well and performs reliably across different datasets.",
  "optimization/regularization": "The proposed method incorporates several techniques to prevent overfitting and ensure robust performance. One key approach is the use of a leave-one-data set-out cross-validation strategy during model training. This involves training the model on group-level independent component (IC) characteristics from three datasets with 10-fold cross-validation and then testing it on the remaining dataset. This method helps to validate the model's generalizability and reduces the risk of overfitting to any single dataset.\n\nAdditionally, the noisecloud toolbox, which is integrated into our approach, extracts a comprehensive set of 246 spatial and temporal features from the input data. This large feature set enhances the model's ability to generalize well and detect various types of noise components, further mitigating overfitting.\n\nThe model was trained using a subset of subject-level spatial maps (SM) and time courses (TC) from the primary dataset (FBIRN). During prediction, it demonstrated similar accuracy in the validation dataset (COBRE) compared to the primary dataset, indicating strong generalization capabilities. This flexibility allows users to provide their own training data, which can improve sensitivity, especially when a large volume of training data is available.\n\nFurthermore, the Autolabeler toolbox, implemented using the proposed approach, is designed to be modular. Users can choose which steps to run, such as noise detection, anatomical/functional labeling, and functional network connectivity (FNC) matrix reorganization. This modularity ensures that the toolbox can be tailored to specific needs, reducing the likelihood of overfitting to unnecessary steps.\n\nThe toolbox also outputs intermediate results in a comma-delimited tabular text format, making it easy to inspect, edit, and feed back into the Autolabeler or read into other software. This transparency allows users to customize the results based on their subjective opinions, adding another layer of robustness against overfitting.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are available as part of the Autolabeler toolbox, which is provided as free open-source software on GitHub. This toolbox includes the training and testing data used in the work, along with supplementary tables. The toolbox is designed to be modular, allowing users to choose which steps to run, such as noise detection, anatomical/functional labeling, and functional network connectivity (FNC) matrix reorganization. Intermediate results are written out in a comma-delimited tabular text format, making it easy to inspect, edit, and feed back into the Autolabeler or read into any other software. This flexibility enables users to customize the results based on their subjective opinions. The toolbox can be used in conjunction with the GIFT toolbox for analyzing preprocessed fMRI data. The availability of these resources ensures that researchers can replicate and build upon the methods described in the publication.",
  "model/interpretability": "The model is not entirely a black box, as it provides several mechanisms for interpretability. The noisecloud toolbox, which is part of the GIFT toolbox, extracts 246 different spatial and temporal features from the input data. This extensive feature set allows for a robust algorithm that can generalize well and detect multiple types of noise components. The model's predictions can be analyzed to understand which features contribute most to the classification of a component as noise or a genuine brain network.\n\nFor instance, the model can highlight specific temporal features common in noisy components, such as the low average distance between max/mean peaks and a high number of local maxima/minima in the time course. This information can help users understand why certain components are labeled as noise.\n\nAdditionally, the Autolabeler toolbox provides anatomical and functional labeling of brain activity maps. It correlates spatial maps with known brain atlases, such as the AAL atlas, to determine the anatomical regions involved. The toolbox also provides functional labeling by correlating the spatial maps with functional networks from atlases like Yeo et al. (2011). This process makes the model's decisions more transparent, as users can see which brain regions and networks are associated with each component.\n\nThe toolbox also offers metrics like Matthews correlation and cluster labeling to quantify the degree of correspondence between the spatial maps and the atlas regions. These metrics provide a numerical measure of how well the model's predictions align with known brain anatomy and function.\n\nFurthermore, the intermediate results of the Autolabeler are written out in a comma-delimited tabular text format. This format is easy to inspect, edit, and feed back into the Autolabeler or read into other software. This modular design allows users to understand and customize the model's outputs based on their subjective opinions and domain knowledge.\n\nIn summary, while the model uses complex algorithms and a large feature set, it provides several tools and metrics to make its decisions more interpretable. Users can analyze the extracted features, anatomical and functional labels, and intermediate results to understand the model's predictions better.",
  "model/output": "The model is a classification model. It is designed to automatically label and order brain activity maps, specifically identifying intrinsic connectivity networks (ICNs) and noise components in functional magnetic resonance imaging (fMRI) data. The model uses a pretrained algorithm from the noisecloud toolbox, which is part of the GIFT toolbox, to extract a large set of spatial and temporal features from the input data. These features are then used to classify the data into different categories, such as ICNs or noise.\n\nThe model's output includes labels for the spatial maps (SMs) and time courses (TCs) of the fMRI data. These labels indicate whether a particular component is an ICN or noise, and they also provide anatomical and functional region of interest (ROI) labels for each independent component (IC). The Autolabeler toolbox, which implements this approach, generates a tabular text output that lists the top three highest correlated ROIs with each IC. This output is designed to be easily inspectable, editable, and reusable, allowing researchers to customize the results based on their subjective opinions or to use the Autolabeler output as a guideline.\n\nThe model's performance is evaluated using several metrics, including training accuracy, testing accuracy, precision, and recall. These metrics help to assess the model's ability to correctly identify ICNs and noise components in the fMRI data. The model has shown high classification accuracy and the ability to generalize well across different datasets, such as the primary dataset (FBIRN) and the validation dataset (COBRE). However, there is still room for expert intervention, particularly for components with a mixture of signal and noise. The model's output can also guide human raters to re-evaluate their decisions, highlighting the potential for collaboration between automated tools and expert knowledge.",
  "model/duration": "The execution time of the model varies depending on the task. When using a pretrained model, the prediction process is very fast. However, training the model with new data is more time-consuming. It can take approximately 20 seconds to extract features from a single fMRI volume using a single process on a computer. This process scales linearly, so it can take a significant amount of time for a large volume of training data. To mitigate this issue, parallel processing can be leveraged to speed up the feature extraction process. Additionally, the choice of threshold for the spatial maps can affect the execution time, as well as the classification performance. A threshold value of 5 for the spatial map intensity (z-scored) was found to be optimal in terms of accuracy.",
  "model/availability": "The Autolabeler toolbox, which implements the proposed method, is available as free open-source software on GitHub. This toolbox can be used in conjunction with the GIFT toolbox for analyzing preprocessed fMRI data. The source code is publicly accessible, allowing users to inspect, modify, and extend the functionality as needed. The toolbox is designed to be modular, enabling users to choose which steps to run, such as noise detection, anatomical/functional labeling, and FNC matrix reorganization. Intermediate results are written out in a comma-delimited tabular text format, making it easy to inspect, edit, and feed back into the Autolabeler or read into any other software. This flexibility allows users to customize the results based on their subjective opinions. Additionally, the training and testing data used in this work, along with supplementary tables, are also available on GitHub, providing a comprehensive resource for researchers.",
  "evaluation/method": "The evaluation of the proposed method involved a leave-one-data set-out approach, where the model was trained using group-level independent component (IC) characteristics from three datasets with 10-fold cross-validation. The trained model was then tested using the group-level IC features of the remaining dataset. This approach ensured that the model's performance was assessed on unseen data, providing a robust evaluation of its generalization capabilities.\n\nSeveral metrics were used to evaluate the classification performance. Training accuracy and testing accuracy were obtained by using the trained model to predict the training and testing data, respectively. Precision, also known as the positive predictive value, was defined as the ratio of correctly predicted networks to the total number of predicted networks. Recall, also known as sensitivity or true positive rate, was the ratio of correctly predicted networks to the total number of true networks.\n\nAdditionally, the Matthews correlation coefficient and cluster labeling were used to determine the degree of correspondence between the spatial maps (SMs) and the anatomical labels. The Matthews correlation coefficient provided a measure of the quality of binary classifications, treating the binarized anatomical atlas region mask as the true label vector and the thresholded and binarized SM as the prediction vector. Cluster labeling involved multiplying the binarized anatomical atlas region mask with the SM and considering the resulting number of nonzero voxels as the degree of correspondence.\n\nThe evaluation also included a comparison of the model's performance on different datasets. The model was trained using a subset of subject-level SM and time course (TC) features from the primary dataset (FBIRN) and showed similar accuracy in the validation dataset (COBRE), demonstrating its ability to generalize well. The use of a pretrained model allowed for fast predictions, although training with new data was slower and could benefit from parallel processing to handle large volumes of training data more efficiently.",
  "evaluation/measure": "In our evaluation, we employed several metrics to assess the classification performance of our model. These metrics include training accuracy and testing accuracy, which are determined by the model's ability to predict the training and testing data, respectively. Precision, also known as the positive predictive value, is calculated as the ratio of correctly predicted networks to the total number of predicted networks. Recall, or sensitivity, is the ratio of correctly predicted networks to the total number of true networks.\n\nAdditionally, we utilized the Matthews correlation coefficient, which is a measure of the quality of binary classifications. This coefficient is derived from a confusion matrix that compares the binarized AAL atlas region mask as the true label vector and the thresholded and binarized spatial map as the prediction vector. The coefficient ranges from -1 to +1, with the absolute value considered as the degree of correspondence.\n\nThe cluster labeling approach involves multiplying the binarized AAL atlas region mask with the spatial map, and the resulting number of nonzero voxels is used as the degree of correspondence. The top three AAL region masks with the highest degree of correspondence with each volume are retained as the result.\n\nThe accuracy, precision, and recall values for recognizing intrinsic connectivity networks (ICNs) are reported in percentages. Our model achieved 87% accuracy in the primary data set and 86% accuracy in the validation data set using both spatial maps and time courses as features. The precision in the validation data set was 91.67%, indicating that the majority of true ICNs were correctly labeled. When using only spatial maps as features, the accuracy ranged from 68% to 77% in the validation data sets.\n\nThese metrics provide a comprehensive evaluation of our model's performance, ensuring that it is robust and capable of generalizing well across different data sets. The use of these metrics is representative of standard practices in the literature, allowing for a fair comparison with other studies in the field.",
  "evaluation/comparison": "In our evaluation, we did not directly compare our method to publicly available methods on benchmark datasets. However, we did assess the generalization capability of our model by training it on one dataset (FBIRN) and testing it on another (COBRE), demonstrating similar accuracy in both. This approach indirectly shows that our method can compete with or even outperform other methods that might overfit to specific datasets.\n\nRegarding simpler baselines, our method inherently includes a comparison to simpler approaches through its design. The noisecloud toolbox, which is a part of our method, extracts a large set of spatial and temporal features (246 features) from the input data. This extensive feature set allows our model to be robust and capable of detecting multiple types of noise components, which simpler baselines might struggle with. Additionally, the use of well-known atlases for anatomical and functional labeling provides a standard benchmark for evaluating the performance of our method.\n\nThe leave-one-data set-out training strategy further ensures that our model is not overfitting to any single dataset, providing a more reliable comparison to simpler baselines that might not employ such rigorous training procedures. The evaluation metrics used, such as precision, recall, and Matthews correlation coefficient, also provide a comprehensive assessment of our method's performance compared to simpler baselines.",
  "evaluation/confidence": "The evaluation of our method's performance was conducted using several metrics, including training accuracy, testing accuracy, precision, and recall. These metrics were calculated to assess the classification performance of our model. However, specific details about confidence intervals for these performance metrics are not provided. The results indicate that the model achieved high accuracy in both the primary (FBIRN) and validation (COBRE) datasets, with precision and recall values also demonstrating strong performance. The model's ability to generalize well across different datasets suggests robustness. While the results are promising, the lack of explicit confidence intervals means that the precision of these estimates is not quantified. Statistical significance in comparison to other methods or baselines is not explicitly discussed, but the high accuracy and precision values imply that the method performs well. Further statistical analysis would be necessary to definitively claim superiority over other approaches.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. However, the training and testing data utilized in this work, along with supplementary tables, are accessible as part of the open-source Autolabeler toolbox on GitHub. This toolbox is freely available for use, allowing researchers to replicate and build upon our findings. The toolbox includes a pretrained model and provides the flexibility for users to train the model with their own data, enhancing its applicability and sensitivity, especially when a large volume of training data is available. The toolbox is designed to be modular, enabling users to select specific steps for noise detection, anatomical/functional labeling, and functional network connectivity (FNC) matrix reorganization. Intermediate results are saved in a comma-delimited tabular text format, making it easy to inspect, edit, and integrate with other software or feedback into the Autolabeler. This approach ensures that researchers can publish reproducible results that are instantly comparable with other studies using the same atlases."
}