{
  "publication/title": "Deep-learning model for predicting 30-day postoperative mortality.",
  "publication/authors": "Fritz BA, Cui Z, Zhang M, He Y, Chen Y, Kronzer A, Ben Abdallah A, King CR, Avidan MS",
  "publication/journal": "British journal of anaesthesia",
  "publication/year": "2019",
  "publication/pmid": "31558311",
  "publication/pmcid": "PMC6993109",
  "publication/doi": "10.1016/j.bja.2019.07.025",
  "publication/tags": "- Anaesthesiology\n- Deep learning\n- Machine learning\n- Postoperative complications\n- Risk prediction\n- Surgery\n- Predictive models\n- Patient outcomes\n- Medical data analysis\n- Time-series data",
  "dataset/provenance": "The dataset used in this study was sourced from a centralised database maintained by hospital administration, which captures both in-hospital and post-discharge deaths. This database was used to obtain the target variable, which was 30-day mortality. Additionally, preoperative laboratory values were retrieved from the Center for Biomedical Informatics at Washington University in St. Louis.\n\nThe dataset included 111,419 unique patients. However, 2,112 patients were excluded because of age less than 18 years, and 13,400 patients were excluded because they did not undergo tracheal intubation for surgery. Therefore, the final analysis included 95,907 patients.\n\nThe data used in this study has not been previously published or used by the community in other papers. The dataset consists of patient characteristics, co-morbid conditions, preoperative laboratory values, intraoperative vital signs, ventilator parameters, and administration documentation for intraoperative medications and fluids. The complete list of features is available in the supplementary materials.",
  "dataset/splits": "The dataset was divided into three distinct splits: training, validation, and testing. The distribution of data points across these splits followed a ratio of approximately 7:1:2. This means that the majority of the data, around 70%, was used for training the model. A smaller portion, about 10%, was allocated for validation purposes to tune the model's hyperparameters and prevent overfitting. The remaining 20% of the data was reserved for testing the final model's performance on unseen data. This split ensures a comprehensive evaluation of the model's generalizability and robustness.",
  "dataset/redundancy": "The dataset used in this study was split into three distinct sets: training, validation, and testing. The split ratio was approximately 7:1:2, ensuring that the majority of the data was used for training the model, a smaller portion for validation to tune hyperparameters, and the remainder for testing the final model's performance.\n\nThe training and test sets were designed to be independent. This independence was enforced by randomly splitting the dataset, which helps to ensure that the model's performance on the test set is a true reflection of its generalizability to new, unseen data. This random split helps to mitigate the risk of data leakage, where information from the test set might inadvertently influence the training process.\n\nRegarding the distribution of the dataset, it included a large number of unique patients, with specific exclusions applied to ensure the relevance of the data. Patients under the age of 18 and those who did not undergo tracheal intubation for surgery were excluded, resulting in a final dataset of 95,907 patients. The population predominantly consisted of ASA class 2 and 3 patients undergoing a variety of surgeries. Common co-morbid conditions included hypertension, gastro-oesophageal reflux disease, cancer, and anaemia. The dataset also included a wide range of preoperative vital signs and laboratory values, providing a comprehensive view of the patients' health status before surgery.\n\nThe target variable, 30-day mortality, had a positive-to-negative ratio of approximately 1:100, indicating a significant imbalance. This imbalance was addressed using upsampling of positive training examples, which led to better performance metrics in the validation cohort. The dataset's characteristics and the methods used to handle imbalances are crucial for the model's ability to accurately predict postoperative mortality.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is a multipath convolutional neural network (MPCNN). This algorithm is not entirely new, as convolutional neural networks (CNNs) and long short-term memory (LSTM) networks have been extensively studied and used in various applications. However, the specific architecture and implementation of our MPCNN, which combines CNN and LSTM pathways, is tailored for time-series data and postoperative mortality prediction.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of our work is on its application in biomedical research, specifically in predicting postoperative mortality. The innovation lies in the application of deep learning techniques to process raw intraoperative data for real-time risk prediction, rather than in the development of a novel machine-learning algorithm. Our study adheres to the guidelines for developing and reporting machine learning predictive models in biomedical research, ensuring that the methodological details are transparent and reproducible.\n\nThe MPCNN was implemented using PyTorch, a popular open-source machine learning library. This choice was made to leverage the library's capabilities in handling complex neural network architectures and its efficiency in utilizing graphics processing units (GPUs) for accelerated computation. The use of established libraries like PyTorch allows for easier integration and comparison with other machine-learning models, as seen in our study where we compared the MPCNN with deep neural networks, random forests, support vector machines, and logistic regression.",
  "optimization/meta": "The model described in this publication is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it is a multipath convolutional neural network (MPCNN) that directly processes patient characteristics, co-morbid conditions, preoperative laboratory values, and intraoperative numerical data.\n\nThe MPCNN model was compared with several widely used classifiers, including a deep neural network (DNN), random forest (RF), support vector machine (SVM), and logistic regression (LR). These comparisons were made to evaluate the performance of the MPCNN against established methods. However, the MPCNN itself does not integrate these other methods as part of its architecture.\n\nThe training data for the MPCNN was split into training, validation, and testing sets using a ratio of roughly 7:1:2. This splitting ensures that the data used for training is independent of the data used for validation and testing, maintaining the integrity of the model's performance evaluation.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure that the data was in a suitable format for training and evaluation.\n\nFor continuous features, values outside acceptable ranges were considered input errors and set to missing. Missing values were imputed using the mean value from the historical dataset. All continuous features were then scaled to a range of [0, 1].\n\nCategorical features that did not exist in the data dictionary were labeled as missing values. Missing values for categorical features were treated as a new category rather than being imputed.\n\nIntraoperative time series data were captured at 1-minute intervals. Epochs of 60 minutes were used, as this duration led to better performance metrics in the validation cohort. Simple linear interpolation was used to impute less frequently measured time series to every minute. If a time series had no records during the query time span, the patient mean value was used to fill in the missing entries. For intraoperative medications, the cumulative dose until the end of the epoch was used, normalized by the time since the beginning of surgery.\n\nMulti-resolution standardization was applied to the intraoperative time series. This involved standardizing each time series to have a mean of 0 and a standard deviation of 1. The mean and standard deviation statistics for each patient were then z-transformed across patients and added to the list of preoperative continuous features. This method ensured that features with large numerical scales did not dominate the model.\n\nTo address the imbalanced outcome of 30-day mortality, two methods were tested: using class weights inversely proportional to their proportion and upsampling positive training examples. The upsampling method led to higher performance metrics in the validation cohort and was therefore used in all analyses.\n\nThe dataset was randomly split into training, validation, and testing sets using a ratio of roughly 7:1:2. This split ensured that the model was trained on a large enough dataset while still having sufficient data for validation and testing.",
  "optimization/parameters": "The model utilized a comprehensive set of input parameters to predict postoperative 30-day mortality. These parameters were divided into several groups, each representing different aspects of patient data.\n\nFor patient characteristics, continuous features included age, height, weight, ideal body weight, and body mass index (BMI). Categorical features encompassed sex, race, Charlson Comorbidity Index, functional capacity, ASA physical status, ASA emergency status, and surgery type.\n\nCo-morbid conditions were represented by categorical features such as hypertension, coronary artery disease, diabetes mellitus, chronic kidney disease, and others. Preoperative vital signs included systolic and diastolic blood pressure, pulse oximeter readings, and heart rate. Preoperative laboratory values covered a range of tests, including albumin, creatinine, glucose, and white blood cell count.\n\nIntraoperative data were captured at one-minute intervals and included vital signs like systolic and diastolic blood pressure, heart rate, and temperature, as well as ventilator parameters such as ventilatory frequency, tidal volume, and fraction of inspired oxygen. Additionally, the cumulative doses of various intraoperative medications and fluids were recorded.\n\nThe selection of these parameters was based on their clinical relevance and availability in the electronic health records. The model was designed to handle both continuous and categorical data, with continuous features being scaled to a range of [0, 1] and categorical features being treated as new categories if they did not exist in the data dictionary. Missing values were imputed using historical data means for continuous features and treated as new categories for categorical features.\n\nThe total number of parameters (p) used in the model is the sum of all these features. Specifically, there were 20 continuous features and 36 categorical features representing patient characteristics, co-morbid conditions, laboratory values, and other preoperative data. Additionally, there were 5 intraoperative vital sign features and 6 ventilator parameters, along with administration documentation for 17 intraoperative medications and fluids. This results in a total of 84 features.\n\nThe selection of these parameters was driven by the need to capture a wide range of patient information that could influence postoperative outcomes. The model's architecture, including the use of convolutional neural networks (CNN) and long short-term memory (LSTM) networks, allowed for the effective processing of these diverse input parameters to make accurate predictions.",
  "optimization/features": "The model utilized a total of 61 features as input, comprising 20 continuous features and 36 categorical features representing patient characteristics, co-morbid conditions, laboratory values, and other preoperative data. Additionally, five intraoperative vital sign features and six ventilator parameters were captured with a maximum resolution of 1 minute. Administration documentation for 17 intraoperative medications and fluids was also retrieved.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, all relevant features were included based on domain knowledge and their potential impact on postoperative mortality. The features were carefully chosen to cover a wide range of patient characteristics, co-morbid conditions, and intraoperative variables. This approach ensured that the model had a comprehensive set of inputs to make accurate predictions.\n\nThe preprocessing steps included handling missing values and scaling continuous features to span the range [0, 1]. For preoperative continuous features, values outside acceptable ranges were set to missing and imputed using the mean value over the historical dataset. For preoperative categorical features, missing values were treated as a new category. For intraoperative time series, data from a 60-minute epoch was used, with simple linear interpolation to impute less frequently measured time series. If a time series had no records during the query time span, the patient mean value was used to fill in the missing entries. Multi-resolution standardization was applied to prevent features with large numerical scales from dominating the model.\n\nThe target outcome was 30-day mortality, which was obtained from a centralized database maintained by the hospital administration. The dataset was randomly split into training, validation, and testing sets using a ratio of roughly 7:1:2. This split ensured that the model was trained and validated on different subsets of the data, reducing the risk of overfitting and providing a robust evaluation of its performance.",
  "optimization/fitting": "The model employed in this study is a multipath convolutional neural network (MPCNN), which incorporates three distinct pathways: numerical feature mapping, categorical feature embedding, and time-series feature mapping. The numerical feature mapping pathway involves identity mapping for each feature, while the categorical feature embedding pathway utilizes a lookup table with numerical embeddings for each category.\n\nThe time-series feature mapping pathway includes two parallel processes: a one-dimensional multivariate convolutional neural network (CNN) and a long short-term memory (LSTM) network. The CNN extracts information based on the shape of the time series and consists of three convolutional blocks, each with a convolution layer, a batch normalization layer, and a rectified linear unit (ReLU) activation function. The LSTM network, on the other hand, can memorize the previous state and make predictions based on the previous state and the current input value. A three-layer LSTM network was used.\n\nThe outputs from these three pathways are concatenated vertically and then fed into multiple fully connected layers, each followed by a ReLU layer. To prevent overfitting, dropout was used after the first fully connected block. The output of the final fully connected layer is fed into a softmax layer for prediction. The model was trained by minimizing the cross-entropy loss between the true outcome distribution and the prediction distribution.\n\nTo address the issue of overfitting, dropout was implemented after the first fully connected block. Additionally, the dataset was split into training, validation, and testing sets using a ratio of roughly 7:1:2, ensuring that the model's performance could be evaluated on unseen data. The use of a validation set also helped in tuning hyperparameters and preventing overfitting.\n\nUnderfitting was addressed by using a complex architecture that included convolutional and LSTM layers, which are capable of capturing intricate patterns in the data. The model's performance was evaluated using multiple metrics, including the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC), ensuring that it could generalize well to new data. The use of class weights and upsampling for the imbalanced outcome also helped in improving the model's performance and preventing underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One of the key methods used was dropout. Dropout is a regularization technique where, during training, a random subset of neurons is temporarily removed from the network. This helps to prevent the model from becoming too reliant on any single neuron or path, thereby reducing overfitting. We applied dropout after the first fully connected block in our neural network architecture.\n\nAdditionally, we utilized batch normalization. Batch normalization helps to stabilize and accelerate the training process by normalizing the inputs of each layer. This technique reduces the internal covariate shift, making the training more efficient and less prone to overfitting.\n\nWe also implemented data augmentation techniques, such as upsampling the positive training examples. Given the imbalanced nature of our dataset, with a positive-to-negative ratio of approximately 1:100, upsampling helped to ensure that the model received adequate exposure to the minority class, thereby improving its ability to generalize to new, unseen data.\n\nFurthermore, we performed multi-resolution standardization of the intraoperative time series data. This involved standardizing not only the values within each time series but also the high-level time-series statistics across different patients. This step was crucial in preventing features with large numerical scales from dominating the model, thereby promoting a more balanced learning process.\n\nThese regularization methods collectively contributed to the development of a robust and generalizable model for predicting postoperative mortality.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are not explicitly detailed in the provided information. However, the implementation of the multipath convolutional neural network (MPCNN) was done using PyTorch, and baseline methods for comparison were implemented using the scikit-learn library within Python. These libraries are open-source and widely used in the machine learning community, which means that the general frameworks and tools we used are publicly accessible.\n\nThe specific model files and optimization parameters are not directly reported in the provided context. However, the performance metrics and evaluation criteria, such as the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC), are thoroughly discussed. These metrics provide insights into the model's performance and can be used to replicate or build upon the work.\n\nFor those interested in the detailed implementation, the use of PyTorch and scikit-learn suggests that the code can be made available upon request, adhering to standard academic practices for sharing research materials. The licenses for PyTorch and scikit-learn are permissive, allowing for both academic and commercial use, which facilitates reproducibility and further development by other researchers.",
  "model/interpretability": "The model employed in this study is a multipath convolutional neural network (MPCNN), which inherently possesses some level of interpretability due to its architecture. Unlike purely black-box models, the MPCNN allows for the visualization of feature importance through back-propagation-based methods. This technique calculates the gradient of each neuron with respect to the input space, providing insights into how different input features influence the model's predictions. The magnitude of these gradients serves as an indicator of feature importance, enabling clinicians to understand which patient characteristics and intraoperative data points are most critical in predicting postoperative mortality.\n\nFor instance, by analyzing the gradients, it is possible to identify specific static features that contribute significantly to the model's predictions for individual patients. This interpretability is crucial for building trust in the model and for clinicians to make informed decisions based on the model's outputs. Additionally, the use of convolutional layers and long short-term memory (LSTM) units in the MPCNN architecture allows for the capture of temporal patterns in the intraoperative data, further enhancing the model's ability to provide meaningful insights into patient risk factors.",
  "model/output": "The model is a classification model designed to predict postoperative 30-day mortality. It utilizes a multipath convolutional neural network (MPCNN) architecture, which integrates patient characteristics, co-morbid conditions, preoperative laboratory values, and intraoperative numerical data. The model's output is a probability distribution, specifically the likelihood of a patient dying within 30 days post-surgery. This probability is generated through a softmax layer, which is the final component of the network. The model was trained to minimize the cross-entropy loss between the true outcome distribution and the predicted distribution, indicating its classification nature. The performance of the model was evaluated using metrics such as the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC), further confirming its role as a classifier.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the models involved several key metrics and techniques to ensure robust performance assessment. The models were evaluated using the area under the receiver operating characteristic curve (AUROC), sensitivity, and specificity. Given the imbalanced nature of the dataset, with a low positive-to-negative ratio, the receiver operating characteristic (ROC) curve alone might not have been sufficient. Therefore, the precision-recall curve was also plotted, and the area under the precision-recall curve (AUPRC) was calculated. This additional metric measures the average precision and is particularly useful in imbalanced datasets.\n\nModel predictions were calibrated against the observed mortality in the validation set using the histogram binning technique. This involved fixing the number of deaths per bin and plotting the observed mortality against the calibrated predicted probability of mortality in the test set. Perfect calibration would result in a perfect diagonal in this plot.\n\nFurthermore, stratified descriptive statistics were performed on patients for whom the model produced true-positive, false-positive, false-negative, and true-negative predictions. These predictions were derived using an uncalibrated decision threshold of 0.5280, which maximized Youden's index (sensitivity + specificity - 1). This threshold was chosen to optimize the balance between sensitivity and specificity.\n\nThe dataset included 111,419 unique patients, with 95,907 patients included in the analysis after excluding those under 18 years old and those who did not undergo tracheal intubation for surgery. The dataset was randomly split into training, validation, and testing sets using a ratio of roughly 7:1:2. This split ensured that the models were trained on a substantial portion of the data while leaving enough data for validation and testing to assess performance accurately.",
  "evaluation/measure": "In our study, we employed several performance metrics to comprehensively evaluate the models. The primary metrics reported include the Area Under the Receiver Operating Characteristic Curve (AUROC) and the Area Under the Precision-Recall Curve (AUPRC). These metrics are widely used in the literature and provide a robust evaluation of model performance, especially in imbalanced datasets like ours, where the positive-to-negative ratio is low.\n\nThe AUROC measures the model's ability to distinguish between positive and negative classes across all threshold levels, providing a single scalar value that summarizes the trade-off between sensitivity and specificity. This metric is particularly useful for understanding the overall performance of the model.\n\nGiven the imbalanced nature of our dataset, we also calculated the AUPRC, which focuses on the performance of the model in predicting the positive class. Precision, synonymous with positive predictive value, and recall, synonymous with sensitivity, are crucial in scenarios where the cost of false positives and false negatives differs. The AUPRC provides a more nuanced view of the model's performance by considering the precision-recall trade-off.\n\nAdditionally, we evaluated the models using sensitivity and specificity. Sensitivity measures the proportion of actual positives correctly identified by the model, while specificity measures the proportion of actual negatives correctly identified. These metrics are essential for understanding the model's performance in clinical settings, where the consequences of false positives and false negatives can be significant.\n\nTo ensure the reliability of our results, we also performed calibration of model predictions against observed mortality in the validation set using the histogram binning technique. This step helps in assessing the quality of the calibration, where perfect calibration would result in a perfect diagonal in the calibration plot. Furthermore, we derived dichotomous predictions using an uncalibrated decision threshold that maximized Youden's index, which is the sum of sensitivity and specificity minus one. This approach provides a balanced measure of the model's performance in terms of both sensitivity and specificity.\n\nIn summary, our evaluation criteria are representative of the literature and include a comprehensive set of metrics that provide a thorough assessment of model performance. The use of AUROC, AUPRC, sensitivity, and specificity, along with calibration techniques, ensures that our models are evaluated rigorously and are suitable for clinical application.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of our proposed multipath convolutional neural network (MPCNN) with several widely used classifiers to evaluate its performance. We compared two versions of MPCNN: one utilizing convolutional neural networks (CNN) and the other using long short-term memory (LSTM) networks for time-series features.\n\nTo ensure a thorough evaluation, we compared MPCNN against a deep neural network (DNN) that shared the same architecture but omitted the time-series feature-mapping pathway. Instead, this DNN included the mean and standard deviation of each time-series feature as numerical features. Additionally, we compared MPCNN to traditional machine learning methods, including random forest (RF), support vector machine (SVM), and logistic regression (LR). These methods also incorporated the mean and standard deviation of the time-series features.\n\nThe evaluation criteria included the area under the receiver operating characteristic curve (AUROC), sensitivity, and specificity. Given the imbalanced nature of our dataset, with a low positive-to-negative ratio, we also plotted the precision-recall curve and calculated the area under the precision-recall curve (AUPRC) to measure the average precision as an additional evaluation criterion. Precision is equivalent to the positive predictive value, while recall is synonymous with sensitivity.\n\nOur results demonstrated that both versions of MPCNN\u2014MPCNNeCNN and MPCNNeLSTM\u2014outperformed all four comparison methods in terms of AUROC and AUPRC. Specifically, MPCNNeLSTM achieved the highest AUROC of 0.867 and the highest AUPRC of 0.097, indicating superior performance in predicting postoperative mortality.",
  "evaluation/confidence": "The evaluation of our models included the use of confidence intervals for the performance metrics. Specifically, the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC) were reported with their respective 95% confidence intervals. For instance, the multipath convolutional neural network using long short-term memory (MPCNNeLSTM) achieved an AUROC of 0.867 with a 95% confidence interval of 0.835 to 0.899, and an AUPRC of 0.097 with a 95% confidence interval of 0.085 to 0.109. These intervals provide a range within which the true performance metrics are expected to lie, giving a measure of the uncertainty associated with the estimates.\n\nThe results indicated that both versions of the multipath convolutional neural network (MPCNNeCNN and MPCNNeLSTM) outperformed the comparison methods, including a deep neural network (DNN), random forest (RF), support vector machine (SVM), and logistic regression (LR). The statistical significance of these results was not explicitly stated, but the confidence intervals did not overlap significantly with those of the other models, suggesting that the observed differences in performance are likely to be statistically significant. This implies that the multipath convolutional neural network models are superior to the baseline models in predicting postoperative 30-day mortality.",
  "evaluation/availability": "Not enough information is available."
}