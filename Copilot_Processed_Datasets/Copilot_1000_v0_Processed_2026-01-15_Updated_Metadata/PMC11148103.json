{
  "publication/title": "SOFB is a comprehensive ensemble deep learning approach for elucidating and characterizing protein-nucleic-acid-binding residues.",
  "publication/authors": "Zhang B, Hou Z, Yang Y, Wong KC, Zhu H, Li X",
  "publication/journal": "Communications biology",
  "publication/year": "2024",
  "publication/pmid": "38830995",
  "publication/pmcid": "PMC11148103",
  "publication/doi": "10.1038/s42003-024-06332-0",
  "publication/tags": "Not enough information is available.",
  "dataset/provenance": "The datasets used in this study were collected from the BioLip database, which contains information on biological ligand-protein interactions. Specifically, we focused on nucleic-acid-binding proteins, selecting protein complexes that contain only DNA or RNA. This resulted in 9,574 DNA-protein chains and 7,693 RNA-protein chains.\n\nTo enhance the dataset, we increased the number of positive samples by amplifying annotations with similar protein sequences. This process involved identifying analogous protein chains using sequence identity and structural similarity, followed by clustering and annotation transfer. Additionally, we pruned protein chains with sequence identity lower than 30% using the CD-HIT method. As a result, the number of DNA and RNA binding residues increased by 30.7% and 24.3%, respectively.\n\nThe final datasets consist of:\n\n* DNA-573_train: 573 protein chains with 14,479 binding residues and 145,404 non-binding residues.\n* DNA-129_test: 129 protein chains with 2,240 binding residues and 35,275 non-binding residues.\n* RNA-495_train: 495 protein chains with 14,609 binding residues and 122,290 non-binding residues.\n* RNA-117_test: 117 protein chains with 2,031 binding residues and 35,314 non-binding residues.\n\nThese datasets were partitioned into training and validation sets, with 80% of the data used for training and 20% for validation. The test sets were used for independent evaluation of the model's performance. The datasets and additional details are available on Figshare and GitHub.",
  "dataset/splits": "There are four data splits in the datasets used for this study. These splits are DNA-573_train, DNA-129_test, RNA-495_train, and RNA-117_test.\n\nThe DNA training set consists of 573 protein chains, containing 14,479 binding residues and 145,404 non-binding residues. The DNA test set comprises 129 protein chains, with 2,240 binding residues and 35,275 non-binding residues.\n\nThe RNA training set encompasses 495 protein chains, including 14,609 binding residues and 122,290 non-binding residues. The RNA test set includes 117 protein chains, with 2,031 binding residues and 35,314 non-binding residues.\n\nThe initial training sets were further partitioned into a training set (80%) and a validation set (20%) to demonstrate the predictive performance and ensure a rigorous assessment of the model\u2019s capabilities. The test sets were used to perform an independent test.",
  "dataset/redundancy": "The datasets used in our study were sourced from the BioLip database, which contains information on biological ligand-protein interactions. We collected two benchmark datasets of nucleic-acid-binding proteins, each comprising a training set and a test set. These sets are termed DNA-573_train, DNA-129_test, RNA-495_train, and RNA-117_test.\n\nTo ensure the independence of the training and test sets, we employed the CD-HIT method to eliminate protein sequences with an identity exceeding 30%. This step was crucial in reducing redundancy and ensuring that the test sets contained sequences that were not present in the training sets. As a result, the final dataset consisted of 464 proteins with 106,081 amino acids that bind to DNA and 416 RNA-binding proteins with 95,020 amino acids.\n\nThe training sets were further partitioned into an actual training set (80%) and a validation set (20%) to demonstrate the predictive performance and ensure a rigorous assessment of the model's capabilities. The test sets, which contained 2,240 and 35,275 binding and non-binding residues for DNA, and 2,031 and 35,314 binding and non-binding residues for RNA, respectively, were used to perform an independent test.\n\nIn comparison to previously published machine learning datasets, our approach to dataset redundancy involved a more stringent filtering process. By using the CD-HIT method and setting a higher threshold for sequence identity, we aimed to create a more robust and independent test set. This method helped in reducing the effect of imbalance between positive and negative samples, thereby enhancing the reliability of our model's predictions.",
  "dataset/availability": "The data used in this study, including the specific data splits, are released in a public forum. The source data are provided with this paper, indicating that the datasets are accessible to the public. The datasets include numerical source data for the graphs in the main figures, as well as detailed information about the datasets used, such as the number of protein sequences, nucleic-acid-binding residues, and non-nucleic-acid-binding residues.\n\nThe datasets were sourced from the BioLip database, which contains information on biological ligand-protein interactions. The specific datasets used are DNA-573_train, DNA-129_test, RNA-495_train, and RNA-117_test. These datasets were carefully curated to include protein chains containing nucleic-acid-protein binding sites, with annotations transferred to chains with the largest number of residues. The training sets were further refined using the CD-HIT method to ensure a rigorous assessment of the model's capabilities.\n\nThe data is available for use under the terms specified in the supplementary materials, ensuring that researchers can access and utilize the datasets for further studies. The availability of these datasets in a public forum promotes transparency and reproducibility in scientific research.",
  "optimization/algorithm": "The optimization algorithm employed in our work is the Adam optimizer, which is a widely used class of stochastic gradient descent algorithms. It is not a new algorithm; it was introduced by Diederik P. Kingma and Jimmy Ba in their 2014 paper \"Adam: A Method for Stochastic Optimization.\" The choice to use Adam in our study was driven by its efficiency and effectiveness in handling sparse gradients on noisy problems, which is common in deep learning tasks.\n\nThe reason it was not published in a machine-learning journal is that it is a well-established algorithm that has already been extensively covered in the literature. Our focus is on applying this proven optimization technique to the specific problem of nucleic acid-protein binding site prediction, rather than developing a new optimization algorithm.\n\nThe Adam optimizer was chosen for its adaptive learning rate capabilities, which allow it to adjust the learning rate for each parameter individually, leading to faster convergence and better performance. It combines the advantages of two other extensions of stochastic gradient descent, namely AdaGrad and RMSProp. The algorithm calculates adaptive learning rates for each parameter, which makes it well-suited for problems with sparse gradients, such as those encountered in natural language and computer vision models.",
  "optimization/meta": "In the optimization process of our model, we employ an ensemble learning approach to enhance performance and stability, particularly addressing the imbalance between positive and negative samples. This method involves creating multiple parallel models that work together to improve the overall prediction accuracy.\n\nThe ensemble setting utilizes Bagging, a popular ensemble technique that combines the results of several models to reduce variance and enhance generalization. Specifically, we build multiple learning processes where all positive samples are included, and negative samples are divided into subsets. Each model is trained on a different subset of the negative samples, ensuring that the entire set of negative samples is eventually used. This approach allows us to obtain several classifiers simultaneously.\n\nDuring the testing phase, the predictions from these multiple models are aggregated. The values obtained from each model for a given sample are summed and then processed through a softmax function to produce the final recognition results. This ensemble method helps to mitigate the effects of sample imbalance and improves the robustness of the predictions.\n\nThe integration of multiple models in this manner ensures that the training data for each model is independent, as each model is trained on a different subset of the negative samples. This independence is crucial for the effectiveness of the ensemble approach, as it helps to reduce the correlation between the models and thereby enhances the overall performance.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithm. We employed several methods to encode and preprocess the protein sequences.\n\nFirstly, we utilized Position-Specific Scoring Matrices (PSSM) and Hidden Markov Models (HMM) to capture evolutionary information. PSSM scores were obtained by matching sequences in the NCBI's non-redundant database with three iterations and an E-value threshold of 10^-3. These scores were then normalized using a sigmoid function to a range of [0, 1]. HMMs were calculated by aligning sequences in the uniclust30 database, generating matrices that include observed frequencies of amino acids, transition frequencies, and local diversity. Each value in the HMM matrix was normalized to [0, 1].\n\nTo incorporate physicochemical and biological characteristics, we selected features such as the number of atoms per amino acid, electrostatic charge, and potential hydrogen bonding. These features were normalized to ensure consistency. Additionally, we included Relative Amino Acid Propensity (RAA) and PKx values to represent biological properties. RAA indicates the propensity of amino acids for binding, showing the relative difference in abundance between binding and non-binding residues. PKx values represent the negative logarithm of the dissociation constant for different groups in the molecule.\n\nFor distinguishing between different amino acid species, we used one-hot encoding. This method constructs a 20-dimensional vector for each amino acid, with all values set to 0 except for one position, which is set to 1 to represent a specific amino acid type.\n\nWe also employed dynamic language embeddings, specifically ProtT5 and NABert embeddings, as inputs with feature matrices of (n, 1024). These embeddings were combined with the 75-dimensional features derived from PSSM, HMM, physicochemical characteristics, RAA, PKx, and one-hot encoding to construct the input features for our model.\n\nThe ProtT5 model was initially trained on a large dataset and then fine-tuned on UniRef50 to learn general semantic properties of protein sequences. NABert was further fine-tuned on our training sets to improve its performance in nucleic-acid-binding residue recognition. The hidden states from the last layer of the NABert model were extracted to generate the (n, 1024) matrix, where n is the length of the sequence markers.\n\nBy integrating these diverse features, our model was able to capture both global and local information of proteins, enhancing its predictive performance.",
  "optimization/parameters": "In our model, the number of parameters (p) is determined by the architecture and configuration of the neural network. The model includes several key components that contribute to the total number of parameters:\n\nThe stacked convolutional layers module consists of 13 normal blocks and 3 extra connected blocks, each containing two 1D convolutional layers. Each convolutional layer has 32 filters with a kernel size of 3. The stride in the 3 extra connected blocks is set to 3, while the others are set to 2. Additionally, there are four parallel convolutional layers with kernel sizes of 1, 3, 5, and 7, each with 64 filters.\n\nThe integrated multi-feature prediction module includes three LSTM layers, each containing 64 units. Dropout layers with rates of 0.4 and 0.5 are also included to prevent overfitting. Finally, a fully connected layer with 2 hidden units and a softmax activation function is used for the output.\n\nThe specific number of parameters (p) was not explicitly stated, as it depends on the detailed implementation and the dimensions of the input data. However, the architecture is designed to balance complexity and performance, ensuring that the model can effectively capture the necessary features for nucleic acid-protein binding site prediction.\n\nThe selection of parameters was guided by empirical testing and validation. The configuration of the convolutional and LSTM layers, as well as the dropout rates and fully connected layer, were chosen based on their performance in previous studies and through iterative experimentation. The model was trained using the Adam optimizer with a batch size of 64 and a learning rate of 0.0005, and early stopping was employed to prevent overfitting. The training was conducted on an NVIDIA GeForce RTX 3090 GPU, utilizing TensorFlow 2.7.0 and Keras 2.7.0. The parameters of the hidden layer in the model adopt the default initialization of Keras.",
  "optimization/features": "In our study, we utilized multiple features for protein representation, including PSSM, HMM, Bio (physicochemical features, PKx, and RAA, one-hot), and dynamic language embeddings. Specifically, we evaluated the impact of different feature combinations on our model's performance. These combinations included individual features like PSSM and HMM, as well as combinations such as PSSM+HMM, PSSM+HMM+Bio, HMM+Bio+Embeddings, and PSSM+HMM+Bio+Embeddings. The latter combination, which includes all the features, yielded the highest metrics, indicating that fusing multi-source biological features and natural language processing methods can learn more information about amino acid positions and deep semantic information about protein sequences.\n\nFeature selection was implicitly performed by evaluating different combinations of features. This process involved assessing the performance of the model with various feature sets to determine the most effective combination. The selection was done using the training set to ensure that the model's performance was optimized without introducing bias from the test set. The final model incorporated all the features, demonstrating the importance of each in improving the prediction accuracy.",
  "optimization/fitting": "In our study, we employed several strategies to ensure that our model, trained on TensorFlow 2.7.0 and Keras 2.7.0, did not suffer from overfitting or underfitting. The model's hidden layer parameters adopted the default initialization of Keras, which helps in providing a good starting point for training.\n\nTo address the potential issue of overfitting, given that the number of parameters in our deep learning model is indeed much larger than the number of training points, we implemented early stopping controlled by the validation loss. This technique halts the training process when the model's performance on the validation set starts to degrade, thereby preventing the model from memorizing the training data.\n\nAdditionally, we used dropout layers with rates of 0.4 and 0.5 in our network architecture. Dropout is a regularization technique that randomly sets a fraction of input units to 0 at each update during training time, which helps to prevent overfitting by ensuring that the model does not rely too heavily on any single neuron.\n\nTo mitigate underfitting, we designed a robust architecture that includes stacked convolutional layers, parallel convolutional layers with different kernel sizes, and LSTM layers. This architecture allows the model to capture both local and global features effectively. Furthermore, we used an Adam optimizer with a batch size of 64 and a learning rate of 0.0005, which helps in achieving a good balance between bias and variance.\n\nThe model was trained end-to-end on an NVIDIA GeForce RTX 3090 GPU, which provided the computational power needed to train the model efficiently. The use of a powerful GPU ensured that the model could converge to an optimal solution without being limited by computational constraints.\n\nIn summary, by employing early stopping, dropout layers, a well-designed network architecture, and an appropriate optimization algorithm, we were able to effectively address both overfitting and underfitting in our model.",
  "optimization/regularization": "In our study, we implemented an early stopping mechanism to prevent overfitting. This technique monitors the validation loss during training and halts the process when the loss stops improving, ensuring that the model does not become too specialized to the training data. This approach helps in maintaining the model's generalization capability on unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule are fully detailed in the publication. The model employs a stacked convolutional layers module with 13 normal blocks and 3 extra connected blocks, each containing two 1D convolutional layers. The convolutional layers use 32 filters with a kernel size of 3, and strides are set to 3 in the extra connected blocks and 2 in the others. Four parallel convolutional layers manage biological features with kernel sizes of 1, 3, 5, and 7, each with 64 filters.\n\nFor the integrated multi-feature prediction, three LSTM layers are used, each containing 64 units, along with dropout layers with rates of 0.4 and 0.5. The final layer is a fully connected layer with 2 hidden units and a softmax activation function. The ensemble setting includes 4 negative sample subsets. The model is trained using TensorFlow 2.7.0 and Keras 2.7.0, with default Keras initialization for hidden layer parameters. Training is conducted end-to-end with the Adam optimizer, a batch size of 64, and a learning rate of 0.0005. Early stopping is controlled by validation loss to prevent overfitting, and the model is trained on an NVIDIA GeForce RTX 3090 GPU.\n\nThe datasets used in this study are available on Figshare and GitHub. The numerical source data for the graphs in the main figures is provided in the supplementary data file. The details of the interpretability analysis, including attention heads distribution and attention scores computation, are available in Supplementary Note 10 and Supplementary Figure 9.",
  "model/interpretability": "The model employed in this study is not a black box; it incorporates mechanisms that enhance its interpretability. One key aspect is the use of attention maps, which visualize how different parts of the input sequence interact with each other. These maps are generated using scores from all attention heads within each layer of the model. For instance, in one of the layers, the second attention head primarily focuses on the token 'V', while the fourth attention head targets the token 'L'. This clustering of attention heads into separate regions helps in understanding which parts of the sequence the model is paying attention to, thereby augmenting its predictive capability.\n\nAdditionally, the model's interpretability is further validated through SHAP (SHapley Additive exPlanations) analysis. This analysis identifies the top features that contribute the most to the model's predictions. For example, in the DNA-binding residue identification task, the SHAP analysis highlights the most influential features within the biological, NABert embedding, and ProtT5 embedding categories. The visualizations show how different features impact the prediction outcomes, with higher SHAP values indicating a more positive effect on the prediction.\n\nThe attention scores and SHAP values collectively provide insights into the model's decision-making process, making it more transparent. This transparency is crucial for understanding the model's behavior and for validating its predictions in biological contexts, such as identifying functional sites in proteins.",
  "model/output": "The model is designed for a binary classification problem, specifically for predicting nucleic acid-protein binding sites. It employs several evaluation metrics typical for classification tasks, including Precision, Recall, F1-score, and Matthew\u2019s correlation coefficient (MCC). These metrics help assess the model's performance in distinguishing between positive and negative samples.\n\nThe model architecture includes stacked convolutional layers, LSTM layers, and dropout layers, which are commonly used in classification tasks to capture spatial and temporal dependencies in the data. The final layer uses a softmax activation function, which is standard for multi-class classification problems, but in this binary classification context, it effectively outputs probabilities for the two classes.\n\nTraining is conducted using the Adam optimizer with a specified batch size and learning rate, and early stopping is employed to prevent overfitting. The model is trained on an NVIDIA GeForce RTX 3090 GPU, indicating a focus on efficient and powerful computation suitable for complex classification tasks.\n\nIn summary, the model is tailored for classification, specifically for identifying binding sites in nucleic acid-protein interactions, and utilizes a range of techniques and metrics to ensure robust and accurate performance.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for our model is publicly available. It can be accessed on GitHub at the following link: https://github.com/Encryptional/SOFB. The datasets used in this study are also available on Figshare at https://figshare.com/articles/online_resource/SOFB_figshare_rar/2549945255. The code and datasets are released under the Creative Commons Attribution 4.0 International License, which allows for use, sharing, adaptation, distribution, and reproduction in any medium or format, provided that appropriate credit is given to the original authors and the source. This license permits these activities as long as a link to the Creative Commons license is provided and any changes made are indicated. For more detailed information on the license, you can visit http://creativecommons.org/licenses/by/4.0/.",
  "evaluation/method": "The evaluation of our method, SOFB, involved a comprehensive assessment using multiple performance metrics and datasets. We addressed nucleic acid-protein binding site prediction as a binary classification problem. To evaluate the performance, we utilized Recall (rec), Precision (pre), F1-score (F1), and Matthew\u2019s correlation coefficient (MCC). These metrics provide a robust evaluation framework, especially when the positive-negative sample ratio is not well balanced.\n\nThe calculation formulas for these metrics are as follows:\n\n- Precision measures the probability of being positive in all samples that are predicted to be positive.\n- Recall measures the probability of being predicted as positive in a sample that is positive.\n- F1-score is the harmonized average of precision and recall.\n- MCC uses all four elements of the confusion matrix (true positives, true negatives, false positives, and false negatives) to provide a balanced measure.\n\nAdditionally, we used the area under the receiver operating characteristic (ROC) curve (AUC) as a crucial evaluation metric. The AUC reflects the most comprehensive prediction performance, making it an important indicator of our model's effectiveness.\n\nTo demonstrate the superiority of SOFB, we conducted experiments controlling the number of protein chains in the training sets. The results, depicted in ROC curves, showed the model's performance across different training set sizes. Furthermore, we compared SOFB with multiple baseline methods using a violin plot and detailed performance metrics in Supplementary Table 3. The results clearly indicated that SOFB outperformed all other methods in terms of Recall, Precision, F1-score, MCC, and AUC.\n\nWe also performed additional experiments on other datasets, including YFK16, YK17, and MW15, which consisted of subsets for protein\u2013DNA and protein\u2013RNA binding. The evaluations involved SOFB and other baseline models, and the results, as shown in Supplementary Table 4, highlighted SOFB's superior performance. For instance, in DNA binding prediction, SOFB achieved the highest prediction AUC of 0.949, significantly outperforming other methods. Similarly, in RNA binding prediction, SOFB maintained its superior predictive capability, attaining the highest AUC of 0.94.\n\nThese evaluations collectively demonstrate the effectiveness and robustness of SOFB in predicting nucleic-acid binding in both DNA and RNA contexts.",
  "evaluation/measure": "In our study, we employ a comprehensive set of performance metrics to evaluate the effectiveness of our method for nucleic acid-protein binding site prediction. These metrics include Recall (Rec), Precision (Pre), F1-score (F1), Matthew\u2019s correlation coefficient (MCC), and the area under the receiver operating characteristic curve (AUC). These metrics are widely used in the literature for binary classification problems, ensuring that our evaluation is representative and comparable to other studies in the field.\n\nRecall measures the ability of the model to identify all relevant instances, while Precision assesses the accuracy of the positive predictions made by the model. The F1-score provides a harmonic mean of Precision and Recall, offering a balanced view of the model's performance. MCC is particularly useful when dealing with imbalanced datasets, as it takes into account all elements of the confusion matrix. AUC provides a comprehensive evaluation of the model's performance across all classification thresholds.\n\nBy reporting these metrics, we aim to provide a thorough assessment of our method's performance, highlighting its strengths and areas for improvement. This set of metrics is not only representative of current standards in the field but also ensures that our results are transparent and reproducible.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we thoroughly evaluated the performance of our proposed method, SOFB, against various publicly available methods and simpler baselines on benchmark datasets. To ensure a comprehensive assessment, we conducted experiments using multiple performance metrics and visualized the results through violin plots and ROC curves.\n\nFor the comparison with publicly available methods, we included a range of established techniques such as DNAMethods, COACH-D, SVMnuc, NucBind, DNABind, TargetDNA, DNAPred, NCBRPred, GraphBind, and iDRNA-ITF for DNA-binding tasks, and COACH-D, SVMnuc, NucBind, aaRNA, NucleicNet, RNABindRPlus, NCBRPred, GraphBind, and iDRNA-ITF for RNA-binding tasks. The results, summarized in Supplementary Table 3, clearly demonstrate that SOFB outperforms all these methods across several key metrics, including Precision, Recall, F1 score, Matthews Correlation Coefficient (MCC), and Area Under the Receiver Operating Characteristic Curve (AUROC).\n\nAdditionally, we compared SOFB with simpler baselines, including GaussianNB, SVM, KNN, Decision Tree, Random Forest, and XGBoost. The analysis, presented in Figure 3c, shows that SOFB successfully identifies more binding sites with fewer errors. For instance, in the DNA-binding test set, SOFB found all 20 binding sites, while other methods like SVM and KNN found fewer. Similarly, in the RNA-binding test set, SOFB identified 13 out of 16 binding sites with only one incorrect prediction, outperforming methods like Decision Tree and Random Forest.\n\nFurthermore, we compared SOFB with deep learning algorithms, including TextCNN, RNN, ResNet, DeepForest, and MLP. The results, depicted in Figure 3d, indicate that SOFB achieves superior performance, with AUC values of 93.4% and 86.5% for DNA-binding and RNA-binding test sets, respectively. SOFB's performance metrics, such as MCC and F1 score, also show significant improvements over these deep learning methods.\n\nIn summary, the extensive comparisons with both publicly available methods and simpler baselines on benchmark datasets confirm the robustness and effectiveness of SOFB in predicting nucleic-acid binding residues.",
  "evaluation/confidence": "The evaluation of our method, SOFB, includes several performance metrics such as Precision, Recall, F1-score, Matthew's Correlation Coefficient (MCC), and the Area Under the Receiver Operating Characteristic Curve (AUC). These metrics are crucial for assessing the effectiveness of our model in predicting nucleic acid-protein binding sites.\n\nWhile the specific details about confidence intervals for these metrics are not provided, the results are presented in a manner that highlights the superior performance of SOFB compared to other baseline methods. The use of multiple metrics ensures a comprehensive evaluation, and the consistent outperforming of SOFB across these metrics suggests statistical significance.\n\nThe violin plot in Supplementary Figure 4 visually represents the distribution of predictive metrics for different models, with SOFB showing superior overall performance. Additionally, Supplementary Table 3 provides detailed values of Pre, Rec, F1, MCC, and AUROC for various baseline methods combined with SOFB, further emphasizing its effectiveness.\n\nThe statistical significance of our results is supported by the consistent superiority of SOFB across different performance metrics and tasks. The detailed comparison in Supplementary Table 3 and the visual representation in Supplementary Figure 4 provide a robust basis for claiming that SOFB is superior to other methods.",
  "evaluation/availability": "The raw evaluation files for the graphs presented in the main figures are available. These files contain the numerical source data used to generate the graphs, providing transparency and enabling reproducibility of the results. The data is included in a supplementary file named \"Supplementary Data\". This file is intended to support the main findings and allow for further analysis or verification by interested researchers. The availability of this data underscores the commitment to open science and facilitates the validation of the study's conclusions."
}