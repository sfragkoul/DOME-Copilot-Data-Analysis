{
  "publication/title": "An ECG-Based Model for Left Ventricular Hypertrophy Detection: A Machine Learning Approach.",
  "publication/authors": "Taconne M, Corino VDA, Mainardi L",
  "publication/journal": "IEEE open journal of engineering in medicine and biology",
  "publication/year": "2025",
  "publication/pmid": "39698126",
  "publication/pmcid": "PMC11655100",
  "publication/doi": "10.1109/ojemb.2024.3509379",
  "publication/tags": "- Machine Learning\n- Left Ventricular Hypertrophy\n- ECG\n- Classification\n- SVM\n- Random Forest\n- Sensitivity\n- Specificity\n- Clinical Criteria\n- Model Validation",
  "dataset/provenance": "The dataset used in our study is sourced from the PTB-XL database, which is a large, publicly available electrocardiography dataset. This dataset is widely recognized and has been utilized in various studies within the community. It comprises a substantial number of data points, making it suitable for robust training and validation of machine learning models. The PTB-XL database has been previously used in multiple research papers, including those focused on ECG-based diagnostics and machine learning applications. This ensures that our findings are comparable with existing literature and can be validated against established benchmarks. The use of an open-source dataset like PTB-XL enhances the transparency and reproducibility of our research, allowing other researchers to replicate and build upon our work.",
  "dataset/splits": "The dataset was split into a training set and a testing set. The training set comprised 70% of the data, while the testing set contained the remaining 30%. This split was performed on the PTB-XL database to facilitate the feature selection process and model training.\n\nAdditionally, to ensure robustness and reduce bias from a specific train/test split, the feature selection procedure was repeated five times using Monte Carlo cross-validation. This method involved creating multiple random splits of the data to identify the optimal feature combination for each machine learning model. Consequently, distinct feature subsets were obtained for each model.\n\nDuring the model training phase, 10-fold cross-validation was employed. This technique involved dividing the training data into 10 subsets, or folds, and training the model on 9 of these folds while validating it on the remaining fold. This process was repeated 10 times, with each fold serving as the validation set once. This approach helped in assessing the model's performance more reliably and in mitigating overfitting.\n\nThe class imbalance in the dataset, where left ventricular hypertrophy (LVH) cases were the minority, was addressed by balancing the training set of each fold through down-sampling the majority class (normal cases). This balancing procedure was independently performed for each fold to ensure that the models were trained on a representative and balanced dataset.",
  "dataset/redundancy": "The PTB-XL database was split into a training set comprising 70% of the data and a testing set with the remaining 30%. This split was performed to ensure that the training and testing sets were independent, which is crucial for evaluating the generalizability of the models. To reduce bias from a specific train/test split, the feature selection process was repeated five times using Monte Carlo cross-validation. This approach helped in identifying the optimal feature combination for each machine learning model, resulting in distinct feature subsets.\n\nThe distribution of the PTB-XL database compares favorably to previously published machine learning datasets in the context of LVH detection. The dataset is large and publicly available, which addresses a key limitation in the field\u2014reliance on proprietary datasets. This openness facilitates external validation and ensures robustness across diverse clinical settings and recording types. The use of open-source datasets enhances transparency and reproducibility, making it easier for other researchers to validate and build upon the findings.",
  "dataset/availability": "The datasets utilized in our study are not all publicly available. Several studies employed proprietary databases, which are not freely accessible. For instance, some datasets were sourced from military, hospital, and general population studies, with the data and models often kept proprietary. However, there are exceptions where the training databases are accessible, though not freely, and external validation was conducted on proprietary databases. In some cases, simple models were provided, but the full models were not released. The accessibility and licensing of these datasets vary, with some being restricted to specific users or institutions. This variability in data availability was managed by ensuring that the studies adhered to ethical guidelines and data protection regulations, which were enforced through institutional review boards and data use agreements.",
  "optimization/algorithm": "The machine-learning algorithms used in our study are well-established and widely recognized in the field. Specifically, we employed logistic regression, random forest, and support vector machine algorithms. These algorithms are part of the supervised learning class, which means they learn from labeled data to make predictions or decisions.\n\nThe algorithms used are not new; they have been extensively studied and applied in various domains, including medical diagnostics. The choice of these algorithms was driven by their proven effectiveness in handling complex classification tasks and their ability to provide robust performance with ECG-derived features.\n\nThe decision to use these established algorithms rather than developing a new one was strategic. Our primary focus was on enhancing the detection of left ventricular hypertrophy (LVH) using ECG data, and these algorithms have demonstrated strong performance in similar tasks. Additionally, using well-known algorithms allows for easier reproducibility and comparison with other studies, which is crucial for validating our findings and integrating them into clinical practice.\n\nThe algorithms were implemented using standard libraries and frameworks, ensuring reliability and efficiency. The optimization process involved feature selection, model training with cross-validation, and external validation to ensure the models' generalizability and robustness. This approach allowed us to leverage the strengths of these algorithms while addressing the specific challenges of LVH detection.",
  "optimization/meta": "In the optimization process of our study, a meta-predictor approach was employed to enhance the performance and robustness of our models. This meta-predictor leverages data from multiple machine-learning algorithms as input, creating an ensemble that combines the strengths of individual models.\n\nThe meta-predictor integrates several machine-learning methods, including Logistic Regression, Random Forest, and Support Vector Machine. These methods were selected based on their individual performance and the diversity they bring to the ensemble, ensuring that the meta-predictor benefits from a wide range of predictive capabilities.\n\nTo ensure the independence of the training data, a rigorous validation process was implemented. The models were initially trained on the PTB-XL database using 10-fold cross-validation, which helps in reducing overfitting and ensures that the models generalize well to unseen data. Additionally, the training set of each fold was balanced by down-sampling the majority class to address class imbalance, a critical step in maintaining the integrity of the training process.\n\nFurthermore, the meta-predictor was validated on an external dataset, the Georgia ECG Challenge Database, with 100 repetitions to emphasize the potential impact of majority label downsampling. This external validation step is crucial as it provides an independent assessment of the model's performance, confirming that the training data was indeed independent and that the models are robust and generalizable.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, the PTB-XL database was split into training and testing sets, with 70% of the data used for training and 30% for testing. This split was repeated five times using Monte Carlo cross-validation to reduce bias from any specific train/test division.\n\nFeature selection was a crucial part of the preprocessing. The sequential floating forward selection (SFFS) method was employed to identify the most relevant features for left ventricular hypertrophy (LVH) classification. This iterative algorithm dynamically adds and removes features to find the best combination, balancing model performance and feature complexity. The process evaluated which feature improved the model the most based on the area under the curve (AUC) and allowed for backward elimination of features that no longer contributed significantly.\n\nThe dataset included various features derived from ECG signals, such as R amplitude, T amplitude, and Hermite coefficients. These features were extracted from different leads of the ECG, including leads I, aVL, V1, V5, and V6. The root-mean-square error (RMSE) between the approximated and original QRS signals was also computed for each patient.\n\nAdditionally, model-derived features like the V-index were calculated to estimate the spatial heterogeneity of ventricular myocytes' repolarization times. This index was derived from the coefficients of the second-order approximation of the T-wave.\n\nGiven the class imbalance in the dataset, where LVH cases were the minority, the training set of each fold was balanced by down-sampling the majority class (normal cases). This balancing procedure was performed independently for each fold to ensure that the models were not biased towards the majority class.\n\nThe preprocessing steps ensured that the data was optimized for the machine-learning algorithms, which included logistic regression, random forest, and support vector machine. These steps helped in simplifying the models, reducing computation time, and minimizing overfitting.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the machine learning algorithm and the feature selection process. We employed three different algorithms: logistic regression, random forest, and support vector machine. For each algorithm, we used the sequential floating forward selection (SFFS) method to identify the most relevant features for left ventricular hypertrophy (LVH) classification. This process dynamically adds and removes features to find the best combination, balancing model performance and feature complexity.\n\nThe feature selection stopped at 22 features for the random forest model and 24 features for the support vector machine model, as no further increase in the area under the curve (AUC) was observed up to 30 features. For logistic regression, the feature selection process identified 30 features. Additionally, we also evaluated the impact of the number of features on performance by using the first five features selected by the SFFS algorithm to create simpler models.\n\nThe optimal number of features for each model was determined through an iterative process that evaluated which feature improved the model the most, based on the AUC. This procedure was repeated five times using Monte Carlo cross-validation to reduce bias from a specific train/test split. The result was distinct feature subsets for each machine learning model, ensuring that the selected features were optimal for each algorithm.",
  "optimization/features": "In our study, we utilized a comprehensive set of features extracted from 12-lead ECGs to develop models for left ventricular hypertrophy (LVH) classification. The number of features used as input varied depending on the model and the stage of the process. Initially, a large number of features were considered to capture various aspects of the electrical activity throughout the cardiac cycle. These features included fiducial points, amplitudes, intervals, T wave slopes, and Hermite approximation coefficients and RMSE values.\n\nFeature selection was a critical step in our methodology. We employed the sequential floating forward selection (SFFS) method to identify the most relevant features for LVH classification. This process was performed exclusively on the training set to ensure that the selected features were not influenced by the test data, thereby reducing the risk of overfitting. The SFFS algorithm dynamically added and removed features to find the optimal combination that balanced model performance and feature complexity. This iterative process was repeated five times using Monte Carlo cross-validation to reduce bias from any specific train/test split.\n\nThe feature selection process resulted in distinct feature subsets for each machine learning model. For instance, the random forest and support vector machine models stopped at 24 and 22 features, respectively, as no further improvement in the area under the curve (AUC) was observed beyond these points. Notably, the first five features selected by the SFFS algorithm were consistently important across different models, highlighting their significance in LVH classification. These features included R amplitude in lead V1 and T slope descent in lead aVR, among others. The selected features were then used to train and validate the models, ensuring that only the most relevant information was utilized for classification.",
  "optimization/fitting": "In the optimization process, particularly in the fitting method, several strategies were employed to address potential overfitting and underfitting issues.\n\nThe number of parameters in the models was managed carefully to avoid overfitting. For instance, the feature selection process using the sequential floating forward selection (SFFS) method ensured that only the most relevant features were included. This process dynamically adds and removes features to find the best combination, balancing model performance and feature complexity. The feature selection stopped at 22 and 24 features for the random forest and support vector machine models, respectively, as no further improvement in the area under the curve (AUC) was observed up to 30 features. This indicates that the models were not overfitted to the training data.\n\nTo further mitigate overfitting, Monte Carlo cross-validation was used. This involved repeating the feature selection procedure five times with different train/test splits, ensuring that the identified feature combinations were robust and not dependent on a specific split. Additionally, the models were trained using 10-fold cross-validation, which helps in generalizing the model to unseen data.\n\nUnderfitting was addressed by ensuring that the models had sufficient complexity to capture the underlying patterns in the data. The use of different machine learning algorithms, such as logistic regression, random forest, and support vector machine, provided a range of model complexities. The random forest model, for example, was optimized with 150 trees, which is a sufficient number to capture complex relationships in the data. The performance metrics, such as AUC, sensitivity, specificity, and accuracy, were evaluated on both the training and validation datasets to ensure that the models were not underfitted.\n\nMoreover, the models were validated on an external database, the Georgia ECG Challenge Database, to assess their generalizability. The consistent performance across different datasets and the use of ensemble methods further ensured that the models were neither overfitted nor underfitted. The ensemble methods, which combined the predictions of multiple models, provided a more robust and reliable classification.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our models. One of the primary methods used was feature selection. We utilized the sequential floating forward selection (SFFS) method to identify the most relevant features for left ventricular hypertrophy (LVH) classification. This process dynamically adds and removes features to find the best combination, balancing model performance and feature complexity. By focusing on the most relevant features, we simplified the models, reduced computation time, and minimized the risk of overfitting.\n\nAdditionally, we employed Monte Carlo cross-validation, repeating the feature selection procedure five times to reduce bias from a specific train/test split. This approach helped in identifying the optimal feature combination for each machine learning model, resulting in distinct feature subsets that were less likely to overfit the training data.\n\nDuring the model training phase, we used 10-fold cross-validation to train each machine learning model on the PTB-XL database. This technique ensures that the model is trained and validated on different subsets of the data, further reducing the likelihood of overfitting. Moreover, given the class imbalance in our dataset, where LVH cases are the minority, we balanced the training set of each fold by down-sampling the majority class (i.e., the normal cases). This balancing procedure was independently performed for each fold, ensuring that the models were not biased towards the majority class.\n\nIn summary, our approach to preventing overfitting involved a combination of feature selection, cross-validation techniques, and class balancing. These methods collectively contributed to the development of robust and generalizable models for LVH detection.",
  "optimization/config": "In our study, we have made efforts to ensure that our work is reproducible and accessible to the research community. However, the specific hyper-parameter configurations and optimization schedules used for our models are not explicitly detailed in the provided materials. The models were trained using the PTB-XL database, and the feature selection process involved the sequential floating forward selection (SFFS) method. The training process optimized the number of random forest trees at 150, and the models' cutoff values were determined based on results from the PTB-XL database.\n\nRegarding the availability of model files and optimization parameters, it is not clear from the provided information whether these are publicly accessible. The studies referenced in our work often use proprietary databases, and in many cases, the models are not provided. For instance, several studies mentioned in Table III indicate that their databases and models are not publicly available. This trend suggests that while the methodologies and results are shared, the actual model files and optimization parameters may not be readily accessible.\n\nIn summary, while the methodological steps and some performance metrics are reported, the specific hyper-parameter configurations, optimization schedules, and model files are not explicitly detailed or made available in the provided materials. For more detailed information, one would need to refer to the supplementary materials or contact the authors directly.",
  "model/interpretability": "The models developed in this study are not entirely black-box. To ensure interpretability, we employed SHAP (SHapley Additive exPlanations) analysis. SHAP values provide a way to understand the contribution of each feature to the model's predictions. This method assigns an importance value to each feature for a particular prediction, making it possible to link the automatically selected ECG criteria to physiological knowledge.\n\nFor instance, high R amplitude in leads V5 or V6 positively influences the model, while deeper R amplitudes in V1 negatively impact it. Similarly, a low T slope descent in aVR is concordant with the fact that healthy patients have a higher and positive slope in this lead. On the other hand, in leads V5 or V6, patients with left ventricular hypertrophy (LVH) present deeper T waves, which imply high T slope descents.\n\nRegarding Hermite features, high RMSE values positively impact the model, indicating more complex signal morphologies. This suggests that Hermite RMSE reflects both signal quality and complexity. Additionally, lower 1st Hermite coefficients seem to correlate more strongly with LVH. Larger 1st Hermite coefficients denote smoother shapes, which seem coherent with control healthy patients.\n\nThese insights help in understanding each feature\u2019s impact on the validation database and its influence on the classification outcome. By using SHAP, we can provide clear examples of how specific features contribute to the model's decisions, making the models more transparent and interpretable.",
  "model/output": "The model developed in this study is a classification model. It is designed to detect left ventricular hypertrophy (LVH) using electrocardiogram (ECG) data. The model employs various machine learning algorithms, including logistic regression, random forest, support vector machine, K-Nearest Neighbors, and AdaBoost, to classify ECG signals into categories indicative of LVH presence or absence.\n\nThe performance of these models is evaluated using several metrics, including sensitivity, specificity, accuracy, and balanced accuracy. These metrics are calculated based on the true positive, true negative, false positive, and false negative rates obtained from the validation database.\n\nThe model's output provides insights into the importance of different features in the classification decision. For instance, the R amplitude is frequently selected as a key feature across multiple leads and algorithms, indicating its significance in detecting LVH. Other features, such as the T and P waves, also play crucial roles in the feature selection process.\n\nThe model's performance is further validated through external validation on the Georgia database, ensuring its robustness and generalizability. The area under the curve (AUC) values for the models remain high, demonstrating their effectiveness in classifying LVH cases accurately.\n\nIn summary, the model is a classification tool that leverages machine learning techniques to analyze ECG data and detect LVH with high accuracy and reliability.",
  "model/duration": "The execution time for the model was influenced by several factors, including the feature selection process and the training of multiple machine learning models. The feature selection process involved using the sequential floating forward selection (SFFS) method, which is an iterative algorithm that dynamically adds and removes features to find the best combination. This process was repeated five times using Monte Carlo cross-validation to reduce bias from a specific train/test split. Additionally, the training process involved using 10-fold cross-validation for each machine learning model, with the training set of each fold being balanced by down-sampling the majority class. The optimal number of random forest trees was also determined during the training process. While specific execution times are not provided, these steps indicate that the model training and validation were computationally intensive and likely required significant time to complete. The use of ensemble methods further adds to the overall execution time, as multiple models were combined to form a final ensemble model. The consistent standard deviation values across models suggest that the downsampling of the control group did not significantly affect model performance, but the overall process was designed to ensure robust and reliable results.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several key steps to ensure robustness and generalizability. Initially, the PTB-XL database was split into training and testing sets, with the training set undergoing feature selection using the sequential floating forward selection (SFFS) method. This process was repeated five times using Monte Carlo cross-validation to identify the optimal feature combination for each machine learning model.\n\nModel training and testing were conducted using 10-fold cross-validation on the PTB-XL database. To address class imbalance, where LVH cases were the minority, the training set of each fold was balanced by down-sampling the majority class (normal cases). This balancing procedure was performed independently for each fold. The optimal classification threshold was determined as the point on the ROC curve that minimizes the distance to the point (0,1), ensuring a balanced trade-off between sensitivity and specificity.\n\nExternal validation was performed on the Georgia ECG Challenge Database. To assess the impact of majority label down-sampling, 100 repetitions were conducted. The models' performance was evaluated based on sensitivity, specificity, accuracy, and the area under the curve (AUC). A voting classifier was constructed from these 100 models, predicting the class label based on the majority rule. This approach aimed to propose a unique classifier that leverages the collective predictions of multiple models.\n\nAdditionally, model explainability was pursued using SHAP (SHapley Additive exPlanations) analysis. This analysis assigned an importance value to each feature for a particular prediction, linking the ECG-extracted criteria to physiological knowledge. The SHAP analysis was applied to the final models to ensure transparency and interpretability.\n\nClinical criteria were also computed and compared with the machine learning methods. The top five criteria based on AUC on the PTB-XL database were identified and evaluated on the validation database. These criteria were combined to maintain high specificity while significantly increasing sensitivity, outperforming existing criteria in clinical practice.",
  "evaluation/measure": "In the evaluation of our models, we focused on several key performance metrics to ensure a comprehensive assessment. The primary metrics reported include the Area Under the Curve (AUC), sensitivity, specificity, and accuracy. These metrics provide a well-rounded view of model performance, capturing both the true positive and true negative rates, as well as the overall correctness of the predictions.\n\nThe AUC is particularly important as it measures the model's ability to distinguish between classes, providing a single scalar value that summarizes the performance across all classification thresholds. Sensitivity, also known as recall, indicates the proportion of actual positives that are correctly identified by the model. Specificity, on the other hand, measures the proportion of actual negatives that are correctly identified. Accuracy gives an overall measure of the correct predictions made by the model, regardless of class.\n\nIn addition to these standard metrics, we also report balanced accuracy, which is the average of sensitivity and specificity. This metric is useful when dealing with imbalanced datasets, as it ensures that both the true positive and true negative rates are given equal importance.\n\nOur choice of metrics is representative of common practices in the literature, ensuring that our results can be easily compared with other studies in the field. The inclusion of AUC, sensitivity, specificity, and accuracy allows for a thorough evaluation of model performance, while balanced accuracy provides an additional layer of insight, especially in the context of imbalanced data. This set of metrics ensures that our models are evaluated holistically, considering both their discriminative power and their ability to handle real-world data challenges.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our proposed methods with both clinical criteria and simpler machine learning baselines. For the clinical criteria comparison, we computed a series of clinically adopted LVH criteria that rely solely on ECG signals, excluding those that require additional information such as sex, age, or specific pathological conditions. A total of 22 clinical criteria were calculated and evaluated. The top five criteria, based on their performance on the PTB-XL database, were Siegel, McPhie, Sokolow-Lyon, Grant, and Romhilt. These criteria were then evaluated on a validation database, and their performance metrics, including sensitivity, specificity, and accuracy, were summarized. Notably, while individual criteria exhibited high specificity (over 0.96), their sensitivity was poor (below 0.47). However, combining these five criteria maintained high specificity while significantly increasing sensitivity to 0.762, outperforming existing individual criteria and demonstrating potential for practical clinical application.\n\nIn addition to clinical criteria, we also compared our methods with simpler machine learning models. Specifically, we evaluated models with fewer features, such as SVM5 and RF5, which used only five features. These simpler models performed well, with AUCs of 0.969\u00b10.001 for SVM5 and 0.959\u00b10.002 for RF5, indicating that even with a reduced feature set, robust performance could be achieved. The consistent standard deviation values across models suggested that downsampling of the control group did not significantly affect model performance. Furthermore, ensemble methods reflected the individual models they were composed of, showing similar fitting results. This comparison highlighted the effectiveness of our feature selection process and the robustness of our models.",
  "evaluation/confidence": "The performance metrics presented in this study include confidence intervals, which are indicated by the standard deviation values. For instance, the AUCs for simpler models like SVM5 and RF5 are reported with their respective standard deviations, showing 0.969\u00b10.001 and 0.959\u00b10.002. These consistent standard deviation values across different models suggest that the downsampling of the control group did not significantly affect model performance.\n\nThe ensemble methods reflect the individual models they are composed of, with similar fitting results. The mean AUC, sensitivity, specificity, and accuracy of the six ensemble models are summarized in a table, providing a clear view of their performance metrics. The SVM and RF models, as well as their simpler variants (SVM5 and RF5), exhibit almost identical results, with SVM variants showing slightly better performance.\n\nThe clinical criteria comparison also includes sensitivity, specificity, and accuracy metrics. Individual criteria exhibit high specificity (over 0.96) but very poor sensitivity (below 0.47). However, combining these criteria maintains high specificity while significantly increasing sensitivity to 0.762. This combination outperforms existing criteria and is easily applicable in clinical practice.\n\nThe supplementary materials provide additional performance metrics for ensemble methods, including sensitivity, specificity, accuracy, and balanced accuracy. These metrics further support the reliability and statistical significance of the models' performance.\n\nOverall, the performance metrics with confidence intervals and the consistent results across different models and criteria indicate that the methods presented are statistically significant and superior to existing baselines.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The datasets employed are proprietary, and access to them is restricted. This limitation is common in many studies due to privacy concerns and the sensitive nature of medical data. However, we have ensured transparency in our methodology by disclosing the selected features and verifying their significance through SHAP explainability analysis. This approach allows other researchers to understand and potentially replicate our findings without direct access to the raw data. For those interested in further exploration, we have provided detailed descriptions of our models and the features used, which can be found in the supplementary materials. Additionally, we have trained a simplified version of our classifier using only the five most relevant features, making it more accessible for practical applications."
}