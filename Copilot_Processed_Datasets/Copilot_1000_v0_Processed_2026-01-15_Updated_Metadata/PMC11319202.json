{
  "publication/title": "Unsupervised representation learning on high-dimensional clinical data improves genomic discovery and prediction.",
  "publication/authors": "Yun T, Cosentino J, Behsaz B, McCaw ZR, Hill D, Luben R, Lai D, Bates J, Yang H, Schwantes-An TH, Zhou Y, Khawaja AP, Carroll A, Hobbs BD, Cho MH, McLean CY, Hormozdiari F",
  "publication/journal": "Nature genetics",
  "publication/year": "2024",
  "publication/pmid": "38977853",
  "publication/pmcid": "PMC11319202",
  "publication/doi": "10.1038/s41588-024-01831-6",
  "publication/tags": "- Scientific Research\n- Biomedical Studies\n- Molecular Biology\n- Genetic Analysis\n- Experimental Data\n- Biological Systems\n- Research Methods\n- Data Interpretation\n- Scientific Discovery\n- Biomedical Innovations",
  "dataset/provenance": "The datasets utilized in this study were sourced from several large-scale biobanks and research initiatives. The UK Biobank provided a substantial portion of the data, with the research conducted under Application Number 65275. The COPDGene study, supported by NIH grants U01 HL089856 and U01 HL089897, also contributed significant data. Additionally, the EPIC-Norfolk study, funded by the Medical Research Council and Cancer Research UK, was a key data source. The Indiana Biobank, supported by the Indiana Clinical and Translational Sciences Institute, provided further data. The eMERGE III study, accessible through dbGaP under accession phs001584.v2.p2, was another crucial dataset. Specific institutions like the Cincinnati Children\u2019s Hospital Medical Center, Mayo Clinic, and Vanderbilt University also contributed data through their respective genomic and phenotype studies. The datasets from these sources have been used in various previous research efforts and by the broader scientific community, ensuring a robust and well-vetted foundation for our analyses. The exact number of data points varies across datasets, but collectively, they represent a comprehensive and diverse set of genomic and phenotypic information.",
  "dataset/splits": "In our study, we utilized data from the UK Biobank, focusing on individuals with genetically inferred European ancestry. The dataset was divided into several splits for different analyses.\n\nFor the modeling of spirometry data, we initially considered all individuals with valid spirograms, totaling 325,027 individuals. This dataset was split into a training set comprising 80% of the data and a validation set with the remaining 20%. After removing related individuals, the training set consisted of 259,692 individuals, and the validation set included 65,266 individuals.\n\nAdditionally, individuals with invalid spirograms were used as a polygenic risk score (PRS) holdout set, which included 110,739 individuals. This holdout set was not used in the machine learning modeling or genome-wide association studies (GWASs).\n\nFor the modeling of photoplethysmogram (PPG) data, we considered all individuals with PPGs, totaling 170,714 individuals. This dataset was also split into a training set with 80% of the data and a validation set with the remaining 20%. After removing related individuals, the training set consisted of 112,730 individuals, and the validation set included 28,545 individuals.\n\nIn summary, our dataset splits included training and validation sets for both spirometry and PPG data, as well as a PRS holdout set for individuals with invalid spirograms. The distribution of data points in each split was designed to ensure robust training and validation of our models.",
  "dataset/redundancy": "The datasets used in this study were carefully split to ensure independence between training and test sets. For the modeling dataset, which included individuals with valid spirograms and PPGs, we considered a total of 325,027 individuals. This dataset was divided into training and validation sets, with 80% of the data allocated for training and 20% for validation. This split was designed to minimize the risk of overfitting and to ensure that the model's performance could be reliably evaluated on unseen data.\n\nTo further enforce independence, individuals with invalid spirograms were used as a separate holdout set for polygenic risk score (PRS) evaluations. This holdout set consisted of European genetically inferred ancestry (GIA) individuals who were not included in the machine learning (ML) modeling or genome-wide association studies (GWAS). This approach ensured that the PRS evaluations were conducted on a completely independent dataset, providing a robust assessment of the model's generalizability.\n\nThe distribution of the datasets used in this study compares favorably to previously published ML datasets in terms of size and diversity. The large number of individuals included in the modeling dataset, along with the careful splitting into training and validation sets, ensures that the model is trained on a representative sample of the population. Additionally, the use of a separate holdout set for PRS evaluations adds an extra layer of validation, ensuring that the model's performance is not overestimated.\n\nIn summary, the datasets were split in a manner that ensures independence between training and test sets, with a careful consideration of the distribution to match or exceed the standards of previously published ML datasets. This approach provides a strong foundation for the reliability and generalizability of the model's performance.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study is the Adam optimizer, a widely-used class of adaptive learning rate optimization algorithms. Adam is not a new algorithm; it was introduced by Diederik P. Kingma and Jimmy Ba in their 2014 paper \"Adam: A Method for Stochastic Optimization.\" This algorithm is well-established in the machine learning community and is known for its efficiency and effectiveness in training deep learning models.\n\nThe reason Adam was not published in a machine-learning journal is that it has already been extensively validated and accepted within the field. It is a standard choice for many deep learning tasks due to its ability to handle sparse gradients on noisy problems, making it suitable for a wide range of applications, including our work on unsupervised representation learning for genomic discovery and disease risk prediction.\n\nThe Adam optimizer was chosen for its robustness and adaptability, which are crucial for training complex models like convolutional Variational Autoencoders (VAEs). These models require careful tuning of hyperparameters, and Adam's adaptive learning rate helps in achieving stable and efficient convergence. Additionally, the absence of a learning rate scheduler in our training procedure ensures that the learning rate remains consistent throughout the training process, which is beneficial for maintaining the stability of the model.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. Instead, it leverages a label-efficient approach for combining polygenic risk scores (PRSs) from genome-wide association studies (GWAS) on several learned coordinates. Specifically, each coordinate PRS retains its original effect sizes, and a disease-specific PRS is constructed as a learned weighted sum of a handful of coordinate PRSs. This approach allows for the adaptation of premade PRSs for risk prediction in new settings with very few disease labels.\n\nThe method involves developing a mechanism for identifying genetic influences on organ function in the absence of labeled data. It naturally admits incorporating expert features into the model and provides a method to create disease/trait-specific PRSs with minimal labels. The training data for the model is derived from individuals of European genetic ancestry, and the PRS evaluation was performed on multiple datasets and ancestries. However, the impact of ancestry-specific model training was not explored.\n\nThe model architecture and training strategies were not fully optimized specifically for genomic discovery. The VAE objective, particularly the reconstruction error, may not be optimal for genetic analyses. Future research could focus on explicitly incorporating an objective to maximize the heritability of the learned representation. Additionally, the model was trained on spirograms and photoplethysmogram (PPG) obtained from the UK Biobank, which may limit its generalizability to other datasets. Further investigation with additional datasets having the same data modality would be beneficial to assess the generalizability of the encodings.",
  "optimization/encoding": "The data encoding process involved several steps to prepare spirograms and photoplethysmograms (PPGs) for machine learning analysis. For spirograms, we utilized both classic readouts and residual encodings. The classic readouts included five well-established spirogram measurements. Additionally, we employed a Variational Autoencoder (VAE) to generate residual spirogram encodings, which captured information beyond the classic readouts. These residual encodings were derived by injecting five expert-defined features (EDFs) into the VAE, allowing the model to learn additional latent dimensions that represent residual biological signals.\n\nFor PPGs, a similar approach was taken. We constructed residual photoplethysmogram encodings (RPLENCs) by injecting five PPG EDFs, which included the absence of notch, the position of notch, the position of peak, the position of shoulder, and the peak-to-peak time. This process enabled the model to capture residual information in PPGs that was not accounted for by the classic EDFs.\n\nThe model architecture for the VAE included several convolutional layers followed by max-pooling layers, which reduced the dimensionality of the input data. The encoder part of the VAE consisted of multiple convolutional layers with increasing numbers of filters, followed by max-pooling layers. The output of the encoder was then flattened and passed through several dense layers to produce the mean and log-variance of the latent space. The latent space was sampled using a Gaussian distribution, and the sampled points were concatenated with the injected EDFs.\n\nThe training procedure involved using the Adam optimizer with varying learning rates and batch sizes. The models were trained for 100 epochs, and the final hyperparameters were chosen to minimize the VAE loss in the validation set. The encoders of the trained models were used to generate encodings for each individual, using the mean value of the learned Gaussian distribution of the encodings. The learned variance for the VAE was not utilized in the final encodings.\n\nFor dimensionality reduction, we also performed Principal Component Analysis (PCA) and cubic spline fitting on spirograms as baseline methods. For PCA, we concatenated volume\u2013time and flow\u2013time curves and used both as inputs. For cubic spline fitting, we used only volume\u2013time curves, as cubic splines perform better for smoother curves. We generated five principal components and five cubic spline coefficients to match the number of EDFs and the dimension of the spirogram encodings.\n\nIn summary, the data encoding process involved using both classic readouts and residual encodings derived from VAEs, with injected EDFs to capture additional biological signals. The model architecture and training procedure were designed to optimize the encoding of spirograms and PPGs for further analysis.",
  "optimization/parameters": "The model architecture consists of a total of 72,091 parameters, all of which are trainable. The selection of these parameters was guided by the need to balance model complexity and performance. Specifically, the architecture was designed to include layers that capture essential features from the input data while maintaining computational efficiency.\n\nThe encoder part of the model includes convolutional layers with specific kernel sizes, strides, and padding, followed by dense layers that reduce the dimensionality of the data. The decoder mirrors this structure, using upsampling and transposed convolutional layers to reconstruct the input from the latent space representation.\n\nThe choice of the number of parameters was influenced by the requirement to achieve strong reconstruction performance while keeping the total number of latent dimensions comparable to those in a related model. This balance was crucial for ensuring that the model could generalize well to new data without overfitting. The final architecture was selected based on extensive hyperparameter tuning, which involved evaluating different configurations of learning rates and batch sizes to minimize the validation loss. This process ensured that the model's parameters were optimized for the task at hand.",
  "optimization/features": "The input features for our models are derived from clinical data, specifically spirograms and photoplethysmograms (PPGs). For spirograms, we use volume\u2013time and flow\u2013time curves as inputs. For PPGs, we utilize the raw signal data. The number of input features corresponds to the dimensionality of these curves, which is high-dimensional clinical data.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, we employ a convolutional Variational Autoencoder (VAE) to learn representations of these high-dimensional data in an unsupervised manner. This approach allows us to capture complex patterns and encode additional genetic information that is not captured by traditional clinical biomarkers, termed expert-defined features (EDFs).\n\nThe decision to use five latent dimensions for both SPINCs (Spirogram Encodings) and PLENCs (Photoplethysmogram Encodings) was made to match the number of commonly-used EDFs, ensuring fair model comparisons. This choice was also informed by the observation that both VAE and PCA reconstruct the data very well in the range of 4 to 6 latent dimensions.\n\nThe models were trained using the Adam optimizer with varying learning rates and batch sizes, without a learning rate scheduler. Training was conducted for 100 epochs, and the final hyperparameters were chosen to minimize the VAE loss in the validation set. This process ensures that the learned representations are optimized for the given data and task.",
  "optimization/fitting": "The model architecture and training procedure were designed to mitigate both overfitting and underfitting. The number of parameters in the model is not excessively large compared to the number of training points, which helps in preventing overfitting. To further ensure that overfitting is not an issue, several measures were taken. First, the model was trained for 100 epochs, and the final hyperparameters were chosen based on the performance on a validation set, which helps in selecting the best model that generalizes well to unseen data. Additionally, the reconstruction errors for both the training and validation datasets were plotted and observed to follow similar patterns, indicating that the model is not overfitting to the training data. Multiple tests were conducted to confirm that overfitting is not occurring.\n\nTo address underfitting, the model's capacity was carefully tuned. The choice of latent dimensions was crucial in this regard. For example, the decision to retain two residual latent dimensions for the model was based on visual inspection and the need to capture relevant information without introducing unnecessary complexity. This balance ensures that the model is neither too simple to capture the underlying patterns nor too complex to overfit the training data.\n\nThe use of the Adam optimizer with varying learning rates and batch sizes, without a learning rate scheduler, also contributes to finding a good balance between underfitting and overfitting. The final learning rate and batch size values were selected to minimize the VAE loss on the validation set, ensuring that the model performs well on unseen data.\n\nFurthermore, the model's performance was evaluated on a separate test dataset that was not used during training, ensuring that the results are not biased and that the model generalizes well to new data. This approach helps in ruling out both overfitting and underfitting by providing a robust evaluation of the model's performance.",
  "optimization/regularization": "To address overfitting prevention techniques, we employed several strategies during the training of our models. Firstly, we monitored the reconstruction error on both the training and validation datasets. Figures 1 and 2 illustrate the reconstruction errors for SPINCs and PLENCs, respectively, showing similar patterns between the training and validation sets, which suggests that overfitting is not a significant issue.\n\nAdditionally, we trained our models for 100 epochs, selecting the final hyperparameters based on the validation set performance. This approach helps in ensuring that the model generalizes well to unseen data. We used the Adam optimizer with varying learning rates and batch sizes, but did not employ a learning rate scheduler. The final learning rate and batch size values were chosen to minimize the VAE loss in the validation set.\n\nWe also considered the possibility of training two models on different halves of the samples and applying them to the alternative halves. However, due to the potential misalignment of the latent dimensions between the two models, we did not pursue this approach. Instead, we relied on the small latent embedding size to mitigate overfitting risks.\n\nFurthermore, we performed multiple tests to ensure that overfitting was not occurring. The limited latent embedding size and the observed results indicate a low chance of overfitting. Even if overfitting occurred, it should not affect downstream genetic analysis, as all evaluations for PRS were performed on a separate test dataset that was never used for training the model.\n\nIn summary, our approach to preventing overfitting involved careful monitoring of reconstruction errors, extensive training with hyperparameter tuning based on validation performance, and leveraging a small latent embedding size. These measures collectively help in ensuring the robustness and generalizability of our models.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are indeed available. The final hyperparameters used for each method, including the optimizer, learning rate, and batch size, are detailed in Supplementary Table 1. The model was trained using the Adam optimizer with varying learning rates and batch sizes, and no learning rate scheduler was employed. The training process involved 100 epochs, with the final hyperparameters chosen to minimize the VAE loss in the validation set.\n\nThe complete model definition, written in TensorFlow/Keras, is publicly accessible in our open-source code repository. This repository, available at https://github.com/Google-Health/genomics-research under the regle directory, provides all necessary details for reproducibility. Additionally, all variant weights for PRSs used in this paper, including intermediate EDFs and encodings PRSs and disease-specific PRSs, are available in the same Google Cloud Storage location under the prs_model directory.\n\nThe code and trained model weights are licensed under terms that allow for open access and use, facilitating further research and development in the field. For specific licensing details, one can refer to the repository's documentation. This comprehensive availability ensures that other researchers can replicate our findings and build upon our work.",
  "model/interpretability": "The model employed in our study is a convolutional variational autoencoder (VAE), which inherently introduces some level of interpretability through its latent space representations. While VAEs are often considered more interpretable than purely black-box models like deep neural networks, they are not entirely transparent. The latent dimensions learned by the VAE can capture complex, non-linear relationships in the data, which can be challenging to interpret directly.\n\nHowever, we have taken steps to enhance the interpretability of our model. For instance, we chose to use five latent dimensions for the spirogram encodings to match the number of classical phenotypes, providing a clear baseline for comparison. Additionally, we retained two residual latent dimensions for the residual spirogram encodings based on strong reconstruction performance, which helps in understanding the additional information captured beyond the classical readouts.\n\nThe model architecture is detailed in the supplementary materials, including the convolutional layers, kernel sizes, strides, and padding, which provide insights into how the model processes the input data. Furthermore, we have made the complete model definition publicly available in our open-source code repository, allowing for transparency and reproducibility.\n\nIn terms of biological meaning, while minimizing the VAE loss does not guarantee that the latent space encodings have the \"best\" biological meaning from a subjective human perspective, we have provided examples in the supplementary figures and tables. For example, spirogram encoding 3 appears to represent nearly the same biological signal as FEV1 and FVC, while spirogram encoding 2 represents a partial recovery of the inverse of FEV1/FVC. This demonstrates that the latent dimensions can capture meaningful biological signals, even if they do not always align perfectly with expert-defined features.\n\nOverall, while the model is not entirely transparent, we have provided sufficient details and examples to enhance its interpretability and allow for a better understanding of the learned representations.",
  "model/output": "The model presented in this publication is neither purely classification nor regression. Instead, it is an unsupervised learning approach using convolutional Variational Autoencoders (VAEs) to learn representations of higher-dimensional clinical data. The primary goal is to encode additional genetic information that is not captured by traditional clinical biomarkers, known as expert-defined features (EDFs). The model, named REGLE, is applied to two types of data: spirograms and photoplethysmograms (PPGs). It aims to generate latent representations that can be more predictive of relevant phenotypes and diseases, such as asthma, COPD, hypertension, and systolic blood pressure. These representations also facilitate the discovery of genetic variants that might not be identified using EDFs alone. The output of the model consists of learned representations that encode biological signals and can be used for further genetic analyses and phenotype predictions. The model's architecture includes an encoder and a decoder, with the encoder generating latent dimensions that capture both linear and nonlinear aspects of the input data. The decoder reconstructs the input data from these latent dimensions, incorporating EDFs to ensure that the learned representations complement rather than replicate the information provided by EDFs. The model's performance is evaluated based on its ability to reconstruct the input data accurately and to capture genetic information that is not present in the EDFs. The choice of latent dimensions is crucial and is determined based on reconstruction performance and the need to maintain a comparable number of total latent dimensions to the EDFs. The model's output is consistent across random initializations, and the training procedure includes clear statements about the optimizer, learning rate, and batch size. However, details about the number of epochs and the use of learning rate schedulers or early stopping are not explicitly mentioned. The model's simplicity and its ability to account for expert-defined features are key strengths, making it an appealing tool for future analyses in exploring the diversity of phenotypes collected in biobanks.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models and algorithms used in this research is publicly available. The open-source code, along with the trained model weights, can be accessed via a GitHub repository. This repository is maintained under the Google-Health organization and is specifically located in the genomics-research directory, within the regle subdirectory. The code is released under a permissive license, allowing researchers and developers to utilize, modify, and distribute the software as needed.\n\nIn addition to the source code, pretrained models are also provided, enabling investigators with access to the UK Biobank to run modified genome-wide association studies (GWAS) with different adjustments or inclusion criteria. This facilitates reproducibility and further exploration of the data by the scientific community.\n\nFor those interested in the specific details of the model architecture and training procedures, supplementary figures and documentation are available. These resources provide insights into the convolutional variational autoencoder (VAE) model architecture, including convolution kernel size, stride, padding, and activation functions. The complete model definition is written in TensorFlow/Keras and can be found in the lib/models.py file within the repository.\n\nFurthermore, the variant weights for polygenic risk scores (PRSs), including intermediate encodings and disease-specific PRSs for conditions such as asthma, COPD, hypertension, and systolic blood pressure, are available in a specified Google Cloud Storage location under the prs_model directory. This ensures that all necessary components for replicating and building upon the research are accessible to the public.",
  "evaluation/method": "The evaluation of our methods involved multiple strategies to ensure robustness and generalizability. We generated polygenic risk scores (PRSs) for various traits using data from the UK Biobank and evaluated these PRSs in several independent datasets. For instance, PRSs for hypertension and systolic blood pressure were assessed in the UK Biobank test set, COPDGene, eMERGE III, and EPIC-Norfolk datasets. Similarly, PRSs for chronic obstructive pulmonary disease (COPD) and asthma were evaluated in COPDGene, eMERGE III, EPIC-Norfolk, and the Indiana Biobank.\n\nOur evaluation included statistical measures such as 95% confidence intervals generated through bootstrapping, with 300 repetitions to ensure reliability. The center points of these intervals represent the bootstrapping means, providing a clear indication of the PRS performance. Horizontal dashed lines in our figures show the total prevalence in the evaluation sets, and star symbols indicate statistically significant differences between our methods and others, using paired bootstrapping with a 95% confidence level (two-sided P < 0.05).\n\nAdditionally, we conducted experiments to address potential overfitting concerns. We plotted the mean squared error (MSE) for both training and validation datasets for SPINCs and PLENCs, observing similar patterns that suggested minimal overfitting. All evaluations were performed on separate test datasets that were never used for training, further mitigating overfitting risks.\n\nWe also explored the generalization of our models to newer datasets, acknowledging it as a potential limitation. This involved considering the need for additional spirogram and photoplethysmogram (PPG) data to fully investigate the generalizability of our REGLE embeddings. Furthermore, we updated our methods to include EDF+RPLENCs for PPG in genome-wide association studies (GWAS) loci and PRS, adding relevant results to our supplementary materials.\n\nIn summary, our evaluation method was rigorous and multifaceted, involving independent datasets, statistical validation, and thorough checks for overfitting, ensuring the reliability and generalizability of our findings.",
  "evaluation/measure": "In the evaluation of our method, REGLE, we focus on several key performance metrics to assess the effectiveness of our approach in learning representations of higher-dimensional clinical data. One of the primary metrics we report is the reconstruction error, which measures how well our model can reconstruct the input data from the learned latent representations. This metric is crucial for understanding the fidelity of the representations generated by our convolutional Variational Autoencoders (VAEs).\n\nWe also evaluate the predictive power of the learned representations in downstream tasks, such as Polygenic Risk Scores (PRS) and Genome-Wide Association Studies (GWAS). For PRS, we assess the performance in predicting relevant phenotypes and diseases, such as asthma, COPD, hypertension, and systolic blood pressure. The evaluation includes comparing our method against expert-defined features (EDFs) and other dimensionality reduction techniques like Principal Component Analysis (PCA). We report metrics such as precision-recall curves and statistical significance (p-values) to demonstrate the superiority of our approach.\n\nIn addition to these metrics, we provide a detailed analysis of the genetic information captured by our learned representations. This includes evaluating the discovery of genetic variants that are not found with EDFs alone. We also report on the generalization of our model to independent datasets, such as UK Biobank, COPDGene, eMERGE III, and EPIC-Norfolk, to ensure the robustness and applicability of our method.\n\nThe set of metrics reported is representative of the current literature in the field of unsupervised learning and dimensionality reduction for clinical data. By focusing on reconstruction error, predictive power in downstream tasks, and genetic information capture, we provide a comprehensive evaluation of our method's performance. This approach allows us to demonstrate the strengths and limitations of our method, making it a valuable tool for future analyses in biobanks and clinical research.",
  "evaluation/comparison": "In the evaluation of our method, we conducted a thorough comparison with publicly available methods and simpler baselines to ensure the robustness and superiority of our approach. Specifically, we evaluated our polygenic risk scores (PRSs) generated from learned representations against those derived from expert-defined features (EDFs) across multiple independent datasets. These datasets included UK Biobank, COPDGene, eMERGE III, EPIC-Norfolk, and Indiana Biobank. The evaluations were performed for various phenotypes such as hypertension, systolic blood pressure, COPD, and asthma.\n\nFor the learned representations, we utilized convolutional variational autoencoders (VAEs) to capture complex patterns in high-dimensional clinical data, such as spirograms and photoplethysmograms (PPGs). These learned representations, termed SPINCs and PLENCs, were compared against EDFs to assess their ability to encode additional genetic information and improve predictive performance.\n\nIn addition to comparing against EDFs, we also considered simpler baselines such as Principal Component Analysis (PCA). While PCA is a linear dimensionality reduction technique, our VAE-based approach offers non-linear capabilities, which are crucial for capturing intricate structures in the data. We demonstrated that our method outperforms PCA in terms of statistical efficiency and the ability to discover genetic variants not found with EDFs.\n\nThe evaluations were conducted using bootstrapping techniques to generate 95% confidence intervals and assess statistical significance. Solid vertical intervals in the figures represent these confidence intervals, and star symbols indicate statistically significant differences between the methods. This rigorous comparison highlights the advantages of our unsupervised representation learning approach over simpler baselines and publicly available methods.",
  "evaluation/confidence": "In our evaluation, we have ensured that the performance metrics are robust and reliable. All estimates of the polygenic risk score (PRS) results include 95% confidence intervals, which are generated through bootstrapping with 300 repetitions. This approach provides a clear indication of the variability and reliability of our results.\n\nStatistical significance is a crucial aspect of our evaluation. We use paired bootstrapping with 300 repetitions to determine if there is a statistically significant difference between our method and others, including baselines. A two-sided P-value of less than 0.05 is considered significant. In our figures, star symbols indicate where this significance is achieved, highlighting instances where our method outperforms others with a high degree of confidence.\n\nFor example, in the evaluation of PRSs for hypertension and systolic blood pressure generated on UK Biobank, we assessed their performance in multiple independent datasets, including UK Biobank, COPDGene, eMERGE III, and EPIC-Norfolk. Similarly, PRSs for COPD and asthma generated on UK Biobank were evaluated in COPDGene, eMERGE III, EPIC-Norfolk, and Indiana Biobank. In all these evaluations, the use of confidence intervals and statistical significance testing ensures that our claims of superiority are well-founded.\n\nAdditionally, we have compared our method against principal component analysis (PCA) in terms of GWAS power. The results show that our method generally outperforms PCA, especially in scenarios with fewer dimensions. This further supports the robustness and superiority of our approach.\n\nIn summary, our evaluation includes comprehensive confidence intervals and statistical significance testing, providing a strong basis for claiming that our method is superior to others and baselines.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, the variant weights for the polygenic risk scores (PRSs) used in this paper, including intermediate effect direction files (EDFs) and encodings PRSs, as well as disease-specific PRSs for asthma, COPD, hypertension (HTN), and systolic blood pressure (SBP), are accessible. These files can be found in a specified Google Cloud Storage location under the prs_model directory.\n\nAdditionally, the open-source code and trained model weights are available on GitHub under the repository Google-Health/genomics-research, specifically in the regle directory. This repository provides access to the tools and models used in the evaluation process, allowing for reproducibility and further research."
}