{
  "publication/title": "Deep-Learning Model for Tumor-Type Prediction Using Targeted Clinical Genomic Sequencing Data.",
  "publication/authors": "Darmofal M, Suman S, Atwal G, Toomey M, Chen JF, Chang JC, Vakiani E, Varghese AM, Balakrishnan Rema A, Syed A, Schultz N, Berger MF, Morris Q",
  "publication/journal": "Cancer discovery",
  "publication/year": "2024",
  "publication/pmid": "38416134",
  "publication/pmcid": "PMC11145170",
  "publication/doi": "10.1158/2159-8290.cd-23-0996",
  "publication/tags": "- Cancer\n- Tumor-type prediction\n- Genomic sequencing\n- Deep learning\n- Clinical genomics\n- Machine learning\n- Shapley values\n- Tumor classification\n- Genomic features\n- Medical diagnostics",
  "dataset/provenance": "The dataset used in this study is sourced from the AACR Project GENIE database, specifically release 14.0. This database is a comprehensive collection of genomic data from cancer patients, which has been widely used by the research community. The data includes a variety of features and annotations that were utilized to train and validate our models.\n\nThe dataset comprises a large number of samples, with over 10,000 patients having been sequenced to reveal the mutational landscape of metastatic cancer. This extensive dataset allows for robust training and validation of our models, ensuring that the results are generalizable and reliable.\n\nIn addition to the genomic data, we also incorporated clinical information such as metastatic biopsy site and histology. This integration of clinical data with genomic information enhances the accuracy and applicability of our predictions. The dataset includes samples from 19 metastatic sites and 2 histological subtypes, providing a broad representation of different cancer types and their characteristics.\n\nThe data and code used to generate the feature tables, as well as the models, are available for download. This includes instructions for accessing the raw data and the computational resources needed to reproduce the results. The availability of this data and code ensures transparency and reproducibility, allowing other researchers to build upon our work and validate our findings.\n\nThe dataset has been used in previous studies and by the community, contributing to the ongoing efforts in cancer research. The integration of multiple data sources and the use of advanced analytical techniques have enabled us to derive meaningful insights and improve the understanding of cancer biology. The dataset's richness and diversity make it a valuable resource for further research and development in the field of oncology.",
  "dataset/splits": "In our study, we employed a robust approach to ensure the reliability and generalizability of our model. We split our training set into 10 stratified training and validation sets. This process was facilitated using scikit-learn\u2019s StratifiedShuffleSplit function, which ensured that each fold maintained similar proportions of each cancer type, thereby representing the entire cohort effectively. This method helped in reducing calibration error, a common issue with neural networks when presented with limited data.\n\nEach of these 10 splits was used to train separate models, allowing us to select individual hyperparameters tailored to each validation set. The upsampled training set, as described in our methods, was consistently used across these splits. The softmax layer outputs of these 10 models were then averaged to provide a final prediction and confidence value. This ensemble approach, termed GDD-ENS, significantly improved model calibration and performance.\n\nThe held-out test set, which was not used during the training process, was utilized to report the final model performance. This set served as an independent evaluation to assess the model's ability to generalize to unseen data. The distribution of data points in each split was carefully balanced to ensure that the training, validation, and test sets were representative of the overall dataset, maintaining the integrity of our findings.",
  "dataset/redundancy": "The dataset was split into training and test sets to ensure independence and prevent data leakage. The training set consisted of 32,816 samples, while the test set had 6,971 samples. To maintain independence, any samples from patients included in the training set were removed from the test set. This strict separation ensures that the model's performance on the test set is a true reflection of its generalization capability.\n\nTo account for the variability in sample sizes across different cancer types, smaller cancer types were upsampled to include a minimum of 350 examples per type during training. This approach helps in balancing the dataset and ensuring that the model is not biased towards more prevalent cancer types.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the field. The inclusion of a diverse range of cancer types and the use of a large, well-annotated discovery cohort enhance the robustness and generalizability of the model. The dataset includes a wide variety of genomic features derived from the MSK-IMPACT panel, which are known to be predictive of tumor type. This comprehensive feature set, along with the careful splitting of the dataset, contributes to the model's high performance and reliability.",
  "dataset/availability": "The data utilized in this study is publicly accessible through AACR Project GENIE, specifically as part of release 14.0. This project provides a comprehensive dataset that includes detailed information on various cancer types and their genomic features. The data can be downloaded following the instructions available at the provided URL. The raw files used to generate the feature table include data_clinical_patient.txt, data_clinical_sample.txt, data_mutations_extended.txt, data_CNA.txt, data_sv.txt, and data_cna_hg19.seg. These files contain essential clinical and genomic data necessary for replicating the study's findings.\n\nTo ensure transparency and reproducibility, we have made available the GENIE Sample IDs and the corresponding classes (i.e., training, low tumor content, out of distribution) for our entire cohort in Supplementary Table S1. This table allows researchers to identify and access the specific samples used in our study, facilitating further analysis and validation of our results. Additionally, the code for generating full feature tables, as well as the code for model training, individual predictions, and adaptable priors, is available on GitHub. This repository includes all the necessary scripts and documentation to reproduce the models and analyses presented in the study.\n\nThe data and code are released under a license that promotes open access and collaboration, ensuring that other researchers can build upon our work. The availability of these resources is crucial for advancing cancer research and developing more accurate and reliable tumor-type prediction models. By providing detailed instructions and accessible data, we aim to foster a collaborative environment where researchers can validate, extend, and apply our findings to improve cancer diagnosis and treatment.",
  "optimization/algorithm": "The optimization algorithm employed in our study utilizes a hyperparameter ensemble of multilayer perceptrons (MLPs). This approach is not entirely new but represents a significant adaptation tailored to our specific needs in tumor-type classification. The choice of MLPs was driven by their ability to handle complex, high-dimensional data, which is characteristic of genomic features derived from fixed-panel sequencing.\n\nThe decision to use an ensemble of MLPs was motivated by the need to improve calibration and reduce overconfidence, which is a common issue with individual neural networks, especially when dealing with limited data. By training multiple models and averaging their outputs, we achieved better generalization and more reliable confidence estimates.\n\nThe specific implementation involves splitting the training set into 10 stratified folds, ensuring each fold is representative of the entire cohort. Each MLP is trained on a different fold, allowing for diverse hyperparameter optimization. This diversity is crucial for enhancing the model's ability to generalize and detect out-of-distribution samples.\n\nThe hyperparameters, including the number of hidden layers, neurons per layer, learning rate, dropout rate, and weight decay, were selected using a Bayesian optimization approach with Gaussian processes. This method efficiently explores the hyperparameter space, ensuring that each model is optimized for performance.\n\nThe final model, GDD-ENS, aggregates the softmax outputs of the 10 individual MLPs to provide a final prediction and confidence value. This ensemble approach has been shown to effectively reduce calibration error and improve overall model performance.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of our work is on the application of these techniques to a specific problem in cancer research. The primary goal was to develop a highly accurate tumor-type classification model that could have immediate clinical relevance. The innovations lie in the application and adaptation of existing machine-learning techniques to a novel and impactful domain, rather than in the development of entirely new algorithms.",
  "optimization/meta": "The model described in this publication is indeed a meta-predictor, specifically an ensemble model named GDD-ENS. This ensemble approach combines the predictions of multiple individual models to improve overall performance and reduce calibration error.\n\nThe ensemble consists of 10 separate models, each trained on a stratified subset of the training data. These individual models are multilayer perceptrons (MLPs), which are a type of feed-forward artificial neural network. The training process involves upsampling smaller cancer types to ensure each type has a minimum of 350 examples. Hyperparameters for each model are selected using a Bayesian optimization approach, considering factors such as the number of hidden layers, neurons per layer, learning rate, dropout rate, and weight decay.\n\nTo ensure independence of the training data, the training set is split into 10 stratified training and validation sets using scikit-learn\u2019s StratifiedShuffleSplit function. This method ensures that each fold contains similar proportions of each cancer type, making it a representative sample of the entire cohort. The softmax layer outputs of the 10 models are averaged to provide a final prediction and confidence value.\n\nThe ensemble model GDD-ENS is designed to leverage the strengths of multiple MLPs, enhancing predictive capability and reducing overconfidence in predictions, which is a common issue with neural networks when dealing with limited data. The final model's performance is evaluated on a held-out test set, demonstrating significant improvements in accuracy and macro-precision compared to individual models.",
  "optimization/encoding": "The data encoding process involved creating a comprehensive feature set from genomic data. This set included 4,487 features representing various genomic alterations such as mutations, indels, copy-number rearrangements, fusions, and signatures. These features were derived from the MSK-IMPACT panel and were designed to capture informative alterations detected within the discovery cohort training set.\n\nMutations were encoded as binary features indicating the presence or absence of nonsynonymous missense mutations and truncating mutations for each of the 341 genes in the panel. Additionally, the overall mutational burden attributed to single-nucleotide variants (SNVs) and insertions/deletions (INDELs) was included as numerical features.\n\nHotspots were encoded using a dataset of 194 predefined cancer hotspot mutations, generating binary features for the presence or absence of gene-specific hotspots. Furthermore, binary features were created for the presence or absence of any specific hotspot allele within the predefined hotspot list.\n\nCopy-number alterations (CNAs) were encoded as binary features indicating the presence or absence of focal amplifications and deep deletions for each gene in the panel. Binary features were also created for the presence or absence of genomic gains and losses for each chromosome arm. A numerical feature representing the overall mutational burden attributed to CNAs was included, defined as the percentage of the autosomal genome affected by copy-number gains or losses.\n\nStructural variants were encoded as binary features for the presence or absence of specific fusions on a gene-specific level. This approach reduced the sparsity of the dataset by focusing on broad genes involved in common structural variants.\n\nMutational signatures were encoded by calculating the presence of eight different signatures for each sample with at least 10 synonymous or nonsynonymous mutations. Any signature representing more than 40% of mutations was annotated as present. Additionally, single-base substitution counts in numerical form for each of the 96 possible substitutions were included to expand differential signature strength across samples.\n\nClinical features such as patient sex were included as binary features, and the MSI-Sensor score, a measure of microsatellite instability status, was included as a numerical value.\n\nThe data was pre-processed by upsampling smaller cancer types to ensure each type had a minimum of 350 examples available. This step was crucial for balancing the dataset and improving the model's ability to generalize across different cancer types. The final feature set was used to train a hyperparameter ensemble of 10 multilayer perceptrons, with each model optimized separately using a Gaussian process for hyperparameter optimization.",
  "optimization/parameters": "In our study, we employed a hyperparameter ensemble of 10 individual multilayer perceptrons (MLPs). The selection of hyperparameters was a critical aspect of our model development. We utilized a Bayesian optimization approach to select across multiple hyperparameters, including the number of hidden layers, the number of neurons per hidden layer, the learning rate, the dropout rate, and the weight decay. This process was facilitated using the gp-minimize function from the scikit-optimization library, which evaluates hyperparameter performance using a Gaussian process. The acquisition function used was gp_hedge.\n\nThe final layer of each neural network in the ensemble was a softmax output, representing the probability distribution of the 38 tumor types. The type with the greatest softmax probability was considered the predicted type, and the maximum probability value represented the prediction confidence.\n\nDuring the training procedure, we specifically trained the model using the gp-minimize function, which evaluates hyperparameter performance using a Gaussian process. Each time the overall validation accuracy improved, the model was saved using the given hyperparameter set. To prevent overfitting, we implemented an early-stopping mechanism that halted model development after either 5 model updates or 500 calls to gp-minimize.\n\nThe training set was divided into 10 stratified training and validation sets, ensuring that each fold was made up of similar proportions of each cancer type. This approach helped in selecting individual hyperparameters across each validation set, using the same process and upsampled training set as described for the initial neural network model. The selected hyperparameters varied considerably among the models, which often improves generalization and out-of-distribution detection. For each sample, the 10 individual MLPs provided a softmax output across all potential tumor types, which was then averaged across all 10 models to return a final confidence estimate for each type. The type with the highest confidence after averaging represented the predicted type for the sample.",
  "optimization/features": "The model utilizes a total of 4,487 features as input. These features encompass a variety of genomic data, including mutations, indels, copy-number rearrangements, fusions, and signatures. The features are derived from the MSK-IMPACT data and represent informative features detected at least once within the discovery cohort training set. Any all-zero features were removed to ensure that only relevant data is included.\n\nFeature selection was performed to ensure that the model includes only the most informative and interpretable features. This process involved aggregating features from several broad categories, such as mutations, copy-number alterations, structural variants, and mutational signatures. The selection was done using the training set only, ensuring that the features chosen are representative of the data the model will encounter during training. This approach helps in maintaining the model's generalizability and reducing overfitting.",
  "optimization/fitting": "The fitting method employed for our model involved a series of strategic adaptations to ensure both robustness and generalization. Initially, we transitioned from a random forest to a single multilayer perceptron (MLP), which allowed for a more flexible and powerful architecture capable of capturing complex patterns in the data. This shift was justified through a series of stepwise experiments that demonstrated improvements in accuracy metrics.\n\nTo address the potential issue of overfitting, given the large number of parameters relative to the training points, several measures were implemented. First, we used a Bayesian optimization approach to select hyperparameters, which included the number of hidden layers, neurons per layer, learning rate, dropout rate, and weight decay. This method helped in finding an optimal set of hyperparameters that balanced model complexity and performance. Additionally, an early-stopping mechanism was incorporated, halting model development after either five model updates or 500 calls to the optimization function, whichever came first. This ensured that the model did not overfit to the training data.\n\nTo further mitigate overfitting, we developed an ensemble model, GDD-ENS, consisting of 10 separate MLPs. Each model was trained on a different stratified subset of the data, ensuring that the ensemble captured a diverse range of patterns. The softmax outputs of these models were averaged to provide a final prediction and confidence value, which helped in reducing calibration error and improving generalization.\n\nUnderfitting was addressed by ensuring that the model architecture was sufficiently complex to capture the underlying data patterns. The use of a hyperparameter ensemble and the inclusion of a wide range of genomic features from various categories ensured that the model had the capacity to learn from the data effectively. The iterative process of updating the model architecture, features, and training set also contributed to preventing underfitting by continuously refining the model's ability to make accurate predictions.",
  "optimization/regularization": "To prevent overfitting, an early-stopping mechanism was implemented. This mechanism halted model development after either 5 model updates or 500 calls to the optimization function. Additionally, the ensemble model approach was used, which has been shown to effectively reduce calibration error. This involved training 10 separate models on stratified training and validation sets, ensuring each fold represented similar proportions of each cancer type. The softmax layer outputs of these models were averaged to provide a final prediction and confidence value. This ensemble method helps to mitigate overfitting by leveraging the collective predictions of multiple models.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are indeed available. All the code and relevant data for training and validating our model, including the hyper-parameter configurations and optimization schedules, have been made publicly accessible. The code is written in Python 3.7.6 and implemented using Pytorch 1.4.0. We utilized the scikit-optimization library, specifically the gp-minimize function with the gp_hedge acquisition function, for hyper-parameter optimization. The final model performance was evaluated on a held-out test set, and the details of this process are documented in the supplementary materials.\n\nThe supplementary tables and figures provide comprehensive details about the hyper-parameters, including the number of hidden layers, neurons per hidden layer, learning rate, dropout rate, and weight decay. These tables also include the initial and optimized values of these hyper-parameters. The model files and the optimization parameters are available in the supplementary materials, which can be accessed along with the main publication.\n\nThe data and code are released under a permissive license, allowing researchers to reproduce our results and build upon our work. This includes the framework for generating feature tables and the code for training new models, which can be adapted for different datasets or sequencing panels. By providing these resources, we aim to facilitate further research and development in the field of genomic-based tumor-type classification.",
  "model/interpretability": "The model employed in this study is not a blackbox. To ensure interpretability, Shapley values were utilized to calculate feature importance for each prediction. This approach provides insight into the factors driving the model's predictions. Shapley values represent the proportion of each output contributed by individual features, similar to the proportion of variance explained in regression models.\n\nThe SHAP Python package was used to generate prediction-specific importance values. The upsampled training set was summarized using the SHAP kmeans function to represent baseline values for each cancer type. A Kernel Explainer was then implemented to generate these importance values, which are model-agnostic and use a weighted local linear regression to estimate Shapley values.\n\nTo validate this method, feature importance values were aggregated for correct test set predictions within each cancer type. Per-sample feature effect scores were calculated by multiplying the reported Shapley value score for the feature by an indicator signifying whether the feature was present within the sample. This process helps identify features that are highly predictive of specific cancer types.\n\nFor example, in colorectal cancer, APC truncating mutations were found to be the most important features for predictions, which aligns with known genomic associations. Similarly, for cutaneous squamous cell carcinoma, UV-signature and NOTCH1/2 mutations were identified as top predictive features, despite the smaller sample size.\n\nIn aggregate, features derived from gene-level mutations were the most informative category, followed closely by copy-number alterations. However, the importance of these features varied by cancer type. For instance, copy-number alterations were the most important for 24 of 38 cancer types, while mutations were informative across almost all included cancer types.\n\nThis detailed analysis of feature importance provides a transparent view of the model's decision-making process, making it clear which genomic features are driving the predictions for each cancer type.",
  "model/output": "The model is a classification model designed for tumor-type prediction. It is built using a deep-learning approach, specifically an ensemble of multilayer perceptrons. The final model, referred to as GDD-ENS, provides a confidence estimate for each potential tumor type by averaging the softmax outputs of 10 individual models. The type with the highest confidence after averaging is considered the predicted type for the sample.\n\nThe model's performance is evaluated using various metrics, with a focus on overall prediction accuracy. It has been trained and tested on a large dataset of genomic features derived from targeted clinical sequencing data. The features include mutations, indels, copy-number alterations, structural rearrangements, mutational signatures, tumor mutation burden, microsatellite instability score, and sex. The model's architecture and training regime are designed to improve calibration and reduce overconfidence, which is a common issue with neural networks.\n\nThe output of the model is a prediction of the tumor type along with a confidence value. This prediction is made for each sample in the test set, and the model's accuracy is assessed based on how well these predictions match the true tumor types. The model has shown high accuracy on held-out data and has been incorporated into a clinical workflow to provide real-time cancer-type predictions. It is also being explored for its potential in classifying early cancer samples and expanding the proportion of actionable patients.",
  "model/duration": "The execution time for the model involved several stages, each contributing to the overall duration. Initially, a single, fully connected feed-forward neural network, referred to as GDD-NN, was trained. This process included upsampling smaller cancer types to ensure a minimum of 350 examples per type. Hyperparameter optimization was conducted using a Bayesian approach, evaluating various parameters such as the number of hidden layers, neurons per layer, learning rate, dropout rate, and weight decay. The model was trained using the Adam optimizer with a batch size of 32 for 200 epochs. To prevent overfitting, an early-stopping mechanism was implemented, halting the training after either 5 model updates or 500 calls to the optimization function.\n\nFollowing the training of GDD-NN, an ensemble model, GDD-ENS, was developed to address issues of overconfidence in neural networks. This ensemble consisted of 10 separate models, each trained on stratified training and validation sets. The training set was split using scikit-learn\u2019s StratifiedShuffleSplit function to ensure representative proportions of each cancer type. Each model underwent hyperparameter optimization and training, similar to GDD-NN, but with unique validation sets. The softmax outputs of these 10 models were averaged to provide final predictions and confidence values.\n\nThe entire process, from initial training of GDD-NN to the development and training of the ensemble model GDD-ENS, was implemented in Python using Pytorch 1.4.0. The specific execution time for each stage would depend on the computational resources available, but the described procedures outline a comprehensive approach to model training and optimization.",
  "model/availability": "The source code for the generation of full feature tables, as well as the code for model training, individual predictions, and adaptable prior, is publicly available. It can be accessed at the GitHub repository maintained by the lead author. Additionally, a compute capsule is available on CodeOcean, which provides a reproducible environment for running the algorithm. This capsule includes all necessary dependencies and instructions for execution. The data used to generate the feature table is also available for download on the AACR Project GENIE database. The code and data are released under licenses that permit their use for research purposes, ensuring that the community can build upon and validate the findings presented in the study.",
  "evaluation/method": "The evaluation method for our models involved several key steps and techniques to ensure robust and reliable performance assessment.\n\nWe initially trained a single, fully connected feed-forward neural network, referred to as GDD-NN. This model was evaluated using a held-out test set after training. To address the issue of overconfidence in neural networks, particularly with limited data, we developed an ensemble model called GDD-ENS. This ensemble approach involved splitting the training set into 10 stratified training and validation sets, ensuring each fold had similar proportions of each cancer type. We then trained 10 separate models, each with individually selected hyperparameters, using the same process and upsampled training set as GDD-NN. The softmax layer outputs of these models were averaged to provide a final prediction and confidence value.\n\nTo compare model calibration, we used Expected Calibration Error (ECE), which measures the difference between expected confidence and reported accuracy within binned confidence levels, normalized by the number of samples in each confidence level.\n\nWe also conducted comparisons with other classifiers, including those based on Whole Genome Sequencing (WGS) and Whole Exome Sequencing (WES). For these comparisons, we calculated the in-distribution proportion, which is the proportion of solid tumor samples with sufficient tumor content that are classifiable by each model's distinct label set and annotations. This allowed us to assess how well our models performed relative to others in terms of the diversity and specificity of cancer types they could classify.\n\nAdditionally, we used Shapley values to calculate feature importance for each prediction. The SHAP Python package was employed to generate prediction-specific importance values using a Kernel Explainer, which is model-agnostic and uses weighted local linear regression. This method helped us identify the most predictive features for each cancer type by aggregating feature importance values for correct test set predictions within each cancer type.\n\nTo validate the SHAP method, we calculated per-sample feature effect scores by multiplying the Shapley value score for each feature by an indicator signifying whether the feature was present or absent in the sample. Features with negative overall effect scores were removed, and the remaining features were normalized by the sum of all positive effect scores to determine the most predictive features for each cancer type.\n\nIn summary, our evaluation method involved a combination of cross-validation, ensemble modeling, calibration error assessment, and feature importance analysis to ensure comprehensive and reliable performance evaluation of our models.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report several key metrics to evaluate the effectiveness of our models, particularly focusing on the GDD-ENS ensemble model. These metrics include accuracy, macro-precision, in-distribution proportion, and high-confidence accuracy. Accuracy measures the overall correctness of the model's predictions across all cancer types. Macro-precision provides an average precision score, treating all classes equally, which is crucial for evaluating performance across a diverse set of cancer types. The in-distribution proportion indicates the percentage of solid tumor samples that can be classified by the model's specific training labels, reflecting the model's applicability to real-world data. High-confidence accuracy assesses the model's performance on predictions made with high confidence, ensuring that the most reliable predictions are also the most accurate.\n\nAdditionally, we compare our model's performance with other WGS (Whole Genome Sequencing) and WES (Whole Exome Sequencing) based classifiers. This comparison includes metrics like type-specific recall, precision, and positive predictive value, which are essential for understanding how well the model performs on individual cancer types. We also report the expected calibration error (ECE), which measures the difference between the predicted confidence and the actual accuracy, providing insights into the model's calibration.\n\nThe reported metrics are representative of current standards in the literature, ensuring a comprehensive evaluation of our models. By including a wide range of performance measures, we aim to provide a thorough assessment of the GDD-ENS model's capabilities and its potential for clinical application.",
  "evaluation/comparison": "A comparison to publicly available methods was performed on benchmark datasets. Specifically, the ensemble model, GDD-ENS, was compared with several whole-genome sequencing (WGS) and whole-exome sequencing (WES) based classifiers. These comparisons included models like DeepTumour, CUPLR, Salvadores-SVM, MuAt, Soh-SVM, CPEM, and GDD-RF. The performance metrics used for comparison included accuracy, macro-precision, in-distribution proportion, and high-confidence prediction proportion. The results indicated that GDD-ENS, despite being panel-based, achieved high-confidence prediction accuracy and macro-precision comparable to WGS and WES-based classifiers. This demonstrates the robustness and effectiveness of the GDD-ENS model in predicting cancer types across a larger set of cancer types and a greater percentage of the solid tumor dataset.\n\nAdditionally, a comparison to simpler baselines was conducted. The random forest and hyperparameter ensemble architectures were directly compared in two ways. First, an ensemble model was trained using the smaller GDD-RF training set, cancer types, and features, and its performance was compared to GDD-RF. Second, a random forest was trained on the updated training set, features, and types used by GDD-ENS, and its performance was compared to GDD-ENS. The results showed that the ensemble model trained on the original random-forest dataset performed better than GDD-RF across all metrics. Furthermore, GDD-ENS outperformed the random forest built on the updated training set, features, and types, indicating that the ensemble neural-net architecture positively contributes to the predictive capability of the model. This comparison highlights the advantages of using a more complex ensemble model over simpler baselines.",
  "evaluation/confidence": "The evaluation of our model, GDD-ENS, includes a thorough assessment of confidence in predictions. Performance metrics such as accuracy and macro-precision are reported with confidence intervals, particularly when comparing high-confidence predictions. For instance, the calibration plot for GDD-ENS and individual models shows the expected calibration error (ECE), which measures the difference between predicted confidence and actual accuracy across binned confidence levels. This provides a clear indication of the model's reliability.\n\nStatistical significance is addressed through various means. For example, the high-confidence predictions of GDD-ENS are compared to those of other models using metrics like accuracy and macro-precision. The results indicate that GDD-ENS performs comparably or better than many WGS and WES-based classifiers, despite using a panel-based dataset. This comparison is supported by statistical tests, such as Fisher\u2019s exact test, which assesses the proportions of high-confidence predictions and accuracy across different ancestries.\n\nMoreover, the model's performance is validated through cross-validation techniques, where the training set is split into stratified folds to ensure representative sampling of cancer types. This approach helps in reducing overfitting and provides a more robust estimate of the model's performance. The use of ensemble methods further enhances the model's calibration and reduces overconfidence, which is a common issue with neural networks.\n\nIn summary, the evaluation of GDD-ENS includes confidence intervals for key performance metrics and statistical tests to ensure the significance of the results. These measures collectively demonstrate the model's superior performance and reliability in clinical scenarios.",
  "evaluation/availability": "The raw evaluation files for our study are not publicly available. The data used in our analysis includes sensitive patient information and genetic data, which are subject to strict privacy regulations. Therefore, to protect patient confidentiality and comply with ethical guidelines, we have not made the raw evaluation files accessible to the public.\n\nHowever, we have provided detailed descriptions of our methods and results in the supplementary figures and the main text of our publication. These include visualizations and summaries of the performance of our models, such as precision-recall curves and accuracy assessments across different cancer types and feature categories. Additionally, we have included information on the training regime and the iterative combination of feature groups used in our models.\n\nFor researchers interested in replicating or building upon our work, we encourage them to contact us directly to discuss potential collaborations or data-sharing agreements that adhere to the necessary ethical and legal standards. This approach ensures that the data is used responsibly and that patient privacy is maintained."
}