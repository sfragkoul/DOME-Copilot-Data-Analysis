{
  "publication/title": "Early prediction of acute respiratory distress syndrome complicated by acute pancreatitis based on four machine learning models.",
  "publication/authors": "Zhang M, Pang M",
  "publication/journal": "Clinics (Sao Paulo, Brazil)",
  "publication/year": "2023",
  "publication/pmid": "37196588",
  "publication/pmcid": "PMC10199163",
  "publication/doi": "10.1016/j.clinsp.2023.100215",
  "publication/tags": "- Acute Respiratory Distress Syndrome\n- Acute Pancreatitis\n- Machine Learning\n- Predictive Modeling\n- Bayesian Classifier\n- Ensembles of Decision Trees\n- Support Vector Machine\n- Nomogram\n- Early Prediction\n- Clinical Outcomes\n- Hypertriglyceridemia\n- ARDS Risk Factors\n- ML Algorithms\n- Feature Selection\n- Retrospective Study",
  "dataset/provenance": "The dataset used in this study was sourced from patients admitted to the Xuanwu Hospital of Capital Medical University between January 2017 and August 2022. The hospital has a dedicated acute pancreatitis therapy center, including a gastroenterology intensive care unit. The study initially identified 497 patients with acute pancreatitis (AP), but 37 were excluded based on specific criteria, leaving a total of 460 patients included in the study. These patients were divided into training and testing cohorts, with 368 patients in the training cohort and 92 in the testing cohort.\n\nThe dataset includes a variety of clinical characteristics and laboratory findings collected within 24 hours of admission. Key demographic and clinical features recorded include age, gender, Body Mass Index (BMI), etiology of AP (such as hypertriglyceridemia, biliary, alcohol, and other causes), Heart Rate (HR), Respiratory Rate (RR), body Temperature (T), and history of hypertension, diabetes, and Non-Alcoholic Fatty Liver Disease (NAFLD). Additionally, 42 laboratory parameters were obtained at admission, covering a wide range of biochemical and hematological markers.\n\nThe data used in this study has not been previously published or used by the community in other studies. The dataset is unique to this research and was specifically collected for the purpose of developing and validating machine learning models to predict the risk of Acute Respiratory Distress Syndrome (ARDS) in patients with AP. The study aims to provide a more accurate and convenient early predictive tool to help physicians identify and prevent the progression to ARDS.",
  "dataset/splits": "The dataset was split into two main cohorts: a training cohort and a testing cohort. The data was randomly distributed in a 4:1 ratio, resulting in 368 patients in the training cohort and 92 patients in the testing cohort. The training cohort was used to develop the models, while the testing cohort was used to evaluate their performance. The distribution of patients with Acute Respiratory Distress Syndrome (ARDS) was similar in both cohorts, with 66 cases in the training cohort and 17 cases in the testing cohort. This split ensured that the models were trained on a larger dataset and tested on an independent set to validate their predictive performance.",
  "dataset/redundancy": "The dataset consisted of 497 patients with acute pancreatitis (AP), with 37 exclusions based on specific criteria, resulting in 460 patients included in the study. The data were split into training and testing cohorts at a 4:1 ratio, with 368 patients in the training cohort and 92 in the testing cohort. This split ensured that the training cohort was used to develop the machine learning (ML) models, while the testing cohort served as an independent set to evaluate the models' performance.\n\nThe training and testing sets were independent, which was enforced by the random distribution of the complete data. This independence is crucial for validating the models' generalizability and ensuring that the performance metrics are not overestimated.\n\nThe distribution of the dataset compares favorably to previously published ML datasets in terms of the ratio used for training and testing. The 4:1 ratio is a common practice in ML studies to ensure a robust training process while maintaining a sufficient sample size for reliable testing. The dataset's distribution also reflects real-world clinical scenarios, where the occurrence of acute respiratory distress syndrome (ARDS) in AP patients is relatively low, with 83 out of 460 patients (18.04%) developing ARDS. This distribution is consistent with the clinical context and enhances the models' practical applicability.",
  "dataset/availability": "The data presented in this study are available on request from the corresponding author. The data are not publicly released in a forum. The data can be accessed by contacting the corresponding author, who will facilitate the request according to the relevant guidelines and regulations. The specific license under which the data would be shared is not detailed, but it would typically follow institutional policies and ethical considerations. The enforcement of data sharing would be managed through the institutional review board and ethical committees, ensuring that the data is used responsibly and in accordance with privacy and confidentiality standards.",
  "optimization/algorithm": "The optimization algorithm used in our study is a Bayesian hyperparameter optimizer. This class of algorithms is well-established in the field of machine learning and is used to build a probabilistic model of the objective function. The primary goal is to select the most promising set of hyperparameters to evaluate, thereby enhancing the performance of the machine learning models.\n\nThe Bayesian hyperparameter optimizer is not a new algorithm; it has been extensively used in various machine learning applications. The reason it was not published in a machine-learning journal is that our focus was on applying established methods to a specific medical problem\u2014predicting Acute Respiratory Distress Syndrome (ARDS) in patients with Acute Pancreatitis (AP). Our contribution lies in the application of these optimization techniques to improve the predictive performance of our models in this clinical context, rather than in the development of a new algorithm.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithms involved several steps. Initially, missing values in the original dataset were handled using multiple imputation with the bagImpute method, which is based on a bagged tree model. This ensured that the dataset was complete before further analysis.\n\nContinuous variables were presented as medians with interquartile ranges for skewed distributions or as means with standard deviations for normally distributed variables. Categorical variables were presented as frequencies and proportions. Statistical tests such as Student\u2019s t-test or the Mann-Whitney test were used to compare continuous variables between the ARDS and non-ARDS groups, while Pearson\u2019s Chi-Square or Fisher\u2019s exact test was applied for categorical data. These statistical analyses were performed using SPSS 23.0 software.\n\nThe dataset was then randomly split into training and testing cohorts at a 4:1 ratio. The training cohort was used to develop the models, while the testing cohort was reserved for evaluating their performance. Variables that showed significant differences (p<0.05) in univariate analysis between patients with and without ARDS were selected for input into the machine-learning algorithms.\n\nFour machine-learning algorithms were employed: Support Vector Machine (SVM), Ensembles of Decision Trees (EDTs), Bayesian Classifier (BC), and a nomogram algorithm. These algorithms were implemented using Matlab 2014. Internal validation was conducted using five-fold cross-validation of the training set, repeated ten times to ensure robustness.\n\nFeature selection was performed using the random forest algorithm and Recursive Feature Elimination (RFE) strategy to identify the optimal subset of features that effectively predicted the risk for ARDS. Features with strong internal correlations were tested, and those with the strongest correlations to the target variable ARDS were retained. Ultimately, the best eight features were identified and entered into the machine-learning models. Hyperparameter optimization was carried out using a Bayesian optimizer to build a probabilistic model of the objective function and select the most promising set of hyperparameters.",
  "optimization/parameters": "In our study, we initially considered 42 laboratory parameters obtained at admission. To identify the most relevant features for predicting ARDS, we employed a feature selection process. This involved testing all feature correlations and retaining those with the strongest correlations using the target variable ARDS. Additionally, we used the random forest algorithm and the Recursive Feature Elimination (RFE) strategy to determine an optimal subset of features. Ultimately, the best eight features were identified as the optimal subset for our machine learning models. These features were entered into the models after ensuring they had significant differences (p<0.05) in the univariate analysis between patients with and without ARDS.",
  "optimization/features": "In the optimization process of our machine learning models, we began with a comprehensive set of 42 laboratory parameters obtained at patient admission. To enhance the predictive performance and reduce redundancy, we performed feature selection. This involved testing all feature correlations and retaining those with the strongest correlations to the target variable, ARDS. Through this process, we identified the best eight features as the optimal subset. These features were then entered into the machine learning models. The feature selection was conducted using the training set only, ensuring that the models were evaluated on an independent test set. This approach helped us build a probabilistic model of the objective function and select the most promising set of hyperparameters.",
  "optimization/fitting": "The study involved 460 patients with acute pancreatitis (AP), with 368 included in the training cohort. This cohort was used to develop machine learning (ML) models with various algorithms, including Support Vector Machine (SVM), Ensembles of Decision Trees (EDTs), Bayesian Classifier (BC), and a nomogram algorithm. The number of features used in the models was optimized to eight, which is relatively small compared to the number of training points, thus mitigating the risk of overfitting.\n\nTo further ensure that overfitting was not an issue, the models underwent internal validation using five-fold cross-validation. This process was repeated 10 times to enhance the robustness of the validation. Additionally, a Bayesian hyperparameter optimizer was employed to select the most promising set of hyperparameters, which helped in building a probabilistic model of the objective function and in fine-tuning the models to prevent overfitting.\n\nUnderfitting was addressed by carefully selecting features that had significant differences between patients with and without ARDS. The feature selection process involved using the random forest algorithm and the Recursive Feature Elimination (RFE) strategy to determine an optimal subset of features. This ensured that the models were trained on the most relevant and predictive features, reducing the likelihood of underfitting.\n\nThe models were evaluated using various metrics, including the Area Under the Curve (AUC), accuracy, precision, recall, True Negative Rate (TNR), F1 score, Negative Predictive Value (NPV), and False Discovery Rate (FDR). The Bayesian Classifier (BC) achieved the highest AUC value of 0.891 in the testing set, indicating strong predictive performance and suggesting that underfitting was not a concern. The Ensembles of Decision Trees (EDTs) also showed superior evaluation metrics from the training set, further supporting the models' robustness.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One key method used was feature selection, where we identified and retained only the most relevant features that had strong correlations with the target variable, ARDS. This process helped in reducing the complexity of the models and minimizing the risk of overfitting.\n\nAdditionally, we utilized a Bayesian hyperparameter optimizer to select the most promising set of hyperparameters. This optimization process helped in fine-tuning the models and improving their generalization performance.\n\nTo further validate our models and prevent overfitting, we performed five-fold cross-validation on the training set. This technique involved splitting the training data into five subsets, training the model on four subsets, and validating it on the remaining subset. This process was repeated five times, with each subset serving as the validation set once. This approach ensured that the models were evaluated on different portions of the data, providing a more reliable estimate of their performance.\n\nMoreover, we used a 4:1 ratio to split the complete data into training and testing cohorts. This ensured that the models were trained on a sufficient amount of data while still having an independent test set to evaluate their performance. The use of an independent test set helped in assessing the models' ability to generalize to new, unseen data.\n\nBy combining these techniques, we aimed to build robust and reliable machine learning models that could effectively predict the risk of ARDS in patients with acute pancreatitis.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are not explicitly detailed in the main text. However, the process involved using a Bayesian hyper-parameter optimizer to select the most promising set of hyperparameters to evaluate. This optimization was crucial for building a probabilistic model of the objective function and ensuring that the machine learning models were finely tuned for optimal performance.\n\nThe model files and specific optimization parameters are not directly reported in the publication. The development of the machine learning models involved several algorithms, including Support Vector Machine (SVM), Ensembles of Decision Trees (EDTs), Bayesian Classifier (BC), and a nomogram algorithm. These models were implemented using Matlab 2014.\n\nRegarding the availability of the data and methods, the data presented in this study are available on request from the corresponding author. This ensures that interested researchers can access the necessary information to replicate or build upon our findings. The study protocol was reviewed and approved by the Ethics Committee of Xuanwu Hospital Capital Medical University, and all methods were performed in accordance with relevant guidelines and regulations.\n\nThe supplementary materials associated with this article can be found in the online version at the provided DOI. These materials may include additional details on the hyper-parameter configurations and optimization processes. The authors declare no conflicts of interest, and the research received no specific grant from any funding agency.",
  "model/interpretability": "The models developed in this study include both interpretable and black-box components. The Bayesian Classifier (BC) and Ensembles of Decision Trees (EDTs) can be considered more interpretable compared to the Support Vector Machine (SVM) and the nomogram. The BC provides probabilistic insights into the classification process, making it easier to understand the likelihood of outcomes. EDTs, on the other hand, offer a tree-based structure that can be visualized and interpreted, showing how different features contribute to the final prediction.\n\nThe nomogram, while visually interpretable, relies on logistic regression, which can be seen as a transparent model. It translates complex statistical predictions into a user-friendly format, allowing clinicians to estimate the probability of ARDS by summing the points associated with each predictor variable.\n\nIn contrast, the SVM is often considered a black-box model due to its complex mathematical framework, which makes it difficult to interpret the relationships between input features and the output predictions directly. However, the use of feature selection techniques, such as the Recursive Feature Elimination (RFE) strategy, helps in identifying the most important features, adding a layer of interpretability to the model.\n\nOverall, while some models offer clear interpretability, others rely on more complex mechanisms that are less transparent. The choice of model depends on the trade-off between predictive performance and the need for interpretability in clinical decision-making.",
  "model/output": "The model is a classification model. It is designed to predict the early onset of Acute Respiratory Distress Syndrome (ARDS) in patients with Acute Pancreatitis (AP). The model categorizes patients into two groups: those who will develop ARDS and those who will not. This classification is based on various clinical and laboratory features collected at admission. The performance of the model is evaluated using metrics such as Area Under the Curve (AUC), accuracy, precision, recall, True Negative Rate (TNR), F1 score, Negative Predictive Value (NPV), and False Discovery Rate (FDR). The Bayesian Classifier (BC) model achieved the highest AUC of 0.891, indicating its superior predictive performance in the test set. Additionally, Ensembles of Decision Trees (EDTs) showed high accuracy, precision, and F1 score in the training set, suggesting its strong fitting ability. The model uses a subset of optimal features, including PaO2, CRP, PCT, LA, Ca2+, the NLR, WBC, and AMY, to make predictions. These features were selected through correlation analysis and recursive feature elimination to ensure the least redundancy and the highest predictive power.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method for the machine learning models involved several key steps to ensure robust and reliable performance assessment. Initially, the complete dataset was randomly split into training and testing cohorts at a 4:1 ratio. The training cohort was used to develop the models, while the testing cohort served as an independent dataset to evaluate their predictive performance.\n\nTo validate the models internally, a five-fold cross-validation technique was employed on the training set. This process was repeated ten times to ensure stability and reliability of the results. The performance metrics used for evaluation included the Receiver Operating Characteristics (ROC) curve, the average Area Under the Curve (AUC), accuracy, precision, recall, True Negative Rate (TNR), F1 score, Negative Predictive Value (NPV), and False Discovery Rate (FDR). These metrics provided a comprehensive assessment of the models' predictive capabilities.\n\nThe final models, trained on the optimal feature subsets, were then tested using the independent test set. This approach helped in assessing the generalizability of the models to new, unseen data. The evaluation metrics for the different models in the training set are summarized in a table, showing their performance across various dimensions. Additionally, the ROC curves for the models after five-fold cross-validation are visualized to provide a graphical representation of their performance.",
  "evaluation/measure": "In our study, we evaluated the performance of four machine learning models using a comprehensive set of metrics to ensure a thorough assessment of their predictive capabilities. The metrics reported include the Area Under the Curve (AUC), accuracy, precision, recall, True Negative Rate (TNR), F1 score, Negative Predictive Value (NPV), and False Discovery Rate (FDR). These metrics provide a holistic view of the models' performance, covering aspects such as the models' ability to discriminate between positive and negative cases (AUC), their overall correctness (accuracy), the proportion of true positives among predicted positives (precision), the proportion of true positives among actual positives (recall), the proportion of true negatives among predicted negatives (TNR), the harmonic mean of precision and recall (F1 score), the proportion of true negatives among predicted negatives (NPV), and the proportion of false positives among predicted positives (FDR).\n\nThis set of metrics is representative of standard practices in the literature for evaluating machine learning models, particularly in medical and predictive analytics contexts. The inclusion of AUC, accuracy, precision, recall, and F1 score is common, as these metrics provide a balanced view of model performance. Additionally, reporting TNR, NPV, and FDR is crucial in medical diagnostics, where the consequences of false positives and false negatives can be significant. By including these metrics, we aim to provide a transparent and comprehensive evaluation of our models' performance, ensuring that our findings are robust and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we developed and evaluated four machine learning models to predict the early onset of Acute Respiratory Distress Syndrome (ARDS) in patients with Acute Pancreatitis (AP). The models compared were Support Vector Machine (SVM), Ensembles of Decision Trees (EDTs), Bayesian Classifier (BC), and a nomogram algorithm.\n\nTo ensure a robust comparison, we utilized a five-fold cross-validation technique on the training set for each model. This method involved splitting the data into five subsets, training the model on four subsets, and validating it on the remaining subset. This process was repeated ten times to ensure the stability and reliability of our results.\n\nWe evaluated the models using several key metrics, including the Area Under the Curve (AUC), accuracy, precision, recall, True Negative Rate (TNR), F1 score, Negative Predictive Value (NPV), and False Discovery Rate (FDR). These metrics provided a comprehensive view of each model's performance, allowing us to identify strengths and weaknesses.\n\nThe BC model demonstrated the best predictive performance in the testing set, with the highest AUC of 0.891, recall of 0.563, and NPV of 0.909. This indicates that the BC model was the most effective in accurately predicting the occurrence of ARDS in AP patients. The EDTs model, while not the best overall, showed strong performance with the highest accuracy (0.891), precision (0.800), and F1 score (0.615) in the training set, suggesting its potential for large sample sizes.\n\nWe did not compare our models to publicly available benchmark datasets or simpler baselines, as our focus was on developing and validating models specifically tailored to our dataset and clinical context. Instead, we optimized our models using a Bayesian hyperparameter optimizer to select the most promising set of hyperparameters, ensuring that each model was fine-tuned for optimal performance.\n\nIn summary, our comparison of the four models revealed that the BC model was the most effective for predicting ARDS in AP patients, while the EDTs model showed strong potential for future applications with larger datasets.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The data presented in this study are available on request from the corresponding author. This means that the raw evaluation files are not publicly released. Interested parties can contact the corresponding author to obtain the data. The specific terms or license under which the data would be shared are not detailed."
}