{
  "publication/title": "What variables are important in predicting bovine viral diarrhea virus? A random forest approach.",
  "publication/authors": "Machado G, Mendoza MR, Corbellini LG",
  "publication/journal": "Veterinary research",
  "publication/year": "2015",
  "publication/pmid": "26208851",
  "publication/pmcid": "PMC4513962",
  "publication/doi": "10.1186/s13567-015-0219-7",
  "publication/tags": "- Veterinary Epidemiology\n- Machine Learning\n- Random Forests\n- Bovine Viral Diarrhea Virus\n- Predictive Modeling\n- Feature Selection\n- Cross-Validation\n- Gradient Boosting Machines\n- Support Vector Machines\n- Dairy Cattle Health",
  "dataset/provenance": "The dataset used in this study was collected from a prevalence study of reproductive disease in dairy cattle in the State of Rio Grande do Sul, Brazil. The target population included all dairy herds in this state, with a total of 81,307 registered dairy herds. From this population, 384 herds were selected for Bovine Viral Diarrhea Virus (BVDV) testing using one-stage stratified random sampling.\n\nThe data collection involved a cross-sectional survey that estimated the prevalence of BVDV, Neospora caninum, and Infectious Bovine Rhinotracheitis (IBR) based on bulk tank milk samples. Additionally, a questionnaire was applied to gather information on various predictor variables related to farm management practices, animal husbandry, and other relevant factors.\n\nThe dataset consisted of 40 predictor variables, which were used to train a Random Forest (RF) classification model. These variables included information such as who inseminates the animals, the number of neighboring farms with cattle, the proportion of farm income based on milk production, the duration of milk production on the farm, and various other factors related to farm management and animal health.\n\nThe dataset was split into a training set (80% of observations) and an independent testing set (20% of observations). The training set was used to train the RF model using a cross-validation process, while the testing set was used to evaluate the model's performance on independent data. This approach is common when external validation data is not available and aims to ensure that the model is representative and generalizable to future applications.\n\nThe dataset has not been used in previous papers by the community, as the application of machine learning algorithms in veterinary epidemiology is still uncommon. This study represents one of the few efforts to apply RF and other machine learning methods to analyze cross-sectional data in this field. The dataset and the results of this study contribute to the understanding of BVDV prevalence and the identification of potential risk factors associated with the disease.",
  "dataset/splits": "The dataset was divided into two primary splits: a training set and an independent testing set. The original dataset was randomly and uniformly split, maintaining the same proportion of classes as in the original dataset. The training set comprised 80% of the observations, while the testing set included the remaining 20%. This subdivision was designed to ensure a representative sample size for future applications of the model and is a common strategy for evaluating machine learning models when external validation data is not available.\n\nThe training set was used for training the classifier through a cross-validation process. Specifically, a repeated 10-fold cross-validation was employed to assess the model's performance. This approach helps in providing precise and unbiased estimates of the predictive accuracy and generalization power of the machine learning classifiers. The testing set was then used to compare the models' performance based on independent test data, ensuring that the evaluation was conducted on data not seen during the training phase.",
  "dataset/redundancy": "The dataset used in this study was split into a training set and an independent testing set. The original dataset was randomly and uniformly divided, maintaining the same proportion of classes as in the original dataset. This resulted in 80% of the observations being allocated to the training set and the remaining 20% to the testing set. This subdivision was designed to ensure that the sample size was representative for future applications of the model, a common strategy when external validation data is not available.\n\nThe training set was used to train the classifier through a cross-validation process, while the testing set was employed to evaluate the model's performance on independent data. This approach helps in assessing how well the model generalizes to new, unseen data.\n\nThe distribution of the dataset reflects an attempt to create a representative sample for future applications. This strategy is particularly useful in machine learning when external validation data is not available, ensuring that the model's performance is evaluated on data that it has not seen during training. The random and uniform splitting helps in maintaining the integrity of the class proportions, which is crucial for models that might be biased towards the majority class.",
  "dataset/availability": "Not applicable.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is ensemble learning, specifically focusing on tree-based methods. The primary algorithms employed were Random Forests (RF) and Gradient Boosting Machines (GBM). These are well-established algorithms in the field of machine learning and have been extensively used in various domains, including life sciences and veterinary epidemiology.\n\nThe algorithms used are not new; they have been widely studied and applied in numerous research works. Random Forests, for instance, have been utilized in genomic data analysis and ecology as classifiers. The choice of these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to provide robust predictions.\n\nThe decision to use these algorithms in a veterinary epidemiology context was motivated by their demonstrated success in similar fields. The study aimed to identify important predictors for disease occurrence and evaluate the predictive power of these machine learning models. The algorithms were chosen for their ability to manage class imbalance and provide accurate predictions, which are crucial in epidemiological studies.\n\nThe study did not introduce a new machine-learning algorithm; rather, it applied existing algorithms in a novel context. The focus was on optimizing the performance of these algorithms for the specific problem at hand, which involved careful training processes, parameter optimization, and strategies to address class imbalance. The results showed that these algorithms, particularly Random Forests, performed well in identifying biologically plausible predictors of Bovine Viral Diarrhea Virus (BVDV).\n\nThe algorithms were implemented using established packages in the R statistical environment, such as randomForest and caret. These packages are widely used and trusted in the machine learning community, ensuring the reliability and reproducibility of the results. The study's findings contribute to the growing body of knowledge on the application of machine learning in veterinary epidemiology, highlighting the potential of these methods to outperform traditional statistical approaches.",
  "optimization/meta": "The model described in this publication does not use data from other machine-learning algorithms as input. It is not a meta-predictor. Instead, it employs a single machine-learning method, specifically Random Forests (RF), to analyze cross-sectional data derived from an investigation for Bovine Viral Diarrhea Virus (BVDV) prevalence in Southern Brazil. The goal is to identify important predictors for disease occurrence and evaluate the predictive power of this machine-learning model in veterinary epidemiology.\n\nThe Random Forests method is used to build an ensemble of decision trees, which are trained on different subsets of the data. This approach helps to improve the model's predictive accuracy and robustness. The training process involves parameters optimization and strategies to address class imbalance issues, ensuring that the model performs well even when the classes are not equally represented in the data.\n\nThe evaluation of the model's performance is conducted using a 10-fold cross-validation technique, which is known for providing precise and unbiased estimates of the predictive accuracy and generalization power of machine-learning classifiers. This technique ensures that the training data is independent for each fold, reducing the risk of overfitting and providing a reliable assessment of the model's performance.\n\nIn summary, the model relies solely on the Random Forests algorithm and does not incorporate predictions from other machine-learning methods. The training data is carefully partitioned to ensure independence and robustness in the evaluation process.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the robustness and accuracy of the model. Initially, a dataset consisting of 40 predictor variables was collected through a survey. These variables encompassed a wide range of factors related to farm management and animal husbandry practices, such as the number of neighboring farms with cattle, the proportion of farm income from milk production, and the frequency of veterinary assistance.\n\nGiven the severe class imbalance in the dataset, a down-sampling procedure was incorporated. This procedure involved sampling the majority class to make its frequency closer to that of the minority class. This step was crucial to prevent the machine learning algorithm from being biased towards the majority class, which could lead to a high misclassification rate for the minority class.\n\nThe dataset was then split into a training set, which comprised 80% of the observations, and an independent testing set, which comprised the remaining 20%. This split was done randomly and uniformly, maintaining the same proportion of classes as in the original dataset. The training set was used for training the classifier through a cross-validation process, while the testing set was used to evaluate the model's performance on independent data.\n\nAdditionally, a correlation matrix was computed for the set of 40 variables to identify and eliminate highly correlated variables. This step helped in reducing the dimensionality of the data and improving the model's performance. Variables such as the frequency of technical assistance, frequency of veterinary assistance, origin of animals brought into the farm, and whether the farmer had seen weak calves were identified as highly correlated and were removed from the dataset.\n\nThe final set of variables used for training the model was selected based on their importance, as assessed by the average decrease in the nodes' impurity measured by the Gini index during the construction of the random forest model. The top 25 predictor variables were identified as the most important for BVDV prediction and were used to train the final model. This selection process ensured that the model was trained on the most relevant variables, improving its predictive accuracy and robustness.",
  "optimization/parameters": "In our study, we initially utilized a comprehensive set of 40 predictor variables to train our Random Forest (RF) model. These variables were selected based on their potential relevance to the prediction of Bovine Viral Diarrhea Virus (BVDV) occurrence. The number of variables, p, was determined through a systematic process of feature selection and importance analysis.\n\nTo optimize the model's performance, we conducted a variable importance analysis, which assessed the average decrease in node impurity measured by the Gini index during the construction of the RF model. This analysis helped us identify the most influential variables. We then performed a restricted forward feature selection, incrementally adding variables from the most relevant to the least relevant and evaluating the model's performance at each step. This process aimed to identify and retain only the most impactful variables, thereby simplifying the model and potentially improving its generalization power.\n\nThrough this feature selection process, we found that the model trained with the top 25 predictor variables achieved a slight increase in average accuracy and specificity compared to the model trained with all 40 variables. However, this increase was not statistically significant, suggesting that feature selection did not introduce substantial benefits to the model's performance in this particular scenario. Nonetheless, the top 25 variables were deemed sufficient for maintaining the model's predictive power while reducing complexity.\n\nThe final model was trained using these 25 selected variables, with the number of variables randomly sampled as candidates for node splitting (mtry) optimized to 16. This value was determined using the caret package in R, which performed a grid search to minimize the model's error rate. The use of cross-validation ensured that the selected parameters were robust and generalizable to independent data sets.",
  "optimization/features": "The study initially utilized a set of 40 predictor variables collected through a survey to train the BVDV classification model. These variables encompassed various aspects of farm management and animal husbandry practices.\n\nFeature selection was indeed performed to identify the most relevant predictors. This process involved analyzing the importance of each variable using the Gini index during the construction of the random forest model. The variables were then ranked based on their importance, and a restricted forward feature selection was carried out. This selection aimed to incrementally add variables from the most relevant to the least relevant, evaluating the model's performance at each step.\n\nThe feature selection process was conducted using the training set only, ensuring that the evaluation of variable importance and the subsequent selection of top predictors were based on the data used for model training. This approach helps to prevent data leakage and maintains the integrity of the model evaluation process.\n\nThe final model was trained with the top 25 predictor variables, which were identified as the most impactful for BVDV prediction. This subset of variables was chosen based on the balance of model performance metrics, including the AUC score, specificity, and sensitivity, as well as model complexity. The use of these top 25 variables resulted in a slight improvement in accuracy and specificity compared to the model trained with the full set of 40 variables, although the increase was not statistically significant.",
  "optimization/fitting": "In our study, we employed Random Forests (RF), Support Vector Machines (SVM), and Gradient Boosting Machines (GBM) for classification tasks. The number of parameters in these models, particularly in RF and GBM, is indeed large due to the ensemble nature of these methods. However, we took several steps to mitigate overfitting and ensure robust performance.\n\nFor RF, the algorithm's inherent properties, such as bootstrapping and out-of-bag (OOB) estimates, help in reducing overfitting. The OOB data, which is about one-third of the original data, provides an internal unbiased estimate of the generalization error. Additionally, the ensemble of trees and the random selection of variables at each split contribute to the model's robustness and reduce sensitivity to overfitting.\n\nFor SVM, the performance is heavily dependent on the tuning parameters. We adopted a parameter optimization procedure based on grid search methods to minimize total error rates. This approach helps in finding the optimal parameters that balance bias and variance, thus reducing the risk of overfitting.\n\nGBM, while similar to RF in creating an ensemble of trees, builds trees sequentially, with each tree aiming to correct the errors of the previous ones. This iterative process can lead to overfitting if not properly controlled. To address this, we used techniques such as early stopping and regularization parameters to prevent the model from becoming too complex.\n\nTo further ensure that our models were not overfitting, we used a 10-fold cross-validation technique. This method involves partitioning the data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. The average performance across all folds provides a more reliable estimate of the model's generalization ability.\n\nMoreover, we repeated the entire training process, including data preparation and partitioning, 10 times to observe the variability in performance. This repeated partitioning helped us understand the consistency of our models and ensured that the results were not due to a particular random split of the data.\n\nIn terms of underfitting, we carefully selected the number of trees in the RF and the depth of trees in GBM. For RF, we did not optimize the number of trees based on previous findings that RF is not very sensitive to this parameter. For GBM, we used techniques like early stopping to prevent the model from becoming too simple and underfitting the data.\n\nOverall, our approach involved a combination of parameter tuning, cross-validation, and repeated partitioning to ensure that our models were neither overfitting nor underfitting the data. This rigorous process helped us achieve a balanced and robust performance across different classifiers.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the primary methods used was cross-validation, specifically the 10-fold cross-validation technique. This approach involves partitioning the data into 10 subsets, training the model on 9 of these subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. This method helps in providing a more accurate estimate of the model's performance and reduces the risk of overfitting by ensuring that the model is evaluated on different portions of the data.\n\nAdditionally, we performed repeated partitioning of the original data into training and testing sets. This was done to assess the variability in model performance due to different data splits. By repeating the training process 10 times, we observed significant variance in the performance of the methods, highlighting the importance of consistent data partitioning.\n\nWe also utilized parameter optimization procedures based on grid search methods to minimize total error rates. This involved tuning the parameters of our models to find the optimal settings that would generalize well to unseen data. For instance, in the case of the random forest model, we optimized the number of predictor variables selected for splitting a new node during the production of the decision trees.\n\nFurthermore, we addressed the issue of class imbalance, which is a common problem in machine learning that can lead to overfitting. We applied a down-sampling strategy to balance the classes, ensuring that the model did not become biased towards the majority class. This intervention was crucial in improving the model's performance and preventing overfitting.\n\nIn summary, our approach to preventing overfitting included the use of cross-validation, repeated data partitioning, parameter optimization, and techniques to handle class imbalance. These methods collectively contributed to the development of robust and generalizable models.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available through the use of the caret R package, which was employed to train and tune our models. The specific parameters optimized include the number of predictor variables selected for splitting a new node during the production of the decision trees (mtry) for the Random Forest (RF) model. The optimization process involved a grid search method to minimize total error rates.\n\nThe model files themselves are not explicitly provided in the publication, but the methods and results described allow for replication of the models. The optimization parameters, such as the values tested for mtry, are detailed within the text, enabling others to replicate the tuning process.\n\nRegarding the availability and licensing, the caret R package is open-source and freely available under the GPL-2 license. This means that the methods and tools used for optimization are accessible to the public for further research and application. The specific configurations and schedules used in our study can be inferred from the descriptions provided in the publication, ensuring transparency and reproducibility.\n\nFor those interested in accessing the data and detailed configurations, additional files are mentioned, which include descriptive statistics, frequency of important predictor variables, a correlation matrix, and models' performance for randomly generated independent test data sets. These files provide further insights into the data preparation and model evaluation processes.",
  "model/interpretability": "The model employed in this study is a Random Forest (RF) classifier, which is often considered a \"gray box\" rather than a completely transparent model. While RF provides insights into the importance of variables, it is not as interpretable as simpler models like linear regression. However, it offers several features that enhance interpretability.\n\nOne key aspect of RF that aids in interpretability is its ability to determine variable importance. This is achieved by assessing the average decrease in the nodes' impurity, measured by the Gini index, during the construction of the decision trees within the forest. Variables with higher Gini importance are considered more relevant for the model's predictive power. For instance, in our analysis, variables such as \"who inseminates the animals\" and \"the number of neighboring farms that have cattle\" were identified as highly important predictors for BVDV.\n\nAdditionally, partial dependence plots were used to explore the relevance of variables for classification results. These plots provide a graphical depiction of the marginal effect of a variable on the class probability, helping to understand how changes in a predictor variable influence the outcome. For example, greater y-values in these plots indicate that an observation for a specific variable is associated with a higher probability of classifying new instances as BVDV positive.\n\nFurthermore, the RF model underwent a restricted forward feature selection process. This involved training several RF models, starting from a single variable and incrementally adding new variables based on their importance. By evaluating the performance of each classifier generated, we could identify the most impactful variables and optimize the model's performance. This process not only improved the model's accuracy but also made it more interpretable by focusing on the most relevant predictors.\n\nIn summary, while the RF model is not entirely transparent, it offers valuable tools for interpretability, such as variable importance analysis and partial dependence plots. These features allow for a deeper understanding of the relationships between predictors and the outcome, making the model more interpretable than many other complex machine learning algorithms.",
  "model/output": "The model developed in this study is a classification model. It was designed to predict the occurrence of Bovine Viral Diarrhea Virus (BVDV) based on various predictor variables. The Random Forest (RF) algorithm was employed to build this model, which is well-suited for classification tasks. The performance of the model was evaluated using metrics such as accuracy, sensitivity, and specificity, which are commonly used in classification problems. The model's output provides a classification of instances into positive or negative cases of BVDV, rather than predicting a continuous value, which confirms that it is a classification model.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the methods involved a comprehensive approach to ensure robust and unbiased results. We employed 10 repetitions of 10-fold cross-validation, which is a well-established technique for assessing the performance of machine learning models. This process ensured that each data point was used exactly once for validation, providing a thorough evaluation of the models' predictive accuracy and generalization power.\n\nIn addition to cross-validation, we also evaluated the models using an independent test set. The original dataset was split into a training set (80% of observations) and an independent testing set (20% of observations). This subdivision was done to simulate a real-world scenario where the model would be applied to new, unseen data. The training set was used for model training via cross-validation, while the testing set was used to compare the models' performance on independent data.\n\nFor the cross-validation process, we used the caret R package to train Support Vector Machine (SVM) and Gradient Boosting Machine (GBM) models, tuning parameters to ensure a fair comparison with Random Forests (RF). The performance metrics evaluated included the Area Under the Curve (AUC) score, sensitivity, and specificity, which were derived from the average confusion matrix across all repetitions.\n\nTo further validate the models, we performed a pairwise Wilcoxon rank test to assess the statistical significance of the differences in AUC scores among the models. This test helped us determine whether the observed differences in performance were statistically significant.\n\nMoreover, we compared the performance of RF with logistic regression, a traditional statistical approach frequently used for risk factor analysis. Logistic regression was estimated using the glm() function in R, and its performance was evaluated using the same 10 repetitions of 10-fold cross-validation. This comparison allowed us to demonstrate the superior performance of RF in terms of AUC score, sensitivity, and specificity.\n\nIn summary, the evaluation method involved a rigorous combination of cross-validation and independent testing, along with statistical tests to ensure the reliability and validity of the results. This approach provided a comprehensive assessment of the models' performance and their potential for real-world applications.",
  "evaluation/measure": "In our study, we employed several performance metrics to comprehensively evaluate the models. The primary metrics reported include the area under the Receiver Operating Characteristic curve (AUC), sensitivity (SEN), and specificity (SPE). These metrics were derived from the confusion matrix, which quantifies the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n\nThe AUC score is particularly important as it provides the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one. This metric is crucial for understanding the overall performance of the model across different threshold settings.\n\nSensitivity, also known as recall, measures the proportion of actual positives that are correctly identified by the model. It is calculated as the ratio of true positives to the sum of true positives and false negatives. High sensitivity indicates that the model is effective at identifying positive cases.\n\nSpecificity, on the other hand, measures the proportion of actual negatives that are correctly identified. It is calculated as the ratio of true negatives to the sum of true negatives and false positives. High specificity indicates that the model is effective at identifying negative cases.\n\nIn addition to these metrics, we also reported the total prediction accuracy (ACC), which is the ratio of the sum of true positives and true negatives to the total number of instances. This metric provides an overall measure of the model's performance but can be misleading if the classes are imbalanced.\n\nThe set of metrics used in this study is representative of standard practices in the literature for evaluating classification models. These metrics provide a comprehensive view of the model's performance, covering aspects such as the model's ability to correctly identify positive and negative cases, as well as its overall accuracy. By reporting these metrics, we aim to provide a clear and transparent evaluation of the models' performance, allowing for comparisons with other studies and models in the field.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of the Random Forest (RF) model with other machine learning methods to evaluate its predictive power. We compared RF with Support Vector Machine (SVM) and Gradient Boosting Machine (GBM), which are popular but not extensively assessed in veterinary epidemiology. To ensure a fair comparison, we used the same procedure for training all models, i.e., 10 repetitions of 10-fold cross-validation. This approach guaranteed that the exact same data points were used in each step of model training and testing, maintaining the same subsampling of the training data.\n\nWe utilized the caret R package to train SVM and GBM models, tuning their parameters to facilitate a fair comparison with RF. The performance of the models was assessed based on the results from cross-validation, focusing on metrics such as AUC score, sensitivity, and specificity derived from the average confusion matrix. To determine the statistical significance of the differences in model performance, we employed a pairwise Wilcoxon rank test.\n\nAdditionally, we compared RF with logistic regression, a traditional statistical approach frequently used for analyzing risk factors. Logistic regression was estimated using the glm() function in R, and its performance was evaluated using the same 10 repetitions of 10-fold cross-validation. This ensured that the distribution of data used for RF training was consistent across all folds and repetitions.\n\nFurthermore, we evaluated the models' predictive accuracy using an independent test set derived from the original data. The original dataset was subdivided into training data (80%) and testing data (20%), which was not used in the cross-validation process. The results from this independent test set provided additional insights into the models' performance.\n\nIn summary, our comparison involved both advanced machine learning methods and a traditional statistical approach, ensuring a comprehensive evaluation of RF's predictive power in the context of veterinary epidemiology.",
  "evaluation/confidence": "The evaluation of our models included a thorough assessment of performance metrics, ensuring that our claims of superiority are statistically sound. We employed 10 repetitions of 10-fold cross-validation for all models, which provided a robust estimate of their predictive accuracy. This method ensured that the same data points were used consistently across training and testing phases, maintaining the integrity of our comparisons.\n\nTo assess the statistical significance of the differences in performance, we utilized the pairwise Wilcoxon rank test. This test helped us determine whether the observed differences in AUC scores between models were statistically significant. For instance, when comparing Random Forests (RF) to Support Vector Machines (SVM) and Gradient Boosting Machines (GBM), the p-values indicated that while RF tended to perform better, the differences were not statistically significant at conventional levels (P-value = 0.064 for RF vs. SVM and P-value = 0.075 for RF vs. GBM). This suggests that, although RF showed a slight edge, the differences in AUC scores were not conclusive enough to claim outright superiority.\n\nHowever, when comparing RF to logistic regression, the results were more definitive. RF demonstrated a superior average AUC score of 0.702 compared to 0.610 for logistic regression. The density plots from the cross-validation procedure clearly showed that RF's distribution of AUC scores was shifted to the right, indicating better predictive power. Additionally, RF exhibited a more balanced performance in terms of sensitivity and specificity compared to logistic regression, with statistically significant improvements in both metrics.\n\nFor the independent test set, we evaluated the models' predictive accuracy using ROC curves and AUC scores. The AUC scores were 0.697 for RF, 0.703 for SVM, and 0.785 for GBM. To account for the variability introduced by random data partitioning, we repeated the training and testing process 10 times. This procedure revealed that RF and GBM were consistently the top-performing classifiers, with RF outperforming GBM in 6 out of 10 repetitions. This variability underscores the importance of cross-validation in providing a more reliable estimate of model performance.\n\nIn summary, while the performance metrics for RF showed promising results, the statistical significance varied. The comparisons with logistic regression were clear-cut, favoring RF. However, when comparing RF to other machine learning methods like SVM and GBM, the differences in performance were not statistically significant, highlighting the need for cautious interpretation. The use of confidence intervals and statistical tests ensured that our evaluations were rigorous and reliable.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study includes several additional files that provide supplementary information, such as descriptive statistics on the study population, the frequency of important predictor variables, and a correlation matrix for predictor variables. However, these files do not contain the raw evaluation data used for model performance assessment. The models' performance for 10 randomly generated independent test data sets is summarized in an additional file, but the actual raw data from these evaluations is not provided. The study focuses on presenting the results and insights derived from the evaluations rather than making the raw data publicly accessible."
}