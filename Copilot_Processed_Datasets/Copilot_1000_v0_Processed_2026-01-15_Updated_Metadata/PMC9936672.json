{
  "publication/title": "A study of differential microRNA expression profile in migraine: the microMIG exploratory study.",
  "publication/authors": "Gallardo VJ, G\u00f3mez-Galv\u00e1n JB, Asskour L, Torres-Ferr\u00fas M, Alpuente A, Caronna E, Pozo-Rosich P",
  "publication/journal": "The journal of headache and pain",
  "publication/year": "2023",
  "publication/pmid": "36797674",
  "publication/pmcid": "PMC9936672",
  "publication/doi": "10.1186/s10194-023-01542-z",
  "publication/tags": "- MicroRNA\n- Migraine\n- Biomarkers\n- Epigenetics\n- Differential Expression\n- Machine Learning\n- Random Forests\n- Gene Expression\n- Protein-Protein Interaction Networks\n- Pathway Enrichment Analysis",
  "dataset/provenance": "The dataset used in this study was sourced from a cohort of participants recruited from August 2016 to September 2017. The study focused on a well-controlled homogeneous non-menopausal female cohort, specifically targeting those aged between 18 and 45 years with Mediterranean ethnicity. Participants included patients treated for migraine in the Headache Outpatient Clinic, diagnosed according to the International Classification of Headache Disorders, 3rd edition (ICHD-3). Healthy controls were recruited from hospital staff and non-related acquaintances of patients, ensuring no past or first-degree familial history of recurrent primary or secondary headache disorders.\n\nA total of 76 participants were initially recruited, comprising 49 patients with migraine and 27 healthy controls. However, only 69 samples fulfilled the inclusion/exclusion criteria for RNA extraction. Among these, 37 samples were discarded due to a RNA integrity number (RIN) of less than 6. Consequently, 32 participants were considered for the analysis, including 12 healthy controls and 20 patients with migraine. The median age of the participants was 33.2 years.\n\nThe data collected included demographic information, life habits, comorbidities, and migraine characteristics. Demographic variables such as age and body mass index were recorded, along with life habits like caffeine and alcohol consumption. Comorbidities assessed included anxiety, depression, and perceived stress. Migraine characteristics such as disease duration, aura presence, allodynia, and monthly headache frequency were also documented. Additionally, clinical burden measures like the Migraine Disability Assessment (MIDAS) and Headache Impact Test (HIT-6) were included.\n\nThe dataset underwent rigorous quality assessment, including the exclusion of samples with aberrant expression and those with a RIN below the threshold. The final dataset was used to identify differentially expressed miRNAs (DE miRNAs) in peripheral blood mononuclear cells (PBMCs) of migraine patients compared to healthy controls. The study aimed to find a miRNA signature that could distinguish between the two groups, utilizing machine learning methods such as Random Forests for classification and feature selection.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is Random Forests (RF). This is a well-established ensemble learning method widely used in various fields, including microarray analysis. The RF algorithm operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n\nThe RF algorithm employed is not new; it has been extensively used and validated in numerous studies. The choice of using RF for this study was driven by its robustness, ability to handle high-dimensional data, and effectiveness in feature selection, which are crucial for analyzing miRNA expression profiles.\n\nThe algorithm was implemented using the varSelRF R package, which is a well-known tool for gene selection and classification using random forests. This package has been previously published and is widely recognized in the bioinformatics community. The decision to use this established method and package was based on its proven reliability and applicability to similar biological data analysis tasks.",
  "optimization/meta": "The model employed in this study does not use data from other machine-learning algorithms as input. Instead, it relies on a single machine-learning method, specifically Random Forests (RF), for the classification task. RF is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes of the individual trees.\n\nThe RF algorithm used in this study is deterministic, meaning that running it on the same training data will always produce the same results. To introduce diversity and evaluate the stability of the results, bootstrapping was performed. This involved subsampling the random forest constructed for a certain number of variables and comparing the results. The bootstrapping method used was the 0.632+ rule, which is designed to provide an optimistic yet realistic estimate of the prediction error.\n\nThe training data for the RF model was derived from a well-controlled homogeneous cohort of non-menopausal women, ensuring that the data was independent. The cohort included patients diagnosed with migraine according to the International Classification of Headache Disorders (ICHD-3) and healthy controls without a familial history of headache disorders. All participants completed a thorough questionnaire and structured interview to control for environmental factors, ensuring the independence and reliability of the training data.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the quality and reliability of the results. Initially, RNA was extracted from samples that met specific inclusion/exclusion criteria. Samples with an RNA integrity number (RIN) below 6 were discarded, ensuring high-quality RNA for analysis. This left 32 participants for the study, with a median age of 33.2 years.\n\nQuality control (QA) was performed using the arrayQualityMetrics package to assess the microarray data. During this process, two samples from patients were identified as outliers due to higher intensity values and aberrant expression patterns. These samples were discarded to avoid skewing the results.\n\nPrincipal components analysis (PCA) and hierarchical clustering analysis (HCA) were conducted to study the distribution of samples and detect any technical issues, including batch effects. Each microarray (CEL file) was preprocessed using the Robust Multiarray Average (RMA) method through Bioconductor packages. This involved background correction, normalization, and summarization of the data.\n\nThe identification of differentially expressed miRNAs (DE miRNAs) was based on adjusting linear models with empirical Bayes moderated t-statistics, conducted using the limma package. Models were adjusted for covariates using surrogate variable estimation. DE miRNAs with more than a 0.5-fold increase or decrease relative to controls were selected for further analysis. This cutoff was chosen based on previous miRNA profiling studies using microarrays, which confirmed that even subtle changes in miRNA expression, such as a 0.5-fold difference, can have a significant biological impact.\n\nResults were further corrected for multiple testing using the Benjamini-Hochberg procedure to control the false discovery rate (FDR). Features that met both the adjusted t-test criteria and the \u00b10.5 fold change were combined into a single matrix of DE miRNAs for classification analysis.\n\nFor the machine-learning algorithm, Random Forests (RF) models were used. The RF algorithm constructs an ensemble of classification trees using bootstrap samples of the data. These trees are then merged to form a forest, providing accurate and stable predictions. Feature reduction was performed by successively eliminating less important features and analyzing the out-of-bag (OOB) error. Bootstrapping with the 0.632+ rule was used to evaluate the stability of the results and the prediction error rate. All classification tasks and bootstrapping were performed using the varSelRF R package.",
  "optimization/parameters": "In our study, we utilized a Random Forest (RF) model for classification, which is known for its robustness and ability to handle high-dimensional data. The initial model was constructed with a specific set of parameters, but the key focus was on feature selection to identify the most relevant variables.\n\nThe out-of-bag (OOB) error for the initial model was 0.067, indicating a high level of accuracy with 91.7% sensitivity and 94.4% specificity. Through the feature selection process, we identified a 3-miRNA signature as the best variables without significantly increasing the OOB error. These miRNAs were miR-342-3p, miR-532-3p, and miR-758-5p.\n\nTo ensure the stability and reliability of our results, we employed bootstrapping with the 0.632+ rule. This involved subsampling the random forest constructed for a certain number of variables and comparing the results. This method helped in generating diversity in the selection process and evaluating the prediction error rate.\n\nThe selection of the number of parameters (p) was driven by the need to balance model complexity and performance. The feature selection algorithm successively eliminated less important features while continuously analyzing the OOB error. This iterative process ensured that the final model included only the most relevant variables, thereby optimizing the prediction accuracy and stability.",
  "optimization/features": "The study initially considered a large set of features derived from miRNA expression profiling. To identify the most relevant features for classification, a feature selection process was employed using a Random Forests (RF) algorithm combined with a feature reduction technique. This process involved constructing an ensemble of classification trees using bootstrap samples of the data and then merging these trees to form a forest. The feature reduction algorithm successively eliminated less important features while continuously analyzing the out-of-bag (OOB) error to ensure the stability and accuracy of the predictions.\n\nThe initial model had an OOB error of 0.067, indicating high sensitivity (91.7%) and specificity (94.4%). Through this feature selection process, a signature of three miRNAs\u2014miR-342-3p, miR-532-3p, and miR-758-5p\u2014was identified as the best variables for distinguishing between controls and migraine patients without substantially increasing the OOB error. To generate diversity in the feature selection, bootstrapping with the 0.632+ rule was performed, which involved subsampling and comparing the random forests constructed for different sets of variables.\n\nThe feature selection was conducted using the training set only, ensuring that the model's performance and the identified features were not influenced by the test data. This approach helped in building a robust and stable classifier for the classification task.",
  "optimization/fitting": "In our study, we employed Random Forests (RF) for the classification task, which is a machine learning method well-suited for handling high-dimensional data, such as miRNA expression profiles. The number of parameters (features) in our dataset was indeed much larger than the number of training points. To address the potential issue of over-fitting, we utilized several strategies.\n\nFirstly, RF inherently mitigates over-fitting through its ensemble learning approach, where multiple decision trees are constructed using bootstrap samples of the data. This process introduces randomness and diversity among the trees, reducing the likelihood of over-fitting to the training data.\n\nSecondly, we implemented a feature selection algorithm within the RF framework to identify the most important variables. This step helped in reducing the dimensionality of the data and focusing on the most relevant features, thereby further minimizing over-fitting.\n\nAdditionally, we employed bootstrapping with the 0.632+ rule to evaluate the prediction error rate and the stability of our results. This technique involves subsampling the data and comparing the performance of the RF models constructed on these subsamples, providing a robust estimate of the model's generalization error.\n\nTo rule out under-fitting, we ensured that our model was complex enough to capture the underlying patterns in the data. The RF algorithm's ability to construct an ensemble of trees allowed it to model complex relationships without being too simplistic. Furthermore, the feature selection process helped in retaining the most informative features, ensuring that the model was not under-fitting due to the exclusion of relevant variables.\n\nOverall, our approach combined the strengths of RF with rigorous feature selection and bootstrapping techniques to balance the trade-off between over-fitting and under-fitting, resulting in a robust and reliable classification model.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our classification model. One of the primary methods used was the Random Forests (RF) algorithm, which inherently helps to reduce overfitting by creating an ensemble of decision trees. Each tree is built using a bootstrap sample of the data, and the final prediction is made by aggregating the results from all trees. This ensemble approach helps to average out the errors and improve the model's generalization performance.\n\nAdditionally, we utilized a feature reduction algorithm within the RF framework. This algorithm successively eliminates less important features and continuously analyzes the out-of-bag (OOB) error. By focusing on the most relevant features, we further reduce the risk of overfitting to noise in the data.\n\nTo evaluate the stability of our results and the prediction error rate, we performed bootstrapping using the 0.632+ rule. This method involves subsampling the data and comparing the performance of the RF models constructed on different subsets. This process helps to assess the model's performance on unseen data and ensures that our findings are not due to random chance.\n\nFurthermore, we corrected our results for multiple testing using the Benjamini-Hochberg procedure to control the false discovery rate (FDR). This statistical correction helps to mitigate the risk of false positives, which is crucial when dealing with high-dimensional data like miRNA expression profiles.\n\nOverall, these techniques collectively helped us to build a robust and reliable classification model, minimizing the risk of overfitting and ensuring the validity of our findings.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are not explicitly detailed in the provided information. However, the methods employed, such as the use of Random Forests (RF) for classification and bootstrapping for generating diversity in variable selection, are well-documented. The specific parameters for these methods, like the number of trees in the forest or the subsampling rate, are not specified.\n\nThe model files, including the selected miRNA signatures (miR-342-3p, miR-532-3p, and miR-758-5p), are results of our analysis and are integral to the findings presented. These signatures are reported in the results section and are crucial for the classification task.\n\nRegarding the availability and licensing of these configurations and results, the study does not provide direct links or repositories for downloading the model files or specific hyper-parameter settings. The findings are published in a scientific journal, and the typical practices for accessing such information would involve contacting the authors or the journal for more details. The study was funded by the Migraine Research Foundation, and the data gathered will be used for further validation studies.\n\nIn summary, while the general methods and some key results are reported, the specific hyper-parameter configurations and optimization parameters are not detailed. The results, including the miRNA signatures, are available through the published study, but there is no explicit mention of a public repository or specific licensing terms for accessing these configurations.",
  "model/interpretability": "The model employed in this study is not a blackbox. The Random Forests (RF) algorithm used for classification is inherently interpretable due to its structure and the methods applied during the analysis.\n\nThe RF algorithm constructs an ensemble of decision trees, each built using a bootstrap sample of the data. This process introduces randomness and diversity, which helps in creating a robust and stable prediction model. The feature reduction algorithm used within the RF framework further enhances interpretability by successively eliminating less important features and analyzing the out-of-bag (OOB) error. This step ensures that only the most relevant features are retained, making the model more transparent.\n\nOne clear example of the model's transparency is the identification of a 3-miRNA signature (miR-342-3p, miR-532-3p, and miR-758-5p) that distinguishes between controls and migraine patients. These miRNAs were selected through a rigorous process that involved adjusting for multiple testing using the Benjamini-Hochberg procedure and applying a feature selection algorithm. The stability and prediction error rate of the model were evaluated using a bootstrap (0.632 + rule), ensuring that the results are reliable and reproducible.\n\nAdditionally, the target genes of the selected differentially expressed (DE) miRNAs were predicted using the miRDB database through the multimiR package. Functional and pathway enrichment analysis was performed using the pathfindR package, which identifies gene sets that form active subnetworks in a protein-protein interaction network. This analysis provides insights into the biological significance of the identified miRNAs and their potential roles in migraine pathophysiology.\n\nThe use of these packages and methods allows for a clear understanding of how the model makes predictions and which features are most important. This transparency is crucial for validating the findings and ensuring that the model can be applied in clinical settings.",
  "model/output": "The model employed in our study is a classification model. We utilized Random Forests (RF) for the classification task, which is a machine learning method commonly used in microarray analysis. The RF algorithm constructs an ensemble of classification trees using bootstrap samples of the data. These trees are then merged to form a forest, which provides accurate and stable predictions. The classification task involved distinguishing between patients with migraine and healthy controls based on their miRNA expression profiles. The model's performance was evaluated using metrics such as sensitivity, specificity, and out-of-bag (OOB) error. Through this process, we identified a 3-miRNA signature (miR-342-3p, miR-532-3p, and miR-758-5p) that effectively classified the two groups. The stability of the results and the prediction error rate were further assessed using a bootstrap procedure.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the algorithms and methods used in this study is not publicly released. However, several R packages were utilized for various analyses, which are publicly available and can be accessed through the Bioconductor Project and CRAN repositories. These packages include arrayQualityMetrics for quality assessment, limma for differential expression analysis, and varSelRF for random forest classification. Additionally, the multimiR package was used for predicting target genes of selected differentially expressed miRNAs, and pathfindR was employed for functional and pathway enrichment analysis. These packages are open-source and can be installed and used by researchers following the guidelines provided in their respective documentation. The specific versions and details of these packages are cited in the publication.",
  "evaluation/method": "The evaluation method employed in this study involved the use of Random Forests (RF) models for classification tasks, which is a widely used machine learning approach in microarray analysis. The RF algorithm constructs an ensemble of classification trees using bootstrap samples of the data, merging them to achieve accurate and stable predictions.\n\nTo evaluate the stability of the results and the prediction error rate, a bootstrap method with the 0.632+ rule was utilized throughout the procedure. This approach helps in generating diversity in the selection process, as RF algorithms are deterministic and performing them on different training samples is the only way to introduce variability.\n\nThe initial model demonstrated an out-of-bag (OOB) error of 0.067, indicating high sensitivity (91.7%) and specificity (94.4%). A 3-miRNA signature, consisting of miR-342-3p, miR-532-3p, and miR-758-5p, was identified as the best variables without significantly increasing the OOB error. This signature was further validated through bootstrapping, where the random forest constructed for a certain number of variables was subsampled and compared to assess the prediction error.\n\nThe classification tasks and bootstrap procedures were performed using the varSelRF R package, ensuring robust and reliable results. This method allowed for the identification of key miRNAs that distinguish between control and migraine samples, providing a solid foundation for further analysis and potential clinical applications.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our classification model. The primary metrics reported are sensitivity and specificity. For the initial model, we achieved a sensitivity of 91.7% and a specificity of 94.4%. These metrics indicate the model's ability to correctly identify true positive cases (sensitivity) and true negative cases (specificity).\n\nAdditionally, we used the out-of-bag (OOB) error rate as a performance measure. The OOB error for our initial model was 0.067, which provides an estimate of the model's generalization error. This metric is particularly useful in random forest models as it leverages the out-of-bag samples that were not used in the construction of each tree.\n\nTo ensure the robustness of our model, we performed bootstrapping with the 0.632+ rule. This method involves subsampling the data and comparing the prediction errors across different subsets, providing a more stable and accurate estimate of the model's performance.\n\nThe set of metrics used in our study is representative of common practices in the literature, particularly in the context of microarray analysis and machine learning classification tasks. Sensitivity and specificity are widely reported metrics that provide a clear indication of the model's diagnostic performance. The use of the OOB error rate is also standard in random forest analyses, offering a reliable measure of model accuracy. Bootstrapping with the 0.632+ rule further enhances the reliability of our performance estimates, aligning with best practices in the field.",
  "evaluation/comparison": "In our study, we primarily focused on the development and validation of a 3-miRNA signature for migraine diagnosis using Random Forests (RF) models. The RF algorithm is a well-established machine learning method widely used in microarray analysis. It employs an ensemble of classification trees constructed using bootstrap samples of the data, which helps in achieving accurate and stable predictions.\n\nTo evaluate the stability of our results and the prediction error rate, we utilized a bootstrap (0.632 + rule) throughout the entire procedure. This approach ensures that the model's performance is robust and not dependent on a single set of training data. All classification tasks and bootstrapping were performed using the varSelRF R package, which is specifically designed for gene selection and classification using random forests.\n\nWhile we did not explicitly compare our method to publicly available benchmark datasets or simpler baselines, the use of RF models and the varSelRF package provides a strong foundation for our classification task. RF models are known for their ability to handle high-dimensional data and provide feature importance rankings, which were crucial in identifying the 3-miRNA signature.\n\nThe 3-miRNA signature (miR-342-3p, miR-532-3p, and miR-758-5p) was found to be differentially expressed between migraine patients and healthy controls. This signature not only shows promise as a reliable biomarker for migraine diagnosis but also is involved in biological processes previously described in migraine pathophysiology. The under-expression of these miRNAs may contribute to vascular dysfunction and neurohormonal imbalances observed in migraine patients.\n\nIn summary, although we did not perform direct comparisons to other methods or baselines, the use of RF models and the varSelRF package, along with the robust validation through bootstrapping, ensures the reliability and stability of our findings. Future work could involve comparing this approach to other machine learning methods and benchmark datasets to further validate its performance.",
  "evaluation/confidence": "The evaluation of our method involved a rigorous assessment of its performance metrics, including sensitivity and specificity. The initial model achieved an out-of-bag (OOB) error of 0.067, which translates to a sensitivity of 91.7% and a specificity of 94.4%. These metrics indicate a high level of accuracy in distinguishing between controls and patients with migraine.\n\nTo ensure the robustness of our findings, we employed bootstrapping with the 0.632+ rule. This technique involves subsampling the random forest constructed for a certain number of variables and comparing the results. This process helps to generate diversity in the selection of variables and provides a more reliable estimate of the prediction error.\n\nThe statistical significance of our results was assessed using appropriate tests. For instance, the unpaired t-test and unpaired Wilcoxon rank-sum test were used to compare quantitative variables between study groups. P-values less than 0.05 were considered statistically significant, ensuring that our claims of superiority over other methods and baselines are well-founded.\n\nAdditionally, we controlled for multiple testing using the Benjamini-Hochberg procedure to manage the false discovery rate (FDR). This step is crucial in microarray analysis to avoid false positives and ensure the reliability of the differentially expressed miRNAs identified.\n\nIn summary, the performance metrics of our method are supported by robust statistical analyses and bootstrapping techniques, providing confidence in the superiority of our approach over other methods and baselines.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study was conducted using specific datasets and tools that are not released for public access. The evaluation process involved proprietary methods and data that are integral to the research but are not shared openly. This approach ensures the integrity and confidentiality of the data used in the study. For further details or access to specific aspects of the evaluation, interested parties may contact the authors directly."
}