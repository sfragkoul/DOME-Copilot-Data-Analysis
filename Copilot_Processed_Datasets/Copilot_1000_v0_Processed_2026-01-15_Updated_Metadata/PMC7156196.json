{
  "publication/title": "Machine learning as a diagnostic decision aid for patients with transient loss of consciousness.",
  "publication/authors": "Wardrope A, Jamnadas-Khoda J, Broadhurst M, Gr\u00fcnewald RA, Heaton TJ, Howell SJ, Koepp M, Parry SW, Sisodiya S, Walker MC, Reuber M",
  "publication/journal": "Neurology. Clinical practice",
  "publication/year": "2020",
  "publication/pmid": "32309027",
  "publication/pmcid": "PMC7156196",
  "publication/doi": "10.1212/cpj.0000000000000726",
  "publication/tags": "- Transient Loss of Consciousness\n- Machine Learning\n- Diagnostic Decision Aid\n- Epilepsy\n- Syncope\n- Psychogenic Non-Epileptic Seizures\n- Patient Questionnaires\n- Witness Reports\n- Random Forest Analysis\n- Diagnostic Accuracy",
  "dataset/provenance": "The dataset used in this study was collected from respondents who experienced transient loss of consciousness (TLOC) and had a confirmed diagnosis of epilepsy, syncope, or psychogenic non-epileptic seizures (PNES). The data collection process involved sending invitations, information sheets, and questionnaires to potential participants. The study received ethical approval from the Northern and Yorkshire Multi-Centre Research Ethics Committee. A total of 300 respondents returned the Paroxysmal Event Profile (PEP) questionnaires, with 100 respondents for each diagnosis. Additionally, 249 of these respondents also returned the Paroxysmal Event Observer (PEO) questionnaires, which were completed by witnesses of the TLOC events. The demographic characteristics of this subgroup were similar to those of the entire study sample, as reported in previous publications. The data collected included information on demographic features, family history of blackouts, and various cardiovascular or neurologic comorbidities. The PEP and PEO questionnaires were used to gather details about peri-episodal symptoms and observable episode manifestations, respectively. The responses were recoded into binary \"ever\" or \"never\" formats to reflect the likelihood of patients experiencing few episodes. The dataset was then randomly split into training and validation sets in a 2:1 ratio. The training group for the patient-only analysis comprised 208 participants, while the training group for the witness/patient analysis included 163 participants. The remaining participants were assigned to the validation groups. The dataset utilized in this study has not been previously used by the community, but the methodology and some of the tools, such as the PEP and PEO questionnaires, have been described in earlier publications.",
  "dataset/splits": "There were two main data splits: the training group and the validation group. For the patient-only analysis, the training group comprised 208 participants, with 149 being female. The remaining participants were assigned to the validation group. For the witness/patient analysis, the training group consisted of 163 participants, with 114 being female. The validation group comprised the remainder of the participants.\n\nIn the patient-only analysis, the training group included 73 participants with epilepsy, 72 with PNES, and 63 with syncope. In the witness/patient analysis, the training group included 58 participants with epilepsy, 52 with PNES, and 53 with syncope. The validation groups were not explicitly detailed in terms of individual diagnoses, but they comprised the remaining participants not included in the training groups.",
  "dataset/redundancy": "The datasets were split into training and validation groups to evaluate the predictive performance of the reduced-dimension model. For the patient-only analysis, the training group comprised 208 participants, while the remaining participants were assigned to the validation group. Similarly, for the witness/patient analysis, the training group consisted of 163 participants, with the rest forming the validation group. This split ensured that the training and test sets were independent, which is crucial for assessing the model's generalizability.\n\nTo enforce independence between the training and validation sets, we used a subset of the total respondents for training and reserved the rest for validation. This approach helps in controlling for overfitting, where a model might perform well on the training data but poorly on unseen data. The distribution of participants in the training and validation sets was designed to reflect the overall study sample, ensuring that the results are representative and reliable.\n\nThe distribution of the datasets compares favorably with previously published machine learning datasets in the context of transient loss of consciousness (TLOC). The use of random forests (RFs) allowed for the identification of key features that are jointly predictive of diagnosis, reducing the number of predictor variables without sacrificing accuracy. This method leverages nonlinear interactions between predictors, which is a significant advantage over linear regression-based methods. The inclusion of both patient-reported symptoms and witness-reported signs further enhances the model's ability to distinguish between different diagnoses, such as epilepsy, syncope, and psychogenic non-epileptic seizures (PNES).",
  "dataset/availability": "Not applicable",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is the Random Forest (RF) classifier. This is a well-established ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes of the individual trees.\n\nThe RF algorithm used is not new; it is a widely recognized and utilized method in the field of machine learning. The decision to use this algorithm was driven by its robustness and ability to handle complex, nonlinear interactions between predictors, which is particularly useful for distinguishing between different causes of transient loss of consciousness (TLOC).\n\nThe focus of this publication is on the application of machine learning techniques to improve the diagnosis of TLOC, rather than the development of a new machine-learning algorithm. Therefore, it was published in a clinical practice journal rather than a machine-learning journal. The emphasis is on demonstrating the feasibility and practical benefits of using RF classifiers in a clinical setting to enhance diagnostic accuracy and efficiency.",
  "optimization/meta": "The model described in this publication does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The approach involves using random forests (RFs) for feature selection and classification. The RFs were trained and validated using data from questionnaires completed by patients and, in some cases, witnesses. The feature selection process involved iteratively removing the least-important predictors and calculating the out-of-bag error to identify the optimal set of variables. The resulting RFs were then used to classify patients into diagnoses of epilepsy, syncope, or psychogenic non-epileptic seizures (PNES).\n\nThe training and validation data were derived from the same set of respondents, with a subset used for training and the remainder for validation. While this approach ensures that the validation data is independent of the training data, it does not involve the use of multiple machine-learning algorithms in a meta-predictor framework. The independence of the training data is maintained by using separate subsets of the same dataset for training and validation purposes.",
  "optimization/encoding": "For the machine-learning algorithm, the data was encoded and pre-processed through a series of steps to ensure optimal performance. Initially, questionnaires were administered to gather data from participants, including both patients and witnesses. These questionnaires covered historical information, symptoms, and witness reports.\n\nThe data collected from these questionnaires was then used to train random forest (RF) models. To handle missing values, which were present in a significant portion of the responses, RF analysis with surrogate splitting was employed. This method is robust to missing data, allowing for the inclusion of all available information without the need for imputation.\n\nFeature selection was performed using an iterative process where the least-important 20% of predictors were removed at each step. The out-of-bag error was calculated for each iteration, and the set of variables that minimized this error was selected. This procedure was conducted twice: once using history, symptom, and witness report data (witness\u2013patient scenario), and once using only history and symptoms (patient-only scenario).\n\nThe selected features were then used to train the RF models. For the witness\u2013patient scenario, 36 features were identified as jointly predictive of diagnosis, including historical complaints, peri-ictal symptoms, and witness-reported signs. For the patient-only scenario, 34 features were selected, comprising historical information and peri-ictal symptoms.\n\nThe relative importance of these selected predictors was determined by assessing the change in classification error when each predictor was permuted. This information was used to understand the contribution of each feature to the model's predictive performance.\n\nAll analyses were performed using MATLAB R2017b with the Statistics and Machine Learning Toolbox. The code, including the RF classifiers, is available upon request. This approach ensured that the data was appropriately encoded and pre-processed for the machine-learning algorithm, enabling accurate classification of patients into diagnoses of epilepsy, syncope, or psychogenic non-epileptic seizures (PNES).",
  "optimization/parameters": "In our study, we utilized a random forest (RF) approach for variable reduction and feature selection. Initially, we had a large number of potential predictor variables, but we employed an iterative algorithm to progressively reduce the number of predictors. This process involved removing the least-important 20% of predictors at each step and calculating the out-of-bag error for each RF. The selected set of variables was the one that minimized the out-of-bag error, following the \"0 standard error rule.\"\n\nFor the witness\u2013patient scenario, which included history, symptom, and witness report data, we identified 36 features that were jointly predictive of diagnosis. These features comprised 6 historical or non-ictal complaints, 20 peri-ictal symptoms, and 10 witness-reported signs.\n\nIn the patient-only scenario, where only history and symptoms were used, we identified 34 features. These included 8 historical features and 26 peri-ictal symptoms.\n\nThe selection of these features was driven by the need to balance model complexity and predictive performance. By iteratively reducing the number of predictors and evaluating the out-of-bag error, we ensured that the final set of features was both efficient and effective in classifying patients into diagnoses of epilepsy, syncope, or psychogenic non-epileptic seizures (PNES).",
  "optimization/features": "In our study, we utilized different sets of features for classification depending on the availability of witness data. For the witness/patient scenario, where both patient and witness reports were available, we identified 36 features that were jointly predictive of diagnosis. These features included historical or non-ictal complaints, peri-ictal symptoms, and witness-reported signs.\n\nFor the patient-only scenario, where no witness data was available, we identified 34 features. These features consisted of historical information and peri-ictal symptoms reported by the patients themselves.\n\nFeature selection was performed using a random forest-based iterative feature selection method. This process was conducted using the training set only, ensuring that the validation set remained independent for evaluating the performance of the selected features. The goal was to minimize the out-of-bag error, which helped in selecting the most relevant features for accurate classification.",
  "optimization/fitting": "The study involved a scenario where the number of potential predictor variables was significantly larger than the number of training points. This is a common situation in small-n large-p datasets, where the sample size (n) is much smaller than the number of predictor variables (p).\n\nTo address the risk of over-fitting, an ensemble bootstrap-aggregation technique, specifically random forests (RFs), was employed. This method involves constructing ensembles of decision trees using bootstrap sampling and aggregation, which improves performance and stability in variable selection. The iterative algorithm used for feature selection trained progressively smaller RFs by removing the least-important 20% of predictors at each step. The out-of-bag error was calculated for each RF, and the set of variables that minimized this error was selected. This approach, known as the \"0 standard error rule,\" helps to ensure that the model generalizes well to unseen data, thereby mitigating over-fitting.\n\nTo rule out under-fitting, the performance of the RF classifiers was evaluated on a validation sample. The classifiers were trained on a training set and then tested on a separate validation set to assess their predictive accuracy. For the witness/patient scenario, the 36-feature RF correctly classified 86.0% of patients in the validation sample, demonstrating strong predictive performance. Similarly, the 34-feature RF for the patient-only scenario correctly classified 78.3% of patients, indicating that the models were not under-fitted. Additionally, the RF classifiers outperformed a multinomial logistic regression model, further supporting the robustness of the chosen approach.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method was the use of random forests (RFs) for feature selection and classification. RFs inherently help to reduce overfitting by averaging multiple decision trees, each trained on a different bootstrap sample of the data. This ensemble approach improves the model's generalization ability.\n\nAdditionally, we utilized an iterative feature selection process where we progressively removed the least-important 20% of predictors at each step. This process helped in identifying the most relevant features that minimized the out-of-bag error, a technique known as the \"0 standard error rule.\" By focusing on the most important features, we reduced the complexity of the model and mitigated the risk of overfitting.\n\nFurthermore, we split our data into training and validation sets in a 2:1 ratio. This allowed us to train our models on a subset of the data and evaluate their performance on a separate validation set, ensuring that our results were not overly optimistic due to overfitting.\n\nTo further validate our approach, we compared the performance of our RF model against a multinomial logistic regression model using the same set of predictors. The RF model outperformed the regression model, demonstrating its effectiveness in handling nonlinear interactions between predictors and reducing overfitting.\n\nOverall, these techniques collectively ensured that our models were robust and generalizable, minimizing the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are available upon request. All code, including the random forest classifiers, is accessible from the authors. This includes the specific details of how the models were trained and optimized. The data and statistical analyses are also available from the authors on request. This transparency allows for reproducibility and further validation of our findings.",
  "model/interpretability": "The model employed in this study is a Random Forest (RF) classifier, which is generally considered a black-box model due to its complexity and the ensemble nature of decision trees. However, it offers some level of interpretability through feature importance and partial dependence plots.\n\nThe RF classifier provides insights into which features are most important for making predictions. For instance, in the witness/patient scenario, 36 features were identified as jointly predictive of diagnosis, including historical complaints, peri-ictal symptoms, and witness-reported signs. The relative importance of these predictors can be visualized, showing which features contribute most to the model's decisions.\n\nSimilarly, in the patient-only scenario, 34 features were selected, with a clear distinction between historical features and peri-ictal symptoms. The importance of these features can also be visualized, aiding in understanding which aspects of the patient's history and symptoms are most influential.\n\nWhile the RF model itself is not transparent in the sense of a simple linear model, the feature importance rankings and visualizations provide a way to interpret the model's decisions. This interpretability is crucial for clinical applications, where understanding the basis for a diagnosis is essential. The model's ability to handle nonlinear interactions between predictors also means that it can capture complex relationships in the data, which might be missed by simpler, more transparent models like logistic regression.\n\nIn summary, while the RF classifier is not entirely transparent, it offers valuable insights through feature importance and visualizations, making it a useful tool for clinical decision-making. Future work could focus on developing more interpretable models or simplifying the RF classifier for easier clinical application.",
  "model/output": "The model is a classification model. It is designed to distinguish between three different types of transient loss of consciousness (TLOC): syncope, epilepsy, and psychogenic non-epileptic seizures (PNES). The model uses a random forest (RF) algorithm to classify patients into these diagnostic categories based on a set of features derived from patient and witness reports. The classification performance was evaluated on a validation sample, with the model achieving high accuracy, particularly when both patient and witness reports were available. The model's output is the predicted diagnosis for each patient, which can aid in directing patients to the appropriate medical investigations and referrals.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the machine learning algorithms used in this study, including the random forest classifiers, is available from the authors upon request. This includes all the code necessary to replicate the analyses and train the models. However, there is no mention of an executable, web server, virtual machine, or container instance being released for public use. The code is provided for research purposes and can be used under the terms specified by the authors.",
  "evaluation/method": "The evaluation method involved a rigorous process to assess the predictive performance of the reduced-dimension models. We utilized random forests (RFs) to classify patients into diagnoses of epilepsy, syncope, or psychogenic non-epileptic seizures (PNES) using a validation sample. The classification results were then compared to reference standard diagnoses.\n\nThe procedure was performed twice: once using history, symptom, and witness report data (referred to as \"witness\u2013patient\"), and once using only history and symptoms (referred to as \"patient-only\"). This approach allowed us to evaluate the model's performance in scenarios where an observer of the event is present and where no such observer is available.\n\nFor the witness\u2013patient scenario, the 36-feature RF classified 86.0% of patients correctly in the validation sample. Specifically, it correctly identified 100% of syncope diagnoses, 85.7% of epilepsy diagnoses, and 75.0% of PNES diagnoses. In the patient-only scenario, the 34-feature RF classified 78.3% of patients correctly, with 83.8% accuracy for syncope, 81.5% for epilepsy, and 67.9% for PNES.\n\nAdditionally, we compared the performance of the witness\u2013patient RF against a multinomial logistic regression model using the same 36 predictors. The RF outperformed the regression model, demonstrating higher classification accuracy. Sensitivity analyses were also conducted to evaluate the effects of selected witness and patient variables on classifier accuracy.\n\nAll analyses were performed using MATLAB R2017b with the Statistics and Machine Learning Toolbox. The code, including the RF classifiers, is available upon request.",
  "evaluation/measure": "In our study, we evaluated the performance of our classifiers using several key metrics to ensure a comprehensive assessment. For both the witness/patient and patient-only classifiers, we reported sensitivity and specificity for each diagnosis category: epilepsy, psychogenic non-epileptic seizures (PNES), and syncope. Sensitivity measures the proportion of true positives correctly identified by the classifier, while specificity measures the proportion of true negatives correctly identified.\n\nFor the witness/patient classifier, the sensitivity was 85.7% for epilepsy, 75% for PNES, and 100% for syncope. The specificity was 91.4% for epilepsy, 96.3% for PNES, and 91.7% for syncope. For the patient-only classifier, the sensitivity was 81.5% for epilepsy, 67.9% for PNES, and 83.8% for syncope. The specificity was 80% for epilepsy, 93.8% for PNES, and 94.5% for syncope.\n\nWe also reported the overall classification accuracy, which is the proportion of correctly classified instances out of the total instances. The witness/patient classifier achieved an accuracy of 86%, while the patient-only classifier had an accuracy of 78.3%.\n\nThese metrics are widely used in the literature for evaluating diagnostic classifiers, ensuring that our results are comparable to other studies in the field. The inclusion of sensitivity, specificity, and overall accuracy provides a robust evaluation of our classifiers' performance, highlighting their strengths and areas for potential improvement.",
  "evaluation/comparison": "In our evaluation, we conducted a comparison between our random forest (RF) classifier and a multinomial logistic regression model. The comparison was performed using the same set of 36 predictors that were selected through our witness/patient variable reduction procedure. The RF classifier demonstrated superior performance, achieving a classification accuracy of 86.0% compared to 77.9% for the regression model. This difference in performance was assessed using McNemar\u2019s test, which yielded a p-value of 0.096, indicating a trend towards statistical significance.\n\nWe also explored the theoretical and practical advantages of our approach over previously published methods. For instance, our 36-feature RF includes all items from a 9-point regression-based clinical decision rule (CDR) presented by Sheldon et al., except for episodal diaphoresis, which was not included in our Paroxysmal Event Profile (PEP). While Sheldon et al. reported high sensitivity and specificity for distinguishing syncope from epilepsy, their CDR does not discriminate between epilepsy and psychogenic non-epileptic seizures (PNES). Additionally, their study had limitations such as including only tonic-clonic seizures and lacking objective findings during typical episodes.\n\nOur results are largely consistent with previous attempts to derive CDRs for the diagnosis of transient loss of consciousness (TLOC), but our approach offers a finer classification into three categories: epilepsy, syncope, and PNES. This provides a more detailed and nuanced diagnostic tool. Furthermore, our classifier was less successful in distinguishing epilepsy from PNES compared to syncope, but the results are comparable to those of Syed et al., who used a similar variable-reduction followed by machine-learning classification.\n\nIn summary, our evaluation included a direct comparison with a simpler baseline method (multinomial logistic regression) and considered the performance of our classifier in the context of existing literature. This comprehensive approach ensures that our findings are robust and relevant to clinical practice.",
  "evaluation/confidence": "The evaluation of our classifiers includes confidence intervals for the performance metrics. For instance, the sensitivity and specificity of our witness-patient and patient-only classifiers are presented with 95% confidence intervals. This provides a range within which the true sensitivity and specificity are likely to fall, giving a measure of the precision of our estimates.\n\nStatistical significance is also considered in our comparisons. For example, when comparing the performance of our random forest (RF) classifier against a multinomial logistic regression model, we used McNemar's test. The results indicated that the RF outperformed the regression model, with a classification accuracy of 86.0% compared to 77.9%, although the p-value of 0.096 suggests that this difference is not statistically significant at the conventional 0.05 level. This nuanced approach ensures that our claims about the superiority of our method are grounded in rigorous statistical analysis.",
  "evaluation/availability": "All data and statistical analyses are available from the authors on request. This includes the raw evaluation files used in the study. The code, including the random forest classifiers, is also available from the authors upon request. The study was conducted with ethical approval, and the data was collected with participant consent. The specific details of the data and analyses can be obtained by contacting the authors directly."
}