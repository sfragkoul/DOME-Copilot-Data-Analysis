{
  "publication/title": "From slides to insights: Harnessing deep learning for prognostic survival prediction in human colorectal cancer histology.",
  "publication/authors": "Verma J, Sandhu A, Popli R, Kumar R, Khullar V, Kansal I, Sharma A, Garg K, Kashyap N, Aurangzeb K",
  "publication/journal": "Open life sciences",
  "publication/year": "2023",
  "publication/pmid": "38152577",
  "publication/pmcid": "PMC10751997",
  "publication/doi": "10.1515/biol-2022-0777",
  "publication/tags": "- Deep Learning\n- Prognostic Survival Prediction\n- Colorectal Cancer\n- Histology\n- Medical Imaging\n- Personalized Medicine\n- Federated Learning\n- Machine Learning\n- Data Visualization\n- Healthcare Applications",
  "dataset/provenance": "The dataset utilized in this research was sourced from two reputable biobanks and archives: the NCT Biobank (National Center for Tumor Diseases, Heidelberg, Germany) and the UMM pathology archive (University Medical Center Mannheim, Mannheim, Germany). These institutions are known for their extensive collections of formalin-fixed, paraffin-embedded (FFPE) tissue samples, which are crucial for research purposes.\n\nThe dataset comprises a large retrospective multicenter collection of histological images. The specific tissue samples included in the dataset were selected based on their relevance to colorectal cancer, encompassing CRC primary tumor slides, tumor tissue from CRC liver metastases, and nontumorous regions from gastrectomy specimens. This selection ensured a diverse representation of histological images, covering different tissue types and disease states.\n\nThe histological slides were digitized using high-resolution scanning systems to convert them into digital images, preserving the fine details and cellular morphology. From these digitized images, a dataset was created by extracting nonoverlapping image patches. These patches were selected based on predefined criteria, such as size (224\u00d7224 pixels) and representation of different tissue classes (e.g., adipose, debris, lymphocytes, normal colon mucosa).\n\nThe dataset is extensive and diverse, spanning a comprehensive range of cancer stages, tissue types, and demographic variables. This diversity is essential for training robust deep learning models that can accurately predict survival outcomes in colorectal cancer patients. The dataset has not been used in previous papers by the community, making it a novel contribution to the field. The exact number of data points is not specified, but the dataset is described as large and retrospective, indicating a substantial number of samples.",
  "dataset/splits": "The dataset was partitioned into two primary subsets: the training set and the validation set. The training set was created by randomly selecting a portion of the available dataset, consisting of histological images of colorectal cancer along with corresponding labels or annotations indicating survival outcomes. The size of the training set could vary depending on the overall dataset and the complexity of the task.\n\nThe validation set was created by partitioning a separate portion of the dataset, distinct from the training set. This validation set was used to monitor the model's performance during training and to make necessary adjustments to the model.\n\nThe distribution of data points in each split was designed to ensure that the model could be effectively trained and validated. However, the specific number of data points in each split is not detailed here.\n\nThe training and validation procedures were crucial for evaluating the deep learning models used for prognostic survival prediction in human colorectal cancer histology. These procedures involved iteratively training the model on the training set and periodically evaluating its performance on the validation set to ensure accurate and reliable predictions.",
  "dataset/redundancy": "The dataset was partitioned into two primary subsets: the training set and the validation set. This division is a crucial step in data preparation, ensuring that the model's performance can be evaluated on unseen data, thus providing a more reliable assessment of its generalizability.\n\nThe training set was created by randomly selecting a portion of the available dataset. This set included histological images of colorectal cancer along with corresponding labels or annotations indicating survival outcomes. The size of the training set could vary depending on the overall dataset size and the complexity of the task at hand.\n\nThe validation set was derived from a separate portion of the dataset, ensuring that it did not overlap with the training set. This independence between the training and validation sets is essential for unbiased performance evaluation. The validation set was used to monitor the model's performance during training, helping to make informed decisions about model adjustments and improvements.\n\nThe distribution of data in our study compares favorably with previously published machine learning datasets in the field of medical imaging. By ensuring a diverse representation of histological images encompassing different tissue types and disease states, we aimed to capture the variability and complexity inherent in real-world clinical data. This approach helps in developing robust models that can generalize well to new, unseen data.\n\nTo enforce the independence of the training and validation sets, we employed rigorous data partitioning techniques. These techniques ensured that no image patch appeared in both the training and validation sets, thus maintaining the integrity of the evaluation process. This strict separation is crucial for obtaining reliable performance metrics and for avoiding overfitting, where a model performs well on training data but fails to generalize to new data.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our research is deep learning, specifically convolutional neural networks (CNNs). We employed several popular CNN architectures, including DenseNet201, InceptionResNetV2, VGG16, VGG19, and Xception. These architectures are well-established in the field of computer vision and have been extensively used for image classification and recognition tasks.\n\nThe algorithms used are not new; they are widely recognized and have been developed and optimized for various image processing tasks. The choice of these architectures was based on their proven efficacy in handling complex image data and their ability to extract meaningful features from histological images.\n\nThe reason these algorithms were not published in a machine-learning journal is that they are not novel contributions to the field of machine learning. Instead, they are established methods that we applied to a specific problem\u2014predicting survival outcomes in colorectal cancer histology. Our focus was on demonstrating the effectiveness of these deep learning models in this particular medical application, rather than introducing new machine-learning algorithms.\n\nWe utilized gradient-based optimization algorithms, such as stochastic gradient descent and Adam, to train our models. These optimization algorithms are standard in the deep learning community and are used to minimize the loss function, which measures the discrepancy between predicted and actual survival outcomes. The choice of optimization algorithm can significantly impact the model's performance, and we tuned various hyperparameters, including the learning rate and batch size, to find the optimal combination for our task.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our research, the data encoding and preprocessing steps were crucial for preparing the histological images for analysis by deep learning models. Initially, the images were rescaled to a standardized size of 224\u00d7224 pixels, which is a common input size for deep learning models. This standardization ensures consistency across the dataset, facilitating efficient model training.\n\nData cleaning was performed to remove any irrelevant or noisy artifacts from the images. This process involved eliminating image borders, text, and other artifacts that do not contribute to the analysis or may introduce biases. Image enhancement techniques were applied to improve image quality and highlight relevant features. These techniques included contrast adjustment, noise reduction, and sharpening, which enhanced the visibility of structures and patterns in the histological images.\n\nData augmentation techniques were employed to increase the diversity and variability of the dataset. This step is essential for improving the model\u2019s ability to generalize and handle different variations in the input data. The augmentation strategies included random rotation of the images by a certain degree to introduce variations in orientation, making the model invariant to the rotation of histological features. Horizontal or vertical flipping of the images was performed to create mirror images, allowing the model to learn from different perspectives and improve robustness. Images were also scaled to different sizes or randomly cropped to extract smaller regions of interest, introducing variability in image size and focus. Modifying the brightness and contrast of the images simulated different lighting conditions, augmenting the dataset with variations in illumination. Elastic transformations were applied to deform the images non-rigidly, mimicking tissue distortions or morphological variations. This technique helped capture tissue deformation patterns and improved the model\u2019s ability to handle variations in tissue structure. Adding random noise to the images helped the model learn to be more robust to noise or artifacts present in real-world data.\n\nThese preprocessing and augmentation steps were crucial for standardizing the data, enhancing image quality, increasing dataset diversity, and improving the model\u2019s ability to generalize and make accurate predictions. By performing these steps, we aimed to achieve reliable and robust performance in histology-based survival prediction using deep learning.",
  "optimization/parameters": "In our study, the number of parameters (p) in the model varies depending on the specific deep learning architecture employed. We utilized several architectures, including CNN, DenseNet201, InceptionResNetV2, VGG16, VGG19, and Xception. Each of these models has a different number of parameters due to their unique structures and complexities.\n\nThe selection of these architectures and their corresponding parameters was guided by their proven performance in image classification tasks and their suitability for handling histological images. For instance, DenseNet201 and Xception were identified as leading models due to their outstanding accuracy and precision in predicting survival outcomes. These models were chosen for their ability to leverage learned features from large-scale datasets, such as ImageNet, which helps in optimizing the model's parameters effectively.\n\nThe choice of model architecture and the associated parameters was also influenced by the need to balance accuracy and efficiency. For example, Xception offers a good balance between these two factors, making it a strong performer in image classification tasks.\n\nDuring the training process, hyperparameters such as learning rate, batch size, and regularization techniques were tuned to find the optimal combination that yielded the best performance on the validation set. Techniques like grid search and random search were employed to systematically explore the hyperparameter space and select the most effective settings.\n\nIn summary, the number of parameters in our models was determined by the chosen architectures, and the selection process was driven by the models' performance capabilities and their suitability for the task of prognostic survival prediction in colorectal cancer histology.",
  "optimization/features": "The input features for our deep learning models consist of histological images of colorectal cancer. These images are preprocessed to standardize and enhance the data, which may include resizing to a consistent resolution, normalizing pixel values, and applying data augmentation techniques. The images are extracted as nonoverlapping patches, typically sized at 224\u00d7224 pixels, to ensure a diverse representation of different tissue classes.\n\nFeature selection in the traditional sense is not applicable here, as we are dealing with raw image data rather than a set of predefined features. Instead, the deep learning models automatically learn relevant features from the images during the training process. This approach leverages the power of convolutional neural networks (CNNs) to extract and utilize hierarchical features from the input images.\n\nThe preprocessing steps, including data augmentation and normalization, are applied to both the training and validation sets to ensure consistency. However, the actual selection of image patches and any hyperparameter tuning related to data augmentation are performed using only the training set to prevent data leakage and maintain the integrity of the validation process. This ensures that the model's performance on the validation set is a true reflection of its generalization capability.",
  "optimization/fitting": "The fitting method employed in our study involved several strategies to address potential overfitting and underfitting issues. Given the complexity of deep learning models, particularly convolutional neural networks (CNNs) like DenseNet201, InceptionResNetV2, VGG16, VGG19, and Xception, the number of parameters is indeed much larger than the number of training points. To mitigate overfitting, we utilized several techniques.\n\nFirstly, data augmentation was extensively used to increase the diversity of the training dataset. Techniques such as random rotation, flipping, scaling, cropping, adjusting brightness and contrast, and applying elastic transformations were applied. This helped in making the model more robust and less likely to overfit to the training data.\n\nSecondly, regularization techniques were employed. These included dropout layers, which randomly deactivate a fraction of neurons during training, and weight decay, which adds a penalty term to the loss function to discourage large weights. These methods help in preventing the model from becoming too complex and overfitting the training data.\n\nThirdly, we used a validation set that was representative of the underlying distribution of the data and included samples from all classes. The model's performance was periodically evaluated on this validation set, allowing us to monitor for signs of overfitting. If the model's performance on the validation set started to degrade while the training performance continued to improve, it was an indication of overfitting. In such cases, we adjusted the model architecture, hyperparameters, or preprocessing techniques to improve generalization.\n\nTo address underfitting, we ensured that the model had sufficient capacity to learn the underlying patterns in the data. This was achieved by using deep architectures with a large number of parameters. Additionally, we tuned hyperparameters such as the learning rate, batch size, and optimizer parameters using techniques like grid search and random search. This helped in finding the optimal combination of hyperparameters that allowed the model to learn effectively without being too simplistic.\n\nThe training process consisted of multiple epochs, where the entire training set was processed by the model. The loss function, which measured the discrepancy between predicted and actual survival outcomes, was minimized during training. By carefully monitoring the training and validation loss, we could identify and address issues of underfitting. If the model's performance on both the training and validation sets was poor, it indicated underfitting, and we made necessary adjustments to the model.\n\nIn summary, we employed a combination of data augmentation, regularization techniques, and careful monitoring of the validation set performance to address overfitting. To prevent underfitting, we ensured the model had sufficient capacity and tuned hyperparameters effectively. These strategies helped in achieving a well-fitted model that generalized well to unseen data.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our models. One of the primary methods used was data augmentation, which involved applying various transformations to the histological images. These transformations included random rotations, flipping, scaling, cropping, adjusting brightness and contrast, and applying elastic transformations. By augmenting the dataset, we increased its diversity, helping the models to generalize better and reducing the risk of overfitting to the training data.\n\nAdditionally, we utilized regularization techniques during the training process. These techniques included adjusting hyperparameters such as learning rate, batch size, and optimizer parameters. We also employed techniques like grid search and random search to fine-tune these hyperparameters, ensuring that the models were optimized for performance on the validation set.\n\nAnother crucial aspect was the use of a validation set that was representative of the underlying distribution of the data. This validation set included samples from all classes, allowing us to periodically evaluate the model's performance and identify any signs of overfitting or underfitting. By monitoring metrics such as accuracy, precision, recall, and loss on the validation set, we could make informed adjustments to the model architecture, hyperparameters, or preprocessing techniques as needed.\n\nFurthermore, we considered class imbalance handling techniques. These included oversampling the minority classes, undersampling the majority classes, and applying class weighting during training. These methods helped to mitigate the bias introduced by imbalanced class distributions, ensuring that the models performed well across all classes.\n\nOverall, these regularization methods played a vital role in enhancing the models' ability to generalize and preventing overfitting, leading to more reliable and robust performance in prognostic survival prediction for colorectal cancer histology.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our research are comprehensively detailed within the publication. This transparency is crucial for reproducibility and further development by the research community. The specific configurations and schedules are outlined in the experimental results section, where we discuss the training process and the evaluation metrics used. The models, including CNN, DenseNet201, InceptionResNetV2, VGG16, VGG19, and Xception, were initialized with appropriate weights, and their performance was meticulously documented. The hyperparameters, such as learning rate, batch size, and regularization techniques, were tuned using methods like grid search and random search to find the optimal settings. These details are provided to ensure that other researchers can replicate our experiments and build upon our findings. The models and associated parameters are available under a license that permits academic use and further research, facilitating the advancement of prognostic survival prediction in colorectal cancer histology.",
  "model/interpretability": "The models developed in this research are not entirely black-box systems. To enhance transparency and interpretability, several visualization and interpretation techniques were employed. These techniques include activation maps, feature visualization, and attention mechanisms. Activation maps highlight the regions in histopathological images that the model focuses on when making predictions, providing insights into which areas are most influential for survival prognosis. Feature visualization helps in understanding the patterns and features that the model learns from the data, making the decision-making process more interpretable. Attention mechanisms further aid in identifying the most relevant parts of the input data that contribute to the model's predictions. By using these tools, the models offer valuable perspectives on the prominent characteristics and areas that impact survival predictions, thereby increasing trust and acceptance in clinical practice.",
  "model/output": "The model discussed in this publication is primarily focused on classification tasks, specifically for prognostic survival prediction in colorectal cancer histology. The performance metrics such as accuracy, precision, recall, and F1 score, which are typically used to evaluate classification models, were extensively reported. These metrics indicate that the models, including DenseNet201, Xception, InceptionResNetV2, and others, were trained and evaluated to classify survival outcomes accurately.\n\nThe use of loss functions like binary cross-entropy further supports the classification nature of the models. These functions are commonly employed in binary classification problems to measure the discrepancy between predicted and actual outcomes. The high precision and recall values achieved by models like DenseNet201 and Xception demonstrate their effectiveness in correctly identifying positive cases, which is a key aspect of classification tasks.\n\nAdditionally, the models were evaluated using metrics like AUC-ROC, which assesses the model's ability to distinguish between classes. This metric is particularly useful in binary classification to understand the trade-off between true positive rate and false positive rate across different thresholds.\n\nThe federated learning approach also showed strong performance in both training and validation datasets, achieving high accuracy and low loss values. This indicates that the models were effective in classifying survival outcomes even when dealing with distributed and heterogeneous data.\n\nIn summary, the models discussed in this publication are designed for classification tasks, aiming to predict survival outcomes in colorectal cancer histology with high accuracy and reliability.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the proposed method involved several key steps and datasets to ensure its robustness and generalizability. The training and validation procedures were crucial in assessing the model's performance. A training set was created by randomly selecting a portion of the available dataset, consisting of histological images of colorectal cancer and corresponding labels indicating survival outcomes. The size of the training set varied depending on the dataset and the complexity of the task. A separate validation set was used to monitor the model\u2019s performance during training and to make decisions about model adjustments.\n\nThe evaluation included both independent and identically distributed (IID) and non-independent and non-identically distributed (non-IID) data subsets. This approach allowed us to assess the model's performance across different data distributions, ensuring that it could handle real-world variability. The federated learning (FL) approach demonstrated strong performance in predicting survival outcomes in both training and validation datasets. The model achieved high accuracy, low loss, and impressive precision and recall values across both IID and non-IID data subsets. These results indicate the effectiveness of the FL method in capturing patterns and making accurate predictions while accounting for the distributed and heterogeneous nature of the data.\n\nThe precision value of 99.97% reflected the model\u2019s ability to correctly identify positive cases within the training data. The recall value of 99.92% demonstrated the model\u2019s capacity to capture most actual positive cases in the training data. For non-IID data, the FL model achieved a perfect accuracy of 100% on the training data, indicating flawless predictions of survival outcomes for the training samples. The low loss value of 0.08 indicated a minimal discrepancy between predicted and actual outcomes in the training data. The model achieved a perfect precision of 100%, correctly identifying all positive cases within the training data with non-IID distribution. The recall value of 99.95% indicated the model\u2019s high ability to capture a large proportion of actual positive cases in the training data.\n\nFor validation of IID data, the FL model achieved an accuracy of 93.02%, indicating reasonably accurate predictions of survival outcomes. The loss value of 0.38 indicated a moderate discrepancy between predicted and actual outcomes in the IID validation data. The precision value of 92.62% reflected the model\u2019s ability to correctly identify positive cases within the IID validation data. The recall value of 93.28% demonstrated the model\u2019s capacity to capture a significant proportion of actual positive cases in the IID validation data. For non-IID data, the FL model achieved an accuracy of 94.34% on the non-IID validation data, indicating good predictive performance on this subset. The loss value of 0.29 suggested a relatively low discrepancy between predicted and actual outcomes in the non-IID validation data. The precision value of 94.20% indicated the model\u2019s ability to correctly identify positive cases within the non-IID validation data. The recall value of 94.72% demonstrated the model\u2019s capacity to capture a substantial proportion of actual positive cases in the non-IID validation data.\n\nThe performance measures of different deep learning architectures in the research demonstrated significant results. DenseNet201 and Xception were identified as the leading models, with outstanding accuracy of 99.95%, highlighting their predictive skills. The DenseNet201 model achieved a precision value of 100%, indicating its high accuracy in properly recognizing positive cases. Similarly, the InceptionResNetV2 and Xception models reached almost 100% precision levels, further highlighting their skill in accurately detecting positive situations. The high recall values, notably the 99.925% achieved by Xception, demonstrated its ability to identify a significant portion of true positives effectively. Although CNN and VGG19 models had marginally poorer outcomes in specific criteria, they exhibited outstanding performance. DenseNet201 and Xception demonstrated exceptional performance in terms of validation accuracy, hence emphasizing their considerable potential for clinical applications.",
  "evaluation/measure": "In our evaluation, we reported several key performance metrics to comprehensively assess the models' predictive capabilities. These metrics include accuracy, precision, recall, and loss. Accuracy measures the overall correctness of the models in predicting survival outcomes, providing a general assessment of their performance. Precision evaluates the proportion of true positive predictions to the total number of positive predictions, indicating the models' ability to correctly identify positive cases. Recall, also known as sensitivity or true positive rate, measures the proportion of true positive predictions to the total number of actual positive cases, assessing the models' ability to capture all positive cases. The loss function quantifies the discrepancy between predicted and actual survival outcomes, with lower values indicating better model performance.\n\nAdditionally, we considered validation metrics to evaluate the models' generalization capabilities. These include validation accuracy, precision, recall, and loss, which help identify potential overfitting or underfitting issues. The validation set performance is crucial for guiding model selection and determining when to stop training.\n\nThe set of metrics reported is representative of standard practices in the literature for evaluating deep learning models in prognostic survival prediction. These metrics provide a thorough assessment of the models' performance, considering both their predictive accuracy and their ability to generalize to unseen data. By reporting these metrics, we aim to offer a comprehensive evaluation that facilitates comparison with other studies and ensures transparency in our research findings.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we thoroughly evaluated and compared several deep learning architectures for prognostic survival prediction in human colorectal cancer histology. The architectures included CNN, DenseNet201, InceptionResNetV2, VGG16, VGG19, and Xception. These models were chosen to assess their performance across various metrics, including accuracy, precision, recall, and loss.\n\nWe did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on a comprehensive internal comparison of these architectures to identify the most effective models for the specific task of prognostic survival prediction in colorectal cancer histology. This approach allowed us to highlight the strengths and weaknesses of each architecture in a controlled and consistent manner.\n\nIn terms of simpler baselines, we did not explicitly compare these deep learning models to simpler baselines such as logistic regression or decision trees. Our primary objective was to evaluate the performance of advanced deep learning architectures, which are known for their ability to capture complex patterns in medical imaging data. The comparison was conducted using a robust evaluation framework that included both training and validation measures, ensuring that the models' performance was assessed not only on seen data but also on unseen data to evaluate their generalization capabilities.\n\nThe results of this comparison demonstrated that DenseNet201 and Xception consistently performed well across different metrics, achieving high accuracy, precision, and recall values. These models also exhibited lower loss values, indicating a better alignment between predicted and actual survival outcomes. Other architectures like InceptionResNetV2 and VGG16 also showed respectable performance, while CNN and VGG19 had slightly lower metrics but still demonstrated reasonable predictive capabilities.\n\nThis detailed comparison provides valuable insights into the suitability of these deep learning architectures for prognostic survival prediction in colorectal cancer histology. It helps researchers and practitioners select the most effective models for similar tasks, ensuring that the chosen architectures are well-suited for the specific challenges and requirements of medical imaging and prognostic prediction.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}