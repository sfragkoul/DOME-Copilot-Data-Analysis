{
  "publication/title": "A Genome-Wide Association Study and Machine-Learning Algorithm Analysis on the Prediction of Facial Phenotypes by Genotypes in Korean Women.",
  "publication/authors": "Yoo HY, Lee KC, Woo JE, Park SH, Lee S, Joo J, Bae JS, Kwon HJ, Park BJ",
  "publication/journal": "Clinical, cosmetic and investigational dermatology",
  "publication/year": "2022",
  "publication/pmid": "35313536",
  "publication/pmcid": "PMC8933694",
  "publication/doi": "10.2147/ccid.s339547",
  "publication/tags": "- Customized cosmetics\n- Single-nucleotide polymorphism\n- Genome-wide association study\n- Machine-learning algorithm\n- Microarray\n- Skin phenotype prediction\n- Facial phenotype\n- Ridge regression\n- Korean women\n- Skin biophysical parameters",
  "dataset/provenance": "The dataset used in this study was sourced from a cohort of Korean women aged 30\u201350 years. A total of 749 participants were initially recruited, but after quality control measures, 730 samples were used for the primary discovery analysis. The data collection process involved measuring five skin biophysical parameters: melanin level, gloss, hydration, wrinkle, and elasticity. These measurements were taken from four distinct facial sites: forehead, cheek, canthus, and chin. The study also included genotyping data obtained using the Infinium Global Screening Array (GSA) Bead Chip, which contains multiethnic genome-wide content and quality control markers. Additionally, 34,136 markers specific to pharmacogenomics and Korean-specific content were designed, resulting in a total of 680,960 markers. The final dataset comprised 375,084 SNPs after applying quality control criteria. The dataset has not been used in previous papers or by the community, as this study represents the primary discovery analysis.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a test set. The training set consisted of 584 samples, which accounted for 80% of the total dataset. The test set comprised 146 samples, making up the remaining 20%.\n\nAdditionally, the training set was further divided using a five-fold cross-validation procedure. This means the training data was split into five subsets, or folds. In each iteration of the cross-validation, one fold was used as a validation set, while the remaining four folds were used for training. This process was repeated five times, with each fold serving as the validation set once. This approach ensured that each data point was used for both training and validation, providing a robust evaluation of the machine-learning models.",
  "dataset/redundancy": "The dataset used in this study consisted of 730 samples. To ensure robust evaluation of the machine-learning models, the dataset was randomly divided into training and test sets. The training set comprised 80% of the data, totaling 584 samples, while the test set included the remaining 20%, amounting to 146 samples. This split was designed to maintain the independence of the training and test sets, ensuring that the models were evaluated on unseen data.\n\nTo enforce the independence of the training and test sets, all steps of feature selection and model training were conducted solely on the training set. The test set was used exclusively for the final evaluation of the models, ensuring that it remained independent and unbiased. This approach helps to prevent data leakage and overfitting, which are common issues in machine-learning model evaluation.\n\nThe distribution of the dataset in this study is comparable to previously published machine-learning datasets in terms of the proportion of training and test data. The use of a 80-20 split is a common practice in the field, as it provides a good balance between having enough data for training the models and having a sufficient amount of data for evaluating their performance. This split ensures that the models are trained on a representative sample of the data while still being able to generalize well to new, unseen data.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the class of linear regression models. Specifically, three well-established algorithms were employed: linear regression, ridge regression, and linear support vector regression (SVR). These algorithms are not new; they are widely used and recognized in the field of machine learning.\n\nThe choice of these algorithms was driven by the need to find the optimal solution for personal skin diagnosis by identifying genetic markers associated with skin biophysical parameters. The study aimed to define skin genotype\u2013phenotype correlations and provide the most appropriate machine-learning method for predicting skin phenotype from genotype.\n\nRidge regression, in particular, showed the highest accuracy across all skin traits in the cross-validation results, making it the best-performing model for this specific application. The use of these established algorithms ensures robustness and reliability in the predictions, which is crucial for developing customized cosmetics and accurate personal skin diagnosis.\n\nThe focus of this study was on applying these algorithms to a specific biological problem rather than developing new machine-learning techniques. Therefore, the results were published in a dermatology journal rather than a machine-learning journal, as the primary contribution lies in the application and validation of these methods in the context of skin phenotype prediction.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The model employs three well-established machine-learning algorithms: linear regression, ridge regression, and linear support vector regression (SVR). These algorithms were evaluated using five-fold cross-validation with the training set. The training set was randomly divided into training (80%) and test (20%) sets, ensuring that the testing sets were used only for the final evaluation. All steps of feature selection and training were conducted on the training sets only, maintaining the independence of the training data.",
  "optimization/encoding": "For the machine-learning algorithm, the data encoding and preprocessing involved several key steps. Initially, the dataset comprised 730 samples, which were randomly divided into training and test sets, with 80% of the data used for training and 20% reserved for testing. This division ensured that the model's performance could be robustly evaluated.\n\nThe phenotype measurements, which included skin biophysical parameters such as melanin level, skin gloss, hydration level, fine wrinkles, and skin elasticity, were used as the dependent variables (y). These phenotypes were continuous values, making linear regression methods suitable for analysis. The genotype data, consisting of single nucleotide polymorphisms (SNPs), served as the predictors (x). No additional covariates were included in the regression analysis.\n\nTo ensure data quality, various quality control measures were applied. Variants with significant differences in minor allele frequency were compared and removed. For generic variants, quality control criteria included a minor allele frequency greater than 1%, Hardy\u2013Weinberg equilibrium with a p-value greater than 0.00001, a genotype call rate above 98%, and an individual missing rate below 2%. Samples with an individual missing rate exceeding 2% were excluded. Outliers were removed based on interquartile range and principal component analysis.\n\nFeature selection was performed to identify the most promising predictors. Autosomal variants with a genome-wide association study p-value less than 0.05 were selected for machine-learning analyses. XGBoost was applied to choose markers with nonzero \"gain\" coefficients, ensuring that only the most relevant genetic markers were included in the model.\n\nThe preprocessing steps also involved imputation, which was performed using SHAPEIT v2.r904 and Minimac3 v2.0.1 software. The 1000 Genomes Phase 3 dataset was used as a reference, and imputed genetic dosage with R-squared statistics greater than 0.8 was included in the association analysis. This imputation process helped to fill in missing genotype data, enhancing the completeness and reliability of the dataset.\n\nIn summary, the data encoding and preprocessing involved rigorous quality control, feature selection, and imputation steps. These processes ensured that the genotype and phenotype data were accurately represented and ready for analysis using machine-learning algorithms.",
  "optimization/parameters": "In our study, the number of parameters (p) used in the model varied depending on the specific phenotype being analyzed. Initially, we started with a large set of genetic markers, specifically 680,960 markers, which included multiethnic genome-wide content, curated clinical research variants, and quality control markers. However, not all of these markers were used in the final model. We applied quality control criteria to filter out variants with minor allele frequency differences, Hardy\u2013Weinberg equilibrium deviations, low genotype call rates, and high individual missing rates. This filtering process reduced the number of SNPs to 375,084.\n\nFor the machine-learning analyses, we further refined the set of predictors. We selected only potentially associated autosomal variants with a genome-wide association study p-value less than 0.05. To extract the most promising predictors, we employed XGBoost, which helped us identify markers with nonzero \"gain\" coefficients. This feature selection process ensured that only the most relevant genetic markers were included in the final model.\n\nThe exact number of parameters (p) used in the model for each phenotype was determined through this feature selection process. The final models were built using these selected markers, and their performance was evaluated using cross-validation and test set analyses. This approach allowed us to optimize the model by including only the most informative genetic markers, thereby improving the predictive accuracy for each skin phenotype.",
  "optimization/features": "In the study, feature selection was indeed performed. Initially, we focused on potentially associated autosomal variants identified through genome-wide association studies (GWAS) with a p-value threshold of less than 0.05. To refine this selection further, we employed the XGBoost algorithm, which helped us identify markers with nonzero \"gain\" coefficients. This process was conducted using the Sklearn v0.20.3 package and Xgboost in Python v3.0.\n\nIt is important to note that all steps of feature selection were carried out exclusively on the training set. The test set was kept separate and was only used for the final evaluation of the models. This approach ensures that the feature selection process does not introduce any bias from the test data, maintaining the integrity of the model's performance assessment.\n\nThe number of features (f) used as input varies depending on the specific phenotype being analyzed. However, the final dataset comprised 375,084 single nucleotide polymorphisms (SNPs) after quality control measures were applied. These SNPs served as the input features for the machine-learning models.",
  "optimization/fitting": "In our study, we employed three linear regression-based algorithms: linear regression, ridge regression, and linear support vector regression (SVR). The number of predictors (genotype markers) was indeed much larger than the number of observations (samples), which is a common scenario in genome-wide association studies.\n\nTo address the risk of overfitting, we utilized ridge regression, which incorporates L2-regularization. This technique adds a penalty equal to the sum of the squared values of the coefficients to the loss function. By doing so, it shrinks the coefficients of less important predictors, effectively reducing the model's complexity and preventing overfitting. The tuning parameter \u03bb in ridge regression controls the amount of shrinkage, with larger values leading to more regularization.\n\nFor linear SVR, we also employed L2-regularization to maximize the flatness of the function, which helps to prevent overfitting by encouraging simpler models. The parameter C in linear SVR regulates the trade-off between achieving a low training error and a low testing error, with smaller values of C leading to more regularization.\n\nTo ensure that our models were not underfitting, we evaluated their performance using five-fold cross-validation on the training set. This procedure involves dividing the training data into five subsets, training the model on four subsets, and validating it on the remaining subset. This process is repeated five times, with each subset serving as the validation set once. By averaging the performance across the five folds, we obtained a robust estimate of the model's predictive power. Additionally, we compared the performance of different algorithms and selected the one with the highest average r\u00b2 value and the smallest deviation, indicating a good balance between bias and variance.\n\nFurthermore, we used a separate test set, which was not involved in the training or cross-validation process, to evaluate the final performance of the selected algorithms. This independent test set provided an unbiased estimate of the model's generalization performance, helping to ensure that our models were not underfitting.",
  "optimization/regularization": "In our study, we employed ridge regression as one of our machine-learning algorithms, which is a regularization technique designed to prevent overfitting. Ridge regression adds a penalty equal to the sum of the squared coefficients multiplied by a tuning parameter (\u03bb) to the loss function. This penalty shrinks the coefficients, reducing the model's complexity and helping to avoid overfitting, especially when dealing with large multivariate data with more predictors than observations.\n\nThe tuning parameter \u03bb controls the model\u2019s complexity. When \u03bb is set to zero, ridge regression behaves like traditional linear regression. However, by increasing \u03bb, the model becomes more regularized, and the coefficients shrink towards zero, which can help to mitigate overfitting.\n\nIn addition to ridge regression, we also used linear support vector regression (SVR), which incorporates an L2-norm regularization. This regularization minimizes the squared sum of the regression coefficients, promoting a flatter decision boundary and thus helping to prevent overfitting.\n\nThese regularization methods were crucial in ensuring that our models generalized well to unseen data, providing robust predictions for skin biophysical parameters based on genotype data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are available within the text of the publication. Specifically, details about the machine-learning algorithms employed, including linear regression, ridge regression, and linear support vector regression (SVR), are provided. The tuning parameter \u03bb for ridge regression, which controls the model\u2019s complexity, is discussed, along with the regularization techniques used in linear SVR.\n\nThe optimization schedule and model files are not explicitly detailed in the text, but the methods and processes used for feature selection and model training are described. For instance, the dataset was randomly divided into training (80%) and test (20%) sets, and five-fold cross-validation was used to evaluate the models. The performance metrics, such as the r\u00b2 values for different phenotypes, are reported, providing insights into the optimization process.\n\nRegarding the availability and licensing of the configurations and parameters, the study mentions the use of specific software and packages, such as Sklearn v0.20.3 and Xgboost in Python v3.0, which are open-source and freely available. The data and code used for the analyses are not explicitly mentioned as being available for download, but the methods and tools used are well-documented and can be replicated using the information provided in the publication.",
  "model/interpretability": "The models employed in this study, including linear regression, ridge regression, and linear support vector regression (SVR), are generally considered transparent or interpretable. These models are not black-box models, as they provide clear insights into the relationships between the predictors (genotype) and the phenotype (skin biophysical parameters).\n\nLinear regression, for instance, provides coefficients that indicate the strength and direction of the relationship between each predictor and the outcome. These coefficients can be easily interpreted to understand how changes in specific genetic markers influence skin traits.\n\nRidge regression, which is a regularized version of linear regression, also provides interpretable coefficients. The regularization term helps to prevent overfitting by shrinking the coefficients, but the model remains interpretable. The tuning parameter \u03bb controls the model\u2019s complexity, and when \u03bb is set to zero, ridge regression reduces to ordinary linear regression.\n\nLinear SVR, while more complex than linear regression, still aims to find a linear function that best fits the data while maximizing flatness. The coefficients in linear SVR can also be interpreted in a similar manner to linear regression, although the presence of the regularization term and the \u03b5-insensitive loss function adds some complexity.\n\nIn summary, the models used in this study are transparent and provide interpretable results. The coefficients from linear regression and ridge regression offer clear insights into the genetic predictors of skin traits, making these models valuable for understanding the underlying mechanisms of skin phenotype variations.",
  "model/output": "The model employed in this study is a regression model. Specifically, we utilized linear regression, ridge regression, and linear support vector regression (SVR) to predict continuous skin biophysical parameters based on genotype data. These parameters include melanin level, skin gloss, hydration, fine wrinkles, and skin elasticity. The regression models were chosen because the phenotype values are continuous, making it challenging to distinguish between two or three consecutive values. The performance of these models was evaluated using the coefficient of determination (r\u00b2), which indicates the proportion of variance in the data explained by the respective variables in the model. The results provided the r\u00b2 value for each fold, the average r\u00b2 value of five folds, and the r\u00b2 value of the test set. Ridge regression generally showed the best performance across all phenotypes, demonstrating its effectiveness in handling multivariate data with more predictors than observations. The final model selected for prediction was based on the algorithm with the largest r\u00b2 value and the smallest deviation.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a combination of cross-validation and independent testing to assess the performance of the machine-learning models. Initially, the dataset, consisting of 730 samples, was randomly divided into a training set (80%) and a test set (20%). The training set was used to develop and optimize the models through a five-fold cross-validation procedure. This approach ensured a robust evaluation of the models' predictive power by training and validating them on different subsets of the data.\n\nThree well-established machine-learning algorithms were evaluated: linear regression, ridge regression, and linear support vector regression (SVR). The performance of these models was compared using the coefficient of determination (r\u00b2), which measures the proportion of the variance in the dependent variable that is predictable from the independent variables.\n\nCross-validation results indicated that ridge regression generally performed the best across all phenotypes. However, the performance varied depending on the specific phenotype. For instance, linear regression performed poorer for melanin compared to ridge regression and linear SVR. Conversely, linear SVR showed poorer performance for gloss compared to linear regression.\n\nThe test results, which involved evaluating the models on the independent test set, showed that the performance was generally poorer than the cross-validation results. This discrepancy was observed across all phenotypes and models, with the differences between the ridge regression results and cross-validation results ranging from 0.1232 to 0.2164 depending on the phenotype.\n\nIn summary, the evaluation method involved a rigorous process of cross-validation and independent testing to ensure the reliability and generalizability of the machine-learning models. The performance of the models was assessed using standard metrics, and the results highlighted the strengths and weaknesses of different algorithms for predicting various skin phenotypes.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our machine-learning models in predicting skin phenotypes. The primary metric used was the coefficient of determination, denoted as r\u00b2. This metric represents the proportion of variance in the data that is explained by the respective variables in the model. We reported the r\u00b2 value for each fold of the cross-validation process, the average r\u00b2 value across the five folds, and the r\u00b2 value for the test set. This comprehensive reporting allowed us to assess the consistency and generalizability of our models.\n\nAdditionally, we compared the performance of different machine-learning algorithms, including linear regression, ridge regression, and linear support vector regression (SVR). By evaluating these algorithms, we aimed to identify the most robust model for each skin phenotype. The performance metrics indicated that ridge regression generally outperformed the other models, particularly for phenotypes like melanin and elasticity. However, the performance varied depending on the phenotype, highlighting the importance of selecting the appropriate algorithm for each specific prediction task.\n\nThe use of r\u00b2 as a primary performance metric is consistent with standard practices in the literature, as it provides a clear and interpretable measure of model fit. Furthermore, the inclusion of cross-validation results ensures that our models are not overfitted to the training data and can generalize well to new, unseen data. This approach enhances the reliability and validity of our findings, making them comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on evaluating the performance of different machine-learning algorithms within the context of our specific dataset and research questions.\n\nWe did, however, compare the performance of three well-established machine-learning algorithms: linear regression, ridge regression, and linear support vector regression (SVR). These algorithms were chosen for their relevance to our data and their widespread use in similar studies. By evaluating these algorithms, we were able to identify which approach performed best for predicting various skin phenotypes.\n\nAdditionally, we used cross-validation to ensure that our models were robust and generalizable. This involved dividing our dataset into training and test sets, and then further splitting the training set into five folds for cross-validation. This process helped us to assess the performance of each algorithm more accurately and to select the best-performing model for each phenotype.\n\nWhile we did not compare our methods to simpler baselines in the traditional sense, the inclusion of linear regression as one of our algorithms served as a baseline comparison. Linear regression is a simpler model compared to ridge regression and linear SVR, and its performance provided a reference point for evaluating the more complex models.\n\nIn summary, our evaluation focused on comparing different machine-learning algorithms within our dataset, using cross-validation to ensure robustness and generalizability. We did not perform a direct comparison to publicly available methods or simpler baselines beyond the inclusion of linear regression as a baseline model.",
  "evaluation/confidence": "The evaluation of our models involved a rigorous assessment of their performance using several metrics. Specifically, we employed the coefficient of determination (r\u00b2) to quantify the proportion of variance in the data explained by our models. This metric was calculated for each fold in our five-fold cross-validation process, providing an average r\u00b2 value across the folds. Additionally, we reported the r\u00b2 value for the test set to evaluate the model's generalization performance.\n\nTo ensure the robustness of our findings, we compared the performance of three different machine-learning algorithms: linear regression, ridge regression, and linear support vector regression. The performance metrics for these models were similar for most phenotypes, with ridge regression generally showing the best performance. The differences between the ridge regression results and the cross-validation results were quantified for each phenotype, providing a clear indication of the model's performance consistency.\n\nStatistical significance was assessed through the p-values obtained from our genome-wide association studies (GWAS). For instance, single-nucleotide polymorphisms (SNPs) with the highest association signals for each phenotype were identified, and their p-values were reported. This allowed us to determine which genetic variants were significantly associated with the phenotypes under study.\n\nWhile confidence intervals for the performance metrics were not explicitly stated, the use of five-fold cross-validation and the reporting of average r\u00b2 values across folds provide a measure of the variability and reliability of our results. The differences between the ridge regression results and the cross-validation results further support the statistical significance of our findings.\n\nIn summary, our evaluation process included comprehensive performance metrics, statistical significance testing, and cross-validation to ensure the confidence and reliability of our results. The use of multiple machine-learning algorithms and the detailed reporting of performance differences across phenotypes and models strengthen the validity of our conclusions.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study involved collecting and analyzing sensitive personal data, including genetic information and detailed skin condition measurements. Due to privacy and ethical considerations, such data is typically not shared publicly. Participants were assured of the confidentiality of their information, and the study was conducted in accordance with the Korean public institutional review board guidelines. Therefore, the raw evaluation files remain restricted and are not accessible to the public."
}