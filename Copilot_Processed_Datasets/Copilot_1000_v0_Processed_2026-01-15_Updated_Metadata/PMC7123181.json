{
  "publication/title": "Improved prediction of MHC class I binders/non-binders peptides through artificial neural network using variable learning rate: SARS corona virus, a case study.",
  "publication/authors": "Soam SS, Bhasker B, Mishra BN",
  "publication/journal": "Advances in experimental medicine and biology",
  "publication/year": "2011",
  "publication/pmid": "21431562",
  "publication/pmcid": "PMC7123181",
  "publication/doi": "10.1007/978-1-4419-7046-6_22",
  "publication/tags": "- Variable learning rate\n- Artificial neural network\n- SARS Corona virus\n- MHC class I binder/non-binder\n- Epitope prediction\n- Vaccine designing\n- T-cell immune response\n- Machine learning\n- Predictive performance\n- ROC analysis\n- Cross-validation\n- Data resources\n- Computational challenge\n- Adaptive immune response\n- MHC peptide binding preference",
  "dataset/provenance": "The dataset used in this study was obtained from the IEDB Beta 2.0 database for various MHC class I alleles, specifically HLA-A*0201, HLA-A*0301, HLA-A*1101, HLA-A*0202, HLA-A*0203, HLA-A*0206, HLA-A*2902, HLA-A*3002, and HLA-B*4002. The dataset consists of binders and non-binders for these alleles, with strong binders retrieved for IC50 < 500 and strong non-binders for IC50 > 5000. All 9-mer sequences were filtered after removing duplicates. To maintain a nearly 1:1 ratio of binders to non-binders and reduce bias in learning, additional 9-mer non-binders were retrieved through the EBI-Expasy protein database. This resulted in a total of 90 datasets, with nine different MHC class I alleles each undergoing tenfold cross-validation. The final sets of binders and non-binders for various alleles are detailed in the study.",
  "dataset/splits": "In our study, we utilized tenfold cross-validation to ensure robust and reliable results. This method involves splitting the dataset into ten subsets of nearly equal size. The artificial neural network (ANN) was trained ten times for each MHC allele, each time leaving out one of the subsets for prediction. This process was repeated for each subset, ensuring that every data point was used for both training and validation.\n\nFor each MHC allele, we had a dataset of binders and non-binders. The binders were defined as peptides with an inhibitory concentration (IC50) of less than 500, while non-binders had an IC50 greater than 5,000. The datasets were carefully curated to maintain a nearly 1:1 ratio of binders to non-binders, which helps in reducing bias during the learning process.\n\nThe specific MHC alleles studied included HLA-A*0201, HLA-A*0301, HLA-A*1101, HLA-A*0202, HLA-A*0203, HLA-A*0206, HLA-A*2902, HLA-A*3002, and HLA-B*4002. For each allele, the datasets were split into ten folds, each containing a similar number of binders and non-binders. This approach allowed us to evaluate the performance of our ANN model comprehensively across different subsets of the data.\n\nThe distribution of data points in each fold was designed to be as balanced as possible, with each fold containing a representative sample of the overall dataset. This balance is crucial for ensuring that the model's predictions are generalizable and not overly influenced by any particular subset of the data. The use of tenfold cross-validation provides a robust measure of the model's performance, as it allows for multiple evaluations of the model's predictive accuracy.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The datasets used for training and testing were obtained from the IEDB Beta 2.0 database for specific MHC Class I alleles. These datasets include strong binders with IC50 values less than 500 and strong non-binders with IC50 values greater than 5000. All 9-mer sequences were filtered to remove duplicates, ensuring a clean and unbiased dataset. To maintain a nearly 1:1 ratio of binders to non-binders, additional 9-mer non-binders were retrieved from the EBI-Expasy protein database. This approach helps in reducing bias in the learning process. The final sets of binders and non-binders for various alleles are detailed in the provided tables. The datasets are not publicly released in a forum, but the sources from which they were obtained are specified. The IEDB Beta 2.0 database and the EBI-Expasy protein database are publicly accessible, and the data can be retrieved following their respective usage policies and licenses.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is artificial neural networks (ANN). Specifically, a backpropagation network is employed, which learns using a method of gradient descent to minimize the mean squared distance between the network\u2019s class prediction and the actual class label of the samples.\n\nThe optimization algorithm used is not entirely new; it builds upon existing backpropagation techniques. However, the novelty lies in the implementation of a variable learning rate. This approach adjusts the learning rate dynamically after each input vector in a given training set, which helps in improving convergence, especially for small datasets. The variable learning rate is updated based on the error calculated after each training vector, increasing the learning rate if the error decreases and decreasing it geometrically if the error increases. This adaptation aims to avoid getting stuck at local minima and encourages finding the global minimum, thereby enhancing the overall prediction accuracy.\n\nThe reason this algorithm was not published in a machine-learning journal is that the primary focus of this study is on its application in bioinformatics, specifically in predicting MHC class I binders and non-binders for the SARS coronavirus. The study demonstrates the effectiveness of the variable learning rate approach in improving the prediction quality for small datasets, which is crucial in biological systems where data can be limited. The algorithm's implementation and evaluation are tailored to address specific challenges in epitope prediction and vaccine design, making it more relevant to biological and medical research journals.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, the data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithm. We focused on preparing datasets for various MHC class I alleles, specifically for the SARS Corona virus. The datasets were obtained from the IEDB Beta 2.0 database, which provided strong binders with IC50 values less than 500 and strong non-binders with IC50 values greater than 5,000. All 9-mer peptides were filtered to remove duplicates, ensuring a clean and unbiased dataset.\n\nTo maintain a balanced ratio of binders to non-binders, we retrieved additional 9-mer non-binders from the EBI-Expasy protein database. This step was essential to reduce bias in the learning process and to ensure that the model could generalize well. The final sets of binders and non-binders for each allele were carefully curated to achieve a nearly 1:1 ratio, which is important for training robust machine-learning models.\n\nThe preprocessing involved tenfold cross-validation, where the artificial neural network (ANN) was trained ten times for each MHC allele. Each training session left out one of the ten subsets, using the left-out subset for prediction. This method helped in validating the results and ensuring that the model's performance was consistent across different subsets of the data.\n\nThe ANN was trained using a variable learning rate, which was adjusted after each input vector in the training set. This approach improved convergence, especially for small datasets, by avoiding local minima and encouraging the finding of the global minimum. The learning rate was increased if the error on the subsequent training vector decreased and decreased geometrically if the error increased. This dynamic adjustment of the learning rate enhanced the model's ability to learn from the data effectively.\n\nOverall, the data encoding and preprocessing steps were designed to create a balanced and robust dataset, which was essential for training an effective ANN model for predicting MHC class I binders and non-binders.",
  "optimization/parameters": "In our study, the model utilizes several key parameters to optimize the prediction of MHC class I binders and non-binders. The primary parameters include the learning rate (L), which is adjusted dynamically during training, and coefficients (a and b) that govern the changes in the learning rate. The learning rate values range from 0.48 to 0.67, while the coefficients a and b vary between 0.3 to 0.51 and 0.0117 to 0.0725, respectively.\n\nThe selection of these parameters was driven by the need to enhance the model's convergence and accuracy, especially for small datasets. The variable learning rate approach was chosen to avoid the pitfalls of a fixed learning rate, which often leads to poor convergence. By adjusting the learning rate after each input vector, the model can better navigate the decision space, avoiding local minima and oscillatory behavior. This adaptive mechanism ensures that the model learns more efficiently, leading to improved prediction performance.\n\nAdditionally, the model incorporates other parameters such as weights (wij) and biases (\u03b8j) for the connections between units in the neural network. These parameters are updated iteratively based on the error calculations at the output and hidden layers. The error terms (Errj) are crucial for backpropagation, guiding the adjustment of weights and biases to minimize the difference between predicted and actual outputs.\n\nThe use of tenfold cross-validation further ensures that the model's performance is robust and generalizable. By training the artificial neural network ten times for each MHC allele, each time leaving out one subset for prediction, we can reliably evaluate the model's predictive accuracy. This rigorous validation process helps in fine-tuning the parameters and ensuring that the model performs well across different datasets.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The fitting method employed in this study involves training an artificial neural network (ANN) with a variable learning rate to predict MHC class I binders and non-binders. The ANN was trained using datasets for various MHC alleles, with the learning rate adjusted dynamically during the training process. This approach helps in avoiding local minima and ensures better convergence, especially for small datasets.\n\nThe number of parameters in the ANN, which include weights and biases, is indeed larger than the number of training points in some cases. To address the potential issue of overfitting, a variable learning rate was used. This method updates the learning rate after each input vector, increasing it if the error decreases and decreasing it if the error increases. This adaptive learning rate helps in fine-tuning the model parameters more effectively, reducing the risk of overfitting.\n\nAdditionally, tenfold cross-validation was employed to validate the results. This technique involves dividing the dataset into ten subsets, training the model on nine subsets, and testing it on the remaining one. This process is repeated ten times, with each subset used as the test set once. This method ensures that the model generalizes well to unseen data, further mitigating the risk of overfitting.\n\nTo rule out underfitting, the model's performance was evaluated using several metrics, including sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). The average sensitivity was found to be 91.08%, indicating that a high percentage of binders were correctly predicted. The average specificity was 82.85%, showing that a significant portion of non-binders were also correctly identified. The average PPV was 85.50%, and the average NPV was 89.47%, demonstrating the model's reliability in predicting both binders and non-binders.\n\nThe area under the receiver operating characteristic (ROC) curve (AROC) was used to measure the overall prediction accuracy. For most MHC alleles, the AROC values were above 0.8, indicating good to excellent prediction performance. This comprehensive evaluation ensures that the model is neither overfitting nor underfitting the data.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, the learning rate (L) was varied between 0.48 and 0.67, coefficient a between 0.3 and 0.51, and coefficient b between 0.0117 and 0.0725. These parameters were crucial for training the artificial neural network (ANN) to predict MHC class I binders and non-binders.\n\nThe optimization schedule involved using a variable learning rate, which was adjusted after each input vector in the training set. This approach helped in improving the convergence of the ANN, especially for small datasets. The error at each node was calculated, and the learning rate was increased or decreased based on whether the error decreased or increased, respectively.\n\nModel files and specific optimization parameters are not explicitly provided in the publication. However, the methodology and the equations used for updating weights, biases, and the learning rate are detailed. This includes the formulas for calculating the change in weights (\u0394wij), change in bias (\u0394\u03b8j), and the update rules for the learning rate (\u0394L).\n\nThe implementation of the training, classification, and results modules was done in C using pointers to enhance efficiency. This code is not publicly available, and there is no mention of a specific license for the code or the data used.\n\nFor those interested in replicating the study, the detailed equations and the range of hyper-parameters provided should be sufficient to implement a similar approach. Additionally, the datasets used for training and testing were obtained from the IEDB Beta 2.0 database and the EBI-Expasy protein database, which are publicly accessible.",
  "model/interpretability": "The model presented in this study is primarily an artificial neural network (ANN) used for predicting MHC class I binders and non-binders. ANNs are generally considered black-box models due to their complex, multilayered structure, which makes it challenging to interpret the internal workings and decision-making processes directly.\n\nThe transparency of the model is limited by the nature of neural networks. While the inputs and outputs are clear\u2014inputs consist of peptide sequences and outputs are predictions of binding affinity\u2014the intermediate layers and the weights connecting neurons are not easily interpretable. This lack of transparency is a common characteristic of deep learning models, including ANNs.\n\nHowever, the model's performance can be evaluated through various metrics, such as the area under the ROC curve (AROC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). These metrics provide a quantitative measure of the model's predictive accuracy and reliability. For instance, the AROC values for most datasets fall between 0.806 and 1.0, indicating excellent predictive performance.\n\nAdditionally, the use of tenfold cross-validation ensures that the model's performance is robust and generalizable. This technique involves training the model on nine subsets of the data and testing it on the remaining subset, repeating this process ten times with different subsets. This approach helps to validate the model's predictions and reduces the risk of overfitting.\n\nIn summary, while the ANN model itself is a black-box, its performance and reliability can be assessed through rigorous evaluation metrics and validation techniques. This ensures that despite the lack of transparency in the model's internal workings, its predictions are accurate and trustworthy.",
  "model/output": "The model is a classification model designed to predict MHC class I binders and non-binders. It utilizes an artificial neural network (ANN) with a variable learning rate to improve prediction accuracy, particularly for small datasets. The model's performance is evaluated using receiver operating characteristics (ROC) analysis, with the area under the ROC curve (AROC) serving as a key metric for overall prediction accuracy. The model aims to classify peptides as either binders or non-binders, making it a classification model rather than a regression model. The output of the model includes predictions for each peptide, indicating whether it is likely to be a binder or a non-binder based on the training data and the variable learning rate approach.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not applicable",
  "evaluation/method": "The evaluation of the method involved a comprehensive approach using tenfold cross-validation to ensure the robustness and reliability of the predictions. This technique was employed to validate the results by training the artificial neural network (ANN) ten times for each MHC allele. During each iteration, one of the ten subsets was left out for prediction, while the remaining nine subsets were used for training. This process was repeated ten times, with each subset being used once as the test set.\n\nThe predictive performance of the model was assessed using receiver operating characteristic (ROC) analysis. The area under the ROC curve (AROC) served as a key metric for evaluating the overall prediction accuracy. An AROC value greater than 90% was considered excellent, indicating high prediction accuracy. The ROC curve was generated by plotting sensitivity (the percentage of correctly predicted binders) against 1-specificity (the percentage of incorrectly predicted non-binders).\n\nAdditionally, other evaluation parameters such as sensitivity, specificity, accuracy, positive predictive value (PPV), and negative predictive value (NPV) were calculated for various MHC alleles. Sensitivity measures the proportion of true binders correctly identified, while specificity measures the proportion of true non-binders correctly identified. PPV indicates the probability that a predicted binder is actually a binder, and NPV indicates the probability that a predicted non-binder is actually a non-binder.\n\nThe datasets used for training and testing were obtained from the IEDB Beta 2.0 database for specific MHC class I alleles. Strong binders with an inhibitory concentration (IC50) of less than 500 were retrieved, and all 9-mer peptides were filtered after removing duplicates. For strong non-binders, records with an IC50 greater than 5000 were retrieved, and duplicates were removed from both binder and non-binder sets. To maintain a balanced ratio of binders to non-binders, additional 9-mer non-binders were retrieved from the EBI-Expasy protein database.\n\nThe results demonstrated that the use of a variable learning rate significantly improved the prediction accuracy compared to a fixed learning rate. For the variable learning rate approach, the AROC values for most datasets were between 0.806 and 1.0, indicating very good performance. In contrast, the fixed learning rate approach yielded poor predictions, with AROC values mostly around 0.5.\n\nOverall, the evaluation method provided a rigorous assessment of the ANN's predictive performance, highlighting the effectiveness of using a variable learning rate for training on small datasets.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the predictive capability of our model for MHC class I binders and non-binders. The primary metric used was the area under the receiver operating characteristic curve (AROC). This metric provides a comprehensive measure of overall prediction accuracy, with AROC values categorized as follows: less than 70% indicates poor prediction, between 70% and 80% suggests fair prediction, between 80% and 90% denotes good prediction, and above 90% signifies excellent prediction.\n\nIn addition to AROC, we reported sensitivity (SN) and specificity (SP). Sensitivity measures the percentage of correctly predicted binders, while specificity measures the percentage of correctly predicted non-binders. These metrics are crucial for understanding the model's ability to accurately identify true positives and true negatives.\n\nWe also calculated the positive predictive value (PPV) and negative predictive value (NPV). PPV indicates the probability that a predicted binder is actually a binder, while NPV indicates the probability that a predicted non-binder is actually a non-binder. These metrics are essential for assessing the reliability of the model's predictions in practical applications.\n\nThe reported average values for these metrics across various MHC alleles were as follows: sensitivity was 91.08%, specificity was 82.85%, PPV was 85.50%, and NPV was 89.47%. These values demonstrate the model's strong performance in predicting both binders and non-binders.\n\nThe use of tenfold cross-validation ensured that our results were robust and not dependent on a particular subset of the data. This approach involved training the model ten times, each time leaving out one of the ten subsets, and using the left-out subset for prediction. The average AROC values for different MHC alleles ranged from 0.82 to 0.9615, indicating excellent predictive performance for most cases.\n\nOverall, the set of metrics reported in our study is representative of standard practices in the field, providing a comprehensive evaluation of the model's predictive accuracy and reliability.",
  "evaluation/comparison": "Not applicable.",
  "evaluation/confidence": "Evaluation Confidence\n\nThe evaluation of the model's predictive performance was conducted using receiver operating characteristics (ROC) analysis, which provides a comprehensive measure of overall prediction accuracy. The area under the ROC curve (AROC) was used as the primary metric, with specific thresholds to categorize the prediction quality: AROC < 70% for poor, AROC > 80% for good, and AROC > 90% for excellent prediction. This approach ensures that the model's performance is assessed rigorously.\n\nThe study employed tenfold cross-validation, a robust technique that involves training the model ten times, each time leaving out one of the ten subsets and using it for prediction. This method helps in validating the results and ensuring that the model generalizes well to unseen data. The average AROC values for various MHC alleles were reported, indicating excellent predictions for most cases. For instance, the average AROC for HLA-A*0201 was 0.9485, demonstrating high predictive accuracy.\n\nAdditionally, the sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) were calculated to provide a detailed evaluation of the model's performance. Sensitivity, which measures the percentage of correctly predicted binders, averaged 91.08%, indicating that almost all potential binders were included in the predicted results. Specificity, which measures the percentage of correctly predicted non-binders, averaged 82.85%. The PPV, which indicates the probability that a predicted binder will actually be a binder, averaged 85.50%. The NPV, which indicates the probability that a predicted non-binder will actually be a non-binder, averaged 89.47%.\n\nThe use of a variable learning rate in the artificial neural network (ANN) training process significantly improved the model's performance compared to a fixed learning rate. The fixed learning rate approach yielded poor predictions with an AROC of 0.5, whereas the variable learning rate approach resulted in AROC values between 0.806 and 1.0 for 76 out of 90 datasets, indicating very good performance in most cases.\n\nThe statistical significance of the results was not explicitly mentioned, but the consistent high performance across multiple datasets and the use of tenfold cross-validation suggest that the findings are reliable. The model's performance metrics, such as AROC, sensitivity, specificity, PPV, and NPV, provide a comprehensive evaluation of its predictive accuracy and reliability. The use of a variable learning rate and the detailed evaluation metrics contribute to the confidence in the model's superiority over other methods and baselines.",
  "evaluation/availability": "Not applicable"
}