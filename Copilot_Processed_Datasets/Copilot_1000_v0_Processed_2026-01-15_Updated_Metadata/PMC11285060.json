{
  "publication/title": "pKalculator: A p<i>K</i> <sub>a</sub> predictor for C-H bonds.",
  "publication/authors": "Borup RM, Ree N, Jensen JH",
  "publication/journal": "Beilstein journal of organic chemistry",
  "publication/year": "2024",
  "publication/pmid": "39076289",
  "publication/pmcid": "PMC11285060",
  "publication/doi": "10.3762/bjoc.20.144",
  "publication/tags": "- pKa prediction\n- C\u2013H bonds\n- Machine learning\n- Gradient boosting\n- LightGBM\n- Regression models\n- Chemical properties\n- Organic chemistry\n- Data splitting\n- Cross-validation\n- Performance metrics\n- Binary classification\n- pKalculator",
  "dataset/provenance": "The dataset used in our study comprises 775 compounds with 3910 pKa values. These values were compiled from two primary sources: the Bordwell dataset and the iBonD database. The Bordwell dataset includes experimental C\u2013H pKa values in DMSO for 419 molecules. The iBonD database provided experimental C\u2013H pKa values in DMSO for 313 molecules, although these were initially in image format. To convert these images into SMILES strings, we utilized the \"Deep Learning for Chemical Image Recognition\" software (DECIMER v. 2.0), developed by Rajan and co-workers. Manual intervention was necessary to ensure the accuracy of the SMILES strings.\n\nAdditionally, to align with the dataset used by Roszak et al., we incorporated 43 heterocycles without experimental pKa values from Shen et al. This integration resulted in our final dataset of 775 compounds. This dataset was used to calculate quantum chemistry (QM) pKa values using our QM workflow.\n\nWe also created a separate dataset from Reaxys, which contains 1043 pKa-controlled reactions. This dataset includes 584 aldol, 408 Claisen, and 51 Michael reactions. This Reaxys dataset was used as an out-of-sample dataset to evaluate how well our machine learning (ML) model predicts reaction sites. Furthermore, we utilized six pharmaceutical intermediates that undergo selective borylation to compare our QM workflow and ML model with experimentally determined reaction sites.",
  "dataset/splits": "The dataset used in this study consists of 775 compounds with a total of 3910 pKa values. This dataset was randomly split into two primary sets: a training set and a held-out test set. The training set comprises 80% of the data, which translates to 620 compounds and 3121 pKa values. The held-out test set contains the remaining 20%, consisting of 155 compounds and 789 pKa values.\n\nWithin the training set, a fivefold randomly shuffled cross-validation (CV) was conducted. For each fold, the original training set was further split into a new training set, which includes 90% of the original training set, and a validation set, which includes the remaining 10%. This process ensures that the models are trained and validated on different subsets of the data, enhancing the robustness of the results.\n\nThe distribution of data points in each split is as follows:\n\n* Training set: 620 compounds, 3121 pKa values\n* Held-out test set: 155 compounds, 789 pKa values\n* Within each fold of the fivefold cross-validation:\n\t+ New training set: 90% of the original training set\n\t+ Validation set: 10% of the original training set",
  "dataset/redundancy": "The dataset used in this study consists of 775 compounds with a total of 3910 pKa values. To ensure robust model training and evaluation, the dataset was randomly split into a training set and a held-out test set. The training set comprises 80% of the data, which translates to 620 compounds and 3121 pKa values. The remaining 20% forms the held-out test set, consisting of 155 compounds and 789 pKa values.\n\nTo further validate the models, a fivefold randomly shuffled cross-validation (CV) was conducted within the training set. In each fold, the original training set was randomly split into a new training set, which contains 90% of the original training set, and a validation set, which contains the remaining 10%. This process allows for the evaluation of different models and the estimation of their performance.\n\nThe independence of the training and test sets is crucial for the reliability of the model's performance metrics. This independence was enforced by ensuring that the held-out test set was not used during the training or validation phases. Instead, the models were trained on the original training set and evaluated against the held-out test set only after the training process was complete.\n\nComparing the distribution of this dataset to previously published machine learning datasets, it is evident that the approach taken here is consistent with standard practices in the field. The use of a held-out test set and cross-validation ensures that the models are not overfitted to the training data and that their performance can be generalized to new, unseen data. This methodology provides a robust framework for evaluating the predictive accuracy of the models developed in this study.",
  "dataset/availability": "The data used in this study is not publicly released. The dataset consists of 775 compounds with 3910 pKa values. It was randomly split into a training set (80%; 620 compounds; 3121 pKa values) and a held-out test set (20%; 155 compounds; 789 pKa values). This split was enforced by randomly assigning compounds to either the training or test set, ensuring that the test set remained unseen during the model training process. The specific data splits are not made available to the public.",
  "optimization/algorithm": "The machine-learning algorithm class used is gradient boosting decision trees (GBDT) and dropout-based trees (DART), specifically implemented through the LightGBM framework. These algorithms are not new; they are well-established in the field of machine learning and are widely used for regression and classification tasks due to their efficiency and effectiveness.\n\nThe choice of LightGBM for this work is driven by its suitability for handling large datasets and its ability to provide high predictive performance. LightGBM is known for its speed and performance, making it a popular choice for various machine-learning applications. The decision to use LightGBM in this context is based on its proven track record in similar tasks rather than the introduction of a novel algorithm.\n\nGiven that LightGBM is a mature and widely recognized algorithm, it was not necessary to publish the details of the algorithm itself in a machine-learning journal. Instead, the focus of this work is on applying LightGBM to predict pKa values and identify the site of metabolism in molecules, which is a specific application in the domain of organic chemistry. The optimization and application of existing machine-learning algorithms to new problems are valuable contributions to the scientific community, even if the algorithms themselves are not novel.",
  "optimization/meta": "The meta-predictor described in the publication does not use data from other machine-learning algorithms as input. Instead, it relies on a specific set of atomic descriptors derived from CM5 atomic charges, which are computed using semiempirical tight-binding calculations. These descriptors are used to predict pKa values and identify the site with the lowest pKa value in a molecule.\n\nThe machine-learning models employed in this work include LightGBM regression and classification models. LightGBM is a gradient boosting framework that uses tree-based learning algorithms. Two specific configurations of LightGBM were used: Gradient Boosting Decision Tree (GBDT) and Dropouts meet Multiple Additive Regression Trees (DART). The DART configuration introduces dropout techniques to prevent overfitting.\n\nThe dataset used for training and testing these models is split into a training set (80% of the compounds) and a held-out test set (20% of the compounds). Within the training set, a fivefold randomly shuffled cross-validation is conducted. This means that the original training set is further split into a new training set (90% of the original training set) and a validation set (10% of the original training set) within each fold. This process ensures that the training data is independent for each fold, providing a robust evaluation of the model's performance.\n\nThe performance metrics for the different regression models are evaluated using mean absolute error (MAE) and root mean squared error (RMSE) for both the cross-validation and the held-out test set. The classification models are evaluated using accuracy (ACC), Matthew's correlation coefficient (MCC), precision (positive predictive value - PPV), recall (true-positive rate - TPR), specificity (true-negative rate - TNR), and negative predictive value (NPV).\n\nIn summary, the meta-predictor does not rely on other machine-learning algorithms as input. It uses LightGBM regression and classification models, with a clear separation of training and test data to ensure independence and robust performance evaluation.",
  "optimization/encoding": "For the machine-learning algorithm, the data encoding and preprocessing involved several key steps. Initially, atomic descriptors were generated using charge model 5 (CM5) atomic charges, which were computed from semiempirical tight-binding calculations. To enhance accuracy, 20 random conformers were produced from a SMILES string and optimized using molecular mechanics force fields. The CM5 atomic charges of the lowest-energy conformer were then used to create atomic descriptors by sorting the CM5 charges for a given atom.\n\nThe dataset, consisting of 775 compounds with 3910 pKa values, was split into a training set (80%) and a held-out test set (20%). Within each fold of the fivefold cross-validation, the original training set was further divided into a new training set (90%) and a validation set (10%). This approach allowed for the evaluation of different models and the estimation of their performance.\n\nFor the regression task, the target values were the QM-computed pKa values. In the binary classification task, which aimed to predict the site with the lowest QM-computed pKa value, labels were assigned as '1' for the lowest pKa value and '0' for all other values. A tolerance was introduced to account for slight variations, where a pKa value within +1 or +2 pKa units of the lowest value was also accepted as '1'. This was necessary due to the significant imbalance between the two classes, with '0's far outnumbering '1's. The hyperparameter scale_pos_weight was invoked during optimization to address this imbalance. Additionally, a \"null model\" was established for the classification task, where all sites were predicted as '0'.\n\nThe dataset was prepared using the Optuna framework for hyperparameter optimization, specifically employing the Bayesian optimization technique with the tree-structured Parzen estimator. This method was used to identify optimal hyperparameters for LightGBM regression and classification models. The performance of the models was evaluated using metrics such as mean absolute error (MAE) and root mean squared error (RMSE) for regression, and accuracy, Matthew's correlation coefficient, recall, precision, and negative predictive value for classification.",
  "optimization/parameters": "In our study, we utilized the Optuna framework for hyperparameter optimization of our LightGBM regression and classification models. The specific number of parameters (p) optimized can vary depending on the model and the task at hand. For instance, in the regression task, the target values were the QM-computed pKa values, while in the binary classification task, the labels were assigned based on the lowest QM-computed pKa value, with tolerances of +1 or +2 pKa units.\n\nThe selection of hyperparameters was conducted using Bayesian optimization with the tree-structured Parzen estimator. This method systematically explores the hyperparameter space to identify the optimal settings. Given the imbalance between the classes in the binary classification task, the hyperparameter scale_pos_weight was also invoked to address this issue.\n\nThe dataset was split into a training set (80% of the compounds) and a held-out test set (20% of the compounds). Within the training set, a fivefold randomly shuffled cross-validation was performed, further splitting the data into new training and validation sets. This approach allowed for a thorough evaluation of different models and the estimation of their performance.\n\nThe best-performing model was then selected based on its performance on the held-out test set. This rigorous optimization process ensured that the models were well-tuned to predict pKa values accurately.",
  "optimization/features": "The input features used in our study are atomic descriptors derived from CM5 atomic charges. These descriptors are computed using an automated approach that involves generating multiple random conformers from a SMILES string and optimizing the structure with molecular mechanics force fields. The CM5 atomic charges of the lowest-energy conformer are then used to generate the atomic descriptors. The shell radius for these descriptors was adjusted from 5 to 6 to improve the performance of the machine learning models in predicting pKa values.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the descriptors were carefully chosen based on established methodologies and modified to enhance their effectiveness. The process of generating and selecting these descriptors was conducted using the training set only, ensuring that the validation and test sets remained unbiased. This approach allowed us to focus on the most relevant atomic features for predicting pKa values accurately.",
  "optimization/fitting": "The fitting method employed in this study involved a careful balance to avoid both overfitting and underfitting. The dataset consisted of 775 compounds with 3910 pKa values, which were split into a training set (80%) and a held-out test set (20%). Within each fold of the fivefold cross-validation, the original training set was further divided into a new training set (90%) and a validation set (10%). This approach ensured that the model was evaluated on unseen data, helping to mitigate overfitting.\n\nTo address the potential issue of overfitting, given the relatively large number of parameters compared to the number of training points, several strategies were implemented. Firstly, the use of cross-validation provided a robust estimate of model performance, ensuring that the model generalized well to new data. Secondly, hyperparameter optimization was conducted using the Optuna framework, which employed Bayesian optimization techniques to explore the hyperparameter space efficiently. This process helped in finding the optimal set of hyperparameters that minimized the risk of overfitting.\n\nAdditionally, the model's performance was evaluated using multiple metrics, including mean absolute error (MAE) and root mean squared error (RMSE), both for the cross-validated training and validation sets, as well as the held-out test set. This comprehensive evaluation ensured that the model's performance was consistent across different datasets, further reducing the likelihood of overfitting.\n\nTo prevent underfitting, the model's complexity was carefully tuned. The LightGBM regression model, known for its efficiency and effectiveness in handling large datasets, was used. The hyperparameter optimization process ensured that the model was neither too simple nor too complex, striking a balance that allowed it to capture the underlying patterns in the data without being overly simplistic.\n\nIn summary, the fitting method involved a rigorous cross-validation process, hyperparameter optimization, and thorough performance evaluation to ensure that the model neither overfitted nor underfitted the data. This approach resulted in a robust model capable of accurately predicting pKa values.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One key method involved the use of cross-validation. Specifically, we conducted a fivefold randomly shuffled cross-validation. This process involved splitting the original training set into a new training set (90% of the original training set) and a validation set (10% of the original training set) within each fold. This approach allowed us to evaluate the performance of different models and estimate their generalization capabilities more accurately.\n\nAdditionally, we utilized the Optuna framework for hyperparameter optimization. This framework employs Bayesian optimization with a tree-structured Parzen estimator to explore the hyperparameter space effectively. By optimizing hyperparameters, we aimed to find the best configuration that minimizes overfitting and maximizes model performance.\n\nFurthermore, we addressed class imbalance in our binary classification task. Given that the class '0' significantly outnumbered the class '1', we invoked the hyperparameter scale_pos_weight during optimization. This adjustment helped the model to better handle the imbalance and improve its predictive accuracy for the minority class.\n\nAnother important aspect was the establishment of a \"null model\" for the classification task, where all sites were predicted as '0'. This baseline model provided a reference point to evaluate the performance of our machine learning models.\n\nBy combining these techniques, we aimed to build models that generalize well to unseen data and avoid overfitting to the training set.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available and have been detailed in the supporting information files. Specifically, the Optuna framework was employed for hyperparameter optimization, utilizing a Bayesian optimization technique with a tree-structured Parzen estimator. The details of this process, including the specific hyperparameters tuned and the optimization schedule, can be found in Supporting Information File 1, section \u201cMachine learning models\u201d.\n\nThe model files and optimization parameters are not explicitly provided in the main text or supporting information. However, the methodology and the steps taken to compute CM5 atomic charges and generate atomic descriptors are thoroughly described. This includes the use of semiempirical tight-binding (GFN1-xTB) calculations and molecular mechanics force fields with RDKit for structure optimization. The modifications made to enhance the accuracy of the computed CM5 atomic charges, such as generating multiple conformers and adjusting the shell radius, are also detailed.\n\nRegarding the availability and licensing of the data and methods, the supporting information files provide comprehensive details on the datasets used, including the split between training and test sets. The datasets consist of 775 compounds with 3910 pKa values, randomly split into a training set (80%) and a held-out test set (20%). The performance metrics for different regression and classification models are also reported in the supporting information files.\n\nFor those interested in replicating or building upon our work, the supporting information files offer a clear roadmap of the procedures and optimizations undertaken. However, specific model files and optimization parameters are not directly accessible. Interested researchers are encouraged to refer to the detailed methodology and supporting information for guidance on implementing similar approaches.",
  "model/interpretability": "The models employed in our study, specifically the LightGBM variants, are gradient boosting decision tree models. These models are generally considered to be interpretable to a certain extent, but they are not entirely transparent. Gradient boosting decision trees build an ensemble of decision trees, where each tree corrects the errors of the previous ones. This structure allows for some level of interpretability, as individual trees can be examined to understand the decision-making process.\n\nOne of the key advantages of LightGBM is its ability to provide feature importance scores, which indicate the relative importance of each feature in making predictions. This can help in understanding which molecular descriptors or features are most influential in determining the pKa values. Additionally, the model can generate SHAP (SHapley Additive exPlanations) values, which provide a way to interpret the output of any machine learning model. SHAP values assign each feature an importance value for a particular prediction, making it easier to understand the contribution of each feature.\n\nFor example, in our binary classification tasks, the models can identify which pKa sites are most likely to be the lowest pKa site in a molecule. By examining the feature importance scores and SHAP values, one can gain insights into the molecular properties that are most predictive of these sites. This interpretability is crucial for chemists and researchers who need to understand the underlying mechanisms driving the model's predictions.\n\nHowever, it is important to note that while these models offer some level of interpretability, they are not entirely transparent. The ensemble nature of gradient boosting decision trees means that the final prediction is the result of many interacting decision trees, which can make the overall model somewhat complex to interpret fully. Nonetheless, the tools and methods available, such as feature importance scores and SHAP values, provide valuable insights into the model's decision-making process.",
  "model/output": "The model discussed in this publication is primarily a regression model, but it has also been evaluated as a binary classifier. The regression models were trained to predict pKa values, with various performance metrics such as mean absolute error (MAE) and root mean squared error (RMSE) reported for different methods like LightGBM GBDT and LightGBM DART. Additionally, the best regression model was used as a binary classifier to identify the lowest pKa site in molecules, with performance metrics like accuracy (ACC), Matthews correlation coefficient (MCC), precision (PPV), true positive rate (TPR), true negative rate (TNR), and negative predictive value (NPV) provided for different classification scenarios. The model's performance was assessed on a held-out test set comprising 20% of the dataset, which included 155 compounds and 789 pKa values. The evaluation also considered different thresholds for classifying pKa sites, such as accepting +1 or +2 pKa units of the lowest pKa value as the true site. The results indicate that the model performs well in both regression and binary classification tasks, with the best-performing models achieving high accuracy and other performance metrics.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models and algorithms used in this study is not publicly released. However, the RDKit library, which was utilized in the development process, is available under the BSD 3-Clause License. The specific version used is the RDKit 2022_09_4 (Q3 2022) Release, and it can be accessed via Zenodo with the DOI 10.5281/zenodo.7541264.\n\nNo executable, web server, virtual machine, or container instance is provided for running the algorithms directly. The focus of the publication is on the methodology and results obtained from the models, rather than the distribution of the software tools themselves.",
  "evaluation/method": "The evaluation of the methods involved a comprehensive approach using a dataset comprising 775 compounds with 3910 pKa values. The dataset was randomly split into a training set (80%, 620 compounds, 3121 pKa values) and a held-out test set (20%, 155 compounds, 789 pKa values). A fivefold randomly shuffled cross-validation (CV) was conducted within the training set. For each fold, the original training set was further split into a new training set (90% of the original training set) and a validation set (10% of the original training set). This process allowed for the evaluation of different models and the estimation of their performance.\n\nFor regression models, performance metrics included mean absolute error (MAE) and root mean squared error (RMSE). These metrics were calculated for both the cross-validation process and the held-out test set. The regression models were trained on the original training set and tested against the held-out test set to assess their predictive accuracy.\n\nIn binary classification, the dataset was similarly split, and a fivefold cross-validation was performed. The classification models aimed to predict the site of reaction by identifying the lowest pKa value in a molecule. A tolerance was introduced where pKa values within +1 or +2 units of the lowest pKa value were accepted as true sites. Performance metrics for binary classification included accuracy (ACC), Matthew's correlation coefficient (MCC), precision (positive predictive value - PPV), recall/sensitivity (true-positive rate - TPR), specificity (true-negative rate - TNR), and negative predictive value (NPV). These metrics were evaluated for both the training and validation sets, as well as the held-out test set.\n\nThe evaluation process ensured that the models were robust and generalizable by testing them on unseen data. The use of cross-validation and a held-out test set provided a thorough assessment of the models' performance, ensuring that they could accurately predict pKa values and identify reaction sites.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report a comprehensive set of metrics to evaluate the performance of our regression and binary classification models. For regression models, we use mean absolute error (MAE) and root mean squared error (RMSE) to assess the accuracy of pKa value predictions. These metrics are calculated for both fivefold cross-validation and the held-out test set, providing a robust evaluation of model performance.\n\nFor binary classification models, we employ several key metrics: accuracy (ACC), Matthew's correlation coefficient (MCC), precision (positive predictive value, PPV), recall/sensitivity (true positive rate, TPR), specificity (true negative rate, TNR), and negative predictive value (NPV). These metrics are derived from the confusion matrix, which compares the model's predictions to the calculated sites. The confusion matrix categorizes predictions as true positives (TP), true negatives (TN), false positives (FP), or false negatives (FN), enabling the calculation of the aforementioned evaluation parameters.\n\nThe reported metrics are representative of standard practices in the literature, ensuring that our evaluation is both thorough and comparable to other studies in the field. By including a diverse range of metrics, we aim to provide a holistic view of model performance, capturing various aspects of predictive accuracy and reliability. This approach allows for a nuanced understanding of how well our models generalize to new data and perform under different conditions.",
  "evaluation/comparison": "In the evaluation of our methods, we conducted a thorough comparison with publicly available methods using benchmark datasets. Specifically, we utilized the Bordwell dataset, which comprises 419 compounds, to assess the performance of various quantum mechanical (QM) methods. These methods included single-point calculations and re-optimizations based on GFN2-xTB, employing ORCA with DMSO as the solvent. The composite electronic structure method r2SCAN-3c, combined with its custom basis set and the universal solvation model (SMD), was also evaluated. Additionally, we performed calculations using density functional theory (DFT) with the CAM-B3LYP functional, the def2-TZVPPD basis set, and the CPCM implicit solvation model. Our findings indicated that the CAM-B3LYP D4 functional showed the best performance, aligning with previous benchmark studies that suggested long-range-corrected functionals with dispersion corrections excel in computing anions.\n\nFurthermore, we compared our models to simpler baselines, such as null models, to establish a performance benchmark. For binary classification tasks, we set all sites to '0' in the null models, providing a baseline against which to measure the improvement offered by our machine learning models. This comparison was conducted on multiple datasets, including a primary dataset of 775 compounds with 3910 pKa values and the Reaxys dataset, which comprises 1043 reactions. The performance metrics, such as accuracy (ACC), Matthews correlation coefficient (MCC), precision (PPV), true positive rate (TPR), true negative rate (TNR), and negative predictive value (NPV), were evaluated for different binary classification models, including the best regression models. The results demonstrated that our models significantly outperformed the null models, highlighting their effectiveness in predicting the lowest pKa sites in molecules.",
  "evaluation/confidence": "The evaluation of our models includes several performance metrics, and we have taken steps to ensure the reliability and statistical significance of our results.\n\nFor regression models, we report mean absolute error (MAE) and root mean squared error (RMSE) with confidence intervals derived from fivefold cross-validation. This approach provides a measure of the variability and reliability of our predictions. The reported metrics include the mean and standard deviation of these errors across the folds, giving an indication of the confidence in our model's performance.\n\nIn binary classification tasks, we evaluate models using accuracy (ACC), Matthew's correlation coefficient (MCC), precision (positive predictive value - PPV), recall (true positive rate - TPR), specificity (true negative rate - TNR), and negative predictive value (NPV). These metrics are calculated on both the training/validation sets and the held-out test set. The use of cross-validation ensures that our results are not dependent on a particular split of the data, providing a more robust estimate of model performance.\n\nTo assess statistical significance, we compare our models against null models and other baselines. The null models, where all sites are predicted as '0', serve as a simple baseline to highlight the improvement achieved by our machine learning models. The significant differences in performance metrics between our models and the null models indicate that our approaches are superior and not due to random chance.\n\nAdditionally, we evaluate models with different tolerances (+1 and +2 pKa units) to account for slight variations in the lowest pKa value. This flexibility in the evaluation criteria helps to ensure that our models are robust and can handle real-world variations in data.\n\nOverall, the inclusion of confidence intervals, cross-validation, and comparisons against baselines provides a comprehensive evaluation of our models' performance and statistical significance. This rigorous approach ensures that our claims of superiority are well-founded and reliable.",
  "evaluation/availability": "Not enough information is available."
}