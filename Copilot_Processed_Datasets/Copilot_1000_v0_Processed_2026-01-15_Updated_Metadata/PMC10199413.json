{
  "publication/title": "Comparison of the output of a deep learning segmentation model for locoregional breast cancer radiotherapy trained on 2 different datasets.",
  "publication/authors": "Bakx N, van der Sangen M, Theuws J, Bluemink H, Hurkmans C",
  "publication/journal": "Technical innovations & patient support in radiation oncology",
  "publication/year": "2023",
  "publication/pmid": "37213441",
  "publication/pmcid": "PMC10199413",
  "publication/doi": "10.1016/j.tipsro.2023.100209",
  "publication/tags": "- Radiation Oncology\n- Deep Learning\n- Medical Imaging\n- Breast Cancer\n- Delineation\n- Inter-Observer Variation\n- Quantitative Analysis\n- Model Validation\n- Clinical Guidelines\n- Segmentation Models",
  "dataset/provenance": "The dataset used in this study consists of 30 breast cancer patients, with 15 left-sided and 15 right-sided cases. These patients were treated for locally advanced breast cancer between January 2019 and February 2022. The delineation of target volumes was performed by radiation oncologists, while organs at risk were contoured by radiotherapy technologists, all following established guidelines. The dataset includes 11 regions of interest for both left and right sides, encompassing target volumes such as the breast, axillary lymph node levels, and supraclavicular lymph nodes, as well as organs at risk like the heart, lungs, esophagus, thyroid, and humeral head.\n\nThe models evaluated in this study were trained on different datasets from two institutions. The in-house model was trained using data from 160 patients, with varying numbers of delineations for different regions of interest. The external model was trained on a dataset from St. Olavs Hospital and \u00c5lesund Hospital in Norway for target structures and internally collected data from RaySearch Laboratories for organs at risk. This external model was trained on 170 left-sided patients, with most cases containing all regions of interest.\n\nThe data used for training followed specific guidelines for target structures and atlases for organs at risk delineations. The in-house models were trained separately for left- and right-sided breast cancer, resulting in two models, while the external model was a common model for both sides. This approach allowed for a comprehensive comparison of the performance of models trained on different datasets but evaluated on the same in-house collected dataset.",
  "dataset/splits": "The study utilized a dataset consisting of 30 breast cancer patients, with 15 left-sided and 15 right-sided cases. These patients were treated for locally advanced breast cancer between January 2019 and February 2022.\n\nThe dataset was used for the evaluation of deep learning models, which were trained on different datasets. The in-house models were trained using 160 patients in total, with 80 patients for each side. However, not all patients contained delineations of all regions of interest (ROIs). For instance, there were 82 thyroid and 147 lung delineations.\n\nThe external model was trained on a dataset containing 170 left-sided patients, with only a few cases that did not contain all ROIs.\n\nThe evaluation dataset consisted of 30 patients, with an equal distribution of left-sided and right-sided cases. This dataset was used to compare the performance of the in-house and external models against manual delineations. The models were evaluated using metrics such as the Dice similarity coefficient (DSC), surface DSC (sDSC), and the 95th percentile of the Hausdorff Distance (95% HD).",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is deep learning (DL), specifically utilizing an adapted version of the U-net architecture. This architecture is well-established in the field of medical image segmentation and is known for its effectiveness in capturing complex patterns in imaging data.\n\nThe algorithm itself is not entirely new, as U-net has been widely used and validated in various medical imaging applications. However, the specific adaptations and implementations used in this study may introduce novel aspects tailored to the segmentation of target volumes and organs at risk in breast cancer radiotherapy. The choice to publish in a radiation oncology journal rather than a machine-learning journal is likely due to the focus on the clinical applicability and validation of the DL models within the context of radiotherapy treatment planning. The primary goal is to demonstrate the practical benefits and clinical usefulness of these models in a real-world medical setting, rather than to introduce a entirely new machine-learning algorithm. This approach aligns with the journal's emphasis on technical innovations and patient support in radiation oncology.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the models could effectively learn from the datasets. The patient dataset consisted of 15 left-sided and 15 right-sided breast cancer patients, all treated for locally advanced breast cancer between January 2019 and February 2022. The contouring of target volumes was performed by radiation oncologists, while organs at risk were contoured by radiotherapy technologists, following established guidelines. All contours were reviewed and adjusted by an experienced radiation oncologist before being included in the evaluation.\n\nThe models were trained using a framework provided by RaySearch Laboratories AB in RayStation version 9B, with evaluation performed in version 10B-SP1. This framework utilized multiple sub-models, each based on an adapted version of the U-net architecture. The training data included both in-house collected datasets and external data from St. Olavs Hospital and \u00c5lesund Hospital in Norway for target structures, and internally collected data from RaySearch Laboratories for organs at risk. All data followed specific delineation guidelines for targets and atlases for organs at risk.\n\nThe preprocessing steps ensured that the data was consistent and aligned with the guidelines, which is crucial for the performance of deep learning models. The in-house models were trained on 160 patients, with varying numbers of delineations for different regions of interest. The external model was trained on 170 left-sided patients, with only a few cases missing some regions of interest. This comprehensive preprocessing and encoding process enabled the models to achieve high accuracy in delineating target volumes and organs at risk, as evidenced by the evaluation metrics.",
  "optimization/parameters": "In our study, the deep learning models utilized a framework provided by RaySearch Laboratories AB in RayStation, specifically versions 9B for training and 10B-SP1 for evaluation. The models were based on an adapted version of the U-net architecture, which is known for its efficiency in medical image segmentation tasks. The exact number of parameters (p) in the models was not explicitly stated, as the focus was on the comparative performance rather than the architectural details.\n\nThe selection of the U-net architecture was driven by its proven effectiveness in similar applications. This architecture was chosen for its ability to capture both local and global contextual information, which is crucial for accurate segmentation of medical images. The models were trained on datasets that followed established guidelines, ensuring consistency and reliability in the training process. The in-house models were trained using data from locally advanced breast cancer patients treated between January 2019 and February 2022, while the external model utilized data from St. Olavs Hospital and \u00c5lesund Hospital in Norway, along with internally collected data from RaySearch Laboratories. This approach ensured that the models were robust and generalizable across different datasets.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "Not applicable.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models evaluated in this study are based on a deep learning architecture adapted from U-net, which is known for its transparency in terms of how it processes input data to generate output contours. The models are not black-box systems; instead, they provide interpretable results that can be visually inspected and quantitatively analyzed.\n\nOne of the key aspects of interpretability in our models is the use of visual inspection to check for abnormalities and investigate the origin of differences between the automatically generated contours and manual delineations. For instance, figures illustrating the delineations of different structures by the in-house and external models, compared to manual delineations, help in understanding how the models interpret the input data. These visual comparisons reveal specific areas where the models differ from manual delineations, such as the cranial extent of the CTVn4 or the length of the esophagus. Such visual evidence aids in identifying and addressing discrepancies, making the models more transparent and interpretable.\n\nAdditionally, the use of quantitative metrics like the Dice similarity coefficient (DSC), surface DSC (sDSC), and the 95th percentile of the Hausdorff Distance (95% HD) provides a clear measure of the models' performance. These metrics allow for a detailed comparison between the automatically generated contours and manual delineations, as well as between the contours generated by the two models. The statistical analysis using the Wilcoxon signed rank test further enhances the interpretability by highlighting significant differences in performance.\n\nThe models' architecture and training process are also transparent. The framework used for training and evaluation is provided by RaySearch Laboratories AB in RayStation, and the models are based on an adapted version of U-net. The training datasets, delineation guidelines, and the number of patients used for training are well-documented, ensuring that the models' development process is clear and reproducible.\n\nIn summary, the models evaluated in this study are transparent and interpretable. Visual inspections, quantitative metrics, and detailed documentation of the training process contribute to the models' transparency, making it easier to understand their performance and identify areas for improvement.",
  "model/output": "The model discussed in this study is a deep learning (DL) architecture designed for segmentation, which is a type of classification task at the pixel or voxel level. It is specifically tailored for auto-segmentation in the context of locoregional breast cancer radiotherapy. The primary goal of this model is to automate the delineation of target volumes and organs at risk (OARs), a process that is traditionally time-consuming and prone to intra- and inter-observer variations.\n\nThe models evaluated consist of the same DL architecture but were trained on different datasets from two institutions. This approach allows for a comparison of the performance of an externally trained model versus an in-house trained model. The evaluation metrics used include the Dice similarity coefficient (DSC), surface DSC (sDSC), and the 95th percentile of the Hausdorff Distance (95% HD). These metrics provide a quantitative assessment of the overlap between the automatically generated contours and the manually delineated contours.\n\nThe study found statistically significant differences between the two models for several structures. However, these differences were generally within the range of reported inter-observer variations (IOV), suggesting that both models perform comparably to human observers. The sDSC metric, in particular, was introduced to better reflect the corrections needed by quantifying the deviation in contours rather than volumes. The in-house developed model showed higher sDSC values for most structures, indicating potential time savings in delineation.\n\nThe evaluation also highlighted the importance of thorough validation of DL models in the clinical setting. Differences in delineation guidelines and their interpretation were noted, particularly for the CTVn4 structure. This finding underscores the need for clear delineation guidelines and additional visual inspection beyond quantitative analysis. The study led to practical adjustments, such as revising in-house delineation guidelines for the esophagus and planning a multi-center study to further investigate inter-institute variations.\n\nIn summary, the model is a classification-based DL architecture for auto-segmentation in radiotherapy, with the primary aim of reducing delineation time and improving consistency. The comparison of in-house and externally trained models provides valuable insights into the performance and clinical applicability of such models.",
  "model/duration": "The execution time of the models was not explicitly detailed in the study. However, it was noted that both models resulted in a reduction of delineation time, which is the main outcome. This suggests that the models are efficient in terms of execution time, but specific durations were not provided. The study focused more on the qualitative and quantitative performance of the models rather than the exact execution time. The reduction in delineation time is a significant finding, indicating that the models can potentially speed up the clinical workflow. Further research on actual time saving is recommended to validate this hypothesis.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our study, the evaluation method involved a comprehensive quantitative analysis to compare the outcomes of the in-house and external models with manual delineations. We utilized several metrics to assess the performance of the models. The Dice similarity coefficient (DSC) and the 95th percentile of the Hausdorff Distance (95% HD) were measured to evaluate the overlap between the automatically and manually generated contours for both models. Additionally, the surface DSC (sDSC) with a tolerance of 3 mm was introduced to better reflect the corrections needed by quantifying the deviation in contours rather than volumes. This metric showed a stronger correlation with the time needed for correction and relative time saved compared to other quantitative metrics.\n\nThe Wilcoxon signed rank test was employed to test for differences between the two models when compared to manual delineations, with a p-value of \u2264 0.05 considered significant. Visual inspection was also performed to check for abnormalities and investigate the origin of found differences. Furthermore, the values for DSC and 95% HD were compared with inter-observer variations (IOV) present for manual delineations of these structures, taken from various studies performed after the ESTRO guidelines were published.\n\nThe evaluation was conducted on a dataset consisting of 15 left- and 15 right-sided, randomly selected breast cancer patients. The models were trained using a framework provided by RaySearch Laboratories AB in RayStation, with the in-house model trained on an in-house collected dataset and the external model trained on data from St. Olavs Hospital and \u00c5lesund Hospital in Norway for target structures and internally collected data of RaySearch Laboratories for organs at risk (OARs). The evaluation aimed to reflect the reality in which an institute might use an externally trained model, emphasizing the importance of thorough validation of the model's outcomes in the local clinic.",
  "evaluation/measure": "In our study, we employed several quantitative metrics to evaluate and compare the performance of two deep learning (DL) models for delineation tasks in radiation oncology. The primary metrics reported include the Dice Similarity Coefficient (DSC) score, the 95% Hausdorff Distance (95% HD), and the surface Dice Similarity Coefficient (sDSC) score.\n\nThe DSC score measures the overlap between the automatically generated contours and the manual delineations, providing a measure of volumetric agreement. This metric is widely used in the literature and offers a straightforward assessment of model performance. The 95% HD, on the other hand, evaluates the maximum distance between the surfaces of the contours, highlighting any discrepancies in the boundary definitions. This metric is crucial for identifying local errors that might not be captured by the DSC score alone.\n\nThe sDSC score, introduced by Nikolov et al., was included to better reflect the corrections needed by quantifying the deviation in contours rather than volumes. This metric showed a stronger correlation with the time needed for correction and the relative time saved, making it a valuable addition to our evaluation framework. The sDSC score was particularly useful in assessing the practical implications of the delineation errors, as it directly relates to the effort required for manual adjustments.\n\nThese metrics were chosen to provide a comprehensive evaluation of the models' performance, covering both global and local aspects of contour accuracy. The use of multiple metrics ensures that our assessment is robust and representative of the challenges faced in clinical practice. The results were compared against inter-observer variation (IOV) values from previous studies, ensuring that our findings are contextualized within the existing literature. This approach allows us to assert that the models perform as well as human observers, with differences falling within the range of IOV values reported for manual delineations.\n\nIn summary, the reported metrics\u2014DSC, 95% HD, and sDSC\u2014are representative of the current standards in the field and provide a thorough evaluation of the models' delineation capabilities. The inclusion of the sDSC score, in particular, adds a novel dimension to our assessment, focusing on the practical aspects of contour correction and time efficiency.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, the focus was on evaluating the performance of two deep learning (DL) models trained on different datasets. The comparison was not performed on publicly available benchmark datasets. Instead, the evaluation was conducted using in-house collected data from 30 breast cancer patients. This dataset included 15 left-sided and 15 right-sided cases, all treated for locally advanced breast cancer between January 2019 and February 2022.\n\nThe two models compared were an in-house trained model and an externally trained model. The in-house model was trained on data from the same institution where the evaluation took place, following ESTRO guidelines for target structures and specific atlases for organs at risk (OARs). The external model was trained on data from St. Olavs Hospital and \u00c5lesund Hospital in Norway for target structures and internally collected data from RaySearch Laboratories for OARs.\n\nThe evaluation metrics used included the Dice Similarity Coefficient (DSC), surface DSC (sDSC) with a tolerance of 3 mm, and the 95th percentile of the Hausdorff Distance (95% HD). These metrics were chosen to assess the overlap between the automatically generated contours of the two models and the manually delineated contours. The Wilcoxon signed rank test was employed to identify statistically significant differences between the models, with a p-value of \u2264 0.05 considered significant.\n\nVisual inspection was also performed to check for abnormalities and to investigate the origins of any observed differences. Additionally, the DSC and 95% HD values were compared with inter-observer variations (IOV) reported in various studies conducted after the publication of the ESTRO guidelines.\n\nThe comparison did not involve simpler baselines or publicly available methods. The primary goal was to examine the differences in performance between the in-house and externally trained models, reflecting the potential impact of using commercially available models trained on external data. This approach aimed to highlight the importance of thorough validation of model outcomes within the specific clinical context where they will be applied.",
  "evaluation/confidence": "The evaluation of our models includes a thorough statistical analysis to ensure the robustness and significance of our results. We employed the Wilcoxon signed rank test to compare the performance metrics of our models against manual delineations. A p-value of \u2264 0.05 was considered significant, indicating that the differences observed are statistically meaningful and not due to random chance.\n\nFor the Dice Similarity Coefficient (DSC) and 95% Hausdorff Distance (HD), we provide mean values along with standard deviations, which serve as a form of confidence interval, showing the variability and reliability of our metrics. Significant differences between the models and manual delineations are marked with an asterisk, highlighting where one model outperforms the other in a statistically significant manner.\n\nThe results demonstrate that for most structures, the differences between the models are within the range of inter-observer variations, suggesting that the models perform comparably to human experts. However, for certain structures like the CTVn4, esophagus, and thyroid, the differences are more pronounced and require further investigation. This indicates that while our models are generally reliable, there are specific areas where improvements can be made.\n\nOverall, the statistical significance of our results provides confidence in the performance of our models, and the detailed metrics allow for a nuanced understanding of where our models excel and where they may need refinement.",
  "evaluation/availability": "Not enough information is available."
}