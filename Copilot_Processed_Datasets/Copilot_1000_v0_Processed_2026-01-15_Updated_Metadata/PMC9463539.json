{
  "publication/title": "Using Machine Learning Methods Incorporating Individual Reader Annotations to Classify Paediatric Chest Radiographs in Epidemiological Studies.",
  "publication/authors": "Mwaniki P, Kamanu T, Akech S, Eijkemans MJC",
  "publication/journal": "Wellcome open research",
  "publication/year": "2021",
  "publication/pmid": "36111213",
  "publication/pmcid": "PMC9463539",
  "publication/doi": "10.12688/wellcomeopenres.17164.2",
  "publication/tags": "- Machine learning\n- Chest Radiograph\n- Pneumonia\n- Epidemiological Studies\n- Pediatric Chest Radiographs\n- Reader Annotations\n- Classification\n- Medical Imaging\n- Data Augmentation\n- Model Generalizability",
  "dataset/provenance": "The dataset used in this study is the Pneumonia Etiology Research for Child Health (PERCH) dataset. This dataset contains chest X-ray (CXR) images of pediatric patients hospitalized with pneumonia. The PERCH study aimed to investigate the causes of pneumonia in children and was conducted across nine sites in seven low- and middle-income countries: Kilifi, Kenya; Basse, The Gambia; Nakhon Phanom and Sa Kaeo, Thailand; Bamako, Mali; Soweto, South Africa; Lusaka, Zambia; and Dhaka and Matlab, Bangladesh.\n\nThe PERCH dataset consists of 4,172 CXR images from 4,008 pediatric patients who were hospitalized with severe or very severe pneumonia, as classified by the World Health Organization (WHO) pneumonia classification. The CXR images were obtained using various machines available at the study sites before the study commenced, meaning different sites used different CXR machines and scanners.\n\nThe dataset has been used in previous research, notably in studies by Fancourt et al. The images were classified into five categories based on WHO amendments. The study protocol for the initial PERCH study was approved by the Institutional Review Boards or Ethical Review Committees for each of the seven institutions and at The Johns Hopkins School of Public Health. Parents or guardians of participants provided written informed consent.\n\nThe data will be made publicly available in ClinEpiDB. Investigators can submit a data request describing the purpose for which the data will be used, which will be shared and reviewed by the PERCH Executive Committee prior to approval. Additionally, the analysis code is available on GitHub, and an archived version of the analysis code at the time of publication is available on Zenodo.",
  "dataset/splits": "The dataset was split into two main parts: a training/validation set and a testing set. The testing set consisted of a random sample of chest X-rays (CXRs) from 20% of the patients, which amounted to 802 out of 4008 patients. These patients were selected using simple random sampling to ensure that all patients had an equal chance of being included in the testing dataset. The remaining 80% of the data, comprising 3206 patients, was used for model training and hyper-parameter selection.\n\nThe decision to use a single hold-out test set instead of K-fold cross-validation was due to computational resource restrictions. While the test set was considered large enough to assess model performance, K-fold cross-validation would have allowed for the computation of confidence intervals for model accuracy. Additionally, splitting the dataset by site could have helped in assessing the model's generalizability to sites not included during training. This is important because variations in machines and acquisition procedures across different sites could affect model performance during implementation phases, potentially hindering the application of machine learning models in multi-site epidemiological studies.",
  "dataset/redundancy": "The dataset used in this study consists of 4,172 CXR images from 4,008 pediatric patients hospitalized with severe or very severe pneumonia. These images were collected from nine sites across seven low and middle-income countries. The CXR images were classified into five categories based on WHO amendments.\n\nTo ensure the independence of the training and test sets, a random sample of CXRs from 20% of the patients (802 out of 4,008) from all sites were set aside for final model evaluation and testing. The remaining 80% of the data were used for model training and hyper-parameter selection. Simple random sampling was employed to select the CXRs for the testing dataset, ensuring that all patients had an equal chance of being included.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets in the field of medical imaging, particularly those focusing on pneumonia diagnosis in pediatric patients. The use of data from multiple sites and countries enhances the generalizability of the findings, addressing variability in CXR machines and acquisition procedures across different settings.\n\nThe study design, which includes a single hold-out test dataset instead of K-fold cross-validation, was chosen due to computational resource constraints. While K-fold cross-validation would have allowed for the computation of confidence intervals of model accuracy, the large size of the test set was deemed sufficient to assess model performance. However, it is acknowledged that splitting the dataset by site could have allowed for a more robust assessment of model generalizability to sites not included during training. Future work could explore data augmentation techniques, such as contrast and brightness adjustment, to enhance the robustness of the models to differences in digital CXR machines and scanners used in various sites.",
  "dataset/availability": "The data used in this study is from the Pneumonia Etiology Research for Child Health (PERCH) dataset, which consists of 4,172 chest X-ray (CXR) images from 4,008 pediatric patients hospitalized with severe or very severe pneumonia. The data was collected from nine sites across seven low and middle-income countries.\n\nThe underlying data will be made publicly available in ClinEpiDB. Investigators can submit a data request describing the purpose for which the data will be used. This request will be shared and reviewed by the PERCH Executive Committee prior to approval.\n\nThe data splits used for training, validation, and testing are not explicitly detailed in the public forum. However, it is mentioned that a random sample of CXRs from 20% of patients from all sites were set aside for final model evaluation/testing, while the rest were used for model training and hyper-parameter selection. Simple random sampling was used to select CXRs to be included in the testing dataset, ensuring that all patients had an equal chance of being selected.\n\nThe analysis code is available on GitHub and has been archived at the time of publication. The license for the code is MIT, which permits free use, modification, and distribution.\n\nThe data availability and sharing process is enforced through a review mechanism by the PERCH Executive Committee, ensuring that the data is used appropriately and ethically.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is convolutional neural networks (CNNs), specifically ResNet models. These models are well-established in the field of computer vision and have been widely used for image classification tasks, including medical imaging.\n\nThe algorithm is not new; ResNet models have been extensively studied and applied in various domains. The choice to use ResNet models was driven by their proven effectiveness in capturing complex patterns in image data, which is crucial for the accurate classification of chest X-ray (CXR) images.\n\nGiven that ResNet models are already well-documented and widely used, publishing the specifics of these models in a machine-learning journal would not add significant novel contributions to the field. Instead, our focus was on applying and adapting these models to the specific context of CXR classification, incorporating individual reader\u2019s classifications to enhance model performance and reduce inter-reader variability. This application is what sets our work apart and contributes to the broader goal of standardizing CXR analysis across multiple sites.",
  "optimization/meta": "The model incorporates a meta-predictor approach by utilizing reader embeddings to predict how individual readers would classify a given chest X-ray (CXR) image. This method involves training models to make predictions based on the annotations of each reader, rather than aggregating all readers' annotations into a single final classification.\n\nThe meta-predictor leverages multiple predictions for each CXR, one for each reader, which are then aggregated to produce the final output. This approach allows the model to account for inter-reader variability and potentially improve the overall accuracy and robustness of the predictions.\n\nThe machine-learning methods that constitute the whole include various ResNet architectures (ResNet18, ResNet34, and ResNet50), which are convolutional neural networks used for image classification. These models are trained with hyper-parameters that include training batch size, dropout proportion, weight decay coefficients, learning rate, and data augmentation techniques such as affine transformations, brightness, and contrast adjustments.\n\nThe reader embeddings add an additional layer of complexity by allowing the model to learn how different readers might interpret the same image. This is achieved through element-wise multiplication of CXR embeddings and reader embeddings, enabling the model to assign different weights to image features based on the reader.\n\nRegarding the independence of training data, the models were trained using a single hold-out test set due to computational resource constraints. While this approach does not allow for the computation of confidence intervals through K-fold cross-validation, it ensures that the test set is independent of the training data. This independence is crucial for evaluating the model's performance and generalizability.\n\nThe use of reader embeddings and the aggregation of multiple predictions help mitigate issues related to inter-reader and intra-reader variability, making the models more suitable for multi-site studies or long-term epidemiological research.",
  "optimization/encoding": "All chest X-ray (CXR) images were initially down-sampled to 300x300 pixels to reduce computational costs. These images were then resized to 224x224 pixels for training and validation. The training pipeline included random resized cropping, random brightness and contrast augmentation, random horizontal flipping, and random affine transformations to mitigate overfitting. Both the training and validation pipelines applied normalization similar to the ImageNet dataset by subtracting specific values from and dividing by other values from the red, green, and blue channels.\n\nFor the machine-learning models, pre-trained ResNet architectures (ResNet18, ResNet34, and ResNet50) were used. The last fully connected layer of these ResNet models was replaced with a new fully connected layer having five output units, corresponding to each World Health Organization (WHO) category. Additionally, reader embeddings were incorporated to transform each reader's identifier into a vector of 32 units using entity embeddings for categorical variables. These reader embeddings were then projected to match the dimension of the image embeddings, with various activation functions applied. The reader and image embeddings were combined using element-wise multiplication, followed by a fully connected layer with softmax activation for the final prediction.\n\nDuring training, each CXR image was sampled once per epoch to ensure consistent weight updates across models with and without embeddings. The models with reader embeddings were trained to predict individual reader annotations rather than the final classification. There were 18 readers, resulting in 18 predictions per CXR image. During inference, these predictions were aggregated using an unweighted mean to produce the final prediction.",
  "optimization/parameters": "In our study, we employed ResNet architectures (ResNet18, ResNet34, and ResNet50) for classifying chest radiographs (CXRs). The number of parameters in each model varies depending on whether reader embeddings are included.\n\nFor models without reader embeddings, the parameter counts are as follows:\n- ResNet18 has approximately 11.2 million parameters.\n- ResNet34 has around 21.3 million parameters.\n- ResNet50 has about 23.5 million parameters.\n\nWhen reader embeddings are incorporated, the parameter counts increase due to the additional layers and embeddings. Specifically:\n- ResNet18 with reader embeddings has approximately 11.2 million + 16,928 parameters.\n- ResNet34 with reader embeddings has around 21.3 million + 16,928 parameters.\n- ResNet50 with reader embeddings has about 23.5 million + 67,416 parameters.\n\nThe selection of these parameters was guided by the Asynchronous Successive Halving Algorithm (ASHA), which is a hyper-parameter optimization technique. We randomly sampled 300 hyper-parameter configurations from the search space and stopped poor-performing configurations after 10, 20, 40, and 80 epochs. This process ensured that we identified the optimal hyper-parameters for each model architecture, both with and without reader embeddings.\n\nThe hyper-parameters tuned included training batch size, dropout proportion, weight decay coefficients for convolutional and fully connected layers, learning rates, and proportions of training images with various augmentations. For models with reader embeddings, additional hyper-parameters were tuned, such as the maximum L2-norm of reader embeddings, learning rates for embedding weights, and weight decay coefficients for the fully connected layer projecting reader embeddings.\n\nThis systematic approach allowed us to fine-tune the models effectively, ensuring that they performed optimally on the task of classifying CXRs.",
  "optimization/features": "The input features for our models are derived from chest X-ray (CXR) images. Each image is preprocessed to have dimensions of 3 \u00d7 224 \u00d7 224 pixels, which means the input features consist of three color channels (red, green, and blue) each with 224 \u00d7 224 pixels. Therefore, the total number of input features per image is 3 \u00d7 224 \u00d7 224 = 150,528.\n\nFeature selection in the traditional sense was not performed, as we utilized the entire image data. However, the models were designed to focus on relevant regions of the CXR images through the use of reader embeddings and gradient-weighted class activation mapping (Grad-CAM). This approach allows the model to highlight important areas in the images that contribute to the predictions, effectively selecting relevant features during the training process.\n\nThe preprocessing steps, including resizing, normalization, and data augmentation, were applied uniformly to both the training and validation datasets. This ensures that the feature extraction process is consistent and does not introduce any bias from the validation set. The training pipeline included random resized cropping, random brightness and contrast augmentation, random horizontal flipping, and random affine transformations to enhance the robustness of the models. These augmentations help in making the models generalizable and reduce overfitting.",
  "optimization/fitting": "The fitting method employed in this study involved training deep learning models, specifically ResNet architectures, to classify chest radiographs (CXRs). The models were trained using a subset of the data, with 20% of the patients' CXRs set aside for final model evaluation/testing. This random sampling ensured that all patients had an equal chance of being included in the testing dataset.\n\nThe hyper-parameter search was conducted using the ASHA (Asynchronous Successive Halving Algorithm) method, which involved randomly sampling 300 hyper-parameter configurations from the search space. Poor-performing configurations were stopped early after 10, 20, 40, and 80 epochs, ensuring efficient use of computational resources. The models were trained for a maximum of 150 epochs, with the learning rate halved after 50 and 100 epochs to fine-tune the training process.\n\nTo address the potential issue of over-fitting, several techniques were employed. First, dropout layers were included in the models to randomly set a fraction of input units to zero during training, which helps prevent over-reliance on specific neurons. Second, L2 regularization was applied to both convolutional and fully connected layers to penalize large weights, thereby reducing the model's complexity. Additionally, data augmentation techniques such as affine transformations and brightness/contrast adjustments were used to increase the diversity of the training data, making the model more robust.\n\nUnder-fitting was mitigated by using deep neural networks with sufficient capacity to learn complex patterns in the data. The models were also trained for an extended number of epochs, with learning rate adjustments to ensure thorough optimization. The inclusion of reader embeddings further enhanced the models' ability to capture nuanced variations in reader interpretations, improving overall performance.\n\nThe number of parameters in the models was not excessively large compared to the number of training points. For instance, ResNet50 with reader embeddings had 67,416 additional parameters, while ResNet18 and ResNet34 had 16,928 additional parameters each. These increases were manageable given the size of the training dataset, and the models demonstrated improved performance without significant over-fitting risks.\n\nIn summary, the fitting method involved careful hyper-parameter tuning, regularization techniques, data augmentation, and the use of reader embeddings to balance model complexity and performance. These strategies collectively ensured that the models were neither over-fitted nor under-fitted, leading to robust and reliable classification of CXRs.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and improve the generalization of our models. Specifically, we used dropout, L2 regularization, and data augmentation.\n\nDropout was applied to randomly set a fraction of input units to 0 at each update during training time, which helped prevent overfitting by ensuring that the model did not become too reliant on any single neuron. The dropout rates varied depending on the model and whether reader embeddings were used.\n\nL2 regularization, also known as weight decay, was applied to both convolutional and fully connected layers. This technique adds a penalty equal to the squared magnitude of the coefficients to the loss function, encouraging the model to keep the weights small and reducing the risk of overfitting.\n\nAdditionally, we performed data augmentation by adjusting the color brightness and contrast, as well as applying affine transformations to the training images. This technique helps to increase the diversity of the training data, making the model more robust and less likely to overfit to the specific details of the training set.\n\nThese regularization methods were crucial in enhancing the performance and reliability of our models, particularly when dealing with complex tasks such as classifying chest radiographs.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are fully reported. The optimal hyper-parameters for models with and without reader embeddings are detailed in a table, providing specific values for various settings such as batch size, dropout, learning rates, and regularization coefficients. The optimization process involved using the Asynchronous Successive Halving Algorithm (ASHA) to identify these optimal parameters, and this method is well-documented in the text.\n\nThe models were trained using pre-trained ResNet architectures from the torchvision library, with modifications to the last fully connected layer to accommodate the specific classification task. The training process included data augmentation techniques such as random brightness and contrast adjustments, horizontal flips, and affine transformations to reduce overfitting. These details are provided to ensure reproducibility.\n\nThe Python code for the analysis is available on GitHub, making it accessible for others to replicate the experiments. All libraries used in the analysis are open-source and can be downloaded using standard package installers. This ensures that the computational environment can be easily set up by other researchers.\n\nRegarding the availability of model files and optimization parameters, the study does not explicitly mention the provision of saved model files. However, the detailed reporting of the hyper-parameter configurations and the availability of the code should allow others to train similar models from scratch. The use of open-source libraries and the provision of the code on GitHub ensure that the optimization parameters and configurations are transparent and reproducible.",
  "model/interpretability": "The model employed in this study is not a black box. To enhance interpretability, we have incorporated visualization techniques to ensure that the model focuses on relevant regions of the chest x-ray (CXR) images when making predictions.\n\nWe utilized Gradient Class Activation Maps (Grad-CAM) to generate heatmaps that highlight the areas of the CXR images that the model considers most important for its predictions. These heatmaps are overlaid on the original CXR images, providing a clear visual indication of the regions that contribute most to the model's decisions. By examining these heatmaps, it is evident that the model is attending to the relevant anatomical structures within the lung fields, rather than relying on irrelevant or extraneous features.\n\nThis approach not only validates the model's focus but also aligns with recent findings that emphasize the importance of verifying model attention in medical imaging tasks. By including these visualizations, we aim to provide transparency and build trust in the model's predictive capabilities.",
  "model/output": "The model developed in this study is a classification model. It is designed to classify chest radiographs (CXRs) into different categories. Specifically, the model performs a five-way classification, distinguishing between normal CXRs and various pathologies. The output of the model is a prediction of how a given reader would classify an image, rather than a final aggregate classification. This approach allows the model to make predictions that are tailored to individual readers, improving accuracy and area under the curve (AUC) metrics. The model's performance was evaluated using metrics such as accuracy and AUC, which are typical for classification tasks. The best model achieved an accuracy of 61% and demonstrated high accuracy in identifying normal CXRs, at 80%. The model's outputs are visualised using Grad-CAM heatmaps, which highlight the regions of the CXR images that were most relevant in making the predictions. This visualization helps in understanding which parts of the image the model focuses on for classification.",
  "model/duration": "The models were trained using PyTorch 1.7 on a desktop equipped with 32GB of RAM and a single Nvidia Titan RTX graphical processing unit. The training process involved running the models for a maximum of 150 epochs, with the learning rate being halved after 50 and 100 epochs. The hyper-parameter search was conducted using the Asynchronous Successive Halving Algorithm (ASHA), which involved randomly sampling 300 hyper-parameter configurations and stopping poor-performing configurations after 10, 20, 40, and 80 epochs.\n\nThe specific execution time for the model to run is not explicitly stated, but the use of a high-performance GPU and the described training regimen suggest that the training process was optimized for efficiency. The models were trained on images down-sampled to 300\u00d7300 pixels and resized to 224\u00d7224 pixels, which would have reduced the computational load. Additionally, data augmentation techniques such as random brightness and contrast adjustment, horizontal flipping, and affine transformations were applied to enhance the robustness of the models.\n\nThe training pipeline included normalization similar to the ImageNet dataset, which further streamlined the training process. The models were evaluated using a single hold-out test dataset, and the performance was assessed based on metrics such as accuracy and AUC. The use of pre-trained ResNet architectures (ResNet18, ResNet34, and ResNet50) from the torchvision library also contributed to reducing the training time, as these models were already trained on large datasets and fine-tuned for the specific task of CXR classification.\n\nIn summary, while the exact execution time is not provided, the training process was designed to be efficient, leveraging high-performance hardware and optimized training techniques. The models were trained for a maximum of 150 epochs, with intermediate stopping of poor-performing configurations, ensuring that the training time was managed effectively.",
  "model/availability": "The source code for the analysis conducted in this study is publicly available. It can be accessed via a GitHub repository, which provides the necessary scripts and tools to replicate the analysis. The repository is located at https://github.com/pmwaniki/xray-analysis.\n\nAdditionally, an archived version of the analysis code at the time of publication is available on Zenodo. This archived version ensures that the exact code used for the study can be accessed and verified. The archived code can be found at https://doi.org/10.5281/zenodo.5501796.\n\nThe code is released under the MIT license, which permits free use, distribution, and modification, provided that the original authors are credited. This licensing approach ensures that the code can be widely used and adapted by other researchers and developers while maintaining proper attribution.",
  "evaluation/method": "The evaluation method employed in this study involved setting aside a random sample of chest X-rays (CXRs) from 20% of patients for final model evaluation and testing. This sample was selected using simple random sampling to ensure that all patients had an equal chance of being included. The remaining 80% of the data was used for model training and hyper-parameter selection.\n\nModel performance was assessed using a single hold-out test dataset. While this approach provided a large enough test set to evaluate model performance, it is acknowledged that K-fold cross-validation would have allowed for the computation of confidence intervals of model accuracy. Additionally, splitting the dataset by site could have facilitated the assessment of model generalizability to sites not included during training. This is important because variations in machines and acquisition procedures across different sites can affect model performance.\n\nThe study also considered the potential benefits of data augmentation techniques, such as contrast and brightness adjustment, to enhance the robustness of machine learning models to differences in digital CXR machines and scanners used in various sites. However, the single train/test split used in this study did not allow for such assessments.\n\nIn summary, the evaluation method focused on a hold-out test set for assessing model performance, with acknowledgments of potential improvements through cross-validation and site-specific data splitting. The study aimed to demonstrate the effectiveness of machine learning models in standardizing CXR analysis across multiple sites, reducing inter- and intra-reader variability.",
  "evaluation/measure": "In the evaluation of our machine learning models for chest X-ray (CXR) classification, several performance metrics were reported to provide a comprehensive assessment of model effectiveness. The primary metrics included accuracy and the area under the curve (AUC) for multi-class classification tasks. Specifically, we reported the multi-class accuracy and the unweighted mean AUC for different model architectures, such as ResNet18, ResNet34, and ResNet50. These metrics were chosen because they are widely used in the literature and provide a clear indication of model performance across various classes.\n\nAccuracy was reported as the proportion of correctly classified CXRs out of the total number of CXRs. This metric is straightforward and provides an overall measure of model performance. However, accuracy alone can be misleading, especially in imbalanced datasets, which is why we also reported the AUC. The AUC measures the model's ability to distinguish between different classes and is particularly useful for evaluating the performance of models on imbalanced datasets.\n\nIn addition to these metrics, we also provided disaggregated AUCs for each class, which allowed for a more detailed analysis of model performance across different types of CXR findings. This level of detail is important for understanding the strengths and weaknesses of the models and for identifying areas where performance can be improved.\n\nThe reported metrics are representative of those commonly used in the literature for evaluating machine learning models in medical imaging. For instance, studies by Rajpurkar et al. and Dunnmon et al. have also used accuracy and AUC as primary performance metrics. This consistency with the literature ensures that our results are comparable to other studies in the field and provides a benchmark for future research.\n\nFurthermore, we included visualizations such as Grad-CAM heatmaps to illustrate the regions of the CXR images that the model focused on when making predictions. This qualitative assessment complements the quantitative metrics and provides additional insights into the model's decision-making process.\n\nIn summary, the performance metrics reported in our study are comprehensive and representative of those used in the literature. They provide a clear and detailed assessment of model performance, enabling a thorough evaluation of the effectiveness of our machine learning models for CXR classification.",
  "evaluation/comparison": "The evaluation of our machine learning models for chest X-ray (CXR) classification involved several key comparisons to assess their performance and robustness. We did not perform a direct comparison to publicly available methods on benchmark datasets. However, we did compare our models' performance to that of human readers, which provided valuable insights.\n\nOur models were evaluated against the initial readers' annotations, showing that while the best model had a slightly lower accuracy (61%) compared to the readers (67%), this comparison was biased in favor of the readers because their annotations were used to derive the final aggregate annotation. Despite this, our models demonstrated high accuracy in identifying normal CXRs (80%), suggesting their potential utility in classifying normal versus abnormal CXRs.\n\nWe also noted that the agreement between the initial readers and model accuracy improved with the age of the children, indicating that both readers and models faced challenges in interpreting CXRs from younger children. This difficulty may be attributed to the challenges in obtaining quality CXR images from very young children and the presence of other body parts in the images.\n\nAdditionally, we observed a wide variation in model accuracy across different sites, ranging from 45% to 71%. This variation could be due to differences in pathology distribution or variability in image quality across sites. The model performed poorly in sites with a lower proportion of normal images, such as Zambia and South Africa, but achieved high accuracy in Bangladesh, where CXRs were acquired via analogue means. This suggests that our models can be applied in settings where digital CXR machines are not available.\n\nFurthermore, we did not annotate the CXRs used for training with bounding boxes of pathologies of interest, which would have allowed for a more robust evaluation of the model's ability to identify correct regions of interest. However, visual inspection of Grad-CAM heatmaps on randomly selected CXRs indicated that the machine learning models were focusing on relevant areas of the images.\n\nIn summary, while we did not compare our methods to publicly available benchmarks, our evaluation included comparisons to human readers and assessments across different sites and age groups. These comparisons provided valuable insights into the strengths and limitations of our models.",
  "evaluation/confidence": "The evaluation of our model's performance was conducted using a single hold-out test dataset, which was chosen due to computational resource constraints. While this approach provided a robust assessment of model performance, it did not allow for the computation of confidence intervals for model accuracy. Typically, K-fold cross-validation would have been preferable as it enables the calculation of such intervals, offering a more comprehensive understanding of the model's variability and reliability.\n\nThe statistical significance of our results is supported by the large and diverse dataset used, which included chest X-rays from multiple sites. This diversity helps to ensure that the model's performance is generalizable across different settings. However, the lack of K-fold cross-validation means that we cannot provide confidence intervals for our performance metrics. Despite this, the model demonstrated strong and consistent performance across the hold-out test set, suggesting its robustness and reliability.\n\nTo further validate the model's generalizability, it would have been beneficial to split the dataset by site and assess performance across different sites. This approach would have allowed us to evaluate how well the model performs in settings not included in the training data, addressing potential variations in image acquisition procedures and equipment. Unfortunately, this level of detailed analysis was not feasible within the current study's constraints.\n\nIn summary, while the performance metrics are robust and the results are statistically significant, the absence of confidence intervals due to the single hold-out test dataset is a limitation. Future work could address this by employing K-fold cross-validation and site-specific evaluations to provide a more nuanced understanding of the model's performance and generalizability.",
  "evaluation/availability": "The raw evaluation files are not publicly available at the moment. However, the underlying data will be made publicly available in ClinEpiDB. Investigators can submit a data request describing the purpose for which the data will be used, which will be shared and reviewed by the PERCH Executive Committee prior to approval. This process ensures that the data is used responsibly and ethically.\n\nThe analysis code used for the evaluation is publicly available. It can be accessed from a GitHub repository. Additionally, the archived analysis code as of the time of publication is available on Zenodo, with a DOI provided for easy access. The code is released under the MIT license, which permits free use, distribution, and modification, provided that the original authors are credited. This open access to the code facilitates reproducibility and allows other researchers to build upon the work presented in the publication."
}