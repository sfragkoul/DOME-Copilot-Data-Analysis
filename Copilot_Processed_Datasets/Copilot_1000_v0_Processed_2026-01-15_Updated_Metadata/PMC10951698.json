{
  "publication/title": "Comparing image normalization techniques in an end-to-end model for automated modic changes classification from MRI images.",
  "publication/authors": "Cina A, Haschtmann D, Damopoulos D, Gerber N, Loibl M, Fekete T, Kleinst\u00fcck F, Galbusera F",
  "publication/journal": "Brain & spine",
  "publication/year": "2024",
  "publication/pmid": "38510635",
  "publication/pmcid": "PMC10951698",
  "publication/doi": "10.1016/j.bas.2023.102738",
  "publication/tags": "- Modic Changes\n- MRI\n- Deep Learning\n- Medical Imaging\n- Spine Conditions\n- Classification\n- Machine Learning\n- Vertebral Bodies\n- Image Segmentation\n- Automated Detection",
  "dataset/provenance": "Two datasets of MRI images collected from two different hospitals were used in this study. All subjects provided informed consent for the scientific and educational use of the images.\n\nThe first dataset consisted of T2-weighted MRI images from 761 patients. Each image was annotated with the x-y coordinates of the bounding box that localized the intervertebral discs from T12/L1 to L5/S1. This dataset was split into 80% for training and 20% for testing.\n\nThe second dataset was sourced from an earlier study and contained both T1- and T2-weighted MRI images. This dataset included images from 912 patients labeled according to the Modic Changes (MCs) classification scheme. The distribution of instances among the three classes was extremely imbalanced, with 80% for MC0, 5% for MC1, and 15% for MC2. Due to this pronounced class imbalance, a specific approach was employed, namely stratified cross-validation to maintain consistent proportions of the three classes across distinct folds. Specifically, a 5-fold cross-validation strategy was applied 10 times (repeated stratified K-fold).",
  "dataset/splits": "Two datasets were utilized in this study. The first dataset consisted of T2-weighted MRI images from 761 patients. This dataset was divided into two splits: 80% for training and 20% for testing. Specifically, 609 samples were used for training, and 152 samples were reserved for testing.\n\nThe second dataset, sourced from an earlier study, included both T1- and T2-weighted MRI images from 912 patients. This dataset was split using a repeated stratified 5-fold cross-validation strategy, applied 10 times. In each fold, 730 samples were used for training, and 182 samples were used for validation. This approach ensured that the distribution of the three Modic Change (MC) classes\u2014MC0, MC1, and MC2\u2014was maintained at 80%, 5%, and 15% respectively across all folds.",
  "dataset/redundancy": "In our study, two distinct datasets were utilized, each sourced from different hospitals, ensuring a diverse range of MRI images. The first dataset comprised T2-weighted MRI images from 761 patients, with annotations marking the bounding boxes for intervertebral discs from T12/L1 to L5/S1. This dataset was divided into training and testing sets, maintaining an 80/20 split to ensure a substantial amount of data for model training while reserving a significant portion for unbiased evaluation.\n\nThe second dataset, originating from a previous study, included both T1- and T2-weighted MRI images from 912 patients. This dataset was labeled according to the Modic Changes (MCs) classification scheme, with a notable class imbalance: 80% for MC0, 5% for MC1, and 15% for MC2. To address this imbalance and ensure consistent class proportions across different folds, we employed a repeated stratified 5-fold cross-validation strategy. This approach was applied 10 times, ensuring that each fold maintained the same distribution of MC classes, thereby mitigating the risk of bias due to class imbalance.\n\nThe independence of the training and test sets was enforced through careful splitting and cross-validation techniques. For the first dataset, the split was straightforward, with 80% of the data used for training and the remaining 20% for testing. For the second dataset, the repeated stratified 5-fold cross-validation ensured that the model was trained and validated on different subsets of the data, enhancing the robustness of the evaluation process.\n\nComparing our datasets to previously published machine learning datasets in medical imaging, the class imbalance in the second dataset is a common challenge. However, our use of stratified cross-validation is a well-established method to handle such imbalances, ensuring that the model's performance is evaluated fairly across all classes. The independence of the training and test sets is a critical aspect that we have rigorously enforced, aligning with best practices in machine learning to prevent data leakage and ensure reliable model performance.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study is the Adam optimizer. This is a widely-used class of machine-learning algorithms known for its efficiency in training deep learning models. It is not a new algorithm; it was introduced by Diederik P. Kingma and Jimmy Ba in 2014. The Adam optimizer combines the advantages of two other extensions of stochastic gradient descent. Specifically, it computes adaptive learning rates for each parameter, which allows for faster convergence and better performance on problems with sparse gradients, such as those encountered in natural language and computer vision tasks.\n\nThe choice of the Adam optimizer was driven by its proven effectiveness in handling large datasets and its ability to adapt the learning rate for each parameter, which helps in achieving faster and more stable convergence. This optimizer is particularly well-suited for the complex tasks involved in medical image analysis, where the models need to learn intricate patterns from high-dimensional data.\n\nGiven that the Adam optimizer is a well-established algorithm, it was not necessary to publish it in a machine-learning journal. Instead, our focus was on applying this robust optimization technique to the specific challenges of detecting and classifying Modic Changes in MRI images. The use of the Adam optimizer, along with other advanced techniques such as transfer learning and image augmentation, contributed to the development of a highly accurate and reliable model for automated MC assessment.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The model is composed of two distinct deep learning models. The first model is a Faster R-CNN, which is used for object detection tasks. It localizes and labels each lumbar vertebra from the central slice of a T2-weighted MRI image. The second model is a 3D CNN, which processes the regions of interest (ROIs) extracted by the Faster R-CNN model. It extracts feature vectors from T1-and T2-weighted images and concatenates them to predict the Modic changes (MCs).\n\nThe training data for the models is independent. The first model is trained using the first dataset, while the second model is trained using the second dataset. The second dataset is processed through the Faster R-CNN detection model to extract ROIs, which are then used to train the 3D CNN model. The use of stratified cross-validation ensures that the proportions of the three classes are consistent across distinct folds, maintaining the independence of the training data.",
  "optimization/encoding": "For the data encoding and preprocessing, several steps were undertaken to ensure the machine-learning algorithms could effectively learn from the MRI images. Initially, the images were processed using open-CV for image processing tasks. This included resizing and normalizing the images to a standard format suitable for input into the deep learning models.\n\nImage augmentation techniques were employed to enhance the robustness of the models. These techniques included rotations between 10 and 10 degrees and shifts of 10 to 10 pixels. Albumentations, a popular library for image augmentation, was utilized to implement these transformations.\n\nFour normalization techniques were explored to preprocess the images: no normalization, standardization (zero mean and unit variance), Contrast Limited Adaptive Histogram Equalization (CLAHE), and Gamma Correction. Standardization was identified as the optimal choice for all evaluation metrics, significantly improving model performance compared to other techniques.\n\nThe images were then fed into the deep learning models, which were developed using PyTorch. For the first dataset, the Faster R-CNN model with a ResNet50 backbone was used to localize and label each lumbar vertebra from the central slice of a T2-weighted MRI image. The model was trained to determine the bounding box coordinates for each level, associate the bounding box with its respective level label, and assign a confidence score to the labeling.\n\nFor the second dataset, the extraction of Regions of Interest (ROIs) was achieved through the Faster R-CNN detection model. This involved processing T1-and T2-weighted images to identify six ROIs per image. Each ROI consisted of five slices, symmetrically extracted around the midsagittal slice. These slices were then processed by two identical 3D CNN models to extract feature vectors, which were concatenated to predict the Modic Changes (MCs).\n\nThe models were optimized using the Adam optimizer, starting with a learning rate of 0.001. Early stopping was implemented to halt training if the validation loss did not improve for five epochs. This approach ensured that the models were trained efficiently and effectively, avoiding overfitting.",
  "optimization/parameters": "In our study, the models were developed using deep learning techniques, specifically convolutional neural networks (CNNs). The number of parameters (p) in these models is not explicitly stated, as it can vary significantly depending on the architecture and the specific layers used. However, it is important to note that we employed transfer learning, which involves using pre-trained models and fine-tuning them for our specific tasks. This approach helps in reducing the number of parameters that need to be trained from scratch, making the model more efficient.\n\nThe selection of parameters was guided by the use of established architectures and techniques. For the detection model, we used the Faster R-CNN model with a ResNet50 backbone. This choice was based on the model's proven performance in object detection tasks. For the classification model, we utilized a 3D CNN, specifically a 3D version of the ResNet network, which was adapted using the Medical Open Network for Artificial Intelligence (MONAI) framework. This framework is designed to facilitate the development of deep learning models for medical image analysis.\n\nThe optimization process involved using the Adam optimizer with an initial learning rate of 0.001. Early stopping was implemented to halt training if the validation loss did not improve for five consecutive epochs. This strategy helps in preventing overfitting and ensures that the model generalizes well to unseen data.\n\nImage augmentation techniques, including rotations between 10 and 10 degrees and shifts of 10 to 10 pixels, were applied to increase the robustness of the models. These augmentations help in making the models more resilient to variations in the input data, thereby improving their performance on diverse datasets.\n\nIn summary, while the exact number of parameters is not specified, the models were designed using well-established architectures and optimization techniques. The use of transfer learning and image augmentation further enhances the models' efficiency and robustness.",
  "optimization/features": "The input features for our models primarily consist of MRI images, specifically T1- and T2-weighted sagittal MRI images. For the first model, the input features are the central slices of these MRI images, which are used to localize and label each lumbar vertebra. The second model processes these images to extract regions of interest (ROIs), which consist of five slices symmetrically extracted around the midsagittal slice for each vertebral level.\n\nFeature selection in the traditional sense was not performed, as the models rely on the raw image data. However, the process of extracting ROIs can be seen as a form of spatial feature selection, focusing the model's attention on the relevant areas of the images. This extraction is done using a Faster R-CNN model, which identifies six ROIs per image corresponding to the lumbar vertebrae from T12/L1 to L5/S1.\n\nThe extraction of ROIs was performed using the training set only, ensuring that the validation and test sets remained unseen during this process. This approach helps to maintain the integrity of the evaluation process and prevents data leakage, which could otherwise lead to overly optimistic performance estimates. The models were then trained on these extracted features, using techniques such as transfer learning and image augmentation to improve their robustness and generalization capabilities.",
  "optimization/fitting": "In our study, we employed deep learning models that inherently have a large number of parameters, significantly more than the number of training points. To address potential overfitting, we utilized several strategies. First, we implemented early stopping, which halted training if the validation loss did not improve for five consecutive epochs. This ensured that the model did not memorize the training data but rather generalized well to unseen data. Additionally, we applied image augmentation techniques, including rotations and shifts, to increase the robustness of the models. These augmentations effectively expanded our training dataset, helping to prevent overfitting.\n\nTo mitigate underfitting, we employed transfer learning. This technique involved using pre-trained models on large datasets and fine-tuning them for our specific task. By leveraging knowledge from extensive datasets, we enhanced the model's ability to learn relevant features from our smaller dataset. Furthermore, we used stratified cross-validation to maintain consistent proportions of the different classes across folds, ensuring that the model was exposed to a diverse range of examples during training. This approach helped in capturing the underlying patterns in the data, thereby reducing the risk of underfitting.\n\nThe combination of these techniques\u2014early stopping, image augmentation, transfer learning, and stratified cross-validation\u2014enabled us to effectively balance the model's complexity, ensuring it neither overfitted nor underfitted the data.",
  "optimization/regularization": "In our study, several regularization methods were employed to prevent overfitting and enhance the robustness of our models. Early stopping was utilized to halt training if the validation loss did not improve for five consecutive epochs. This technique helps to ensure that the model does not overfit to the training data by stopping the training process when performance on the validation set ceases to improve.\n\nAdditionally, image augmentation techniques were applied, including rotations between 10 and 10 degrees and shifts of 10 to 10 pixels. These augmentations increase the diversity of the training data, making the models more generalizable and less likely to overfit to the specific patterns in the training set.\n\nStratified cross-validation was also implemented, particularly for the second dataset, to maintain consistent proportions of the three classes across different folds. This approach helps to ensure that the model is trained and evaluated on representative subsets of the data, reducing the risk of overfitting to any particular subset.\n\nFurthermore, repeated cross-validation was performed, applying a 5-fold cross-validation strategy 10 times. This method provides multiple evaluations on different subsets of the main dataset, offering a more reliable estimate of the model's performance and helping to mitigate overfitting.\n\nThe use of transfer learning, where pretrained models were adapted to our specific tasks, also contributed to regularization. By leveraging knowledge from large, general datasets, the models were better equipped to handle the medical imaging tasks at hand, reducing the likelihood of overfitting to the smaller, domain-specific datasets.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are fully reported within the publication. Specifically, we utilized the Adam optimizer with an initial learning rate of 0.001 and employed early stopping to halt training if the validation loss did not improve for five consecutive epochs. This approach ensures that the model training process is efficient and prevents overfitting.\n\nImage augmentation techniques, including rotations between -10 and 10 degrees and shifts of up to 10 pixels, were applied to enhance the robustness of the models. These techniques are crucial for improving the generalization capabilities of the models by exposing them to a variety of transformed images during training.\n\nThe code implementation details are also provided. We developed the models using Python and the PyTorch library, which is widely used for deep learning tasks. For image processing, we relied on open-CV, a robust library for computer vision tasks. Albumentations was used for image augmentation, offering a comprehensive set of tools for augmenting medical images.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly detailed in the publication. However, the methods and configurations described are sufficient for replicating the experiments. For access to the specific model files and optimization parameters, interested parties may need to contact the authors directly, as the publication does not provide direct links or repositories for these resources. The license under which the code and models might be shared is not specified in the publication.",
  "model/interpretability": "The model developed in this study is primarily a black-box model, particularly the deep learning components such as the Faster R-CNN and the 3D CNN. These models, while highly effective in their tasks of object detection and classification, do not inherently provide clear, interpretable reasons for their predictions. The Faster R-CNN model, for instance, is designed to localize and label lumbar vertebrae in MRI images by predicting bounding box coordinates and associating them with specific vertebral levels. Similarly, the 3D CNN model classifies regions of interest into different Modic Change (MC) types based on features extracted from T1- and T2-weighted MRI images.\n\nHowever, efforts can be made to enhance the interpretability of these models. Techniques such as Grad-CAM (Gradient-weighted Class Activation Mapping) and Class Activation Mapping (CAM) can be employed to visualize which parts of the input images the model focuses on when making predictions. These visualization tools can help in understanding the decision-making process of the CNN by highlighting the regions in the MRI images that contribute most to the classification of MC types. By applying these techniques, it is possible to gain insights into how the model identifies and differentiates between various MC types, thereby making the model more transparent.\n\nAdditionally, the use of transfer learning from large, general image datasets like ImageNet to medical imaging tasks can be seen as a step towards interpretability. Transfer learning leverages knowledge gained from a vast amount of data to improve performance on specific tasks, which can sometimes make the model's behavior more predictable and aligned with human understanding. However, this does not fully address the black-box nature of the model but rather provides a foundation that can be built upon for better interpretability.\n\nIn summary, while the current model is largely a black-box, there are established methods and ongoing research directions that can be pursued to make the model more interpretable. Visualization techniques and the strategic use of transfer learning are steps in the right direction, but further work is needed to fully understand and explain the model's decisions in a clinically meaningful way.",
  "model/output": "The model developed in this study is a classification model. It is designed to categorize regions of interest (ROIs) in MRI images into three specific classes of Modic Changes (MCs): no MC (MC0), type 1 (MC1), and type 2 (MC2). The model utilizes a combination of a detection model, specifically the Faster R-CNN, and a 3D Convolutional Neural Network (CNN) for this classification task. The Faster R-CNN model is used to locate and label each lumbar vertebra from the central slice of a T2-weighted MRI image, while the 3D CNN model processes the extracted ROIs to classify them into the respective MC types. The classification performance is evaluated using metrics such as sensitivity, balanced accuracy, and F1-score, which are indicative of the model's ability to correctly identify and classify the different types of MCs. The use of stratified cross-validation ensures that the model's performance is assessed across a balanced distribution of the three classes, addressing the inherent class imbalance in the dataset. Overall, the model's output is a classification of the MC types present in the MRI images, providing a standardized assessment that can aid in diagnosis and treatment planning.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models developed in this study is not publicly released. However, the implementation was done using widely available libraries and frameworks. Specifically, PyTorch was used for model development, OpenCV for image processing, and Albumentations for image augmentation. These tools are open-source and can be accessed through their respective repositories.\n\nThe models were trained and evaluated using standard deep learning practices, including the use of the Adam optimizer and early stopping based on validation loss. Image augmentation techniques such as rotations and shifts were applied to enhance the robustness of the models.\n\nWhile the specific code and trained models are not available for public use, the methodologies and techniques employed are well-documented and can be replicated using the mentioned libraries. This approach ensures that other researchers can build upon the work presented, utilizing the same tools and frameworks to develop similar models for medical image analysis.",
  "evaluation/method": "The evaluation method employed for assessing the detection model's performance involved several key metrics and strategies. The Intersection over Union (IoU) metric was utilized to quantify the overlap between the predicted bounding boxes and the ground truth annotations. This metric provided a precise measure of localization accuracy, ensuring that the model's predictions aligned closely with the actual data.\n\nFor the second dataset, repeated five-fold cross-validation was used, specifically repeated 10 times. This approach allowed for multiple evaluations on different subsets of the main dataset, ensuring robustness and reliability in the model's performance assessment. Stratified cross-validation was employed to maintain consistent proportions of the three classes across distinct folds, addressing the pronounced class imbalance in the dataset.\n\nThe performance metrics included sensitivities for each class, balanced accuracy, and the F1-score. Sensitivities measured the model's capability to correctly classify the Modic Changes (MCs), while balanced accuracy provided an overall performance measure by considering equal weight to each class. The F1-score combined precision and recall, offering a single metric that balanced true positives and false positives.\n\nAdditionally, various normalization techniques were evaluated, including no normalization, standardization, Contrast Limited Adaptive Histogram Equalization (CLAHE), and Gamma Correction. Standardization was identified as the optimal choice for all evaluation metrics, significantly improving the model's performance compared to other techniques.\n\nThe evaluation also considered the confidence scores assigned to each object prediction within the bounding boxes. High confidence scores, all above 0.9, demonstrated the model's accuracy in classifying vertebral levels. The IoU values on the test set were above 0.7, indicating a strong overlap between true and predicted bounding boxes, which further validated the model's reliability in extracting Regions of Interest (ROIs) from medical images.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our models in detecting and classifying Modic Changes (MCs) from MRI images. The primary metric used for assessing the detection model's performance was the Intersection over Union (IoU). This metric quantifies the overlap between the predicted bounding boxes and the ground truth annotations, providing a precise measure of localization accuracy. IoU values above 0.7 indicated a high degree of alignment between the predicted and actual bounding boxes, demonstrating the model's robustness in identifying Regions of Interest (ROIs).\n\nAdditionally, we assigned a confidence score to each object prediction within the bounding boxes. These scores, all above 0.9, reflected the model's high confidence in classifying vertebral levels. This metric is crucial for ensuring reliable extraction of ROIs from medical images.\n\nFor the classification task, we evaluated the model using sensitivity for each MC class, balanced accuracy, and the F1-score. Sensitivity measures the model's ability to correctly classify each type of MC, with mean sensitivities of 0.83, 0.85, and 0.78 for MC0, MC1, and MC2, respectively. Balanced accuracy, which considers equal weight for each class, was 0.80, indicating the model's overall performance across imbalanced classes. The F1-score, which combines precision and recall, was 0.88, highlighting the model's balance between true positives and false positives.\n\nThese metrics are representative of the current literature in medical image analysis. IoU and confidence scores are standard in object detection tasks, while sensitivity, balanced accuracy, and F1-score are commonly used in classification tasks, especially when dealing with imbalanced datasets. Our choice of metrics ensures a comprehensive evaluation of both the detection and classification performance of our models.",
  "evaluation/comparison": "Not enough information is available.",
  "evaluation/confidence": "The evaluation of the detection model's performance utilized the Intersection over Union (IoU) metric to measure the overlap between predicted bounding boxes and ground truth annotations. This metric provides a precise quantification of localization accuracy. IoU values above 0.7 indicate a high degree of overlap, demonstrating strong alignment between the predicted and true bounding boxes.\n\nConfidence scores were assigned to each object prediction within the bounding boxes, with all scores exceeding 0.9. This high confidence level underscores the model's reliability in identifying vertebral levels. The consistent performance across different intervertebral levels, as shown in the results, further supports the model's robustness.\n\nFor the second dataset, repeated five-fold cross-validation was employed to ensure multiple evaluations on different subsets of the main dataset. This approach helps in assessing the model's performance across various data splits, providing a more comprehensive evaluation.\n\nThe performance metrics included sensitivities for each class, balanced accuracy, and the F1-score. Sensitivities for the three classes (MC0, MC1, and MC2) were 0.83, 0.85, and 0.78, respectively, with standard deviations indicating variability. The balanced accuracy of 0.80 and the F1-score of 0.88 highlight the model's ability to maintain a balance between precision and recall, ensuring reliable classification across imbalanced classes.\n\nStandardization was identified as the optimal normalization technique, significantly improving model performance compared to other techniques. The use of stratified cross-validation ensured that the class distribution was maintained consistently across folds, addressing the issue of class imbalance.\n\nIn summary, the evaluation metrics and confidence intervals demonstrate the model's strong performance and reliability. The high IoU values, confidence scores, and balanced accuracy metrics indicate that the model is effective in localizing and classifying vertebral levels and Modic changes. The statistical significance of these results supports the claim that the method is superior to others and baselines.",
  "evaluation/availability": "Not enough information is available."
}