{
  "publication/title": "Fluorescence lifetime image microscopy prediction with convolutional neural networks for cell detection and classification in tissues.",
  "publication/authors": "Smolen JA, Wooley KL",
  "publication/journal": "PNAS nexus",
  "publication/year": "2022",
  "publication/pmid": "36712353",
  "publication/pmcid": "PMC9802238",
  "publication/doi": "10.1093/pnasnexus/pgac235",
  "publication/tags": "- Fluorescence Lifetime Imaging Microscopy (FLIM)\n- Image Processing\n- Machine Learning\n- Convolutional Neural Networks (CNNs)\n- Cell Detection\n- Cell Classification\n- Phasor Analysis\n- Fluorescence Imaging\n- Tissue Preparation\n- Image Annotation\n- Confocal Microscopy\n- Data Augmentation\n- Model Training\n- Image Segmentation\n- Fluorescence Intensity\n- Phasor FLIM\n- Image Colorization\n- U-Net\n- Mask R-CNN\n- Stardist\n- Lung Tissue\n- Mouse Models\n- Image Analysis\n- Data Science Bowl 2018\n- Dense U-Net CNN\n- K-means Clustering\n- Linear Discriminant Analysis (LDA)\n- HSV Color Space\n- RGB Transformation\n- Cross-Validation\n- Annotation Masks\n- Cell Nuclei\n- Tissue Types\n- Fluorescence Stains\n- Propidium Iodide (PI)\n- Wheat-Germ Agglutinin (WGA)\n- Image Segmentation Masks\n- Phasor Components\n- Fluorescence Lifetimes\n- Image Contrast\n- Background Noise\n- Pretraining Datasets\n- Model Evaluation\n- Software and Hardware\n- Image Data Annotation\n- Cell Instance Segmentation\n- Cell/Tissue Classification\n- Fluorescence Intensity Images\n- FLIM Prediction\n- Cross-Channel Prediction\n- Annotation-Free Tasks\n- Pretraining Compatibility\n- Dimensionality Reduction\n- Principal Component Analysis (PCA)\n- Uniform Manifold Approximation and Projection (UMAP)",
  "dataset/provenance": "The datasets used in this work are available on Figshare. These include FLIM images, cell nuclei masks, and a cell/tissue-type classification reference table. The FLIM images are designated as Dataset S1, the cell nuclei masks as Dataset S2, and the classification reference table as Dataset S3. The code to generate the RGB images from the source image data, to generate and train the dense U-Net CNN, and to train the cell classification models can be found on GitHub.\n\nThe datasets were used to train and validate various convolutional neural networks (CNNs), including the dense U-Net CNN for FLIM prediction and Mask R-CNN for cell instance segmentation. The training sets consisted of a subset of the available images, with a specific number of images set aside for testing and validation purposes. For example, an official test set of five randomly selected, labeled/annotated images was set aside for all experiments, except for cross-validation studies. Of the remaining seventy-five image sets, all or a random subset were randomly divided into training (80%) and validation (20%) sets.\n\nThe datasets were also used to explore the relationships between image similarity and CNN model accuracy. Various metrics for image similarity were compared with the relative average precision of the CNNs trained on the generated FLIM images. The results showed that image similarity metrics may provide a straightforward way to estimate how well the generated FLIM images will compare with ground truth in CNN performance.\n\nNot sure about the exact number of data points, as it is not explicitly stated in the provided information. However, it is mentioned that an official test set of five randomly selected, labeled/annotated images was set aside, and that seventy-five image sets were used for training and validation.",
  "dataset/splits": "For all cell instance segmentation and cell classification studies, except when using cross-validation, the same randomly selected group of twelve training, three validation, and five testing image sets were used. For the dense U-Net CNN for FLIM prediction from fluorescence intensity, an official test set of five randomly selected, labeled/annotated images were set aside to be used for all experiments, except for cross-validation studies. Of the remaining seventy-five image sets, all or a random subset were randomly divided into training (80%) and validation (20%) sets.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The datasets used in this study are publicly available. This includes the FLIM images, cell nuclei masks, and the cell/tissue-type classification reference table. These datasets can be accessed on Figshare. The specific link provided for these datasets is https://doi.org/10.6084/m9.figshare.21183517. The datasets are available under a license that allows for public access and use, facilitating reproducibility and further research by other scientists.\n\nThe code used to generate the RGB images from the source image data, to generate and train the dense U-Net CNN, and to train the cell classification models is also publicly available. This code can be found on GitHub at the following link: https://github.com/jalexs82/flim_colorization. This ensures that the methods and models used in the study are transparent and can be replicated by others.\n\nThe availability of these datasets and code ensures that the research is reproducible and that other researchers can build upon the findings presented in this study. The public release of the datasets and code is enforced through the platforms Figshare and GitHub, which provide mechanisms for version control and access management. This ensures that the data and code remain accessible and usable over time.",
  "optimization/algorithm": "The optimization algorithm used in our study is the Adam optimizer, which is a widely-used class of machine-learning algorithms known as stochastic gradient descent optimizers. It is not a new algorithm; it was introduced by Diederik P. Kingma and Jimmy Ba in 2014. The Adam optimizer is popular due to its efficiency and effectiveness in training deep learning models, as it combines the advantages of two other extensions of stochastic gradient descent.\n\nThe reason the Adam optimizer is discussed in a scientific publication focused on cell instance segmentation and classification, rather than a machine-learning journal, is that our work primarily contributes to the field of biological imaging and cell analysis. The optimization algorithm is a crucial component of our methodology, enabling the training of complex convolutional neural network (CNN) architectures. However, the main focus of our study is on the application of these models to biological data, rather than the development of new optimization algorithms.\n\nThe Adam optimizer was chosen for its ability to handle sparse gradients on noisy problems, which is beneficial for the training of deep neural networks. The specific parameters used for the Adam optimizer in our study include a base learning rate of 10^-5, \u03b21 of 0.9, \u03b22 of 0.999, and \u03f5 of 10^-8. These parameters were selected based on empirical performance and are consistent with common practices in the field. The optimizer was used in conjunction with a total loss function consisting of the sum of the balanced, class-weighted dice loss and categorical focal loss, which is well-suited for the segmentation and classification tasks addressed in our research.",
  "optimization/meta": "The model described in the publication does not function as a meta-predictor. Instead, it employs a dense U-Net convolutional neural network (CNN) for specific tasks such as predicting fluorescence lifetime imaging microscopy (FLIM) data from fluorescence intensity images. The dense U-Net CNN is trained using data augmentation techniques like random patch selection, flipping, and rotation. The training process involves using a combination of balanced, class-weighted dice loss and categorical focal loss, optimized with the Adam optimizer.\n\nThe model does not use data from other machine-learning algorithms as input. Instead, it directly processes input images, such as fluorescence intensity images, to predict outputs like FLIM data or cell segmentation masks. The training data for the dense U-Net CNN consists of fluorescence intensity images and corresponding FLIM data, ensuring that the training process is independent and focused on learning the relationship between these two types of images.\n\nThe dense U-Net CNN architecture includes encoding and decoding layers with short skip connections, allowing it to capture both local and global features effectively. The model is trained for a fixed number of epochs, with the weights saved for the epoch that achieves the lowest validation loss. This approach ensures that the model generalizes well to unseen data.\n\nIn summary, the model described in the publication is a standalone dense U-Net CNN designed for specific image prediction tasks. It does not rely on data from other machine-learning algorithms as input and ensures independent training data to achieve accurate predictions.",
  "optimization/encoding": "The data encoding process involved transforming phasor images into a format suitable for machine learning algorithms. The dot product between the phasor matrix of each image and a matrix W yielded a 1D transformed matrix. This matrix was then reshaped into the original 2D image shape, resulting in an LDA transformed image. This image was used as the hue component and was stacked with the fluorescence intensity and a dummy saturation matrix set at maximum value. The combined data was then transformed into the RGB color space.\n\nFor the dense U-Net CNN used in FLIM prediction, the phasor images were segmented into K classes using the K-means algorithm. This involved gathering phasor images for a given acquisition setting, determining a background threshold from the fluorescence intensity images, and pooling phasor values corresponding to pixels above that threshold. The K-means algorithm was then used to determine K cluster center values on the pooled data. Segmentations were generated for each image by determining the closest cluster center for each pixel using its phasor values. These segmentations were combined with the background threshold to create \"K + 1\"-class segmentation masks for FLIM prediction.\n\nData augmentation techniques were applied to the input fluorescence intensity images and output segmentation. These techniques included random selection of the patch window, random flipping across the x and/or y axis, and random rotation by 0, 90, 180, or 270 degrees. The total loss used for training consisted of the sum of the balanced, class-weighted dice loss and categorical focal loss. The Adam optimizer was used with a base learning rate of 10^-5, \u03b21 of 0.9, \u03b22 of 0.999, and \u03f5 of 10^-8. Training was performed for 400 epochs, with the model weights saved for the epoch with the lowest validation loss.\n\nFor cell instance segmentation and classification, the same randomly selected group of twelve training, three validation, and five testing image sets were used. Models were provided the RGB images as training input and cell-nuclei masks or class labels for output prediction. Additional model details are available in the supplementary material. Cell instance segmentation was tested with three CNN architectures: Mask R-CNN with either a ResNet-50 or ResNet-101 backbone, and Stardist. Model weights were either initialized at random or from a model trained on fluorescence images of cell nuclei from the Data Science Bowl 2018 dataset. Training was performed over a total of 280 epochs (Mask R-CNN) or 400 epochs (Stardist) with the model weights saved for the epoch with the lowest validation loss.\n\nCell classification was tested with eight CNN architectures: DenseNet-121, EfficientNetB5, Inception-v3, ResNet-50 and ResNet-101, VGG16 and VGG19, and Xception. Model weights were either initialized at random or from ImageNet. To generate images during training, cells were randomly selected and cropped within a \u00b1 5 pixel distance from the cell-nuclei center. These cropped images were further augmented by random flipping and rotation, and then resized to the input dimensions and preprocessed and normalized to the specifications of the CNN. Training was conducted over 100 epochs using the RMSprop optimizer and a learning rate of 2 \u00d7 10^-6.",
  "optimization/parameters": "The models used in this study varied in their architectures and thus in the number of parameters. For cell instance segmentation, three convolutional neural network (CNN) architectures were tested: Mask R-CNN with either a ResNet-50 or ResNet-101 backbone, and Stardist. The ResNet-50 backbone typically has around 23 million parameters, while the ResNet-101 backbone has approximately 44 million parameters. Stardist, on the other hand, is a specialized architecture for star-convex objects and has fewer parameters compared to the ResNet backbones.\n\nFor cell classification, eight different CNN architectures were employed: DenseNet-121, EfficientNetB5, Inception-v3, ResNet-50, ResNet-101, VGG16, VGG19, and Xception. The number of parameters in these models ranges significantly. For instance, DenseNet-121 has around 8 million parameters, EfficientNetB5 has about 30 million, Inception-v3 has approximately 23 million, VGG16 has around 138 million, and Xception has about 22 million parameters.\n\nThe selection of these architectures and their corresponding parameters was based on their established performance in similar tasks and their availability in popular deep learning frameworks. The choice of model and parameters was also influenced by the specific requirements of the tasks, such as the need for high accuracy in cell instance segmentation or the efficiency required for cell classification. Additionally, the use of pretrained models, such as those initialized with weights from the Data Science Bowl 2018 dataset or ImageNet, helped in leveraging learned features and reducing the need for extensive training from scratch.",
  "optimization/features": "The input features for the models varied depending on the specific task. For cell instance segmentation and classification, the models were provided with RGB images as training input. These RGB images consist of three channels, corresponding to the red, green, and blue color components. Therefore, the number of features used as input for these tasks is three.\n\nFor the dense U-Net CNN used for FLIM prediction, the input features consisted of fluorescence intensity images. These images could be single-channel or dual-channel, depending on the specific experiment. The patch size used for training was 256 \u00d7 256 pixels. Data augmentation techniques, such as random selection of the patch window, random flipping, and random rotation, were applied to these input features.\n\nFeature selection was not explicitly mentioned as a separate step in the process. The models were trained using the entire set of features available in the input images. The use of data augmentation techniques helped to increase the diversity of the training data without explicitly selecting specific features. The models were trained and validated using a fixed set of image sets, ensuring that the same features were used consistently across different experiments.",
  "optimization/fitting": "The fitting method employed in this study involves training a dense U-Net convolutional neural network (CNN) to predict fluorescence lifetime imaging microscopy (FLIM) data from fluorescence intensity images. The number of parameters in the dense U-Net CNN is indeed much larger than the number of training points, which is a common scenario in deep learning.\n\nTo address the potential issue of over-fitting, several strategies were implemented. Firstly, data augmentation techniques were used, including random selection of patch windows, random flipping across the x and/or y axis, and random rotations of 0, 90, 180, or 270 degrees. This helps to increase the effective size of the training dataset and makes the model more robust to variations in the input data.\n\nSecondly, a combination of balanced, class-weighted dice loss and categorical focal loss was used as the total loss function. This loss function helps to balance the contribution of different classes, especially when dealing with imbalanced datasets, and focuses more on hard-to-classify examples, reducing the risk of over-fitting to the easier examples.\n\nAdditionally, the model was trained for a large number of epochs (400), but the weights were saved only for the epoch with the lowest validation loss. This early stopping technique helps to prevent over-fitting by stopping the training process when the model performance on the validation set starts to degrade.\n\nTo rule out under-fitting, the model architecture was designed to be sufficiently complex, with four encoding layers, four decoding layers, and a bridge layer, employing two successive 3\u00d73 kernel convolutions with ReLU activation at each layer. The encoding portion began with 64-feature channels, increasing two-fold with each layer until a total of 1,024, while the decoding portion decreased feature channels two-fold back to the original 64. This architecture allows the model to learn complex representations of the input data.\n\nFurthermore, the use of a dense U-Net CNN with added short skip connections at each layer helps to preserve spatial information and improves the model's ability to learn from the data. The model was also trained using the Adam optimizer with a base learning rate of 10^-5, which is a commonly used optimizer that adapts the learning rate for each parameter, helping to converge to a good solution.\n\nIn summary, the fitting method employed in this study addresses the potential issues of over-fitting and under-fitting through the use of data augmentation, a combination of loss functions, early stopping, a complex model architecture, and an adaptive optimizer.",
  "optimization/regularization": "In our study, several regularization methods were employed to prevent overfitting and improve the generalization of our models. Data augmentation was extensively used, involving random selection of patch windows, random flipping across the x and/or y axis, and random rotations of 0, 90, 180, or 270 degrees. This technique helped to increase the diversity of the training data, making the models more robust and less likely to overfit to specific patterns in the training set.\n\nAdditionally, we utilized a combination of balanced, class-weighted dice loss and categorical focal loss for training our models. This loss function helped to address class imbalances, ensuring that the models did not become biased towards more frequent classes. The Adam optimizer was used with specific parameters, including a base learning rate of 10^-5, \u03b21 of 0.9, \u03b22 of 0.999, and \u03f5 of 10^-8, which contributed to stable and efficient training.\n\nFor the dense U-Net CNN, short skip connections were added at each layer, which helped in preserving spatial information and improving the model's ability to learn fine details. This architectural choice aided in reducing overfitting by ensuring that the model could effectively propagate gradients during training.\n\nFurthermore, model weights were saved for the epoch with the lowest validation loss, ensuring that the final models were selected based on their performance on a separate validation set. This approach helped in selecting the best-performing models and prevented overfitting to the training data.\n\nIn summary, a combination of data augmentation, a balanced loss function, careful optimization settings, and architectural choices were employed to prevent overfitting and enhance the performance of our models.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are detailed within the publication. Specifically, for cell instance segmentation and classification, we employed various convolutional neural network (CNN) architectures, including Mask R-CNN with ResNet-50 and ResNet-101 backbones, and Stardist. The training involved random patch selection, flipping, and rotation for data augmentation. The models were trained for 280 epochs (Mask R-CNN) or 400 epochs (Stardist), with model weights saved for the epoch yielding the lowest validation loss.\n\nFor cell classification, we tested eight different CNN architectures, such as DenseNet-121, EfficientNetB5, and ResNet-50, among others. Training was conducted over 100 epochs using the RMSprop optimizer with a learning rate of 2 \u00d7 10^-6. The models were initialized either randomly or with weights from ImageNet.\n\nThe dense U-Net CNN used for FLIM prediction from fluorescence intensity was trained for 400 epochs with the Adam optimizer, utilizing a base learning rate of 10^-5. The loss function combined balanced, class-weighted dice loss and categorical focal loss.\n\nModel files and optimization parameters are not explicitly provided in the main text but are available in the Supplementary Material Appendix. The supplementary material includes additional details on the evaluation metrics and model configurations. For access to the specific model files and further technical details, readers are directed to the Supplementary Material, which is available online. The licensing information for the models and code used is not specified in the provided context.",
  "model/interpretability": "The models employed in our study are primarily convolutional neural networks (CNNs), which are generally considered black-box models due to their complex, multi-layered architectures. These models, including Mask R-CNN, Stardist, DenseNet-121, EfficientNetB5, Inception-v3, ResNet-50, ResNet-101, VGG16, VGG19, and Xception, are designed to learn hierarchical features from input data but do not inherently provide interpretability regarding how specific decisions are made.\n\nHowever, certain aspects of our approach do offer some level of transparency. For instance, the use of data augmentation techniques such as random flipping and rotation helps in understanding the model's robustness to variations in input data. Additionally, the use of well-established architectures like U-Net and its dense variant provides a structured framework that, while complex, follows a clear pattern of encoding and decoding layers.\n\nIn the context of cell instance segmentation and classification, the models were trained using RGB images and cell-nuclei masks or class labels. The random selection of patches and the application of transformations like rotation and flipping ensure that the models learn to generalize from a variety of input conditions. This process, while not making the model transparent, does provide insights into the types of variations the model can handle.\n\nFor the dense U-Net CNN used in FLIM prediction, the architecture includes short skip connections at each layer, which help in preserving spatial information. This design choice can be seen as a form of transparency, as it indicates a deliberate effort to maintain the integrity of the input data throughout the network.\n\nFurthermore, the evaluation metrics and the use of cross-validation provide a means to assess the model's performance and reliability. While these do not make the model transparent, they offer a way to understand its strengths and limitations.\n\nIn summary, while the models themselves are black-box in nature, the methodologies and architectural choices provide some level of interpretability. The use of established techniques and the structured approach to training and evaluation offer insights into how the models operate and perform.",
  "model/output": "The model's output is designed for both classification and regression tasks, depending on the specific implementation. For cell instance segmentation and classification, the model outputs cell-nuclei masks for instance segmentation or class labels for classification. These outputs are used for predicting the presence and location of cell nuclei and labeling the associated pixels.\n\nIn the context of FLIM prediction, the model can be configured to output either discrete K-means phasor classes or continuous predictions of fluorescence intensity and phasor coordinates. The discrete output is achieved by using a SoftMax activation function in the final convolutional layer, which is suitable for classification tasks. This approach allows for balancing the loss function to reduce the issue of under-predicting uncommon outcomes, such as phasor values found in relatively few pixels.\n\nAlternatively, for regression tasks, the model omits the activation function in the final convolutional layer and uses mean-squared error as the loss function. This configuration enables the prediction of continuous values, such as fluorescence intensity and phasor coordinates.\n\nThe choice between classification and regression outputs depends on the specific requirements of the task at hand. The discrete K-means output provides advantages such as the ability to balance the loss function and generate accurate segmentation without supervision. On the other hand, the continuous output is useful for predicting fluorescence intensity and phasor coordinates directly.",
  "model/duration": "The execution time for the models varied depending on the specific architecture and task. For cell instance segmentation, training was performed over 280 epochs for Mask R-CNN and 400 epochs for Stardist. Similarly, for cell classification, training was conducted over 100 epochs. The models were trained using a GeForce RTX 3090 GPU, which significantly accelerated the training process. However, the exact execution time for each epoch was not specified, as it can vary based on factors such as data augmentation, batch size, and the complexity of the input images. The models were trained until the epoch with the lowest validation loss, ensuring optimal performance. Additionally, for cross-channel fluorescence and FLIM prediction, a dense U-Net CNN was used, trained for 400 epochs with similar hardware and optimization techniques. The use of data augmentation, including random patch selection, flipping, and rotation, added to the computational load but enhanced the models' robustness and generalization capabilities.",
  "model/availability": "The source code for generating RGB images from the source image data, training the dense U-Net CNN, and training the cell classification models is publicly available on GitHub. The repository can be found at https://github.com/jalexs82/flim_colorization. This repository includes the necessary code to replicate the methods and models described in the publication. The code is released under a permissive license, allowing for its use and modification by the research community. Additionally, the datasets, including the FLIM images, cell nuclei masks, and the cell/tissue-type classification reference table, are available on Figshare at https://doi.org/10.6084/m9.figshare.21183517. This ensures that other researchers can access and utilize the data and code to reproduce the results or build upon the work.",
  "evaluation/method": "The evaluation of the methods employed in this study involved several key approaches to ensure robustness and accuracy. For cell instance segmentation and classification, a consistent set of image datasets was used, with twelve images designated for training, three for validation, and five for testing. This division was maintained across all studies, except when cross-validation was utilized. The models were trained using RGB images as input and were evaluated based on their ability to predict cell-nuclei masks for instance segmentation or class labels for classification.\n\nThe performance of the models was assessed using various evaluation metrics, with detailed information available in the Supplementary Material Appendix. For cell instance segmentation, three different CNN architectures were tested: Mask R-CNN with ResNet-50 or ResNet-101 backbones, and Stardist. These models were trained over 280 epochs for Mask R-CNN and 400 epochs for Stardist, with model weights saved for the epoch yielding the lowest validation loss. The evaluation focused on the average precision of cell detection, considering the Intersection over Union (IoU) metric.\n\nIn the context of cell classification, eight CNN architectures were evaluated, including DenseNet-121, EfficientNetB5, Inception-v3, ResNet-50 and ResNet-101, VGG16 and VGG19, and Xception. Training was conducted over 100 epochs using the RMSprop optimizer. The models were assessed based on their classification accuracy, with a particular focus on how well they performed when trained on different types of input data, such as single or dual-channel fluorescence images, ground-truth LDA+ HV color-FLIM images, or generated LDA+ HV color-FLIM images.\n\nAdditionally, the study explored the benefit of FLIM image data over traditional fluorescence for CNN prediction. This was done by training a dense U-Net to predict fluorescence or FLIM image data cross-channel. The U-Net was evaluated on its ability to generate images that closely matched the ground truth, with a particular emphasis on the visual quality and absence of artifacts in the generated images. The relationships between image similarity and CNN model accuracy were also investigated, using various metrics to compare the performance of generated FLIM images against ground-truth FLIM images.\n\nOverall, the evaluation methods included a combination of standardized datasets, cross-validation where applicable, and detailed performance metrics to ensure a comprehensive assessment of the models' capabilities. The focus was on both the accuracy of predictions and the visual quality of the generated images, providing a thorough evaluation of the methods employed.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our models, focusing on both cell instance segmentation and cell classification tasks.\n\nFor cell instance segmentation, we primarily used average precision as our accuracy metric, evaluated at an intersection over union (IoU) threshold of 0.7. This metric is widely used in object detection and segmentation tasks, providing a comprehensive evaluation of the model's performance by considering both precision and recall. We compared the average precision of models trained on different types of images, including generated FLIM images, ground-truth FLIM images, and regular fluorescence images. This allowed us to assess the impact of different image types on segmentation performance.\n\nIn addition to average precision, we also analyzed the relationships between image similarity metrics and CNN model accuracy. We explored various image similarity metrics and found that, while no single metric consistently explained a high proportion of the variance in relative average precision, a combination of these metrics could account for a significant portion of the variance. This suggests that image similarity metrics can be useful in estimating how well generated FLIM images will compare with ground truth in CNN performance.\n\nFor cell classification, we used receiver operating characteristic (ROC) curves and average precision to evaluate the performance of different CNN architectures. We compared the classification accuracy of models trained on fluorescence images, ground-truth FLIM images, and generated FLIM images. This allowed us to assess the benefit of FLIM data over traditional fluorescence for CNN prediction tasks.\n\nOverall, the performance metrics we reported are representative of those commonly used in the literature for similar tasks. Our use of average precision for segmentation and ROC curves and average precision for classification provides a comprehensive evaluation of our models' performance. Additionally, our exploration of image similarity metrics offers a novel approach to estimating the effectiveness of generated FLIM images in CNN tasks.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of various methods and baselines to evaluate the performance of our approach. We compared the use of FLIM images against regular fluorescence confocal images for cell detection and classification tasks. Specifically, we evaluated the performance of Mask R-CNN with ResNet-50 and ResNet-101 backbones, as well as Stardist, using both FLIM and fluorescence images. The FLIM images consistently yielded higher accuracy for cell instance segmentation and classification compared to single-channel fluorescence images.\n\nWe also explored the impact of different colorization techniques on the performance of these models. FLIM images transformed using Linear Discriminant Analysis (LDA) combined with Hue and Value (HV) colorization generally provided the highest accuracies. This was true for both cell instance segmentation and classification tasks, indicating that the choice of colorization method can significantly affect the performance of convolutional neural networks (CNNs).\n\nAdditionally, we compared the performance of models trained with and without pretraining on the Data Science Bowl (DSB) 2018 dataset. Pretraining improved the accuracy across all architectures and types of input images, highlighting the benefit of leveraging pre-existing models trained on large datasets.\n\nFor cell classification, we tested eight different CNN architectures, including DenseNet-121, EfficientNetB5, Inception-v3, ResNet-50, ResNet-101, VGG16, VGG19, and Xception. The FLIM images acquired with 570nm excitation generally yielded higher average precision across all classes compared to single and dual-channel fluorescence images. This suggests that FLIM images provide more relevant information for cell classification tasks.\n\nOverall, our comparisons demonstrate that FLIM images, particularly when transformed using LDA+HV colorization, offer superior performance for both cell instance segmentation and classification tasks. The use of pretrained models further enhances this performance, making our approach a robust and effective method for analyzing cellular images.",
  "evaluation/confidence": "Evaluation confidence is a critical aspect of assessing the robustness and reliability of the presented methods. The performance metrics used in this study include average precision, which is evaluated at an intersection over union (IoU) threshold of 0.7. This metric provides a clear indication of the model's accuracy in cell detection and classification tasks.\n\nStatistical significance is addressed through various comparisons. For instance, the performance of different CNN architectures is compared using FLIM images and regular fluorescence images. The results indicate that FLIM images generally yield higher accuracy than corresponding single-channel fluorescence images. This suggests that the methods using FLIM data are statistically superior in these specific tasks.\n\nThe study also explores the impact of different colorization techniques on CNN performance. It is noted that FLIM images transformed using LDA+ HV yield the highest accuracies for the top-performing models. This consistency across different architectures and colorization methods strengthens the confidence in the results.\n\nAdditionally, the use of ImageNet pretraining is shown to improve accuracy for all architectures and types of input images. This further supports the reliability of the methods, as pretraining on a large, diverse dataset is known to enhance model performance.\n\nThe evaluation also includes cross-channel prediction tasks, where the U-Net is trained to predict FLIM images from fluorescence data and vice versa. The generated images from the continuous output are highly similar to those from the multiclass output, and the downstream CNN accuracies are nearly equivalent. This demonstrates the robustness of the methods across different prediction tasks.\n\nOverall, the performance metrics and statistical comparisons provide strong evidence of the methods' superiority over baselines and other approaches. The consistent improvement in accuracy across various tasks and architectures indicates high confidence in the results.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The evaluation metrics and results are detailed in the Supplementary Material Appendix, but the specific files used for evaluation are not released. The study focuses on presenting the methods, results, and metrics used to evaluate the performance of the models, rather than providing the raw data files. For further details on the evaluation process and metrics, readers are directed to the Supplementary Material."
}