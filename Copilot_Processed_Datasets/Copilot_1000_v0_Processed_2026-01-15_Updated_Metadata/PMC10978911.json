{
  "publication/title": "SANS-CNN: An automated machine learning technique for spaceflight associated neuro-ocular syndrome with astronaut imaging data.",
  "publication/authors": "Kamran SA, Hossain KF, Ong J, Zaman N, Waisberg E, Paladugu P, Lee AG, Tavakkoli A",
  "publication/journal": "NPJ microgravity",
  "publication/year": "2024",
  "publication/pmid": "38548790",
  "publication/pmcid": "PMC10978911",
  "publication/doi": "10.1038/s41526-024-00364-w",
  "publication/tags": "- Spaceflight Associated Neuro-Ocular Syndrome (SANS)\n- Machine Learning\n- Convolutional Neural Networks (CNN)\n- Optical Coherence Tomography (OCT)\n- Astronaut Health\n- Deep Learning\n- Medical Imaging\n- EfficientNet\n- Space Medicine\n- Automated Detection",
  "dataset/provenance": "The dataset used in our study consists of Optical Coherence Tomography (OCT) B-scans provided by NASA. These scans span across many years and were evaluated by different experts, which introduces some variability in the data. The dataset includes images from both healthy individuals and those with Space-Associated Neuro-Ocular Syndrome (SANS).\n\nFor our experimentation, we utilized a total of 6990 OCT B-scans. Specifically, we used 2506 SANS and 3797 normal OCT B-scans for training. For validation, we incorporated 627 SANS and 950 normal OCT B-scans. The hold-out test set consisted of 467 SANS and 478 normal images. All images were normalized to have pixel intensities between 0 and 1 and were pre-processed to a resolution of 512 \u00d7 512 pixels.\n\nThe control images were taken from pre-flight, in-flight, and post-flight OCT volumes, while the SANS images were obtained from in-flight and post-flight OCT volumes. This dataset has been used specifically for our study on detecting SANS using deep learning techniques. To the best of our knowledge, this particular dataset has not been widely used by the community in previous studies.",
  "dataset/splits": "The dataset used for experimentation consists of three main splits: training, validation, and testing. These splits are based on unique astronauts to ensure that the data is independent across the different sets.\n\nFor the training split, a total of 6303 OCT B-scan images were utilized. Specifically, this includes 2506 SANS images and 3797 Normal OCT B-scans.\n\nThe validation split comprises 1577 OCT B-scans, with 627 SANS images and 950 Normal OCT B-scans.\n\nThe testing split, which serves as a hold-out set, contains 945 images. This includes 467 SANS images and 478 Normal images.\n\nAll images across the splits were normalized to have pixel intensities ranging from 0 to 1, and they were pre-processed to a resolution of 512 \u00d7 512 pixels. This standardization ensures consistency in the data used for training, validation, and testing.",
  "dataset/redundancy": "The datasets used in our study were split based on unique astronauts to ensure independence between the training, validation, and testing sets. This approach was chosen to prevent data leakage and to mimic real-world scenarios where the model would encounter new, unseen data.\n\nThe Optical Coherence Tomography (OCT) B-scans were categorized into control and Spaceflight-Associated Neuro-Ocular Syndrome (SANS) images. Control images were obtained from pre-flight, in-flight, and post-flight OCT volumes, while SANS images were taken from in-flight and post-flight OCT volumes. This separation helped in maintaining the integrity of the datasets and avoiding any overlap between them.\n\nFor training, we utilized 2506 SANS and 3797 Normal OCT B-scans. The validation set consisted of 627 SANS and 950 Normal OCT B-scans. The hold-out test set included 467 SANS and 478 Normal images. All images were normalized to have pixel intensities ranging from 0 to 1 and were pre-processed to a resolution of 512 \u00d7 512.\n\nThe distribution of our datasets differs from some previously published machine learning datasets in that we specifically ensured that the data from each astronaut was used in only one of the splits (training, validation, or testing). This strict separation is crucial in medical imaging, especially when dealing with a limited number of subjects, to avoid overfitting and to ensure that the model's performance is generalizable to new data.\n\nIn summary, the datasets were carefully split to maintain independence and to reflect the unique challenges of working with medical imaging data from a limited pool of subjects. This approach helps in building a robust model that can be reliably used in real-world applications.",
  "dataset/availability": "The supporting astronaut data used in this study is not publicly available due to the nature of the research. This decision is made to protect the privacy and confidentiality of the astronauts involved. The data includes sensitive medical information that is not suitable for public dissemination. Therefore, the dataset, including the data splits used for training, validation, and testing, is not released in any public forum. The data is kept secure and is only accessible to authorized researchers involved in the study. This ensures that the data is used responsibly and ethically, adhering to the guidelines and regulations set by the relevant authorities.",
  "optimization/algorithm": "The optimization algorithm employed in our study is the Adam optimizer, which is a widely-used class of stochastic gradient descent algorithms. Adam is known for its efficiency and effectiveness in adapting learning rates for each parameter, making it suitable for a variety of deep learning tasks.\n\nThe Adam optimizer is not a new algorithm; it has been extensively used and validated in the machine learning community. It was introduced by Diederik P. Kingma and Jimmy Ba in their 2014 paper titled \"Adam: A Method for Stochastic Optimization.\" The decision to use Adam in our study was driven by its proven track record in handling sparse gradients on noisy problems, which is common in deep learning tasks, including our specific application in detecting spaceflight-associated neuro-ocular syndrome (SANS) from optical coherence tomography (OCT) images.\n\nGiven that Adam is a well-established algorithm, it was not necessary to publish it in a machine-learning journal. Instead, our focus was on applying this robust optimization technique to our specific problem domain, which involves the automated detection of SANS using convolutional neural networks (CNNs). The use of Adam allowed us to efficiently train our models, achieving high accuracy and other performance metrics as reported in our results.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it directly processes OCT B-scans to classify images as either SANS or Normal. The architecture consists of an encoder, which takes OCT B-scans as input, a decoder, and an output layer for prediction. The encoder utilizes pre-trained networks with multiple residual and downsampling blocks, specifically from architectures like ResNet50-v2, MobileNet-v2, and EfficientNet-v2. These architectures were trained on the ImageNet 2012 dataset. The model employs a supervised cross-entropy loss function for training, and the data used for training, validation, and testing is derived from OCT volumes of astronauts, ensuring that the datasets are independent and separated based on unique astronauts. The training process involves the Adam optimizer with specific learning rate adjustments and callbacks to enhance performance.",
  "optimization/encoding": "The data used for our experimentation consisted of OCT B-scans, which were separated into training, validation, and testing sets based on unique astronauts. The control images were sourced from pre-flight, in-flight, and post-flight OCT volumes, while the SANS images were taken from in-flight and post-flight OCT volumes. For training, we utilized 2506 SANS and 3797 Normal OCT B-scans. The validation set included 627 SANS and 950 Normal OCT B-scans, and the test set consisted of 467 SANS and 478 Normal images.\n\nAll images were normalized to have pixel intensities ranging from 0 to 1, scaled down from the original 0 to 255 range. Additionally, the images were pre-processed to a resolution of 512 \u00d7 512 pixels. This normalization and resizing ensured consistency across the dataset, facilitating better training and performance of the machine-learning models.\n\nThe labels used for the images were \"Normal\" and \"SANS,\" and we employed a supervised cross-entropy loss function for training the models. The objective function incorporated weighted categorical cross-entropy loss, where the weights were chosen based on the number of samples per class. Specifically, a weight of 0.83 was assigned to the majority class (Normal) and 1.257 to the minority class (SANS) to address class imbalance. This weighting scheme helped in giving more importance to the minority class during training, thereby improving the model's ability to accurately classify SANS cases.",
  "optimization/parameters": "In our study, we utilized a lightweight convolutional neural network (CNN) incorporating an EfficientNet encoder for detecting Spaceflight Associated Neuro-Ocular Syndrome (SANS) from Optical Coherence Tomography (OCT) images. The EfficientNet architecture is known for its efficiency and scalability, balancing network depth, width, and resolution.\n\nThe specific parameters (p) used in the model were determined through a combination of empirical testing and hyperparameter tuning. We employed the Adam optimizer with an initial learning rate of 0.0001. The batch size was set to 16, and the model was trained for 30 epochs. Class weights were chosen to address the imbalance in the dataset, with a weight of 0.83 for the majority class (Normal) and 1.257 for the minority class (SANS).\n\nThe selection of these parameters was guided by the need to optimize the model's performance while ensuring computational efficiency, which is crucial for deployment in resource-limited environments such as space missions. The use of callbacks, including \"Reduce Learning Rate on Plateau\" and \"Model Checkpointer,\" further aided in fine-tuning the model during training. The \"Reduce Learning Rate on Plateau\" callback reduced the learning rate by 0.1 if the validation loss did not decrease for six consecutive epochs, while the \"Model Checkpointer\" saved the best model weights for each epoch.",
  "optimization/features": "The input features for our model consist of OCT B-scan images. Specifically, we utilized 6303 OCT B-scan images for training and validation, and 945 for testing. These images were preprocessed to have a resolution size of 512 \u00d7 512 and were normalized to have values between 0 and 1 from 0\u2013255-pixel intensities.\n\nFeature selection in the traditional sense was not performed, as we are dealing with raw image data. Instead, the model learns relevant features directly from the images during the training process. The images used for training, validation, and testing were separated based on unique astronauts to ensure that the model generalizes well to new, unseen data. This approach helps in avoiding data leakage and ensures that the model's performance is evaluated on truly independent data.\n\nThe images were categorized into two classes: SANS and Normal. For training, we used 2506 SANS and 3797 Normal OCT B-scans. For validation, we incorporated 627 SANS and 950 Normal OCT B-scans. For the hold-out test set, we utilized 467 SANS and 478 Normal images. This separation ensures that the model is trained and validated on a diverse set of images, enhancing its robustness and generalizability.",
  "optimization/fitting": "The fitting method employed in our study utilized convolutional neural networks (CNNs) to classify optical coherence tomography (OCT) B-scans into SANS and Normal categories. The models were trained using a substantial dataset, with 2506 SANS and 3797 Normal images for training, 627 SANS and 950 Normal images for validation, and 467 SANS and 478 Normal images for testing. This ensured that the number of parameters in our models was not excessively larger than the number of training points, mitigating the risk of overfitting.\n\nTo further address overfitting, several techniques were implemented. Firstly, data augmentation was used to artificially increase the diversity of the training dataset. Secondly, dropout layers were incorporated into the neural network architectures to prevent the model from becoming too reliant on specific neurons. Additionally, early stopping was employed by using the \"Reduce Learning rate on Plateau\" callback, which decreased the learning rate by a factor of 0.1 if the validation loss did not improve for six consecutive epochs. This helped in preventing the model from overfitting to the training data.\n\nUnderfitting was addressed by ensuring that the models had sufficient capacity to learn the underlying patterns in the data. This was achieved by using well-established and powerful CNN architectures such as EfficientNet-v2, ResNet50-v2, and MobileNet-v2. These architectures have been proven to be effective in various image classification tasks. Furthermore, the models were trained for a sufficient number of epochs (30 epochs) to allow them to learn the necessary features from the data. The use of class weights during training also helped in balancing the contribution of the minority class (SANS) and the majority class (Normal), ensuring that the model did not underfit the minority class.\n\nThe performance of the models was evaluated using multiple metrics, including accuracy, sensitivity, specificity, precision, and F1-score. The best-performing model, EfficientNet-v2, achieved high scores across all metrics, indicating that it was neither overfitting nor underfitting the data. The qualitative results, visualized using GRAD-CAM and GRAD-CAM++, further confirmed the model's ability to focus on relevant features in the OCT B-scans, providing visual justifications for the classification decisions.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and enhance the generalization of our models. One key method used was the \"Reduce Learning Rate on Plateau\" callback. This technique monitors the validation loss during training, and if it does not improve for a specified number of epochs (in our case, six epochs), the learning rate is reduced by a factor of 0.1. This helps in fine-tuning the model by allowing it to converge more smoothly and avoid getting stuck in local minima.\n\nAdditionally, we utilized the \"Model Checkpointer\" callback, which saves the best model weights based on the validation performance. This ensures that we retain the model configuration that performs best on the validation set, rather than the final model after all epochs, which might be overfitted to the training data.\n\nWe also incorporated class weights to address the imbalance in our dataset. The majority class (Normal) was assigned a weight of 0.83, while the minority class (SANS) was given a weight of 1.257. This weighting scheme helps the model to pay more attention to the minority class during training, thereby improving its ability to recognize SANS images accurately.\n\nFurthermore, we normalized all images to have pixel intensities between 0 and 1, which helps in stabilizing and speeding up the training process. This preprocessing step is crucial for ensuring that the model does not get biased by the range of pixel values in the input images.\n\nBy combining these regularization techniques, we aimed to build robust models that generalize well to unseen data, which is particularly important for the automated detection of SANS in space exploration scenarios.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we utilized the Adam optimizer with an initial learning rate of 0.0001, a mini-batch size of 16, and trained our models for 30 epochs. We also employed two callbacks: \"Reduce Learning Rate on Plateau\" and \"Model Checkpointer.\" The former reduces the learning rate by 0.1 if the validation loss does not decrease for six consecutive epochs, while the latter saves the best model weights for each epoch.\n\nRegarding model files and optimization parameters, the code used for our experiments is available upon reasonable request. This includes the specific configurations and parameters that were tuned during the optimization process. However, the supporting astronaut data, due to its sensitive nature, is not publicly available.\n\nFor those interested in replicating or building upon our work, the code and detailed methodologies are accessible under reasonable request, ensuring that the community can benefit from our findings while respecting the confidentiality of the data.",
  "model/interpretability": "The model employed in this study, SANS-CNN, incorporates mechanisms to enhance its interpretability, moving beyond the typical black-box nature of many deep learning models. To achieve this, we utilized GRAD-CAM and GRAD-CAM++ techniques. These methods leverage back-propagated gradients to highlight the regions in the input images that are most influential in the model's decision-making process. By visualizing these activation maps, we can gain insights into which parts of the OCT B-scans the model focuses on when classifying an image as either SANS or Normal.\n\nFor instance, GRAD-CAM generates heatmaps that overlay the original images, indicating areas of high activation. These heatmaps provide a visual justification for the model's predictions, showing which features or regions in the OCT scans are critical for the classification task. This approach allows us to interpret the model's decisions more transparently, making it easier to understand how the model differentiates between SANS and Normal cases.\n\nAdditionally, the use of pre-trained encoders from architectures like Ef\ufb01cientNet-v2, ResNet50-v2, and MobileNet-v2, which have been extensively studied and applied in various vision tasks, adds another layer of interpretability. These architectures have well-understood building blocks, such as residual and downsampling blocks, which help in extracting meaningful features from the input data. The encoder-decoder structure of our model further aids in interpreting how features are processed and transformed through different layers.\n\nIn summary, while deep learning models are often criticized for their black-box nature, the integration of GRAD-CAM and the use of established architectures in SANS-CNN provide a level of transparency. This allows us to better understand and trust the model's predictions, which is crucial for applications in medical imaging and space health.",
  "model/output": "The model is designed for classification. It specifically focuses on distinguishing between two classes: SANS and Normal OCT B-scans. The architecture includes an encoder that processes the input images, a decoder, and an output layer that predicts between these two classes. The labels used are \"Normal\" and \"SANS,\" and the model is trained using a supervised cross-entropy loss function. This setup is typical for classification tasks where the goal is to categorize input data into predefined classes.\n\nThe model's performance is evaluated using several metrics, including accuracy, sensitivity, specificity, precision, and F1-score. These metrics are calculated based on the true positive, true negative, false positive, and false negative rates, which are standard for assessing the effectiveness of classification models. The best-performing model, EfficientNet-v2, achieved an accuracy of 84.2%, sensitivity of 82.8%, specificity of 85.6%, precision of 85.5%, and an F1-score of 84.1%. These results indicate that the model is effective in classifying the images into the correct categories.\n\nThe model utilizes pre-trained encoders from architectures like ResNet50-v2, MobileNet-v2, and EfficientNet-v2, which have been trained on large datasets such as ImageNet. These pre-trained models are fine-tuned for the specific task of classifying SANS and Normal OCT B-scans. The use of pre-trained models helps in leveraging features learned from extensive datasets, improving the model's performance on the target task.\n\nThe training process involves using the Adam optimizer with an initial learning rate of 0.0001, a mini-batch size of 16, and training for 30 epochs. Class weights are assigned to handle the imbalance between the majority (Normal) and minority (SANS) training samples. Additionally, callbacks such as \"Reduce Learning Rate on Plateau\" and \"Model Checkpointer\" are used to optimize the training process and save the best model weights.\n\nThe model's output is further analyzed using techniques like GRAD-CAM and GRAD-CAM++, which provide visual justifications for the classification decisions. These techniques highlight important regions of the images that contribute to the model's predictions, offering insights into how the model makes its classifications. This visualization helps in understanding the model's decision-making process and ensures that it is focusing on relevant features in the images.",
  "model/duration": "The model was trained for 30 epochs using a mini-batch size of 16. The specific execution time for training the model is not provided. However, the training process utilized two callbacks: \"Reduce Learning Rate on Plateau\" and \"Model Checkpointer\". The first callback reduced the learning rate by 0.1 if the validation loss did not decrease for six epochs, while the second callback saved the best snapshot of the model weights for each epoch. These callbacks were implemented to optimize the training process and ensure the model's performance. The training was conducted using the Keras deep learning library with a TensorFlow backend.",
  "model/availability": "The source code for the models and algorithms used in this study is available upon reasonable request. This approach allows for collaboration and further development by other researchers in the field. The code is not publicly released on a platform like GitHub or similar, but interested parties can contact the authors to obtain it. This method ensures that the code is used responsibly and in accordance with the intended research purposes. No executable, web server, virtual machine, or container instance is provided at this time.",
  "evaluation/method": "The evaluation of our method involved a comprehensive approach using a dataset of OCT B-scans separated into training, validation, and testing sets based on unique astronauts. The training set consisted of 2506 SANS and 3797 Normal OCT B-scans, while the validation set included 627 SANS and 950 Normal OCT B-scans. The hold-out test set comprised 467 SANS and 478 Normal images. All images were normalized and pre-processed to a resolution of 512 \u00d7 512.\n\nFor training, we utilized the Adam optimizer with an initial learning rate of 0.0001, a mini-batch size of 16, and trained the models for 30 epochs. Class weights were set to 0.83 for the majority class (Normal) and 1.257 for the minority class (SANS). Two callbacks were employed during training: \"Reduce Learning Rate on Plateau,\" which decreased the learning rate by 0.1 if the validation loss did not improve for six epochs, and \"Model Checkpointer,\" which saved the best model weights for each epoch.\n\nThe performance of the models was evaluated using several metrics: Accuracy, Sensitivity (True Positive Rate), Specificity (True Negative Rate), Precision (Positive Predictive Value), and F1-score. These metrics were calculated using the formulas provided, where N represents the number of samples and K represents the number of classes (K = 2). The metrics were defined as follows:\n\n- Accuracy: The proportion of true results (both true positives and true negatives) among the total number of cases examined.\n- Sensitivity: The proportion of true positives correctly identified by the model.\n- Specificity: The proportion of true negatives correctly identified by the model.\n- Precision: The proportion of true positives among the cases identified as positive by the model.\n- F1-score: The harmonic mean of precision and sensitivity, providing a single metric that balances both concerns.\n\nThe best-performing model, EfficientNet-v2, achieved an accuracy of 84.2%, sensitivity of 82.8%, specificity of 85.6%, precision of 85.5%, and an F1-score of 84.1%. In comparison, ResNet50-v2 showed significantly lower performance across all metrics, while MobileNet-v2 demonstrated a higher sensitivity but lower precision, specificity, F1-score, and accuracy.\n\nTo provide visual justifications for the model's decisions, we employed GRAD-CAM and GRAD-CAM++ techniques. These methods utilize back-propagated gradients to highlight important regions of the images that contribute to the classification decision, offering insights into the model's focus areas.",
  "evaluation/measure": "In our evaluation, we employed a comprehensive set of performance metrics to assess the effectiveness of our models in distinguishing between SANS and Normal OCT B-scans. The metrics we utilized include Accuracy, Sensitivity (True Positive Rate), Specificity (True Negative Rate), Precision (Positive Predictive Value), and F1-score. These metrics provide a well-rounded evaluation of the models' performance, covering various aspects of classification quality.\n\nAccuracy measures the overall correctness of the model's predictions, providing a general sense of how well the model performs. Sensitivity, also known as recall, focuses on the model's ability to correctly identify positive cases, which is crucial for detecting SANS. Specificity evaluates the model's capability to correctly identify negative cases, ensuring that normal images are not misclassified as SANS. Precision assesses the proportion of true positive predictions among all positive predictions, indicating the reliability of the model's positive classifications. The F1-score combines precision and recall into a single metric, offering a balanced measure of the model's performance, especially useful when dealing with imbalanced datasets.\n\nThese metrics are standard in the literature and are widely used in similar studies, ensuring that our evaluation is representative and comparable to other works in the field. By reporting these metrics, we aim to provide a clear and comprehensive understanding of our models' strengths and areas for improvement.",
  "evaluation/comparison": "In our evaluation, we compared the performance of three deep learning models: Ef\ufb01cientNet-v2, MobileNet-v2, and ResNet50-v2. These models were chosen to assess their effectiveness in recognizing SANS (Spaceflight-Associated Neuro-ocular Syndrome) versus normal images using OCT (Optical Coherence Tomography) B-scans.\n\nEf\ufb01cientNet-v2 demonstrated superior overall performance across all six metrics: accuracy, sensitivity, specificity, precision, and F1-score. It achieved an accuracy of 84.2%, sensitivity of 82.8%, specificity of 85.6%, precision of 85.5%, and an F1-score of 84.1%. This model outperformed the others, showing a significant advantage in detecting SANS conditions.\n\nMobileNet-v2, while achieving a notable improvement in sensitivity (94.9%), fell short in other metrics. It had lower precision (64.5%), specificity (46.6%), and accuracy (71.1%), resulting in an F1-score of 76.8%. This indicates that although MobileNet-v2 is effective in identifying true positives, it struggles with false positives and negatives.\n\nResNet50-v2 performed the worst among the three, with all metrics showing 17\u201323% less effectiveness compared to Ef\ufb01cientNet-v2. Its accuracy was 62.8%, sensitivity 63.1%, specificity 62.5%, precision 63.3%, and F1-score 63.2%. This model did not perform well in distinguishing between SANS and normal images.\n\nThe comparison highlights that Ef\ufb01cientNet-v2 is the most reliable model for this task, providing a balanced performance across all evaluated metrics. MobileNet-v2, despite its high sensitivity, is less reliable due to its lower precision and specificity. ResNet50-v2, while simpler, did not meet the performance standards set by the other models. These results underscore the importance of choosing the right architecture for specific medical imaging tasks to ensure accurate and reliable diagnoses.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files, specifically the supporting astronaut data used in this study, are not publicly available due to the nature of the research. This data is sensitive and restricted, as it pertains to astronaut health and privacy. However, the code used for the analysis is available upon reasonable request. This approach ensures that the methodology can be replicated and verified by other researchers while protecting the confidentiality of the data."
}