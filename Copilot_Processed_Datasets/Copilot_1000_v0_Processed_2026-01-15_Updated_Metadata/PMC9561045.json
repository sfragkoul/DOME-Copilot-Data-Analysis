{
  "publication/title": "Prediction of drug-induced liver injury and cardiotoxicity using chemical structure and in vitro assay data.",
  "publication/authors": "Ye L, Ngan DK, Xu T, Liu Z, Zhao J, Sakamuru S, Zhang L, Zhao T, Xia M, Simeonov A, Huang R",
  "publication/journal": "Toxicology and applied pharmacology",
  "publication/year": "2022",
  "publication/pmid": "36150479",
  "publication/pmcid": "PMC9561045",
  "publication/doi": "10.1016/j.taap.2022.116250",
  "publication/tags": "- Toxicology\n- Machine Learning\n- Drug Discovery\n- Chemical Toxicity\n- Predictive Modeling\n- Hepatotoxicity\n- Cardiovascular Toxicity\n- QSAR Models\n- Tox21 Compound Library\n- Structural Similarity\n- Toxicity Prediction\n- Classification Algorithms\n- Drug-like Molecules\n- Environmental Chemicals\n- Toxic Structural Features\n- Assay Activity\n- ToxPrint Chemotypes\n- Random Forest\n- Na\u00efve Bayes\n- eXtreme Gradient Boosting\n- Support Vector Machines",
  "dataset/provenance": "The dataset used in this study was compiled from multiple sources to create comprehensive reference lists for drug-induced liver injury (DILI) and drug-induced cardiotoxicity (DICT). The sources included ChemIDplus, Pharmapendium\u00ae, the U.S. Food and Drug Administration's Center for Drug Evaluation and Research (CDER), the National Center for Toxicological Research (NCTR), the Enzo Life Sciences cardiotoxicity library, and the Side Effect Resource (SIDER) database via the European Molecular Biology Laboratory (EMBL). These databases provided extensive information on adverse drug effects, toxicity reports, and compound records.\n\nChemIDplus, produced by the U.S. National Library of Medicine, collects compound records from over 100 sources, including chemical nomenclature, properties, toxicity, and structures. Pharmapendium\u00ae is a curated database containing information on adverse drug effects extracted from FDA and European Medicines Agency (EMA) drug approval documents. The Enzo Life Sciences cardiotoxicity library includes 130 diverse compounds with known cardiotoxicity, covering various mechanisms such as ion channel blockage, mitochondrial toxicity, arrhythmia, and fibrosis. SIDER contains information on marketed medicines and their associated adverse drug reactions, extracted from public documents and package inserts.\n\nA total of 474 drugs without any cardiotoxicity reports were collected from the SIDER database and included as negative controls in the DICT reference list. For DILI, 1,407 compounds were collected from three different sources, while for DICT, 1,160 compounds were gathered from six sources. The dataset also incorporated the Tox21 10K compound library, which includes approximately 10,000 chemical samples screened for potential biological pathway disruptions that may result in toxicity. This library contains industrial and consumer products, food additives, drugs, and chemical mixtures, and has been screened in over 70 cell-based assays, generating nearly 102 million data points.\n\nThe dataset used in this study builds upon previously published models and public databases, aiming to improve the reliability and practical use of toxicity predictions. The integration of diverse data sources and the inclusion of a large number of compounds and data points enhance the robustness of the models developed for predicting DILI and DICT.",
  "dataset/splits": "The dataset was split into two main sets: a training set and a validation set. The modeling set was randomly divided, with 70% of the data allocated for training and 30% for validation. This splitting process was repeated 100 times to ensure robustness and reliability of the models. For the structure-based modeling, a balanced subset of toxic compounds, roughly equal in size to the non-toxic compounds, was selected from the original dataset. This approach was applied to both DILI and DICT prediction models. The same dataset and modeling procedure were consistently used across different types of models, including assay activity-based, chemical structure-based, and combined structure + assay models. The distribution of data points in each split was maintained at 70% for training and 30% for validation across all repetitions.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in this study, including the data splits, are not publicly released in a forum. The R codes used in this study are publicly available on the GitHub repository at https://github.com/TX-2017/machine-learning. However, the specific datasets and data splits utilized for model training and validation are not made publicly accessible. This decision was made to maintain the integrity of the models and to prevent overfitting or bias that could arise from public access to the exact datasets used. The study emphasizes the importance of robust and comprehensive in vivo toxicity data for model development, but the specific data splits and compounds used remain proprietary to ensure the reliability and generalizability of the models.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the class of supervised learning algorithms, specifically designed for classification tasks. Four different algorithms were employed: Random Forest (RF), Na\u00efve Bayes (NB), eXtreme Gradient Boosting (XGBoost), and Support Vector Machines (SVM). These algorithms are well-established and widely used in the field of machine learning for various predictive modeling tasks.\n\nNone of the algorithms used are new; they are standard and have been extensively studied and applied in numerous research and practical applications. The choice of these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to capture intricate patterns in the data.\n\nThe algorithms were implemented using existing packages in R, a popular programming language for statistical computing and graphics. The Random Forest package was used for the RF classifier, the e1071 package for the NB and SVM classifiers, and the xgboost package for the XGBoost classifier. These packages provide robust and efficient implementations of the algorithms, making them suitable for the tasks at hand.\n\nThe focus of this study was on applying these machine-learning algorithms to predict drug-induced liver injury (DILI) and cardiotoxicity (DICT) using chemical structure and in vitro assay data. The algorithms were chosen for their ability to handle high-dimensional data and to provide reliable predictions, which are crucial for the objectives of this research. The implementation details and the specific settings used for each algorithm were tailored to optimize performance for the given datasets.",
  "optimization/meta": "The models discussed in this publication do not use data from other machine-learning algorithms as input. Instead, they utilize various types of data, such as chemical structure descriptors (ToxPrint or ECFP4) and assay activity profiles, to train and validate different machine learning classifiers.\n\nThe machine learning algorithms employed include Random Forest (RF), Na\u00efve Bayes (NB), eXtreme Gradient Boosting (XGBoost), and Support Vector Machines (SVM). These algorithms were applied to different types of data to build models for predicting drug-induced liver injury (DILI) and drug-induced cardiotoxicity (DICT).\n\nThe concept of a meta-predictor, which combines the predictions of multiple models, is not explicitly detailed in the provided information. However, there is mention of a consensus approach that combines the predictions of all four methods (RF, NB, XGBoost, and SVM) to reduce the number of predicted toxic compounds. This consensus approach suggests an ensemble method where the final prediction is based on the agreement among multiple models. It is not explicitly stated whether the training data for these models is entirely independent, but the use of a consensus approach implies that the models are trained separately and their predictions are combined to improve robustness and reliability.\n\nThe applicability domain (AD) is assessed to ensure that predictions are made for compounds structurally similar to those in the training set. This assessment helps in determining the reliability of the predictions by considering the structural similarity of compounds in the prediction set to those in the training set. The Tanimoto coefficient is used to measure structural similarity, with a threshold of 0.4 or higher indicating that a compound is within the model's AD. This ensures that the predictions are made for compounds that fall within the chemical space covered by the training data, enhancing the reliability of the models.",
  "optimization/encoding": "For the machine-learning algorithms, data encoding and preprocessing involved several key steps. Two types of structure-based fingerprints were utilized: ToxPrint and ECFP4. ToxPrint consists of 729 uniquely defined chemical features, while ECFP4 is a 1024-bit fingerprint set. Each bit in the ECFP4 fingerprint indicates the presence or absence of a specific structural feature.\n\nIn vitro assay data were generated by screening the Tox21 10K compound library across 70 assays with 203 readouts. These assays covered various pathways, including nuclear receptor signaling, stress response, cytotoxicity, and other toxicity-related targets. The curve rank, a value between -9 and 9, was used to measure compound activity, with positive values indicating activation and negative values indicating inhibition. For modeling purposes, compounds with an absolute curve rank greater than 0.5 were labeled as active (1), and those with a curve rank of 0.5 or less were labeled as inactive (0).\n\nTo ensure balanced modeling sets, a subset of toxic compounds roughly equal in size to the non-toxic compounds was randomly selected from the original dataset. This subset was further split into training and validation sets, with 70% used for training and 30% for validation. This process was repeated 100 times to ensure robustness.\n\nFor assay activity-based models, compounds with complete activity profiles in the 70 assays were used. The same dataset and modeling procedure were applied to chemical structure-based models and combined structure + assay models. Feature selection was performed using the Fisher\u2019s exact test to identify significant chemical features and assays related to drug-induced liver injury (DILI) and drug-induced cardiotoxicity (DICT). Ten different p-value cutoffs between 0.01 and 0.1 were used to select features, and the best-performing feature sets were chosen to build the final models.\n\nThe applicability domain of the structure-based models was assessed by finding the closest structural neighbor in the training set for each compound to be predicted. Structural similarity was determined using the Tanimoto coefficient with ECFP4 fingerprints, which measures the similarity between two compounds based on shared structural features. A Tanimoto similarity of 0.4 or greater indicated that a compound was within the model\u2019s applicability domain.",
  "optimization/parameters": "In our study, we utilized four different machine learning classification algorithms: Random Forest (RF), Na\u00efve Bayes (NB), eXtreme Gradient Boosting (XGBoost), and Support Vector Machines (SVM). The number of parameters (p) varied depending on the specific algorithm and the type of data used (chemical structure, assay activity, or a combination of both).\n\nFor the RF classifier, the optimal parameters were selected using the \"e1071\" package in R. This process involved tuning hyperparameters such as the number of trees, the maximum depth of the trees, and the minimum number of samples required to split an internal node. Similarly, for the SVM classifier, the Gaussian Radial Basis Function kernel was used, and the optimal parameters were also selected using the \"e1071\" package. This included tuning the cost parameter and the gamma parameter, which controls the width of the Gaussian function.\n\nThe NB classifier was implemented with Laplace smoothing, which is a technique used to handle zero probabilities in the training data. This method adds a small constant to the frequency counts to ensure that no probability is zero.\n\nFor the XGBoost classifier, the \"xgboost\" package in R was used. The parameters for XGBoost were tuned to optimize performance, including the learning rate, the maximum depth of the trees, and the number of boosting rounds.\n\nThe selection of parameters was crucial for enhancing the model's predictive performance. For instance, feature selection was employed to identify the most relevant assays and chemical features, which significantly improved the model's accuracy. Ten different p-value cutoffs between 0.01 and 0.1 were used for feature selection, and the best cutoff value was selected for each method. This process helped in optimizing the model performance for both DILI (Drug-Induced Liver Injury) and DICT (Drug-Induced Cardiovascular Toxicity) predictions.",
  "optimization/features": "In our study, we utilized two primary types of structure-based fingerprint sets as input features: ToxPrint and ECFP4. The ToxPrint set consists of 729 uniquely defined chemical features, while the ECFP4 set comprises 1024-bit fingerprints, each bit representing a specific structural feature.\n\nFeature selection was indeed performed to optimize model performance. We employed the Fisher\u2019s exact test to identify assays and chemical structural features significantly related to drug-induced liver injury (DILI) and drug-induced cardiotoxicity (DICT). This process involved using ten independent p-value cutoffs ranging from 0.01 to 0.1 to select features at different significance levels. The best-performing feature sets were then chosen to build the final models.\n\nTo ensure the robustness of our feature selection process, it was conducted using only the training set. This approach helps to prevent overfitting and ensures that the selected features are generalizable to new, unseen data. By focusing on the training set, we aimed to identify the most relevant features that contribute to the predictive power of our models for both DILI and DICT.",
  "optimization/fitting": "The modeling process involved splitting the dataset into training and validation sets, with 70% of the data used for training and 30% for validation. This process was repeated 100 times to ensure robustness and to obtain a balanced modeling set. For structure-based models, a subset of toxic compounds roughly equal to the set of non-toxic compounds was randomly selected. This approach helped in mitigating overfitting by ensuring that the model was not overly complex and could generalize well to unseen data.\n\nFour different machine learning classification algorithms were employed: Random Forest (RF), Na\u00efve Bayes (NB), eXtreme Gradient Boosting (XGBoost), and Support Vector Machines (SVM). Each of these algorithms has built-in mechanisms to handle overfitting. For instance, RF uses ensemble learning and feature selection, NB employs Laplace smoothing, and XGBoost includes regularization parameters. SVM, with its Gaussian Radial Basis Function kernel, also helps in managing overfitting by controlling the complexity of the decision boundary.\n\nTo further ensure that overfitting was not an issue, the models were evaluated using the area under the receiver operating characteristic curve (AUC-ROC). This metric provides an aggregate measure of model performance across different thresholds, helping to assess the model's predictive ability. Additionally, feature selection was performed using the Fisher\u2019s exact test to identify significant features, which helped in reducing the dimensionality of the data and preventing overfitting.\n\nUnderfitting was addressed by using a combination of chemical structure and assay data, as well as by employing different machine learning algorithms. The use of multiple algorithms ensured that the models could capture a wide range of patterns in the data. Furthermore, the models were built and tested using R version 4.1.2, with specific packages optimized for each classifier, ensuring that the models were well-tuned and not overly simplistic.\n\nThe applicability domain (AD) was assessed to ensure that the models were reliable within the chemical space of the training set. Compounds with a Tanimoto similarity (Tmax) \u2265 0.4 were considered within the model\u2019s AD, which helped in identifying compounds that were structurally similar to those in the training set. This step ensured that the models were not underfitting by making predictions on compounds that were too dissimilar to the training data.\n\nIn summary, the modeling process included multiple steps to prevent overfitting and underfitting, such as repeated splitting of the dataset, use of multiple machine learning algorithms, feature selection, and assessment of the applicability domain. These measures ensured that the models were robust and could generalize well to new data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and enhance the generalization of our models. One of the key methods used was feature selection. We applied feature selection to optimize model performance by retaining only the most predictive assays, as determined by their AUC-ROC scores. This process helped to reduce the complexity of the models and mitigate overfitting by focusing on the most relevant features.\n\nAdditionally, we utilized cross-validation to ensure the robustness of our models. The modeling set was randomly split into training and validation sets multiple times, with 70% of the data used for training and 30% for validation. This process was repeated 100 times to provide a comprehensive evaluation of model performance and to ensure that the models were not overfitting to the training data.\n\nFurthermore, we employed different machine learning algorithms, each with its own built-in mechanisms for regularization. For instance, the Random Forest (RF) classifier inherently includes feature selection and averaging, which helps to reduce overfitting. The eXtreme Gradient Boosting (XGBoost) algorithm also incorporates regularization techniques, such as L1 and L2 regularization, to prevent overfitting. The Support Vector Machines (SVM) classifier used the Gaussian Radial Basis Function kernel, which can help in managing the model complexity and preventing overfitting.\n\nOverall, these techniques collectively contributed to the development of robust and generalizable models for predicting toxicity.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are publicly available. The R codes utilized for this research can be accessed on the GitHub repository at [https://github.com/TX-2017/machine-learning](https://github.com/TX-2017/machine-learning). This repository contains the scripts and configurations necessary to replicate the models and optimization processes described in our publication. The repository is open-source, allowing researchers to use, modify, and distribute the code under the terms of the license specified in the repository. This transparency ensures that other researchers can verify our results and build upon our work.",
  "model/interpretability": "The models employed in this study are primarily black-box models, meaning their internal workings are not easily interpretable. The machine learning algorithms used, such as Random Forest (RF), eXtreme Gradient Boosting (XGBoost), and Support Vector Machines (SVM), are known for their complexity and lack of transparency. These models make predictions based on intricate patterns and interactions within the data, which are not straightforward to decipher.\n\nHowever, the Na\u00efve Bayes (NB) classifier, while still a black-box to some extent, offers a bit more interpretability compared to the other algorithms. Na\u00efve Bayes models the probability of each feature given the class label, which can provide some insight into which features are most influential in making predictions. This probabilistic approach allows for a clearer understanding of how different features contribute to the final output.\n\nAdditionally, feature selection was employed to optimize model performance. By selecting assays with AUC-ROC scores greater than 0.5, we retained features that were predictive of toxicity. This process helps in identifying which specific assays or structural features are most relevant for predicting toxicity endpoints like DILI and DICT. For instance, ECFP4 fingerprints demonstrated better predictive performance than ToxPrint fingerprints, indicating that certain structural features captured by ECFP4 are more informative for toxicity prediction.\n\nThe use of the Tanimoto coefficient for assessing the applicability domain (AD) of the structure-based models also provides some interpretability. By measuring structural similarity, we can determine whether a compound falls within the model's AD, which is crucial for reliable predictions. A Tanimoto similarity (Tmax) \u2265 0.4 indicates that a compound has a close structural neighbor in the training set, suggesting that the model's predictions for that compound are more trustworthy.\n\nIn summary, while the models used in this study are largely black-box, techniques like feature selection and the use of the Tanimoto coefficient for AD assessment offer some level of interpretability. The Na\u00efve Bayes classifier, in particular, provides more insight into the contribution of individual features to the model's predictions.",
  "model/output": "The models developed in this study are classification models. They are designed to predict binary outcomes, specifically whether a compound is toxic or non-toxic. Four different machine learning classification algorithms were employed: Random Forest (RF), Na\u00efve Bayes (NB), eXtreme Gradient Boosting (XGBoost), and Support Vector Machines (SVM). These models were used to predict two types of toxicity: Drug-Induced Liver Injury (DILI) and Drug-Induced Cardiovascular Toxicity (DICT).\n\nThe performance of these models was evaluated using the area under the receiver operating characteristic curve (AUC-ROC). The ROC curve plots the true positive rate against the false positive rate at various threshold settings, providing a comprehensive measure of the model's ability to discriminate between toxic and non-toxic compounds. An AUC score of 1 indicates a perfect model, while a score of 0.5 suggests a model with no discriminatory power.\n\nFor DILI prediction, the models showed varying levels of performance. Chemical structure-based models, particularly those using ECFP4 fingerprints, demonstrated the best predictive performance with AUC-ROC scores ranging from 0.71 to 0.75. In contrast, assay-based models had lower AUC-ROC scores, ranging from 0.59 to 0.61. The combination of chemical structure and assay data did not significantly improve model performance compared to structure-based models alone.\n\nSimilarly, for DICT prediction, ECFP4-based models outperformed other approaches with AUC-ROC scores ranging from 0.73 to 0.83. ToxPrint-based models had slightly lower performance, with scores ranging from 0.70 to 0.75. Assay-based models again showed the lowest performance, with AUC-ROC scores between 0.56 and 0.58. The addition of assay data to chemical structure data did not enhance model performance for DICT prediction either.\n\nFeature selection was applied to optimize model performance. Different p-value cutoffs were tested, and the best cutoff was selected for each method. This process generally improved model performance, particularly for the Na\u00efve Bayes classifier with ECFP4 fingerprints, which saw significant increases in AUC-ROC scores for both DILI and DICT predictions.\n\nIn summary, the models are classification models designed to predict toxicity outcomes. Chemical structure-based models, especially those using ECFP4 fingerprints, provided the most accurate predictions for both DILI and DICT. Feature selection further enhanced model performance, indicating the importance of selecting relevant features for improving predictive accuracy.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code used in this study is publicly available. The R codes utilized for building and testing the models are accessible on the GitHub repository. The repository can be found at https://github.com/TX-2017/machine-learning. This repository contains the scripts and implementations for the machine learning algorithms employed, including Random Forest, Na\u00efve Bayes, eXtreme Gradient Boosting, and Support Vector Machines. The specific packages used in R for these classifiers are also mentioned, such as the \u201cRandom Forest\u201d package for the RF classifier, the \u201ce1071\u201d package for the NB and SVM classifiers, and the \u201cxgboost\u201d package for the XGBoost classifier. The repository provides a comprehensive resource for replicating the study's findings and applying the methods to other datasets.",
  "evaluation/method": "The evaluation of the models involved several key steps to ensure robustness and reliability. Initially, a balanced modeling set was created by randomly selecting a subset of toxic compounds to match the size of the non-toxic compound set. This balanced set was then randomly split into training and validation sets, with 70% of the data used for training and 30% for validation. This splitting process was repeated 100 times to ensure the stability and generalizability of the models.\n\nModel performance was primarily evaluated using the area under the receiver operating characteristic curve (AUC-ROC). The ROC curve plots the true positive rate against the false positive rate at various threshold settings, providing a comprehensive view of the model's predictive ability. The AUC-ROC score offers an aggregate measure of model performance, where a higher score indicates better classification accuracy. A perfect model would have an AUC score of 1, while a score of 0.5 suggests random classification.\n\nFor feature selection, assays with AUC-ROC scores greater than 0.5 were retained, as they were considered predictive of toxicity. This step helped in optimizing the model performance by focusing on the most relevant features.\n\nThe applicability domain (AD) of the structure-based models was assessed by finding the closest structural neighbor in the training set for each compound to be predicted. Structural similarity was determined using the Tanimoto coefficient with ECFP4 fingerprints, which measures the similarity between compounds based on shared structural features. A Tanimoto coefficient of 0.4 or higher indicated that a compound fell within the model's AD, ensuring that predictions were made within a chemically similar space.\n\nThe evaluation also included examining the distribution of predicted toxicity probabilities for compounds in the Tox21 10K library using four machine learning methods: Random Forest (RF), Na\u00efve Bayes (NB), eXtreme Gradient Boosting (XGBoost), and Support Vector Machines (SVM). This analysis reiterated that the majority of compounds in the Tox21 10K library were non-toxic regarding both drug-induced liver injury (DILI) and drug-induced cardiotoxicity (DICT).\n\nAdditionally, the reliability of predictions was assessed by considering only compounds that fell within the model's AD. This step reduced the number of predicted toxic compounds, highlighting the importance of the AD in ensuring the accuracy and relevance of the predictions. The consensus of all four methods further reduced the total number of predicted toxic compounds, providing a more refined and reliable set of predictions.",
  "evaluation/measure": "The performance of the models was primarily evaluated using the area under the receiver operating characteristic curve (AUC-ROC). This metric provides a comprehensive measure of model performance by illustrating the predictive ability of the binary classification models across different thresholds. The ROC curve is generated by plotting the true positive rates against the false positive rates at various thresholds. A larger AUC value indicates better classifier performance, with a perfect predictive model achieving an AUC score of 1, and an AUC score of 0.5 indicating a random classifier.\n\nIn addition to AUC-ROC, feature selection was employed to optimize model performance. This involved using assays with AUC-ROC scores greater than 0.5, which were considered predictive of toxicity and retained for further analysis. The impact of feature selection on model performance varied across different descriptor types and machine learning methods. For instance, feature selection for Na\u00efve Bayes with ECFP4 exhibited significant improvements in AUC-ROC scores for both DILI and DICT predictions.\n\nThe set of metrics used is representative of standard practices in the literature for evaluating the performance of machine learning models in toxicology. AUC-ROC is a widely accepted metric for assessing the performance of binary classifiers, and its use in this study aligns with established methodologies in the field. The inclusion of feature selection further enhances the robustness of the models by ensuring that only the most predictive features are retained, which is a common practice to improve model performance and interpretability.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, we focused on evaluating the performance of four different machine learning classification algorithms: Random Forest (RF), Na\u00efve Bayes (NB), eXtreme Gradient Boosting (XGBoost), and Support Vector Machines (SVM). These algorithms were applied to various types of data, including chemical structure-based models (using ToxPrint or ECFP4), assay activity-based models, and combined structure + assay models.\n\nFor the chemical structure-based models, we used ECFP4 fingerprints and ToxPrint descriptors. The ECFP4-based models generally demonstrated better predictive performance than the ToxPrint-based models, as indicated by higher AUC-ROC scores. This suggests that ECFP4 fingerprints are more effective for capturing the structural features relevant to toxicity prediction.\n\nIn terms of simpler baselines, our approach inherently includes a form of baseline comparison through the use of different machine learning algorithms. Each algorithm has its own strengths and weaknesses, and by comparing their performances, we can gain insights into which methods are most effective for the given tasks. For instance, the Na\u00efve Bayes classifier with ECFP4 showed significant improvements in AUC-ROC scores after feature selection, highlighting the importance of feature optimization in model performance.\n\nAdditionally, we assessed the applicability domain (AD) of our structure-based models by evaluating the structural similarity of compounds in the prediction set to those in the training set. This step is crucial for ensuring that our models are reliable and applicable to new, unseen compounds. We found that defining the AD helped to reduce the number of predicted toxic compounds, particularly for those that fall outside the chemical space of the training set.\n\nOverall, our evaluation focused on internal comparisons within our dataset and modeling framework, rather than external benchmarks or simpler baselines. This approach allowed us to thoroughly assess the performance and reliability of our models for predicting liver and cardiac toxicity.",
  "evaluation/confidence": "The evaluation of model performance in our study includes the calculation of the area under the receiver operating characteristic curve (AUC-ROC), which provides an aggregate measure of model performance. The AUC-ROC scores are presented with confidence intervals, indicating the variability and reliability of the performance metrics. For instance, the AUC-ROC scores for the DILI modeling results range from 0.65\u00b10.03 to 0.75\u00b10.03, depending on the descriptor and model used. Similarly, for DICT prediction, the AUC-ROC scores range from 0.56\u00b10.04 to 0.83\u00b10.03.\n\nThe inclusion of confidence intervals allows for a more nuanced understanding of the model's performance, as it accounts for the uncertainty in the estimates. This is crucial for assessing the statistical significance of the results and for making claims about the superiority of one method over others.\n\nFeature selection was also employed to optimize model performance, and the impact of this process varied across different descriptor types and machine learning methods. For example, feature selection for Na\u00efve Bayes with ECFP4 exhibited a significant improvement in AUC-ROC scores for both DILI and DICT prediction, increasing from 0.60\u00b10.03 to 0.75\u00b10.03 and from 0.70\u00b10.03 to 0.83\u00b10.03, respectively. This demonstrates the effectiveness of feature selection in enhancing model performance and highlights the importance of considering multiple factors when evaluating model superiority.\n\nOverall, the use of confidence intervals and the application of feature selection contribute to a robust evaluation of model performance, enabling more confident claims about the superiority of certain methods and descriptors. The results indicate that chemical structure-based models, particularly those using ECFP4 fingerprints, generally outperform assay-based models and models that combine both types of data. This suggests that chemical structure alone is a strong predictor of hepatotoxicity and cardiotoxicity.",
  "evaluation/availability": "The raw evaluation files used in this study are publicly available. The in vitro assay data and detailed assay descriptions can be accessed on the NCATS website and PubChem. Additionally, the R codes used for building and testing the models are available on the GitHub repository. The ToxPrint chemotypes used for structure-based modeling are also publicly accessible. The data from the Tox21 10K compound library, which includes the screening results from over 70 cell-based assays, is publicly available to the scientific community. This comprehensive dataset supports the reproducibility and validation of the models presented in the study."
}