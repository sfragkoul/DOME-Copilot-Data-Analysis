{
  "publication/title": "Artificial Intelligence in the Prediction of Gastrointestinal Stromal Tumors on Endoscopic Ultrasonography Images: Development, Validation and Comparison with Endosonographers.",
  "publication/authors": "Lu Y, Wu J, Hu M, Zhong Q, Er L, Shi H, Cheng W, Chen K, Liu Y, Qiu B, Xu Q, Lai G, Wang Y, Luo Y, Mu J, Zhang W, Zhi M, Sun J",
  "publication/journal": "Gut and liver",
  "publication/year": "2023",
  "publication/pmid": "36700302",
  "publication/pmcid": "PMC10651383",
  "publication/doi": "10.5009/gnl220347",
  "publication/tags": "- AI in Medicine\n- Endoscopic Ultrasound\n- Gastrointestinal Stromal Tumors\n- Deep Learning\n- Medical Imaging\n- Diagnostic Accuracy\n- Subepithelial Lesions\n- Machine Learning in Healthcare\n- Endosonography\n- Model Validation\n- Gastrointestinal Tumors\n- Artificial Intelligence in Diagnostics\n- Convolutional Neural Networks\n- Medical Image Analysis\n- Diagnostic Performance",
  "dataset/provenance": "The dataset used in this study was sourced from multiple hospitals to ensure diversity and robustness. Specifically, images were collected from Fudan University Shanghai Cancer Center, the Fourth Hospital of Hebei Medical University, Zhoushan Hospital of Zhejiang Province, and Yangjiang Hospital of Traditional Chinese Medicine. These images were used for external validation of the model.\n\nThe dataset consists of a total of 2,057 images, which include 1,320 images of gastrointestinal stromal tumors (GISTs) and 737 images of non-GISTs. These images were obtained from 367 patients, encompassing 375 subepithelial lesions (SELs). The SELs included in the dataset are categorized as 245 GISTs, 120 leiomyomas, 8 ectopic pancreas, 1 sclerotic fibroma, and 1 schwannoma.\n\nThe images were randomly divided into training sets and test sets with a ratio of 9:1, and 10-fold cross-validation was applied to ensure the model's reliability. Various echoendoscopes were used to capture these images, including models like GF-UE260-AL5, GF-UCT240-AL5, mini-probes UM-2R or UM-3R, EG-3670URK, EG-580UR, EG-530UT2 or EG-580UT, and mini-probes P2612-M or P2615-M. This diversity in equipment helps to minimize the influence of differences in image resolution or quality among the machines.\n\nThe dataset has not been used in previous papers by the community, as this is the first step towards building an endoscopic ultrasonography artificial intelligence (EUS-AI) model for distinguishing SELs. The focus was on gastric SELs originating from the muscularis propria, given their frequency and complexity. Future work may include expanding the dataset to include SELs from other parts of the gastrointestinal tract, such as the esophagus, duodenum, and large intestine, to build a more comprehensive model.",
  "dataset/splits": "The dataset was divided into three main splits: training sets, test sets, and external validation sets.\n\nThe training sets consisted of 1,851 images, with 1,188 images of gastrointestinal stromal tumors (GISTs) and 663 images of non-GISTs. The test sets included 206 images, with 132 images of GISTs and 74 images of non-GISTs.\n\nThe external validation sets were collected from four different hospitals. The total number of images in the external validation sets was 914, with 752 images of GISTs and 162 images of non-GISTs. The distribution of images by hospital was as follows: 241 images from Fudan University Shanghai Cancer Center, 494 images from the Fourth Hospital of Hebei Medical University, 149 images from Zhoushan Hospital of Zhejiang Province, and 30 images from Yangjiang Hospital of Traditional Chinese Medicine.\n\nThe images were randomly divided into the training and test sets with a ratio of 9:1. Additionally, 10-fold cross-validation was applied during the training process. The model was trained for 600 epochs, and the training was stopped when the preset loss value (loss <0.0005) was reached or after 600 epochs.",
  "dataset/redundancy": "The datasets were split into training and test sets with a ratio of 9:1. This division was done randomly to ensure that the model could be trained on a substantial amount of data while still having a separate set to evaluate its performance. The training set consisted of 1,851 images, while the test set had 206 images. This split ensures that the training and test sets are independent, which is crucial for evaluating the model's generalization capabilities.\n\nTo enforce the independence of the training and test sets, we used a random division process. This method helps to minimize the risk of data leakage, where information from the test set might inadvertently influence the training process. Additionally, we applied 10-fold cross-validation during the training phase. This technique involves dividing the training data into 10 subsets, training the model on 9 of these subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. Cross-validation helps to ensure that the model's performance is consistent and not dependent on a particular split of the data.\n\nThe distribution of the datasets in terms of the number of images and tumors selected from each hospital in the external validation sets was carefully considered. For instance, Fudan University Shanghai Cancer Center contributed the largest number of images and tumors, followed by the Fourth Hospital of Hebei Medical University, Zhoushan Hospital of Zhejiang Province, and Yangjiang Hospital of Traditional Chinese Medicine. This distribution reflects the availability of data from these institutions and ensures that the model is validated on a diverse set of images and tumors.\n\nCompared to previously published machine learning datasets in similar domains, our dataset includes a substantial number of images and tumors, which is essential for training robust models. The use of multiple hospitals for external validation adds to the diversity and generalizability of the dataset. The random division of the datasets and the application of cross-validation are standard practices in machine learning to ensure the independence and reliability of the training and test sets. These methods help to mitigate overfitting and ensure that the model performs well on unseen data.",
  "dataset/availability": "The data used in this study is not publicly available. The images were collected from multiple hospitals, including Fudan University Shanghai Cancer Center, the Fourth Hospital of Hebei Medical University, Zhoushan Hospital of Zhejiang Province, and Yangjiang Hospital of Traditional Chinese Medicine. The dataset consists of endoscopic ultrasonography (EUS) images, which were divided into training sets and test sets with a ratio of 9:1. Additionally, external validation was performed using images from the aforementioned hospitals.\n\nThe study involved a total of 2,057 images from 367 patients, including 1,320 images of gastrointestinal stromal tumors (GISTs) and 737 images of non-GISTs. The images were processed and augmented using techniques such as mirror flipping, horizontal flipping, and rotation to enhance the training set.\n\nThe echoendoscopes used in the study included various models from Olympus, Fujifilm, and Pentax Lifecare, with frequencies ranging from 5 to 20 MHz. The specific echoendoscopes used in the models and external validation sets are detailed in supplementary tables.\n\nThe study was approved by the Institutional Review Board of The Sixth Affiliated Hospital, Sun Yat-sen University, and the informed consent was waived. The study has been registered in the Chinese Clinical Trial Registry (No. ChiCTR2100051191). However, the dataset itself is not released in a public forum, and there is no information provided about the license or enforcement of data release.",
  "optimization/algorithm": "The optimization algorithm employed in our study is the stochastic gradient descent optimizer, which is a widely used class of machine-learning algorithms. This optimizer is not new; it has been extensively utilized in various machine-learning applications due to its efficiency and effectiveness in training deep learning models.\n\nThe stochastic gradient descent optimizer was chosen for its ability to handle large datasets and its convergence properties. We introduced first-order momentum with a value of 0.9 to make the gradient updates more inertial, thereby achieving better convergence. The initial learning rate was set to 1e-3, and cosine annealing was used as the attenuation method. The model was trained for 600 epochs, with the final learning rate reaching 1e-6. The training process was stopped either when the preset loss value (loss <0.0005) was achieved or after completing 600 epochs.\n\nThe reason this algorithm was not published in a machine-learning journal is that it is a well-established method in the field. Our focus was on applying this optimization technique to develop a deep convolutional neural network (DCNN) classifier, specifically the ResNeSt50 model, for distinguishing between gastrointestinal stromal tumors (GISTs) and non-GISTs in endoscopic ultrasonography (EUS) images. The innovation lies in the application of this optimization algorithm to our specific medical imaging problem rather than the development of a new algorithm.",
  "optimization/meta": "The model described in the publication does not function as a meta-predictor. It is a deep convolutional neural network (DCNN) designed specifically for the evaluation of endoscopic ultrasonography (EUS) images to predict gastrointestinal stromal tumors (GISTs) or non-GISTs. The DCNN processes the EUS images directly, without incorporating data from other machine-learning algorithms as input.\n\nThe DCNN architecture includes multiple convolutional layers, dense connections, and a fully connected layer with a softmax output to classify the images. The training process involved dividing the images into training and test sets with a 9:1 ratio and applying 10-fold cross-validation. The stochastic gradient descent optimizer with first-order momentum was used, and the model was trained for 600 epochs with cosine annealing for learning rate attenuation.\n\nThe evaluation of the model's performance included sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and accuracy, which were calculated using the Scikit-learn package in Python. The diagnostic performance was assessed both by images and by tumors, and the results were compared with the judgments of experts and novices.\n\nIn summary, the model is a standalone DCNN designed for image classification in the context of EUS, and it does not rely on meta-prediction or the integration of other machine-learning methods. The training data was carefully divided to ensure independence between training and test sets, adhering to standard practices for model validation.",
  "optimization/encoding": "The data encoding process involved several steps to prepare the images for input into the deep learning model. Initially, two experts marked the borders of tumors in the endoscopic ultrasonography (EUS) images using LabelMe, an open annotation tool. These marked tumors were considered regions of interest. The images were then trimmed to fit these regions precisely, ensuring that only the relevant parts of the images were included.\n\nSome images contained measuring lines or marks that could affect the accuracy of the deep learning models. These were removed using the \"clone stamp\" tool in Adobe Photoshop, version 13.0. Only images with minimal measuring lines or marks were selected to preserve the original image quality.\n\nTo augment the training dataset, various image processing techniques were applied, including mirror flipping, horizontal flipping, and rotation at certain degrees. These augmentations were performed without disturbing the textures of the EUS images. The preprocessed images were then converted into RGB three-channel format, which served as the input for the deep learning model.\n\nThe images were randomly divided into training and test sets with a ratio of 9:1. A 10-fold cross-validation approach was employed to ensure the robustness of the model. The stochastic gradient descent optimizer was used with a first-order momentum of 0.9 to enhance convergence. The initial learning rate was set to 1e-3, and cosine annealing was used as the attenuation method. The model was trained for 600 epochs, with the final learning rate reaching 1e-6. Training was stopped either when the preset loss value (loss < 0.0005) was achieved or after completing 600 epochs.",
  "optimization/parameters": "The model utilized a deep convolutional neural network (DCNN) architecture, which inherently involves a large number of parameters due to the multiple convolutional and fully connected layers. The specific number of parameters was not explicitly stated, but it can be inferred that the model is complex, given the use of techniques like dense connections and cardinality.\n\nThe selection of the model's parameters was guided by several key choices in the optimization process. The stochastic gradient descent optimizer was employed, with a first-order momentum of 0.9 to enhance convergence. The initial learning rate was set to 1e-3, and cosine annealing was used as the learning rate attenuation method. The model was trained for 600 epochs, with a final learning rate of 1e-6. Training was stopped either when the loss value fell below 0.0005 or after completing 600 epochs.\n\nThe images used for training and testing were divided in a 9:1 ratio, and 10-fold cross-validation was applied to ensure robust performance evaluation. These choices collectively contributed to the selection and tuning of the model's parameters, aiming to achieve optimal performance in classifying subepithelial lesions (SELs) as either gastrointestinal stromal tumors (GISTs) or non-GISTs based on endoscopic ultrasonography (EUS) images.",
  "optimization/features": "The input features for our model consist of endoscopic ultrasonography (EUS) images. Specifically, the images were preprocessed to focus on the regions of interest, which were the tumors marked by experts. These images were then trimmed to squares or rectangles containing only the regions of interest, and any measuring lines or marks were erased to avoid affecting the model's accuracy.\n\nThe images were converted into RGB three-channel format, which means each image has three color channels (red, green, and blue). This results in a total of 3 * height * width features per image, where height and width are the dimensions of the trimmed image.\n\nFeature selection in the traditional sense was not performed, as we used the entire image data as input. However, the process of trimming the images to focus on the regions of interest can be seen as a form of implicit feature selection, as it emphasizes the relevant parts of the images for the model.\n\nThis trimming process was done using the training set only, ensuring that the test set and external validation set remained unseen during this preprocessing step. This approach helps to maintain the integrity of the evaluation process and prevents data leakage.",
  "optimization/fitting": "The model employed a deep convolutional neural network (DCNN) architecture, which inherently involves a large number of parameters. To address potential overfitting, several strategies were implemented. The images were randomly divided into training and test sets with a 9:1 ratio, and 10-fold cross-validation was applied. This approach ensures that the model's performance is evaluated across multiple subsets of the data, reducing the risk of overfitting to any single subset.\n\nAdditionally, stochastic gradient descent with first-order momentum was used as the optimizer. The momentum value was set to 0.9, which helps in smoothing the updates and accelerating convergence. The initial learning rate was set to 1e-3, and cosine annealing was used as the learning rate schedule. This method gradually reduces the learning rate, allowing the model to converge more effectively.\n\nThe model was trained for 600 epochs, with early stopping criteria set to halt training when the loss fell below 0.0005 or when the maximum number of epochs was reached. This prevents the model from overfitting by stopping training once it has sufficiently learned the training data without memorizing it.\n\nTo further mitigate overfitting, data augmentation techniques such as mirror flipping, horizontal flipping, and rotation were applied to the images. These techniques increase the diversity of the training data, helping the model to generalize better to unseen data.\n\nUnderfitting was addressed by ensuring that the model had sufficient capacity to learn the complex patterns in the data. The DCNN architecture, with its multiple convolutional layers and dense connections, provides the necessary complexity. The use of a large dataset, consisting of 2,057 images from 367 patients, also helps in providing enough information for the model to learn effectively.\n\nThe diagnostic performance of the model was evaluated using metrics such as sensitivity, specificity, positive predictive value, negative predictive value, and accuracy. The receiver operating characteristic curve and the area under the curve were also calculated to assess the model's performance. These evaluations indicate that the model achieved good accuracy in classifying gastric subepithelial lesions (SELs) into gastrointestinal stromal tumors (GISTs) and non-GISTs, suggesting that both overfitting and underfitting were effectively managed.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One of the key methods used was 10-fold cross-validation. This technique involves dividing the dataset into 10 subsets, training the model on 9 of these subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. This approach helps to ensure that the model generalizes well to unseen data.\n\nAdditionally, we utilized image augmentation techniques to artificially expand our training dataset. This involved applying transformations such as mirror flipping, horizontal flipping, and rotating the images to certain degrees. These augmentations help the model to learn more robust features that are invariant to these transformations, thereby reducing the risk of overfitting.\n\nWe also incorporated a stochastic gradient descent optimizer with first-order momentum. The momentum term, set to 0.9, helps to accelerate gradients vectors in the right directions, thus leading to faster converging. This technique smooths out updates and helps the model to escape shallow local minima, contributing to better generalization.\n\nFurthermore, we employed cosine annealing as the learning rate attenuation method. This technique gradually reduces the learning rate according to a cosine curve, which helps in fine-tuning the model and preventing it from getting stuck in local minima. The initial learning rate was set to 1e-3 and was reduced to 1e-6 over 600 epochs, ensuring that the model converges smoothly.\n\nThe training process was stopped either when the loss value reached a preset threshold (loss < 0.0005) or after completing 600 epochs. This early stopping criterion helps to prevent overfitting by halting the training process once the model starts to overfit the training data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we employed a stochastic gradient descent optimizer with a first-order momentum of 0.9. The initial learning rate was set to 1e-3, and cosine annealing was used as the attenuation method. The model was trained for 600 epochs, with a final learning rate of 1e-6. Training was stopped when the loss value fell below 0.0005 or after completing 600 epochs.\n\nThe model files and optimization parameters are not explicitly provided in the publication. However, the development of the deep learning models was supported by the Tianjin Jinyu Artificial Intelligence Medical Technology Co., Ltd., and the process was mainly performed using Python (version 3.7) and PyTorch. The specific details about the availability of model files and optimization parameters, including any associated licenses, are not mentioned. Therefore, it is not clear whether these resources are publicly accessible or under what terms they might be shared.",
  "model/interpretability": "The model developed in this study is primarily a deep convolutional neural network (DCNN), which is generally considered a black-box model. This means that the internal workings of the model are not easily interpretable by humans. The DCNN processes endoscopic ultrasonography (EUS) images through multiple convolutional layers, feature maps, and fully connected layers to output a probability for the pathological type of subepithelial lesions (SELs), specifically distinguishing between gastrointestinal stromal tumors (GISTs) and non-GISTs.\n\nThe schematic diagram of the DCNN architecture illustrates the flow of data from the input EUS images through various convolutional layers, dense connections, and finally to the fully connected layer that produces the output. However, the exact reasoning behind the model's predictions is not transparent. The model's decisions are based on complex patterns and features learned from the training data, which are not explicitly defined or easily understandable.\n\nTo enhance interpretability, future work could involve techniques such as Grad-CAM (Gradient-weighted Class Activation Mapping) or LIME (Local Interpretable Model-agnostic Explanations). These methods can highlight the regions of the input images that most influence the model's predictions, providing some insight into what the model is focusing on. However, such techniques were not applied in the current study, and the model remains largely a black-box in terms of its decision-making process.",
  "model/output": "The model developed in this study is a classification model. Specifically, it is designed to classify gastric subepithelial lesions (SELs) originating from the muscularis propria into two categories: gastrointestinal stromal tumors (GISTs) and non-GISTs. The output of the model is the probability for the pathological type of the SELs based on the evaluation of endoscopic ultrasonography (EUS) images. This probability is then used to make a binary classification, indicating whether a lesion is a GIST or a non-GIST.\n\nThe model employs deep convolutional neural networks (DCNNs) to process the EUS images. The architecture includes multiple convolutional layers, dense connections, and a fully connected layer with a softmax activation function to produce the final classification probabilities. The schematic diagram of the DCNNs illustrates the flow from the input EUS image to the final prediction, highlighting the layers and connections involved in the classification process.\n\nThe performance of the model was evaluated using metrics such as sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and accuracy. These metrics were calculated for both the training and test sets, as well as for the external validation sets. The model demonstrated good accuracy in distinguishing between GISTs and non-GISTs, with the external validation showing a sensitivity of 83.75%, specificity of 71.43%, and accuracy of 80.56%.\n\nThe model's output is intended to assist endosonographers in making more accurate diagnoses. By providing a probabilistic output, the model allows endosonographers to consider the AI's prediction alongside their own expertise, potentially enhancing their diagnostic sensitivity, specificity, and accuracy. The model's performance was compared with that of both expert and novice endosonographers, showing that it can improve the diagnostic capabilities of some endosonographers, particularly in terms of specificity.",
  "model/duration": "The model was trained for a total of 600 epochs. The training process was designed to stop either when the preset loss value of less than 0.0005 was achieved or after completing 600 epochs, whichever came first. The final learning rate at the end of the training was 1e-6. The stochastic gradient descent optimizer with first-order momentum of 0.9 was used to enhance convergence. Cosine annealing was employed as the learning rate attenuation method. However, the exact execution time in terms of hours or days is not specified.",
  "model/availability": "The source code for the deep learning models used in this study is not publicly released. The development of the deep learning models was supported by Tianjin Jinyu Artificial Intelligence Medical Technology Co., Ltd. The process was mainly performed using Python (version 3.7) and PyTorch. The specific software tools used include LabelMe for marking the regions of interest and Adobe Photoshop (version 13.0) for erasing measuring lines or marks. The Scikit-learn package in Python was used for statistical analysis. The model was trained using the stochastic gradient descent optimizer with first-order momentum and cosine annealing as the attenuation method. The training was conducted for 600 epochs, and the final learning rate was 1e-6. The output of the model provides the probability for the pathological type of the subepithelial lesions (SELs) based on the evaluation of the endoscopic ultrasonography (EUS) images.",
  "evaluation/method": "The evaluation method employed a combination of cross-validation and external validation to assess the model's performance. Initially, the images were randomly divided into training and test sets with a 9:1 ratio, and 10-fold cross-validation was applied to ensure robust training. The model was trained for 600 epochs, with the training stopping early if the loss value fell below 0.0005.\n\nFor external validation, images from four additional hospitals were collected, ensuring the same inclusion and exclusion criteria as the initial dataset. These images were also processed to remove measuring lines and marks. The diagnostic performance of the AI model was evaluated by counting the number of images diagnosed as GISTs or non-GISTs. If the counts were equal, the pooled predictive probability determined the final diagnosis.\n\nThree experts and three novices, blinded to the histopathological results, independently classified the subepithelial lesions (SELs) in the external validation sets. They repeated this classification after knowing the AI model's diagnosis, allowing them to reconsider their initial judgments. The sensitivity, specificity, and accuracy of the AI model and the endosonographers were calculated and compared using statistical methods, including the chi-square test. The receiver operating characteristic curve and the area under the curve were also plotted to further evaluate the model's performance.",
  "evaluation/measure": "In the evaluation of our EUS-AI model, several key performance metrics were reported to assess its diagnostic capabilities. These metrics include sensitivity, specificity, and accuracy, each presented with their respective 95% confidence intervals. Sensitivity measures the model's ability to correctly identify positive cases, specificity measures its ability to correctly identify negative cases, and accuracy provides an overall measure of the model's correctness.\n\nThe reported metrics are representative of standard practices in the field, as they are commonly used in similar studies to evaluate diagnostic models. Sensitivity, specificity, and accuracy are widely accepted metrics that provide a comprehensive view of a model's performance. Additionally, the inclusion of confidence intervals adds robustness to the reported metrics, indicating the reliability of the estimates.\n\nThe performance of the EUS-AI model was compared against that of both expert and novice endosonographers. This comparison further validates the model's effectiveness, as it demonstrates how the AI performs relative to human experts with varying levels of experience. The metrics were evaluated across different datasets, including training sets, test sets, and external validation sets, ensuring a thorough assessment of the model's generalizability and robustness.\n\nIn summary, the performance metrics reported\u2014sensitivity, specificity, and accuracy\u2014are standard and representative of the literature. The comprehensive evaluation against human performance and across various datasets strengthens the credibility of the reported metrics.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, our focus was on evaluating the diagnostic performance of our EUS-AI model against human endosonographers. We included three experts and three novices in our evaluation, each with varying levels of experience in endoscopic ultrasonography (EUS) procedures.\n\nThe experts had performed over 1,000 EUS procedures, while the novices had conducted fewer than 500. This approach allowed us to assess how our AI model compared to both experienced and less experienced practitioners.\n\nIn terms of simpler baselines, our evaluation did not explicitly include comparisons to such models. The primary baseline for our AI model was the performance of the human endosonographers. We measured sensitivity, specificity, and accuracy for both the AI model and the endosonographers across different datasets, including training, test, and external validation sets.\n\nThe external validation sets were particularly important as they included data from four different hospitals, ensuring that our model's performance was assessed in diverse clinical settings. This validation process helped us understand the generalizability and robustness of our AI model in real-world scenarios.\n\nNot applicable",
  "evaluation/confidence": "The evaluation of the EUS-AI model and the endosonographers includes detailed performance metrics with confidence intervals. Sensitivity, specificity, and accuracy are presented with 95% confidence intervals (CIs) for various datasets, including training sets, test sets, and external validation sets. These intervals provide a range within which the true performance metrics are likely to fall, offering a measure of the reliability of the reported values.\n\nStatistical significance is assessed using the chi-square test, with p-values indicating the likelihood that observed differences are due to chance. For instance, in the external validation sets by tumors, the AI model's sensitivity, specificity, and accuracy are compared against those of experts and novices. Significant p-values suggest that the differences in performance are statistically meaningful. For example, Expert 1's sensitivity is significantly higher than the AI model's (p-value = 0.021), while Expert 3's specificity is also significantly higher (p-value = 0.002). These comparisons help to determine whether the AI model's performance is superior, equivalent, or inferior to human endosonographers.\n\nAdditionally, subgroup analyses by size and hospital further validate the model's performance. The AI model demonstrates high accuracy for larger tumors (\u226520 mm) and varies in performance across different hospitals, highlighting its robustness and adaptability. The inclusion of confidence intervals and statistical tests ensures that the claims of superiority or equivalence are supported by rigorous statistical evidence.",
  "evaluation/availability": "Not enough information is available."
}