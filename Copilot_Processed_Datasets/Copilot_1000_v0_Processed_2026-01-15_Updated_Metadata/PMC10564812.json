{
  "publication/title": "MLACNN: an attention mechanism-based CNN architecture for predicting genome-wide DNA methylation.",
  "publication/authors": "Bai J, Yang H, Wu C",
  "publication/journal": "Theory in biosciences = Theorie in den Biowissenschaften",
  "publication/year": "2023",
  "publication/pmid": "37648910",
  "publication/pmcid": "PMC10564812",
  "publication/doi": "10.1007/s12064-023-00402-3",
  "publication/tags": "- DNA methylation\n- Deep learning\n- Neural networks\n- Feature fusion\n- Attention mechanisms\n- Performance evaluation\n- Bioinformatics\n- Machine learning\n- Computational biology\n- Predictive modeling",
  "dataset/provenance": "The dataset used in our study was obtained from the Gene Expression Omnibus (GEO) database. Specifically, the raw geo and GRCh37 datasets are available from the following accession numbers: GSM432685, GSM2868190, GSM2868192, GSM2868191, and GSM2868193. These datasets were utilized to train and validate our MLACNN model for predicting DNA methylation at site resolution.\n\nThe datasets include whole-genome bisulfite sequencing (WGBS) data, which provides comprehensive methylation information across the genome. The specific number of data points is not explicitly stated, but the datasets are publicly accessible and have been used in previous studies, ensuring their reliability and relevance for our research.\n\nThe datasets have been previously used by the community and in other studies, such as those mentioned in our references. This includes works by Abbas et al. (2020), Alam et al. (2020), and others, who have utilized similar datasets for predicting DNA methylation and related biological features. The use of these well-established datasets allows for comparisons and validations of our model's performance against existing methods.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The raw datasets utilized in this study are publicly available. Specifically, the geo and grch37 datasets can be accessed through the NCBI GEO database using the following accession numbers: GSM432685, GSM2868190, GSM2868192, GSM2868191, and GSM2868193. Additionally, the grch37 datasets are available via FTP from the NCBI Genomes archive.\n\nThe MLACNN software, which is an open-source Python software, is also available on GitHub. This ensures that the tools and methods used in the study are accessible to the public for reproducibility and further research.\n\nThe data is licensed under the Creative Commons Attribution 4.0 International License. This license permits the use, sharing, adaptation, distribution, and reproduction of the data in any medium or format, provided that appropriate credit is given to the original authors and the source. A link to the Creative Commons license must be provided, and any changes made to the data should be indicated. This licensing approach ensures that the data is freely available for use while maintaining the integrity and attribution of the original work.",
  "optimization/algorithm": "The machine-learning algorithm class used in our work is a convolutional neural network (CNN), specifically designed for the task of predicting methylation sites in DNA sequences. This model, referred to as MLACNN, is not a completely new algorithm in the broader sense of machine learning, as CNNs are well-established in the field. However, MLACNN is a novel application tailored for bioinformatics, particularly for handling DNA methylation data.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of our research is on its application in biosciences rather than the development of new machine-learning techniques. Our primary contribution lies in demonstrating the effectiveness of CNNs in predicting methylation sites across different tissue types, showcasing the model's universality and robustness. The model's architecture and training strategies are optimized for this specific biological problem, leveraging known techniques such as RMSprop for optimization, batch normalization, and dropout to prevent overfitting.\n\nAdditionally, the model incorporates feature fusion methods and attention mechanisms to enhance the extraction and abstraction of relevant information from DNA sequences. The performance evaluation metrics used, such as sensitivity, specificity, precision, accuracy, Matthew correlation coefficient, and AUC, are standard in the bioinformatics community but are applied here to validate the model's predictive power in a novel context.",
  "optimization/meta": "The model described, MLACNN, is indeed a meta-predictor that leverages multiple machine-learning methods to enhance its predictive capabilities. It integrates three different coding methods\u2014one-hot, NCP, and EIIP-vector coding\u2014to encode methylated sequences. These encoded sequences are then input into three separate recurrent neural networks (RNNs) equipped with an attention mechanism. The attention mechanism allows the model to focus on the most relevant parts of the input sequences, thereby improving feature extraction.\n\nThe features extracted from these three networks are then fused using an attention-based feature fusion mechanism. This fusion process combines the strengths of each individual network, resulting in a more robust and accurate prediction model. The fused features are subsequently passed through a fully connected layer with 128 hidden neurons before reaching the output layer.\n\nRegarding the independence of the training data, it is not explicitly stated whether the data used for training each of the individual networks (one-hot, NCP, and EIIP-vector) is completely independent. However, the model's design suggests that the feature fusion process is intended to integrate diverse and complementary information from different coding methods, which implies a level of independence in the features being combined. This approach helps to mitigate overfitting and enhances the model's generalization capabilities.",
  "optimization/encoding": "In our study, we employed three distinct encoding methods to preprocess the DNA sequence data for our machine-learning algorithm. Initially, we began with a single strand of DNA of length L. We then applied one-hot coding, NCP coding, and EIIP vector coding to transform this sequence into an L\u00d7D matrix, where D represents the dimension of a single base. This matrix serves as the input for our model, allowing it to capture various aspects of the DNA sequence.\n\nOne-hot coding is a straightforward method where each nucleotide (A, T, C, G) is represented by a binary vector. NCP coding, on the other hand, uses a more complex representation that considers the physical and chemical properties of the nucleotides. The EIIP vector coding method assigns a unique numerical value to each nucleotide based on its electron-ion interaction potential.\n\nThese encoded matrices were then used to train our model, which employs a convolutional neural network (CNN) with an attention mechanism to extract and fuse features. The attention mechanism allows the model to focus on the most relevant parts of the input data, enhancing its predictive performance.\n\nTo prevent overfitting, we utilized several regularization techniques, including L1 and L2 regularization, early stopping, max pooling, batch normalization, and dropout. These methods help to control the complexity of the model and improve its generalization to new, unseen data.\n\nAdditionally, we used the RMSprop optimizer with a learning rate of 0.0001 and a batch size of 128 to adjust the learning rate of each batch. The cross-entropy function served as the loss function for the neural network, guiding the optimization process.\n\nIn summary, our data encoding and preprocessing pipeline involves transforming DNA sequences into numerical matrices using one-hot, NCP, and EIIP vector coding methods. These matrices are then fed into a CNN with an attention mechanism, which is trained using regularization techniques and an appropriate optimizer and loss function. This approach enables our model to effectively learn from the encoded DNA data and make accurate predictions.",
  "optimization/parameters": "In our model, the number of parameters (p) is influenced by several factors, including the architecture of the neural network, the size of the input data, and the techniques used to prevent overfitting.\n\nThe model employs multiple coding methods\u2014one-hot coding, NCP coding, and EIIP vector coding\u2014to form an L\u00d7D matrix, where L is the length of the DNA strand and D is the dimension of a single base. This matrix serves as the input to the convolutional layers.\n\nThe convolutional layers use various techniques to extract features, including one-dimensional and D-dimensional convolutions. The model also incorporates attention mechanisms to enhance feature extraction. After feature extraction, the features are fused using an attention-based method.\n\nTo prevent overfitting, several regularization techniques are employed, including L1 and L2 regularization, batch normalization, dropout, and early stopping. These techniques help in controlling the complexity of the model and reducing the number of effective parameters.\n\nThe final prediction is obtained by passing the fused features through a fully connected layer with 128 hidden neurons, followed by an output layer. The use of max pooling with a 2\u00d71 kernel further reduces the number of parameters by downsampling the feature maps.\n\nThe RMSprop optimizer is used to adjust the learning rate of each batch, with a learning rate of 0.0001 and a batch size of 128. The cross-entropy function serves as the loss function for the neural network.\n\nThe specific number of parameters (p) can vary depending on the exact architecture and the size of the input data. However, the model is designed to be modular and easily extensible, allowing for the addition of new coding methods or feature extraction techniques with minimal changes to the overall architecture. This flexibility ensures that the model can be adapted to different datasets and tasks while maintaining a manageable number of parameters.",
  "optimization/features": "The model begins with a single strand of DNA of length L, which is encoded using three different methods: one-hot coding, NCP coding, and EIIP vector coding. These encoding methods transform the DNA sequence into an L \u00d7 D matrix, where D represents the dimension of a single base. This matrix serves as the input feature set for the model.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the model employs a sophisticated feature extraction process. It uses multiple coding methods to capture different aspects of the DNA sequence, ensuring that a rich set of features is available for the subsequent layers of the neural network.\n\nThe feature extraction process involves several layers designed to learn high-dimensional features from the input data. These layers include convolutional layers with attention mechanisms, max pooling, batch normalization, and dropout layers. The attention mechanisms, specifically ECA attention and Spatial Attention Modules (SAM), help the model focus on the most relevant features, effectively performing a form of feature selection during the training process.\n\nThe model's architecture is designed to be modular and easily extensible. Adding a new coding method for feature fusion requires retraining only the feature extraction components, ensuring that the model can adapt to new types of input features without extensive reconfiguration.\n\nThe use of multiple coding methods and attention mechanisms ensures that the model can handle a diverse set of input features, making it robust and adaptable to different types of DNA sequences. This approach allows the model to learn complex patterns and relationships within the data, leading to improved prediction accuracy.",
  "optimization/fitting": "In our model, we employed several strategies to address both overfitting and underfitting. Overfitting is a common issue when the number of parameters in a model is much larger than the number of training points, leading to the model capturing noise rather than the underlying pattern. To mitigate this, we implemented a combination of regularization techniques, including L1 and L2 regularization, which help to correct the weight and variance of the filters. Additionally, we used early stopping to prevent the model from overfitting in the epoch dimension. Max pooling was utilized not only to speed up the fitting of neural networks but also to prevent the network from becoming too deep and having too many parameters. For each pooling layer, we applied a max pooling of 2 \u00d7 1.\n\nBatch normalization (BN) layers were added after each convolutional layer to further prevent overfitting. These BN layers normalize the input values of each neuron to follow a standard normal distribution, ensuring that the activation input values fall within the sensitive range of the nonlinear function. This approach helps to avoid the problem of gradient disappearance and accelerates the training speed. Dropout technology was also employed to randomly discard some neurons in the network, which helps to prevent overfitting by ensuring that the model does not rely too heavily on any single neuron.\n\nTo address underfitting, where the model is too simple to capture the underlying patterns in the data, we used a deep convolutional neural network with multiple layers. This allows the model to learn high-dimensional features effectively. We also utilized attention mechanisms, such as ECA attention and Spatial attention, to focus on key information in the data. The attention-based feature fusion method ensures that the model can integrate and predict features more accurately. Additionally, we employed the RMSprop optimizer to adjust the learning rate of each batch, which helps in achieving better convergence and preventing underfitting.\n\nIn summary, our model incorporates a range of techniques to balance between overfitting and underfitting, ensuring robust and accurate predictions.",
  "optimization/regularization": "In our study, several regularization methods were employed to prevent overfitting and ensure the robustness of our model. We utilized a combination of L1 and L2 regularization functions to correct the weight and variance of the filter, which helps in reducing overfitting by penalizing large weights. Additionally, early stopping was implemented to halt the training process when the model's performance on a validation set ceased to improve, thereby preventing overfitting in the epoch dimension.\n\nTo further mitigate overfitting, we incorporated max pooling, which not only accelerates the fitting of neural networks but also prevents the network from becoming too deep and having too many parameters. For each pooling layer, a max pooling of 2 \u00d7 1 was used.\n\nBatch normalization (BN) layers were added after each convolutional layer. These BN layers normalize the input values of each neuron to follow a standard normal distribution with a mean of 0 and a variance of 1. This normalization helps in maintaining the activation input values within the sensitive range of the nonlinear function, thereby avoiding the problem of gradient disappearance and speeding up the training process.\n\nDropout technology was also employed, which randomly discards some neurons in the network during training. This technique helps in preventing overfitting by ensuring that the model does not rely too heavily on any single neuron.\n\nThese regularization techniques collectively contribute to the model's ability to generalize well to unseen data, enhancing its predictive performance.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we employed the RMSprop optimizer with a learning rate of 0.0001 and a batch size of 128. The cross-entropy function was utilized as the loss function, and a combination of L1 and L2 regularization was applied to prevent overfitting. Additionally, early stopping and max pooling were implemented to further mitigate overfitting and enhance training efficiency.\n\nThe model files and optimization parameters are not directly provided within the publication. However, the MLACNN software is available as open-source Python software on GitHub. This repository includes the necessary code and configurations to replicate the experiments and optimize the model. The raw datasets used in our study are accessible from the NCBI GEO database and the GRCh37 assembly, ensuring reproducibility and accessibility for further research.\n\nThe software is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction in any medium or format, provided that appropriate credit is given to the original authors and the source. This license allows for broad usage and modification, facilitating collaboration and further development in the scientific community.",
  "model/interpretability": "The MLACNN model incorporates several mechanisms that enhance its interpretability, making it less of a black box compared to other deep learning models. One key aspect is the use of attention mechanisms, which allow the model to focus on specific parts of the input data, highlighting the most relevant features for prediction. This focus can be visualized and interpreted, providing insights into which parts of the DNA sequences are crucial for methylation prediction.\n\nAdditionally, the model employs feature fusion techniques that combine information from different coding methods (one-hot, NCP, and EIIP-vector). This fusion process can be analyzed to understand how different types of features contribute to the final prediction. For instance, the confusion matrices generated by the model show the performance of individual coding methods and their combined effect, offering a clear view of how feature fusion improves prediction accuracy.\n\nThe use of t-SNE and PCA clustering further aids in interpretability. These techniques help visualize the high-level features extracted by the model, showing how raw data transforms through feature fusion and full connection layers. The clustering results demonstrate that the model effectively abstracts and classifies methylation data, with clear separations in the clustered outputs.\n\nMoreover, the model's architecture, including convolutional layers and attention blocks, can be dissected to understand the role of each component. The specific architecture of the CNN attention block and the feature fusion module provides a transparent view of how data is processed at each stage. This transparency allows researchers to trace back the decision-making process of the model, making it more interpretable.\n\nIn summary, while MLACNN leverages complex deep learning techniques, its use of attention mechanisms, feature fusion, and visualization tools like t-SNE and PCA makes it more interpretable. These elements provide clear examples of how the model processes and predicts methylation status, reducing its opacity as a black box.",
  "model/output": "The model is designed for classification tasks, specifically for predicting methylation status. It employs a sigmoid activation function in the output layer, ensuring that the predicted values fall between 0 and 1. This is indicative of a binary classification problem, where the output represents the probability of a sample belonging to a particular class (e.g., methylated or unmethylated).\n\nThe model uses several performance evaluation metrics typical for classification tasks, such as sensitivity (SN), specificity (SP), precision, accuracy (ACC), Matthews correlation coefficient (MCC), and the area under the curve (AUC). These metrics are used to quantify the model's performance and compare it with other methods.\n\nThe final prediction result is obtained by connecting the features after fusion through a fully connected layer with 128 hidden neurons, followed by an output layer. This architecture is tailored for classification, as it aims to distinguish between different classes based on the input features.\n\nThe model's performance is evaluated on various tissue types, including human liver tissue, human skin tissue, human pancreatic islet cells, and human lung tissue. The results, presented in terms of accuracy (ACC) and AUC, demonstrate the model's effectiveness in classifying methylation status across different biological contexts.\n\nIn summary, the model is a classification model designed to predict methylation status using a combination of feature extraction and fusion techniques, with performance evaluated using standard classification metrics.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for our model, MLACNN, is publicly available as open-source Python software. It can be accessed and downloaded from the GitHub repository at https://github.com/jrebai/MLACNN. This release allows users to utilize the model for their own research and applications. The software is licensed under the Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction in any medium or format, provided that appropriate credit is given to the original authors and the source. This license also requires users to provide a link to the Creative Commons license and indicate if any changes were made. For more details on the license, users can visit http://creativecommons.org/licenses/by/4.0/.",
  "evaluation/method": "To evaluate the performance of our model, we employed several common metrics: sensitivity (SN), specificity (SP), precision, accuracy (ACC), Matthew correlation coefficient (MCC), and the area under the curve (AUC). These metrics provide a comprehensive assessment of the model's ability to correctly identify positive and negative samples.\n\nWe compared our model, MLACNN, with other existing methods such as DeepCpG, DeepCpG RNN module, and MRCNN. The comparison was based on the aforementioned metrics, allowing us to highlight the strengths and weaknesses of each model. For instance, DeepCpG primarily uses one-hot coding and location information, which results in a relatively simple network structure and limited feature extraction. Similarly, MRCNN, while using one-hot coding and convolutional layers, suffers from a simple network structure that may damage the original data structure due to folding.\n\nOur model, MLACNN, utilizes one-hot, NCP, and EIIP-vector coding methods to extract high-level features through deeper convolutional layers with an attention mechanism. This approach allows for more effective feature fusion and better prediction of methylation status. The median values of the evaluation metrics for MLACNN were significantly higher than those of the compared models, demonstrating its superior performance.\n\nAdditionally, we evaluated MLACNN's performance across different tissues and cell types, including human liver tissue, human skin tissue, human pancreatic islet cells, and human lung tissue. The results showed high accuracy (ACC) and AUC values, indicating the model's robustness and generalizability across various biological contexts.\n\nIn summary, the evaluation of MLACNN involved a rigorous comparison with existing methods using standard performance metrics and demonstrated its effectiveness in predicting DNA methylation at site resolution.",
  "evaluation/measure": "In our evaluation, we employed a comprehensive set of performance metrics to thoroughly assess the effectiveness of our model. The metrics we reported include sensitivity (SN), specificity (SP), precision, accuracy (ACC), Matthew's correlation coefficient (MCC), and the area under the curve (AUC). These metrics are widely recognized and commonly used in the literature for evaluating the performance of machine learning models, particularly in the context of classification tasks.\n\nSensitivity, also known as recall, measures the proportion of actual positives that are correctly identified by the model. Specificity, on the other hand, measures the proportion of actual negatives that are correctly identified. Precision indicates the proportion of predicted positives that are actually positive. Accuracy provides an overall measure of the model's correctness by calculating the proportion of true results (both true positives and true negatives) among the total number of cases examined. Matthew's correlation coefficient offers a balanced measure that takes into account true and false positives and negatives, providing a value between -1 and 1, where 1 indicates a perfect prediction. The AUC, which stands for the area under the receiver operating characteristic curve, provides an aggregate measure of performance across all classification thresholds.\n\nThis set of metrics is representative of the standards in the field, ensuring that our evaluation is both rigorous and comparable to other studies. By including these metrics, we aim to provide a clear and comprehensive understanding of our model's performance, highlighting its strengths and areas for potential improvement.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of our model, MLACNN, with several publicly available methods to assess its performance. We benchmarked MLACNN against DeepCpG, DeepCpG RNN module, and MRCNN. These comparisons were performed using six common performance evaluation indicators: sensitivity (SN), specificity (SP), precision, accuracy (ACC), Matthews correlation coefficient (MCC), and the area under the curve (AUC).\n\nDeepCpG primarily uses one-hot coding and location information, inputting data into CNN and RNN models and performing simple feature fusion. Its network structure is relatively simple, leading to limited feature extraction. Similarly, MRCNN employs one-hot coding of DNA data and a straightforward convolutional process, which may damage the original data structure due to simple folding techniques.\n\nOur model, MLACNN, utilizes one-hot, NCP, and EIIP-vector coding methods to extract more high-level features through deeper convolutional layers with an attention mechanism. This approach allows for effective feature fusion and accurate prediction of methylation status. The median values of the performance indicators for MLACNN were significantly higher than those of the compared models, demonstrating its superior performance.\n\nAdditionally, we evaluated MLACNN on different tissues and cell types, including human liver tissue, human skin tissue, human pancreatic islet cells, and human lung tissue. The results showed that MLACNN maintains high accuracy and AUC across various tissue types, further validating its robustness and generalizability.\n\nIn summary, our comparison with publicly available methods and simpler baselines clearly indicates that MLACNN outperforms existing models in predicting DNA methylation at site resolution. This is evident from the higher median values of SN, SP, precision, ACC, MCC, and AUC achieved by MLACNN.",
  "evaluation/confidence": "In our study, we evaluated the performance of our model, MLACNN, using six common performance evaluation indicators: sensitivity (SN), specificity (SP), precision, accuracy (ACC), Matthew correlation coefficient (MCC), and AUC (area under the curve). These metrics were used to quantify the performance of MLACNN and compare it with other methods, such as DeepCpG, DeepCpG RNN module, and MRCNN.\n\nTo ensure the reliability of our results, we conducted experiments on different tissues and cell types, including human liver tissue, human skin tissue, human pancreatic islet cells, and human lung tissue. The results, presented in a figure, reveal the ACC and AUC for these different tissues, providing a comprehensive evaluation of our model's performance across various biological contexts.\n\nWe also compared the median values of the six performance indicators for MLACNN with those of the other models. The median values for MLACNN were significantly higher across all indicators, suggesting that our model has superior performance. Specifically, the median values for SN, SP, precision, ACC, MCC, and AUC were 0.964253, 0.93088502, 0.92837567, 0.947527445, 0.89562882, and 0.948907645, respectively.\n\nTo assess the statistical significance of our results, we performed t-SNE and PCA clustering analysis on the original methylation data, the methylation data after feature fusion, and the methylation data after full connection. These analyses helped us to visualize the separation between different classes and to confirm that the features extracted by our model are effective in distinguishing between methylation states.\n\nAdditionally, we used techniques such as max pooling, early stopping, batch normalization, dropout, and L1 and L2 regularization to prevent overfitting and to ensure the robustness of our model. These techniques, combined with the use of the RMSprop optimizer and the cross-entropy loss function, contributed to the reliable and statistically significant performance of MLACNN.\n\nIn summary, our evaluation of MLACNN included a comprehensive set of performance metrics, experiments on different tissues, and statistical analyses to ensure the reliability and significance of our results. The superior performance of MLACNN across all indicators and biological contexts provides strong evidence of its effectiveness in predicting methylation sites.",
  "evaluation/availability": "The raw evaluation files used in our study are publicly available. Specifically, the datasets include raw geo and grch37 data, which can be accessed through the NCBI GEO database. The accession numbers for these datasets are GSM432685, GSM2868190, GSM2868192, GSM2868191, and GSM2868193. Additionally, the grch37 data can be found via FTP at the NCBI Genomes archive.\n\nOur software, MLACNN, is also available as open-source Python software on GitHub. This allows other researchers to replicate our results and build upon our work. The software is licensed under the Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction in any medium or format, provided that appropriate credit is given to the original authors and the source.\n\nThe license also requires that any changes made to the material be indicated, and a link to the Creative Commons license must be provided. For any third-party material included in the article, the respective copyright holders should be contacted for permission if the intended use is not covered by the article's Creative Commons license."
}