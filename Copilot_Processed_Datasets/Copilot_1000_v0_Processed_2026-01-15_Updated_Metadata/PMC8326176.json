{
  "publication/title": "Development of Novel Artificial Intelligence to Detect the Presence of Clinically Meaningful Coronary Atherosclerotic Stenosis in Major Branch from Coronary Angiography Video.",
  "publication/authors": "Yabushita H, Goto S, Nakamura S, Oka H, Nakayama M, Goto S",
  "publication/journal": "Journal of atherosclerosis and thrombosis",
  "publication/year": "2021",
  "publication/pmid": "33012741",
  "publication/pmcid": "PMC8326176",
  "publication/doi": "10.5551/jat.59675",
  "publication/tags": "- Artificial Intelligence\n- Coronary Artery Disease\n- Coronary Angiography\n- Machine Learning\n- Convolutional Neural Networks\n- Cardiovascular Imaging\n- Predictive Modeling\n- Medical Diagnostics\n- Clinical Research\n- Cardiovascular Health",
  "dataset/provenance": "The dataset used in this study consists of coronary angiogram (CAG) videos. A total of 1,838 CAG videos from 199 patients were identified for the study. These videos were used to train, validate, and test an artificial intelligence model designed to detect clinically meaningful coronary stenosis.\n\nThe dataset was divided into three cohorts: a training cohort, a validation cohort, and a test cohort. The training cohort included 1,359 videos from 146 patients, which was further split into a derivation cohort of 989 videos from 109 patients and a validation cohort of 370 videos from 37 patients. The test cohort consisted of 479 videos from the remaining 53 patients.\n\nThe videos in the derivation cohort contained 319 instances of clinically meaningful stenosis, while the validation cohort had 204 such instances. At the patient level, 45 out of 109 patients in the derivation cohort, 24 out of 37 in the validation cohort, and 28 out of 53 in the test cohort were diagnosed with clinically significant stenosis of 75% or more.\n\nThe dataset was not used in any previous papers by the authors or by the community. The videos were specifically collected and curated for this study to develop and evaluate the AI model's predictive performance in detecting coronary stenosis.",
  "dataset/splits": "The dataset consists of coronary angiogram (CAG) videos from 199 patients, totaling 1,838 videos. The data was split into three main cohorts: training, validation, and test.\n\nThe training cohort includes 146 patients, comprising 1,359 videos. This cohort was further divided into two subsets: the derivation cohort, which contains 109 patients and 989 videos, and the validation cohort, which includes 37 patients and 370 videos.\n\nThe test cohort consists of the remaining 53 patients, with a total of 479 videos.\n\nIn terms of clinically meaningful stenosis, the derivation cohort has 319 videos with significant stenosis out of 989, while the validation cohort has 204 such videos out of 370. At the patient level, 45 out of 109 patients in the derivation cohort, 24 out of 37 in the validation cohort, and 28 out of 53 in the test cohort were diagnosed with clinically significant stenosis.",
  "dataset/redundancy": "The dataset used in this study consisted of coronary angiogram (CAG) videos from 199 patients, totaling 1,838 videos. To ensure the independence of the datasets and prevent redundancy, the patients were randomly divided into three distinct cohorts: training, validation, and test.\n\nThe training cohort included 146 patients, comprising 1,359 videos. This cohort was further split into two groups: 109 patients (989 videos) for the derivation set and 37 patients (370 videos) for the validation set. The derivation set was used exclusively for training the AI model, while the validation set was employed for hyperparameter tuning and selecting the best model within 30 epochs.\n\nThe remaining 53 patients, with 479 videos, constituted the test dataset. This cohort was used solely for evaluating the performance of the final model, ensuring that there was no overlap of patients across the different cohorts. This strict separation of patients into independent cohorts helped to mitigate any potential bias and ensured that the model's performance was assessed on entirely unseen data.\n\nThe distribution of clinically meaningful stenosis within the datasets was also considered. In the derivation cohort, 319 out of 989 videos contained clinically meaningful stenosis, while in the validation cohort, 204 out of 370 videos did. At the patient level, 45 out of 109 patients in the derivation cohort, 24 out of 37 in the validation cohort, and 28 out of 53 in the test cohort were diagnosed with clinically significant stenosis.\n\nThis approach to dataset splitting and independence is crucial for developing a robust AI model and is consistent with best practices in machine learning, ensuring that the model's performance is reliable and generalizable to new, unseen data.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used is a convolutional neural network (CNN), specifically a multi-layer 3-dimensional CNN. This type of algorithm is well-established and widely used in the field of machine learning, particularly for tasks involving image and video data.\n\nThe algorithm is not new; it builds upon existing CNN architectures and techniques. The choice to use this type of algorithm is driven by its proven effectiveness in handling spatial and temporal data, which is crucial for analyzing coronary angiogram (CAG) videos.\n\nThe focus of the publication is on the application of this algorithm to a specific medical problem\u2014detecting clinically meaningful coronary arterial stenosis from CAG videos\u2014rather than on the development of a new machine-learning algorithm. Therefore, it is published in a medical journal rather than a machine-learning journal. The emphasis is on the medical implications and the practical application of the algorithm in a clinical setting.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It is a standalone artificial intelligence model designed specifically for the analysis of coronary angiogram (CAG) videos. The model does not utilize data from other machine-learning algorithms as input. Instead, it processes raw CAG video data, which is preprocessed into a 3D matrix format, to predict the presence of clinically meaningful coronary arterial stenosis.\n\nThe model architecture is a multilayer 3-dimensional Convolutional Neural Network (3D CNN). This architecture is trained on video-level data with binary classification labels indicating the presence or absence of significant coronary arterial stenosis. The training process involves a derivation cohort, a validation cohort, and a test cohort, ensuring that there is no overlap of patients across these cohorts. This separation guarantees the independence of the training data, preventing any data leakage that could bias the model's performance.\n\nThe model's performance is evaluated using metrics such as the area under the receiver operating characteristic curve (AUC), which provides a measure of the model's ability to discriminate between positive and negative cases. The final prediction accuracy is tested on a separate test cohort, which was not used during the model's training or validation phases. This rigorous separation of data ensures that the model's performance is assessed on truly independent data, providing a reliable estimate of its generalizability to new, unseen cases.",
  "optimization/encoding": "For the machine-learning algorithm, the data encoding and preprocessing involved several steps to ensure the input data was suitable for a multi-layer 3-dimensional Convolutional Neural Network (CNN). Initially, each coronary angiogram (CAG) video underwent frame extraction, where the first 45 frames were selected. These frames were then resized to create 224 x 224 pixel 2-dimensional CAG images. Each pixel in these images contained density information ranging from 0 to 255, representing the grayscale values.\n\nNon-square images were converted to a square shape by padding with zeros, which appeared as black color in the images. This process resulted in a 3D matrix structure for the input data, denoted as (Tnk, Yni, Xnj), where Tnk represents the time dimension (45 frames), and Yni and Xnj represent the spatial dimensions (224 x 224 pixels). Each image in the sequence was captured at intervals of 33 milliseconds, ensuring temporal consistency in the data.\n\nThe resulting 3D matrix, with density values from 0 to 255, was then used as input for the multi-layer 3-dimensional CNN. This encoding method allowed the model to learn spatial and temporal features effectively from the CAG videos. The data was further split into training, validation, and test cohorts, ensuring no overlap of patients across these sets. This rigorous preprocessing and encoding strategy was crucial for the model's ability to accurately predict clinically meaningful coronary arterial stenosis from the video streams.",
  "optimization/parameters": "The model utilized for this study is a multi-layer 3-dimensional Convolutional Neural Network (3D CNN). The architecture of the model includes several Conv3D layers, BatchNormalization layers, MaxPooling3D layers, and Dense layers. The specific number of parameters (p) in the model can vary based on the configuration of these layers, including the number of filters, kernel sizes, and the dimensions of the input data.\n\nThe input data for the model consists of 45 frames extracted from each coronary angiogram (CAG) video, resized to 224 x 224 pixels. Each pixel contains density information ranging from 0 to 255. The resulting structure of the input data is a 3D matrix of density (D): (Tnk, Yni, Xnj), where Tnk ranges from 1 to 45 frames, and both Yni and Xnj range from 0 to 224.\n\nThe model was trained using a binary cross-entropy loss function and the RMSProp optimizer. The training process involved 20 epochs with a mini-batch size of 20 CAG videos randomly selected from the training dataset. The performance of the trained model was evaluated using the validation dataset at the end of each epoch, and the model with the best performance on the validation set within 30 epochs was selected as the final model.\n\nThe selection of hyperparameters, including the number of filters, kernel sizes, and other architectural choices, was likely based on empirical testing and validation to optimize the model's performance. The specific details of how these parameters were selected may be found in the supplemental materials or additional documentation accompanying the study.",
  "optimization/features": "The input features for the model consist of video frames from coronary angiograms (CAG). Specifically, the initial 45 frames from each CAG video are extracted and resized to create input data with 224 x 224 pixels. Each pixel contains density information ranging from 0 to 255. Non-square images are converted to a square shape by padding with 0 (black color). This results in a 3D matrix of density (D) with dimensions (Tnk, Yni, Xnj), where Tnk ranges from 1 to 45 frames, and both Yni and Xnj range from 0 to 224. Each image is obtained every 33 milliseconds.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the model directly utilizes the raw pixel density information from the video frames. The input data structure is designed to capture the temporal and spatial information from the CAG videos, which are essential for the model's predictions. The model is trained on video-level data with a clinically meaningful label of 75% or greater coronary arterial stenosis, ensuring that the input features are relevant to the task at hand.",
  "optimization/fitting": "The fitting method employed in this study utilized a multi-layer 3-dimensional Convolutional Neural Network (CNN) to analyze coronary angiogram (CAG) videos. The input data consisted of 45 frames from each video, resized to 224x224 pixels, forming a 3D matrix. This structure allowed the model to capture both spatial and temporal features effectively.\n\nThe model was trained using a derivation cohort of 989 videos from 109 patients, with a clinically meaningful label of 75% or greater coronary arterial stenosis. The training process involved 20 epochs with a mini-batch size of 20 CAG videos, selected randomly from the training dataset. To mitigate overfitting, several techniques were employed. Batch normalization was applied after each convolutional layer to stabilize and accelerate training. Dropout layers were also included to prevent the model from becoming too reliant on specific neurons. Additionally, the model's performance was evaluated using a validation cohort of 370 videos from 37 patients at the end of each epoch, ensuring that the model generalized well to unseen data.\n\nTo address the potential issue of underfitting, the model architecture was designed with multiple convolutional layers followed by max-pooling layers, which allowed the network to learn hierarchical features from the input data. The use of rectified linear unit (ReLU) activation functions helped in capturing non-linear relationships. The final dense layer with a sigmoid activation function was used to handle the binary classification problem effectively.\n\nThe optimization process utilized the RMSProp optimizer, which adaptively adjusts the learning rate for each parameter, helping to converge faster and more accurately. The binary cross-entropy loss function was minimized during training, ensuring that the model's predictions aligned closely with the true labels. The final model, selected based on the best performance on the validation set within 30 epochs, was then tested on a separate test cohort of 479 videos from 53 patients to evaluate its predictive accuracy. This rigorous training and validation process ensured that the model was neither overfitted nor underfitted, providing reliable predictions for clinically meaningful coronary stenosis.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One of the key methods used was dropout, which is a regularization technique that helps to prevent overfitting by randomly setting a fraction of input units to zero at each update during training time. This forces the network to learn redundant representations and prevents it from becoming too reliant on any single neuron.\n\nAdditionally, we utilized batch normalization, which not only accelerates the training process but also acts as a regularizer. Batch normalization helps to stabilize and speed up the training by normalizing the inputs of each layer, making the model more robust and less prone to overfitting.\n\nFurthermore, we incorporated kernel regularization in our convolutional layers. This technique adds a penalty to the loss function based on the magnitude of the weights, encouraging the model to keep the weights small and thus reducing the complexity of the model.\n\nThese regularization methods collectively contributed to the model's ability to generalize well to unseen data, thereby enhancing its predictive performance.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule are available in the supplemental code section of the publication. This code includes details about the model architecture, such as the number of layers, kernel sizes, activation functions, and regularization techniques used. It also specifies the optimization parameters, including the learning rate, batch size, and the number of epochs.\n\nThe model was trained using the RMSProp optimizer, and the binary cross-entropy loss function was employed for the binary classification problem. The training process involved 20 epochs with a batch size of 20. The model's performance was evaluated using a validation cohort, and the final model was selected based on the best c-statistics observed within 30 epochs.\n\nThe supplemental code provides a comprehensive overview of the model definition and the running process, including the data preprocessing steps and the evaluation metrics used. This code is essential for replicating the experiments and understanding the optimization process.\n\nRegarding the availability of model files, the code includes a line that loads weights from a file named 'weightFile.hdf5'. However, the specific location or license details for accessing this file are not provided in the given context. Therefore, it is not clear whether the model files are publicly available or under what terms they can be accessed.\n\nIn summary, the hyper-parameter configurations, optimization schedule, and model files are partially reported. The supplemental code offers detailed insights into the model architecture and training process, but the availability and licensing details of the model files remain unclear.",
  "model/interpretability": "The model employed in this study is a multilayer 3-dimensional Convolutional Neural Network (CNN), which is inherently a black-box model. This means that while the model can make predictions based on input data, the internal decision-making process is not easily interpretable. The CNN processes input data through multiple layers of convolutions, pooling, and dense layers, transforming the input into a complex representation that is used to make predictions.\n\nThe input to the model is a 3D matrix derived from coronary angiogram (CAG) videos. Each pixel in these videos contains density information, and the model learns patterns from these density values across multiple frames. The use of 3D convolutions allows the model to capture both spatial and temporal features, but this complexity makes it difficult to trace back how specific predictions are made.\n\nThe model's architecture includes several convolutional layers followed by batch normalization and max-pooling layers. These layers extract features at different levels of abstraction, but the exact features being learned at each layer are not explicitly defined or easily interpretable. The final layers include a global average pooling layer and dense layers, which further abstract the features before making a binary classification using a sigmoid activation function.\n\nWhile the model's structure and the types of layers used can provide some insight into what kinds of features it might be learning, the specific details of how it makes predictions for individual cases are not transparent. This lack of interpretability is a common characteristic of deep learning models, which trade off interpretability for the ability to learn complex patterns from data.\n\nTo enhance interpretability, techniques such as visualization of feature maps, saliency maps, or gradient-based methods could be employed. These methods can highlight which parts of the input data are most influential in the model's predictions, providing some level of insight into the decision-making process. However, these techniques were not explicitly used in this study, and the model remains largely a black-box in terms of its internal workings.",
  "model/output": "The model is designed for binary classification. It predicts whether a coronary angiogram (CAG) video contains clinically meaningful stenosis, defined as 75% or greater coronary luminal stenosis in at least one of the three main branches of the coronary artery. The final layer of the model uses a sigmoid activation function, which is typical for binary classification tasks. This layer outputs a probability score between 0 and 1, indicating the likelihood of the presence of significant stenosis. The model's performance is evaluated using metrics suitable for classification, such as sensitivity, specificity, accuracy, and the F-measure. Additionally, the use of binary cross-entropy as the loss function further confirms that the model is intended for classification rather than regression.",
  "model/duration": "The execution time for the model was significantly optimized through the use of high-performance computing systems. Specifically, the model was run on a combination of HPC5000-XSLGPU4TS and HPC3000-XKL2Uquad systems. The HPC5000-XSLGPU4TS system features four NVIDIA Tesla V100 GPUs, while the HPC3000-XKL2Uquad system includes four Xeon Phi 7210 processors. These systems were utilized to handle the computational demands of training and testing the model efficiently. The model was trained for 20 epochs with a mini-batch size of 20 CAG videos randomly selected from the training datasets. The performance of the trained model was evaluated using the validation dataset at the end of each epoch, ensuring that the best model was selected based on the highest c-statistics within the 30 epochs. This approach allowed for efficient model training and testing, leveraging the powerful computational resources available.",
  "model/availability": "The source code for the model definition and running the AI is provided as supplemental material. This code includes the necessary scripts to define the model architecture and execute the training and testing processes. The code utilizes TensorFlow and Keras frameworks, along with other libraries such as NumPy and Pandas, to handle data processing and model training.\n\nThe code is designed to be run in an environment equipped with high-performance computing resources, specifically utilizing combinations of NVIDIA Tesla V100 GPUs and Intel Xeon Phi processors. This setup ensures efficient computation and training of the model.\n\nThe supplemental code section provides detailed instructions and scripts for setting up the environment, loading the data, defining the model, and running the training and testing procedures. This allows other researchers to replicate the study and further build upon the work.\n\nThe code is made available to facilitate reproducibility and to encourage further research in the field. However, specific details about the licensing terms for the code are not provided. Interested parties should refer to the supplemental material for more information on how to access and use the code.",
  "evaluation/method": "The evaluation method for the developed artificial intelligence model involved a structured approach using distinct datasets for training, validation, and testing. The model was initially trained using videos from a derivation cohort, which consisted of 109 patients and 989 videos. Training was conducted for 20 epochs with a mini-batch of 20 coronary angiogram (CAG) videos randomly selected from the training datasets. The performance of the trained model was evaluated using the c-statistic on a validation dataset at the end of each epoch. This validation cohort comprised 37 patients and 370 videos. The model that demonstrated the best c-statistic on the validation set within the 30 epochs was selected as the final model. This final model was then tested on an independent test dataset, which included 53 patients and 479 videos. The evaluation metrics calculated for the test dataset included sensitivity, specificity, accuracy, and F-measure, using the median of predicted values as the cutoff. Additionally, the predictive accuracy of the model was assessed using the receiver operating characteristic (ROC) analysis, which calculated the area under the curve (AUC) to quantify the model's predictive accuracy. This comprehensive evaluation process ensured that the model's performance was rigorously tested and validated across different stages.",
  "evaluation/measure": "In the evaluation of our artificial intelligence model, several key performance metrics were reported to assess its predictive accuracy. The primary metric used was the c-statistic, also known as the area under the receiver operating characteristic curve (AUC). For the validation cohort, the model achieved a c-statistic value of 0.61, indicating the model's ability to distinguish between the presence and absence of clinically meaningful 75% or greater coronary stenosis.\n\nAdditionally, the sensitivity, specificity, accuracy, and F-measure were calculated at the median of predicted values as the cut-off. These metrics provide a comprehensive view of the model's performance by evaluating its true positive rate (sensitivity), true negative rate (specificity), overall correctness (accuracy), and the harmonic mean of precision and recall (F-measure).\n\nThe reported metrics are representative of standard practices in the literature for evaluating binary classification models, particularly in medical imaging and diagnostic tasks. The use of the AUC, sensitivity, specificity, accuracy, and F-measure ensures that the model's performance is assessed from multiple angles, providing a robust evaluation of its predictive capabilities. However, the modest predictive value with a c-statistic of 0.61 suggests that while the model shows promise, there is room for improvement, especially in terms of sensitivity and specificity. Further studies with larger and more diverse datasets are necessary to enhance the model's predictive accuracy and generalizability.",
  "evaluation/comparison": "Not enough information is available.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The evaluation process involved proprietary datasets consisting of coronary angiogram (CAG) videos from patients, which are sensitive medical data. Due to patient privacy and confidentiality concerns, these datasets are not released publicly.\n\nThe study utilized a total of 1,838 CAG videos from 199 patients. These videos were split into training, validation, and test datasets to develop and evaluate the AI model. The training dataset consisted of 1,359 videos from 146 patients, further divided into derivation and validation cohorts. The test dataset included 479 videos from the remaining 53 patients.\n\nThe evaluation metrics, such as sensitivity, specificity, accuracy, and F-measure, were calculated based on the performance of the AI model on the test dataset. The model's predictive accuracy was assessed using the area under the receiver operating characteristic curve (AUC), with a c-statistic value of 0.61 reported for the validation cohort.\n\nWhile the raw evaluation files are not available, the methods and results of the evaluation are thoroughly documented in the publication. This includes details on the dataset splitting, model training, and evaluation metrics used to assess the AI model's performance. Researchers interested in replicating or building upon this work can refer to the supplemental code provided, which outlines the model definition and running procedures. However, access to similar CAG video datasets would be required for such endeavors."
}