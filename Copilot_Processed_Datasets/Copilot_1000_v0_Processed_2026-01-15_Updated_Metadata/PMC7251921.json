{
  "publication/title": "Prediction of in-hospital mortality in patients with post traumatic brain injury using National Trauma Registry and Machine Learning Approach.",
  "publication/authors": "Abujaber A, Fadlalla A, Gammoh D, Abdelrahman H, Mollazehi M, El-Menyar A",
  "publication/journal": "Scandinavian journal of trauma, resuscitation and emergency medicine",
  "publication/year": "2020",
  "publication/pmid": "32460867",
  "publication/pmcid": "PMC7251921",
  "publication/doi": "10.1186/s13049-020-00738-5",
  "publication/tags": "- Traumatic Brain Injury\n- In-hospital Mortality\n- Machine Learning\n- Predictive Modeling\n- Artificial Neural Networks\n- Support Vector Machines\n- Clinical Decision Support\n- Emergency Medicine\n- Data Mining\n- Prognostic Models",
  "dataset/provenance": "The dataset used in this study was sourced from the trauma registry of Hamad General Hospital, a level 1 trauma center operated by Hamad Medical Corporation. The study focused on adult patients who sustained traumatic brain injury (TBI) and were admitted to the hospital between January 2014 and February 2019.\n\nA total of 2318 patients with TBI were initially registered in the trauma registry during this period. However, after excluding pediatric patients (under 14 years old) and records with missing data, 1620 eligible patients were included in the final analysis. The dataset comprised various variables such as age, gender, mechanism of injury, vital signs, Glasgow Coma Score, CT findings, intubation status, comorbidities, procedures performed, and in-hospital complications.\n\nThe dataset has not been used in previous publications by the authors or the community. The study aimed to design a supervised machine learning predictive model to early predict in-hospital mortality in adult TBI patients, utilizing the specific dataset collected from the trauma registry.",
  "dataset/splits": "The dataset was partitioned into two main splits: a training set and a testing set. The overfitting prevention was set at 30%, which means that 30% of the data was reserved for testing, and the remaining 70% was used for training. This partitioning helps in validating the models' performance and preventing overfitting.\n\nThe study included 1620 eligible patients. Therefore, approximately 1134 patients' data were used for training, and around 486 patients' data were used for testing. This split ensures that the models are trained on a substantial amount of data while also having a significant portion reserved for evaluating their performance on unseen data.",
  "dataset/redundancy": "The dataset was partitioned into training and testing sets to prevent overfitting and validate the models' performance. The training set comprised 70% of the data, totaling 1120 cases, while the testing set included 30% of the data, totaling 500 cases. This partitioning ensured that the training and test sets were independent, with the training set used to build the models and the testing set reserved for evaluating their performance.\n\nThe distribution of the dataset was carefully managed to reflect real-world scenarios. In the training set, there were 977 alive patients and 143 dead patients. Similarly, the testing set consisted of 440 alive patients and 60 dead patients. This distribution was designed to maintain a representative sample of the overall population, ensuring that the models could generalize well to new, unseen data.\n\nThe partitioning process was crucial for assessing the models' ability to predict in-hospital mortality accurately. By using a significant portion of the data for training and a separate portion for testing, the study aimed to achieve robust and reliable results. This approach is consistent with best practices in machine learning, where independent training and testing sets are essential for validating model performance and ensuring that the results are not due to overfitting.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The study utilized two well-established machine learning techniques: Artificial Neural Networks (ANN) and Support Vector Machines (SVM). These algorithms are widely recognized and have been extensively used in various predictive modeling tasks, including those in the medical field.\n\nNeither of these algorithms is new; they have been subjects of extensive research and application for decades. ANN and SVM are both supervised learning algorithms that have proven their efficacy in classification tasks. ANN is particularly known for its ability to model complex relationships through layers of interconnected nodes, while SVM is renowned for its effectiveness in finding optimal hyperplanes that separate different classes in the data.\n\nThe choice of these algorithms was driven by their proven track record in predicting in-hospital mortality, especially in the context of traumatic brain injury (TBI). Both ANN and SVM have demonstrated strong performance in similar studies, making them suitable for comparison in this research.\n\nThe decision to use these established algorithms rather than novel ones was strategic. The primary goal was to compare their performance with each other and with previous studies to recommend the model that achieves the optimal performance and highest practicality in supporting clinical decision-making. This approach ensures that the findings are robust and can be readily applied in clinical settings.\n\nThe study did not aim to introduce a new machine-learning algorithm but rather to evaluate and compare the performance of existing, well-understood algorithms. This focus aligns with the objective of providing actionable insights for clinical practice, where reliability and validation are paramount.",
  "optimization/meta": "Not applicable. The study does not employ a meta-predictor. Instead, it utilizes two distinct machine learning techniques\u2014Artificial Neural Networks (ANN) and Support Vector Machines (SVM)\u2014to predict in-hospital mortality following traumatic brain injury (TBI). The study compares the performance of these two models and ultimately selects SVM for deployment due to its superior performance across various evaluation metrics. The data used for training and testing these models is partitioned to prevent overfitting, with 70% of the data allocated to the training set and 30% to the testing set. This partitioning ensures that the training data is independent of the testing data, maintaining the integrity of the model evaluation process.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for ensuring the effectiveness of the machine-learning algorithms used in our study. Initially, we included a wide range of variables such as age, gender, mechanism of injury, mode of arrival, alcohol blood level, blood pressure, heart rate, Glasgow Coma Score (GCS), CT findings, intubation status and location, date/time of injury, time of admission to the Emergency Department (ED), patients' known comorbidities, performed procedures, blood transfusion, administration of the Venous Thromboembolism (VTE) prophylaxis, in-hospital complications, outcome, and date of disposition.\n\nTo handle missing data, which can significantly impact model performance, we opted to eliminate records with missing values rather than imputing them. This decision was made due to the critical nature of the subject matter and the potential for imputed values to introduce bias.\n\nWe also generated additional variables from the retrieved data, such as the shift of admission, categorized into day (7 am to 6:59 pm) and night (7 pm to 6:59 am). This variable was included to account for potential differences in patient outcomes based on the time of admission.\n\nFor the machine-learning algorithms, we used SPSS Modeler 18.1 to conduct the analysis. The data was partitioned into training and testing sets to prevent overfitting, with 30% of the data reserved for testing. This partitioning helped in validating the models' performance and ensuring that they generalized well to new, unseen data.\n\nThe outcome measure, in-hospital mortality, was encoded as a dichotomous variable (0 = alive and 1 = dead). Patients who were discharged from the trauma surgery section or transferred to another hospital were considered alive.\n\nIn terms of feature selection, we excluded variables that had no predictive power, such as health record number, date of admission, and date of disposition. Additionally, variables that were severely imbalanced, like gender (where female patients were less than 6%), were also excluded to avoid bias in the model.\n\nFor the Support Vector Machine (SVM) model, we used a linear kernel, which was found to optimize the predictive performance in preliminary assessments. This choice was made after comparing the performance of different kernel functions, such as polynomial, sigmoid, and Radial Basis functions.\n\nOverall, the data encoding and preprocessing steps were designed to ensure that the machine-learning models were robust, unbiased, and capable of making accurate predictions.",
  "optimization/parameters": "The study utilized 21 variables as input parameters for the predictive models. These variables were selected based on a thorough exploration of the trauma registry data dictionary and a comprehensive review of the literature. The goal was to identify meaningful variables that have predictive power and to exclude those that do not contribute significantly to the model's performance. Variables such as health record number, date of admission, and date of disposition were excluded due to their lack of predictive value. Additionally, variables with severe imbalances, like gender, were also excluded to ensure the robustness of the models. The selected variables include demographic information, clinical data, and radiological parameters, all of which were deemed relevant for predicting in-hospital mortality in patients with traumatic brain injury.",
  "optimization/features": "The study utilized 21 variables as input features for predicting in-hospital mortality. These variables included a mix of demographic, clinical, and procedural data such as age, gender, mechanism of injury, Glasgow Coma Score (GCS), CT findings, intubation status, and various physiological measurements like blood pressure and heart rate. Additionally, secondary variables like the shift of admission were generated from the retrieved data.\n\nFeature selection was implicitly performed during the data preparation phase. Variables that had no predictive power or were severely imbalanced were excluded. For instance, variables like health record number, date of admission, and date of disposition were excluded due to their lack of predictive value. Similarly, gender was excluded because female patients constituted less than 6% of the dataset, leading to severe imbalance.\n\nThe process of handling missing data involved eliminating records with incomplete information, ensuring that only relevant and complete data were used for model training and testing. This approach helped in maintaining the integrity and reliability of the predictive models. The feature selection and data preparation steps were conducted using the entire dataset before it was split into training and testing sets, ensuring that the models were trained and validated on unbiased data.",
  "optimization/fitting": "The study utilized two powerful supervised machine learning techniques: Artificial Neural Networks (ANN) and Support Vector Machines (SVM). To ensure the models' performance and prevent overfitting, the data was partitioned into training and testing sets. Specifically, 70% of the data was used for training, and 30% was reserved for testing. This partitioning helped in validating the models' performance and preventing overfitting.\n\nANN, known for its strength in classification and pattern identification, was optimized through this data partitioning. The training process continued until the error was no longer reducible, ensuring that the model did not overfit the training data. Once trained, the ANN could be used for future cases where the outcome was unknown.\n\nSVM, another robust classification algorithm, was used with a linear kernel, which optimized the predictive performance in preliminary assessments. The linear kernel was chosen over other functions like polynomial, sigmoid, or Radial Basis functions because it provided the best separation of classes in the data.\n\nTo further validate the models, performance metrics beyond accuracy were considered. These included the Area Under the Curve (AUC), precision, Negative Predictive Value (NPV), sensitivity, specificity, and F-score. This comprehensive evaluation ensured that the models were not underfitting the data, as they achieved high performance across multiple metrics.\n\nIn summary, the study employed a rigorous approach to model fitting, ensuring that both overfitting and underfitting were addressed through careful data partitioning and comprehensive performance evaluation.",
  "optimization/regularization": "To prevent overfitting and to validate the models' performance, the data was partitioned into training and testing sets. The overfit prevention was set at 30%. This means that 30% of the data was used for testing, while the remaining 70% was used for training the models. This partitioning helps in assessing the model's ability to generalize to new, unseen data, thereby reducing the risk of overfitting. Additionally, the performance of the neural network was optimized through this partitioning, ensuring that the training continues until the error is no further reducible. This approach is crucial for developing robust predictive models that can reliably predict in-hospital mortality in patients with traumatic brain injury.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model employed in this study is a Support Vector Machine (SVM), which is generally considered more interpretable than some other machine learning techniques, such as Artificial Neural Networks (ANNs). Unlike ANNs, which are often referred to as \"black-box\" models due to their complex, layered structure, SVMs provide a clearer understanding of how predictions are made.\n\nSVMs work by finding the optimal hyperplane that best separates the classes in the feature space. This hyperplane is defined by a subset of the training data, known as support vectors. The decision boundary created by the SVM can be visualized and understood in terms of the most influential features. In this study, the SVM utilized all 21 variables to predict in-hospital mortality, and it ranked intubation as the most important predictor. This ranking provides insight into which factors are most critical in determining the outcome, making the model more transparent and interpretable.\n\nAdditionally, the use of a linear kernel in the SVM further enhances its interpretability. Linear kernels create a straightforward decision boundary, making it easier to understand the relationship between the input features and the output predictions. This contrasts with more complex kernels, such as polynomial or radial basis functions, which can obscure the decision-making process.\n\nIn summary, the SVM model used in this study offers a good balance between predictive performance and interpretability. By identifying key predictors like intubation and using a linear kernel, the model provides clear insights into the factors driving in-hospital mortality predictions.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict in-hospital mortality for patients who have sustained traumatic brain injury (TBI). The outcome measure is a dichotomous variable, where 0 represents alive and 1 represents dead. This binary classification allows the model to predict whether a patient will survive or not during their initial hospitalization post-TBI.\n\nThe study utilized two powerful supervised machine learning techniques: Artificial Neural Networks (ANN) and Support Vector Machines (SVM). Both techniques are widely used for classification tasks. The performance of these models was evaluated using various metrics, including accuracy, sensitivity, specificity, and the area under the curve (AUC). The SVM model, in particular, demonstrated superior performance across all evaluation metrics, making it the chosen model for deployment.\n\nThe model's ability to classify patients into survivors and non-survivors is crucial for clinical decision-making. It helps clinicians and healthcare managers optimize the management of medical resources, initiate appropriate diagnostics and interventions in a timely manner, and provide guidance to patients' families. The model's performance was validated through partitioning the data into training and testing sets, ensuring that it generalizes well to new, unseen data.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method involved partitioning the data into training and testing sets to prevent overfitting and validate the models' performance. The data was split such that 70% of the cases were used for training and 30% for testing. This partitioning helped in assessing the models' ability to generalize to new, unseen data.\n\nTo evaluate the performance of the models, a confusion matrix was constructed. This matrix displays the relationship between the actual observations and the predicted conditions, providing a clear view of true positives, true negatives, false positives, and false negatives.\n\nSeveral performance metrics were considered to comprehensively evaluate the models. These metrics included accuracy, Area Under the Curve (AUC), precision, Negative Predictive Value (NPV), sensitivity, specificity, and F-score. Accuracy alone was deemed insufficient for evaluating model performance, hence the inclusion of these additional metrics.\n\nThe Support Vector Machine (SVM) model achieved the best performance across all evaluation metrics. This was determined by comparing the performance of SVM with that of Artificial Neural Networks (ANN). The SVM model utilized all 21 variables in predicting in-hospital mortality, with endotracheal intubation during resuscitation identified as the most important predictor.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our machine learning models. The primary metric reported is accuracy, which measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. Both models achieved an accuracy greater than 91%, indicating strong overall performance.\n\nHowever, recognizing that accuracy alone is insufficient for a thorough evaluation, we also considered several other critical metrics. The Area Under the Curve (AUC) provides a measure of the model's ability to distinguish between positive and negative classes. Precision, which is the ratio of true positive predictions to the total predicted positives, helps assess the model's reliability in identifying true positive cases. The Negative Predictive Value (NPV) indicates the proportion of negative predictions that are correctly identified. Sensitivity, or recall, measures the model's ability to identify true positive cases out of all actual positives. Specificity evaluates the model's ability to correctly identify true negative cases. Finally, the F-score, which is the harmonic mean of precision and recall, offers a balanced measure of a model's accuracy.\n\nThese metrics collectively provide a robust evaluation framework, ensuring that our models are assessed not just on their overall correctness but also on their ability to handle different aspects of classification performance. This approach aligns with established practices in the literature, where multiple metrics are used to provide a comprehensive view of model performance.",
  "evaluation/comparison": "In our study, we compared the performance of two powerful supervised machine learning techniques: Artificial Neural Networks (ANN) and Support Vector Machines (SVM). Both methods are widely used in predicting in-hospital mortality and were selected to provide baseline comparative performance.\n\nTo ensure a fair comparison, we partitioned the data into training and testing sets, with 70% of the data used for training and 30% reserved for testing. This partitioning helped prevent overfitting and validated the models' performance.\n\nWe evaluated the models using several performance metrics, including accuracy, Area Under the Curve (AUC), precision, Negative Predictive Value (NPV), sensitivity, specificity, and F-score. Both models achieved accuracy greater than 91%, but SVM outperformed ANN in all other performance evaluation metrics.\n\nIn addition to comparing these machine learning techniques, we also benchmarked our SVM model against conventional multivariate Logistic Regression (LR) based models that utilize conventional TBI prognostic models. The SVM model demonstrated superior performance, with an AUC higher than the 92% reported for conventional prognostic models.\n\nFurthermore, we compared our SVM model's performance with published literature on TBI. The SVM model's performance was found to be higher or similar to that of machine learning models in similar studies, which is crucial for considering the external validity of our study.\n\nIn summary, our comparison involved both simpler baselines (ANN) and publicly available methods (conventional LR-based models), providing a comprehensive evaluation of our SVM model's effectiveness in predicting in-hospital mortality for TBI patients.",
  "evaluation/confidence": "The evaluation of the models' performance was conducted using a confusion matrix to display the relationship between actual observations and predicted conditions. The performance metrics considered included accuracy, AUC, precision, NPV, sensitivity, specificity, and F-score. Both models achieved an accuracy greater than 91%, but since accuracy alone is insufficient, other metrics were also taken into account.\n\nThe Support Vector Machine (SVM) model demonstrated the best performance among the evaluated metrics. However, specific details about confidence intervals for these performance metrics are not provided. The statistical significance of the results is implied by the comparison with other models and baselines, showing that SVM outperformed the Artificial Neural Network (ANN) and conventional multivariate logistic regression (LR) based models. The SVM model's performance was also compared favorably with published literature on traumatic brain injury (TBI), indicating its robustness and generalizability.\n\nThe study highlights that the SVM model achieved higher or similar performance to other machine learning models in similar studies, which is crucial for considering the external validity of the findings. The ranking of intubation as the most important predictor for in-hospital mortality further supports the model's reliability. However, without explicit mention of confidence intervals or p-values, it is challenging to quantify the statistical significance of these findings precisely.",
  "evaluation/availability": "Not applicable"
}