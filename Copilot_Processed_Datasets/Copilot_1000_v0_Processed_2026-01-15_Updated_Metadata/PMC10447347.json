{
  "publication/title": "Augmenting Community Diagnosis of Safe Ear Disease Through Tele-Myringoscopy with Borescope Using AIML Tecniques.",
  "publication/authors": "Katyayan A, Mishra P, Katyayan A, Kishore DM, Mishra A",
  "publication/journal": "Indian journal of otolaryngology and head and neck surgery : official publication of the Association of Otolaryngologists of India",
  "publication/year": "2023",
  "publication/pmid": "37636704",
  "publication/pmcid": "PMC10447347",
  "publication/doi": "10.1007/s12070-023-03769-3",
  "publication/tags": "- Telemyringoscopy\n- Borescope\n- Artificial intelligence\n- Machine learning\n- Ear disease\n- Tympanic membrane\n- Community ear care\n- Telemedicine\n- Otolaryngology\n- Ear screening",
  "dataset/provenance": "The dataset used in this study consists of archival images of tympanic membranes (TMs) collected during the COVID-19 period from patients with ear complaints. These images were obtained using a low-cost borescope integrated with a smartphone, allowing for social distancing during the examination. The total number of images considered for analysis includes 83 images of perforated TMs and 32 images of normal TMs. To enhance the training of the AIML system, these images were augmented to a total of 230 normal TM images and 225 perforated TM images. The augmented samples were then randomized and split into three categories: training, validation, and testing. The training set was the largest, followed by the validation set, and the testing set. This dataset was specifically compiled to assess the predictability of detecting central perforations in TMs using AIML techniques. The images were recorded by an assistant who introduced the borescope into the external auditory canal under the guidance of an otologist, who reviewed and recorded the otoscopy pictures on a smartphone screen. The images were later compiled and segregated into different categories based on the condition of the TMs. The study focused on the first two categories: normal TMs and central perforations, excluding the other two categories due to the lack of clinical parameters and patient identifiers.",
  "dataset/splits": "The dataset was augmented to a total of 230 normal and 225 perforated tympanic membrane images. These augmented samples were then randomized into three categories. The largest category was used for training the AI model. Another set was used for validating the training phase to guide the training process. The remaining set was used for testing the model's performance. The specific distribution of data points in each split is depicted in a table, but the exact numbers are not provided here.",
  "dataset/redundancy": "The dataset used in this study consisted of images of tympanic membranes, specifically focusing on central perforations and normal membranes. A total of 83 images of perforated tympanic membranes and 32 normal images were initially considered. To enhance the training of the AIML system, these images were augmented to 225 perforated and 230 normal images.\n\nThe augmented samples were then randomized and split into three categories: training, validation, and testing. The training set was the largest, followed by the validation set, and finally the testing set. Specifically, 161 normal and 158 perforated images were used for training. The validation set consisted of 35 normal and 34 perforated images, while the testing set included 34 normal and 33 perforated images.\n\nTo ensure independence between the training and test sets, the images were randomized before being split. This randomization process helped to distribute the images evenly across the three sets, minimizing the risk of overlap and ensuring that the model's performance could be accurately evaluated on unseen data.\n\nThe distribution of the dataset in this study differs from previously published ML datasets, which often include a larger number of high-quality images. For instance, some studies have analyzed over 10,000 images obtained through expensive recording systems. In contrast, this study used a smaller sample size of 115 images, which were of variable quality due to factors such as different shades of illumination and the configuration of the external auditory canal. Despite these limitations, the study aimed to assess the feasibility of using a low-cost borescope integrated with a smartphone for community ear care, particularly in settings where access to specialized equipment and trained otologists is limited.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the class of Convolutional Neural Networks (CNNs). Specifically, three different CNN architectures were employed: VGG19, Inception-Net-v4, and a Modified VGG19 with Batch Normalization.\n\nThe Modified VGG19 with Batch Normalization is not a completely new algorithm but rather an enhancement of the existing VGG19 architecture. Batch Normalization is a technique used to improve the training of deep neural networks by normalizing the inputs of each layer. This modification was implemented to potentially improve the performance and stability of the VGG19 model.\n\nThe reason this enhanced algorithm was not published in a machine-learning journal is that the primary focus of this study was on the application of AI and machine learning techniques to a specific medical problem\u2014detecting central perforations in the tympanic membrane using telemyringoscopy with a borescope. The study aimed to demonstrate the feasibility and potential of using these techniques in a clinical setting, rather than introducing a novel machine-learning algorithm. The modifications made to the VGG19 architecture were specific to the needs of this particular application and were not the main contribution of the study.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data used in this study consisted of images of tympanic membranes, specifically focusing on those with central perforations and normal membranes. These images were obtained using a low-cost borescope integrated with a smartphone, adhering to COVID-19 protocols that required social distancing. The images varied in clarity due to factors such as the configuration of the external auditory canal, illumination, and the angle of the borescope.\n\nTo enhance the dataset for training the AI model, the images were augmented, resulting in a total of 230 normal and 225 perforated tympanic membrane images. These augmented samples were then randomized and split into three categories: training, validation, and testing. The training set was the largest, used to train the AI model, while the validation set guided the training process, and the testing set evaluated the model's performance.\n\nThe input shape for the images was set to 224x224 pixels with three channels for RGB color. The learning rate was set to 0.001, and the batch size was 32. The optimizer used was Stochastic Gradient Descent (SGD), and the model was trained for 50 epochs. The loss function employed was binary cross-entropy, which is suitable for binary classification problems. A dropout rate of 0.5 was used to prevent overfitting by randomly setting a fraction of input units to 0 at each update during training time.\n\nThree convolutional neural network (CNN) techniques were utilized: VGG19, Inception-Net-v4, and a modified version of VGG19 with batch normalization. The modified VGG19 with batch normalization showed the highest training accuracy of approximately 85%. However, the validation accuracy across all algorithms was relatively low, with the highest being around 60%. This indicates that while the model could learn from the training data, its predictive power on unseen data was limited. The model loss, which measures the difference between predicted and true classifications, did not decrease significantly, further highlighting the challenges in achieving high accuracy with the given dataset.",
  "optimization/parameters": "The model utilized several key parameters to optimize the performance of the convolutional neural networks (CNNs) employed in the study. The input shape for the images was set to 224x224 pixels with 3 channels, corresponding to the RGB color space. This dimension was chosen to balance the need for detailed image information with computational efficiency.\n\nThe learning rate was set to 0.001, a common choice that ensures gradual and stable convergence during the training process. The batch size was configured to 32, which is a standard size that provides a good trade-off between training speed and the stability of gradient estimates.\n\nThe stochastic gradient descent (SGD) optimizer was used due to its effectiveness in handling large datasets and its ability to converge to optimal solutions efficiently. The number of epochs was set to 50, allowing the model sufficient iterations to learn from the data without overfitting.\n\nThe loss function employed was binary cross-entropy, which is suitable for binary classification problems like detecting the presence of a central perforation in the tympanic membrane. A dropout rate of 0.5 was applied to prevent overfitting by randomly setting half of the neurons to zero during training, thereby encouraging the model to learn more robust features.\n\nThese parameters were selected based on empirical evidence and common practices in the field of deep learning, aiming to achieve a balance between model complexity and performance. The choice of these values was informed by previous studies and the specific requirements of the dataset used in this research.",
  "optimization/features": "The input features for the AI models used in this study were derived from images of the tympanic membrane. The images were processed to have an input shape of 224 by 224 pixels with 3 channels, corresponding to the RGB color space. This means that the input features consist of the pixel values of the images, which are used directly without any explicit feature selection process.\n\nThe images were augmented to increase the dataset size, but this augmentation was applied uniformly to all images, including those used for training, validation, and testing. Therefore, the feature selection, if any, was inherently part of the image preprocessing and augmentation steps, which were consistent across all datasets.\n\nThe models utilized convolutional neural networks (CNNs), which are designed to automatically learn and extract relevant features from the input images during the training process. This approach eliminates the need for manual feature selection, as the CNNs can identify and emphasize the most important aspects of the images for the classification task.\n\nThe specific CNNs used included VGG19, Inception-Net-v4, and a modified version of VGG19 with batch normalization. These models were trained on the augmented dataset, which included 230 normal tympanic membrane images and 225 perforated tympanic membrane images. The dataset was split into training, validation, and testing sets to ensure that the models could generalize well to new, unseen data.",
  "optimization/fitting": "Not applicable.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and improve the generalization of our AI models. One of the key methods used was dropout, which is a form of regularization where randomly selected neurons are ignored during training. This helps to prevent the model from becoming too reliant on any single neuron and encourages it to learn more robust features. We set the dropout rate to 0.5, meaning that half of the neurons were randomly dropped during each training iteration.\n\nAdditionally, we utilized batch normalization, which helps to stabilize and accelerate the training process. Batch normalization normalizes the inputs of each layer, which can lead to more stable gradients and faster convergence. This technique was particularly beneficial in our Modified VGG19 model, which showed the highest training accuracy among the algorithms we tested.\n\nWe also employed data augmentation techniques to artificially increase the size of our dataset. This involved creating modified versions of the original images through various transformations such as rotations, flips, and changes in brightness. By augmenting the dataset, we were able to provide the model with a more diverse set of training examples, which helped to reduce overfitting and improve the model's ability to generalize to new, unseen data.\n\nFurthermore, we split our augmented dataset into three categories: training, validation, and testing. This allowed us to monitor the model's performance on a separate validation set during training, which helped us to tune the hyperparameters and prevent overfitting. The validation set was used to guide the training process and ensure that the model was generalizing well to new data.\n\nIn summary, we implemented dropout, batch normalization, data augmentation, and a separate validation set to prevent overfitting and enhance the performance of our AI models. These regularization techniques were crucial in achieving the reported training and validation accuracies.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, details such as input shape, learning rate, batch size, optimizer, epochs, loss function, and dropout rate are provided in Table 2. These parameters were carefully selected to ensure the robustness and reliability of our AI models.\n\nThe model files and optimization schedules are not explicitly provided in the publication. However, the methods and techniques used for training and validating the models are thoroughly described. This includes the use of three different convolutional neural network (CNN) techniques: VGG19, Inception-Net-v4, and a Modified VGG19 with Batch Normalization. The performance metrics, including training and validation accuracies, are detailed in Table 3.\n\nRegarding the availability and licensing of the reported configurations and parameters, they are presented within the academic publication. The publication itself is subject to the standard academic publishing licenses, which typically allow for the use and citation of the reported methods and parameters for non-commercial research purposes. For specific licensing details, one would need to refer to the journal's policies or contact the publishers directly.",
  "model/interpretability": "The model utilized in this study is primarily a black-box model, as it employs convolutional neural networks (CNNs) such as VGG19, Inception-Net-v4, and a modified version of VGG19 with batch normalization. These types of models are known for their high predictive power but often lack interpretability, making it difficult to understand the exact reasoning behind their predictions.\n\nThe use of a confusion matrix provides some level of interpretability by showing the true positive, true negative, false positive, and false negative rates. However, this matrix does not delve into the specific features or patterns that the model uses to make its classifications. For instance, the confusion matrix for the Modified VGG19 with batch normalization shows that out of 34 normal tympanic membrane images, all were incorrectly classified as perforations, and out of 33 perforation images, 11 were correctly identified while 22 were misclassified. This indicates some level of performance but does not explain why these misclassifications occurred.\n\nThe model's training and validation accuracy, as well as the loss functions, offer quantitative measures of performance but do not provide qualitative insights into the decision-making process. The training accuracy for the Modified VGG19 with batch normalization reached 85%, while the validation accuracy was around 58%. This discrepancy suggests potential overfitting, where the model performs well on training data but less so on validation data. However, this does not offer clarity on how the model arrives at its predictions.\n\nIn summary, while the model demonstrates some level of predictive capability, it remains largely a black-box, with limited transparency in how it processes and interprets the input images. Future work could focus on developing more interpretable models or techniques to explain the decisions made by these CNNs, which would be beneficial for clinical applications where understanding the reasoning behind diagnoses is crucial.",
  "model/output": "The model employed in this study is a classification model. Specifically, it is designed to predict the presence of a central perforation in the tympanic membrane (TM) using images obtained through telemyringoscopy with a borescope. The model utilizes convolutional neural network (CNN) techniques, including VGG19, Inception-Net-v4, and a modified version of VGG19 with batch normalization. These algorithms were trained to distinguish between images of normal tympanic membranes and those with central perforations.\n\nThe output of the model is a binary classification, indicating whether an image shows a normal TM or a perforated TM. The performance of the model was evaluated using metrics such as training accuracy, validation accuracy, and a confusion matrix. The highest training accuracy achieved was 85% with the modified VGG19 with batch normalization, while the validation accuracy was around 60%. The confusion matrix further illustrated the model's performance in correctly identifying normal and perforated TM images.\n\nDespite the limitations observed, such as variability in image quality and sample size, the model demonstrates potential for improving community ear care through telemyringoscopy. Future work aims to address these limitations to enhance the model's predictive power and reliability.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method involved analyzing a dataset of images of tympanic membranes (TMs), specifically focusing on central perforations and normal TMs. The dataset consisted of 83 images of perforated TMs and 32 normal TM images, which were augmented to 225 perforated and 230 normal TM images to enhance the training of the AI models. These augmented samples were then randomized and split into three categories: one for training the AI model, another for validating the training process, and a third for testing the model's performance.\n\nThree different AI algorithms were employed for training: VGG19, Inception-Net-v4, and a Modified VGG19 with Batch Normalization. The purpose of using these three algorithms was to determine which one performed the best. The evaluation metrics included training accuracy, validation accuracy, and test accuracy. Additionally, a confusion matrix was used to interpret the results of the binary classification problem.\n\nThe training accuracy and validation accuracy were estimated across all three algorithms. The Modified VGG19 with Batch Normalization showed the highest training accuracy of 85%. However, the validation accuracy was around 60%, and the test accuracy was around 50%, indicating that the AI models did not exhibit high predictive power. The model loss, which represents the difference between predicted and true classifications, also showed little improvement, further suggesting the limitations of the current AI models.\n\nThe evaluation highlighted several factors that contributed to the variability in image clarity, such as the configuration of the external auditory canal, different shades of illumination, and the completeness of TM visibility due to the angulation used while negotiating the borescope. These factors, along with the quality of the images, played a significant role in the performance of the AI models. Despite these limitations, the study underscored the potential of AI in enhancing cost-effective tele-myringoscopy, especially in rural and underprivileged areas.",
  "evaluation/measure": "In the evaluation of our AI models, several performance metrics were reported to provide a comprehensive understanding of their effectiveness. The primary metrics focused on accuracy, both in training and validation phases. The training accuracy and validation accuracy were estimated across three different convolutional neural network (CNN) techniques: VGG19, Inception-Net-v4, and Modified VGG19 with Batch Normalization. The Modified VGG19 with Batch Normalization revealed the highest training accuracy of approximately 85%, while the validation accuracy for this model was around 58%. The other models, VGG19 and Inception-Net-v4, showed lower training accuracies of around 77% and 76% respectively, with validation accuracies of approximately 52% and 54%.\n\nIn addition to accuracy, the model loss was also analyzed. The loss function used was binary cross-entropy, which measures the difference between the predicted and true classifications of each image at each epoch. Ideally, the loss should decrease over time for both training and testing data, indicating improved model performance. However, the results showed that the losses did not reduce significantly, suggesting limitations in the model's predictive power.\n\nA confusion matrix was also utilized to interpret the results of the binary classification problem. The confusion matrix for the Modified VGG19 with Batch Normalization showed that while the model correctly identified a significant number of perforated tympanic membrane images, it struggled with normal images, indicating room for improvement in distinguishing between the two classes.\n\nOverall, the reported metrics provide a clear picture of the models' performance, highlighting both their strengths and areas for improvement. The focus on accuracy and loss, along with the use of a confusion matrix, aligns with standard practices in the literature, ensuring that the evaluation is representative and comparable to other studies in the field. However, the relatively low validation accuracies and high model losses indicate that further refinement and validation with a larger and more consistent dataset are necessary to enhance the models' predictive capabilities.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, we focused on evaluating the performance of three specific convolutional neural network (CNN) techniques: VGG19, Inception-Net-v4, and a Modified VGG19 with Batch Normalization. These algorithms were chosen to assess which one works best for detecting central perforations in tympanic membrane images obtained through telemyringoscopy using a borescope.\n\nThe comparison was conducted internally within our dataset, which consisted of 230 augmented normal tympanic membrane images and 225 augmented perforated tympanic membrane images. These images were randomized and split into three categories: training, validation, and testing. The training accuracy and validation accuracy were estimated across all three algorithms to determine their effectiveness.\n\nWhile we did not compare our methods to simpler baselines, the results indicated that the Modified VGG19 with Batch Normalization achieved the highest training accuracy of 85%, compared to the other CNN techniques. However, the validation accuracy across all algorithms was relatively low, with the highest being around 60%. This suggests that while the Modified VGG19 with Batch Normalization performed better in training, the overall predictive power of the AI models was not highly significant.\n\nThe study highlights the potential of AI and machine learning techniques in community ear care, despite the limitations observed. Further validation with a larger and more consistent dataset is recommended to improve the accuracy and reliability of these models.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files used in this study are not publicly available. The analysis was conducted using archival images of tympanic membranes collected during the COVID-19 pandemic. These images were obtained using a low-cost borescope integrated with a smartphone, following strict COVID-19 protocols. The images were augmented and randomized into training, validation, and testing sets to assess the predictability of detecting central perforations in the tympanic membrane using AI/ML techniques.\n\nThe study utilized three different convolutional neural network (CNN) algorithms: VGG19, Inception-Net-v4, and a Modified VGG19 with Batch Normalization. The training and validation accuracies for these algorithms were documented, with the Modified VGG19 with Batch Normalization showing the highest training accuracy of approximately 85%. However, the validation accuracy remained around 60%, indicating that the AI/ML models did not achieve high predictive power.\n\nThe evaluation process involved comparing the training and validation accuracies across the three algorithms, as well as analyzing the model loss and confusion matrices. These results are depicted in various tables and figures within the publication. The images and data used in this study were not released publicly due to ethical considerations and the need to maintain patient privacy, as no consent was obtained from the patients whose images were used.\n\nFor further details on the evaluation process and the results obtained, readers are encouraged to refer to the specific tables and figures mentioned in the publication. These include Table 1, which shows the augmented samples used for training, validation, and testing; Table 2, which outlines the AI/ML parameters; Table 3, which compares the training and validation accuracies across the three CNN techniques; and Table 4, which presents the confusion matrices for each algorithm. Additionally, Figures 3, 4, and 5 provide visual representations of the variations in image clarity, training and validation accuracies, and model loss, respectively."
}