{
  "publication/title": "Classification of Ear Imagery Database using Bayesian Optimization based on CNN-LSTM Architecture.",
  "publication/authors": "Mohammed KK, Hassanien AE, Afify HM",
  "publication/journal": "Journal of digital imaging",
  "publication/year": "2022",
  "publication/pmid": "35296939",
  "publication/pmcid": "PMC9485378",
  "publication/doi": "10.1007/s10278-022-00617-8",
  "publication/tags": "- Bayesian Optimization\n- Hyperparameter Tuning\n- Deep Learning\n- Convolutional Neural Networks\n- Long Short-Term Memory Networks\n- Image Classification\n- Ear Disease Diagnosis\n- Medical Imaging\n- Machine Learning\n- Computational Optimization",
  "dataset/provenance": "The dataset used in this study is the ear imagery database, which is publicly available. This database consists of 880 otoscopic images, collected from patients with both right and left ears using a digital otoscope. The images were obtained from otolaryngologists at the Clinical Hospital from Universidad de Chile. Each image in the database is stored as an RGB image with a resolution of 420 by 380 pixels.\n\nThe dataset includes four different classes of ear conditions: normal, myringosclerosis, earwax plug, and chronic otitis media (COM). Each class contains 220 images, ensuring a balanced representation across the different categories.\n\nThis dataset has been utilized in previous research, where machine learning algorithms were applied to achieve an accuracy of 93.9%. Additionally, a private dataset containing 389 images was used in another study, yielding an accuracy of 86.8% with machine learning algorithms. The use of larger datasets, such as the ear imagery database, has been shown to be more effective in improving the classification process for ear conditions. However, the availability of large samples in the otolaryngology field is limited, which poses restrictions for CNN architectures.",
  "dataset/splits": "The dataset used in our study consists of a public ear imagery dataset containing 880 images. This dataset was split into training and testing sets in three different ratios to evaluate the performance of our approach. The splits were as follows:\n\n1. 90% training data and 10% test data.\n2. 80% training data and 20% test data.\n3. 70% training data and 30% test data.\n\nFor the 90% training and 10% testing split, the training set consisted of 792 images, and the testing set consisted of 88 images. For the 80% training and 20% testing split, the training set consisted of 704 images, and the testing set consisted of 176 images. For the 70% training and 30% testing split, the training set consisted of 616 images, and the testing set consisted of 264 images.\n\nThe dataset includes four classes: normal, myringosclerosis, earwax plug, and chronic otitis media (COM), with each class containing 220 images. This distribution ensures that the dataset is balanced, allowing for a fair evaluation of the classification performance across different ear conditions.",
  "dataset/redundancy": "The datasets used in our study were split into training and testing sets to evaluate the performance of our proposed approach. Specifically, we utilized a public ear imagery dataset containing 880 images, which was previously applied to machine learning algorithms with an accuracy of 93.9%. For our experiments, we employed different splits of the data: 90% for training and 10% for testing, 80% for training and 20% for testing, and 70% for training and 30% for testing. These splits ensured that the training and test sets were independent, which is crucial for obtaining unbiased performance metrics.\n\nTo enforce the independence of the training and test sets, we carefully partitioned the dataset such that no images from the training set were included in the test set. This was achieved by randomly shuffling the dataset and then dividing it according to the specified ratios. This method helps in preventing data leakage and ensures that the model's performance on the test set is a true reflection of its generalization capability.\n\nThe distribution of our dataset compares favorably to previously published machine learning datasets in the field of otolaryngology. Many previous works have relied on smaller, private datasets, which often lack the diversity and size needed for robust model training. In contrast, our public dataset is larger and more balanced, containing 880 images evenly distributed across four classes: normal, myringosclerosis, earwax plug, and chronic otitis media (COM). This balanced distribution helps in achieving more accurate and reliable classification results.\n\nAdditionally, our approach involved the use of Bayesian optimization to automatically extend the iterations and stop when the maximum optimized values were reached. This optimization process helped in generating the best hyperparameters for the LSTM architecture, which in turn improved the multi-class classification performance. The confusion matrix was used to evaluate the classifier's accuracy, providing a clear relation between the correct predicted samples and the misclassified samples.\n\nIn summary, our dataset splitting strategy ensured independent training and test sets, and the distribution of our dataset is more balanced and larger compared to many previously published datasets. This contributes to the robustness and reliability of our classification results.",
  "dataset/availability": "The dataset used in our study is publicly available. Specifically, we utilized the ear imagery dataset containing 880 images, which has been previously applied to machine learning algorithms. This dataset is accessible and has been used in other research works for comparing algorithms. However, it is important to note that this public database is not balanced for classifying the different classes of ear diseases.\n\nThe dataset was split into training and testing sets. For our experiments, we used 90% of the images for training and 10% for testing. This split was enforced to ensure that the model's performance could be evaluated on unseen data, providing a more reliable assessment of its generalization capabilities.\n\nRegarding the license, the dataset is available for research purposes, allowing other researchers to use it for similar studies. This accessibility promotes reproducibility and further advancements in the field of ear condition diagnosis using machine learning techniques.",
  "optimization/algorithm": "The optimization algorithm employed in our study is Bayesian optimization. This method is not new but is widely recognized for its effectiveness in hyperparameter tuning, particularly in complex models like Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks.\n\nBayesian optimization is chosen for its ability to efficiently search for the optimal hyperparameters by leveraging prior knowledge and updating it with new information. This approach helps in reducing computational costs and improving the classification accuracy of the models.\n\nThe reason this algorithm is discussed in a digital imaging journal rather than a machine-learning journal is that our primary focus is on the application of these optimization techniques to improve the performance of medical image classification tasks. Specifically, we applied Bayesian optimization to enhance the accuracy of identifying ear conditions using CNN and BiLSTM architectures. The results demonstrate significant improvements in classification accuracy, making it a valuable contribution to the field of medical imaging rather than purely to machine learning.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. Instead, it focuses on optimizing hyperparameters for a specific architecture, namely the LSTM architecture. The optimization process involves selecting initial hyperparameters, evaluating an objective function using an acquisition function, running iterations, and selecting the best-optimized values. These optimized hyperparameters are then used in the validation database.\n\nThe LSTM architecture itself includes several layers: an input layer consisting of features extracted from a CNN network, a BiLSTM layer with 2000 hidden units, a dropout layer set to 0.5 to prevent overfitting, and a fully connected layer with an output size corresponding to the number of classes, followed by a softmax layer. This architecture is designed to handle sequential data and address issues like vanishing gradients, making it suitable for tasks involving time-series or sequential information.\n\nThe Bayesian optimization algorithm is employed to tune the hyperparameters of the LSTM architecture, including learning rate, momentum, regularization, and maximum epoch. This optimization process aims to find the best hyperparameters efficiently, reducing the time and computational costs associated with manual tuning or traditional optimization methods. The objective function created for the Bayesian optimizer uses the training and testing datasets as inputs, training the LSTM architecture and returning the classification error on the testing set.\n\nIn summary, the model is not a meta-predictor but rather a specialized optimization approach for tuning the hyperparameters of an LSTM architecture. The training data used in this process is independent and specifically designed for the LSTM architecture, ensuring that the optimization is tailored to the unique requirements of the model.",
  "optimization/encoding": "In our study, we utilized an ear imagery dataset containing 880 images, with each image having a size of 420 x 380 pixels. The dataset was divided into four categories: normal, myringosclerosis, earwax plug, and chronic otitis media (COM), with 220 images for each type. The images were pre-processed and encoded using a CNN-EfficientNetb0 architecture to extract features. This process yielded 1280 features for each image, resulting in a total of 880 cells, which were arranged in a cell array to obtain a 1280 x 1 cell matrix for each image. This encoding facilitated the subsequent classification process using a combined CNN-LSTM architecture, which was optimized using Bayesian optimization techniques. The dataset was split into 90% training images and 10% testing images to evaluate the performance of the proposed approach. The training images were used to generate the best hyperparameters for the multi-class classification process, while the testing images were used to record the approach's performance using evaluated metrics.",
  "optimization/parameters": "In our study, four hyperparameters were selected for optimization: learning rate, momentum, regularization, and maximum epoch. These parameters were chosen based on their proven effectiveness in previous research for similar databases. The learning rate determines the step size during gradient descent, influencing how quickly the model learns. Momentum helps accelerate gradients vectors in the right directions, thus leading to faster converging. Regularization prevents overfitting by adding a penalty for large weights. The maximum epoch defines the number of complete passes through the training dataset. These parameters were optimized using Bayesian optimization to achieve the best classification performance. The initial and final values for these hyperparameters are detailed in our results, showing their impact on the model's accuracy.",
  "optimization/features": "In our study, we extracted 1280 features for each image. These features were then arranged in a cell array to obtain a matrix of 1280 by the number of cells for each image. The total number of cells obtained was 880.\n\nFeature selection was not explicitly mentioned as a separate step in our process. The features were directly extracted from the images using a convolutional neural network (CNN) and then used as input for the subsequent steps in our approach. The CNN architecture used was EfficientNetb0, which is known for its efficient feature extraction capabilities.\n\nThe features were extracted from the images and then used to train and test our model. The process of feature extraction and subsequent model training was performed using the training set only, ensuring that the testing set remained independent and unbiased. This approach helped us to maintain the integrity of our evaluation metrics and to ensure that our results were not influenced by data leakage or overfitting.",
  "optimization/fitting": "In our study, we employed Bayesian optimization to fine-tune the hyperparameters of our model, ensuring that we balanced the complexity of the model to avoid both overfitting and underfitting.\n\nThe number of parameters in our model was indeed larger than the number of training points, which is a common scenario in deep learning. To mitigate the risk of overfitting, we utilized regularization techniques. Specifically, we applied ridge regularization, which adds a penalty equal to the square of the magnitude of the coefficients to the loss function. This helps to keep the model weights small, reducing the model's complexity and preventing it from fitting the noise in the training data.\n\nAdditionally, we used an acquisition function in the Bayesian optimization process. This function balances exploration and exploitation, ensuring that the model does not get stuck in local minima and continues to explore the hyperparameter space effectively. This approach helps in finding a set of hyperparameters that generalize well to unseen data.\n\nTo address underfitting, we monitored the model's performance on a validation dataset during the training process. The validation dataset was separate from the training data and was used to evaluate the model's performance objectively. By observing the validation performance, we could ensure that the model was not too simplistic to capture the underlying patterns in the data.\n\nFurthermore, we conducted multiple iterations of Bayesian optimization, allowing the model to refine its hyperparameters progressively. This iterative process helped in fine-tuning the model to achieve a good balance between bias and variance, thereby avoiding underfitting.\n\nIn summary, through the use of regularization, an effective acquisition function, and rigorous validation, we ensured that our model neither overfitted nor underfitted the data. This approach led to a robust model that generalized well to new, unseen data.",
  "optimization/regularization": "In our study, we employed regularization as a technique to prevent overfitting. Regularization is crucial for ensuring that the model generalizes well to unseen data by avoiding excessive complexity. We utilized ridge regularization, which involves adding a penalty equal to the square of the magnitude of coefficients to the loss function. This method helps in minimizing the weights of the model, thereby reducing the risk of overfitting. The regularization parameter, often denoted as \u03bb, controls the strength of this penalty. By carefully tuning this parameter, we aimed to achieve a balance between fitting the training data well and maintaining good generalization performance on validation and test datasets. This approach ensures that our model remains robust and reliable in practical applications.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule are reported in the publication. The selected hyperparameters include learning rate, momentum, regularization, and max epoch. The initial and final values for these hyperparameters are detailed in the text. For instance, the learning rate starts at 1\u00d710\u207b\u2074 and can reach values like 0.000224 after optimization. The momentum begins at 0.8 and can finalize at 0.97692. Regularization starts at 1\u00d710\u207b\u00b9\u2070 and can end at 0.0031402. The max epoch starts at 20 and can increase to 100.\n\nThe optimization process involves running for a specified number of iterations, typically 30, to find the best-optimized values. These values are then used in the validation database to evaluate the model's performance. The evaluation metrics include accuracy, sensitivity, specificity, and positive predictive value (PPV).\n\nThe model files and specific optimization parameters are not explicitly detailed in the text, but the process and results of the optimization are thoroughly described. The optimization criterion and the steps involved in the Bayesian optimization for the LSTM architecture are clearly outlined. This includes selecting initial hyperparameters, evaluating the objective function, running iterations, and selecting the best-optimized values.\n\nRegarding availability and licensing, the text does not provide specific information on where to access the model files or the exact licensing terms. However, the methods and results are presented in a manner that allows for replication and further study. For detailed model files and specific optimization parameters, readers would likely need to contact the authors or refer to supplementary materials if available.",
  "model/interpretability": "The model employed in this study is primarily a combination of Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks, which are known for their complexity and are often considered black-box models. This means that while they can achieve high accuracy in tasks such as image classification, the internal workings and decision-making processes are not easily interpretable.\n\nThe CNN component of the model is designed to extract spatial features from the input images. It consists of convolutional layers that apply filters to the input data to detect various features, pooling layers that downsample the feature maps to reduce dimensionality, and fully connected layers that integrate the extracted features to make final predictions. While the architecture of CNNs is well-defined, the specific features that each filter detects and how these features contribute to the final output are not straightforward to interpret.\n\nThe LSTM component is used to handle sequential data and capture temporal dependencies. It includes layers such as BiLSTM, which processes data in both forward and backward directions, and dropout layers to prevent overfitting. The LSTM architecture is particularly effective for tasks involving time-series data or sequences, but the internal states and gates within the LSTM units are complex and not easily interpretable.\n\nTo enhance the interpretability of the model, Bayesian optimization is used to tune the hyperparameters of the LSTM architecture. This optimization process helps in selecting the best hyperparameters, such as learning rate and regularization, which can improve the model's performance. However, the optimization itself does not provide insights into how the model makes predictions.\n\nIn summary, while the model achieves high performance in classifying ear imagery, it remains largely a black-box model. The use of CNNs and LSTMs, along with Bayesian optimization, contributes to its effectiveness but does not provide clear interpretability. Future work could focus on developing techniques to make these models more transparent, such as using attention mechanisms or generating visual explanations for the model's decisions.",
  "model/output": "The model is designed for classification tasks. Specifically, it is used to classify ear conditions into four distinct categories: normal, myringosclerosis, earwax plug, and chronic otitis media (COM). The model employs a combination of Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks to achieve high accuracy in classifying these conditions. The CNN component extracts spatial features from the ear images, while the LSTM component handles sequential dependencies, enhancing the overall classification performance. The output of the model is a classification result indicating one of the four ear conditions. The model's effectiveness is demonstrated through experimental results, which show perfect accuracy, sensitivity, specificity, and positive predictive value (PPV) on the testing set. This indicates that the model is highly reliable for classifying the specified ear conditions.",
  "model/duration": "The execution time for the model varied depending on the architecture used. For one iteration, the training time on the CNN network was significantly higher, taking approximately 637 minutes. In contrast, the combined CNN-LSTM network achieved a much lower training time of about 25 minutes per iteration. This substantial difference highlights the efficiency of the proposed CNN-LSTM approach with Bayesian optimization, which not only improved accuracy but also reduced the computational time required for training. The CNN-LSTM network's shorter training time makes it a more practical choice for real-world applications where quick diagnosis is crucial.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the proposed approach involved a comprehensive assessment using an ear imagery dataset. The dataset was divided into different training and testing splits to evaluate the performance robustly. Specifically, the dataset was split into 90% training data with 10% test data, 80% training data with 20% test data, and 70% training data with 30% test data. This approach allowed for a thorough evaluation of the model's generalization capabilities.\n\nThe performance of the models was assessed using four key metrics: accuracy, sensitivity, specificity, and positive predictive value (PPV). These metrics provided a holistic view of the model's effectiveness in classifying ear conditions. Accuracy measured the overall correctness of the classifier, sensitivity focused on avoiding false negatives, specificity ensured correct identification of disease-free samples, and PPV evaluated the precision of the classified samples.\n\nThe CNN-LSTM model, combined with Bayesian optimization, demonstrated exceptional performance. For the 90% training and 10% test split, the CNN-LSTM model achieved 100% accuracy, sensitivity, specificity, and PPV. Similarly, for the 80% training and 20% test split, the model maintained 100% across all metrics. Even with a more challenging 70% training and 30% test split, the model achieved high performance with accuracy, sensitivity, specificity, and PPV values of 99.62%, 99.65%, 99.88%, and 99.62%, respectively.\n\nThe Bayesian optimization process involved selecting initial hyperparameters, evaluating an objective function using an acquisition function, running for 30 iterations, and selecting the best-optimized values. These optimized hyperparameters were then applied to the validation dataset to ensure the model's robustness and generalizability.\n\nThe confusion matrix was used to visualize the classifier's accuracy, showing the relationship between correctly predicted samples and misclassified samples. This visualization helped in understanding the model's performance in detail.\n\nOverall, the evaluation method ensured a rigorous assessment of the proposed approach, demonstrating its superiority in classifying ear conditions with high accuracy and reliability.",
  "evaluation/measure": "In our evaluation of the LSTM architecture, we focused on four key performance metrics: accuracy, sensitivity, specificity, and positive predictive value (PPV). These metrics are widely recognized and used in the literature for evaluating classifier performance, ensuring that our results are comparable with previous works.\n\nAccuracy measures the overall effectiveness of the classifier, providing a general indication of how well the model performs across all classes. Sensitivity, also known as recall, is crucial for avoiding false negative samples, which is particularly important in medical diagnostics where missing a positive case can have serious consequences. Specificity, on the other hand, measures the ability of the classifier to correctly identify samples without the disease, ensuring that healthy cases are not misclassified. PPV, or precision, calculates the proportion of true positive predictions among all positive predictions, giving insight into the reliability of positive results.\n\nThese metrics collectively provide a comprehensive view of the classifier's performance, covering different aspects of its effectiveness. By reporting these metrics, we aim to offer a thorough evaluation that is both representative of the current literature and informative for practical applications in ear disease diagnosis.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our proposed approach with other previous works. We built our approach using a combination of CNN-LSTM and a CNN classifier, and applied it to the ear imagery dataset. To assess performance, we calculated accuracy, sensitivity, specificity, and PPV using different splits of training and test data: 90% training with 10% testing, 80% training with 20% testing, and 70% training with 30% testing.\n\nFor the 90% training and 10% testing split, the CNN-LSTM approach outperformed the CNN classifier, achieving 100% in all four metrics. The CNN classifier, on the other hand, reached an accuracy of 86.3%, which is the lowest among previous works. A previous machine learning approach on the same dataset achieved an accuracy of 93.9%.\n\nWe also incorporated Bayesian optimization with the CNN-LSTM, which further improved the performance metrics. For the 80% training and 20% testing split, the CNN-LSTM with Bayesian optimization achieved 100% in all metrics. For the 70% training and 30% testing split, the accuracy, sensitivity, specificity, and PPV were 99.62%, 99.65%, 99.88%, and 99.62%, respectively.\n\nOur comparison with previous works highlights the effectiveness of the CNN-LSTM approach, especially when combined with Bayesian optimization. However, it is important to note that direct comparisons with other studies are challenging due to differences in sample sizes, classes, and implemented techniques. Our work focuses on classifying ear disease images, including normal, myringosclerosis, earwax plug, and chronic otitis media, with high accuracy and minimal computational time.\n\nIn summary, our proposed approach was compared to simpler baselines and publicly available methods on the ear imagery dataset, demonstrating superior performance in various metrics. The use of Bayesian optimization further enhanced the classification accuracy and efficiency.",
  "evaluation/confidence": "In our study, we evaluated the performance of our proposed approach using several metrics, including accuracy, sensitivity, specificity, and positive predictive value (PPV). These metrics were calculated for different splits of training and testing data (90% training with 10% testing, 80% training with 20% testing, and 70% training with 30% testing). For the 90% training and 10% testing split, our CNN-LSTM model achieved 100% in all four metrics, indicating perfect performance on the test dataset. For the 80% training and 20% testing split, the model again achieved 100% in all metrics. Even with a more challenging 70% training and 30% testing split, the model maintained high performance with accuracy, sensitivity, specificity, and PPV of 99.62%, 99.65%, 99.88%, and 99.62%, respectively.\n\nWhile we reported these metrics, we did not provide confidence intervals for them. The high and consistent performance across different data splits suggests strong and reliable results. However, without formal statistical tests or confidence intervals, it is challenging to definitively claim statistical significance compared to other methods. Our approach outperformed previous works, such as a CNN classifier that achieved an accuracy of 86.3% and a machine learning algorithm that reached 93.9%. The incorporation of Bayesian optimization with CNN-LSTM further enhanced performance, indicating its potential superiority.\n\nIn summary, while our results are promising and show high performance, a more rigorous statistical analysis, including confidence intervals and significance tests, would strengthen the claims of our method's superiority. Future work could include such analyses to provide a more comprehensive evaluation of our approach's effectiveness.",
  "evaluation/availability": "Not enough information is available."
}