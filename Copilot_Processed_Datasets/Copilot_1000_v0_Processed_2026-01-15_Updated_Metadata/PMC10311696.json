{
  "publication/title": "Performance of EHR classifiers for patient eligibility in a clinical trial of precision screening.",
  "publication/authors": "Alexander NVJ, Brunette CA, Guardino ET, Yi T, Kerman BJ, MacIsaac K, Harris EJ, Antwi AA, Vassy JL",
  "publication/journal": "Contemporary clinical trials",
  "publication/year": "2022",
  "publication/pmid": "36115637",
  "publication/pmcid": "PMC10311696",
  "publication/doi": "10.1016/j.cct.2022.106926",
  "publication/tags": "- Electronic Health Records\n- Clinical Trials\n- Precision Medicine\n- Genomic Medicine\n- Disease Classifiers\n- Patient Eligibility\n- Pragmatic Trials\n- Polygenic Risk Scores\n- Computable Phenotypes\n- Real-World Data\n- Veterans Health Administration\n- Diagnostic Codes\n- Trial Recruitment\n- Health Informatics\n- Predictive Analytics",
  "dataset/provenance": "The dataset for this study is derived from three primary sources. The first source is the VHA Corporate Data Warehouse (CDW), a relational database that has been housing clinical, accounting, and administrative data since 1999. This database is updated nightly from the Veterans Health Information Systems and Technology Architecture (VistA), the main electronic health record (EHR) system used by VHA providers. The CDW facilitates queries of structured data such as diagnosis codes and prescriptions.\n\nThe second source is VistA itself, which provides detailed EHR data used by VHA providers. The third source is eligibility telephone screen surveys conducted with potential participants at baseline.\n\nThe study initially identified 20,518 patients from V A Boston who met specific age, insurance, and primary care provider (PCP) relationship criteria. This dataset was used to develop and validate classifiers for six common diseases: atrial fibrillation (AFib), coronary artery disease (CAD), type 2 diabetes mellitus (T2D), breast cancer (BrCa), colorectal cancer (CRCa), and prostate cancer (PrCa).\n\nThe classifiers were developed using published criteria and validated through manual chart reviews. The dataset includes structured data such as diagnosis codes and prescription information, but it does not include unstructured data like images or clinical notes. The study aimed to use straightforward and efficient methods to optimize the performance of these classifiers, ensuring that the trial eligibility criteria were computable and improved recruitment efficiency.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "All data generated or analyzed during this study are included in this published article and its supplementary material. However, subject-level data that may allow identification of individual participants are not included. This approach ensures that the data is available for verification and further research while maintaining the privacy and confidentiality of the participants. The supplementary material provides additional details and supporting information that complement the main findings presented in the article. This includes information on the methods used, the data analysis, and any additional results that support the conclusions drawn in the study. The data and supplementary material are made available under the terms of the publishing agreement, which typically allows for non-commercial use and distribution with proper attribution. This ensures that the data can be accessed and used by other researchers for further studies while respecting the rights of the original authors.",
  "optimization/algorithm": "The optimization process described does not involve a traditional machine-learning algorithm. Instead, it relies on a series of iterative refinements based on manual clinician reviews and adjustments to disease classifiers. These classifiers are initially identified through literature searches and are then implemented and refined within the target Electronic Health Record (EHR) system.\n\nThe process begins with the implementation of a relevant disease classifier, which is then used to draw a random selection of positive and negative cases for manual clinician review. Cycles of classifier refinement and additional clinician review occur until the desired performance is achieved. This iterative process involves revising classifiers whose true positive rate (TPR) or true negative rate (TNR) is below 80%. The refinement strategy is inferred by manual inspection of misclassified records.\n\nThis approach is not a novel machine-learning algorithm but rather a method of optimizing existing classifiers through a combination of automated queries and manual reviews. The focus is on ensuring that the classifiers accurately identify patients with specific diseases, thereby improving the reliability of the screening process for clinical trials.\n\nThe reason this method was not published in a machine-learning journal is that it does not introduce a new algorithm but rather a practical application of existing techniques tailored to the specific needs of clinical trial recruitment. The emphasis is on the iterative refinement process and its effectiveness in improving classifier performance within a real-world healthcare setting.",
  "optimization/meta": "Not applicable. The optimization process described does not involve a meta-predictor. The classifiers were refined through manual inspection and heuristic adjustments rather than by integrating outputs from other machine-learning algorithms. The initial classifiers were based on published methods and were optimized through iterative cycles of manual review and refinement. No machine learning methods were used in the optimization process. The training data for the classifiers consisted of electronic health records (EHR) data, and the refinement process involved ensuring that the classifiers accurately reflected the medical records of the patients. The independence of the training data was not a concern in this context, as the focus was on improving the accuracy of the classifiers through manual adjustments.",
  "optimization/encoding": "The data encoding process involved several steps to ensure compatibility and accuracy. Initially, disease classifiers relied on ICD-9-CM and ICD-9-PCS codes, which were converted to their equivalent ICD-10 codes using conversion tables from the Centers for Medicare and Medicaid Services. This conversion was necessary because the Veterans Health Administration transitioned to ICD-10 codes in 2015. The conversion process was semi-automated, using an extraction procedure in R to collect corresponding ICD-10 codes from GEMS tables. Manual review supplemented this process, identifying additional relevant ICD-10 codes not captured by the automated method. The final set of ICD-10 codes was refined through manual text searches, ensuring comprehensive coverage.\n\nFor medication classifiers, data was drawn from both VHA outpatient prescription tables and non-VHA medication list tables. This dual-source approach ensured that all relevant medication information was included, regardless of whether the prescriptions were filled within or outside the VHA system.\n\nThe cancer classifiers were enhanced by incorporating data from the CDW Oncology tables, which contain high-quality ICD-O-encoded diagnoses abstracted by professional cancer registrars. This addition was crucial for improving the accuracy of the breast, prostate, and colorectal cancer classifiers. The Oncology tables, available since 2016, provided a reliable source of cancer diagnoses, leading to a perfect positive predictive value for colorectal cancer when used alone. This success prompted the inclusion of Oncology tables in the breast and prostate cancer classifiers as well.\n\nOverall, the data encoding process involved a combination of automated and manual methods to ensure that the classifiers could accurately process both pre-2015 and post-2015 ICD codes, as well as medication and cancer diagnosis data from various sources. This comprehensive approach aimed to preempt any potential issues arising from the upcoming transition from VistA to Cerner EHR.",
  "optimization/parameters": "The optimization process for the disease classifiers involved several parameters, primarily related to the true positive rate (TPR) and true negative rate (TNR). Initially, six published classifiers were evaluated for six target diseases: atrial fibrillation (AFib), coronary artery disease (CAD), type 2 diabetes (T2D), breast cancer (BrCa), colorectal cancer (CRCa), and prostate cancer (PrCa). The classifiers were assessed using blinded physician review, and any classifier with a TPR or TNR below 80% underwent refinement.\n\nThe selection of parameters was driven by the need to achieve a TPR and TNR of at least 80% for each disease. For AFib and CAD, which initially had TNRs below the optimal threshold, modifications were made based on manual inspection of misclassified records. For AFib, the classifier was refined by requiring two diagnostic codes on two distinct dates, and further optimized by excluding diagnostic codes originating from pharmacy staff. For CAD, the initial refinement involved the same requirement of two diagnostic codes on two distinct dates, which significantly improved the TNR.\n\nThe process of parameter selection and refinement was iterative, involving multiple rounds of blinded chart review and unblinded reassessment. Each round aimed to address specific issues identified in the misclassified records, ensuring that the classifiers met the desired performance criteria. The final optimized classifiers were then implemented as pre-screening exclusion criteria for the GenoV A Study trial, demonstrating high real-world negative predictive values (NPV-RW) across different patient demographics.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "Not applicable.",
  "optimization/regularization": "The optimization process for the disease classifiers involved several rounds of refinement to improve their performance. Initially, all six published classifiers achieved a true positive rate (TPR) of 100%, but the true negative rate (TNR) varied, with some classifiers falling below the optimal threshold of 80%. To address this, manual inspection of false positive (FP) records was conducted. It was found that many FP cases involved ICD codes used for preliminary diagnoses that were later refuted. For instance, the ICD code for unspecified angina pectoris was often used without confirming a coronary artery disease (CAD) diagnosis.\n\nTo mitigate overfitting and enhance the classifiers' performance, modifications were made. For the CAD and atrial fibrillation (AFib) classifiers, which had TNRs below 80%, the requirement was set for two diagnostic codes on two distinct dates. This first round of refinement significantly improved the CAD classifier's TNR to 83%, but the AFib classifier saw less improvement, achieving a TNR of 71%. Further manual review identified additional misclassifications, such as AFib diagnostic codes erroneously used during encounters with the anticoagulation clinic. To address this, the AFib classifier was further optimized by excluding diagnostic codes originating from pharmacy staff, ultimately achieving a TNR of 83%.\n\nThese iterative refinement steps, including manual review and adjustment based on specific misclassification patterns, served as a form of regularization. By ensuring that the classifiers were not overly reliant on single diagnostic codes or specific clinical contexts, the risk of overfitting was reduced. This process helped in achieving a balanced performance across different diseases and patient demographics, ensuring robust and reliable classification in real-world settings.",
  "optimization/config": "Not applicable.",
  "model/interpretability": "The model used in this study is not a blackbox model. It is based on computable disease classifiers that utilize real-world data from electronic health records (EHRs). These classifiers are designed to be interpretable and transparent, as they rely on well-defined criteria and diagnostic codes.\n\nThe classifiers were initially derived from published algorithms and then validated and refined through a systematic process. This process involved blinded physician reviews and iterative modifications based on misclassified records. For instance, the classifier for atrial fibrillation was refined by excluding diagnostic codes originating from pharmacy staff, which were found to be erroneous in some cases. Similarly, the classifier for coronary artery disease was optimized by requiring two diagnostic codes on two distinct dates, reducing false positives.\n\nThe transparency of the model is further ensured by the use of structured query language (SQL) stored procedures. These procedures query the corporate data warehouse (CDW) for specific patient-PCP relationships and visit-associated data. The eligibility table, which is refreshed nightly, identifies new eligible patients and removes those who are no longer eligible due to new diagnoses or aging out of the eligibility criteria.\n\nMoreover, the study provides detailed performance metrics for each disease classifier, including true positive rates, true negative rates, and real-world negative predictive values. These metrics are reported for the overall population as well as for subgroups based on sex and race/ethnicity. This level of detail allows for a clear understanding of how the classifiers perform in different contexts.\n\nIn summary, the model's transparency is evident in its use of well-defined criteria, systematic validation and refinement processes, and detailed performance reporting. This ensures that the classifiers are interpretable and that their decisions can be understood and trusted by clinicians and researchers.",
  "model/output": "The model in question is a classification model. It is designed to identify patients with specific diseases based on electronic health records (EHR) data. The model's performance is evaluated using metrics such as true positive rate (TPR) and true negative rate (TNR), which are typical for classification tasks. The model classifies patients as either having or not having one of six target diseases: atrial fibrillation, coronary artery disease, type 2 diabetes, breast cancer, colorectal cancer, and prostate cancer. The output of the model is used to determine patient eligibility for a clinical trial, further indicating its classification nature.\n\nThe model underwent iterative refinement to achieve a TPR and TNR of at least 80% for each disease. This refinement process involved manual inspection of misclassified records and adjustments to the classification criteria. Once optimized, the model was implemented as a pre-screening exclusion criterion for the GenoV A Study trial. The real-world performance of the model was assessed by calculating the negative predictive value (NPV-RW), which measures the proportion of true negatives among those predicted to be negative by the model.\n\nThe model's output is integrated into a Structured Query Language (SQL) stored procedure that queries the Corporate Data Warehouse (CDW) for patient data. This procedure is automated to refresh the eligibility table nightly, ensuring that the model's output is up-to-date with the latest patient information. The eligibility table is then used by research staff to identify potential participants for the trial and to send recruitment mailings.\n\nIn summary, the model is a classification model that predicts the presence or absence of specific diseases in patients based on EHR data. Its output is used to determine patient eligibility for a clinical trial, and it has undergone rigorous validation and refinement to ensure high performance.",
  "model/duration": "The execution time of the model is not directly mentioned. However, the process involved several stages, including initial identification of patients, refinement of classifiers, and real-world evaluation. The initial identification of patients meeting specific criteria was completed by April 2020. The refinement of classifiers, particularly for atrial fibrillation and coronary artery disease, involved multiple rounds of blinded chart reviews and modifications. The real-world evaluation began in June 2020 and continued through November 2021, during which time the optimized classifiers were implemented as pre-screening exclusion criteria. This period included the screening of 1,077 participants, with the classifiers' performance being assessed over these 16 months. The nightly refresh of the eligibility table and the regular querying for recruitment mailings indicate an ongoing process, but specific execution times for individual model runs are not detailed.",
  "model/availability": "Not applicable",
  "evaluation/method": "The evaluation method for our disease classifiers involved a rigorous process of validation and iterative refinement. Initially, we identified well-performing published computable classifiers for six target diseases: atrial fibrillation, coronary artery disease, type 2 diabetes, breast cancer, colorectal cancer, and prostate cancer. These classifiers were validated in our target population through blinded physician review.\n\nIf the initial classifiers did not meet our performance criteria of true positive and true negative rates of at least 80%, they underwent refinement. This involved manual inspection of misclassified records to infer refinement strategies. Subsequent rounds of classifier modification and chart review of new sets of records were conducted until the desired performance metrics were achieved.\n\nOnce optimized, these classifiers were implemented as pre-screening exclusion criteria for the GenoV A Study. The real-world performance of these classifiers was assessed during the first 16 months of trial enrollment. This involved sending recruitment letters to potentially eligible participants and conducting telephone eligibility screens. During these screens, study staff asked participants about their diagnosis history for the target diseases. Cases where the research staff was uncertain about the diagnosis were escalated to a study physician for chart review and final determination.\n\nThe real-world negative predictive value (NPV-RW) was calculated as the proportion of true negatives within the set of screened predicted-negative patients. This evaluation method allowed us to assess the performance of our disease classifiers in a practical, real-world setting, ensuring their reliability and validity for participant screening in clinical trials.",
  "evaluation/measure": "In the evaluation of our disease classifiers, we report several key performance metrics to assess their accuracy and reliability. The primary metrics we focus on are the true positive rate (TPR) and the true negative rate (TNR). TPR, also known as sensitivity, is calculated as the ratio of true positives (TP) to the sum of true positives and false negatives (FN). TNR, or specificity, is defined as the ratio of true negatives (TN) to the sum of true negatives and false positives (FP).\n\nAdditionally, we quantify the number of call changes, which represent the instances where a reviewer altered their assessment of a medical record after being informed of a discrepancy between their blinded classification and the computerized classification.\n\nDuring the initial stages, we also report the number of mispredicted as negative cases and the negative predictive value in the real world (NPV-RW). NPV-RW is calculated as the proportion of true negatives within the set of screened predicted-negative patients.\n\nThese metrics are evaluated across different demographics, including sex and race/ethnicity, to ensure the classifiers perform well across diverse populations. The metrics are also assessed over multiple rounds of iterative refinement to demonstrate improvements in classifier performance.\n\nThe set of metrics reported is representative of standard practices in the literature for evaluating disease classifiers. TPR and TNR are widely used in medical research to assess the performance of diagnostic tools and classifiers. The inclusion of call changes provides additional insight into the reliability of the classifiers by showing how often human reviewers change their minds based on the classifier's output. The reporting of NPV-RW and its breakdown by demographics ensures that the classifiers are not only accurate but also fair and generalizable across different patient groups.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our approach focused on evaluating and refining previously published disease classifiers to optimize their performance for our specific trial, the Genomic Medicine at V A (GenoV A) Study. These classifiers were selected based on criteria such as ease of computability and suitability for our target electronic health record (EHR) system.\n\nWe did, however, compare the performance of these classifiers against manual expert chart reviews, which served as our gold standard. This process involved an iterative refinement of the classifiers, where we applied them to a randomly selected set of patient records, had a licensed physician review these records blinded to the classifications, and then refined the classifiers based on any discrepancies found. This cycle was repeated until the classifiers achieved true positive and true negative rates of at least 80%.\n\nRegarding simpler baselines, our study did not explicitly compare the classifiers to simpler baselines. Instead, we focused on enhancing the performance of the existing classifiers through iterative refinement. We did not consider more advanced computational methods such as machine learning, as these approaches may require additional time and computational resources that are out of reach for most pragmatic trials. Our goal was to develop computable trial eligibility criteria that greatly improved the efficiency of recruitment using a practical and resource-efficient approach.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "All data generated or analyzed during this study are included in this published article and its supplementary material. However, subject-level data that may allow identification of individual participants are not included. The supplementary material can be found in the web version on PubMed Central. The study was supported by the National Human Genome Research Institute. The data and materials are available to the public, but specific details about the license under which they are released are not provided."
}