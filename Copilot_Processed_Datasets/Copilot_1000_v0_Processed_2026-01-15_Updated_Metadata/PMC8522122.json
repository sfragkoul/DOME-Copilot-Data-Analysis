{
  "publication/title": "Hybrid deep learning of social media big data for predicting the evolution of COVID-19 transmission.",
  "publication/authors": "Chew AWZ, Pan Y, Wang Y, Zhang L",
  "publication/journal": "Knowledge-based systems",
  "publication/year": "2021",
  "publication/pmid": "34690447",
  "publication/pmcid": "PMC8522122",
  "publication/doi": "10.1016/j.knosys.2021.107417",
  "publication/tags": "- COVID-19\n- Data Assimilation\n- Deep Neural Networks\n- Natural Language Processing\n- Machine Learning\n- Time Series Forecasting\n- Social Media Analysis\n- Predictive Modeling\n- Epidemiology\n- Public Health",
  "dataset/provenance": "The dataset used in this study is sourced from Twitter, specifically the COVID-19 related tweets. The data collection period spans from January 23, 2020, to May 10, 2020. On average, there were approximately 1 million unique tweets, including retweets, collected each day within this period. This extensive dataset was gathered using Twitter's streaming API and Tweepy, allowing for the collection of a large-scale dataset that reflects the global community's emotional responses to the COVID-19 pandemic.\n\nThe dataset was pre-processed to remove inherent \"noises\" such as wrong spellings, punctuations, and differing expressions for underlying common messages. This preprocessing step is crucial for minimizing the noise and encapsulating the nuanced relationship between different words and their respective contexts.\n\nThe dataset was then encoded using Natural Language Processing (NLP) feature extraction methods, specifically the Continuous Bag of Words (CBOW) method from Word2Vec, to build semantic word representations of a defined vector size. This encoding process is essential for transforming the raw text data into a format suitable for modeling the proposed G parameter, which quantifies the relationship between the global transmission patterns of COVID-19 and the emotional responses from the global community.\n\nThe total data quantity available for analysis consists of 108 data instances on a daily basis, covering the period from January 25, 2020, to May 11, 2020. This dataset was used to train and validate the ODANN model, with 85% of the data allocated for training and the remaining 15% for testing. Within the training set, 20% was used for validation purposes. The model training did not involve random shuffling of the dataset, as the forecasting of the G parameter is based on continuous data extrapolation using the optimized weights of ODANN's hidden layers. This approach ensures that the model can accurately forecast the growth rate of confirmed COVID-19 cases on a global scale by leveraging the continuous influx of big data information.",
  "dataset/splits": "The dataset used in this study consists of 108 data instances, each representing a day from January 25, 2020, to May 11, 2020. The dataset was split into training, validation, and testing subsets without random shuffling, as the focus was on maintaining the temporal sequence of the data.\n\nThe primary split involved using 85% of the data for training and validation, and the remaining 15% for testing. Within the training and validation set, 20% of the data was used for validation purposes. This resulted in the following distribution:\n\n* Training and validation set: 92 data points (85% of 108)\n    * Training set: 73 data points (80% of 92)\n    * Validation set: 19 data points (20% of 92)\n* Testing set: 16 data points (15% of 108)\n\nAdditionally, to investigate the effects of varying dataset sizes, several other splits were considered:\n\n* 60% for training and validation, 40% for testing\n* 70% for training and validation, 30% for testing\n* 80% for training and validation, 20% for testing\n\nIn each of these cases, 20% of the training and validation set was used for validation. The specific number of data points in each split would vary accordingly based on the percentage allocated.",
  "dataset/redundancy": "The datasets used in our study were split into training, validation, and testing subsets without random shuffling. This approach was taken to maintain the temporal order of the data, which is crucial for time-series modeling. The data spans from January 25, 2020, to May 11, 2020, and consists of 108 daily data instances.\n\nFor the training phase, 85% of the total data was allocated, with 20% of this portion used for validation. The remaining 15% of the data was reserved for testing. This split ensures that the training and test sets are independent, as they do not overlap in time. The validation set, being a subset of the training data, helps in tuning the model parameters without directly affecting the test set performance.\n\nThe decision to avoid shuffling is rooted in the need for continuous data for accurate forecasting. The model, ODANN, relies on the sequential nature of the data to extrapolate future values based on historical trends. This method contrasts with traditional machine learning datasets, where shuffling is common to ensure randomness and independence between training and test sets. However, for time-series data, preserving the temporal order is essential to capture the underlying patterns and dependencies over time.\n\nThe distribution of the data in the training set includes periods of both increasing and decreasing values of the G parameter, which is the metric being modeled. This balanced distribution helps the model learn from diverse patterns, enhancing its predictive capability. The test set, being independent and drawn from a later period, provides an unbiased evaluation of the model's performance.",
  "dataset/availability": "Not applicable.",
  "optimization/algorithm": "The machine-learning algorithm class used is a type of deep neural network, specifically designed for time-series modeling. This algorithm is referred to as ODANN, which stands for a deep neural network with data assimilation.\n\nODANN is not a standard, off-the-shelf algorithm but rather a novel approach tailored for the specific task of modeling the global growth rate parameter (G) related to COVID-19. The novelty of ODANN lies in its ability to concatenate encoded input features from large volumes of COVID-19-related Twitter data with historical time-series records of the G parameter into a single end-to-end model framework. This integration allows for near real-time predictions, which is crucial for understanding and responding to the dynamics of the pandemic.\n\nThe reason ODANN was not published in a machine-learning journal is that the focus of this work is on applying machine learning techniques to a specific problem in the context of COVID-19 modeling. The primary goal is to demonstrate the effectiveness of the proposed method in forecasting the G parameter, rather than introducing a new machine-learning algorithm per se. The innovation here is in the application and integration of existing techniques to address a pressing real-world problem. The results and methodologies are presented in a knowledge-based systems journal, highlighting the interdisciplinary nature of the research.",
  "optimization/meta": "The model discussed in this publication is not a meta-predictor. It does not use predictions from other machine-learning algorithms as input. Instead, it relies on data assimilation using a historical time window of 5 days and input features layers built upon the Continuous Bag of Words (CBOW) feature extraction method. The model in question is an Optimized Deep Autoencoder Neural Network (ODANN), which is trained and validated using varying sizes of datasets to maximize predictive accuracy.\n\nThe cross-validation process for the ODANN model involves investigating the effects of varying the sizes of the training and testing datasets. The datasets used for training and validation range from 60% to 85% of the total available data, with the remaining percentage used for testing. This approach ensures that the training data is independent and that the model's predictive capability is thoroughly evaluated.\n\nThe ODANN model's performance is compared with other time-series models, including ARIMA, AutoARIMA, Random Forest (RF), Support Vector Machine (SVM), Long Short-Term Memory (LSTM), and Prophet. The comparisons are made using various statistical analyses, such as Paired T-test, Kruskal-Wallis test, and Wilcoxon Signed Rank test, as well as correlation coefficients like Pearson, Spearman, and Kendall's tau. These analyses help to determine the model's predictive accuracy and its ability to encapsulate the growth trajectories of the analyzed parameter over time.\n\nIn summary, the ODANN model is a standalone deep learning approach that does not rely on meta-prediction techniques. It is trained and validated using independent datasets, and its performance is rigorously evaluated against other established time-series models.",
  "optimization/encoding": "The data encoding process began with the extraction of COVID-19 Twitter text data from a pool of Tweet IDs collected from relevant historical days, with a one-day lead time. The extracted text data underwent a series of pre-processing steps to remove inherent \"noises.\" This included tokenizing each tweet sentence into individual unique words, removing stopwords using the Natural Language Toolkit (NLTK) library, eliminating punctuations and unknown symbols, and then combining the remaining words to form a new sentence for each Tweet ID.\n\nFollowing pre-processing, the data was encoded into numerically useful input features using various Natural Language Processing (NLP) feature extraction methods. These methods included TfidfVectorizer, Word2Vec\u2013Continuous Bag of Words (CBOW), and Word2Vec\u2013Skip-gram. The encoded input features were then used to build semantic word representations of defined vector sizes. The optimal vector size for the input features layer was determined to be 500 neurons, which was found to improve the model's predictive accuracy.\n\nThe encoded input features were then used in the training of the ODANN model, which also assimilated historical G values based on a pre-defined rolling time window of 5 days. This data assimilation component significantly improved the model's predictive capability, as evidenced by the reduced error scores in the validation and testing steps. The use of CBOW as the feature extraction method, along with a batch size of 2, resulted in the best agreement between predicted and measured G values, indicating that CBOW is effective in capturing the inherent context of the tweets made by netizens towards COVID-19.",
  "optimization/parameters": "In the optimization of our model, several parameters were used and fine-tuned to achieve the best predictive performance. The primary parameters include the vector size for the input features layer, the rolling time-window size for data assimilation, and the batch size for training.\n\nThe vector size for the input features layer was selected from values of 100, 500, and 5000. This selection was based on a series of trial-and-error experiments aimed at determining the optimal size that would achieve the highest prediction accuracy. The vector size is crucial as it aggregates the quantitative contextual relationship among different words, which is essential for modeling the temporal G values on a daily basis.\n\nThe rolling time-window size for data assimilation was explored with sizes of 3, 5, and 7 days. This parameter is critical for incorporating historical data into the model, allowing it to learn from past trends and improve its predictive capability. The optimal rolling time-window size was determined through a grid-search process, which involved evaluating the model's performance with different window sizes.\n\nThe batch size for training was varied to include values of 2, 4, 6, 8, 12, and 16. The batch size affects the stability and convergence of the training process. Through experimentation, the optimal batch size was identified as the one that minimized the error metrics during the validation step.\n\nAdditionally, other hyperparameters such as the number of neurons in hidden layers, the learning rate, the activation function, and the optimization function were also carefully selected and tuned. For instance, the Exponential Linear Unit (ELU) was chosen as the activation function for all hidden layers, and the Adam optimization function was used to update the model weights during training.\n\nIn summary, the selection of parameters was driven by a combination of theoretical considerations and empirical evaluations. The goal was to find the optimal configuration that would minimize the error metrics and maximize the model's predictive accuracy.",
  "optimization/features": "In our study, the input features for the model were derived from Twitter data using various Natural Language Processing (NLP) feature extraction methods. The vector size of the input features layer was a critical hyperparameter that was fine-tuned to optimize the model's predictive accuracy. Specifically, we experimented with vector sizes of 100, 500, and 5000 using the CBOW and Skip-gram feature extraction methods, which are part of the Word2Vec algorithms. Additionally, we used the TfidfVectorizer method, which resulted in a vector size of 94,237.\n\nFeature selection was not explicitly performed in the traditional sense of reducing the number of features. Instead, we focused on determining the optimal vector size for the input features layer, which indirectly influences the number of features used. This optimization process was crucial for improving the model's performance. The vector size was chosen based on its ability to capture the complex emotional responses of the global population towards COVID-19, as reflected in the Twitter data.\n\nThe optimization of the vector size was conducted using the training dataset, ensuring that the model's performance was evaluated on unseen data during the testing phase. This approach helped in mitigating overfitting and ensuring that the model generalizes well to new data. The best results were achieved with a vector size of 500 using the CBOW feature extraction method, coupled with a data assimilation component and a rolling time window of 5 days. This configuration significantly improved the model's predictive accuracy, as evidenced by the reduced error scores in the testing phase.",
  "optimization/fitting": "In our study, we employed a deep learning model, specifically ODANN, to predict the global growth rate of COVID-19 cases using Twitter data. The model's input features were derived from various NLP feature extraction methods, with vector sizes ranging from 100 to 5000. The number of parameters in our model is indeed larger than the number of training points, which could potentially lead to overfitting.\n\nTo mitigate overfitting, we implemented several strategies. Firstly, we used a data assimilation component that incorporated historical time windows of 5 days. This approach helped the model to learn from the temporal dynamics of the data, rather than just memorizing the training points. Secondly, we employed cross-validation by varying the sizes of the training and validation datasets. This process ensured that the model's predictive capability was robust and not merely a result of overfitting to a specific subset of data. Additionally, we used techniques such as early stopping and dropout during the training process to prevent the model from becoming too complex and overfitting the training data.\n\nTo rule out underfitting, we fine-tuned the vector size of the input features layer. We found that increasing the vector size from 100 to 500, using CBOW or Skip-gram feature extraction methods, resulted in better agreement between the measured and predicted values. This indicated that the model was complex enough to capture the underlying patterns in the data. Furthermore, we used a batch size of 2 for the ODANN model, which helped in improving the model's predictive accuracy during the testing phase.\n\nIn summary, we addressed the potential issues of overfitting and underfitting by using data assimilation, cross-validation, early stopping, dropout, and fine-tuning the vector size of the input features layer. These strategies ensured that our model was neither too complex nor too simple, and it could generalize well to unseen data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and enhance the model's generalization capability. One of the key methods used was data assimilation with a historical time window. This approach helped the model to learn from the temporal dynamics of the data, reducing the likelihood of overfitting to noise in the training dataset.\n\nAdditionally, we varied the sizes of the training and validation datasets to cross-validate the model's predictive capability. This process involved different combinations of training and testing dataset splits, such as 60% for training and validation with 40% for testing, and so on. This strategy ensured that the model was robust and not overly reliant on any specific subset of the data.\n\nWe also fine-tuned the vector size of the input features layer, which is crucial for improving the model's goodness-of-fit between predicted and measured values. For instance, using the CBOW feature extraction method with a vector size of 500 was found to be optimal for maximizing the model's predictive accuracy.\n\nFurthermore, we avoided shuffling the time-series data, which is a common practice in traditional machine learning models. Instead, we maintained the temporal order of the data to preserve the sequential dependencies, which is essential for time-series forecasting.\n\nIn summary, our regularization methods included data assimilation, varying dataset sizes for cross-validation, fine-tuning the input features layer, and preserving the temporal order of the data. These techniques collectively helped in mitigating overfitting and improving the model's predictive performance.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are thoroughly detailed within the publication. Specifically, Table 3 summarizes the hyper-parameter values for training both ODANN with and without the data assimilation component. This table includes details such as the number of neurons in each hidden layer, the rolling time-window size, batch size, number of epochs, learning rate, activation function, optimization function, and key cost function.\n\nThe model configurations and optimization parameters are explicitly described to ensure reproducibility. For instance, the vector size for the input features layer varies among 100, 500, and 5000, depending on the feature extraction method used (CBOW or Skipgram). The batch sizes tested include 2, 4, 6, 8, 12, and 16, and the number of epochs is set to 50. The learning rate is fixed at 0.0001, and the activation function for all hidden layers is the Exponential Linear Unit (ELU). The optimization function used is Adam, with the mean squared error (MSE) set as the key cost function, aiming to achieve a value below 0.0100 for the model validation step.\n\nRegarding the availability of model files and optimization parameters, these are not directly provided in the publication. However, the detailed descriptions and configurations offered should enable researchers to replicate the models and optimization processes described. The publication adheres to standard academic practices, ensuring that the methods and results are transparent and reproducible. For specific inquiries about model files or additional data, readers are encouraged to contact the authors directly.",
  "model/interpretability": "The model discussed in this publication is not a blackbox. The ODANN model, which stands for Optimized Data Assimilation Neural Network, incorporates data assimilation techniques that make it more interpretable compared to traditional deep learning models. This is achieved by leveraging historical time-series data and concatenating it with encoded input features from large volumes of COVID-19 related Twitter data. This process allows the model to provide near-real-time predictions while maintaining a level of transparency.\n\nOne clear example of the model's transparency is the use of a rolling historical time-window. This window size, which can be 3, 5, or 7 days, includes additional neurons representing the actual G values from previous days. For instance, with a 3-day rolling time window, the model uses the historical records from the past three days to forecast the G value for the current day. This approach makes it easier to understand how past data influences current predictions.\n\nAdditionally, the model's performance is evaluated using error metrics such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE). These metrics provide a quantitative measure of the model's accuracy and help in understanding the model's predictive capability. The use of different NLP feature extraction methods, such as CBOW and Skip-gram, further enhances the model's interpretability by showing how different types of input features affect the model's performance.\n\nThe model's ability to assimilate additional data features, such as socio-economic factors and government policies, also contributes to its transparency. By incorporating these factors, the model can provide insights into how external variables influence the predicted G values. This makes the model more interpretable and useful for stakeholders who need to understand the underlying factors driving the predictions.\n\nIn summary, the ODANN model is designed to be transparent by incorporating data assimilation techniques, using a rolling historical time-window, evaluating performance with clear error metrics, and assimilating additional relevant data features. These aspects make the model more interpretable and useful for understanding the dynamics of the G parameter in the context of COVID-19.",
  "model/output": "The model is a regression model. It is designed to predict a continuous value, specifically the growth rate (G parameter) of COVID-19 cases on a global scale. The model takes into account historical data and Twitter data to forecast the G parameter, aiming to minimize error metrics such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE) between the predicted and measured G values.\n\nThe model's output is the predicted G value for a specific day, based on the input features extracted from Twitter data and historical time-series records. The predictive capability of the model is evaluated using these error metrics, which quantify the difference between the model's predictions and the actual measured values.\n\nThe model's architecture includes a data assimilation component, which incorporates historical time-series records into the hidden layers of the model. This component helps to improve the model's predictive accuracy by providing additional context from past data. The model is trained using a dataset ranging from January 25, 2020, to May 11, 2020, with 85% of the data used for training and validation, and the remaining 15% used for testing.\n\nThe model's performance is optimized by fine-tuning the vector size of the input features layer and selecting the appropriate feature extraction method. The Continuous Bag of Words (CBOW) method, with a vector size of 500, is found to best maximize the model's predictive accuracy when coupled with a historical time-window size of 5 days. The model's predictive capability is further enhanced by investigating varying sizes of the training and validation datasets, with the selection of 85% of the total available data quantity for training and validation expected to optimize the model's predictive capability.",
  "model/duration": "The model's execution time varies depending on the specific step being performed. Data hydration, which involves processing approximately one million COVID-19 Twitter text data daily, is the most time-consuming step, taking around 6 hours. Data pre-processing to derive the input feature layer takes about 20 minutes. Restoring the trained model takes approximately 20 seconds, while model predictions using the input feature layer, coupled with data assimilation, take around 10 seconds. The total runtime to perform all steps in a near-real-time context is approximately 6.4 hours, which is more than sufficient to ensure that predictions of the G-value on any given day can be performed with both efficiency and accuracy. This lead time of one day is crucial for the model's predictive capability.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the proposed method, ODANN, involved a comprehensive analysis using cross-validation with varying sizes of training, validation, and testing datasets. This approach was chosen to assess the model's predictive capability and robustness. The datasets used spanned from January 25, 2020, to May 11, 2020, encompassing 108 daily data instances.\n\nCross-validation was performed without shuffling the data, as the time-series nature of the data required maintaining the temporal order. The datasets were split into different combinations of training and testing sizes: 60% for training and validation with 40% for testing, 70% for training and validation with 30% for testing, 80% for training and validation with 20% for testing, and 85% for training and validation with 15% for testing. This method allowed for an in-depth investigation of how different data sizes affected the model's performance.\n\nThe performance metrics used for evaluation included Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE). These metrics were calculated for both the validation and testing datasets to provide a thorough assessment of the model's accuracy and reliability.\n\nThe results indicated that reducing the amount of data available for training and validation increased the error scores in the testing phase. This was attributed to a spike in the G value around Day 60, which caused the model to over-predict the remaining period, especially with smaller training datasets. Conversely, using 85% of the total available data for training and validation optimized the model's predictive capability. This larger training dataset included two independent periods that demonstrated both increasing and decreasing trends in the G values, providing a balanced data distribution that benefited the model's learning process.\n\nAdditionally, the evaluation considered the time-efficiency of the proposed method in a near-real-time context. The method was designed to handle daily data inputs with a lead time of one day, ensuring that the model could provide timely and accurate predictions. This approach was crucial for maintaining the model's relevance and effectiveness in a dynamic and rapidly changing environment.",
  "evaluation/measure": "In our evaluation, we report several key performance metrics to assess the accuracy of our model's predictions. These metrics include the Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE). These metrics are computed based on the difference between the predicted G values and the measured G values for each day in the dataset.\n\nThe MSE is calculated as the average of the squared differences between the predicted and measured values, providing a measure of the overall error magnitude. The RMSE is the square root of the MSE, which gives a more interpretable measure of the average error in the same units as the original data. The MAE, on the other hand, is the average of the absolute differences between the predicted and measured values, offering a straightforward measure of the average error without squaring the differences.\n\nThese metrics are widely used in the literature for evaluating the performance of predictive models, particularly in time-series forecasting and regression tasks. They provide a comprehensive view of the model's accuracy by capturing different aspects of the prediction error. The MSE and RMSE are sensitive to large errors, while the MAE is more robust to outliers. By reporting all three metrics, we ensure a thorough evaluation of our model's performance.\n\nIn addition to these standard metrics, we also consider the level of agreement between the predicted and measured G values, which is crucial for assessing the model's goodness-of-fit. This is particularly important in the context of forecasting the growth rate of COVID-19 cases, where accurate predictions are essential for informing public health decisions.\n\nOverall, the set of metrics we report is representative of the best practices in the field and provides a clear and comprehensive evaluation of our model's performance.",
  "evaluation/comparison": "In the evaluation of our proposed ODANN model, a comprehensive comparison was conducted with several alternative time-series prediction models and machine learning algorithms. This comparison aimed to forecast the same G parameter over time, providing a robust validation of ODANN's effectiveness.\n\nSeveral classical time-series prediction models and machine learning algorithms were employed for this comparison, including ARIMA, AutoARIMA, Prophet, Random Forest (RF), Support Vector Machine (SVM), and Long Short-Term Memory (LSTM). These models were used to perform the same combined training, validation, and testing phases as ODANN, with 85% of the total data quantity allocated for model training and validation, and 15% for model testing. This approach allowed for a direct comparison of the dynamic behavior of the proposed G parameter over time.\n\nThe results of this comparison highlighted that ODANN, coupled with data assimilation using a historical time window size of 5 days and input feature layers built upon the CBOW feature extraction method, yielded the most satisfactory results in forecasting the G values over time. The evaluation metrics used were Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE), all of which indicated that ODANN outperformed the alternative models.\n\nStatistical analyses, including Type 1 paired T-test, Kruskal\u2013Wallis test, and Wilcoxon Signed Rank test, were conducted to estimate the probability of obtaining results at least as extreme as those observed, assuming the respective null hypothesis was correct. Additionally, Type 2 analyses, such as Pearson correlation coefficient, Spearman correlation coefficient, and Kendall\u2019s tau, were used to quantify the level of association between the time-series predictions made by ODANN and the alternative models.\n\nThe comparison revealed that while some models, like SVM, showed predictions that were consistently similar to those of ODANN in terms of average, median, and data distribution parameters, ODANN still outperformed them in final prediction accuracy. The trend of the prediction curves from the alternative models, except for Prophet, was reasonably consistent with the measured G values, suggesting their usefulness in forecasting the growth rate of confirmed COVID-19 cases globally. However, ODANN demonstrated superior performance, indicating its capability to better encapsulate the growth trajectories of the analyzed G parameter over time.\n\nOverall, the comparison to publicly available methods and simpler baselines confirmed the robustness and effectiveness of the ODANN model in forecasting the G parameter, highlighting its potential for providing more accurate predictions in real-time contexts.",
  "evaluation/confidence": "In our study, we employed several statistical tests to evaluate the performance of our model, ODANN, and compare it with other time-series models. These tests included the Paired T-test, Kruskal\u2013Wallis test, and Wilcoxon Signed Rank test, which primarily focused on p-value computations to estimate the probability of obtaining results as extreme as those observed, assuming the null hypothesis is correct. A significance value of 0.05 was used for these tests.\n\nThe Paired T-test indicated that the average values of predictions from models like RF, SVM, ARIMA, and AutoARIMA were most similar to those of ODANN, as their p-values were greater than 0.05. Conversely, models like LSTM and Prophet showed significant differences in their average values, with p-values less than 0.05.\n\nThe Kruskal\u2013Wallis test revealed that the median values of predictions from ARIMA, AutoARIMA, RF, SVM, and LSTM were similar to those of ODANN, with p-values greater than 0.05. However, the Prophet model differed significantly in its median values.\n\nThe Wilcoxon Signed Rank test showed that the predictions from RF, SVM, ARIMA, and AutoARIMA had the most similar distribution to that of ODANN, with p-values greater than 0.05. The LSTM and Prophet models, however, showed significant differences in their data distribution.\n\nAdditionally, we used correlation coefficients such as Pearson, Spearman, and Kendall\u2019s tau to quantify the level of association between the predictions of ODANN and other models. The Pearson correlation coefficient indicated that the LSTM model had the closest linear relationship with ODANN, while the Spearman correlation coefficient showed that LSTM also had the closest positive monotonic relationship. Kendall\u2019s tau values suggested that no significant strong agreement existed between the predictions made by ODANN and the other time-series models, with LSTM having the closest agreement.\n\nOverall, the statistical analyses indicated that there is no universal statistical test capable of pre-determining which time-series model can generate test predictions to best match those of ODANN. Each test serves a different objective, depending on the context of the problem. Using RMSE and MAE scores, the test predictions from SVM best matched the predictive accuracy of ODANN, and the computed scores for the Paired T-test, Kruskal\u2013Wallis test, and Wilcoxon Signed Rank test indicated that SVM\u2019s predictions were consistently similar to those of ODANN in terms of average, median, and data distribution parameters. However, ODANN still outperformed SVM in final prediction accuracy, minimizing the RMSE and MAE scores.",
  "evaluation/availability": "Not applicable"
}