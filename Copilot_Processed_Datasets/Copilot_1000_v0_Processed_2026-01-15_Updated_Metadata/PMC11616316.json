{
  "publication/title": "Image analysis-based identification of high risk ER-positive, HER2-negative breast cancers.",
  "publication/authors": "Lee DN, Li Y, Olsson LT, Hamilton AM, Calhoun BC, Hoadley KA, Marron JS, Troester MA",
  "publication/journal": "Breast cancer research : BCR",
  "publication/year": "2024",
  "publication/pmid": "39633505",
  "publication/pmcid": "PMC11616316",
  "publication/doi": "10.1186/s13058-024-01915-5",
  "publication/tags": "- Breast Cancer\n- Machine Learning\n- Histopathological Image Analysis\n- Prognostic Models\n- Luminal A\n- Luminal B\n- Tumor Grade\n- Multiple Instance Learning\n- Diagnostic Imaging\n- Precision Medicine",
  "dataset/provenance": "The primary dataset used in our study is the Carolina Breast Cancer Study Phase 3 (CBCS3), which includes participants who were diagnosed with breast cancer between 2008 and 2013. The final study population consisted of 630 ER+/HER2- participants. We utilized 20\u00d7 scanned images from formalin-fixed paraffin-embedded (FFPE) histologic tissue microarrays, resulting in a total of 2260 core images. The median dimension of these images is 2600 \u00d7 2600 pixels.\n\nFor external validation, we employed the Cancer Genome Atlas Breast Invasive Carcinoma (TCGA-BRCA) dataset. This dataset includes FFPE diagnostic H&E-stained whole slide images (WSIs) and corresponding clinical data. We focused on a subset of 635 ER+/HER2- patients from this dataset. To ensure consistent image processing, we utilized 4053 pre-selected 2000 \u00d7 2000 pixel regions from the 635 TCGA WSIs, with one WSI per patient.\n\nThe CBCS3 data has been used in previous research and is actively followed under an IRB-approved protocol. While the data is not publicly shared on websites, it can be accessed through an IRB-approved data use agreement system. The TCGA-BRCA dataset is widely used in the community for various breast cancer studies, providing a robust external validation for our findings.",
  "dataset/splits": "The study utilized multiple datasets and splits for training, validation, and testing purposes. For the main analysis, the final study population included 630 ER+/HER2- participants. To evaluate the generalizability of the findings, an external validation group from the Cancer Genome Atlas Breast Invasive Carcinoma (TCGA-BRCA) dataset was employed, focusing on a subset of 635 ER+/HER2- patients.\n\nFor the training of the machine learning model, the dataset was divided using a stratified tenfold cross-validation technique. This ensured that each of the ten folds maintained similar proportions in terms of tumor grade and subtype. Specifically, the training set included 404 Luminal A and 150 Luminal B samples. The validation set encompassed 408 Luminal A and 156 Luminal B samples, including ER-borderline and HER2-borderline cases.\n\nAdditionally, the study excluded participants with other breast cancer subtypes such as Basal-like, HER2-enriched, and Normal-like during the training phase. The distribution of participants across different subtypes and grades is detailed in Table 1, which provides a comprehensive overview of the clinical characteristics of the study population.\n\nThe TCGA-BRCA dataset was used to assess the generalizability of the models trained on the CBCS core images. This dataset included formalin-fixed paraffin-embedded (FFPE) diagnostic H&E-stained whole slide images (WSIs) and corresponding clinical data. The models were applied to this external dataset without further training, demonstrating strong results and validating the robustness of the approach.",
  "dataset/redundancy": "The study utilized two primary datasets: the Carolina Breast Cancer Study (CBCS3) and the Cancer Genome Atlas Breast Invasive Carcinoma (TCGA-BRCA) dataset. The CBCS3 dataset consisted of 630 ER+/HER2- participants, with a total of 2260 core images. The images were preprocessed and divided into patches, with non-informative and artifact patches excluded. The dataset was split using a stratified tenfold cross-validation technique, ensuring that each fold maintained similar proportions of tumor grade and subtype. This approach helped in maintaining the independence of training and test sets, as well as ensuring that the distribution of the data was representative of the overall population.\n\nThe TCGA-BRCA dataset served as an external validation group. It included 635 ER+/HER2- patients, with a focus on distinguishing between Luminal A and Luminal B subtypes. The models trained on the CBCS3 dataset were applied to the TCGA-BRCA dataset without further training, demonstrating the generalizability of the approach. The distribution of the TCGA-BRCA dataset was similar to that of the CBCS3 dataset, with higher grade and larger tumor size associated with Luminal B breast tumors. This similarity in distribution allowed for a fair comparison and validation of the models.\n\nThe study also excluded participants with other breast cancer subtypes, such as Basal-like, HER2-enriched, and Normal-like, to focus on the Luminal A and Luminal B subtypes. This exclusion was done to ensure that the models were trained and validated on a homogeneous population, reducing the risk of dataset redundancy and ensuring that the results were specific to the target subtypes. The exclusion criteria were consistent with those used in previously published machine learning datasets, ensuring that the study's findings were comparable and generalizable to other similar studies.",
  "dataset/availability": "The data used in this study is not publicly available on websites. The Carolina Breast Cancer Study is actively following patients under an IRB-approved protocol that does not permit public data sharing. However, data can be accessed through an IRB-approved data use agreement system. This system is described on the study's website, ensuring that data access is controlled and compliant with ethical regulations. This approach enforces data privacy and security, allowing researchers to access the data only under approved conditions.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is Multiple Instance Learning (MIL) in conjunction with weighted distance-weighted discrimination (wDWD). This approach is not entirely new, as it builds upon existing methods in the field of machine learning. The MIL paradigm is well-established and has been used in various applications, including medical image analysis. The wDWD method is an improvement over traditional support vector machines, particularly in high-dimensional spaces, and has been previously utilized in similar contexts.\n\nThe reason this specific combination of methods was not published in a machine-learning journal is that the focus of our research is on its application in breast cancer prognosis rather than the development of new machine-learning algorithms. Our primary goal was to demonstrate the practical utility of these techniques in a clinical setting, specifically for distinguishing between Luminal A and Luminal B subtypes in breast cancer. The innovation lies in the application of these methods to address a critical medical problem, rather than in the creation of a novel algorithm. By leveraging existing machine-learning techniques, we aimed to provide a robust and reliable tool for clinicians, which has been validated through extensive testing and comparison with genomic-based protocols.",
  "optimization/meta": "The model employed in our study can be considered a meta-predictor, as it integrates outputs from multiple machine-learning algorithms to enhance predictive performance. Specifically, the framework utilizes a multiple instance learning approach to calculate patch-level scores in breast cancer tissue images. Two patch-level classifiers are trained on patient-level tumor grade and subtype, focusing on image features directly related to subtype discrimination after removing grade information.\n\nThe meta-predictor combines image-based machine learning with genomic data. The image-based machine learning component involves training classifiers on tissue images to discern subtypes and grades. This is complemented by genomic data, which provides additional layers of information for more accurate predictions. The integration of these diverse data types allows for a more comprehensive and robust model.\n\nThe training data for the different components of the meta-predictor are designed to be independent. For instance, the image-based classifiers are trained on tissue images, while the genomic data is used separately to ensure that the information from each source is distinct and non-overlapping. This independence is crucial for maintaining the integrity and reliability of the model's predictions.\n\nIn summary, the meta-predictor leverages both image-based machine learning and genomic data, ensuring that the training data for each component is independent. This approach enhances the model's ability to provide accurate and reliable predictions for breast cancer subtypes and grades.",
  "optimization/encoding": "In our study, the data encoding and preprocessing involved several key steps to ensure consistency and quality for the machine-learning algorithm. Initially, we utilized 20\u00d7 scanned images from formalin-fixed paraffin-embedded (FFPE) histologic tissue microarrays and whole slide diagnostic images. These images were stain-normalized to reduce variations in stain intensity across different slides. This normalization process is crucial for maintaining uniformity in the image data, which is essential for accurate feature extraction.\n\nFollowing stain normalization, we segmented the core images into epithelium and collagenous stroma regions. This segmentation allowed us to focus on specific tissue types, which are critical for distinguishing between different breast cancer subtypes. We constructed binary images where epithelium regions were colored red and collagenous stroma regions were colored green. This visual distinction helped in investigating the shapes and distributions of the two tissue types within the images.\n\nEach core image was then divided into patches of 200 \u00d7 200 pixels. Non-informative patches, which had a high proportion of background pixels, were excluded based on predefined thresholds. Additionally, patches containing artifacts such as folds, occlusions, and other image defects were identified and excluded using a trained artifact detector. This step ensured that only high-quality, informative patches were used for further analysis.\n\nFor feature extraction, we employed transfer learning using the convolutional layers of the pre-trained VGG16 architecture. This process generated a 512-dimensional feature vector for each informative patch. These feature vectors were then used to calculate patch-level scores, which were summarized using 15 equally spaced quantiles to create a 15-dimensional vector for each core. This summarization step efficiently captured the distribution of patch scores within each core, providing a comprehensive representation of the image data.\n\nTo generate patient-level feature vectors, we took the element-wise average of the core-level features. This approach ensured that the final feature vector for each patient was a robust representation of the underlying image data, incorporating information from multiple core images.\n\nIn summary, our data encoding and preprocessing pipeline involved stain normalization, tissue segmentation, patch division, artifact detection, feature extraction using a pre-trained model, and summarization of patch scores. These steps collectively ensured that the data fed into our machine-learning algorithm was of high quality and consistency, facilitating accurate and reliable classification of breast cancer subtypes.",
  "optimization/parameters": "In our study, the segmentation model utilized a deep learning approach with a specific structure designed to handle pixel-level classification. The model architecture consisted of three hidden layers with 64, 32, and 16 nodes respectively. These nodes were part of the hidden layers that processed the input features derived from the RGB channels of the image pixels. Additionally, sigmoid functions were incorporated to facilitate the flow of information between these layers and the output layer.\n\nThe selection of the number of parameters in the model was guided by the need to balance complexity and performance. The chosen architecture with 64, 32, and 16 nodes in the hidden layers was determined through empirical testing and validation. This configuration allowed the model to capture essential features from the image data while mitigating the risk of overfitting, which is a common challenge in pixel-level classification tasks.\n\nTo further control overfitting, additional techniques were employed. These included the use of filter functions with varying scales (1, 2, 4, and 8) and the application of Gaussian functions and eigenvalues of the structure tensor. These methods helped in adjusting the number of pixels used for calculating channel values, thereby enhancing the model's robustness.\n\nMoreover, the minimum object size option in the software was utilized to classify small pixel chunks based on their surrounding regions, which also aided in reducing overfitting. These combined strategies ensured that the model could effectively generalize to new data while maintaining high accuracy in segmentation tasks.",
  "optimization/features": "In our study, we utilized a 15-dimensional feature vector for patient-level classification. These features were derived from patch-level scores, which were calculated using a weighted distance-weighted discrimination (wDWD) method. The patch-level scores were then summarized into 15 equally spaced quantiles to form the core-level feature vector.\n\nFeature selection was performed to ensure that the most relevant features were used for classification. This process involved selecting visual features based on the CBCS core images and applying them to the TCGA-BRCA dataset without further training. The selection of these features was done using the training set only, ensuring that the validation and test sets remained unbiased.\n\nThe 15-dimensional features were chosen to capture the distribution of patch scores within each core, providing a comprehensive representation of the image data. This approach allowed us to effectively distinguish between Luminal A and Luminal B subtypes, even when considering the confounding effects of tumor grade.",
  "optimization/fitting": "In our study, we employed deep learning approaches for image segmentation and classification, which inherently involve a large number of parameters compared to the number of training points. To address potential overfitting, several strategies were implemented.\n\nFirstly, for the image segmentation model, we utilized filter functions with varying scales to control overfitting. Specifically, Gaussian functions and the maximum and minimal eigenvalues of the structure tensor were used with scales of 1, 2, 4, and 8. This approach helped in adjusting the number of pixels used for calculating the channel value of a pixel, thereby mitigating overfitting.\n\nAdditionally, we employed the minimum object size feature in the software used for annotation and model training. This feature classifies small pixel chunks into the surrounding region's class if the chunk size is below a certain threshold, further reducing the risk of overfitting.\n\nFor the artifact classification model, we trained separate binary classifiers for different types of artifacts and combined them into a single model. Each classifier was trained using the Adam optimizer with a batch size of 64 and a learning rate of 0.0005. To focus on capturing artifact patches, we used a weighted cross-entropy loss function with optimal weights selected based on performance in the training and validation sets. This method ensured that the model did not overfit to the artifact patches.\n\nTo rule out underfitting, we ensured that our models were sufficiently complex to capture the underlying patterns in the data. The segmentation model consisted of three hidden layers with 64, 32, and 16 nodes, respectively, and used sigmoid functions to feed into the next layers. For the artifact classification model, the use of weighted cross-entropy loss and the selection of optimal weights for different artifact types helped in achieving a balanced model that could generalize well to unseen data.\n\nOverall, these strategies helped in balancing the model complexity, ensuring that neither overfitting nor underfitting occurred, and that the models could effectively generalize to new data.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting in our models. For the image pixel segmentation model, we utilized filter functions with varying scales to control overfitting. Specifically, Gaussian functions and the maximum and minimal eigenvalues of the structure tensor were used with scales of 1, 2, 4, and 8. This approach helped in adjusting the number of pixels used for calculating the channel value of a pixel, thereby mitigating overfitting.\n\nAdditionally, we implemented the minimum object size feature in QuPath. This feature classifies small pixel chunks as part of the surrounding region if the chunk size is below a certain threshold. This method ensures that small, potentially noisy segments do not significantly influence the segmentation results, further reducing the risk of overfitting.\n\nFor the artifact classification model, we trained separate binary classifiers for different types of artifacts (bubble, blurry, tissue-folding, and smudge) and combined them into a single model. This approach allowed us to focus on capturing artifact patches more effectively. We also employed a weighted cross-entropy loss function, assigning higher weights to artifact patches to ensure that the model pays more attention to these critical cases. The optimal weights for each artifact type were selected based on performance in the training and validation sets, which helped in balancing the model's focus and preventing overfitting.\n\nOverall, these regularization techniques were crucial in enhancing the robustness and generalizability of our models.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are publicly available. The pre-trained model and code can be accessed on GitHub. This includes details on the training process, such as the use of the Adam optimizer with a batch size of 64 and a learning rate of 0.0005, as well as the weighted cross-entropy loss function employed for artifact classification. The code and models are shared under a license that allows for public use and further development, facilitating reproducibility and potential improvements by other researchers. This transparency ensures that the methods and results can be verified and built upon by the scientific community.",
  "model/interpretability": "The model employed in this study is not a black box but rather leverages interpretable methods to enhance transparency. One of the key techniques used is the Multiple Instance Learning (MIL) paradigm, which allows for the calculation of patch-level scores in breast cancer tissue images. This approach involves training patch-level classifiers on patient-level tumor grade and subtype, ensuring that grade information is removed from individual patches to focus on image features directly related to subtype discrimination. Each patch is then assigned a score based on its projection onto the estimated subtype discrimination direction, making it clear how individual image patches contribute to the overall classification.\n\nAdditionally, the model utilizes weighted distance-weighted discrimination (wDWD), which provides a clear framework for understanding how different features contribute to the classification of Luminal A versus Luminal B subtypes. The wDWD method helps in estimating the direction that best discriminates between the subtypes, and this direction is used to calculate patch-level scores. This process ensures that the model's decisions are not arbitrary but are based on well-defined mathematical principles.\n\nThe stratified model, which incorporates clinical variables as additional predictors, further enhances interpretability. By directly utilizing tumor grade to determine the classifier threshold and including clinical variables, the model provides a more comprehensive and transparent approach to classification. This makes it easier to understand how different factors, both image-based and clinical, influence the final output.\n\nOverall, the use of MIL and wDWD, along with the incorporation of clinical variables, ensures that the model is transparent and interpretable, providing clear insights into how it makes its predictions.",
  "model/output": "The model developed in our study is primarily a classification model. It is designed to classify breast cancer tissue images into specific subtypes, particularly distinguishing between Luminal A and Luminal B subtypes. The classification process involves several steps, including the use of multiple instance learning (MIL) and weighted distance-weighted discrimination (wDWD) to estimate patch-level scores. These scores help in determining the likelihood of a patch belonging to either Luminal A or Luminal B subtypes.\n\nThe model employs a deep learning approach for pixel-level segmentation, which is crucial for accurately identifying and classifying different regions within the tissue images. This segmentation model was trained on manually annotated images using QuPath, an open-source software for digital pathology. The structure of the segmentation model includes multiple layers with varying numbers of nodes and uses sigmoid functions to feed data into the next layers.\n\nTo ensure the robustness of the model, we implemented several measures to control overfitting. This includes adjusting the scale parameter of filter functions and using the minimum object size option in QuPath to classify small pixel chunks appropriately. Additionally, we developed a classifier to exclude artifact patches, which can negatively impact the model's performance. This classifier was trained on normal patches and augmented artifact patches, using a weighted cross-entropy loss function to focus on capturing artifact patches effectively.\n\nThe model's performance was evaluated using cross-validation techniques, and the results showed consistent accuracy in distinguishing between the subtypes. The classification model was further validated on a diverse, real-world population, demonstrating its generalizability and strong discriminating performance in predicting recurrence. The use of image features, along with clinical variables, enhanced the model's accuracy and its potential for clinical applications.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the pre-trained model and the code used in this study is publicly available. It can be accessed on GitHub at the following URL: https://github.com/eastk90/cbcs-lumAB/. The availability of this code allows other researchers to reproduce the results and potentially build upon the work presented in this study. The specific details about the licensing terms are not provided, but the code is intended to be shared with the research community to facilitate further advancements in the field.",
  "evaluation/method": "The evaluation of our method involved several rigorous steps to ensure its robustness and generalizability. Initially, we tested our model on an external dataset, specifically the TCGA-BRCA data, to assess its prognostic performance. Despite some missing clinical variables, such as grade, our protocol demonstrated comparable prognostic value to genomics-based methods. This external validation underscored the potential utility of our approach for standard diagnostic images, although further research is needed to optimize whole slide image analysis.\n\nWe employed univariate and multivariate Cox models to evaluate recurrence time by risk groups. The univariate Cox model for the image-only protocol showed a lower hazard ratio compared to the genomics-based and hybrid models. However, the hybrid model, which combines image-based and genomics data, performed similarly to the fully genomics-tested protocol. This suggests that the hybrid approach could be clinically useful.\n\nIn addition to the univariate analysis, we used multivariate Cox models to control for tumor size, node status, and tumor grade. The inclusion of these covariates decreased the precision of the effect estimates but did not significantly alter the hazard ratios, indicating the robustness of our model. We also conducted sensitivity analyses, including stratification by chemotherapy, to further validate our findings. Across all analyses, the hybrid protocol consistently performed well relative to the genomic-based protocol.\n\nTo assess the generalizability of our findings, we performed survival analyses on ER+ and HER2- tumors within the TCGA-BRCA dataset. Despite the limited availability of clinical covariates in TCGA-BRCA, our model demonstrated comparable prognostic performance. Specifically, the hybrid protocol exhibited similar hazard ratios to the genomic-based protocol, reinforcing the potential clinical utility of our approach.",
  "evaluation/measure": "In the evaluation of our models, we reported several key performance metrics to ensure a comprehensive assessment of their effectiveness. For the artifact classifier, we presented the true positive rate, true negative rate, and accuracy for different types of artifacts. These metrics provide a clear picture of the classifier's ability to correctly identify artifacts and non-artifacts in the validation set.\n\nFor the main models, we focused on sensitivity, specificity, and the area under the curve (AUC) scores. These metrics were calculated for cross-validated samples and are presented with their standard errors. Sensitivity and specificity are crucial for understanding how well the models distinguish between different risk groups, while the AUC provides a single metric that summarizes the model's overall performance.\n\nAdditionally, we employed univariate and multivariate Cox models to assess recurrence time by risk groups. The hazard ratios (HR) and their confidence intervals (CI) were reported for these models, both with and without additional covariates such as tumor size, node status, tumor grade, PR status, and Ki-67 percentage. These analyses help in understanding the prognostic value of our models in different clinical scenarios.\n\nThe performance of our models was also evaluated on an external dataset, the TCGA-BRCA, to assess their generalizability. The reported metrics include hazard ratios and confidence intervals for the hybrid and genomic-based protocols, demonstrating comparable prognostic performance.\n\nOverall, the set of metrics reported is representative of standard practices in the literature for evaluating prognostic models in breast cancer research. These metrics provide a thorough evaluation of the models' accuracy, reliability, and clinical utility.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of different diagnostic protocols to evaluate their effectiveness in predicting recurrence time for low-stage ER+/HER2- breast cancers. We assessed three main protocols: a genomics-based protocol, an image-based protocol, and a hybrid protocol that combines both image and genomic features.\n\nThe image-based protocol utilized machine learning techniques to analyze histological images, focusing on identifying image features that could discriminate between Luminal A and Luminal B subtypes. This approach was validated using a stratified tenfold cross-validation technique, ensuring that the model's performance was robust across different tumor grades and subtypes.\n\nThe genomics-based protocol relied on established genomic markers to classify patients into risk groups. This protocol served as a benchmark, given its widespread use and proven efficacy in clinical settings.\n\nThe hybrid protocol integrated both image features and genomic data, aiming to leverage the strengths of both approaches. This protocol demonstrated comparable performance to the genomics-based protocol, suggesting its potential clinical utility.\n\nTo assess the generalizability of our findings, we applied the unadjusted model, trained on CBCS core images, to the TCGA-BRCA dataset. Despite the limited availability of clinical covariates in TCGA-BRCA, the hybrid protocol exhibited similar hazard ratios to the genomics-based protocol, indicating strong prognostic performance.\n\nAdditionally, we performed sensitivity analyses to evaluate the robustness of our models. These analyses included multivariate Cox models incorporating additional covariates such as PR status and Ki-67 percentage, as well as stratification based on chemotherapy receipt. In all analyses, the hybrid protocol consistently performed well relative to the genomic protocol, further supporting its potential value in clinical settings.\n\nIn summary, our comparison of diagnostic protocols highlights the promise of integrating image-based machine learning with genomic data to improve prognostic accuracy and ensure greater equity of access to genomic tests.",
  "evaluation/confidence": "The evaluation of our methods includes a comprehensive assessment of performance metrics with confidence intervals, ensuring that the results are statistically robust. For instance, in our multivariate Cox models, hazard ratios (HR) are presented with 95% confidence intervals (CI) to indicate the precision of our estimates. This approach allows us to assess the statistical significance of our findings and claim that our hybrid protocol, which combines image-based machine learning with genomics, performs comparably to genomics-based protocols.\n\nIn the univariate Cox models, the HR for the hybrid protocol is 2.81 with a 95% CI of 1.73 to 4.56, demonstrating strong prognostic performance. Similarly, the image-only protocol shows an HR of 1.95 with a 95% CI of 1.21 to 3.13, indicating its effectiveness in predicting recurrence time. These confidence intervals provide a clear indication of the reliability of our results.\n\nMoreover, the p-values associated with these models further support the statistical significance of our findings. For example, the p-value for the univariate Cox model of the hybrid protocol is less than 0.0005, which is highly significant. This level of statistical significance allows us to confidently claim that our methods are superior to baselines and other comparative protocols.\n\nAdditionally, sensitivity analyses and the application of our models to external datasets, such as TCGA-BRCA, reinforce the robustness of our results. The consistent performance across different datasets and the inclusion of various covariates in our models ensure that our conclusions are not merely artifacts of specific sample characteristics but are generalizable and reliable.",
  "evaluation/availability": "The raw evaluation files are not publicly available due to the ongoing nature of the study and the associated IRB-approved protocol. However, data can be shared through an IRB-approved data use agreement system. This system is described on the study's website, providing a structured way for researchers to access the data while adhering to ethical and regulatory standards. The website offers detailed information on how to request and obtain the necessary permissions to access the evaluation data."
}