{
  "publication/title": "Machine learning based approaches for detecting COVID-19 using clinical text data.",
  "publication/authors": "Khanday AMUD, Rabani ST, Khan QR, Rouf N, Mohi Ud Din M",
  "publication/journal": "International journal of information technology : an official journal of Bharati Vidyapeeth's Institute of Computer Applications and Management",
  "publication/year": "2020",
  "publication/pmid": "32838125",
  "publication/pmcid": "PMC7325639",
  "publication/doi": "10.1007/s41870-020-00495-9",
  "publication/tags": "- Artificial intelligence\n- COVID-19\n- Machine learning\n- Ensemble learning\n- Text classification\n- Clinical text data\n- Disease detection\n- Natural language processing\n- Feature engineering\n- Healthcare analytics",
  "dataset/provenance": "The dataset used in this work consists of clinical reports from approximately 212 patients who exhibited symptoms of coronavirus and other viruses. The data includes around 24 attributes, such as patient ID, sex, age, findings, survival status, and various medical measurements like temperature, pO2 saturation, and leukocyte count. Additionally, the dataset includes clinical notes and other relevant notes associated with each patient.\n\nThe clinical notes and findings were specifically extracted for text mining purposes. These notes are in English and have been labeled into four classes: COVID, ARDS, SARS, and Both (COVID, ARDS). The length distribution of these clinical reports was analyzed to ensure that only relevant and well-documented cases were included in the study.\n\nThis dataset was utilized to train and test machine learning models for classifying clinical text into the aforementioned categories. The models were evaluated using a 70:30 train-test split, and a ten-fold cross-validation strategy was employed to ensure the robustness of the results. The dataset's structure and the extracted features played a crucial role in achieving high accuracy in the classification tasks.",
  "dataset/splits": "The dataset was split into two main subsets: a training set and a test set. The data was divided in a 70:30 ratio, meaning 70% of the data was used for training the model, and 30% was reserved for testing. This split was done to evaluate the model's performance on unseen data and to reduce the risk of overfitting.\n\nAdditionally, a tenfold cross-validation strategy was employed. This involved splitting the dataset into ten equal parts, or folds. The model was trained ten times, each time using nine folds for training and the remaining one fold for validation. This process was repeated five times independently to mitigate any sampling bias that might arise from the random partitioning of the dataset in the cross-validation.\n\nIn summary, the dataset underwent a 70:30 train-test split and a tenfold cross-validation process repeated five times. This approach ensured a thorough evaluation of the model's performance and generalization capabilities.",
  "dataset/redundancy": "The dataset used in this study consisted of clinical text reports from 212 patients, labeled into four classes: COVID, ARDS, SARS, and both COVID and ARDS. To ensure the robustness and generalization of our models, the dataset was split into training and test sets using a 70:30 ratio. This means 70% of the data was used for training the models, while the remaining 30% was reserved for testing.\n\nTo further validate the performance and reduce the risk of overfitting, a ten-fold cross-validation strategy was employed. This process was repeated five times independently. Cross-validation helps in assessing how the model will generalize to an independent dataset by splitting the data into multiple folds and training the model on different combinations of these folds. Repeating this process multiple times ensures that the results are not biased by a particular random split of the data.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets in the medical domain. The use of clinical notes and findings, along with the careful splitting and validation strategies, ensures that the models are trained and tested on independent sets, thereby providing a reliable estimate of their performance on unseen data. This approach is crucial for developing models that can be effectively deployed in real-world scenarios, where the data distribution may vary.",
  "dataset/availability": "The dataset used in this study is publicly available. It consists of clinical reports from approximately 212 patients who exhibited symptoms of coronavirus and other viruses. The data includes various attributes such as patient ID, age, sex, findings, survival status, and clinical notes, among others. The clinical notes and findings were specifically extracted for text mining purposes.\n\nThe dataset is labeled into four classes: COVID, ARDS, SARS, and Both (COVID, ARDS). Only reports written in English were considered, and their lengths were calculated and analyzed. The data is available on GitHub, ensuring accessibility for further research and validation. The license details are provided within the dataset repository, allowing users to understand the terms of use.\n\nTo ensure the integrity and reproducibility of the results, the dataset was split into training and testing subsets. A 70:30 ratio was used, with 70% of the data allocated for training the models and 30% reserved for testing. Additionally, a ten-fold cross-validation strategy was employed to evaluate the performance of the machine learning algorithms, and this process was repeated five times independently to mitigate sampling bias. This rigorous approach ensures that the dataset is used consistently and transparently across different experiments and by other researchers.",
  "optimization/algorithm": "The machine-learning algorithms used in this work are classical and ensemble learning techniques. Specifically, logistic regression, multinomial Na\u00efve Bayes, support vector machine, decision tree, bagging, Adaboost, random forest, and stochastic gradient boosting were employed. These algorithms are well-established in the field of machine learning and have been extensively used for various classification tasks.\n\nThe algorithms used are not new; they are widely recognized and have been published in numerous machine learning journals and textbooks. The choice of these algorithms was driven by their proven effectiveness in handling text classification problems, which is the primary focus of this study. The goal was to classify clinical text reports into four different classes: COVID, SARS, ARDS, and both (COVID, ARDS).\n\nThe decision to use these established algorithms was also influenced by the need for robustness and reliability in the classification process. Ensemble methods like random forest and stochastic gradient boosting were particularly useful in improving the performance and generalization of the models. These techniques help in reducing overfitting and enhancing the accuracy of the predictions by combining the strengths of multiple base learners.\n\nThe work was conducted using a Windows system with 4 GB of RAM and 2.3 GHz processors. The Scikit-learn tool was utilized for performing the machine learning classification, with additional libraries like NLTK and STOP-WORDS to improve the accuracy of the algorithms. The data was split into a 70:30 ratio for training and testing, respectively, and a tenfold cross-validation strategy was employed to ensure the models' robustness and to avoid sampling bias.\n\nIn summary, the machine-learning algorithms used in this study are classical and ensemble learning techniques that have been thoroughly validated in the literature. The choice of these algorithms was based on their effectiveness in text classification tasks and their ability to provide reliable and accurate results.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding process involved several steps to prepare the clinical text reports for machine learning classification. Initially, the text data was unstructured and required refinement. This involved cleaning the text by removing unnecessary elements such as punctuation, stopwords, symbols, URLs, and links. Lemmatization was also performed to ensure that the data was in a consistent format.\n\nFor feature extraction, techniques like TF-IDF (Term Frequency-Inverse Document Frequency) and bag of words were employed. Unigrams and bigrams were considered to capture relevant features from the text. A total of 40 features were identified as significant for classification. These features were then weighted appropriately and supplied as input to the machine learning algorithms.\n\nThe preprocessing steps included cleaning the text, removing unnecessary elements, and performing lemmatization. This ensured that the data was in a suitable format for feature extraction. The extracted features were then used to train various machine learning algorithms, including logistic regression, multinomial Na\u00efve Bayes, support vector machines, decision trees, random forests, bagging, Adaboost, and stochastic gradient boosting. These algorithms were used to classify the clinical text reports into four categories: COVID, ARDS, SARS, and both COVID and ARDS.",
  "optimization/parameters": "In our study, we utilized 40 features for the classification task. These features were selected through a process of feature engineering, where techniques such as TF-IDF and bag of words were employed to extract relevant unigrams and bigrams from the clinical text reports. The selection of these 40 features was based on their relevance and contribution to the classification of the text into four distinct categories: COVID, ARDS, SARS, and both COVID and ARDS. These features were then supplied as input to various machine learning algorithms, including support vector machines, decision trees, logistic regression, and others, to achieve accurate classification. The choice of 40 features was determined by the need to balance model complexity and performance, ensuring that the model could generalize well to unseen data while maintaining high accuracy.",
  "optimization/features": "In our study, we utilized a total of 40 features as input for our machine learning models. These features were carefully selected through a process of feature engineering, which involved extracting relevant information from the clinical text reports. The features included unigrams and bigrams, which were identified using techniques such as TF-IDF and bag of words. To ensure the robustness of our feature selection process, we performed it using only the training set. This approach helped to prevent data leakage and ensured that our models could generalize well to unseen data. The selected features were then represented in the form of a table and supplied as input to various machine learning algorithms for classification.",
  "optimization/fitting": "The number of parameters in our models was not excessively large compared to the number of training points. We used a dataset of 212 clinical reports, which were split into a 70:30 ratio for training and testing, respectively. This provided a sufficient number of training points to avoid under-fitting.\n\nTo prevent over-fitting, several techniques were employed. Firstly, we utilized ensemble learning methods such as bagging, AdaBoost, random forest, and stochastic gradient boosting. These methods help to reduce over-fitting by combining multiple models and averaging their predictions. Additionally, we implemented ten-fold cross-validation repeated five times independently. This strategy helped to ensure that our models generalized well to unseen data and reduced the risk of over-fitting.\n\nFurthermore, we used feature engineering to select the most relevant features, reducing the dimensionality of the data and focusing on the most informative attributes. This step helped to mitigate the risk of over-fitting by simplifying the model and making it less prone to capturing noise in the data.\n\nIn summary, by using ensemble learning techniques, cross-validation, and feature engineering, we effectively managed to avoid both over-fitting and under-fitting, ensuring that our models performed well on both training and test datasets.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the generalization of our models. One of the key methods used was subsampling, particularly in the context of gradient boosting. At each iteration, a random subsample of the training data was drawn without replacement from the full training dataset. This subsample was then used to fit the base learner, which helped in reducing the correlation between the trees and mitigated the risk of overfitting.\n\nAdditionally, we utilized a ten-fold cross-validation strategy for all algorithms. This process was repeated five times independently to avoid sampling bias introduced by randomly partitioning the dataset. Cross-validation is a robust technique that helps in assessing the model's performance on different subsets of the data, thereby providing a more reliable estimate of its generalization capability.\n\nFurthermore, we implemented ensemble learning techniques such as bagging and boosting. Bagging, or bootstrap aggregating, involves creating multiple subsets of the training data by sampling with replacement and then training a model on each subset. The predictions from these models are then combined, typically by voting, to produce the final output. This approach helps in reducing the variance and improving the stability of the model.\n\nBoosting, on the other hand, focuses on improving the performance of weak learners by sequentially training them on the data, with each new model attempting to correct the errors of the previous ones. Techniques like AdaBoost and stochastic gradient boosting were employed to enhance the model's accuracy and robustness.\n\nThese regularization methods collectively contributed to the prevention of overfitting and ensured that our models could generalize well to unseen data.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models used in this study are a mix of transparent and black-box models. Traditional machine learning algorithms like logistic regression, multinomial Na\u00efve Bayes, and decision trees are more interpretable. For instance, logistic regression provides probabilities for class membership, making it straightforward to understand the likelihood of a text belonging to a specific class. Decision trees, with their hierarchical structure, offer clear visual representations of how decisions are made based on features, making them easy to interpret.\n\nOn the other hand, ensemble methods like random forest, stochastic gradient boosting, and Adaboost are considered black-box models. These algorithms combine multiple base models to improve predictive performance, but the complexity of their interactions makes it challenging to interpret the decision-making process. For example, random forests aggregate the predictions of many decision trees, and while individual trees are interpretable, the collective behavior of the forest is not as transparent.\n\nSupport vector machines (SVM) also fall into a somewhat gray area. While the basic concept of finding a hyperplane is understandable, the use of kernel tricks and the optimization process involved in finding the optimal hyperplane can make SVM less interpretable, especially for non-linear kernels.\n\nIn summary, while some of the models used provide clear, interpretable results, others are more opaque, relying on complex interactions that are difficult to decipher. This trade-off between interpretability and performance is a common consideration in machine learning.",
  "model/output": "The model is designed for classification tasks. Specifically, it categorizes clinical text reports into four distinct classes: COVID, ARDS, SARS, and a combined class for patients with both COVID and ARDS. Various supervised machine learning algorithms, including logistic regression, multinomial Na\u00efve Bayes, support vector machines, decision trees, random forests, bagging, Adaboost, and stochastic gradient boosting, were employed to achieve this classification. The model's performance was evaluated using metrics such as precision, recall, F1 score, and accuracy, with logistic regression and multinomial Na\u00efve Bayes showing the highest performance, achieving an accuracy of 96.2%. The comparative analysis of these algorithms is presented in tables and figures, highlighting their effectiveness in classifying the clinical reports.",
  "model/duration": "The model was executed on a Windows system equipped with 4 GB of RAM and a 2.3 GHz processor. The execution time varied depending on the specific machine learning algorithm used. However, the overall process, including data preprocessing, feature extraction, and classification, was efficiently managed using tools like Scikit-learn, NLTK, and STOP-WORDS. The use of a pipeline helped improve the accuracy of the algorithms and streamlined the computational process. To ensure the robustness of our results, we employed a tenfold cross-validation strategy, which was repeated five times independently. This approach helped in reducing the sampling bias and provided a comprehensive evaluation of the model's performance.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved a comprehensive approach to ensure the robustness and generalization of the models. A windows system with 4 GB of RAM and 2.3 GHz processors was used for the computations. The dataset consisted of 212 clinical text reports, which were labeled into four classes: COVID, SARS, ARDS, and both (COVID, ARDS). The data was split into a 70:30 ratio, with 70% used for training the models and 30% reserved for testing.\n\nTo explore the generalization of the models from training data to unseen data and to reduce the possibility of overfitting, a tenfold cross-validation strategy was conducted for all algorithms. This process was repeated five times independently to avoid sampling bias introduced by randomly partitioning the dataset in the cross-validation.\n\nThe performance of various machine learning algorithms was compared. Logistic regression and Multinomial Na\u00efve Bayes showed the best results, achieving a precision of 94%, recall of 96%, F1 score of 95%, and accuracy of 96.2%. Other algorithms, such as random forest and stochastic gradient boosting, also performed well, with accuracies of 94.3%.\n\nThe evaluation was conducted in two stages to assess the impact of data size on model accuracy. In the first stage, 75% of the available data was used, resulting in lower accuracy compared to the stage where the entire dataset was utilized. This indicated that increasing the amount of data could improve model performance. The comparative analysis of all the algorithms was visualized to provide deeper insights into their performance.",
  "evaluation/measure": "In our evaluation, we focused on several key performance metrics to assess the effectiveness of our machine learning models. These metrics include precision, recall, F1 score, and accuracy. Precision measures the correctness of the positive predictions made by the model, while recall indicates the model's ability to identify all relevant instances. The F1 score provides a harmonic mean of precision and recall, offering a balanced view of the model's performance. Accuracy, on the other hand, represents the overall correctness of the model's predictions across all classes.\n\nThese metrics are widely used in the literature for evaluating classification models, particularly in the context of medical and text classification tasks. They provide a comprehensive view of the model's performance, ensuring that we capture both the correctness and the completeness of the predictions. By reporting these metrics, we aim to offer a transparent and representative evaluation of our models' capabilities.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of various machine learning algorithms to classify clinical text data into four categories: COVID, SARS, ARDS, and both COVID and ARDS. We utilized both classical and ensemble learning methods to assess their performance.\n\nWe compared the performance of logistic regression, multinomial Na\u00efve Bayes, support vector machines, decision trees, bagging, Adaboost, random forest, and stochastic gradient boosting. The evaluation metrics used included precision, recall, F1 score, and accuracy.\n\nLogistic regression and multinomial Na\u00efve Bayes demonstrated superior performance, achieving a precision of 94%, recall of 96%, F1 score of 95%, and accuracy of 96.2%. Other algorithms like random forest and stochastic gradient boosting also showed strong results, with an accuracy of 94.3%.\n\nThe comparison was performed on a dataset of 212 clinical reports, which were split into training and testing sets using a 70:30 ratio. Additionally, we employed tenfold cross-validation to ensure the robustness of our results and to mitigate the risk of overfitting.\n\nWe did not perform a direct comparison to publicly available methods on benchmark datasets, as our focus was on evaluating the performance of different algorithms on our specific dataset. However, the algorithms we used are widely recognized and have been applied in various text classification tasks.\n\nWe did not compare our methods to simpler baselines, as our primary goal was to assess the effectiveness of established machine learning techniques in classifying clinical text data related to COVID-19. The algorithms chosen are well-known and have been extensively used in similar classification tasks, providing a solid foundation for our comparative analysis.",
  "evaluation/confidence": "To ensure the robustness of our findings, we employed a rigorous evaluation strategy. We utilized a tenfold cross-validation strategy for all algorithms, which was repeated five times independently. This approach helped to mitigate sampling bias and provided a more reliable estimate of our models' performance. The performance metrics, including precision, recall, F1 score, and accuracy, were computed for each fold, and the results were averaged to provide a comprehensive evaluation.\n\nConfidence intervals were not explicitly reported in our study, but the use of cross-validation and repeated experiments helps to build confidence in the stability and generalizability of our results. The statistical significance of our findings was not explicitly tested against baselines or other methods, but the consistent performance across multiple experiments and the high metrics achieved (e.g., 96.2% accuracy for logistic regression and Multinomial Na\u00efve Bayes) suggest that our methods are effective.\n\nThe comparative analysis of different machine learning algorithms, including logistic regression, Multinomial Na\u00efve Bayes, random forest, and stochastic gradient boosting, showed that these methods outperformed others like support vector machines and decision trees. The high precision, recall, and F1 scores indicate that our models are not only accurate but also reliable in classifying clinical reports into the specified categories.\n\nIn summary, while confidence intervals and statistical significance tests were not explicitly reported, the use of cross-validation and the consistent high performance of our models across multiple experiments provide a strong indication of their effectiveness and reliability.",
  "evaluation/availability": "The raw evaluation files used in our work are not publicly available. The data consists of clinical reports from approximately 212 patients, which include various attributes such as patient ID, age, sex, findings, and other medical details. These reports were collected from an open-source data repository on GitHub, but the specific dataset used for our evaluation is not publicly released due to privacy and ethical considerations. The data was labeled into four classes: COVID, SARS, ARDS, and both (COVID, ARDS). While the methodology and results of our evaluation are detailed in the publication, the actual raw data files are not accessible to the public."
}