{
  "publication/title": "Personalized prediction model for seizure-free epilepsy with levetiracetam therapy: a retrospective data analysis using support vector machine.",
  "publication/authors": "Zhang JH, Han X, Zhao HW, Zhao D, Wang N, Zhao T, He GN, Zhu XR, Zhang Y, Han JY, Huang DL",
  "publication/journal": "British journal of clinical pharmacology",
  "publication/year": "2018",
  "publication/pmid": "30043454",
  "publication/pmcid": "PMC6177722",
  "publication/doi": "10.1111/bcp.13720",
  "publication/tags": "- Epilepsy\n- Levetiracetam\n- Support Vector Machine\n- EEG\n- Clinical Features\n- Seizure-Free\n- Antiepileptic Drugs\n- Prediction Model\n- Machine Learning\n- Prognostic Modelling",
  "dataset/provenance": "The dataset used in this study was collected from patients with epilepsy who were treated with levetiracetam. The study involved a total of 46 patients, divided into a training set of 36 patients and a test set of 10 patients. The data included both clinical characteristics and electroencephalography (EEG) features. The clinical features considered important in the model were selected based on actual antiepileptic drug (AED) selection practices in clinical settings. For the EEG data, only the bands that showed significant differences between the seizure-free (SF) and not seizure-free (NSF) groups were extracted to represent the EEG features.\n\nThe study acknowledges that the limited sample size could bias the representativeness of the data. However, the sample size in medical research is often constrained, and the success of the model depends not only on the sample size but also on the kernel function and dimensionality. The use of a support vector machine (SVM) with a radial basis function (RBF) kernel was chosen for its effectiveness in handling small sample sizes and its ability to find a hyper-plane in high-dimensional space.\n\nThe dataset was divided into training and test sets using an 80:20 ratio, which is a common practice to ensure suitable parameters for the SVM model. The training set was further divided into five subsets for a 5-fold cross-validation procedure to optimize the model parameters and minimize absolute mean errors. This approach helped in assessing the model's performance and generalization capabilities. Additionally, jack-knife validation and hold-out validation methods were used to further validate the model's feasibility in classifying and predicting the two groups of patients.",
  "dataset/splits": "The dataset was divided into multiple splits for training and validation purposes. Initially, the data was split into training and test sets with an 80:20 ratio. Specifically, approximately 80% of the patients from each group (seizure-free and not seizure-free) were used as the training set, totaling 36 patients. The remaining 20%, which is about 10 patients from each group, were used as the test set.\n\nAdditionally, to further validate the model, other validation methods were employed. These included jack-knife validation, where each patient was singled out in turn as a test set while the remaining patients were used as the training set. Hold-out validation was also used, where the validation set was equally divided into two subsets, with each subset used as a test set and the remaining one used as the training set to create the model.\n\nFor the training process, a 5-fold cross-validation procedure was utilized. In this method, the dataset was randomly divided into five subsets. Each subset was used as a validation set once, while the remaining four subsets were used as the training set. This process was repeated five times, ensuring that every portion of the data was used to assess the model's performance. The performance of the models was then averaged across these folds.",
  "dataset/redundancy": "The dataset used in this study consisted of 46 patients with epilepsy (PWEs), divided into two groups: 22 in the seizure-free (SF) group and 24 in the not seizure-free (NSF) group. To build and evaluate the support vector machine (SVM) model, the dataset was split into training and test sets.\n\nThe training set comprised approximately 80% of the patients from each group, totaling 36 patients. The remaining 20%, or 10 patients, formed the test set. This 80:20 split ratio was chosen to ensure that the model had sufficient data for training while also having an independent set for evaluation.\n\nTo enforce the independence of the training and test sets, the patients were randomly selected for each set. This randomization helped to mitigate any potential bias that could arise from a non-random split. Additionally, the model's performance was evaluated using multiple validation techniques, including 5-fold cross-validation, jack-knife validation, and hold-out validation. These methods further ensured that the model's generalization ability was thoroughly assessed.\n\nThe distribution of the dataset in this study is comparable to other machine learning datasets in the medical field, where sample sizes can often be limited due to the challenges of data collection and patient availability. The use of cross-validation techniques is a standard practice in machine learning to handle small datasets and to ensure that the model's performance is robust and not overly dependent on a specific split of the data.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used is Support Vector Machines (SVM). This is a well-established method in the field of machine learning and is not a new algorithm. The SVM algorithm was chosen for its effectiveness in handling small sample sizes and its ability to find a hyper-plane in high-dimensional space, which is particularly useful for the given dataset involving clinical and EEG features.\n\nThe SVM algorithm is widely recognized for its robustness in classification tasks, especially when the data distribution is good. The specific implementation used is the Radial Basis Function (RBF) kernel, which is commonly employed to map data into a higher-dimensional space where a linear separator can be found.\n\nThe decision to use SVM in this study was driven by its suitability for the problem at hand, rather than the novelty of the algorithm itself. The focus was on applying a proven method to address the specific challenges of the dataset, such as limited sample size and the need for accurate classification of patient outcomes.\n\nThe optimization of the SVM model involved tuning parameters such as the kernel and regularization parameters (Cost and \u03b3) using a grid search method within a 5-fold cross-validation procedure. This approach ensured that the model parameters were optimized to minimize errors and improve the generalization of the model.\n\nThe use of SVM in this context is not unprecedented, and its application in medical research, particularly in disease classification and prediction, is well-documented. The choice of SVM was based on its proven track record and its ability to handle the complexities of the dataset effectively.",
  "optimization/meta": "The model employed in this study is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on a Support Vector Machine (SVM) with a Radial Basis Function (RBF) kernel to predict the efficacy of levetiracetam (LEV) based on clinical and EEG features.\n\nThe SVM model was trained using a combination of EEG and clinical characteristics. The features used in the model include specific EEG bands from various regions, such as the \u03b2 band from the F2 region, \u03b1 band from the F4 region, \u03b8 band from the C3 region, and \u03b2 band from the F8 region. Additionally, clinical features like MRI findings, family history, seizure circadian rhythm, and others were considered important factors in the model.\n\nThe training process involved dividing the samples into training and test sets, with an 80%:20% ratio. The model's performance was evaluated using cross-validation methods, including 5-fold cross-validation, jack-knife validation, and hold-out validation. These methods ensured that the model's generalization ability was assessed rigorously.\n\nThe training data consisted of patients randomly selected from two groups: seizure-free (SF) and not seizure-free (NSF). The features extracted from these patients were used to build the SVM model. The model's parameters, including the kernel and regularization parameters, were optimized using a grid search method within the cross-validation procedure. This approach helped in minimizing absolute mean errors and ensuring that the model's performance was robust.\n\nIn summary, the model is a standalone SVM classifier that does not incorporate outputs from other machine-learning algorithms. The training data is independent, and the model's performance is evaluated using standard validation techniques to ensure its reliability and generalization ability.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the features were suitable for the Support Vector Machine (SVM) model. Only the EEG bands that showed significant differences between the two groups were extracted to represent the EEG features, ensuring that irrelevant or invalid features did not interfere with the model's performance. Additionally, every clinical feature in the study was considered important, reflecting the actual considerations in clinical antiepileptic drug (AED) selection.\n\nThe features were then used to train three separate models: one using selected EEG features, another using clinical features only, and a third combining both sets of features. The binary classification was performed in two main steps. First, approximately 80% of the patients from each group were randomly selected as the training set to build the model. The features extracted from these patients were then processed using a Lib-SVM classifier. This classifier required two key parameters: a kernel and a regularization parameter (Cost and \u03b3). The Cost parameter controlled the smoothness of the decision boundary in the transformed space, while the \u03b3 parameter, set in the kernel function, determined the distribution of data mapped to a new feature space.\n\nAn RBF kernel was used, defined as k(x, x') = exp(\u03b3|x-x'|^2), and the regularization parameters were identified using a grid search method within a 5-fold cross-validation procedure. This involved randomly dividing the dataset into five subsets, using each subset in turn as a validation set while the remaining four were used as the training set. This process was repeated five times, ensuring that every portion of the data was used to assess the model's performance. The model parameters were iterated to minimize absolute mean errors, which are the differences between predicted and measured output values on the validation set. The performance of the models was evaluated based on the average scores from each fold.\n\nIn addition to the k-fold validation, other methods such as jack-knife validation and hold-out validation were used to further evaluate the model's feasibility in classifying and predicting the two groups of patients. The jack-knife validation involved singling out each patient in turn as a test set, while the remaining patients were used as the training set. The hold-out validation equally divided the validation set into two subsets, using each subset in turn as a test set while the other was used as the training set. These methods provided a comprehensive assessment of the model's generalization and robustness.",
  "optimization/parameters": "In our study, the Support Vector Machine (SVM) model utilized two primary parameters: the regularization parameter (Cost, denoted as C) and the kernel parameter (\u03b3). The C parameter controls the smoothness of the decision boundary in the transformed space, while the \u03b3 parameter determines the distribution of data mapped to a new feature space.\n\nThe selection of these parameters was performed using a grid search method within a 5-fold cross-validation procedure. This approach involved randomly dividing the dataset into five subsets. In each iteration of the cross-validation, one subset was used as the validation set, while the remaining four subsets were used as the training set to create the model. This process was repeated five times, ensuring that each subset was used once as the validation set. The grid search method systematically explored different combinations of the C and \u03b3 parameters to minimize the absolute mean errors, which are the differences between the predicted and measured output values on the validation set. The performance of the models was evaluated based on the average scores of the models trained on each fold. This method ensured that the selected parameters were optimized for generalization and robustness.",
  "optimization/features": "In our study, we utilized a combination of EEG and clinical features as inputs for our model. The specific features included various EEG bands from different regions of the brain, such as F2, F4, and C3, as well as clinical variables like MRI findings, family history, seizure circadian rhythm, and more. The total number of features used is not explicitly stated, but it is clear that both EEG and clinical characteristics were considered important.\n\nFeature selection was indeed performed to ensure that only the most relevant features were included in the model. This process involved extracting only the EEG bands that showed significant differences between the two groups being studied. Additionally, every clinical feature was regarded as an important factor in the model. We tested three different models: one using selected EEG features, another using clinical features only, and a third combining both sets of features.\n\nThe feature selection process was conducted using the training set only, ensuring that the model's performance on the test set remained unbiased. This approach helped in identifying the most impactful features, which were then used to build and validate the model. The mean impact value (MIV) was used as an important index to select the independent features that had a significant impact on the model's performance. This method ensured that the selected features were not only statistically significant but also had a meaningful impact on the classification task.",
  "optimization/fitting": "The fitting method employed in this study utilized a Support Vector Machine (SVM) model, which is particularly advantageous for handling small sample sizes. The SVM model was chosen for its ability to find a hyper-plane in high-dimensional space using feature vectors derived from clinical and EEG data. This approach ensures that the model can effectively classify data even when the sample size is limited.\n\nTo address the potential issues of over-fitting and under-fitting, several strategies were implemented. The SVM model relies on the selection of an appropriate kernel function. The Radial Basis Function (RBF) kernel was used, which is commonly employed to map data into a suitable space for classification. This kernel function helps in managing the complexity of the data and prevents over-fitting by controlling the smoothness of the decision boundary through the Cost parameter.\n\nThe regularization parameters, including the Cost and \u03b3, were identified using a grid search method within a 5-fold cross-validation procedure. This process involved randomly dividing the dataset into five subsets, with each subset used as a validation set while the remaining four were used for training. This method was repeated five times to ensure that every portion of the data was used to assess the model's performance. The tuning of model parameters was optimized to minimize absolute mean errors, thereby enhancing the model's generalization capability.\n\nAdditionally, the study employed jack-knife validation and hold-out validation to further evaluate the model's feasibility in classifying and predicting the two groups of patients. These validation methods help in ensuring that the model is not over-fitting to the training data and can generalize well to new, unseen data.\n\nThe performance of the models was evaluated using statistical measures such as accuracy, sensitivity, specificity, positive predictive value, and negative predictive value. These metrics provide a comprehensive assessment of the model's effectiveness in classifying the data correctly.\n\nIn summary, the fitting method involved the use of an SVM model with an RBF kernel, regularization parameters optimized through grid search and cross-validation, and additional validation techniques to ensure the model's robustness and generalization capability. These steps collectively help in ruling out over-fitting and under-fitting, thereby enhancing the reliability of the model's predictions.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our support vector machine (SVM) model. One of the key methods used was cross-validation, specifically 5-fold cross-validation. This process involved randomly dividing the dataset into five subsets, using four subsets for training and one for validation, and repeating this process five times. This approach helped to ensure that the model's performance was not dependent on a particular subset of the data.\n\nAdditionally, we used a grid search method to optimize the model's parameters, including the regularization parameter (Cost) and the gamma parameter (\u03b3) in the radial basis function (RBF) kernel. This method systematically worked through multiple combinations of parameter tunes to determine the best settings, further reducing the risk of overfitting.\n\nWe also implemented hold-out validation and jack-knife validation to evaluate the model's generalization. In hold-out validation, the dataset was split into two subsets, with each subset used as a test set and the remaining one used as the training set. Jack-knife validation involved singling out each patient in turn as a test set while using the remaining patients as the training set. These validation methods provided additional checks on the model's performance and helped to confirm its reliability.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we employed a radial basis function (RBF) kernel for our support vector machine (SVM) model. The regularization parameters, including the cost (C) and gamma (\u03b3), were identified using a grid search method within a 5-fold cross-validation procedure. This process involved randomly dividing the dataset into five subsets, using each subset in turn as a validation set while the remaining four were used for training. This cycle was repeated five times to ensure that every portion of the data was used to assess the model's performance.\n\nThe tuning of model parameters was aimed at minimizing absolute mean errors, which are the differences between predicted and measured output values on the validation set. The performance of the models was evaluated based on the average scores from each fold.\n\nRegarding the availability of model files and optimization parameters, these specifics are not explicitly provided in the publication. However, the methods and configurations described are reproducible based on the details given. For further information or access to specific model files, interested parties may need to contact the authors directly.\n\nThe study also utilized additional validation methods, such as jack-knife validation and hold-out validation, to further demonstrate the model's feasibility in classifying and predicting patient outcomes. These methods involved singling out each patient as a test set and using the remaining patients as the training set, or equally dividing the validation set into two subsets for testing and training.\n\nIn summary, while the hyper-parameter configurations and optimization schedule are thoroughly reported, the actual model files and optimization parameters are not explicitly available in the publication. Interested researchers can replicate the methods described to achieve similar results.",
  "model/interpretability": "The model employed in this study is not entirely a black box, as it incorporates mechanisms to enhance interpretability. The use of the Mean Impact Value (MIV) algorithm is a key feature that contributes to the model's transparency. This algorithm ranks the relative importance of various clinical and electroencephalographic (EEG) features in predicting the outcomes. For instance, the \u03b2 band from the Fp2 region was identified as having a significant impact on the prediction model, indicating its importance in determining the efficacy of levetiracetam therapy.\n\nAdditionally, the model's design includes the extraction of only significant EEG features, ensuring that the features used are relevant and meaningful. This approach helps in understanding which specific factors are influencing the predictions. The model also considers clinical features as important factors, further adding to its interpretability. By evaluating the model's performance using multiple validation methods, such as 5-fold cross-validation, jack-knife validation, and hold-out validation, the robustness and generalizability of the model are assessed, providing insights into its reliability.\n\nThe combination of EEG and clinical characteristics in the model allows for a more comprehensive understanding of the factors affecting treatment outcomes. The detailed evaluation metrics, including accuracy, sensitivity, specificity, positive predictive value, and negative predictive value, offer a clear view of the model's performance. This transparency is crucial for clinicians and researchers to trust and utilize the model in real-world applications.",
  "model/output": "The model employed in our study is a classification model. Specifically, we utilized a Support Vector Machine (SVM) with a Radial Basis Function (RBF) kernel to perform binary classification. The goal was to predict the efficacy of levetiracetam treatment by classifying patients into two groups: those who would achieve seizure freedom (SF) and those who would not (NSF).\n\nThe model was trained using a combination of EEG features and clinical characteristics. We evaluated its performance using various statistical measures, including accuracy, sensitivity, specificity, positive predictive value, and negative predictive value. Additionally, we generated a receiver operating characteristic (ROC) curve and calculated the area under the curve (AUC) to assess the model's discriminative ability.\n\nThe classification process involved two main steps. First, we randomly selected about 80% of the patients from each group to build the model using a Lib-SVM classifier. The model parameters, including the kernel and regularization parameters, were optimized using a grid search method within a 5-fold cross-validation procedure. This ensured that the model was robust and generalizable.\n\nIn the second step, the remaining 20% of the patients were used as a test set to validate the model's performance. The results showed that the model achieved high accuracy and AUC values, indicating its effectiveness in predicting treatment outcomes.\n\nOverall, the SVM classification model demonstrated promising results in distinguishing between SF and NSF patients based on EEG and clinical features. This approach can be further refined and validated with larger datasets to improve its predictive accuracy and clinical utility.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several steps and techniques to ensure the robustness and generalization of the model. Initially, the performance of the models was assessed using average scores from models trained on each fold of a cross-validation process. This process involved dividing the dataset into multiple subsets, with each subset used as a validation set while the remaining subsets were used for training. This approach helped in optimizing the model parameters and evaluating their performance.\n\nIn addition to cross-validation, other methods such as hold-out validation and resampling validation (including bootstrapping and jack-knife) were employed. In hold-out validation, the validation set was split into two subsets, with each subset used alternately as the test set and the training set. The jack-knife validation involved singling out each patient in turn as the test set, while the remaining patients formed the training set. These methods were used to further demonstrate the model's feasibility in classifying and predicting outcomes for two groups of patients.\n\nThe prediction performance of the Support Vector Machine (SVM) was evaluated using various statistical measures, including accuracy, sensitivity, specificity, positive predictive value, and negative predictive value. These measures provided a comprehensive assessment of the model's effectiveness in distinguishing between true positives, true negatives, false positives, and false negatives. Additionally, a receiver operating characteristic (ROC) curve and the area under the ROC curve (AUC) were generated to visualize and quantify the model's performance.\n\nFor the first step of the evaluation, approximately 80% of the patients from each group were randomly selected as the training set to build the model. The remaining 20% of the patients were used as the test set to validate the model's function. This split ensured that the model was trained on a sufficient number of samples while also having an independent set for evaluation.\n\nThe SVM model used a radial basis function (RBF) as the kernel, which is commonly used to map data into a higher-dimensional space. The model's parameters, including the regularization parameter (Cost) and the gamma parameter (\u03b3), were optimized using a grid search method within a 5-fold cross-validation procedure. This process involved iteratively adjusting the parameters to minimize the absolute mean errors between the predicted and measured output values on the validation set.\n\nIn summary, the evaluation method involved a combination of cross-validation, hold-out validation, and resampling validation techniques. These methods, along with statistical measures and visual tools like the ROC curve, provided a thorough assessment of the model's performance and generalization capabilities.",
  "evaluation/measure": "In our study, we evaluated the performance of our models using several statistical measures to ensure a comprehensive assessment. The primary metrics reported include accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). These metrics provide a detailed overview of the model's performance in classifying patients into seizure-free (SF) and not seizure-free (NSF) groups.\n\nAccuracy measures the overall correctness of the model's predictions, sensitivity (also known as recall or true positive rate) indicates the model's ability to identify true positive cases, and specificity (true negative rate) assesses the model's ability to identify true negative cases. PPV and NPV provide insights into the probability that a positive or negative prediction is correct, respectively.\n\nAdditionally, we generated a receiver operating characteristic (ROC) curve and calculated the area under the ROC curve (AUC) to further evaluate the model's performance. The ROC curve plots the true positive rate against the false positive rate at various threshold settings, and the AUC provides a single scalar value that summarizes the performance across all thresholds.\n\nThese metrics are widely used in the literature for evaluating classification models, particularly in medical research. They offer a balanced view of the model's performance, considering both the true positive and true negative rates, as well as the predictive values. By reporting these metrics, we aim to provide a transparent and thorough evaluation of our model's effectiveness in predicting treatment outcomes for patients.",
  "evaluation/comparison": "In our study, we employed several validation methods to evaluate the generalization and feasibility of our model in classifying and predicting patient groups. These methods included k-fold validation, jack-knife validation, and hold-out validation. The k-fold validation involved dividing the dataset into k subsets, using k-1 subsets for training and the remaining subset for validation, repeated k times. This approach ensured that each data point was used for both training and validation, providing a robust estimate of model performance.\n\nAdditionally, we utilized jack-knife validation, where each patient was singled out in turn as a test set, with the remaining patients used as the training set. This method is particularly useful for small sample sizes, as it maximizes the use of available data. Hold-out validation was also employed, where the validation set was equally divided into two subsets. Each subset was used as a test set, while the remaining subset was used for training. This process was repeated to create the model, ensuring a comprehensive evaluation of its performance.\n\nWe also compared the performance of different feature sets, including selected EEG features, clinical features only, and a combination of both. This comparison allowed us to assess the contribution of each feature type to the model's predictive accuracy. The performance metrics used included accuracy, sensitivity, specificity, positive predictive value, and negative predictive value. These metrics provided a detailed evaluation of the model's ability to correctly classify patients into seizure-free (SF) and not seizure-free (NSF) groups.\n\nFurthermore, we generated a receiver operating characteristic (ROC) curve and calculated the area under the ROC curve (AUC) to evaluate the model's discriminative ability. The AUC provides a single scalar value that summarizes the model's performance across all classification thresholds, offering a comprehensive measure of its effectiveness.\n\nIn summary, our evaluation methods included a combination of k-fold, jack-knife, and hold-out validation techniques, along with a comparison of different feature sets. These approaches ensured a thorough assessment of our model's performance and its ability to generalize to new data.",
  "evaluation/confidence": "The evaluation of our model's performance included several statistical measures, such as accuracy, sensitivity, specificity, positive predictive value, and negative predictive value. These metrics were calculated for both the training and test sets, providing a comprehensive view of the model's effectiveness.\n\nTo ensure the robustness of our results, we employed multiple validation techniques. These included k-fold cross-validation, hold-out validation, and jack-knife validation. The k-fold cross-validation involved dividing the dataset into five subsets, using each subset in turn as a validation set while the remaining four were used for training. This process was repeated five times, and the performance metrics were averaged across all folds. This method helps to mitigate the risk of overfitting and provides a more reliable estimate of the model's generalization capability.\n\nAdditionally, we used hold-out validation, where the validation set was equally divided into two subsets. Each subset was used as a test set while the other was used for training, further validating the model's performance. The jack-knife validation involved singling out each patient in turn as a test set, with the remaining patients used as the training set. This method ensures that every data point is used for both training and testing, providing a thorough evaluation of the model's robustness.\n\nStatistical significance was determined using various tests. For quantitative data with a normal distribution, independent sample t-tests were used. For data with abnormal distributions, the Mann\u2013Whitney U test was applied. Qualitative data were analyzed using the \u03c72 test. A value of P < 0.05 was considered statistically significant, indicating that the results are unlikely to have occurred by chance.\n\nWhile confidence intervals for the performance metrics were not explicitly stated, the use of multiple validation techniques and statistical tests ensures that the results are reliable and generalizable. The model's performance was evaluated using a combination of EEG and clinical characteristics, EEG characteristics alone, and clinical characteristics alone. The results showed that the model combining EEG and clinical characteristics performed the best, with high accuracy, sensitivity, and specificity in both the training and test sets.\n\nIn summary, the evaluation of our model's performance was rigorous and comprehensive, utilizing multiple validation techniques and statistical tests to ensure the reliability and significance of the results. The model's superior performance, as indicated by the various metrics and validation methods, suggests that it is effective in classifying and predicting the outcomes for the two groups of patients.",
  "evaluation/availability": "Not enough information is available."
}