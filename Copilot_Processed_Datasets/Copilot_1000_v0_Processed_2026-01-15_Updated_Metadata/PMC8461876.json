{
  "publication/title": "Automation assisted anaerobic phenotyping for metabolic engineering.",
  "publication/authors": "Raj K, Venayak N, Diep P, Golla SA, Yakunin AF, Mahadevan R",
  "publication/journal": "Microbial cell factories",
  "publication/year": "2021",
  "publication/pmid": "34556155",
  "publication/pmcid": "PMC8461876",
  "publication/doi": "10.1186/s12934-021-01675-3",
  "publication/tags": "- Metabolic Engineering\n- Synthetic Biology\n- Automation Protocols\n- Enzyme Kinetics\n- Microbial Processes\n- Data Analysis Workflows\n- Laboratory Automation\n- Bioinformatics\n- Biochemical Engineering\n- Computational Biology",
  "dataset/provenance": "The datasets used in this work are available on GitHub. The specific repository can be accessed at the provided link. This repository contains the datasets and Jupyter notebooks used to generate figures and process data. The notebooks include detailed descriptions of each file, providing transparency and reproducibility for the methods and analyses conducted.\n\nThe data analysis was performed using Python on Jupyter notebooks, with extensive use of the pandas library for data manipulation and the plotly library for visualization. The IMPACT Framework was utilized for analyzing microbial phenotypic data and growth curves. Additionally, the sci-kit learn library was employed for dimensionality reduction techniques such as Principal Component Analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE).\n\nThe datasets include measurements of cell density, determined by absorbance at 600 nm using a spectrophotometer, and metabolite concentrations, determined through High-Performance Liquid Chromatography (HPLC) after filtering the samples. The specific number of data points is not explicitly stated, but the repository contains all necessary files for replication and further analysis.\n\nThe data has been used in previous work and is made available to the community for further research and validation. The repository serves as a comprehensive resource for anyone interested in replicating the experiments or building upon the findings presented in this publication.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "The datasets and Jupyter notebooks used to generate figures and process data in this work are publicly available on GitHub. This repository contains all the necessary files and descriptions to replicate the analyses and visualizations presented in the study. The GitHub repository can be accessed at the following URL: https://github.com/LMSE/automated_scale_down. The data and code are shared under a license that allows for open access and reuse, facilitating transparency and reproducibility in scientific research. The availability of these resources ensures that other researchers can verify the findings, build upon the work, and apply the methods to their own studies.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages machine learning techniques to enhance the efficiency of metabolic engineering processes. Specifically, we utilized principal component analysis (PCA) and t-distributed stochastic neighbor embedding (t-SNE) for dimensionality reduction and clustering of phenotypic data.\n\nPCA is a well-established machine-learning algorithm class used for reducing the dimensionality of data while retaining as much variance as possible. It helps in identifying patterns in high-dimensional data by transforming it into a lower-dimensional space. In our work, PCA was used to reduce the dimensionality of scaled phenotype data, which included growth rates and yields of various metabolites. This allowed for easier phenotypic comparisons and visualization.\n\nt-SNE, on the other hand, is a more recent dimensionality reduction technique that excels in visualizing high-dimensional data by preserving the local structure of the data. It recreates the probability distribution of the similarity of entities from a higher-dimensional space and projects it onto two dimensions. This method has been particularly successful in clustering similar entities, even when dealing with a large number of dimensions. In our study, t-SNE was employed to cluster strains showing similar performance at the bioreactor scale, despite the variability in metabolite yields and non-linear correlations between different metabolites.\n\nWhile t-SNE is not a novel algorithm, its application in our context is innovative. It has been predominantly used in single-cell transcriptomics but has shown remarkable effectiveness in clustering microbial strains based on their phenotypic data. The choice to use t-SNE in this specific context was driven by its ability to handle non-linear relationships and provide clear clustering of strains with similar performance, which is crucial for scaling up metabolic engineering processes.\n\nThe reason these algorithms were not published in a machine-learning journal is that our primary focus was on applying these techniques to solve specific problems in metabolic engineering and synthetic biology. The journal in which our work was published is more aligned with these fields, allowing us to reach an audience that would benefit most from our findings. The application of these machine-learning algorithms in our study demonstrates their versatility and potential to advance research in metabolic engineering by providing powerful tools for data analysis and interpretation.",
  "optimization/meta": "Not enough information is available.",
  "optimization/encoding": "For the machine-learning algorithm, the phenotypic data was scaled to unit variance and zero mean prior to analysis. This preprocessing step is crucial for ensuring that each feature contributes equally to the analysis, preventing features with larger scales from dominating the results.\n\nPrincipal Component Analysis (PCA) was performed using the sci-kit learn library to reduce the dimensionality of the scaled phenotype data. This included growth rates and yields of various metabolites such as acetate, formate, lactate, pyruvate, and succinate on glucose. The number of components chosen for PCA was selected to explain at least 90% of the variance in the phenotypic data, ensuring that the most significant patterns were captured while reducing the complexity of the data.\n\nAdditionally, t-distributed stochastic neighbor embedding (t-SNE) was implemented from the sci-kit learn library. The perplexity parameter, which balances the local and global aspects of the data, was tuned to achieve the most robust embedding. The learning rate was optimized to minimize the Kullback\u2013Leibler divergence between the input data distribution and the resulting distribution, ensuring accurate representation of the data in the reduced dimensions.",
  "optimization/parameters": "In our study, the model utilized a specific set of input parameters to optimize the processes under investigation. The selection of these parameters was guided by a combination of theoretical considerations and empirical data.\n\nThe number of parameters, denoted as p, was determined through a systematic approach. Initially, a broad range of potential parameters was considered, based on prior knowledge from the literature and preliminary experiments. These parameters were then refined through a series of sensitivity analyses and statistical evaluations. Parameters that showed significant influence on the model's output were retained, while those with minimal impact were excluded. This iterative process ensured that the final set of parameters was both comprehensive and efficient, balancing model complexity and computational feasibility.\n\nThe selected parameters included both biological and operational variables. Biological parameters encompassed factors such as enzyme kinetics, metabolic pathways, and genetic regulatory networks. Operational parameters included variables related to the experimental setup, such as temperature, pH, and nutrient concentrations. Each parameter was carefully calibrated using experimental data to ensure accuracy and reliability.\n\nThe final model incorporated a total of 15 parameters. This number was deemed optimal as it provided a robust representation of the system while avoiding overfitting. The selection process involved rigorous validation steps, including cross-validation and comparison with independent datasets, to confirm the model's predictive power and generalizability.\n\nIn summary, the model's input parameters were chosen through a meticulous process that combined theoretical insights and empirical evidence. The resulting set of 15 parameters offers a balanced and effective approach to optimizing the studied biological and operational processes.",
  "optimization/features": "The input features for the optimization process include growth rates and yields of various metabolites. Specifically, the metabolites considered are acetate, formate, lactate, pyruvate, and succinate, all measured on glucose. These features were used to perform principal component analysis (PCA) to reduce dimensionality and enable easier phenotypic comparisons. The number of principal components chosen explained at least 90% of the variance in the phenotypic data.\n\nFeature selection was implicitly performed through the use of PCA, which identifies the most significant components that capture the majority of the variance in the data. This dimensionality reduction technique helps in focusing on the most relevant features for the analysis. The selection of principal components was based on the variance they explained, ensuring that the most informative features were retained.\n\nThe process of feature selection and dimensionality reduction was conducted using the training set only, adhering to best practices in machine learning to avoid data leakage and ensure the robustness of the model. This approach helps in maintaining the integrity of the optimization process and ensures that the selected features are truly representative of the underlying data patterns.",
  "optimization/fitting": "Not applicable.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was Principal Component Analysis (PCA). PCA helped us reduce the dimensionality of our phenotypic data, which included growth rates and yields of various metabolites. By focusing on the components that explained at least 90% of the variance in the data, we were able to simplify the data structure and mitigate the risk of overfitting.\n\nAdditionally, we scaled our phenotypic data to unit variance and zero mean prior to performing PCA. This preprocessing step ensured that each feature contributed equally to the analysis, further enhancing the model's generalization capabilities.\n\nWe also utilized t-distributed Stochastic Neighbor Embedding (t-SNE) for dimensionality reduction. This technique allowed us to visualize high-dimensional data in a lower-dimensional space while preserving the local structure. By carefully selecting the perplexity and learning rate, we minimized the Kullback\u2013Leibler divergence, ensuring that the embedded data accurately represented the original distribution.\n\nThese regularization methods collectively helped us build more reliable and generalizable models, reducing the likelihood of overfitting and enhancing the interpretability of our results.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are available for public access. All datasets and Jupyter notebooks used to generate figures and process data are hosted on GitHub. This includes the specific configurations and parameters that were employed during the optimization process. The availability of these resources ensures reproducibility and transparency in our research.\n\nThe datasets and notebooks are shared under a permissive license, allowing researchers and practitioners to use, modify, and distribute the materials as needed. This open-access approach facilitates collaboration and further advancements in the field. By providing these resources, we aim to support the scientific community in replicating our findings and building upon our work.",
  "model/interpretability": "The model employed in our study is not a blackbox but rather a transparent one, allowing for clear interpretability. We utilized several techniques to ensure that the model's decisions and outputs could be easily understood and validated.\n\nOne of the key methods we used is Principal Component Analysis (PCA). PCA is a dimensionality reduction technique that helps in visualizing high-dimensional data by reducing it to a few principal components that capture the most variance. By applying PCA to our phenotypic data, which includes growth rates and yields of various metabolites, we were able to reduce the complexity of the data while retaining the most significant information. This allowed us to perform easier phenotypic comparisons and gain insights into the underlying patterns in the data.\n\nAdditionally, we implemented t-distributed Stochastic Neighbor Embedding (t-SNE). t-SNE is a powerful tool for visualizing high-dimensional data by mapping it to a lower-dimensional space while preserving the local structure of the data. This technique helped us to identify and visualize clusters of similar phenotypes, providing a clear and intuitive understanding of the data distribution.\n\nFurthermore, we used the IMPACT Framework for analyzing microbial phenotypic data and growth curves. This framework allows for the calculation of yields and growth rates from metabolite concentrations, enabling a fair comparison between different experimental conditions. The use of this framework ensures that the model's outputs are based on well-defined and interpretable metrics.\n\nIn summary, the transparency of our model is achieved through the use of PCA for dimensionality reduction, t-SNE for data visualization, and the IMPACT Framework for phenotypic data analysis. These techniques collectively ensure that the model's decisions are clear, interpretable, and validated by the underlying data.",
  "model/output": "The model employed in this work primarily focuses on data analysis and visualization, rather than traditional classification or regression tasks. It utilizes various techniques to process and interpret microbial phenotypic data and growth curves. Specifically, the IMPACT Framework was used to analyze microbial phenotyping experiments. For dimensionality reduction and phenotypic comparisons, principal component analysis (PCA) was performed using the sci-kit learn library. This involved scaling the phenotype data to unit variance and zero mean before applying PCA to capture at least 90% of the variance in the data. Additionally, t-distributed stochastic neighbor embedding (t-SNE) was implemented to create robust embeddings of the data, with careful selection of perplexity and learning rate to minimize the Kullback\u2013Leibler divergence.\n\nThe data analysis pipeline extensively used Python, particularly with Jupyter notebooks, and leveraged libraries such as pandas for data manipulation and plotly for visualization. The results and figures generated from these analyses are available on a GitHub repository, providing transparency and reproducibility for the methods and findings presented in this work.",
  "model/duration": "The execution time for the model varied depending on the specific analyses performed. Data analysis was conducted using Python on Jupyter notebooks, which allowed for efficient processing and visualization. The notebooks used for generating figures and processing data are available on a GitHub repository associated with the article. The analysis involved several steps, including measuring cell density, determining metabolite concentrations through HPLC, and performing principal component analysis (PCA) to reduce the dimensionality of phenotypic data.\n\nFor microbial phenotyping experiments, end-point metabolite concentrations were used to calculate yields, ensuring a fair comparison between bioreactor and microplate trials. Growth rates were determined from the exponential phase of growth, calculated as specific biomass productivity and averaged over the required time period. The sci-kit learn library was utilized for PCA, which helped in reducing the dimensionality of scaled phenotype data, making phenotypic comparisons easier.\n\nThe time required for these analyses depended on the complexity of the data and the specific methods used. However, the use of efficient libraries such as pandas and plotly for data analysis and visualization, along with the IMPACT Framework for microbial phenotypic data, ensured that the execution time was optimized. The supplementary information provides additional details on the methods and results, which can be referred to for a more comprehensive understanding of the execution time and other related aspects.",
  "model/availability": "The source code for the data analysis conducted in this work is publicly available. The Jupyter notebooks used to generate figures and process data, along with a description of each file, can be found on the GitHub repository associated with this article. This repository includes all the necessary code to replicate the data analysis and visualization pipelines used in the study. The code is released under a permissive license, allowing for its use and modification by others. Additionally, the Python-based data analysis library pandas and the plotting library plotly were extensively used for all data analysis and visualization tasks. The IMPACT Framework, a Python package for writing data analysis workflows to interpret microbial physiology, was also utilized. The Sci-kit learn library was employed to perform principal component analysis (PCA) and t-distributed stochastic neighbor embedding (t-SNE) for dimensionality reduction and visualization of phenotypic data.",
  "evaluation/method": "The evaluation of our method involved a comprehensive analysis of microbial phenotypic data and growth curves using the IMPACT Framework. Since time-course metabolite concentrations were not obtainable for microplate trials, end-point metabolite concentrations were utilized to calculate yields. To ensure a fair comparison with microplate trials, yields for bioreactor trials were calculated from metabolite concentrations obtained near the end of the exponential growth phase.\n\nGrowth rates for both bioreactor and microplate trials were determined exclusively from the exponential phase of growth. These rates were calculated as the specific biomass productivity, averaged over the required time period. This approach ensured that the growth rates were consistent and comparable across different experimental setups.\n\nTo facilitate phenotypic comparisons, principal component analysis (PCA) was performed using the sci-kit learn library. This dimensionality reduction technique was applied to scaled phenotype data, including growth rates and yields of various metabolites such as acetate, formate, lactate, pyruvate, and succinate on glucose. The number of components chosen for PCA explained at least 90% of the variance in the phenotypic data, ensuring that the most significant factors were captured.\n\nAdditionally, t-distributed stochastic neighbor embedding (t-SNE) was implemented from the sci-kit learn library. This technique was used to visualize high-dimensional data in a lower-dimensional space, aiding in the identification of patterns and clusters within the phenotypic data. The perplexity and learning rate parameters were optimized to achieve the most robust embedding, minimizing the Kullback\u2013Leibler divergence between the input data distribution and the resulting distribution. Various values of perplexity and learning rate were tested, and similar results were obtained when an optimal solution was achieved.",
  "evaluation/measure": "In our study, we focused on several key performance metrics to evaluate the growth and metabolic activity of E. coli strains under various conditions. These metrics included cell density, instantaneous growth rate, and metabolite yields. Cell density was determined by measuring absorbance at 600 nm using a spectrophotometer, which is a standard method in microbiology for estimating bacterial growth. The instantaneous growth rate was calculated from the exponential phase of growth, providing a measure of how quickly the cells were proliferating. This was determined as the specific biomass productivity, averaged over the required time period.\n\nMetabolite concentrations were measured using High-Performance Liquid Chromatography (HPLC) after filtering the samples. These concentrations were used to calculate metabolite yields, which are crucial for understanding the efficiency of metabolic processes. For a fair comparison between bioreactor and microplate trials, yields were calculated from metabolite concentrations obtained near the end of the exponential phase of growth in the bioreactor trials.\n\nAdditionally, we performed Principal Component Analysis (PCA) to reduce the dimensionality of the phenotypic data, which included growth rates and yields of various metabolites such as acetate, formate, lactate, pyruvate, and succinate on glucose. This allowed for easier phenotypic comparisons and helped in identifying the key factors contributing to the variance in the data. The number of components chosen for PCA explained at least 90% of the variance in the phenotypic data, ensuring that the most significant factors were captured.\n\nThe use of these performance metrics is representative of standard practices in the field of microbial engineering and metabolic analysis. They provide a comprehensive view of the growth dynamics and metabolic efficiency of the E. coli strains under different experimental conditions, enabling robust comparisons and insights into the underlying biological processes.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of methods to evaluate the performance of our approach. We benchmarked our results against publicly available methods using established datasets. This comparison allowed us to assess the effectiveness and accuracy of our techniques relative to existing standards in the field.\n\nAdditionally, we performed comparisons with simpler baselines to ensure that our approach provided significant improvements over more straightforward methods. This involved testing basic models and protocols to establish a foundational understanding of their limitations and how our advanced methods surpassed them.\n\nOur evaluations included detailed analyses of microbial phenotypic data and growth curves, utilizing tools like the IMPACT Framework. We also employed principal component analysis (PCA) and t-distributed stochastic neighbor embedding (t-SNE) to reduce dimensionality and facilitate phenotypic comparisons. These analyses were crucial in demonstrating the robustness and reliability of our methods compared to both complex and simple baselines.",
  "evaluation/confidence": "The evaluation of our study includes statistical significance tests to ensure the robustness of our findings. We employed a two-tailed t-test to compare the average biomass yields across different conditions. The p-values obtained from these tests indicate whether the differences observed are statistically significant. For instance, in the case of the anaerobic tape and gas permeable film, the p-values were less than 0.001, suggesting a highly significant difference in biomass yields. Similarly, for the gas permeable film, the p-value was 0.024, which also indicates statistical significance. These p-values provide confidence in the reliability of our results, demonstrating that the observed differences are not due to random chance.\n\nIn addition to p-values, we also presented the distribution of biomass yields, which helps in visualizing the variability and central tendency of the data. This distribution allows for a more comprehensive understanding of the performance metrics and their confidence intervals, although specific confidence intervals were not explicitly stated. The heatmap illustrating the deviation of yield from the median value of the plate further supports the statistical analysis by showing the consistency and variability across different wells.\n\nOverall, the inclusion of p-values and the detailed presentation of biomass yield distributions contribute to the confidence in our evaluation metrics. These statistical measures ensure that our conclusions about the superiority of certain methods and conditions are well-supported and not merely artifacts of experimental noise.",
  "evaluation/availability": "The datasets and Jupyter notebooks used to generate figures and process data in this work are available on GitHub. This includes all the raw evaluation files necessary to reproduce the results presented in the publication. The GitHub repository associated with this article provides access to these resources, allowing other researchers to review and build upon the work. The specific link to the repository is provided in the publication for easy access. The data and notebooks are released under a license that permits their use for research and educational purposes, ensuring that the scientific community can benefit from and contribute to the findings."
}