{
  "publication/title": "Lupus nephritis or not? A simple and clinically friendly machine learning pipeline to help diagnosis of lupus nephritis.",
  "publication/authors": "Wang DC, Xu WD, Wang SN, Wang X, Leng W, Fu L, Liu XY, Qin Z, Huang AF",
  "publication/journal": "Inflammation research : official journal of the European Histamine Research Society ... [et al.]",
  "publication/year": "2023",
  "publication/pmid": "37300586",
  "publication/pmcid": "PMC10257380",
  "publication/doi": "10.1007/s00011-023-01755-7",
  "publication/tags": "- Lupus nephritis\n- Systemic lupus erythematosus\n- Machine learning\n- Diagnostic algorithms\n- Serological markers\n- Meteorological factors\n- Feature selection\n- Hyperparameter optimization\n- XGBoost\n- Clinical data analysis\n- Biomarkers\n- Retrospective studies\n- Multi-center studies\n- Data preprocessing\n- Model performance evaluation",
  "dataset/provenance": "The dataset used in this study was sourced from the Luzhou Environmental Monitoring Station, covering the period from 2017 to 2022. The dataset includes seventeen meteorological indicators, which were matched to the exposure of patients' areas 15 days before their first diagnosis. These indicators encompass various environmental factors such as air humidity, air pressure, and concentrations of pollutants like SO2, NO2, and PM2.5.\n\nThe dataset underwent a rigorous preprocessing and feature conversion process. An exploratory analysis was conducted to understand the data characteristics, during which instances lacking result values were removed. Columns with no values in supervised learning were excluded from the analysis. Univariate analysis was performed between individual characteristics and the results of LN, using the \u03c72 test for categorical characteristics and the Mann\u2013Whitney test for quantitative characteristics.\n\nThe dataset was divided into ten training sets and test sets using tenfold cross-validation partitions through a three-tiered hierarchical strategy. Standard scalar methods were applied for feature transformation and normalization, ensuring that each feature was scaled independently to eliminate the distance between the average and the scaling unit.\n\nThe dataset was also subjected to feature processing and selection to evaluate the importance of specific machine learning modeling algorithms. Mutual information (MI) and multisurf, a relief-based feature selection algorithm, were used to determine whether a given feature should be retained or deleted. Feature selection occurred independently in ten training sets, with the top-ranked feature importance scores outputted to ensure the fidelity of the final feature importance set.\n\nThe dataset was used to develop a machine learning pathway for diagnosing LN based on commonly available clinical, laboratory, and meteorological data. The extreme gradient boosting (XGB) algorithm was identified as the best performer in diagnosing LN. The study also highlighted the need for multi-center studies to verify the models and the potential for future work with advanced algorithms.",
  "dataset/splits": "The dataset was divided into ten training sets and test sets using tenfold cross-validation partitions. This approach ensures that the data is thoroughly utilized for both training and testing the machine learning models. Each fold contains a different subset of the data, allowing for a comprehensive evaluation of the models' performance. The specific distribution of data points in each split is not detailed, but the tenfold cross-validation method ensures that each data point is used for both training and testing across different iterations. This technique helps in assessing the models' generalization capabilities and robustness.",
  "dataset/redundancy": "The datasets were split using a tenfold cross-validation approach. This method involves partitioning the cleaned dataset into ten distinct training sets and test sets. This ensures that each fold of the data is used for both training and testing, providing a robust evaluation of the model's performance.\n\nThe training and test sets are independent in each fold, meaning that the data used for training does not overlap with the data used for testing within the same fold. This independence is enforced through the cross-validation process, where the data is randomly shuffled and then divided into the specified number of folds. Each fold is then used as a test set once, while the remaining folds are used for training.\n\nThe distribution of the data in our datasets compares favorably to previously published machine learning datasets. By using tenfold cross-validation, we ensure that the model is trained and tested on a variety of data subsets, which helps in generalizing the results and reducing the risk of overfitting. This approach is widely accepted in the machine learning community for its ability to provide a comprehensive evaluation of model performance.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in our study are not new but rather well-established methods. These include logistic regression, decision trees, random forests, naive Bayes, extreme gradient boosting (XGB), light gradient boosting (LGB), support vector machines (SVM), and artificial neural networks (ANN). These algorithms are widely recognized and have been extensively used in various machine learning applications.\n\nThe reason these algorithms were not published in a machine-learning journal is that our focus was on applying these established methods to a specific medical problem\u2014diagnosing lupus nephritis (LN)\u2014rather than developing new algorithms. Our work involves optimizing these algorithms for our particular dataset and task, which includes hyperparameter optimization, feature importance estimation, and model performance comparison.\n\nWe chose to publish in a medical journal because our primary contribution is in the clinical application of machine learning. By demonstrating the effectiveness of these algorithms in diagnosing LN, we aim to provide a practical tool for clinicians. This approach aligns with our goal of bridging the gap between advanced machine learning techniques and their real-world medical applications.",
  "optimization/meta": "The model developed in this study does not function as a meta-predictor. Instead, it focuses on optimizing individual machine learning algorithms to improve the diagnosis of lupus nephritis. The process involves several key steps, including hyperparameter optimization, feature importance estimation, and model performance comparison.\n\nHyperparameter optimization is crucial as it allows the model to try different settings in the training set and select those that produce the best performance. This step ensures that the final model is not limited by default settings, which could lead to unbalanced algorithm comparisons and suboptimal performance.\n\nFeature importance estimation is another critical aspect. This step reveals which features are most important for accurate predictions. For algorithms like decision trees and random forests, the importance of features is built into the model and does not require further calculation. For other algorithms, the 'leave-one-out' method is used to estimate feature importance through machine learning retraining.\n\nThe model performance is evaluated using a comprehensive selection of binary classification metrics and visualizations. This includes balanced accuracy, accuracy, F1 score, recall, specificity, precision, area under the curve (AUC), true positive count (TP), true negative count (TN), false positive count (FP), false negative count (FN), precision\u2013recall curve (PRC), and average precision score (APS). These metrics provide an overall perspective of model performance across ten test sets.\n\nIn summary, the model does not rely on data from other machine-learning algorithms as input. It is designed to optimize individual algorithms and evaluate their performance independently. The training data is handled in a way that ensures independence, with tenfold cross-validation used to divide the dataset into training and test sets. This approach helps in comparing and verifying each model's performance and the consistency of feature selection.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, an exploratory analysis was conducted to understand the data and its characteristics. Instances lacking result values were removed, and any columns without values in supervised learning were excluded from the analysis. Univariate analysis was performed between individual characteristics and the results of LN, using the \u03c72 test for categorical characteristics and the Mann\u2013Whitney test for quantitative characteristics.\n\nThe dataset was then divided into ten training sets and test sets using tenfold cross-validation partitions through a three-hierarchical strategy. A standard scalar method was applied as a feature transformation and normalization step. This method operates on each feature independently, eliminating the distance between the average and the scaling unit.\n\nFeature processing and selection were crucial steps. Mutual information (MI) and multisurf, a relief-based feature selection algorithm, were used to evaluate feature importance. MI is effective for evaluating univariate association between features and results, while multisurf is sensitive to both univariate variables and heterogeneity. Feature selection occurred independently in the ten training sets, with each set potentially producing a different set of features. The top-ranked feature importance scores were output to ensure the fidelity of the final feature importance set.\n\nThe data was analyzed using Rstudio (version 3.1.1), Anaconda (version 2.3.1), and Python (version 3.9.13) software. The statistical significance was set at P < 0.05. This comprehensive approach ensured that the data was appropriately encoded and preprocessed for the machine-learning algorithm, enhancing the model's performance and reliability.",
  "optimization/parameters": "In our study, we utilized a comprehensive set of features to train and optimize our machine learning models. The exact number of parameters (p) used in the model varied depending on the specific algorithm and the feature selection process. Initially, we evaluated the importance of features using mutual information (MI) and multisurf, which helped in integrating feature selection. This process ensured that only the most relevant features were incorporated into the models.\n\nThe feature selection methods, MI and multisurf, were employed to highlight the advantages of collective feature selection. Features such as ASO, LA1, LA2, LA1/LA2, and others were selected by these algorithms, complementing the feature selection that may cause lupus nephritis (LN). The importance scores of all features are detailed in supplementary tables.\n\nFor the hyperparameter optimization, we did not use the default settings of each model because default settings can lead to unbalanced algorithm comparisons and may miss the best performance model. Instead, we optimized the hyperparameters for most of the algorithms, except for naive Bayes, which does not have any hyperparameters to optimize due to its modeling characteristics. We tried different hyperparameter settings in the training set and selected those that produced the best performance to establish the final model.\n\nFor example, in logistic regression, the 'solver' hyperparameter was set to 'saga' for optimal performance. In decision trees, the 'splitter' hyperparameter was set to 'best' to enhance model performance. For random forests, the 'oob score' hyperparameter was set to 'False' because it did not affect the model's performance. In support vector machines (SVM), the kernel hyperparameter was set to 'rbf' for better performance compared to 'linear'. For artificial neural networks (ANN), the 'solver' hyperparameter was set to 'adam' to balance the entire model effectively.\n\nIn summary, the number of parameters used in the model was determined through a rigorous feature selection process and hyperparameter optimization, ensuring that the models were trained with the most relevant features and optimal settings.",
  "optimization/features": "In our study, we utilized a comprehensive set of features to train our machine learning models. Specifically, we obtained seventeen meteorological indicators from the Luzhou Environmental Monitoring Station, covering the period from 2017 to 2022. These indicators include air humidity, air pressure, SO2, NO2, PM2.5, and other relevant metrics. Additionally, we incorporated various clinical and laboratory test results, such as ANA, anti-dsDNA, anti-Sm, anti-SSA, anti-SSB, anti-RNP, IgA, IgM, and IgG, among others.\n\nFeature selection was indeed performed to enhance the model's performance and reduce the dimensionality of the data. We employed two primary methods for feature selection: mutual information (MI) and multisurf, a relief-based feature selection algorithm. MI is effective in evaluating the univariate association between features and the target variable, while multisurf is sensitive to both univariate variables and heterogeneity. This collective approach ensures that the most relevant features are retained for modeling.\n\nThe feature selection process was conducted independently within each of the ten training sets generated through tenfold cross-validation. This means that each training set could produce a potentially different set of features. To maintain the fidelity of the final feature importance set, we outputted the top-ranked feature importance scores. This approach ensures that the selected features are consistently important across different subsets of the data, thereby improving the robustness and generalizability of our models.",
  "optimization/fitting": "In our study, we employed a machine learning pipeline designed to diagnose lupus nephritis, focusing on optimizing model performance through hyperparameter tuning and feature importance estimation. The number of parameters in our models was not excessively large compared to the number of training points, which helped mitigate the risk of overfitting. To further ensure robustness, we utilized tenfold cross-validation. This technique divides the data into ten subsets, training the model on nine and testing on the remaining one, repeating this process ten times. This approach helps in assessing the model's performance on different subsets of the data, providing a more reliable estimate of its generalization capability.\n\nTo rule out underfitting, we carefully selected and optimized hyperparameters for each algorithm. For instance, in logistic regression, we found that setting the 'solver' hyperparameter to 'saga' yielded the best performance. Similarly, for decision trees, setting the splitter hyperparameter to 'best' enhanced model performance. We also experimented with different kernel settings in Support Vector Machines (SVM), finding that the radial basis function (RBF) kernel outperformed the linear kernel. For artificial neural networks (ANN), the 'adam' solver was more effective in balancing the model. These optimizations ensured that our models were complex enough to capture the underlying patterns in the data without being too simplistic.\n\nAdditionally, we evaluated feature importance using methods built into algorithms like decision trees and random forests, as well as the 'leave-one-out' method for other algorithms. This process helped in identifying the most relevant features, further refining our models and preventing underfitting. By summarizing the average performance and feature importance of each algorithm after tenfold cross-validation, we ensured that our models were neither overfitted nor underfitted, providing a balanced and accurate diagnostic tool for lupus nephritis.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One of the key methods used was hyperparameter optimization. By systematically trying different hyperparameter settings, we were able to select those that produced the best performance on the training set, thereby reducing the risk of overfitting. This process involved tuning parameters for various algorithms, such as setting the 'solver' hyperparameter to 'saga' for logistic regression and using the 'rbf' kernel for support vector machines (SVM).\n\nAdditionally, we utilized tenfold cross-validation to evaluate the performance of our models. This technique involves dividing the dataset into ten subsets, training the model on nine of them, and validating it on the remaining one. This process is repeated ten times, with each subset serving as the validation set once. This approach helps to ensure that the model generalizes well to unseen data and is not merely memorizing the training set.\n\nFeature selection was another crucial step in our pipeline. We evaluated the importance of features using methods like mutual information (MI) and multisurf, and incorporated them into our models. This helped in reducing the dimensionality of the data and focusing on the most relevant features, which in turn aids in preventing overfitting.\n\nFor algorithms like decision trees and random forests, the importance of features is built into the model, and no further calculation is required after training. For other algorithms, we used the 'leave-one-out' method to estimate feature importance through machine learning retraining. This ensured that our models were not relying on irrelevant or noisy features, further mitigating the risk of overfitting.\n\nIn summary, our approach to preventing overfitting involved a combination of hyperparameter optimization, tenfold cross-validation, and careful feature selection. These techniques collectively helped us to build robust and generalizable machine learning models for diagnosing lupus nephritis.",
  "optimization/config": "The hyperparameter configurations and optimization schedules for various machine learning models are reported in the supplementary figures. Specifically, the optimization of hyperparameters for models such as logistic regression, decision trees, random forests, support vector machines (SVM), and artificial neural networks (ANN) are detailed. For instance, the 'solver' hyperparameter for logistic regression is optimized to 'saga' for better performance. The decision tree's splitter hyperparameter is set to 'best' to enhance model performance. The random forest's 'oob score' is typically set to 'False', and the SVM's kernel hyperparameter performs better when set to 'rbf'. For ANN, the 'solver' hyperparameter is optimized using 'adam'.\n\nThe model files and optimization parameters are not explicitly mentioned as being available for download or access. The supplementary figures provide visualizations and descriptions of the hyperparameter optimization processes, but there is no indication of where these files can be accessed or under what license they might be available. Therefore, while the configurations and schedules are reported, the actual files and parameters are not made readily accessible.\n\nThe performance metrics and feature importance evaluations are summarized and compared across ten training sets, with detailed results presented in supplementary figures. This includes the average AUC of ROC and PRC for logistic regression, decision trees, and random forests, as well as the importance of features evaluated through built-in algorithms and the 'leave-one-out' method. However, the specific model files and optimization parameters used to achieve these results are not provided.",
  "model/interpretability": "The models developed in this study exhibit varying degrees of interpretability, with some being more transparent than others. The XGB (Extreme Gradient Boosting) and LGB (Light Gradient Boosting) algorithms, for instance, provide insights into feature importance, making them relatively interpretable. For XGB, features such as LA1, LA1/LA2, and insolation are extremely prominent, indicating their significant role in the model's decision-making process. Similarly, in the LGB algorithm, features like ASO, LA2, and CRP perform better, highlighting their importance.\n\nThe study also employs composite feature importance (CFIBPs) to understand the collective impact of features. ASOs produce the strongest composite feature importance, while other features like RF, Up/Ucr, and LA1/LA2 also have strong composite effects. This approach helps in identifying key features that contribute to the model's predictions, enhancing its interpretability.\n\nIn contrast, some models like naive Bayes and artificial neural networks (ANN) are more black-box in nature, offering less transparency in how they arrive at their predictions. However, the focus on feature importance and composite effects in the XGB and LGB models provides a clearer understanding of the underlying mechanisms, making these models more interpretable.\n\nOverall, while some models in this study are more transparent and interpretable due to their feature importance analysis, others remain relatively black-box. The emphasis on feature importance and composite effects in certain models helps in understanding their decision-making processes, contributing to the overall interpretability of the study's findings.",
  "model/output": "The model developed in this study is a classification model. Specifically, it is designed for binary classification tasks, focusing on assisting in the diagnosis of lupus nephritis (LN). The performance of the model is evaluated using various binary classification metrics, such as the area under the receiver operating characteristic curve (ROC AUC) and the area under the precision-recall curve (PRC AUC). These metrics indicate that the model is effective in distinguishing between the two classes it was trained to identify.\n\nThe model's output includes the average ROC and PRC curves, which compare the performance of different machine learning algorithms. For instance, the XGB algorithm demonstrated the best performance with an ROC AUC of 0.995 and a PRC AUC of 1.000, while the naive Bayes algorithm performed the worst with an ROC AUC of 0.799 and a PRC AUC of 0.823. These results highlight the model's capability to accurately classify the target condition.\n\nAdditionally, the model's performance is influenced by the selection of hyperparameters and the internal algorithms used. For example, setting the hyperparameter 'oob score' to 'False' in the random forest algorithm ensures consistent performance. Similarly, the choice of the kernel hyperparameter in the support vector machine (SVM) algorithm affects its performance, with the 'rbf' kernel yielding better results than the 'linear' kernel.\n\nThe model's features are evaluated using various methods, such as the 'leave-one-out' method in logistic regression and built-in algorithms in decision trees and random forests. This evaluation helps in identifying the most important features contributing to the model's predictions. For example, features like LA1, LA1/LA2, and insolation are prominent in the XGB algorithm, while ASO, LA2, and CRP perform better in the LGB algorithm.\n\nIn summary, the model is a binary classification model designed to assist in the diagnosis of LN. Its performance is evaluated using standard binary classification metrics, and the selection of hyperparameters and algorithms significantly impacts its accuracy. The model's output provides valuable insights into the importance of different features and the overall effectiveness of the classification algorithms used.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in our study involved a comprehensive approach to assess the performance of various machine learning algorithms in diagnosing lupus nephritis. We utilized tenfold cross-validation to ensure robust and reliable results. This technique divides the dataset into ten subsets, training the model on nine subsets and testing it on the remaining one, repeating this process ten times with different subsets.\n\nTo evaluate the models, we considered a wide range of binary classification metrics. These included balanced accuracy, which averages sensitivity and specificity to emphasize accurate predictions across all categories. Additionally, we calculated and reported accuracy, F1 score, recall, specificity, precision, area under the curve (AUC), true positive count (TP), true negative count (TN), false positive count (FP), and false negative count (FN). We also generated receiver operating characteristic (ROC) curves and precision-recall curves (PRC) for each algorithm, summarizing the average performance of the ten training models.\n\nFor feature importance estimation, we examined the significance of features from the perspective of each machine learning model. This involved using the 'leave-one-out' method to estimate feature importance through machine learning retraining. For decision tree and random forest algorithms, feature importance is inherently built into the model, requiring no further calculation.\n\nPost-analysis involved summarizing the average performance and feature importance of each algorithm after tenfold cross-validation. We compared the average ROC and PRC of all machine learning algorithms and performed Kruskal-Wallis one-way variance statistical analysis to determine if any algorithm performed significantly better or worse for the evaluation metrics. Finally, we generated composite feature importance bar plots to evaluate the consistency of feature selection.",
  "evaluation/measure": "In the evaluation of our machine learning models for diagnosing lupus nephritis, we employed a comprehensive set of binary classification metrics to ensure a thorough assessment of model performance. These metrics include balanced accuracy, accuracy, F1 score, recall, specificity, precision, area under the curve (AUC), true positive count (TP), true negative count (TN), false positive count (FP), false negative count (FN), precision\u2013recall curve (PRC), and average precision score (APS). Additionally, we generated receiver operating characteristic (ROC) and precision\u2013recall curves for each algorithm to visualize performance.\n\nBalanced accuracy was used to emphasize accurate predictions across all categories, providing a more nuanced view than simple accuracy, especially in imbalanced datasets. The F1 score, which is the harmonic mean of precision and recall, offers a single metric that balances both concerns. Recall (sensitivity) and specificity were calculated to understand the model's ability to identify positive cases and correctly identify negative cases, respectively. Precision measures the accuracy of positive predictions, while the AUC provides an aggregate measure of performance across all classification thresholds.\n\nThe inclusion of TP, TN, FP, and FN counts allows for a detailed examination of the model's error types and success rates. The PRC and APS metrics are particularly useful for evaluating performance on imbalanced datasets, as they focus on the performance of the positive class. By reporting these metrics, we aim to provide a holistic view of model performance, ensuring that our evaluation is both rigorous and representative of the broader literature on machine learning in medical diagnostics.",
  "evaluation/comparison": "A comprehensive comparison of model performance was conducted using a variety of binary classification metrics and visualizations. This approach provided an overall perspective of model performance across ten test sets. Key metrics included balanced accuracy, which emphasizes accurate prediction in any category, as well as accuracy, F1 score, recall, specificity, precision, area under the curve (AUC), true positive count (TP), true negative count (TN), false positive count (FP), false negative count (FN), precision\u2013recall curve (PRC), and average precision score (APS).\n\nFor each algorithm, receiver operating characteristic (ROC) and precision\u2013recall curves (PRC) were established, and the performance of the ten training models was summarized using average ROC and PRC. This detailed comparison allowed for a thorough evaluation of each model's strengths and weaknesses.\n\nIn addition to these metrics, the importance of features was examined from the perspective of each machine learning model. This revealed which features were most critical for making accurate predictions. For decision tree and random forest algorithms, the importance of reporting features was built into the model, requiring no further calculation after training. For other algorithms, the 'leave-one-out' method was used to estimate feature importance through machine learning retraining.\n\nThe performance of different algorithms was also compared using the Kruskal\u2013Wallis one-way variance statistical analysis. This analysis helped determine whether any machine learning algorithm performed significantly better or worse for the evaluation metrics. The results of this analysis provided insights into the relative effectiveness of each algorithm in diagnosing lupus nephritis.\n\nNot applicable",
  "evaluation/confidence": "The evaluation of our machine learning models includes a comprehensive set of performance metrics, each accompanied by confidence intervals. These metrics encompass balanced accuracy, accuracy, F1 score, recall, specificity, precision, true positive count (TP), true negative count (TN), false positive count (FP), false negative count (FN), area under the receiver operating curve (ROC AUC), and the area under the precision-recall curve (PRC AUC). Additionally, we report the average precision score (APS) for the precision-recall curve.\n\nTo ensure the robustness of our findings, we performed statistical analysis using the Kruskal-Wallis one-way variance test. This test helps determine whether any machine learning algorithm performs significantly better or worse across the evaluated metrics. The p-values associated with these tests are all less than 0.001, indicating that the differences in performance are statistically significant.\n\nThe confidence intervals for each metric provide a range within which the true performance of the models is likely to fall, giving a sense of the reliability of the reported values. For instance, the balanced accuracy for logistic regression is reported as 0.783 with a confidence interval of (0.029), while for random forest, it is 0.957 with a confidence interval of (0.008). This detailed reporting allows for a nuanced understanding of the model performance and its variability.\n\nIn summary, the performance metrics are rigorously evaluated with confidence intervals, and the statistical significance of the results is confirmed through appropriate tests. This approach ensures that our claims about the superiority of certain models are well-founded and reliable.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The evaluation results presented in the publication are derived from specific datasets and models that were used during the study. These datasets include clinical and laboratory data from patient visits, as well as meteorological data. The evaluation metrics, such as ROC AUC, PRC AUC, and PRC APS, were calculated based on these datasets and are reported in the supplementary tables and figures.\n\nThe study involved comparing the performance of various machine learning algorithms, including logistic regression, decision trees, random forests, naive Bayes, XGB, LGB, SVM, and ANN. The results of these comparisons are detailed in the supplementary materials, which include tables and figures that show the performance metrics for each algorithm.\n\nThe evaluation process included calculating binary classification metrics for each model across ten test sets. The importance of each model's features was also estimated and is shown in the supplementary figures. The performance of each model varied due to differences in parameter settings and internal algorithms.\n\nThe study aimed to develop and verify a clinically applicable algorithm analysis path through data preprocessing, feature processing and selection, machine learning modeling, and analysis. The models showed good performance, with the XGB model based on collective feature selection performing particularly well.\n\nThe evaluation results are comprehensive and provide a clear comparison of the performance of different machine learning algorithms. However, the raw evaluation files themselves are not publicly released, and access to these files would require direct contact with the authors or relevant institutions."
}