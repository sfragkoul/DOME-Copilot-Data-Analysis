{
  "publication/title": "Evaluation of information from artificial intelligence on rotator cuff repair surgery.",
  "publication/authors": "Warren E Jr, Hurley ET, Park CN, Crook BS, Lorentz S, Levin JM, Anakwenze O, MacDonald PB, Klifto CS",
  "publication/journal": "JSES international",
  "publication/year": "2024",
  "publication/pmid": "38312282",
  "publication/pmcid": "PMC10837709",
  "publication/doi": "10.1016/j.jseint.2023.09.009",
  "publication/tags": "- Rotator Cuff Repair\n- AI in Medicine\n- ChatGPT\n- Patient Information\n- Quality Assessment\n- Readability Analysis\n- DISCERN Score\n- JAMA Benchmark Criteria\n- Natural Language Processing\n- Orthopedic Surgery",
  "dataset/provenance": "The dataset utilized in this study was derived from a free, online AI natural language processing model, specifically ChatGPT (GPT-3.5). This model was queried with twenty-four commonly asked questions from patients regarding rotator cuff repair. These questions were curated from a combination of previous studies into patient internet searches about rotator cuff repair and sports medicine.\n\nThe questions were categorized according to the Rothwell classification into one of three themes: fact, policy, or value. This classification helps identify the type of problem being queried. For instance, a question of fact asks whether something is true and to what extent, while a question of policy inquires whether a specific course of action should be undertaken to solve a problem. A question of value seeks an evaluation of an object, idea, event, or person.\n\nThe dataset consists of responses generated by the AI model to these twenty-four questions. The responses were then evaluated using validated scoring systems to assess the reliability, quality, and readability of the information provided. This approach allowed for a comprehensive analysis of how well the AI model performs in addressing patient queries related to rotator cuff repair.\n\nThe number of data points in this study is limited to the twenty-four questions and their corresponding AI-generated responses. This focused dataset enables a detailed examination of the AI's capabilities and limitations in providing medical information. While the dataset is specific to this study, the methodology and findings can contribute to the broader community's understanding of AI's role in medical information dissemination.",
  "dataset/splits": "Not applicable",
  "dataset/redundancy": "Not applicable. The study did not involve the creation or use of machine learning datasets. Instead, it evaluated the reliability, quality, and readability of information generated by an AI language processing model in response to a set of predefined questions about rotator cuff disease and repair. The questions were curated from previous studies on patient internet searches and categorized according to the Rothwell classification into fact, policy, or value themes. The AI model used was ChatGPT (GPT-3.5), and the responses were assessed using validated scoring systems such as the DISCERN score and JAMA benchmark criteria. The study did not involve splitting datasets into training and test sets or comparing distributions to previously published machine learning datasets.",
  "dataset/availability": "The data used in this study, including the specific questions asked and the AI-generated responses, are not publicly released in a forum. The full list of questions is available in Table I within the publication, but the AI-generated responses are provided in a supplementary appendix. This appendix is accessible online at the provided DOI link, but it is not hosted on a public data repository. The supplementary data is intended for supporting the findings of the study and is not licensed for independent use or redistribution. The enforcement of data availability is managed through the journal's policies, ensuring that the supplementary material is accessible to readers and reviewers but not freely available for public download or reuse.",
  "optimization/algorithm": "The optimization algorithm employed in this study leverages deep learning and reinforcement learning techniques. These are well-established classes of machine-learning algorithms that have been extensively used in various fields, including medical research and natural language processing.\n\nThe specific model used is ChatGPT, which is based on the GPT-3.5 architecture. This architecture is not new but represents a significant advancement in the field of natural language processing. The choice to use this model in a medical context, rather than a machine-learning journal, is driven by the practical application and evaluation of its performance in generating reliable and high-quality medical information. The focus is on assessing the model's ability to provide accurate and readable responses to medical queries, particularly regarding rotator cuff disease and repair, rather than on the intricacies of the algorithm itself.\n\nThe decision to publish in a medical journal reflects the study's primary goal of evaluating the model's utility in medical education and patient counseling. The emphasis is on the practical implications and the quality of the information produced, which is crucial for healthcare providers and patients. This approach aligns with the study's objective to ensure that the information generated by AI models is reliable and accessible, thereby enhancing the quality of medical advice available online.",
  "optimization/meta": "The model evaluated in this study is not a meta-predictor. It is a standalone AI natural language processing model, specifically ChatGPT (GPT-3.5), which generates responses based on its own training data and algorithms. It does not use data from other machine-learning algorithms as input.\n\nThe study focuses on assessing the reliability, quality, and readability of information provided by this AI model regarding rotator cuff disease and repair. The evaluation involves validated scoring systems such as the DISCERN score and JAMA benchmark criteria, as well as readability metrics like the Flesch-Kincaid Reading Ease Score and Flesch-Kincaid Grade Level.\n\nSince the model is not a meta-predictor, questions about the constituent machine-learning methods or the independence of training data do not apply. The AI model operates independently, and its performance is evaluated based on its own capabilities and the quality of the information it generates.",
  "optimization/encoding": "Not applicable.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "The study utilized a set of twenty-four questions as input features. These questions were curated from previous studies on patient internet searches about rotator cuff repair and sports medicine. The questions were categorized into three themes based on the Rothwell classification: fact, policy, and value. This classification helps identify the type of problem being queried. The full list of questions is available in a supplementary table.\n\nFeature selection was not explicitly mentioned as a process in this study. The questions were directly used as inputs to evaluate the AI model's responses. The evaluation focused on the reliability, quality, and readability of the AI-generated answers to these questions. The DISCERN score and JAMA benchmark criteria were used to assess the quality of the responses, while the Flesch-Kincaid Reading Ease Score and Flesch-Kincaid Grade Level were used to evaluate readability.",
  "optimization/fitting": "Not applicable.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "Not applicable.",
  "model/interpretability": "The model evaluated in this study, ChatGPT, is largely a black-box system. This means that the internal workings and decision-making processes of the model are not transparent or easily interpretable. The model generates responses based on patterns it has learned from vast amounts of text data, but it does not provide explicit references or sources for the information it presents. This lack of transparency is a significant limitation, as it makes it difficult to verify the accuracy and reliability of the information provided.\n\nOne of the key findings of our study was that the model consistently failed to reference any source material, earning a score of 0 on the JAMA Benchmark criteria across all question types. This absence of attribution makes it challenging for users to assess the credibility of the information. While the model's responses were generally of good quality, the lack of transparency raises concerns about its reliability and potential for providing incorrect or misleading information.\n\nThe model's design, which relies on reinforcement learning with human feedback, further contributes to its black-box nature. This approach allows the model to improve over time based on user interactions, but it does not provide insights into how specific responses are generated. As a result, users must rely on the model's outputs without knowing the underlying rationale or evidence supporting them.\n\nIn summary, the model's black-box nature is a significant limitation that affects its interpretability and reliability. While the model can generate high-quality responses, the lack of transparency and source attribution makes it difficult for users to trust the information provided. This highlights the need for caution when using such models, especially in fields like medicine where accurate and reliable information is crucial.",
  "model/output": "The model discussed in this publication is a classification model. It categorizes questions into three distinct themes based on the Rothwell classification system: fact, policy, or value. This classification helps in evaluating the type of problem being queried, whether it seeks objective evidence, a specific course of action, or an evaluation of an object, idea, event, or person. The model's output is then assessed for reliability, quality, and readability using validated scoring systems. The classification aspect is crucial for understanding how the model processes and responds to different types of inquiries related to rotator cuff repair.",
  "model/duration": "The execution time for the model was not explicitly measured or reported in this study. The focus was primarily on evaluating the reliability, quality, and readability of the information provided by the AI model, rather than the time it took to generate responses. Therefore, specific details about the execution time are not available.",
  "model/availability": "Not applicable.",
  "evaluation/method": "The evaluation method involved querying a free AI language processing software with twenty-four commonly asked questions from patients on rotator cuff repair. These questions were curated from previous studies into patient internet searches about rotator cuff repair and sports medicine. The questions were categorized according to the Rothwell classification into three themes: fact, policy, or value. A question of fact asks whether something is true and to what extent, using objective evidence. A question of policy asks whether a specific course of action should be undertaken to solve a problem. A question of value asks for an evaluation of an object, idea, event, or person.\n\nThe quality of responses was assessed using two validated tools: the DISCERN score and the Journal of the American Medical Association (JAMA) benchmark criteria. The DISCERN instrument evaluates the reliability and treatment information of written patient information about management options for a medical problem. It comprises three sections: reliability, treatment information, and an overall quality rating. Each question is scored from 1 to 5, with higher scores indicating better quality. The JAMA benchmark criteria assess the quality of information based on authorship, attribution, disclosure, and currency, with each standard scoring 1 point.\n\nThe readability of responses was evaluated using the Flesch-Kincaid Reading Ease Score (FRES) and the Flesch-Kincaid Grade Level (FKGL). The FRES generates a score from 0 (unreadable) to 100 (very easy to read) based on the formula 206.835 - 1.015*(total words/total sentences) - 84.6 *(total syllables/ total words). The FKGL provides an estimated reading level based on the complexity of the text.\n\nTwo authors independently scored the responses and discussed discrepancies until final scores were agreed upon. The evaluation aimed to assess the reliability, quality, and readability of the AI-generated responses regarding rotator cuff disease and repair.",
  "evaluation/measure": "The performance metrics reported in this study focus on evaluating the reliability, quality, and readability of information provided by an AI natural language processing model, specifically ChatGPT, regarding rotator cuff disease and repair.\n\nThe quality of the responses was assessed using two validated tools: the DISCERN score and the Journal of the American Medical Association (JAMA) benchmark criteria. The DISCERN instrument evaluates the reliability and treatment information of written patient information about management options for a medical problem. It comprises three sections: reliability, treatment information, and an overall quality rating. Each question is scored from 1 to 5, with higher scores indicating better quality. A score greater than 70 is classified as \"excellent,\" and a score greater than 50 is classified as \"good.\"\n\nThe JAMA benchmark criteria include four standards: authorship, attribution, disclosure, and currency. Each standard scores 1 point, assessing the quality of information based on the origin, referencing, potential conflicts of interest, and relevance of the content.\n\nReadability was evaluated using the Flesch-Kincaid Reading Ease Score (FRES) and the Flesch-Kincaid Grade Level (FKGL). The FRES generates a score from 0 (unreadable) to 100 (very easy to read), while the FKGL denotes the minimum level of U.S.-based schooling required to understand the material. Higher FKGL scores correlate with more difficult-to-understand text.\n\nThe study found that the AI-generated responses had good quality scores across all question types, with DISCERN scores ranging from 51 to 55, all classified as \"good.\" However, the reliability portion of the DISCERN score was low due to the absence of source material references. The readability scores indicated that the responses were at a 10th grade or higher reading level, which is above the recommended 6th grade reading level for patient materials.\n\nCompared to the literature, the AI model performed better than the average website found through search engines and at a similar level to websites affiliated with academic institutions. For instance, a comprehensive analysis of 150 websites concerning rotator cuff repairs found an overall mean DISCERN score of 44, with academic institution-affiliated websites scoring a mean of 51.6. The AI model's DISCERN scores matched or exceeded these published results.\n\nIn summary, the performance metrics used in this study are representative and comparable to those used in the literature. The metrics provide a comprehensive evaluation of the quality and readability of the AI-generated responses, highlighting both strengths and areas for improvement.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, we focused on evaluating the performance of a specific AI language processing model, ChatGPT, in generating responses to commonly asked questions about rotator cuff disease and repair. The questions were curated from previous studies on patient internet searches and categorized using the Rothwell classification into fact, policy, and value themes.\n\nWe assessed the quality of the AI-generated responses using two validated tools: the DISCERN score and the Journal of the American Medical Association (JAMA) benchmark criteria. The DISCERN instrument evaluates the reliability and treatment information of written patient information, while the JAMA benchmark criteria assess authorship, attribution, disclosure, and currency.\n\nFor readability, we used the Flesch-Kincaid Reading Ease Score (FRES) and Flesch-Kincaid Grade Level (FKGL). These tools helped us determine the ease of reading and the educational level required to understand the responses.\n\nWhile we did not compare ChatGPT's performance to simpler baselines or other publicly available methods, our findings indicated that the AI model provided generally high-quality information across all question types. However, it had significant shortcomings in reliability due to the absence of source material citations. The readability of the responses was also above the recommended level for laypeople, which is a notable area for improvement.\n\nNot applicable",
  "evaluation/confidence": "The evaluation of the AI chat system's performance did not include confidence intervals for the metrics used. The study reported mean scores for the DISCERN and Flesch-Kincaid assessments, but it did not provide confidence intervals or standard deviations to indicate the variability or reliability of these scores.\n\nThe results showed that the AI-generated responses had good quality scores across different question types, with DISCERN scores ranging from 51 to 55, which are considered \"good.\" However, the lack of statistical significance testing or confidence intervals makes it challenging to definitively claim that the AI method is superior to other sources or baselines. The study did not compare the AI's performance to a control group or other baseline methods using statistical tests, such as t-tests or ANOVA, to determine if the differences in scores were statistically significant.\n\nAdditionally, the study did not assess the consistency of the AI's responses over time or with different inputs, which could further impact the confidence in its performance. While the AI's responses were generally of good quality, the absence of statistical analysis and confidence intervals limits the ability to make strong claims about its superiority or reliability compared to other information sources.",
  "evaluation/availability": "The raw evaluation files, including the list of questions used and the AI-generated responses, are available in supplementary materials. Specifically, the full list of questions is provided in Table I, and the AI-generated responses can be found in Supplementary Appendix S1. These materials are intended to support the transparency and reproducibility of the study's findings. The supplementary data can be accessed online at the provided DOI link, ensuring that readers have the necessary information to verify and build upon the research conducted."
}