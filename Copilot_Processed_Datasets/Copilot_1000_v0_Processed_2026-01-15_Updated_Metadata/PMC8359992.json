{
  "publication/title": "Artificial Intelligence for Unstructured Healthcare Data: Application to Coding of Patient Reporting of Adverse Drug Reactions.",
  "publication/authors": "L\u00e9tinier L, Jouganous J, Benkebil M, Bel-L\u00e9toile A, Goehrs C, Singier A, Rouby F, Lacroix C, Miremont G, Micallef J, Salvo F, Pariente A",
  "publication/journal": "Clinical pharmacology and therapeutics",
  "publication/year": "2021",
  "publication/pmid": "33866552",
  "publication/pmcid": "PMC8359992",
  "publication/doi": "10.1002/cpt.2266",
  "publication/tags": "- Artificial Intelligence\n- Machine Learning\n- Pharmacovigilance\n- Adverse Drug Reactions\n- Natural Language Processing\n- Gradient Boosting\n- Deep Learning\n- Text Classification\n- Healthcare Data\n- Medical Terminology\n- Data Management\n- Model Validation\n- Overfitting\n- Hyperparameter Tuning\n- External Validation",
  "dataset/provenance": "The dataset used in our study originates from two primary sources: the Bordeaux Pharmacovigilance Centre and the Marseille Centre for Regional Pharmacovigilance. The Bordeaux dataset is substantial, containing a total of 393,407 adverse drug reactions (ADRs) reported over a specific period. For our internal validation, we focused on preferred terms (PTs) with at least 10 occurrences, which resulted in 10,675 potential ADRs and 125 distinct PTs.\n\nThe external validation dataset comes from the Marseille Centre and includes 407 potential ADRs and 85 distinct PTs. These PTs were selected based on their presence in the learning set from Bordeaux. The Marseille dataset was stratified to ensure a variety of cases, particularly to include a broader range of drugs beyond the most frequently reported ones, such as Levothyrox.\n\nThe representativeness of our dataset is strong, with the 125 PTs from Bordeaux corresponding to 64.6% of the ADRs in the Bordeaux dataset, and the 85 PTs from Marseille corresponding to 58.3% of those in the Marseille dataset. This ensures that our models are trained and validated on a diverse and representative sample of ADRs.\n\nThe data used in this study is real-life, expert-annotated, which is a significant strength. It allows us to demonstrate the concrete capabilities of our AI pipeline in classifying ADRs from unstructured textual data. The use of such data ensures that our models are robust and can handle the complexities of real-world pharmacovigilance reports.",
  "dataset/splits": "The dataset was split into three parts: training, internal validation, and external validation.\n\nThe training and internal validation were performed using data from one center, considering preferred terms (PTs) with at least 10 occurrences. This resulted in 10,675 potential adverse drug reactions (ADRs) and 125 distinct PTs. The internal validation set consisted of 10% of the data from this center, amounting to 206 cases.\n\nThe external validation focused on PTs present in the learning set, using data from another center. This led to 407 potential ADRs and 85 distinct PTs from 187 cases, which also represented 10% of the data from this center.\n\nThe distribution of data points in each split ensured a strong representativeness of the overall dataset, with the 125 PTs in the training set corresponding to 64.6% of the ADRs entered in the database for the period, and the 85 PTs in the external validation set corresponding to 58.3% of those.",
  "dataset/redundancy": "The datasets were split into training, internal validation, and external validation sets. The training and internal validation were performed using 90% of the Bordeaux dataset, which consisted of 2,058 forms. The internal validation set comprised 10% of the Bordeaux dataset, totaling 206 cases. This internal validation was crucial for assessing the model's performance on data it had not seen during training.\n\nFor external validation, we used 10% of cases declared to the CRPV of Marseille during the same period, amounting to 187 cases. This external validation set was independent of the training and internal validation sets, ensuring that the model's performance could be evaluated on entirely new data. The independence of the datasets was enforced by using data from different centers, Bordeaux and Marseille, which helped in assessing the model's generalizability and robustness.\n\nThe distribution of the external validation set differed from the Bordeaux dataset, particularly in the drug distribution and the experts who analyzed the cases. This difference allowed for a better estimate of the model's transposability to new, unseen data. The external validation set included a variety of cases, ensuring that the model's performance was not overly optimized for the specific characteristics of the Bordeaux dataset.\n\nCompared to previously published ML datasets, our approach ensured a strong representativeness of the BNPV in our dataset. The 125 distinct preferred terms (PTs) included in our dataset corresponded to 64.6% of the 393,407 ADRs entered in the BNPV for the period. Similarly, the 85 PTs in our external validation dataset corresponded to 58.3% of those, guaranteeing a comprehensive and representative evaluation of the model's performance.",
  "dataset/availability": "Not applicable.",
  "optimization/algorithm": "The machine-learning algorithms used in our study belong to several classes, including traditional machine learning methods and deep learning techniques. Specifically, we employed random forests, gradient boosting, support vector machines, and neural networks. These are well-established algorithms in the field of machine learning and have been extensively used in various applications, including healthcare.\n\nThe gradient boosting model, LightGBM, demonstrated the highest performance with an AUC of 0.93 and an F-measure of 0.72. Other notable models included logistic regression (logit) with an AUC of 0.91 and an F-measure of 0.58, FastText with an AUC of 0.89 and an F-measure of 0.55, and Long Short-Term Memory (LSTM) networks with an AUC of 0.86 and an F-measure of 0.35. Additionally, we explored hybrid models that combined machine learning techniques with regular expressions, which showed performances comparable to the standalone models.\n\nNone of the algorithms used are new; they are established methods in the machine learning community. The choice of these algorithms was driven by their proven effectiveness in handling complex data and their ability to generalize well to new, unseen data. The focus of our study was on applying these algorithms to the specific problem of identifying and determining the seriousness of adverse drug reactions from patient reports, rather than developing new algorithms. Therefore, publishing in a machine-learning journal was not the primary goal, as our contributions lie in the application and validation of these methods in a clinical context.",
  "optimization/meta": "The meta-predictor approach used in our study involves combining predictions from multiple machine-learning models to improve overall performance. This method leverages the strengths of different algorithms to create a more robust and accurate predictive system.\n\nThe meta-predictor integrates predictions from various machine-learning techniques, including gradient boosting, support vector machines, and neural networks. These models were trained on a dedicated train-test tuning split to ensure optimal performance. The gradient boosting model, specifically LightGBM, demonstrated the best performance among the conventional machine-learning models.\n\nIn addition to these conventional models, deep learning techniques such as Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks were also employed. These deep learning models are particularly effective at processing complex data and large datasets, providing an additional layer of predictive power.\n\nThe meta-predictor also incorporates a regular expression (RegEx) approach, which is used to match MedDRA terms directly in the case reports. This method serves as a baseline for the machine-learning models and is combined with them to create hybrid models. The hybrid models aggregate predictions from both the RegEx engine and the machine-learning models, using a method similar to bagging, where the final predictions are built by averaging the vectors from both sources.\n\nTo ensure the independence of the training data, internal validation was carried out on 10% of the Bordeaux dataset, consisting of 206 cases. External validation was performed on 10% of cases declared to the CRPV of Marseille during the same period, totaling 187 cases. This external validation set differed in drug distribution and was analyzed by different experts, providing a better estimate of the model's transposability.\n\nThe use of independent data for external validation is crucial for controlling overfitting, especially with nonparametric and nonlinear models that have more flexibility in learning the target function. This approach ensures that the model's performance is indicative of its future performance on new, unseen data.",
  "optimization/encoding": "For the machine-learning algorithms, data encoding and preprocessing were crucial steps to ensure the models could effectively learn from the input data. The adverse event description texts were vectorized using the term frequency-inverse document frequency (TF-IDF) method. This technique helps to highlight important words in the text by giving more weight to terms that are frequent in a particular document but rare across the entire corpus. The resulting TF-IDF vectors were then expanded with additional features such as age, sex, body mass index, and drugs, creating a comprehensive feature vector for conventional machine-learning models.\n\nFor the recurrent long-short-term memory (LSTM) neural network, the data formatting was slightly different. The LSTM model required word index sequences as input, which were derived from the preprocessed text. This approach allowed the LSTM to capture the sequential dependencies in the text data, which is particularly useful for understanding the context and meaning of words in a sentence.\n\nAdditionally, for the FastText model and its variants, the preprocessed text was provided directly. FastText is designed to handle text classification by representing words and subwords efficiently, which helps in capturing morphological information and improving the model's performance, especially with limited data.\n\nOverall, the data encoding and preprocessing steps were tailored to the specific requirements of each machine-learning model, ensuring that the input data was in the optimal format for training and prediction.",
  "optimization/parameters": "In our study, we employed various machine learning models, each with its own set of hyperparameters that required tuning. For models like random forests, gradient boosting, and neural networks, we utilized a classical grid search strategy on a dedicated train-test tuning split to optimize these hyperparameters. This approach allowed us to systematically work through multiple combinations of parameter tunes to determine the best configuration.\n\nThe specific number of parameters (p) varied depending on the model. For instance, gradient boosting models, such as LightGBM (LGBM), have parameters like learning rate, number of trees, maximum depth, and others that need to be fine-tuned. Similarly, neural networks have parameters related to the architecture, such as the number of layers, number of neurons per layer, activation functions, and optimization algorithms.\n\nThe selection of these parameters was driven by the need to balance model complexity and performance. We aimed to avoid overfitting, which is more likely with nonparametric and nonlinear models that have more flexibility. To control overfitting, we performed external validation on independent data, ensuring that our models generalized well to new, unseen data.\n\nFor the LGBM model, which showed the best performance, the hyperparameters were tuned to achieve an optimal balance between bias and variance. The final model was evaluated using metrics such as the area under the receiver operating characteristic curve (AUC) and the F-measure, which provided a comprehensive assessment of its performance.\n\nIn summary, the number of parameters and their selection were model-specific and were determined through a rigorous tuning process using grid search and external validation to ensure robust and generalizable performance.",
  "optimization/features": "In our study, we utilized a combination of text and structured features as inputs for our models. The exact number of features (f) varied depending on the model and the specific task, but we focused on the 125 most frequent Preferred Terms (PTs) for our primary analysis. Feature selection was indeed performed to ensure that we were working with the most relevant and frequent terms, which helped in managing the complexity of the dataset and improving model performance.\n\nThe feature selection process was conducted using the training set only, adhering to best practices to prevent data leakage and ensure that the models could generalize well to unseen data. This approach helped us to identify the most informative features while maintaining the integrity of our validation process. By limiting our evaluation to these frequent PTs, we aimed to balance the need for comprehensive coverage with the practical constraints of model training and validation.",
  "optimization/fitting": "In our study, we employed several machine learning models, including gradient boosting, neural networks, and others, which indeed have a large number of parameters compared to the number of training points. To address the risk of overfitting, we implemented a rigorous validation strategy. We performed internal validation on 10% of the Bordeaux dataset, comprising 206 cases, and external validation on 10% of cases from the CRPV of Marseille, totaling 187 cases. The external validation dataset differed in drug distribution and was analyzed by different experts, ensuring a robust estimate of our model's transposability.\n\nTo further mitigate overfitting, we used a classical grid search strategy for hyperparameter tuning on a dedicated train-test tuning split. Additionally, we computed 95% confidence intervals for the evaluation metrics using 1,000 bootstrap train-test samples. This approach helped us assess the stability and generalizability of our models.\n\nRegarding underfitting, we ensured that our models were complex enough to capture the underlying patterns in the data. The use of advanced techniques like gradient boosting and deep learning models, such as Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks, provided the necessary flexibility. The performance metrics, including the area under the curve (AUC) and F-measure, indicated that our models were adequately capturing the data's complexity without being too simplistic.\n\nOverall, our validation strategies and model selection processes helped us balance the trade-off between overfitting and underfitting, ensuring that our models generalized well to new, unseen data.",
  "optimization/regularization": "To prevent overfitting, we employed several techniques. We utilized a classical grid search strategy for hyperparameter tuning on a dedicated train-test tuning split. This approach helped in finding the optimal parameters for our models, such as random forests, gradient boosting, and neural networks.\n\nAdditionally, we performed external validation on independent data. This is considered the best solution to control overfitting, especially for nonparametric and nonlinear models that have more flexibility when learning a target function. Our internal validation was carried out on 10% of the Bordeaux dataset, consisting of 206 cases. For external validation, we used 10% of cases declared to the CRPV of Marseille during the same period, totaling 187 cases. This external validation set differed from the Bordeaux dataset in terms of drug distribution and was analyzed by different experts, providing a better estimate of our system's transposability.\n\nFor relatively small datasets, internal validation by bootstrap techniques may not be sufficient. Therefore, we ensured that our models were robust and generalizable by using external validation. This method helped us assess the model's performance on future, unseen data, thereby mitigating the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are not explicitly detailed in the publication. However, we did employ a classical grid search strategy on a dedicated train-test tuning split for hyper-parameter tuning of models like random forests, gradient boosting, or neural networks.\n\nFor those interested in the specific configurations and optimization parameters, example code for the LightGBM (LGBM) model, which demonstrated the best performance, is available on a GitHub repository. The repository can be accessed at [https://github.com/louisletinier/MAIT-AI.git](https://github.com/louisletinier/MAIT-AI.git). This repository includes code that showcases the implementation and tuning of the LGBM model, providing insights into the hyper-parameter configurations and optimization strategies employed.\n\nRegarding the availability of model files, these are not explicitly mentioned in the publication. For further details on data management or algorithms, including specific model files and optimization parameters, readers are encouraged to contact the corresponding author using the email address provided on the first page of the publication. The license under which the code and other materials are shared is not specified in the provided information.",
  "model/interpretability": "The models employed in our study span a range of interpretability levels, from relatively transparent to more opaque, black-box approaches. Conventional machine learning models like logistic regression, support vector machines, random forests, and gradient boosting (specifically LightGBM) offer varying degrees of interpretability.\n\nLogistic regression, for instance, is quite transparent. It provides coefficients that indicate the strength and direction of the relationship between each feature and the outcome, making it straightforward to interpret the impact of individual variables.\n\nRandom forests and gradient boosting models, while more complex, still offer some interpretability. Techniques like feature importance scores can be extracted from these models, showing which variables are most influential in making predictions. This allows for a degree of transparency, although the exact decision paths within the ensemble of trees are not easily interpretable.\n\nOn the other hand, deep learning models such as LSTM and CNN are considered black-box models. These neural networks involve multiple layers and complex interactions between neurons, making it difficult to trace the exact reasoning behind their predictions. While tools and methods exist to probe these models and gain some insights, they generally lack the transparency of simpler models.\n\nFastText and its variant, ExtremeText, also fall into a more interpretable category compared to deep learning models. FastText, in particular, uses word embeddings and n-grams, which can be examined to understand how different parts of the text contribute to the classification. This makes it somewhat easier to interpret the model's decisions compared to the more opaque neural networks.\n\nIn summary, our study utilizes a mix of models with different levels of interpretability. Conventional machine learning models and FastText offer clearer insights into their decision-making processes, while deep learning models remain more enigmatic. This diversity allows us to balance between model performance and the need for interpretability in different applications.",
  "model/output": "The model is primarily focused on classification tasks. Specifically, it addresses the identification of adverse drug reactions (ADRs) and the determination of ADR seriousness from free text in patient reports. The models used, such as logistic regression, support vector machines, random forests, gradient boosting, and deep learning models like LSTM and CNN, are all employed for text classification. The goal is to classify and code ADRs using MedDRA terminology, which involves assigning specific terms to describe adverse events. Additionally, the models assess the seriousness of these ADRs based on predefined criteria, resulting in a binary classification decision. The performance of these models is evaluated using metrics like the area under the curve (AUC) and the F-measure, which are standard for classification tasks.",
  "model/duration": "The models were trained on an Intel Core i7 processor with 16 GB RAM. Specific execution times for each model were not detailed, but the training and internal validation processes were performed on a dataset comprising 10,675 potential adverse drug reactions (ADRs) and 125 distinct preferred terms (PTs). The external validation was conducted on a separate set of 407 potential ADRs and 85 distinct PTs. The training and validation processes involved using various machine learning models, including gradient boosting, logistic regression, FastText, and deep learning models like LSTM and CNN. Additionally, a regular expression (RegEx) engine was used for baseline comparisons and in hybrid models. The best-performing model, LightGBM (LGBM), demonstrated strong performance metrics, such as an AUC of 0.93 and an F-measure of 0.72 on the internal validation set. The external validation of the LGBM model showed an AUC of 0.91 and an F-measure of 0.58. The overall performance of the models was evaluated using metrics like ROC curve AUC, F-measure, true negatives, true positives, false negatives, and false positives. The use of structured features in conjunction with text features resulted in a slight improvement in the F-measure for the LGBM model.",
  "model/availability": "The source code for the models used in our study is publicly available. Specifically, example code for the Light Gradient Boosting Machine (LGBM) model, which demonstrated the best performance, and for the hybrid LGBM with regex approach, can be found on a GitHub repository. The repository is accessible at https://github.com/louisletinier/MAIT-AI.git. This repository provides the necessary code to replicate the models and their implementations. The models were developed using Python libraries such as scikit-learn, lgbm, keras (with TensorFlow backend), and FastText + ExtremeText. For those interested in more detailed information about data management or the algorithms, the corresponding author can be contacted via the email address provided on the first page of the publication.",
  "evaluation/method": "The evaluation of the method involved several key steps to ensure robust and reliable performance metrics. We employed a classical grid search strategy for hyperparameter tuning on a dedicated train-test tuning split, which is crucial for models like random forests, gradient boosting, or neural networks.\n\nTo assess the models' performance in identifying adverse drug reactions (ADRs) and determining their seriousness, we used receiver operating characteristic (ROC) curve/area under the curve (AUC) and F-measure (the harmonic mean of precision and recall). These metrics provide a comprehensive evaluation of the models' effectiveness. We selected the decision threshold that maximizes the F-measure to ensure optimal performance.\n\nFor internal validation, we used 10% of the Bordeaux dataset, comprising 206 cases. This internal validation was supplemented by an external validation on 10% of cases declared to the CRPV of Marseille during the same period, totaling 187 cases. The external validation set differed in drug distribution and was analyzed by different experts, providing a better estimate of the model's transposability.\n\nWe also utilized 1,000 bootstrap train-test samples to compute 95% confidence intervals for the evaluation metrics of each model. This approach helps in understanding the variability and reliability of the model's performance.\n\nThe best-performing model, LGBM (Light Gradient Boosting Machine), was evaluated on the external validation set, showing an AUC of 0.91 and an F-measure of 0.58. The limited size of the validation set did not allow for the computation of confidence intervals.\n\nAdditionally, we compared the performance of the best model trained solely on text features with the same model using both text and structured features. The inclusion of structured features resulted in a slight improvement in the F-measure from 0.69 to 0.72.\n\nOverall, the evaluation method ensured a thorough assessment of the models' performance, considering both internal and external validation to provide a comprehensive understanding of their effectiveness in real-world scenarios.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our machine learning models in identifying and determining the seriousness of adverse drug reactions (ADRs) from patient reports. The primary metrics reported include the Area Under the Curve (AUC) and the F-measure (F1 score). These metrics were chosen for their ability to provide a comprehensive evaluation of model performance, especially in the context of unbalanced datasets, which are common in health data.\n\nThe AUC is a widely used metric that assesses the model's ability to distinguish between positive and negative classes across all possible classification thresholds. It is particularly useful for evaluating models on unbalanced datasets, where the absence of ADRs might be dominant. However, AUC can sometimes present high values if the model is highly specific but not sensitive, which is why we also report the F-measure.\n\nThe F-measure is a single score that balances both the positive predictive value (precision) and sensitivity (recall). This metric is crucial because it provides a more nuanced evaluation by considering both the accuracy of positive predictions and the model's ability to identify all relevant instances. This is especially important in our context, where missing an ADR (false negative) can have significant clinical implications.\n\nIn addition to AUC and F-measure, we also report true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN). These metrics offer a detailed breakdown of the model's performance, allowing us to understand how often the models correctly identify ADRs and how often they make errors. This granularity is essential for understanding the practical implications of our models' performance in real-world applications.\n\nThe use of these metrics is representative of current best practices in the literature, ensuring that our evaluation is both rigorous and comparable to other studies in the field. By reporting AUC, F-measure, and the detailed breakdown of TP, FP, TN, and FN, we provide a comprehensive view of our models' strengths and limitations, which is crucial for their potential application in clinical settings.",
  "evaluation/comparison": "In our study, we employed several methods to evaluate and compare the performance of our models. We used regular expressions (RegEx) as a baseline for our machine learning (ML) models' benchmark. This approach involved matching MedDRA terms directly in the case reports after basic text preprocessing, such as accent removal, case lowering, and stemming. The RegEx engine provided binary vectors (0 or 1), which were then compared to the scores between 0 and 1 generated by the ML models.\n\nWe also developed hybrid models that combined the RegEx approach with ML models. These hybrid models aggregated predictions from both methods, averaging the vectors to build the final predictions. This approach was similar to bagging methods, like random forests, and allowed us to leverage the strengths of both RegEx and ML techniques.\n\nTo assess the performance of our models, we used receiver operating characteristic (ROC) curves/area under the curve (AUC) and F-measure (the harmonic mean of precision and recall). We selected the decision threshold that maximized the F-measure to ensure reliable evaluation metrics. Additionally, we used 1,000 bootstrap train-test samples to compute 95% confidence intervals for the evaluation metrics of each model.\n\nFor models that required hyperparameter tuning, such as random forests, gradient boosting, or neural networks, we employed a classical grid search strategy on a dedicated train-test tuning split. This process helped us optimize the performance of these models.\n\nIn summary, we compared our ML models to a simpler baseline using RegEx and developed hybrid models that combined both approaches. We used standard evaluation metrics and techniques to ensure a thorough and reliable comparison of model performances.",
  "evaluation/confidence": "In our study, we evaluated the performance of various machine learning models using several metrics, including the area under the curve (AUC) and the F-measure. For the internal validation, we were able to compute confidence intervals (CIs) for these metrics, providing a range within which the true performance is likely to fall. This allows for a more nuanced understanding of the model's performance and its variability.\n\nHowever, due to the limited size of the validation set, we were unable to compute CIs for the external validation results. This limitation means that while we can report the point estimates for metrics like AUC and F-measure, we cannot provide a confidence interval to indicate the precision of these estimates. Despite this, the external validation was crucial for assessing how well our models generalize to new, unseen data from different centers.\n\nThe results from the internal validation showed that our models, particularly the Light Gradient Boosting Machine (LGBM), performed well with high AUC values and reasonable F-measures. The external validation, although lacking CIs, still provided valuable insights into the models' performance in a real-world setting. The slight drop in performance during external validation was expected, given that the model was trained on data from a single center. This highlights the need for further training on more diverse datasets to improve generalizability.\n\nIn summary, while we have confidence intervals for the internal validation metrics, the external validation results are presented without CIs due to the limited validation set size. Nonetheless, the external validation strengthened the validity of our results by demonstrating the models' performance on new data.",
  "evaluation/availability": "Not applicable."
}