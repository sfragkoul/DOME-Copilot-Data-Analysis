{
  "publication/title": "Toward generalizable prediction of antibody thermostability using machine learning on sequence and structure features.",
  "publication/authors": "Harmalkar A, Rao R, Richard Xie Y, Honer J, Deisting W, Anlahr J, Hoenig A, Czwikla J, Sienz-Widmann E, Rau D, Rice AJ, Riley TP, Li D, Catterall HB, Tinberg CE, Gray JJ, Wei KY",
  "publication/journal": "mAbs",
  "publication/year": "2023",
  "publication/pmid": "36683173",
  "publication/pmcid": "PMC9872953",
  "publication/doi": "10.1080/19420862.2022.2163584",
  "publication/tags": "- Antibody design\n- Thermostability prediction\n- Machine learning\n- Unsupervised and supervised learning\n- Monoclonal antibodies\n- Single-chain variable fragment\n- Computational biology\n- Protein engineering\n- Deep learning\n- Sequence analysis",
  "dataset/provenance": "The dataset used in this study was curated from historical therapeutic programs focused on generating single-chain variable fragment (scFv) therapeutics. The data collection was not initially intended for training a predictive thermostability model, leading to non-uniform distributions in the dataset. The sequence data includes scFv sequences assembled by performing mutations on heavy and light chains from multiple germlines. A total of 2,700 scFv sequences from 17 different projects targeting various antigens were collated to form the primary sequence dataset. Additionally, sequences from another scFv study currently under clinical trials and an isolated scFv dataset were included as out-of-distribution, blind test sets. For each sequence, thermostability was evaluated using TS50 measurements, which represent the temperature at half-maxima of target binding. For the isolated scFv dataset, thermostability was evaluated using Tm measurements, which indicate the first transition from folding to unfolding as temperature increases. The dataset includes a range of TS50 measurements spanning from 25\u00b0C to 75\u00b0C, making it suitable for both regression and classification tasks. The sequences in the dataset are discrete and amenable to classification tasks, distinguishing based on thermostability. The dataset has not been previously used by the community for similar predictive tasks.",
  "dataset/splits": "The dataset used in this study was collected from historical single-chain variable fragment (scFv) therapeutic generation campaigns. The data was split into several sets for training and evaluation purposes.\n\nThere were 17 experimental sets, each containing scFv sequences targeting different antigens. These sets were used to train the models. Additionally, there were two out-of-distribution test sets: one consisting of sequences from another scFv study currently under clinical trials, and another consisting of isolated scFv sequences.\n\nThe training data comprised 2,700 scFv sequences from the 17 experimental sets. The test data included sequences from the out-of-distribution sets. Within the experimental sets, there were occasional replicates of the same sequence with different TS50 measurements, which were excluded from the training data to avoid redundancy. It is important to note that there is no sequence redundancy across the experimental sets, meaning the same sequence does not appear in different sets.\n\nThe TS50 measurements for the sequences span from 25\u00b0C to 75\u00b0C. These measurements were divided into four bins for the classification task: under-50\u00b0C, 50\u00b0C\u201360\u00b0C, 60\u00b0C\u201370\u00b0C, and 70\u00b0C-up. The distribution of the sequences across these bins is illustrated in supplementary figures.\n\nFor the regression task, the absolute values of the TS50 measurements were used. For the classification task, the TS50 data were divided into the aforementioned bins. The models were trained on all but one target, and evaluations were made on the held-out target. For non-TS50 data, an ensemble of TS50 models was used to make predictions.",
  "dataset/redundancy": "The datasets used in this study were split into experimental sets and test sets. The experimental sets consisted of 2,700 scFv sequences collected from 17 different projects targeting various antigens. These sequences were used for training and validation purposes. The test sets included sequences from another scFv study currently under clinical trials and an isolated scFv dataset, serving as out-of-distribution, blind test sets.\n\nTo ensure independence between the training and test sets, sequence redundancy was carefully managed. Within the same experimental sets, occasional replicates (same sequence with different TS50 measurements) were excluded from the training data. Additionally, there was no sequence redundancy across different experimental sets, meaning the same sequence did not appear in multiple sets. This approach helped in maintaining the independence and diversity of the training data.\n\nThe distribution of the scFv sequences across the experimental sets and the test dataset was illustrated in supplementary figures. The TS50 measurements for the sequences spanned from 25\u00b0C to 75\u00b0C, providing a wide range of thermostability data. This discrete nature of the TS50 values made the data amenable to classification tasks.\n\nCompared to previously published machine learning datasets, the datasets used in this study were curated from historical therapeutic programs and were not explicitly designed for training a predictive thermostability model. This resulted in non-uniform distributions in the data, which is a common challenge in real-world datasets. The careful management of sequence redundancy and the use of out-of-distribution test sets helped in addressing some of these challenges and ensured the robustness of the models trained on these datasets.",
  "dataset/availability": "The experimental thermostability data and sequences used in this study are not publicly available. These sequences are considered intellectual property and are part of internal antibody engineering studies. Therefore, they cannot be shared openly. However, the source code for the models developed in this work, including zero-shot, fine-tuned, and supervised models, is available for non-commercial use. This code can be accessed at the provided GitHub repository. For researchers interested in reanalyzing the data, additional information can be obtained by contacting the lead author. This approach ensures that the intellectual property is protected while still allowing for the scientific community to benefit from the methodological advancements presented in the study.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages convolutional neural networks (CNNs) for thermostability prediction tasks. These CNNs are supervised models, meaning they are trained on labeled data to learn the relationships between input features and the target variable, which in this case is thermostability.\n\nThe CNNs used are not entirely new; they build upon established architectures that have been successfully applied in various prediction tasks. However, the specific application to thermostability prediction of scFv sequences, along with the integration of energetic features derived from structural models, represents a novel approach.\n\nThe reason these models were not published in a machine-learning journal is that the focus of our work is on the biological application and the specific problem of predicting thermostability in scFv sequences. The innovation lies in the biological insights and the practical utility of the models for protein engineering rather than the development of new machine-learning algorithms per se. The models serve as tools to achieve our biological goals, and the primary contributions are in the domain of protein science and engineering.",
  "optimization/meta": "The model does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it directly utilizes sequence and energetic features derived from the scFv sequences.\n\nThe model architecture includes two branches: one for sequence data and another for energetic data. The sequence branch processes one-hot encoded amino acid sequences, while the energetic branch handles a two-dimensional residue energy matrix. These features are independently processed and then combined to make predictions about the thermostability of scFv sequences.\n\nThe training data for the model consists of scFv sequences from various experimental sets, with thermostability measured by TS50 values. The sequences are split into heavy and light chains, and additional tokens are included to indicate the start, end, and chain breaks. The energetic features are derived using the Rosetta ref2015 energy function, which evaluates the contributions of residue interactions to the total energy.\n\nThe model is trained using a supervised convolutional network, and the performance is evaluated on held-out and blind test sets. The training data is curated from historical therapeutic programs, ensuring that the sequences come from diverse sources and are not specifically designed for training the model. This diversity helps in assessing the model's generalizability to new, unseen datasets.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to prepare the input for our machine-learning models. We began by aggregating datasets for TS50 measurements of single-chain variable fragments (scFvs) from various experimental sets, forming a single comprehensive dataset. The scFv sequences, which comprised heavy and light chains linked by a Glycine-Serine (G4S) linker, were split into their respective heavy- and light-chain sequences. Instead of classifying sequences based on thermostability, we included their TS50 measurements directly.\n\nThe amino acid sequences were one-hot encoded to form an input of dimension (VH + VL + 3) \u00d7 21, where VH and VL correspond to the heavy- and light-chain sequences, respectively. An additional token was included in the one-hot encoding to indicate the start and end positions of the scFv sequence and the chain-break between heavy and light chains.\n\nFor the energetic inputs, sequences were first passed through a structural module, specifically the DeepAb protocol for antibody structure prediction. Each predicted structure underwent a Rosetta Relaxation and Refinement protocol for side-chain repacking. The energy estimation started with an energy relaxation step to reduce steric clashes, followed by further refinement with side-chain packing. The lowest energy structure was selected for further calculations. Residue\u2013residue interaction energies were then estimated using the residue_energy_breakdown application, converting these energies into a two-dimensional i-j matrix. This matrix served as the energetic information for training the supervised convolutional neural network (CNN) models. The energy values in the i-j matrix were binned into 20 bins between the lower-end and upper-end energies of -25 and 10 REU, respectively, with an additional bin for the start, end, and chain-break tokens. The dimension of the pairwise energy data was L \u00d7 L \u00d7 21, where L is the length of the sequence.\n\nThis preprocessing ensured that both sequence and energetic features were appropriately encoded and ready for input into our machine-learning models, enabling them to learn and predict thermostability effectively.",
  "optimization/parameters": "In our study, the model architecture and parameters were carefully selected to optimize performance for predicting thermostability of scFv sequences. The models evaluated included supervised convolutional networks that utilized sequence, energetics, and a combination of both features.\n\nFor the sequence inputs, the scFv sequences were one-hot encoded, resulting in an input dimension of (V_H + V_L + 3) * 21, where V_H and V_L correspond to the heavy and light chain sequences, respectively. The additional tokens in the one-hot encoding correspond to delimiters at the start and end positions of the scFv sequence and between heavy and light chains to indicate chain breaks.\n\nThe energetics inputs were derived from structural predictions using the DeepAb protocol, followed by Rosetta Relaxation and Refinement for side-chain repacking. The residue\u2013residue interaction energies were converted into a two-dimensional i-j matrix, which served as the energetic information for training. The energy values in this matrix were binned into 20 bins, with an additional bin for start, end, and chain-break tokens, resulting in a dimension of L * L * 21 for the pairwise energy data.\n\nThe models were trained using the Adam optimizer with a learning rate of 10^-3 for the supervised models and 10^-3 for the fine-tuned pre-trained language models. The parameters of the head model, including the initial down-projection for ESM-1b, were trained, while the parameters of the UniRep and ESM-1b models were frozen during training.\n\nThe architecture of the supervised convolutional networks included parallel branches for sequence and energetic inputs. The sequence input passed through a 1D convolutional layer, while the energetic input passed through a 2D convolutional layer. The outputs were concatenated and passed through additional convolutional layers to produce the final logits. The parameters of these models were optimized through a randomized search for hyperparameters, including layers, dropout, batch size, number of filters, kernel size, strides, epochs, and pooling size.\n\nThe performance of different architectures was assessed using Spearman\u2019s correlation coefficient. The 2D-CNN binned architecture for the energy branch was found to perform better than other architectures, such as 1D-CNNs and 2D-CNNs with absolute energy values. This architecture retained relevant energy information and provided context for individual residue\u2013residue interactions.\n\nIn summary, the model parameters were selected through a combination of structural and sequence-based features, optimized using convolutional networks, and fine-tuned using pre-trained language models. The architecture and hyperparameters were chosen to maximize performance and generalizability across different datasets.",
  "optimization/features": "In our study, we utilized two primary types of input features for our models: sequence features and energetic features.\n\nFor the sequence features, we used one-hot encoded amino acid sequences of single-chain variable fragments (scFvs). These scFvs comprised a heavy and a light chain linked together with a Glycine-Serine (G4S) linker. The sequences were split into their respective heavy- and light-chain sequences, resulting in an input dimension of (V_H + V_L + 3) * 21, where V_H and V_L correspond to the heavy- and light-chain sequences, respectively. The additional token in the one-hot encoding corresponds to the delimiter at the start and end positions of the scFv sequence and between heavy and light chains to indicate a chain-break.\n\nFor the energetic features, we obtained a two-dimensional i-j residue energy matrix. This matrix served as a contact map providing a reduced representation of protein-free energies. The energies were evaluated using the Rosetta ref2015 energy function, which considers various energy terms dependent on physical, empirical, statistical, and knowledge-based parameters. The energy values in the i-j matrix were binned into 20 bins between the lower-end and upper-end energies of -25 and 10 REU, respectively, with an additional bin for the start, end, and chain-break tokens. The dimension of the pairwise energy data is L * L * 21, where L is the maximum sequence length in the dataset.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, we focused on incorporating relevant structural and energetic information into our models. The sequence features were derived directly from the amino acid sequences, and the energetic features were obtained through structural prediction and energy estimation using established protocols. The models were designed to handle both types of features, allowing us to evaluate their individual and combined contributions to thermostability prediction.",
  "optimization/fitting": "In our study, we employed several strategies to address potential overfitting and underfitting issues. The number of parameters in our models, particularly the pretrained language models like ESM-1b and UniRep, is indeed much larger than the number of training points. To mitigate overfitting, we froze the parameters of these pretrained models during training and only trained the parameters of the classification head. This approach ensures that the model leverages the general protein sequence knowledge captured by the pretrained models without overfitting to the specific thermostability prediction task.\n\nAdditionally, we used an ensemble of models trained on different holdout targets for non-TS50 data. This ensemble approach helps to generalize better and reduces the risk of overfitting to any single dataset. For the supervised convolutional networks, we included structure-specific information in the form of energetic features, which provided a localized structural context and helped the model to learn more robust representations.\n\nTo rule out underfitting, we performed extensive hyperparameter tuning using a randomized search. This process involved optimizing various parameters such as the number of filters, kernel size, strides, epochs, and batch size. We also evaluated different CNN architectures for the energy branch, including 1D-CNN and 2D-CNN with absolute energy values. Through this systematic approach, we ensured that our models were complex enough to capture the underlying patterns in the data without being too simplistic.\n\nFurthermore, we compared the performance of our models with randomly initialized weights and weighted random predictions. The superior performance of our trained models over these baselines demonstrated that our models were not underfitting and were effectively learning from the data. Overall, these strategies helped us to balance the model complexity and ensure that our predictions were reliable and generalizable.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the generalizability of our models. One key approach was the use of dropout layers within our convolutional neural network (CNN) architectures. Dropout randomly sets a fraction of input units to zero at each update during training time, which helps prevent overfitting by ensuring that the model does not become too reliant on any single neuron.\n\nAdditionally, we utilized an ensemble of models to make predictions. For non-TS50 data, an ensemble of TS50 models, each trained on a different holdout target, was used to make predictions. This ensemble approach helps to average out the errors and biases of individual models, leading to more robust and generalizable predictions.\n\nWe also performed a randomized search for hyperparameters, including the number of filters, kernel size, strides, epochs, and pooling size. This systematic approach to hyperparameter tuning helped to find the optimal settings that minimized overfitting while maximizing performance on the validation set.\n\nFurthermore, we evaluated the performance of our models using Spearman's correlation coefficient, which is less sensitive to outliers compared to other metrics. This choice of evaluation metric ensured that our models were not overfitting to specific data points but rather capturing the overall trends in the data.\n\nLastly, we supplemented our sequence data with energetic features derived from structural predictions. This multi-modal approach provided the model with additional context, reducing the risk of overfitting to the sequence data alone. The energetic features were binned and fed into a 2D CNN, which helped retain relevant energy information through the convolutional layers.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are reported in detail within the publication. Specifically, we utilized a linear layer with a hidden dimension of 512, followed by a tanh activation function, and then a final layer to predict class logits. The parameters of the UniRep and ESM-1b models were frozen during training, while the parameters of the head model, including the initial down-projection for ESM-1b, were trained using the Adam optimizer with a learning rate of 10^-3.\n\nFor the TS50 data, models were trained on all but one target, with evaluations made on the held-out target. For non-TS50 data, an ensemble of TS50 models, each corresponding to a holdout target, was used to make predictions. This approach ensures that the models are robust and can generalize well to unseen data.\n\nThe model architecture and hyper-parameters are thoroughly documented, allowing for reproducibility. The detailed architecture of our supervised convolutional neural network (CNN) deep-learning model is illustrated in Figure 3a of the publication. This figure provides a comprehensive view of the model's structure, including the sequence branch with one-hot encoded amino-acid sequences and the energetics branch with the pairwise i-j residue-residue energy matrix.\n\nThe model files and optimization parameters are not explicitly mentioned as being available for download. However, the methods and configurations described in the publication are sufficient for readers to replicate the experiments. The publication adheres to standard academic practices, ensuring that all necessary details are provided for reproducibility. The license under which the data or code might be shared is not specified, but typical academic publications allow for the use of methods and configurations for research purposes.",
  "model/interpretability": "The models developed in this study are not entirely black-box systems. While they leverage complex neural network architectures, efforts have been made to incorporate interpretability.\n\nThe models utilize both sequence and energetic features, allowing for a degree of transparency. The sequence branch processes one-hot encoded amino acid sequences, providing a clear mapping of input sequences to model predictions. The energetics branch uses a two-dimensional residue energy matrix, which serves as a contact map. This matrix provides a reduced representation of protein-free energies, making it possible to trace back specific residue interactions that influence thermostability predictions.\n\nAdditionally, the models can be configured to operate with either sequence or energy features alone, or a combination of both. This modularity allows for an analysis of how each type of feature contributes to the final predictions. For instance, the energetics-only model demonstrated high classification accuracy, indicating that residue interaction energies play a significant role in determining thermostability.\n\nFurthermore, the use of t-distributed stochastic neighbor embedding (t-SNE) to project sequence embeddings into two dimensions revealed clustering patterns based on experimental sets. This visualization helps in understanding how the models infer underlying relationships in the data, making them less prone to overfitting on specific datasets.\n\nThe models also include mechanisms for zero-shot and fine-tuned evaluations, providing insights into how pre-trained language models can be adapted for specific tasks. The fine-tuned models, in particular, offer a clearer understanding of how learned representations from supervised data contribute to thermostability predictions.\n\nIn summary, while the models employ advanced machine learning techniques, they are designed with interpretability in mind. The use of clear input features, modular architecture, and visualization techniques enhances the transparency of the models, making it possible to understand the factors influencing their predictions.",
  "model/output": "The model developed for thermostability prediction of scFv sequences is designed to handle both regression and classification tasks. For the regression task, the model predicts the absolute TS50 measurement, which represents the temperature at half-max binding. This task involves predicting a continuous value. On the other hand, for the classification task, the model predicts whether a given sequence corresponds to a thermally stable scFv. The TS50 data are divided into four bins: under-50\u00b0C, 50\u00b0C\u201360\u00b0C, 60\u00b0C\u201370\u00b0C, and 70\u00b0C-up. The model's output for the classification task is the probability of a sequence falling into each of these temperature bins. This is achieved through a dense layer that generates these probabilities based on the input features from both the sequence and energetics branches. The model's performance is evaluated using metrics such as the Spearman correlation coefficient and the area under the receiver-operating characteristic (ROC) curve, particularly focusing on the 70-up bin for identifying thermostable sequences. The model architecture allows for the contributions from either the sequence branch or the energetics branch to be turned off, enabling the assessment of sequence or energy dependence over classification performance.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our method involved several key steps and datasets to ensure its robustness and generalizability. We employed a combination of held-out sets and blind datasets to assess the performance of our models. Specifically, we used sets P and Q as held-out datasets, which were not included in the training process. Additionally, we evaluated our models on two blind datasets representing a test scFv and an isolated scFv. These datasets allowed us to test the models' ability to generalize to new, unseen data.\n\nFor the energetics-only model, we constructed a receiver-operating-characteristic (ROC) curve derived from the prediction of the 70-up bin, which is crucial for identifying thermostable sequences. The area under the ROC curve was over 0.7, indicating high classification accuracy. We also evaluated the Spearman correlation coefficient for all four test datasets using the energetics-only, sequence-only, and energetics + sequences models. The coefficients were over 0.5 for the energetics-only model on the held-out datasets, demonstrating its effectiveness.\n\nTo further validate our models, we performed point mutations over an anti-VEGF antibody and analyzed the classification performance of our predictive models on these mutations. Out of 20 experimentally validated mutations, our networks correctly identified five, with four identified by the SCNN model and one by the ESM-1b fine-tuned model. This evaluation highlighted the models' ability to distinguish between thermostable and thermally degenerate mutations, even though the validation dataset for thermostability was sparse.\n\nAdditionally, we compared the performance of different models, including zero-shot and fine-tuned language models, on TS50 data and blind test sets. While zero-shot predictions did not correlate well with thermostability, fine-tuning the pre-trained models on TS50 data significantly improved their correlation on the held-out target. However, these fine-tuned models did not generalize well to blind test sets, underscoring the importance of evaluating models on diverse and unseen datasets.\n\nIn summary, our evaluation method involved a rigorous assessment using held-out and blind datasets, ROC curve analysis, Spearman correlation coefficients, and point mutation experiments. These evaluations demonstrated the models' ability to generalize and accurately predict thermostability in various scenarios.",
  "evaluation/measure": "In our evaluation, we primarily report the area under the receiver-operating characteristic (ROC) curve and the Spearman correlation coefficient as our key performance metrics.\n\nThe ROC curve is used to demonstrate the classification performance of our models, particularly for the above-70 bin, which is crucial for identifying thermostable sequences. This metric is widely used in the literature for evaluating binary classifiers and provides a comprehensive view of the model's performance across different threshold settings.\n\nThe Spearman correlation coefficient is employed to assess the correlation between the predicted and actual thermostability measurements. This metric is particularly useful for evaluating the ranking performance of our models, as it measures the monotonic relationship between the predicted and observed values. We report this coefficient for various datasets, including held-out and blind datasets, to demonstrate the generalizability of our models.\n\nAdditionally, we use t-distributed stochastic neighbor embedding (t-SNE) to visualize the embeddings learned by our models. This technique helps us understand the underlying structure of the data and the clustering behavior of our models based on different features, such as sequence-only and energetics-only.\n\nThese performance metrics provide a representative evaluation of our models' capabilities in predicting thermostability, as they cover both classification and ranking aspects. The use of ROC curves and Spearman correlation coefficients aligns with common practices in the literature, ensuring that our evaluation is comparable to other studies in the field.",
  "evaluation/comparison": "In our evaluation, we compared our methods to both publicly available models and simpler baselines to ensure a comprehensive assessment of their performance.\n\nWe evaluated several pre-trained language models, including ESM-1v, UniRep, and AntiBERTy, which were trained on diverse protein sequences and antibody sequences, respectively. These models were assessed using zero-shot predictions, where sequences more likely under the model were predicted to be more stable. We also fine-tuned these models on supervised data to predict thermostability more accurately.\n\nIn addition to these advanced models, we compared our methods to simpler baselines. For instance, we used sequence-only models and energetics-only models to understand the contribution of each feature type to thermostability prediction. The sequence-only models were fine-tuned and supervised, allowing us to infer the underlying experimental origin of the sequences, which sometimes skewed predictions. On the other hand, the energetics-only models, which relied on residue-residue interaction energies, showed better generalization to blind datasets.\n\nWe also constructed a supervised convolutional network architecture that incorporated both sequence and energetic features. This model used a two-dimensional i-j residue energy matrix to provide a localized structural context. The performance of this model was evaluated on held-out and blind datasets, demonstrating its ability to extract generalizable information about thermostability.\n\nOverall, our comparisons to publicly available methods and simpler baselines provided a robust evaluation of our approaches, highlighting their strengths and areas for improvement.",
  "evaluation/confidence": "The evaluation of our models involved several performance metrics, but specific confidence intervals for these metrics were not explicitly detailed. However, the statistical significance of our results was assessed through various tests and comparisons.\n\nFor instance, we compared the performance of our models on different datasets, including held-out sets and blind test sets. The energetics-only model showed better correlation on blind datasets compared to sequence-only and energetics + sequences models. This suggests that the energetics-only model generalizes better to unseen data, which is a strong indicator of its robustness and reliability.\n\nAdditionally, we performed tests to demonstrate the significance of learned representations from supervised data. For example, we randomly initialized the weights in the SCNN for the classification task and found that it could not distinguish sequences based on thermostability. This control experiment highlights the importance of the learned representations in our models.\n\nFurthermore, we evaluated the ability of our models to discriminate between thermostable and thermally degenerate mutations. The SCNN model correctly identified the residue position for 18 out of 20 mutations, although it predicted different amino-acid mutations as most thermostable. This level of accuracy, even when the model is not perfectly aligned with experimental data, indicates that our models have learned meaningful patterns related to thermostability.\n\nIn summary, while specific confidence intervals for performance metrics were not provided, the statistical significance of our results was demonstrated through various comparative tests and experiments. These tests show that our models, particularly the energetics-only model, are superior to baselines and other models in predicting thermostability.",
  "evaluation/availability": "Not enough information is available."
}