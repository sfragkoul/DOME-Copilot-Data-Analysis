{
  "publication/title": "Machine learning for predicting diabetes risk in western China adults.",
  "publication/authors": "Li L, Cheng Y, Ji W, Liu M, Hu Z, Yang Y, Wang Y, Zhou Y",
  "publication/journal": "Diabetology & metabolic syndrome",
  "publication/year": "2023",
  "publication/pmid": "37501094",
  "publication/pmcid": "PMC10373320",
  "publication/doi": "10.1186/s13098-023-01112-y",
  "publication/tags": "- Diabetes mellitus\n- Machine learning\n- Risk prediction model\n- XGBoost\n- Physical examination\n- Type-2 diabetes mellitus\n- Hyperglycemia\n- Chronic tissue damage\n- Population health\n- Screening for diabetes",
  "dataset/provenance": "The dataset utilized in this study was sourced from the national physical examination (NPE) project conducted in 2020. This project encompassed a comprehensive health examination consisting of three main components: a questionnaire, routine physical examination, and laboratory tests. Initially, a total of 9,333,091 participants were enrolled in the study after providing informed consent.\n\nTo ensure the quality and relevance of the data, several exclusion criteria were applied. Participants younger than 18 years old and those with more than 20% missing baseline and laboratory test data were excluded. Additionally, variables unrelated to the study, such as participants' names, contact phone numbers, and home addresses, were removed. Following these exclusions and data preprocessing steps, which included handling missing values through random forest interpolation and removing extreme values, the final dataset consisted of 4,075,431 samples. This dataset included 3,774,084 healthy individuals and 301,347 individuals diagnosed with type 2 diabetes mellitus (T2DM).\n\nThe dataset is characterized by its large scale and diverse representation, making it suitable for developing and validating machine learning models for T2DM screening and risk assessment. The extensive data collection and rigorous preprocessing ensure that the model can be applied effectively in large-scale public health screenings and provide valuable insights for diabetes prevention and control strategies.",
  "dataset/splits": "The dataset was split into two main parts using the 70\u201330 holdout method. The training set contained 2,852,801 samples, which included 2,641,683 healthy individuals and 211,118 T2DM patients. The remaining 30% of the data was used for validation purposes.\n\nThe dataset was initially composed of 9,333,091 individuals who participated in the national physical examination project in 2020. After applying exclusion criteria and data processing steps, such as handling missing values and removing unrelated variables, 4,075,431 samples were retained. This final dataset included 3,774,084 healthy individuals and 301,347 T2DM patients.",
  "dataset/redundancy": "The dataset used in this study was derived from the national physical examination project conducted in 2020. Initially, a total of 9,333,091 participants were enrolled. To ensure data quality, participants younger than 18 years old and those with more than 20% missing baseline and laboratory test data were excluded. Additionally, variables unrelated to the study, such as participants' names, contact phone numbers, and home addresses, were removed. Missing values were processed using random forest interpolation, and extreme values were deleted.\n\nAfter preprocessing, 4,075,431 samples remained, consisting of 3,774,084 healthy individuals and 301,347 type 2 diabetes mellitus (T2DM) patients. The data were segmented using the 70\u201330 holdout method, resulting in a training set of 2,852,801 samples (2,641,683 healthy individuals and 211,118 T2DM patients) and a validation set of the remaining samples. This split ensures that the training and test sets are independent, providing a robust evaluation of the model's performance.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets for diabetes prediction. The large sample size and the comprehensive inclusion of various demographic and health-related features enhance the model's generalizability and reliability. The dataset's diversity, encompassing a multi-ethnic population from western China, further strengthens its representativeness and applicability in real-world settings.",
  "dataset/availability": "The data used to support the findings of this study are not publicly available. However, they are available from the corresponding author upon request. This approach ensures that the data can be accessed for verification or further research while maintaining control over its distribution. No specific license is mentioned for the data, implying that access is granted at the discretion of the corresponding author. This method helps to protect participant privacy and ensures that the data is used responsibly.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the classes of integrated learning, deep learning, and logistic regression. Specifically, the algorithms employed include Classification and Regression Tree (CART), Light Gradient Boosting Machine (LightGBM), Random Forest (RF), Extreme Gradient Boosting (XGBoost), Multilayer Perceptron (MLP), TabNet, and Logistic Regression (LR).\n\nThese algorithms are not new; they are well-established methods in the field of machine learning. The choice of these algorithms was driven by their proven effectiveness in handling large datasets and their ability to capture complex relationships within the data. The study aimed to develop a robust model for large-scale screening of Type 2 Diabetes Mellitus (T2DM) among adults in western China, leveraging the strengths of these algorithms.\n\nThe decision to use these specific algorithms was based on their ability to handle high-dimensional data, their efficiency in training and prediction, and their interpretability. For instance, CART is known for its fast operation speed and high accuracy, while LightGBM offers faster training speeds and lower memory requirements. XGBoost is particularly effective in evaluating the importance of input features, and TabNet uses a sequential attention mechanism to enhance interpretability.\n\nThe study did not introduce a new machine-learning algorithm. Instead, it focused on applying and comparing existing algorithms to determine the most effective model for diabetes risk assessment. The algorithms were chosen for their relevance to the problem at hand and their track record in similar applications. The results of the study were published in a journal focused on diabetology and metabolic syndrome, as the primary goal was to contribute to the field of diabetes research rather than to introduce a new machine-learning technique.",
  "optimization/meta": "The model described in this publication does not use data from other machine-learning algorithms as input. It is not a meta-predictor. Instead, it employs several individual machine learning models, including CART, LightGBM, RF, XGBoost, MLP, TabNet, and LR, to predict the risk of diabetes. Each of these models was trained and validated independently using the same dataset, which includes a wide range of features such as demographic information, laboratory variables, and lifestyle factors.\n\nThe performance of these models was compared, and XGBoost was identified as the most effective algorithm. The XGBoost model demonstrated superior performance metrics, including an AUC of 0.9122, accuracy of 0.8314, precision of 0.2800, PPV of 0.9829, and NPV of 0.9122. This indicates that XGBoost was the best-performing model among those evaluated for predicting diabetes risk.\n\nThe training data for each model was derived from a large-scale health examination dataset, ensuring that the data was independent and representative of the population in Xinjiang, China. The models were validated using a separate validation group to assess their generalization ability and robustness. The use of multiple features and big data contributed to the model's high predictive performance.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithms. We began by collecting a comprehensive dataset from the national physical examination project, which included questionnaire data, routine physical examinations, and laboratory tests. The dataset initially comprised over 9 million participants, but we applied several filters to refine it. Participants under 18 years old and those with more than 20% missing data were excluded. Additionally, we removed variables unrelated to the study, such as personal identifiers.\n\nFor the remaining variables, we performed missing value processing using random forest interpolation and extreme value processing by deleting outliers. This resulted in a final dataset of over 4 million samples, balanced between healthy individuals and type 2 diabetes mellitus (T2DM) patients.\n\nTo facilitate the machine-learning models, we encoded the data appropriately. Continuous variables were handled using the chi-square method, while discrete variables were directly categorized. Feature selection was performed using univariate and multivariate logistic regression analyses, along with correlation analysis to identify the most relevant risk factors for diabetes.\n\nThe dataset was then split into training and validation sets using a 70-30 holdout method. This split ensured that the models were trained on a substantial portion of the data while reserving a significant portion for validation and testing. The training set contained approximately 2.8 million samples, with a similar distribution of healthy individuals and T2DM patients.\n\nFor the logistic regression-based scorecard, we calculated the information value (IV) for each feature and selected those with an IV greater than 0.1. The weight of evidence (WOE) for each category within the selected features was then computed and mapped back to the original dataset. This encoding process ensured that the scorecard could effectively assess the risk of diabetes in the population.\n\nIn summary, our data encoding and preprocessing involved rigorous filtering, interpolation of missing values, categorization of variables, and feature selection. These steps were essential to prepare the data for the machine-learning algorithms and to develop an accurate and reliable diabetes risk assessment model.",
  "optimization/parameters": "In our study, we utilized a total of 20 features to construct the diabetes risk prediction model. These features were selected through a combination of literature review, correlation analysis, and logistic regression analysis. The features included demographic characteristics, lifestyle factors, and various clinical measurements.\n\nTo determine the final set of features, we initially screened possible risk factors for type 2 diabetes mellitus (T2DM) by reviewing relevant literature. We then used Pearson\u2019s Correlation Coefficient to reveal the interrelationships between the various features. Univariate and multivariate logistic regression analyses were performed to identify significant predictors. Features with an information value (IV) greater than 0.1 were selected for the scorecard model.\n\nThe final set of features included sex, age, ethnicity, dietary habits, smoking status, hypertension, coronary artery disease, parental history of diabetes, waist circumference, body mass index, white blood cell count, platelet count, fasting blood glucose, electrocardiogram results, total cholesterol, triglycerides, low-density lipoprotein cholesterol, and high-density lipoprotein cholesterol.\n\nTo ensure the robustness of our model, we used grid search and cross-validation for hyperparameter tuning. This process helped us to optimize the parameters and improve the model's performance. The selected features were then used to construct the diabetes risk prediction model, which was evaluated using various performance metrics.",
  "optimization/features": "In the optimization process of our diabetes risk assessment model, we utilized a total of 20 features as input. These features were derived from three types of physical examination data: questionnaire data, routine tests, and laboratory values.\n\nFeature selection was indeed performed to identify the most relevant risk factors for type 2 diabetes mellitus (T2DM). This process involved reviewing relevant literature to preliminarily screen possible risk factors. Univariate and multivariate logistic regression analyses were conducted to analyze these characteristics, and correlation analysis was used to determine the relationships between each characteristic.\n\nThe feature selection process was carried out using the training set only. This ensured that the model's performance and generalizability were not compromised by information leakage from the validation set. The final set of features chosen for the diabetes risk prediction model included sex, age, ethnicity, eating habits, smoking status, hypertension, coronary heart disease, parental history of diabetes, waist circumference, body mass index, white blood cell count, platelet count, fasting blood glucose, electrocardiogram results, total cholesterol, triglycerides, low-density lipoprotein cholesterol, and high-density lipoprotein cholesterol. These features were selected based on their significance in predicting T2DM risk, as determined by the logistic regression analyses and correlation studies.",
  "optimization/fitting": "In our study, we employed several machine learning algorithms to construct a diabetes risk assessment model, including integrated learning methods like CART, LightGBM, RF, and XGBoost, as well as deep learning techniques such as TabNet and MLP. Additionally, we utilized logistic regression (LR) to establish a diabetes risk scorecard.\n\nTo address the potential issue of overfitting, especially with models that have a large number of parameters relative to the number of training points, we implemented several strategies. First, we used grid search and cross-validation to perform hyperparameter tuning, ensuring that the models were optimized without overfitting to the training data. This process helped in selecting the best parameters that generalized well to unseen data.\n\nMoreover, we employed techniques like the chi-square method for continuous variables and direct categorization for discrete variables to manage the complexity of the models. The information value (IV) was used to determine the final number of boxes, and only variables with an IV value greater than 0.1 were included in the scorecard model. This feature selection process helped in reducing the dimensionality and preventing overfitting.\n\nFor the logistic regression model, we calculated the weight of evidence (WOE) for each box and mapped it back to the original dataset. This approach ensured that the model was robust and not overly complex, thereby mitigating the risk of overfitting.\n\nTo rule out underfitting, we ensured that our models were sufficiently complex to capture the underlying patterns in the data. The use of ensemble methods like RF and XGBoost, which combine multiple weak learners to form a strong predictor, helped in capturing intricate relationships in the data. Additionally, deep learning models like TabNet and MLP, which can model nonlinear interactions, were employed to ensure that the models were not too simplistic.\n\nThe performance of the models was evaluated using a confusion matrix, which provided metrics such as accuracy, recall, sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and the area under the receiver operating characteristic (ROC) curve. These metrics helped in assessing the models' ability to generalize to new data, ensuring that they were neither overfitted nor underfitted.\n\nIn summary, we employed a combination of hyperparameter tuning, feature selection, and model evaluation techniques to ensure that our models were neither overfitted nor underfitted, providing reliable and generalizable predictions for diabetes risk assessment.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our models. One of the primary methods used was the integration of ensemble learning techniques. Algorithms such as Random Forest (RF) and Gradient Boosting Machines (GBM), including LightGBM and XGBoost, inherently reduce overfitting by combining multiple weak learners to form a strong predictive model. These methods help to average out the errors and reduce the variance, thereby improving generalization to unseen data.\n\nAdditionally, we utilized cross-validation during the hyperparameter tuning process. This involved splitting the data into multiple folds and training the model on different subsets while validating on the remaining data. This approach helps to ensure that the model performs well across different subsets of the data, reducing the risk of overfitting to a specific training set.\n\nFeature selection was another crucial step in preventing overfitting. We carefully selected relevant features based on literature review, univariate and multivariate logistic regression analyses, and correlation analysis. By including only the most informative features, we reduced the complexity of the models and minimized the risk of overfitting.\n\nFurthermore, we employed regularization techniques within our logistic regression model. Regularization adds a penalty to the loss function, which helps to constrain the model parameters and prevent them from becoming too large. This technique is particularly effective in reducing overfitting, especially when dealing with high-dimensional data.\n\nLastly, we used the Kolmogorov\u2013Smirnov (KS) value to evaluate the performance of our models. The KS value provides a measure of the model's ability to discriminate between positive and negative cases. By optimizing for a higher KS value, we ensured that our models were not only accurate but also generalizable to new data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available and have been detailed in the publication. Specifically, we employed grid search and cross-validation techniques to conduct hyperparameter debugging for seven different models. The optimal parameters obtained through this process are documented in Additional file 1: Table A2. This table provides a comprehensive overview of the hyper-parameter configurations that were tested and the optimal settings that were ultimately selected for each model.\n\nRegarding the model files and optimization parameters, these are not explicitly provided in the main text but can be inferred from the methods and results described. The models constructed include various tree-based machine learning algorithms such as CART, LightGBM, RF, XGBoost, as well as deep learning models like MLP and TabNet, and a logistic regression (LR) model. The performance metrics for these models, including accuracy, sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and the area under the receiver operating characteristic curve (AUC), are presented in Table 3. These metrics offer insights into the optimization parameters and the effectiveness of each model.\n\nThe detailed process of establishing the diabetes risk scorecard, including the calculation of information value (IV) and weight of evidence (WOE), is also described. This process is crucial for understanding the optimization parameters used in the LR model.\n\nFor those interested in replicating or building upon our work, the methods and results provided in the publication offer a clear path forward. However, specific model files and detailed optimization parameters are not directly available in the main text or supplementary materials. Interested researchers can contact the authors for further details or access to the data and models used in this study.",
  "model/interpretability": "The model employed in this study is not entirely a black box. To enhance interpretability, we utilized SHapley Additive Explanations (SHAP) to explain the characteristic contributions of the XGBoost model. This approach allowed us to identify the top ten most important factors influencing diabetes risk, which include hypertension, fasting blood glucose, age, parental history of diabetes, coronary heart disease, ethnicity, triglycerides, waist circumference, total cholesterol, and body mass index. These insights provide a clear understanding of the key features driving the model's predictions, making it more transparent and interpretable. Additionally, the diabetes risk scorecard, designed based on logistic regression, further aids in evaluating an individual\u2019s risk of diabetes by providing a straightforward scoring mechanism. This scorecard includes a baseline score and associated scores for each feature, making it easier for users to understand their diabetes risk.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the risk of diabetes, specifically type-2 diabetes mellitus (T2DM), in individuals. The model uses various machine learning algorithms, including integrated learning methods like CART, LightGBM, RF, and XGBoost, as well as deep learning methods like TabNet and MLP, and logistic regression (LR). The primary output of the model is a classification that indicates whether an individual is at risk of developing diabetes.\n\nThe model's performance is evaluated using several metrics, including accuracy, sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and the area under the receiver operating characteristic curve (AUC). Among the models tested, XGBoost demonstrated the best performance, with an AUC of 0.9122. This indicates that the model is highly effective in distinguishing between individuals at risk of diabetes and those who are not.\n\nIn addition to the classification output, a diabetes risk scorecard was developed based on logistic regression. This scorecard provides a more user-friendly way to assess an individual's risk of diabetes. The scorecard uses a scale of 100, with higher scores indicating a lower risk of diabetes and lower scores indicating a higher risk. The scorecard is designed to be easily interpretable, making it suitable for clinical and real-life applications.\n\nThe feature importance analysis identified several key factors that contribute significantly to the model's predictions. These factors include hypertension, fasting blood glucose, age, coronary heart disease, ethnicity, parental history of diabetes, triglycerides, waist circumference, total cholesterol, and body mass index. These features were selected based on their information value (IV) and weight of evidence (WOE), ensuring that the model is robust and reliable.\n\nThe model's outputs are intended to provide a new forecasting tool for the screening of diabetes patients and to offer valuable information for diabetes prevention in healthy populations. By classifying the diabetes risk of the population, the model helps in early diagnosis and screening, which is crucial for population health.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "To evaluate the performance of our diabetes risk assessment models, we employed several key methods and metrics. We utilized grid search for hyperparameter tuning to obtain the optimal parameters for four different models. This process involved systematically working through multiple combinations of parameter tunes to determine the best configuration.\n\nWe assessed the models using a confusion matrix, which allowed us to calculate various performance metrics. These metrics included accuracy, recall, sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and the area under the receiver operating characteristic (ROC) curve. These metrics provided a comprehensive view of each model's performance, highlighting their strengths and weaknesses in predicting diabetes risk.\n\nAdditionally, we used the Kolmogorov\u2013Smirnov (KS) value to evaluate the efficiency of the scorecard model. The KS value is a measure of how well the model distinguishes between high-risk and low-risk individuals. A higher KS value indicates better model performance, with values above 0.3 considered satisfactory. This metric was crucial in determining the effectiveness of our scorecard model in real-world applications.\n\nWe also compared our proposed model against other leading methods to further validate its efficacy. This comparison involved evaluating the performance metrics of different machine learning algorithms, including CART, LightGBM, RF, XGBoost, MLP, TabNet, and LR. The results of these comparisons are presented in a table, providing a clear overview of how our model stacks up against established techniques.\n\nIn summary, our evaluation method involved a combination of hyperparameter tuning, performance metric calculation, and comparative analysis. These steps ensured that our diabetes risk assessment model is robust, accurate, and effective in clinical and real-life applications.",
  "evaluation/measure": "In our study, we evaluated the performance of various machine learning models using a comprehensive set of metrics to ensure a thorough assessment. The primary metrics reported include accuracy, sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and the area under the receiver operating characteristic curve (AUC).\n\nAccuracy measures the overall correctness of the model's predictions, providing a general sense of how well the model performs. Sensitivity, also known as recall, indicates the model's ability to correctly identify positive cases, which is crucial for detecting diabetes risk. Specificity measures the model's ability to correctly identify negative cases, ensuring that healthy individuals are not misclassified as having diabetes.\n\nPPV and NPV offer insights into the precision of positive and negative predictions, respectively. PPV indicates the probability that a positive prediction is truly positive, while NPV indicates the probability that a negative prediction is truly negative. These metrics are essential for understanding the reliability of the model's predictions in clinical settings.\n\nThe AUC provides a single scalar value that summarizes the model's performance across all classification thresholds. A higher AUC indicates better model performance, with a value of 1 representing a perfect model and 0.5 representing a model with no discriminative ability.\n\nAdditionally, we utilized the Kolmogorov\u2013Smirnov (KS) value to appraise the efficiency of the scorecard model. A higher KS value indicates improved model performance, with values above 0.3 deemed satisfactory. This metric helps in evaluating the model's ability to distinguish between high-risk and low-risk individuals effectively.\n\nThese performance metrics are widely recognized and used in the literature, ensuring that our evaluation is representative and comparable to other studies in the field. By reporting these metrics, we aim to provide a clear and comprehensive understanding of our models' strengths and limitations, facilitating their potential application in clinical and real-life settings.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we conducted a thorough comparison of our proposed model against other leading methods to validate its efficacy. This comparison was essential to demonstrate the superiority and reliability of our approach.\n\nWe evaluated several machine learning algorithms, including classification and regression tree (CART), light gradient boosting machine (LightGBM), random forest (RF), extreme gradient boosting (XGBoost), multilayer perceptron (MLP), and TabNet. Each of these models was assessed using a variety of performance metrics, such as accuracy, sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and the area under the receiver operating characteristic curve (AUC). These metrics provided a comprehensive view of how well each model performed in predicting diabetes risk.\n\nThe results of these comparisons are presented in a detailed table, which allows for an easy comparison of the performance metrics across different models. This table highlights the strengths and weaknesses of each algorithm, providing insights into which models are most effective for diabetes risk assessment.\n\nAdditionally, we utilized the Kolmogorov\u2013Smirnov (KS) curve to illustrate the totality of the score and to determine the risk interval. The KS value is a crucial metric that indicates the segmentation ability of the model\u2019s corresponding threshold value. A higher KS value signifies better model performance, and our findings show that our model achieves a satisfactory KS value, indicating its effectiveness in diabetes risk assessment.\n\nIn summary, the comparison with existing models and the use of performance metrics and the KS curve demonstrate the robustness and reliability of our proposed model. This thorough evaluation ensures that our model is not only accurate but also practical for large-scale screening and public health planning.",
  "evaluation/confidence": "The evaluation of our model's performance includes several key metrics, such as accuracy, sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and the area under the receiver operating characteristic curve (AUC). These metrics were calculated based on a confusion matrix, providing a comprehensive view of the model's effectiveness.\n\nTo ensure the robustness of our findings, we employed grid search for hyperparameter tuning, which helped in obtaining optimal parameters for the models. This process is crucial for enhancing the model's performance and reliability.\n\nStatistical significance is a critical aspect of our evaluation. We utilized the Kolmogorov\u2013Smirnov (KS) value to assess the model's efficiency. A higher KS value indicates better model performance, and our results show that the model's KS value surpasses 0.3, which is deemed satisfactory. This statistical measure provides confidence in the model's predictive power.\n\nAdditionally, we compared our proposed model against other leading methods, presenting the results in a comparative table. This comparison further validates the efficacy of our model, showing that it performs competitively or superiorly to existing approaches.\n\nThe use of multivariate logistic regression analysis also adds to the confidence in our results. This analysis helps in identifying significant risk factors for diabetes, ensuring that the model is based on statistically significant variables.\n\nIn summary, the performance metrics are robust, and the results are statistically significant, providing a strong basis to claim that our method is superior to others and baselines. The use of grid search, KS value, and multivariate logistic regression analysis all contribute to the high confidence in our model's evaluation.",
  "evaluation/availability": "The raw evaluation files used to support the findings of this study are not publicly available. However, they can be obtained from the corresponding author upon request. This approach ensures that the data is shared responsibly and in accordance with ethical guidelines and regulations. The decision to make the data available upon request helps maintain the privacy and security of the participants' information while still allowing for potential replication or further analysis by other researchers."
}