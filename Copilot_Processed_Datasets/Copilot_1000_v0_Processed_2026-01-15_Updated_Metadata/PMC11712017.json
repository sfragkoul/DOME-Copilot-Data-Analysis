{
  "publication/title": "Validating Machine Learning Models Against the Saline Test Gold Standard for Primary Aldosteronism Diagnosis.",
  "publication/authors": "Liu JH, Huang WC, Hu J, Hong N, Rhee Y, Li Q, Chen CM, Chueh JS, Lin YH, Wu VC",
  "publication/journal": "JACC. Asia",
  "publication/year": "2024",
  "publication/pmid": "39802987",
  "publication/pmcid": "PMC11712017",
  "publication/doi": "10.1016/j.jacasi.2024.09.010",
  "publication/tags": "- aldosteronism\n- Deep Neural Network\n- feature extraction\n- feature selection\n- machine learning\n- Random Forest\n- TAIPAI\n- XGBoost\n- primary aldosteronism\n- hypertension",
  "dataset/provenance": "The dataset used in our study originates from multiple sources, including medical centers and regional hospitals across different cities. Specifically, a holistic database was constructed for quality assurance in two medical centers and five regional hospitals. This dataset includes participants with hypertension who underwent screening and confirmation tests for primary aldosteronism (PA).\n\nThe study involved a significant number of data points, with a total of 643 hypertensive patients enrolled in the CONPASS (Chongqing Primary Aldosteronism Study) database at the First Affiliated Hospital of Chongqing. Additionally, the dataset includes participants from other centers, contributing to a large and diverse cohort.\n\nThe data used in this study includes baseline information such as age, gender, body weight, height, blood pressure, past medical history, and biochemical profiles for PA screening, confirmation, and subtype identification. This comprehensive dataset allows for robust analysis and model training, ensuring that the selected features are relevant and the resulting machine learning models are accurate and robust for predicting different diagnoses of hypertension.\n\nNot applicable.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "The datasets used in our study were sourced from multiple international sites, including South Korea and the CONPASS data set. These datasets were utilized to train and validate our machine learning models, ensuring a robust and generalizable performance.\n\nThe training and test sets were designed to be independent. This independence was enforced through careful splitting of the data, ensuring that the models were trained on one subset and tested on a completely separate subset. This approach helps to prevent overfitting and provides a more accurate assessment of the model's performance in real-world scenarios.\n\nThe distribution of our datasets compares favorably to previously published machine learning datasets in the field. Our study benefits from a large sample size, which includes data from multiple centers, adding an extra layer of complexity and variability. This multicenter aspect is crucial for enhancing the generalizability of our models. The inclusion of diverse data from different international sites ensures that our models can perform well across various populations, addressing the limitations often seen in single-center studies.\n\nAdditionally, our models were validated using external datasets, such as the South Korean and CONPASS datasets, which further ensures the independence and robustness of our results. This external validation is a key strength of our study, as it demonstrates the models' ability to generalize beyond the training data. The performance metrics, such as accuracy and AUROC, were consistently high across these external validations, indicating the models' reliability and effectiveness.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in our study include Random Forest, XGBoost, and deep learning techniques. These are well-established classes of algorithms commonly used in predictive modeling and feature selection.\n\nThe algorithms employed are not new; they have been extensively used and validated in various domains, including healthcare. Random Forest is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable. Deep learning techniques involve neural networks with many layers, capable of learning complex representations of data.\n\nThese algorithms were chosen for their robustness and ability to handle large datasets with numerous features, which is crucial for our study involving hypertensive East-Asian patients. The focus of our publication is on the application of these algorithms to predict primary aldosteronism, rather than the development of new machine-learning algorithms. Therefore, the algorithms were not published in a machine-learning journal but in a healthcare-focused journal to highlight their practical application in medical diagnostics.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. Instead, it employs several machine-learning methods independently to predict primary aldosteronism. The methods used include Deep Neural Networks (DNN), Random Forest, and XGBoost. Each of these models was trained and evaluated separately, demonstrating high performance in terms of accuracy and AUROC values. The Random Forest model, for instance, achieved an accuracy of 0.883 and an AUROC of 0.968. The DNN model had an accuracy of 0.887 and an AUROC of 0.943, while the XGBoost model showed an accuracy of 0.889 and an AUROC of 0.974. These models were developed using a systematic approach that included feature selection, model training, and performance evaluation, ensuring robust and reliable predictions. The training data for these models is independent, as each model was trained on distinct datasets from different international sites, including South Korea and Mainland China. This independence helps in validating the models' performance and generalizability across different populations.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning models. We began by handling missing data, which is indispensable for our models as they do not accept data with missing values. Initially, we adopted mean value imputation for missing numerical data, but we found that replacing missing values with a fixed value of 1 yielded a marginal yet notable improvement in model performance. This approach simplified the process and maintained consistency across varying datasets, which is particularly important for models requiring frequent updates.\n\nFor categorical features, we transformed them into one-hot encoded features. This transformation allowed us to effectively incorporate categorical data into our models. We did not standardize continuous variables in our data, focusing instead on handling missing data and converting categorical features.\n\nWe had 30 features in total, and the number of possible feature combinations was excessively large. To manage this, we used a heuristic algorithm that allowed for a limited number of combinations and specified the number of feature subsets. This algorithm generated 3,000 subsets derived from the remaining 30 features, which were then used to train and test our models for differentiating between essential hypertension (EH), unilateral primary aldosteronism (uPA), and bilateral primary aldosteronism (biPA).\n\nFeature selection was performed using Random Forest models configured with entropy criteria to evaluate the disorder. These models included 10 trees to optimize the balance between training time and model accuracy. The selected features were then used to predict the diagnosis and generate new features for constructing the prediction models: Random Forest, Deep Neural Networks (DNN), and XGBoost. Each group (EH vs uPA, EH vs biPA, and uPA vs biPA) had 50 feature subsets, resulting in a total of 150 extracted features for the prediction models. This analytical process ensured that the selected features were relevant and that the resulting machine-learning models were accurate and robust for predicting different diagnoses of hypertension.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the specific comparison being made. For each condition comparison\u2014essential hypertension (EH) vs unilateral primary aldosteronism (uPA), EH vs bilateral primary aldosteronism (biPA), and uPA vs biPA\u2014the top 50 feature subsets with the highest prediction accuracy were identified. This resulted in a total of 150 models evaluated, with each model using a different subset of features.\n\nThe selection of these feature subsets was systematic. We started with all features and then generated subsets by removing one feature at a time until we had 3,000 subsets. These subsets were derived from the remaining 30 features. The top 50 feature subsets per model comparison were chosen based on their prediction accuracy, aiming to balance accuracy and computational efficiency. Expanding beyond 50 feature subsets per model comparison did not significantly enhance accuracy and instead increased computational demands.\n\nThe analytical process involved several stages, including preprocessing, feature selection, feature extraction, prediction model construction, and a scoring function. In the preprocessing stage, missing data was handled using a constant value imputation strategy, which simplified the process and maintained consistency across varying data sets. This approach was chosen for its practical considerations in real-world deployment, where simplicity and reproducibility are crucial.\n\nThe feature extraction stage involved using the selected features to predict the diagnosis and generating new features for constructing the prediction models: Random Forest, Deep Neural Networks (DNN), and XGBoost. The final extracted features for the groups of EH vs uPA, EH vs biPA, and uPA vs biPA were determined in the prediction model construction stage. Each group had 50 feature subsets, resulting in a total of 150 extracted features for the prediction models.\n\nThe scoring function stage involved assessing the performance of the constructed prediction models using metrics such as accuracy, precision, recall, and area under the receiver-operating characteristic curve. This ensured that the selected features were relevant and that the resulting machine learning models were accurate and robust for predicting different diagnoses of hypertension.",
  "optimization/features": "In our study, we initially considered 30 features as potential inputs for our models. To manage the computational complexity, we employed a heuristic algorithm to generate a manageable number of feature subsets. This process involved starting with all features and then iteratively reducing the number of features by one until we had created 3,000 subsets. These subsets were then used to train and test our models for differentiating between essential hypertension (EH), unilateral primary aldosteronism (uPA), and bilateral primary aldosteronism (biPA).\n\nFeature selection was a crucial step in our methodology. We utilized Random Forest to identify the most informative features based on their ability to distinguish between the different conditions. The Random Forest models were configured with entropy criteria to evaluate the disorder and included 10 trees to balance training time and model accuracy. This selection process was performed using the training set only, ensuring that the feature subsets were derived independently of the test data. By focusing on the most informative features, we aimed to enhance the models' predictive performance while maintaining computational efficiency.",
  "optimization/fitting": "The fitting method employed in our study involved a systematic approach to ensure that our models were neither overfitting nor underfitting the data. We utilized Random Forest, a nonparametric method with recursive partitioning trees, which inherently avoids collinearity issues. This method was chosen because it balances the trade-off between bias and variance, making it robust against overfitting.\n\nTo address the potential for overfitting, we employed several strategies. First, we used a large and diverse dataset from multiple international sites, which helped in capturing a wide range of variability. This approach ensured that our models were generalizable and not overly tailored to a specific subset of data. Second, we performed feature selection using Random Forest, which helped in identifying the most informative features. This step reduced the dimensionality of the data, making the model simpler and less prone to overfitting. Third, we evaluated the performance of our models using cross-validation techniques, specifically 10-fold cross-validation. This method provided a reliable estimate of model performance and helped in identifying any signs of overfitting.\n\nTo rule out underfitting, we ensured that our models were complex enough to capture the underlying patterns in the data. We used an ensemble of trees in the Random Forest model, which allowed the model to learn complex relationships. Additionally, we compared the performance of our models with other predictive approaches, such as Deep Neural Networks (DNN) and XGBoost. The comparable performance of these models indicated that our Random Forest model was appropriately complex and not underfitting the data.\n\nFurthermore, we conducted external validations on different datasets, including the South Korean and CONPASS datasets. The consistent performance across these datasets provided additional evidence that our models were neither overfitting nor underfitting. The accuracy and AUROC values obtained from these validations were within acceptable ranges, further confirming the robustness of our fitting method.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our models. One of the key strategies involved using a large and diverse dataset from multiple international sites, which helped to reduce the risk of overfitting by providing a more representative sample of the population. This approach allowed our models to generalize better to new, unseen data.\n\nAdditionally, we utilized feature selection and extraction methods to identify the most important features for accurate diagnosis. By reducing the noise between features, we were able to focus on the most relevant predictors, thereby improving the model's performance and reducing the likelihood of overfitting.\n\nWe also employed cross-validation techniques, specifically 10-fold cross-validation, to evaluate the performance of our models. This method involves dividing the dataset into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. This approach helps to ensure that the model's performance is consistent and not dependent on a particular subset of the data.\n\nFurthermore, we compared the performance of different machine learning algorithms, including Random Forest, Deep Neural Networks (DNN), and XGBoost. Each of these algorithms has its own strengths and weaknesses, and by comparing their performance, we were able to select the most robust model for our purposes. The Random Forest model, in particular, demonstrated good performance with an accuracy of 0.673 and an AUROC value of 0.968, indicating its effectiveness in predicting primary aldosteronism.\n\nIn summary, our study employed a combination of techniques, including the use of a large and diverse dataset, feature selection and extraction, cross-validation, and the comparison of different machine learning algorithms, to prevent overfitting and ensure the robustness of our models.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in our study, particularly the XGBoost, Random Forest, and Deep Neural Networks (DNN), are inherently complex and can be considered as black boxes. This complexity arises from the nature of these machine learning algorithms, which often involve intricate decision-making processes that are not easily interpretable by humans.\n\nTo address this issue, we utilized the SHapley Additive exPlanations (SHAP) algorithm. SHAP is a powerful tool that helps in explaining the output of machine learning models by attributing the contribution of each feature to the prediction. This method provides a clear and intuitive way to understand which features are driving the model's decisions. For instance, SHAP can highlight that certain features, such as plasma renin activity (PRA) and potassium levels, have a significant positive or negative impact on the prediction of primary aldosteronism (PA).\n\nBy using SHAP, we were able to demystify the \"black box\" nature of our models to some extent. This approach allowed us to identify the most important features and understand their influence on the model's predictions. For example, in our study, SHAP revealed that PRA from the TAIPAI and Korean datasets had the highest specificity, indicating its crucial role in distinguishing between different conditions.\n\nIn summary, while our models are complex and can be seen as black boxes, the use of SHAP has provided valuable insights into the decision-making process. This enhances the transparency and interpretability of our models, making them more reliable and trustworthy for clinical applications.",
  "model/output": "The models developed in this study are classification models. They are designed to predict the diagnosis of primary aldosteronism (PA) and distinguish it from essential hypertension (EH). Specifically, the models classify participants into categories such as EH, unilateral primary aldosteronism (uPA), and bilateral primary aldosteronism (biPA).\n\nThe performance of these models was evaluated using metrics such as accuracy and the area under the receiver-operating characteristic curve (AUROC). For instance, the accuracy of the tested datasets was 0.887 for the Deep Neural Network (DNN) model, 0.883 for the Random Forest model, and 0.889 for the XGBoost model. The AUROC values, which indicate the models' ability to discriminate between the classes, were 0.943 for the DNN model, 0.968 for the Random Forest model, and 0.974 for the XGBoost model. These values demonstrate the models' effectiveness in classifying the different conditions.\n\nThe models were trained using feature selection and extraction methods, which enhanced their accuracy. The hierarchical structure revealed main clusters of participants, with shorter distances between EH vs uPA and EH vs biPA compared to uPA vs biPA. This indicates that the models can effectively differentiate between these conditions.\n\nAdditionally, the SHapley Additive exPlanations (SHAP) algorithm was used to explain the XGBoost model predictions, highlighting the features that positively or negatively affect the predictive model. This provides insights into which features are important for the models' classifications.\n\nIn summary, the models are classification models that predict the diagnosis of PA and distinguish it from EH, with high accuracy and AUROC values, indicating their effectiveness in classifying the different conditions.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models and algorithms used in this study is not publicly released. However, the statistical analyses were performed using widely available software, including IBM SPSS Statistics for Windows version 22.0, R software version 3.2.2, and Python version 3.7.10. For the machine learning components, Python was utilized, specifically the scikit-learn package, to calculate the receiver-operating characteristic (ROC) curve and obtain the area under the curve (AUC) function. This package is open-source and can be accessed through standard Python repositories. The models were developed using these tools, but the specific implementations and trained models are not provided for public use.",
  "evaluation/method": "The evaluation of our models involved several rigorous methods to ensure their robustness and accuracy. We employed 10-fold cross-validation to obtain the average accuracy and model performance metrics, such as the area under the receiver-operating characteristic curve (AUROC), across various subgroups, including unilateral primary aldosteronism (uPA) and bilateral primary aldosteronism (biPA). This approach helped in assessing the models' performance consistently across different data splits.\n\nAdditionally, we validated our models using external datasets, specifically the South Korean and CONPASS datasets. The accuracy and AUROC values for these datasets were reported, demonstrating the models' generalizability beyond the training data. For instance, the South Korean dataset showed an accuracy of 0.667 and an AUROC of 0.768, while the CONPASS dataset had an accuracy of 0.629 and an AUROC of 0.775. These results indicate that while the models performed well, there is still room for improvement, particularly by including more diverse datasets in the training process.\n\nWe also compared our models' performance with traditional clinical prediction models and other machine learning techniques. For example, our models outperformed six clinical prediction models for unilateral primary aldosteronism, which had C-statistics ranging from 0.60 to 0.72. This comparison highlights the superior discriminative ability of our models.\n\nFurthermore, we used the SHapley Additive exPlanations (SHAP) algorithm to explain the predictions made by our models, particularly the XGBoost model. This helped in identifying the features that positively or negatively affect the predictive model, providing insights into the model's decision-making process.\n\nIn summary, the evaluation methods included cross-validation, external dataset validation, comparison with existing models, and the use of explainable AI techniques. These approaches collectively ensured that our models are reliable, accurate, and generalizable for predicting primary aldosteronism.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our machine learning models in predicting primary aldosteronism (PA). The primary metrics reported include accuracy, the area under the receiver-operating characteristic curve (AUROC), and confidence intervals (CI) for these metrics.\n\nAccuracy is reported for various models and datasets. For instance, the accuracy of the South Korean dataset is 0.667 with a 95% CI of 0.548-0.786, and the AUROC is 0.768 with a 95% CI of 0.648-0.888. For the CONPASS dataset, the accuracy is 0.629 with a 95% CI of 0.592-0.666, and the AUROC is 0.775 with a 95% CI of 0.739-0.811. These metrics indicate the models' ability to correctly classify instances of PA.\n\nFor the models predicting PA, the accuracy of the tested datasets is reported as 0.887 (95% CI: 0.8646-0.9094) for the Deep Neural Network (DNN) model, 0.883 (95% CI: 0.8603-0.9057) for the Random Forest model, and 0.889 (95% CI: 0.8668-0.9112) for the XGBoost model. The AUROC values are also provided, with the Random Forest model achieving an AUROC of 0.968 (95% CI: 0.955-0.981), the DNN model achieving an AUROC of 0.943 (95% CI: 0.926-0.960), and the XGBoost model achieving an AUROC of 0.974 (95% CI: 0.963-0.985).\n\nThese metrics are representative of the literature, as they are commonly used in evaluating machine learning models for medical diagnoses. The inclusion of AUROC is particularly important as it provides a comprehensive measure of the model's ability to discriminate between different classes, which is crucial in medical diagnostics. The reported confidence intervals further enhance the reliability of these metrics by providing a range within which the true performance measures are likely to fall.\n\nAdditionally, we compared our models' performance with other studies. For example, our models showed better performance with larger AUC values compared to studies like Buffolo et al, which had lower AUC values for similar predictions. This comparison highlights the robustness and effectiveness of our models in predicting PA.\n\nIn summary, the performance metrics reported in our study are comprehensive and representative of the standards in the field. They provide a clear and reliable assessment of our models' ability to predict primary aldosteronism accurately.",
  "evaluation/comparison": "In our study, we compared our machine learning models to simpler baselines and publicly available methods to evaluate their performance. We initially adopted mean value imputation for handling missing numerical data, which is a standard approach. However, we found that replacing missing values with a fixed value of 1 yielded a marginal yet notable improvement in model performance, suggesting that this approach balances computational efficiency with model accuracy.\n\nWe also compared our models to other machine learning techniques used in similar studies. For instance, Kaneko et al. applied various machine learning algorithms to a relatively small sample size of 253 participants, achieving high accuracy and AUC values. However, the generalizability of their results is questionable due to the small sample size and single-center data. In contrast, our study utilized a larger and more diverse dataset from multiple centers, which enhances the robustness and applicability of our findings.\n\nOur models, including Random Forest, Deep Neural Networks (DNN), and XGBoost, were evaluated using metrics such as accuracy, precision, recall, and area under the receiver-operating characteristic curve (AUROC). The XGBoost model achieved the highest accuracy and AUROC, followed closely by Random Forest and DNN. These results indicate that our feature selection and extraction methods significantly enhance diagnostic accuracy, making our machine learning models a comparable alternative to traditional diagnostic tests for primary aldosteronism.\n\nAdditionally, we compared our models to those reported in other studies. For example, Sam et al. conducted external validations of clinical prediction models for unilateral primary aldosteronism, finding that the range of C-statistics for ROC curves varied from 0.60 to 0.72. Our models demonstrated superior discriminative ability, with AUROC values exceeding 0.94, highlighting the effectiveness of our approach.\n\nIn summary, our comparison to simpler baselines and publicly available methods shows that our machine learning models offer a reliable and accurate tool for diagnosing primary aldosteronism. The use of a large and diverse dataset, along with advanced feature selection and extraction techniques, contributes to the robustness and generalizability of our results.",
  "evaluation/confidence": "The evaluation of our models includes performance metrics with confidence intervals, providing a clear indication of the reliability and variability of our results. For instance, the accuracy and AUROC values for our external validation datasets, such as the South Korean and CONPASS datasets, are reported with 95% confidence intervals. This approach ensures that the performance metrics are statistically significant and that our claims of superiority over other methods and baselines are well-supported.\n\nIn our study, we also compared our models' performance with traditional methods and other machine learning models. The confidence intervals for metrics like accuracy and AUROC demonstrate that our models not only perform well but also consistently outperform other approaches. For example, the AUROC values for our Random Forest, DNN, and XGBoost models are reported with confidence intervals, showing that these values are statistically significant and indicative of superior performance.\n\nAdditionally, we conducted extensive validation across multiple datasets and sites, further reinforcing the statistical significance of our results. The use of confidence intervals across various performance metrics ensures that our findings are robust and that the observed improvements in diagnostic accuracy are not due to chance. This rigorous evaluation process underscores the reliability and generalizability of our models, making a strong case for their superiority in predicting primary aldosteronism.",
  "evaluation/availability": "Not enough information is available."
}