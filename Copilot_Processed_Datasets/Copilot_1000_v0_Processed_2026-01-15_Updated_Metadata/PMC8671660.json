{
  "publication/title": "Prediction of Postoperative Delirium in Geriatric Hip Fracture Patients: A Clinical Prediction Model Using Machine Learning Algorithms.",
  "publication/authors": "Oosterhoff JHF, Karhade AV, Oberai T, Franco-Garcia E, Doornberg JN, Schwab JH",
  "publication/journal": "Geriatric orthopaedic surgery & rehabilitation",
  "publication/year": "2021",
  "publication/pmid": "34925951",
  "publication/pmcid": "PMC8671660",
  "publication/doi": "10.1177/21514593211062277",
  "publication/tags": "- Machine Learning\n- Postoperative Delirium\n- Hip Fracture Surgery\n- Predictive Modeling\n- Geriatric Patients\n- Clinical Prediction\n- Random Forest\n- Logistic Regression\n- Stochastic Gradient Boosting\n- Support Vector Machine\n- Neural Network\n- Preoperative Factors\n- Delirium Prediction\n- Elderly Care\n- Surgical Outcomes\n- Medical Comorbidities\n- Data Imputation\n- Model Validation\n- Decision Curve Analysis\n- Calibration Metrics",
  "dataset/provenance": "The dataset utilized in this study was sourced from the American College of Surgeons (ACS) and American Association of Orthopaedic Surgeons (AAOS) National Surgical Quality Improvement Program (NSQIP) Hip Fracture Procedure Targeted files. This database is extensive, collecting over 150 variables related to pre-, peri-, and postoperative conditions up to 30 days following surgery from more than 680 US hospitals. The data undergoes routine auditing, ensuring high-quality information with an inter-reviewer discrepancy rate of less than 2%.\n\nThe specific files queried for this study included data from 2016 through 2019. The dataset focused on patients older than 60 years of age who underwent surgery for femoral neck, intertrochanteric, and subtrochanteric hip fractures. The primary outcome of interest was postoperative delirium within 30 days following hip fracture surgery, as defined by the Hip Fracture Targeted ACS-NSQIP files. A chart-based method, previously validated in both surgical and non-surgical specialties, was used to determine the presence of delirium.\n\nThe dataset was split into a training set and a testing set in an 80:20 ratio. The training set consisted of 22,563 data points, while the testing set included 5,641 data points. This division allowed for the training and internal validation of several machine learning algorithms to predict postoperative delirium. The algorithms employed included Stochastic Gradient Boosting (SGM), Random Forest (RF), Support Vector Machine, Neural Network (NN), and Elastic-Net Penalized Logistic Regression (PLR). The performance of these models was assessed using metrics such as the c-statistic, calibration slope and intercept, and the Brier score.",
  "dataset/splits": "The dataset was randomly split into two subsets: a training set and a testing set. The training set consisted of 22,563 data points, while the testing set had 5,641 data points. This split was done to train and validate the machine learning algorithms used in the study. The training set was used to develop the models, and the testing set was used to evaluate their performance. Additionally, during the training phase, 10-fold cross-validation was employed. This means the training data was further divided into 10 groups, or folds, and the model was trained and validated 10 times, each time using a different fold as the validation set and the remaining folds as the training set. The results were then averaged across all 10 repetitions to ensure robust model performance.",
  "dataset/redundancy": "The dataset utilized in this study was derived from the American College of Surgeons (ACS) and American Association of Orthopaedic Surgeons (AAOS) National Surgical Quality Improvement Program (NSQIP) Hip Fracture Procedure Targeted files, spanning from 2016 through 2019. This dataset includes a comprehensive collection of over 150 variables, encompassing pre-, peri-, and postoperative data up to 30 days following surgery, from more than 680 US hospitals.\n\nTo ensure robust model training and validation, the dataset was randomly split into an 80:20 ratio, creating a training set and a testing set. This split was performed to maintain independence between the training and testing subsets, which is crucial for evaluating the model's generalizability and performance. The training set, consisting of 22,563 patients, was used to develop and optimize the machine learning models. The testing set, comprising 5,641 patients, was reserved for assessing the models' performance and ensuring that they could accurately predict outcomes on unseen data.\n\nThe random split ensures that the training and testing sets are independent, reducing the risk of overfitting and providing a more reliable estimate of the model's performance. This approach is consistent with best practices in machine learning, where the goal is to develop models that can generalize well to new, unseen data.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the field of orthopedic surgery. The large sample size and the comprehensive set of variables included in the NSQIP database provide a rich source of data for developing and validating predictive models. The rigorous data collection and auditing processes of the NSQIP ensure high-quality data, further enhancing the reliability of the models developed from this dataset.",
  "dataset/availability": "Not applicable.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are not new. They are well-established methods in the field of machine learning and have been extensively used in various applications. The algorithms employed include Stochastic Gradient Boosting, Random Forest, Support Vector Machine, Neural Network, and Elastic-Net Penalized Logistic Regression. These algorithms were chosen based on prior research and their known effectiveness in predictive modeling.\n\nThe decision to use these specific algorithms was driven by their proven track record in handling complex datasets and their ability to provide robust predictions. The study aimed to develop a clinical prediction model for postoperative delirium in geriatric hip fracture patients, and these algorithms were selected for their capacity to identify key variables and make accurate predictions.\n\nThe focus of this research was on the clinical application of these algorithms rather than the development of new machine-learning techniques. Therefore, the algorithms were applied to a specific medical problem, and their performance was evaluated within this context. The results demonstrated that the Elastic-Net Penalized Logistic Regression model showed the best performance metrics, including good discrimination, calibration, and overall performance.\n\nThe study's primary contribution lies in the application of these machine-learning algorithms to a clinical problem, rather than the innovation of new algorithms. The findings highlight the potential of these methods in improving patient outcomes by identifying high-risk individuals preoperatively, thereby facilitating targeted preventive interventions.",
  "optimization/meta": "The model developed in this study is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it directly utilizes clinical variables and patient characteristics to predict postoperative delirium in geriatric hip fracture patients.\n\nSeveral machine learning algorithms were trained and internally validated, including Stochastic Gradient Boosting (SGM), Random Forest (RF), Support Vector Machine, Neural Network (NN), and Elastic-Net Penalized Logistic Regression (PLR). The dataset was randomly split into training and testing subsets, with the training set used for model development and the testing set for performance assessment. Cross-validation was employed to ensure robust model training and evaluation.\n\nThe best-performing model, Elastic-Net Penalized Logistic Regression (PLR), demonstrated good discrimination, calibration, and overall performance. The training data for each model was independent, as the dataset was split randomly into training and testing subsets before model training began. This approach ensures that the models were trained and validated on separate data, reducing the risk of overfitting and providing a more reliable assessment of model performance.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the data was suitable for model training. Categorical variables were described using absolute numbers with frequencies, while continuous variables were summarized using medians with interquartile ranges (IQR). Variables potentially associated with postoperative delirium were identified using random forest algorithms with recursive selection. Missing data was handled by imputing variables with less than 30% missing values using multiple imputation with the missForest methodology. This approach helped to manage missing data rates for various preoperative variables, including BMI, ASA class, preoperative delirium, functional status, and preoperative need for mobility aid. Additionally, preoperative laboratory values such as sodium, creatinine, white blood cell count, hematocrit, and platelet were also imputed. This preprocessing ensured that the dataset was complete and ready for training the machine-learning models.",
  "optimization/parameters": "The model utilized several input parameters to predict postoperative delirium in geriatric patients undergoing hip fracture surgery. The selection of these parameters was conducted using random forest algorithms with recursive selection, identifying variables potentially associated with the outcome.\n\nThe final set of predictive variables included:\n\n1. Age\n2. ASA class\n3. Functional status\n4. Preoperative dementia\n5. Preoperative delirium\n6. Preoperative need for mobility-aid\n\nThese variables were chosen based on their significance in predicting postoperative delirium, as determined through the variable selection process. The model's performance was evaluated using various metrics, including the c-statistic, calibration slope and intercept, and the Brier score, ensuring that the selected parameters provided a robust prediction of the outcome.",
  "optimization/features": "The input features used in the model were selected through a process involving random forest algorithms with recursive selection. This feature selection was performed using the training set only, ensuring that the model's performance on the test set remained unbiased. The selected features included age, ASA class, functional status, preoperative dementia, preoperative delirium, and preoperative need for mobility-aid. These variables were chosen for their predictive value in determining the likelihood of postoperative delirium in patients undergoing hip fracture surgery.",
  "optimization/fitting": "The fitting method employed in this study involved several machine learning algorithms, including Stochastic Gradient Boosting, Random Forest, Support Vector Machine, Neural Network, and Elastic-Net Penalized Logistic Regression. The dataset was split into an 80:20 training and testing subset, with the training set consisting of 22,563 patients and the testing set consisting of 5,641 patients.\n\nTo address the potential issue of overfitting, particularly given the large number of parameters relative to the training points, several strategies were implemented. First, 10-fold cross-validation was used during the training phase. This technique helps to ensure that the model generalizes well to unseen data by averaging the results across multiple folds. Additionally, the performance of the models was evaluated using a separate test set, which was not used during the training process. This provided an unbiased estimate of the model's performance.\n\nCalibration plots were used to assess the model's calibration, which helps to rule out overfitting by ensuring that the predicted probabilities match the observed outcomes. A calibration slope close to 1 and an intercept close to 0 indicate good calibration and suggest that the model is not overfitting. The Brier score, which combines both discrimination and calibration, was also used to evaluate the overall performance of the models. A lower Brier score indicates better predictive performance.\n\nTo rule out underfitting, the models were evaluated using the c-statistic, which measures the model's discrimination ability. A higher c-statistic indicates better discrimination between patients who experienced the outcome and those who did not. The decision curve analysis was also conducted to assess the net benefit of the models over a range of predicted probabilities. This analysis helps to ensure that the models provide meaningful clinical benefits and are not underfitting the data.\n\nIn summary, the fitting method involved rigorous validation techniques, including cross-validation, separate test set evaluation, calibration plots, and decision curve analysis. These methods helped to rule out both overfitting and underfitting, ensuring that the models were robust and generalizable.",
  "optimization/regularization": "Regularization techniques were employed to prevent overfitting in the development of the clinical prediction model. Specifically, Elastic-Net Penalized Logistic Regression (PLR) was one of the machine learning algorithms used. Elastic-Net combines the penalties of both L1 (Lasso) and L2 (Ridge) regularization, which helps in reducing the complexity of the model and preventing it from overfitting the training data. This approach ensures that the model generalizes well to unseen data by penalizing large coefficients, thereby promoting simpler and more interpretable models. Additionally, the use of 10-fold cross-validation during the training process further aided in mitigating overfitting by providing a robust estimate of model performance and ensuring that the model's predictions are reliable and not overly tailored to the training dataset.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The clinical prediction model developed in this study is designed to be transparent and not a black box. To ensure interpretability, the model provides explanations for individual patient-level predictions. For instance, if a patient over 90 years old, classified as ASA III, living dependently, and requiring preoperative mobility aid, but without preoperative delirium, is predicted to have a 42% chance of developing postoperative delirium, the model clearly outlines the contributing factors. In this example, factors like preoperative dementia, age, need for mobility aid, dependent functional status, and ASA class increase the likelihood, while the absence of preoperative delirium reduces it. This transparency helps clinicians understand the reasoning behind the model's predictions, making it a valuable tool for decision-making in clinical settings.",
  "model/output": "The model developed is a classification model. It is designed to predict the occurrence of postoperative delirium in geriatric patients undergoing hip fracture surgery. The model outputs a probability indicating the likelihood of a patient developing delirium after surgery. This probability can then be used to classify patients into high-risk or low-risk categories based on a predetermined threshold. The primary outcome of interest is the binary event of delirium occurrence within 30 days post-surgery. The model's performance was evaluated using metrics suitable for classification tasks, such as the c-statistic (area under the receiver operating characteristic curve), calibration slope and intercept, and the Brier score. These metrics help assess the model's ability to discriminate between patients who will and will not experience delirium, as well as its overall calibration and predictive accuracy. The model's output is integrated into a web-based application, allowing clinicians to input patient data and receive a personalized risk assessment for postoperative delirium.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The best model developed in this study has been deployed as an open-access web application. This application is accessible on various devices, including desktops, tablets, and smartphones. The web-based application can be found at https://sorg-apps.shinyapps.io/hipfxdelirium/. This deployment allows clinicians to easily access and utilize the prediction model for assessing the risk of postoperative delirium in patients undergoing hip fracture surgery. The application is designed to be user-friendly and integrates the Elastic-Net Penalized Logistic Regression (PLR) algorithm, which demonstrated the best performance among the evaluated models. The source code and specific details about the software environment used for data preprocessing and analysis, such as R and R-Studio, are not publicly released.",
  "evaluation/method": "The evaluation of the clinical prediction model was conducted using a comprehensive framework that assessed three key aspects: discrimination, calibration, and overall performance. Discrimination was measured using the c-statistic, which is the area under the curve of a receiver operating characteristic curve. This score ranges from 0.50 to 1.0, with higher values indicating better model performance in distinguishing patients who experienced the outcome from those who did not.\n\nCalibration was evaluated using calibration plots, which compare estimated probabilities to observed outcomes. A perfect calibration plot has an intercept of 0 and a slope of 1, indicating that the model's predictions are accurate and consistent across different datasets. In smaller datasets, a slope less than 1 may indicate overfitting, where the model's probabilities are too extreme.\n\nOverall performance was assessed using the Brier score, which combines both discrimination and calibration. The Brier score ranges from 0 to 1, with 0 indicating perfect prediction and 1 indicating the poorest prediction.\n\nAdditionally, a decision curve analysis was performed to evaluate the net benefit of the algorithms over a range of predicted probabilities. This analysis helps in understanding the clinical utility of the model by weighing the true positives against the false positives.\n\nThe dataset was split into training and testing subsets using an 80:20 ratio. The training set was used to develop the models through 10-fold cross-validation, ensuring that the results were averaged across all repetitions of the sequence. The performance of the models was then assessed on the test set to provide an unbiased evaluation of their predictive accuracy.",
  "evaluation/measure": "To evaluate the performance of our clinical prediction model, we employed a comprehensive framework that includes several key metrics. These metrics are widely recognized in the literature and provide a robust assessment of the model's effectiveness.\n\nFirstly, we used the c-statistic, also known as the area under the curve (AUC) of the receiver operating characteristic (ROC) curve. This metric ranges from 0.50 to 1.0, where 1.0 indicates perfect discrimination and 0.50 indicates no discrimination. A higher c-statistic reflects the model's better ability to distinguish between patients who experienced the outcome and those who did not.\n\nSecondly, we assessed calibration using the calibration slope and intercept. Calibration plots compare the estimated probabilities with the observed outcomes. An ideal calibration plot has an intercept of 0 and a slope of 1, indicating that the model's predictions are well-calibrated. Deviations from these values suggest overestimation or underestimation of the probabilities. In smaller datasets, a slope less than 1 often indicates overfitting, where the model's probabilities are too extreme.\n\nThirdly, we calculated the Brier score, which provides a composite measure of both discrimination and calibration. The Brier score ranges from 0 to 1, with 0 indicating perfect prediction and 1 indicating the poorest prediction. This metric gives a single value that summarizes the model's overall performance.\n\nAdditionally, we conducted a decision curve analysis to evaluate the net benefit of our algorithms across a range of predicted probabilities. This analysis helps in understanding the clinical utility of the model by weighing the true positives against the false positives.\n\nThese performance metrics are representative of the standards in the literature for evaluating clinical prediction models. They provide a thorough assessment of the model's ability to discriminate, calibrate, and overall predict the outcome of interest.",
  "evaluation/comparison": "Not enough information is available.",
  "evaluation/confidence": "The evaluation of the clinical prediction model was conducted using a framework that includes several key metrics: discrimination, calibration, and overall performance. Discrimination was assessed using the c-statistic, which ranges from 0.50 to 1.0, with higher values indicating better model performance in distinguishing between patients who experienced the outcome and those who did not. Calibration was evaluated using the calibration slope and intercept, following the method by Cox. A perfect calibration plot would have an intercept of 0 and a slope of 1, indicating that the model's predictions are well-aligned with the observed outcomes. The overall performance was measured using the Brier score, where a score of 0 indicates perfect prediction and a score of 1 indicates the poorest prediction.\n\nThe model's performance was assessed using cross-validation, specifically 10-fold cross-validation, where the dataset was divided into 10 groups, and the model was trained and tested multiple times across these groups. This approach helps to ensure that the results are robust and not dependent on a particular split of the data. The results were subsequently averaged across all repetitions of this sequence, providing a more reliable estimate of the model's performance.\n\nIn addition to these metrics, a decision curve analysis was undertaken to investigate the net benefit of the algorithms over the range of predicted probabilities. This analysis helps to determine the clinical utility of the model by weighing the true positives against the false positives.\n\nThe statistical significance of the results was not explicitly mentioned, but the use of cross-validation and the averaging of results across multiple repetitions suggest a rigorous approach to evaluating the model's performance. The confidence intervals for the performance metrics were not provided, but the methods used indicate a thorough evaluation process. The decision curve analysis further supports the claim that the model provides a net benefit, implying that it is superior to other methods and baselines in predicting postoperative delirium.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study does not provide specific details on the availability or release of the raw evaluation data. The focus of the publication is on the methodology, results, and implications of the clinical prediction model for postoperative delirium following hip fracture surgery. The evaluation metrics, such as the c-statistic, calibration slope and intercept, and Brier score, are discussed in the context of model performance. However, the actual data used for these evaluations is not made accessible to the public. Therefore, researchers or practitioners interested in replicating or building upon this work would need to conduct their own evaluations using similar methodologies."
}