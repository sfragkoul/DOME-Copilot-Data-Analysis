{
  "publication/title": "Using explainable machine learning to investigate the relationship between childhood maltreatment, positive psychological traits, and CPTSD symptoms.",
  "publication/authors": "Zhou X, Liang Z, Zhang G",
  "publication/journal": "European journal of psychotraumatology",
  "publication/year": "2025",
  "publication/pmid": "40007420",
  "publication/pmcid": "PMC11866650",
  "publication/doi": "10.1080/20008066.2025.2455800",
  "publication/tags": "- Childhood Maltreatment\n- CPTSD Symptoms\n- Positive Psychological Traits\n- Machine Learning\n- Adolescent Mental Health\n- Trauma and Abuse\n- Emotional Abuse\n- Self-Compassion\n- Mindfulness\n- Rural Adolescents",
  "dataset/provenance": "The dataset used in this study was collected from a sample of 1894 participants aged between 12 and 17 years. The participants were primarily from rural households, with 83.9% of them residing in rural areas. The dataset includes detailed demographic information, such as gender, household registration, only child status, parental marital status, and parental employment. Additionally, the dataset encompasses various types of maltreatment, including emotional abuse, emotional neglect, physical abuse, physical neglect, and sexual abuse. Positive psychological traits such as self-compassion, mindfulness, and gratitude were also measured. The data was collected with ethical approval and informed consent from participants and their legal guardians. This dataset has not been used in previous papers or by the community, as it is specific to this study. The dataset was split into training and testing sets at an 8:2 ratio to ensure robust model evaluation.",
  "dataset/splits": "The dataset was divided into two primary splits: a training set and a testing set. The distribution of data points between these splits followed an 8:2 ratio. This means that 80% of the data was allocated to the training set, and the remaining 20% was used for the testing set. This split ensures that the model is trained on a substantial portion of the data while reserving a significant enough sample to evaluate its performance accurately.\n\nAdditionally, to enhance the robustness and generalizability of the models, 5-fold cross-validation was employed. This technique involves dividing the training data into five equal parts, or folds. The model is then trained and validated five times, each time using a different fold as the validation set and the remaining four folds as the training set. This process helps in assessing the model's performance more comprehensively by ensuring that each data point is used for both training and validation.",
  "dataset/redundancy": "The dataset was divided into training and testing sets with an 8:2 ratio. This split ensures that the training set is used to build the model, while the testing set is used to evaluate its performance on unseen data, maintaining independence between the two sets.\n\nTo enforce this independence and ensure the robustness and generalizability of the models, 5-fold cross-validation was employed. This technique involves partitioning the training data into five subsets, or \"folds.\" The model is then trained and validated five times, each time using a different fold as the validation set and the remaining four folds as the training set. This process helps to mitigate overfitting and provides a more reliable estimate of the model's performance.\n\nRegarding the distribution of the dataset, it included 1894 participants aged between 12 and 17 years. The demographic breakdown was as follows: 51.5% male and 48.5% female, with 83.9% from rural households, 93.2% not single children, and 75% identified as left-behind children. This demographic distribution is representative of the target population, which consists of adolescents from low-income rural families. The dataset's focus on this specific population addresses a gap in the current literature, which often relies on samples from adult populations in more affluent regions.\n\nThe dataset underwent thorough cleaning and preprocessing. Missing values were imputed based on each variable\u2019s mean, given that none exceeded 0.3%. Demographic variables were standardized to ensure uniform input formats compatible with machine learning algorithms. This preprocessing step is crucial for maintaining the integrity and consistency of the data, which is essential for the accurate and reliable performance of the machine learning models.\n\nIn summary, the dataset was carefully split and validated to ensure independence and robustness. The demographic distribution reflects the target population, and rigorous preprocessing steps were taken to maintain data quality. This approach aligns with best practices in machine learning and ensures that the models developed are reliable and generalizable.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The study utilized machine learning algorithms to analyze the relationships between childhood maltreatment, positive psychological traits, and CPTSD symptoms. The algorithms employed were LightGBM, Random Forest, and Decision Tree. These are well-established algorithms in the field of machine learning and are not new. They were chosen for their effectiveness in handling complex, nonlinear relationships between predictive factors and outcome variables.\n\nLightGBM is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be distributed and efficient, capable of handling large datasets with high dimensionality. Random Forest is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Decision Tree is a non-parametric supervised learning method used for both classification and regression. The choice of these algorithms was driven by their ability to provide robust and interpretable models, which are crucial for understanding the contributions of different maltreatment types and psychological traits to CPTSD symptoms.\n\nThe decision to use these specific algorithms in a psychological study rather than a machine-learning journal is due to the focus of the research. The primary objective was to explore the relationships between childhood maltreatment, positive psychological traits, and CPTSD symptoms, rather than to develop or validate new machine-learning algorithms. The algorithms were selected for their proven effectiveness in similar research contexts and their ability to provide insights into the complex interactions between the variables of interest. The study aimed to leverage the strengths of these algorithms to enhance the interpretability and robustness of the models, thereby contributing to the understanding of CPTSD symptoms in adolescents from low-income rural families.",
  "optimization/meta": "The models employed in this study do not use data from other machine-learning algorithms as input. Instead, they utilize a combination of demographic variables, types of maltreatment, and positive psychological traits as predictors.\n\nThe machine learning models constructed include LightGBM, Random Forest, and Decision Tree algorithms. These models were built to predict CPTSD symptoms based on the specified input features. Model 1 utilized five types of maltreatment and key demographic variables as predictors, while Model 2 incorporated positive psychological traits as an additional variable.\n\nRegarding the independence of training data, the dataset was split into training and testing sets at an 8:2 ratio. This split ensures that the training data is independent of the testing data, which is crucial for assessing the model's performance and generalizability. Additionally, 5-fold cross-validation was employed to further validate the robustness of the models. This approach helps in ensuring that the models are not overfitted to the training data and can generalize well to unseen data.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure compatibility with machine learning algorithms. Initially, demographic variables were standardized to maintain uniform input formats. This standardization was essential for consistent data interpretation and to prevent any single variable from disproportionately influencing the models.\n\nDuring data cleaning, each variable was assessed for missing values, with none exceeding 0.3%. Given the minimal amount of missing data, we chose to impute values based on each variable\u2019s mean. This approach helped to maintain the integrity of the dataset while addressing missing data points effectively.\n\nTo analyze linear relationships among variables, Pearson correlation coefficients were calculated. This step allowed us to examine potential multicollinearity issues, which are critical for the stability and interpretability of machine learning models. Features with significant explanatory power for the dependent variable were identified using Elastic Net, a regularization technique that combines L1 and L2 penalties. Elastic Net is particularly effective in handling multicollinearity while isolating features with strong associations to the outcome variable.\n\nThrough cross-validation, we optimized the Elastic Net parameters to identify features that enhance interpretability while minimizing model complexity. This process ensured that the selected features were both relevant and non-redundant, contributing to the robustness of our models.\n\nThe dataset was then split into training and testing sets at an 8:2 ratio. This split allowed us to train the models on a substantial portion of the data while reserving a separate set for evaluating their performance. Additionally, 5-fold cross-validation was employed to assess model performance, ensuring robustness and generalizability.\n\nOverall, these preprocessing steps were designed to prepare the data for effective machine learning model training and evaluation, ensuring that the models could accurately predict CPTSD symptoms based on the given predictors.",
  "optimization/parameters": "In the optimization process, the number of parameters (p) used in the model was determined through a systematic approach involving data cleaning and feature selection.\n\nInitially, demographic variables and types of maltreatment were standardized to ensure uniform input formats compatible with machine learning algorithms. This standardization was crucial for maintaining consistency across the dataset.\n\nTo handle missing values, which were minimal (none exceeding 0.3%), mean imputation was employed for each variable. This method was chosen due to the minimal amount of missing data, ensuring that the imputed values did not significantly alter the dataset's integrity.\n\nFollowing data cleaning, Pearson correlation coefficients were calculated to analyze linear relationships among variables. This step was essential for identifying potential multicollinearity issues, which could affect the model's performance.\n\nTo further refine the feature set, Elastic Net was employed. This regularization technique combines L1 and L2 penalties, making it effective in handling multicollinearity while isolating features with strong associations to the outcome variable. Through cross-validation, the Elastic Net parameters were optimized to identify features that enhance interpretability while minimizing model complexity.\n\nThe final set of parameters included key demographic variables such as gender and parental marital status, along with types of maltreatment (emotional abuse, emotional neglect, physical abuse, physical neglect, and sexual abuse) and positive psychological traits (mindfulness, self-compassion, and gratitude). This selection process ensured that the model retained only the most relevant features, reducing redundant information and enhancing stability.\n\nIn summary, the number of parameters (p) was carefully selected through a combination of data standardization, mean imputation for missing values, Pearson correlation analysis, and Elastic Net feature selection. This approach ensured that the model was optimized for performance and interpretability.",
  "optimization/features": "In the optimization process, feature selection was indeed performed to enhance model stability and reduce redundant information. This was achieved using the Elastic Net method, which combines L1 and L2 regularization. The selection process was conducted using the training set only, ensuring that the model's performance on unseen data remained unbiased.\n\nThe initial set of features considered included demographic variables such as age, gender, household registration, only child status, parental marital status, and parental education and employment levels. Additionally, types of maltreatment\u2014including emotional neglect, emotional abuse, physical neglect, physical abuse, and sexual abuse\u2014and positive psychological traits like mindfulness, self-compassion, and gratitude were also evaluated.\n\nFollowing the feature selection process, the key features retained for the models were mindfulness, self-compassion, gratitude, emotional abuse, emotional neglect, physical abuse, physical neglect, marital status of parents, and gender. Therefore, the number of input features (f) used in the final models was nine.",
  "optimization/fitting": "In our study, we employed machine learning models to predict CPTSD symptoms using various types of maltreatment and demographic variables. The number of parameters in our models was not excessively large compared to the number of training points, as we had a substantial dataset of 1894 participants. To ensure the robustness and generalizability of our models, we utilized 5-fold cross-validation. This technique helps in assessing model performance by dividing the data into training and testing sets multiple times, thereby reducing the risk of overfitting.\n\nTo further mitigate overfitting, we employed Elastic Net for feature selection. Elastic Net combines L1 and L2 regularization, which helps in handling multicollinearity and selecting features that have a strong association with the outcome variable. This process reduced the number of features, enhancing model stability and interpretability.\n\nAdditionally, we evaluated model performance using multiple metrics, including R\u00b2, MAE, MSE, and RMSE. These metrics provided a comprehensive assessment of prediction accuracy and error magnitude, ensuring that our models were neither overfitting nor underfitting the data. The Random Forest algorithm, in particular, showed significant improvements in performance metrics when positive psychological traits were included, indicating a better fit to the data without overfitting.\n\nIn summary, through cross-validation, regularization techniques, and thorough performance evaluation, we ensured that our models were well-fitted to the data, avoiding both overfitting and underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was regularization, specifically through the Elastic Net algorithm. Elastic Net combines both L1 (Lasso) and L2 (Ridge) regularization, which helps in handling multicollinearity and selecting important features while penalizing less relevant ones. This dual regularization approach is particularly effective in reducing the complexity of the model and preventing it from overfitting to the training data.\n\nAdditionally, we utilized cross-validation, specifically 5-fold cross-validation, to assess the performance of our models. This technique involves splitting the dataset into training and testing sets multiple times, ensuring that the model's performance is evaluated on different subsets of the data. This process helps in validating the model's generalizability and robustness, further mitigating the risk of overfitting.\n\nMoreover, we employed the SHAP (SHapley Additive exPlanations) library to interpret the contributions of each feature in our models. SHAP values provide a global perspective on variable significance and the direction of their influence, helping us to understand the key predictors and their impact on the outcome variable. This interpretability aids in ensuring that the model is not overly complex and that the relationships between predictors and the outcome are meaningful and not artifacts of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are available. We employed three different machine learning algorithms: LightGBM, Random Forest, and Decision Tree. For each algorithm, we optimized the parameters through cross-validation to ensure robust and generalizable model performance.\n\nThe specific configurations and optimization schedules are detailed in the methodology section of our publication. We utilized the SHAP library to generate visualizations that facilitate the interpretation of feature contributions. The dataset was split into training and testing sets at an 8:2 ratio, with 5-fold cross-validation employed to assess model performance.\n\nRegarding the availability of model files, these are not explicitly provided in the publication. However, the detailed steps and parameters used for model training are thoroughly described, allowing replication of the models by interested researchers. The study was conducted with ethical approval, and all participants provided informed consent, ensuring the integrity and reproducibility of our findings.\n\nThe publication does not specify the licensing terms for the data or code, but the detailed methodology and results are presented transparently, enabling other researchers to build upon our work. For further details, readers are encouraged to refer to the full text of the publication.",
  "model/interpretability": "In our study, we employed machine learning models to predict CPTSD symptoms, and while these models are often considered \"black boxes,\" we utilized interpretability methods to shed light on their inner workings. Specifically, we used SHapley Additive exPlanations (SHAP) to provide a global perspective on variable significance and the direction of their influence.\n\nSHAP values help identify key predictors and whether they positively or negatively impact the outcome. For instance, emotional abuse emerged as the most significant predictor of CPTSD symptoms, with a notable SHAP value. This indicates that higher levels of emotional abuse are strongly associated with increased CPTSD symptoms. Similarly, emotional neglect and physical neglect also showed significant SHAP values, though to a lesser extent.\n\nThe visualizations generated through SHAP, such as summary plots and dependency plots, allowed us to observe how different types of maltreatment contribute to CPTSD symptoms. These plots not only highlight the magnitude of each feature's contribution but also illustrate the interdependencies among variables. For example, scatter plots of SHAP values for emotional abuse revealed that as the scores for emotional abuse increase, the SHAP values exhibit a noticeable upward trend, implying a stronger association with higher levels of CPTSD symptoms.\n\nMoreover, we set thresholds for different types of maltreatment based on established criteria. For emotional abuse, a threshold score of 13 was used, indicating that scores of 13 or higher are associated with a history of emotional abuse and higher predicted CPTSD symptom levels. Similarly, thresholds for emotional neglect and physical abuse/neglect were set at 15 and 10, respectively, showing that scores at or above these levels are linked to increased CPTSD symptoms.\n\nBy incorporating these interpretability methods, we were able to analyze the contribution of different maltreatment types to CPTSD symptoms while directly observing their interdependencies. This approach provides a clearer understanding of how various factors influence the outcome, making the model more transparent and the results more interpretable.",
  "model/output": "The model is a regression model. It predicts the severity of CPTSD symptoms, which is a continuous outcome variable. The performance of the model is evaluated using metrics such as R-squared (R\u00b2), Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE). These metrics are typically used for regression tasks to assess the accuracy and error magnitude of the predictions.\n\nTwo models were constructed: Model 1 and Model 2. Model 1 includes types of maltreatment and key demographic variables as predictors. Model 2 incorporates additional positive psychological traits to explore their relationship with maltreatment and CPTSD symptoms. The inclusion of these traits in Model 2 led to improved performance across all algorithms, with notable reductions in both RMSE and MAE. Specifically, the RMSE for the Random Forest algorithm in Model 2 decreased to 6.26, indicating better error control and predictive accuracy.\n\nThe dataset was split into training and testing sets at an 8:2 ratio, with 5-fold cross-validation employed to ensure robustness and generalizability. This approach helps in assessing how well the model performs on unseen data, which is crucial for regression models to ensure they generalize well beyond the training data.\n\nThe SHAP library was utilized to generate various visualizations, including summary plots, SHAP dependency plots, and scatter plots. These visualizations facilitated a deeper understanding of the contribution and relationships of each type of maltreatment and positive psychological traits in predicting CPTSD symptoms. For instance, emotional abuse was identified as the most significant predictor of CPTSD symptoms, followed by emotional neglect and physical neglect. The visualizations also showed how different levels of these predictors influence the outcome variable, providing insights into the model's interpretability.\n\nIn summary, the model is designed to predict a continuous outcome variable, CPTSD symptoms, using various predictors related to maltreatment and psychological traits. The use of regression metrics and cross-validation ensures the model's accuracy and generalizability, making it a robust tool for understanding the factors contributing to CPTSD symptoms.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to ensure the robustness and generalizability of the models. The dataset was split into training and testing sets at an 8:2 ratio. This split allowed for a significant portion of the data to be used for training the models while reserving a substantial portion for evaluating their performance on unseen data.\n\nTo further enhance the reliability of the models, 5-fold cross-validation was utilized. This technique involves dividing the training data into five subsets, or folds. The model is then trained on four of these folds and validated on the remaining fold. This process is repeated five times, with each fold serving as the validation set once. The performance metrics are averaged across these five iterations to provide a more stable and reliable estimate of the model's performance.\n\nThe models were constructed using three different algorithms: LightGBM, Random Forest, and Decision Tree. These algorithms were chosen for their ability to handle complex, nonlinear relationships between the predictive factors and the outcome variable, which in this case was CPTSD symptoms.\n\nModel performance was evaluated using several key metrics: R-squared (R\u00b2), Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE). These metrics provided a comprehensive assessment of the models' prediction accuracy and error magnitude. R\u00b2 indicates the proportion of variance in the dependent variable that is predictable from the independent variables. MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. MSE is the average of the squares of the errors, giving more weight to larger errors. RMSE is the square root of the MSE, providing an error metric in the same units as the dependent variable.\n\nThe evaluation process also included the use of the SHAP (SHapley Additive exPlanations) library to generate various visualizations. These visualizations, including summary plots, SHAP dependency plots, and scatter plots, facilitated a deeper understanding of the contribution and relationships of each type of maltreatment and positive psychological traits in predicting CPTSD symptoms. SHAP values provide a global perspective on variable significance and the direction of influence, identifying key predictors and their impact on the outcome. This interpretability method allowed for analyzing the contribution of different maltreatment types to CPTSD symptoms while directly observing interdependencies among maltreatment types.",
  "evaluation/measure": "In the evaluation of our models, we employed a comprehensive set of performance metrics to ensure a thorough assessment of prediction accuracy and error magnitude. The metrics reported include the coefficient of determination (R\u00b2), Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE). These metrics are widely recognized in the literature for their ability to provide a detailed evaluation of model performance.\n\nThe R\u00b2 value indicates the proportion of variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, with higher values indicating better model performance. MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. It provides an intuitive measure of prediction accuracy. MSE is the average of the squares of the errors, giving more weight to larger errors. RMSE is the square root of the MSE, providing an error metric in the same units as the dependent variable, which is easier to interpret.\n\nThese metrics collectively offer valuable insights into model optimization and the interpretation of results. By reporting these standard metrics, we ensure that our evaluation is representative and comparable to other studies in the field. This approach allows for a robust assessment of our models' performance and their ability to predict the outcome variable accurately.",
  "evaluation/comparison": "In our evaluation, we constructed and assessed two models using different algorithms to predict CPTSD symptoms. Model 1 included only maltreatment types and key demographic variables, while Model 2 incorporated additional positive psychological traits. This approach allowed us to compare the performance of the models with and without the inclusion of psychological traits.\n\nFor Model 1, we utilized LightGBM, Random Forest, and Decision Tree algorithms. The Random Forest algorithm achieved the highest R\u00b2 value of 0.36, indicating it explained 36% of the variance in CPTSD symptoms. It also demonstrated the lowest RMSE and MAE among the algorithms, suggesting better error control. This comparison showed that Random Forest outperformed both LightGBM and Decision Tree in terms of predictive accuracy and error metrics.\n\nIn Model 2, the addition of positive psychological traits led to improved performance across all algorithms. The Random Forest algorithm showed a significant increase in R\u00b2 to 0.58, indicating it explained 58% of the variance in CPTSD symptoms. This model also exhibited reductions in both RMSE and MAE, with the RMSE decreasing to 6.26. The improvements in Model 2 highlight the importance of including positive psychological traits in predicting CPTSD symptoms.\n\nWe did not perform a comparison to publicly available methods on benchmark datasets, as our focus was on evaluating the impact of including psychological traits in our specific models. However, the comparison between Model 1 and Model 2 served as an internal baseline, demonstrating the enhanced predictive power when psychological traits were included. This approach allowed us to assess the relative improvement in model performance without relying on external benchmarks.",
  "evaluation/confidence": "Evaluation Confidence\n\nThe evaluation of the models was conducted using a comprehensive set of performance metrics, including R\u00b2, MAE, MSE, and RMSE. These metrics provide a robust assessment of the models' predictive accuracy and error magnitude. The dataset was split into training and testing sets at an 8:2 ratio, with 5-fold cross-validation employed to ensure the robustness and generalizability of the results. This approach helps in mitigating overfitting and provides a more reliable estimate of the model's performance on unseen data.\n\nThe performance metrics for the models are presented in a table, which includes the values for R\u00b2, MAE, MSE, and RMSE for different algorithms. The Random Forest algorithm, in particular, showed significant improvements in Model 2 compared to Model 1, with reductions in both RMSE and MAE. Specifically, the RMSE for Random Forest decreased to 6.26, indicating better error control and predictive accuracy.\n\nHowever, it is important to note that the provided information does not explicitly mention confidence intervals for the performance metrics. Confidence intervals would provide additional insight into the reliability of the estimated metrics and help in assessing the statistical significance of the results. Without confidence intervals, it is challenging to definitively claim that one method is superior to others and baselines with a high degree of statistical confidence.\n\nThe use of 5-fold cross-validation is a strong indicator of the robustness of the evaluation process. This technique helps in ensuring that the model's performance is consistent across different subsets of the data, reducing the risk of overfitting and providing a more reliable estimate of the model's generalizability. However, the absence of confidence intervals means that while the results are promising, they should be interpreted with caution regarding their statistical significance.\n\nIn summary, while the evaluation process is rigorous and the performance metrics are comprehensive, the lack of confidence intervals limits the ability to make strong statistical claims about the superiority of the methods. Future work could benefit from including confidence intervals to provide a more complete picture of the models' performance and statistical significance.",
  "evaluation/availability": "Not enough information is available."
}