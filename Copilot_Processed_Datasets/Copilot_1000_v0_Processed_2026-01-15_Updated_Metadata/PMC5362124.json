{
  "publication/title": "PREDICTIVE MODELING OF HOSPITAL READMISSION RATES USING ELECTRONIC MEDICAL RECORD-WIDE MACHINE LEARNING: A CASE-STUDY USING MOUNT SINAI HEART FAILURE COHORT.",
  "publication/authors": "Shameer K, Johnson KW, Yahi A, Miotto R, Li LI, Ricks D, Jebakaran J, Kovatch P, Sengupta PP, Gelijns S, Moskovitz A, Darrow B, David DL, Kasarskis A, Tatonetti NP, Pinney S, Dudley JT",
  "publication/journal": "Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing",
  "publication/year": "2017",
  "publication/pmid": "27896982",
  "publication/pmcid": "PMC5362124",
  "publication/doi": "10.1142/9789813207813_0027",
  "publication/tags": "- Predictive Modeling\n- Hospital Readmission\n- Electronic Medical Records\n- Machine Learning\n- Heart Failure\n- Data-Driven Approach\n- Feature Selection\n- Na\u00efve Bayes\n- Healthcare Delivery\n- Patient Outcomes\n- Chronic Conditions\n- Medical Informatics\n- Data Mining\n- Phenomics\n- Logistic Regression",
  "dataset/provenance": "The dataset used in this study was sourced from the Mount Sinai Data Warehouse, which is part of the Mount Sinai Health System. The study cohort consists of 1,068 individuals who were admitted to the Mount Sinai Heart service during the year 2014. The principal diagnosis of heart failure, as per the CMS directive, was used to compile the heart failure patients. Each patient readmitted to any service of Mount Sinai within 30 days after the discharge of a heart failure primary encounter is defined as a \"case.\" The remainder of the patients who did not return to the hospital within 30 days were defined as \"controls.\"\n\nThe dataset includes a wide range of variables extracted from electronic medical records (EMR). A total of 4,205 variables were extracted, categorized into five data modalities: diagnosis codes (ICD-9 codes and IMO codes), procedures (ICD-9, SNOMED-CT, and CPT codes), medications, and vital signs. For each patient, encounter-specific data was extracted from the EMR, with a patient-specific filter used to incorporate data from the most recent visit of patients with multiple admissions.\n\nThe dataset includes 1,763 diagnosis codes, 1,028 medications, 846 laboratory measurements, 564 procedures, and 4 vital signs. The medications prescribed during hospitalization were compiled using Epic and normalized using RxNorm. Laboratory measures were captured in the EMR without normalization. Procedures were encoded using SNOMED-CT or ICD-9-CM procedures. Vital signs, including pulse, respiration rate, systolic blood pressure, and temperature, were compiled from bedside monitor logs captured in a MySQL database.\n\nThis study is one of the first attempts to use phenome-wide data to identify novel factors driving readmissions related to congestive heart failure and develop EMR-wide prediction models with orthogonal validation to predict the readmission event. Previous models have used clinical variables and covariates such as age, sex, race, socioeconomic factors, body mass index, laboratory measures, biomarkers, comorbidities, behavioral factors, functional phenotyping of cardiovascular systems, discharge follow-ups, and medications. Some models have also used billing and procedural codes extracted from EMR or other hospital administration databases. However, the predictive power of such models remains weak, with Area Under Curve (AUC) values generally in the range of 0.6\u20130.7. This study aims to improve upon these models by leveraging a broader range of EMR-wide variables in a hypothesis-free approach.",
  "dataset/splits": "In our study, we utilized two primary data splits for model training and testing. The dataset was divided into a training set and a testing set. The training set comprised 70% of the total dataset, while the testing set consisted of the remaining 30%. This split was employed to ensure that the models were trained on a substantial portion of the data while being evaluated on a separate, unseen subset to assess their generalization performance.\n\nAdditionally, we employed a 5-fold cross-validation approach to estimate the testing accuracies. This method involves dividing the dataset into five equal folds. The model is then trained on four of these folds and tested on the remaining fold. This process is repeated five times, with each fold serving as the test set once. The results from these five iterations are averaged to provide a more robust estimate of the model's performance.\n\nThe distribution of data points in each split was designed to maintain the overall characteristics of the dataset, ensuring that each subset was representative of the entire dataset. This approach helped in mitigating the risk of overfitting and provided a more reliable evaluation of the models' predictive capabilities.",
  "dataset/redundancy": "All models were independently created using 70% of the dataset for training and 30% of the dataset for testing. This split ensures that the training and test sets are independent, which is crucial for evaluating the performance of the models. The independence of the datasets was enforced by randomly assigning data points to either the training or testing set, ensuring that no data point appears in both sets. This approach helps to prevent data leakage and provides an unbiased estimate of the model's performance.\n\nThe distribution of the dataset used in this study is comparable to previously published machine learning datasets in the healthcare domain. The dataset is highly imbalanced, with a small subset of cases (16.7%) compared to the controls (83.3%). To address this imbalance, a random subset of age and sex-matched controls was used to mitigate the bias introduced by the imbalanced datasets. This strategy is commonly employed in healthcare datasets to ensure that the model's predictions are not skewed by the majority class.\n\nThe dataset consists of various data modalities, including diagnosis codes, procedures, medications, laboratory measurements, and vital signs. A total of 4,205 variables were extracted from the electronic medical records (EMR). The data was categorized into these five modalities to capture a comprehensive view of the patient's health status. This approach allows for a detailed analysis of the factors contributing to readmission rates in heart failure patients. The use of multiple data modalities provides a rich feature space for the machine learning models, enabling them to identify complex patterns and relationships in the data.",
  "dataset/availability": "The data used in this study was sourced from the Mount Sinai Data Warehouse and included various modalities such as diagnosis codes, procedures, medications, laboratory measurements, and vital signs. The dataset comprised 4,205 variables extracted from electronic medical records (EMR). However, the specific data splits used for training and testing the models, as well as the raw dataset itself, were not released in a public forum. The study utilized 70% of the dataset for training and 30% for testing, but these splits were not made publicly available. The data was categorized and normalized using standard medical coding systems and tools like ICD-9, RxNorm, and UMLS, ensuring consistency and interoperability. The study design and methodologies were thoroughly documented, but the actual data remains proprietary to the Mount Sinai Health System. Therefore, while the methods and findings are transparent, the dataset itself is not publicly accessible.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the Na\u00efve Bayes classifier. This algorithm is well-established and widely used in the field of machine learning for its simplicity and effectiveness in various classification tasks.\n\nThe Na\u00efve Bayes algorithm employed in our research is not new. It is a standard algorithm provided by the Weka machine learning software suite, which is a popular tool for data mining and machine learning. We utilized the native Na\u00efve Bayesian classifier in Weka without any modifications. The choice of this algorithm was based on prior studies that demonstrated its effectiveness in modeling readmission prediction.\n\nThe decision to use an established algorithm like Na\u00efve Bayes, rather than developing a new one, was driven by the need for reliability and proven performance. Na\u00efve Bayes has been extensively validated in the literature and is known for its robustness in handling large datasets with numerous features. Given our focus on predicting hospital readmissions using electronic medical records (EMR) data, we sought an algorithm that could efficiently process and analyze the vast amount of data available.\n\nWhile the Na\u00efve Bayes algorithm is not novel, its application in the context of EMR-wide data for predicting readmission rates is significant. Our study contributes to the field by demonstrating the utility of this algorithm in a complex and data-rich environment, showcasing its potential for improving healthcare outcomes. The results obtained from our EMR-wide predictive model, with an AUC of 0.78 and an accuracy of 83.19%, highlight the effectiveness of using Na\u00efve Bayes in this specific application.",
  "optimization/meta": "The model developed in this study does not function as a meta-predictor. Instead, it relies on a comprehensive, EMR-wide feature selection process to identify novel predictors of readmission rates. The predictive modeling strategy involves using various data elements from electronic medical records, including diagnoses, procedures, medications, and lab measurements. These elements are encoded and evaluated using different machine learning algorithms, with a particular focus on Bayesian predictors.\n\nThe feature selection method employed is the correlation-based feature subset selection (CFS), which aims to find subsets of features with significant discriminatory power while ensuring they are uncorrelated in the feature space. This approach helps in identifying the most relevant features that contribute to the prediction of readmission rates.\n\nThe model was trained and tested using a dataset split into 70% for training and 30% for testing. Additionally, orthogonal validation methods, such as logistic regression, were used to ensure the robustness of the model. The accuracy and area under the curve (AUC) metrics were evaluated to assess the performance of the model.\n\nIn summary, the model does not use data from other machine-learning algorithms as input in a meta-predictor fashion. Instead, it leverages a diverse set of EMR-wide features and employs a rigorous feature selection process to develop a predictive model for readmission rates. The training data is independent, as evidenced by the split used for training and testing, as well as the orthogonal validation methods applied.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithms. We extracted a total of 4,205 variables from electronic medical records (EMR), categorizing them into five data modalities: diagnosis codes, procedures, medications, laboratory measurements, and vital signs.\n\nDiagnosis codes were mapped to ICD-9 or IMO codes and unified using UMLS for normalization. Medications prescribed during hospitalization were compiled using Epic and normalized using RxNorm. Laboratory measurements were used in their raw values without normalization, creating a matrix of observations with patients as rows and individual tests as columns. Procedures were encoded using SNOMED-CT or ICD-9-CM procedures. Vital signs, such as pulse, respiration rate, systolic blood pressure, and temperature, were compiled from bedside monitor logs captured in a MySQL database.\n\nFor the machine-learning models, features were encoded as binary variables where applicable. This approach was particularly useful for diagnosis codes, procedures, and medications, allowing us to represent the presence or absence of specific conditions, treatments, or drugs. Continuous variables, such as laboratory measurements and vital signs, were used in their raw form to maintain the granularity of the data.\n\nFeature selection was performed using a correlation-based feature subset selection method, which aimed to find subsets of features with significant discriminatory power that were uncorrelated in feature space. This method helped in reducing the dimensionality of the data and improving the performance of our models.\n\nPrincipal component analyses were conducted using the Python-based scikit-learn package to understand the variability of features. These analyses were visualized using matplotlib, providing insights into the inherent relationships and variance within the data.\n\nThe dataset was split into training and testing sets, with 70% of the data used for training and 30% for testing. This split ensured that our models were trained on a sufficient amount of data while also being evaluated on an independent test set. Additionally, we used a 5-fold cross-validation approach to estimate testing accuracies, further validating the robustness of our models.\n\nIn summary, our data encoding and preprocessing steps involved categorizing and normalizing EMR data, encoding features as binary variables where applicable, and using feature selection and principal component analyses to reduce dimensionality and understand feature variability. These steps were essential in preparing the data for effective machine-learning modeling.",
  "optimization/parameters": "In our study, we utilized a composite model that was developed using 105 features. These features were selected through a rigorous process involving feature reduction and model refinement. Initially, we extracted a total of 4,205 variables from the electronic medical records (EMR). These variables were categorized into five data modalities: diagnosis codes, procedures, medications, vital signs, and laboratory measurements.\n\nTo address the challenge of overfitting due to the high-dimensional feature array and the low percentage of cases in our cohort, we employed a feature reduction approach. This involved testing features to assess their predictive value using both classifier-based methods and regression models. We used a correlation-based feature subset selection (CFS) method to identify a subset of highly predictive variables associated with the readmitted subset of patients. The CFS method aims to find features with significant discriminatory power that are uncorrelated in the feature space.\n\nThe final composite model was developed using the 105 most predictive features, which were selected through this process. The model achieved an area under the curve (AUC) of 0.78 and a cross-validation testing accuracy of 83.19%. This selection process ensured that the model was robust and generalizable, reducing the risk of overfitting and improving its predictive accuracy.",
  "optimization/features": "In the optimization process of our predictive model, we initially extracted a total of 4,205 variables from the electronic medical records (EMR). These variables were categorized into five data modalities: diagnosis codes, procedures, medications, vital signs, and laboratory measurements.\n\nTo address the challenge of overfitting due to the high-dimensional feature space and the relatively low percentage of readmitted cases in our cohort, we employed a feature reduction approach. This involved testing features to assess their predictive value using both classifier-based methods and regression models. Feature selection was performed using a correlation-based feature subset selection (CFS) method, which aims to identify a subset of features with significant discriminatory power that are uncorrelated in the feature space. This process was conducted using the training set only, ensuring that the selected features were not influenced by the test data.\n\nThe final composite model was developed using 105 features, which were selected based on their predictive value and orthogonal validation. This reduction in the number of features helped to improve the model's performance and generalization to new data. The use of feature selection techniques was crucial in identifying the most relevant variables associated with patient readmission, thereby enhancing the model's accuracy and reliability.",
  "optimization/fitting": "The study involved a high-dimensional feature array, which could indeed lead to overfitting due to the relatively low percentage of cases in the cohort under investigation. To mitigate this risk, a feature reduction approach was employed. This involved testing features to assess their predictive value using both classifier-based methods and regression models. Feature selection and orthogonal validation were used to identify a subset of highly predictive variables associated with readmitted patients.\n\nThe area under the curve (AUC) values for the regression models were 0.5685 for vitals, 0.6471 for diagnosis codes, 0.7596 for medications, and 0.795 for procedures. The final composite model, developed using 105 features, achieved an AUC of 0.78 and a cross-validation testing accuracy of 83.19%. This indicates that the model generalizes well to unseen data, suggesting that overfitting was effectively managed.\n\nTo further ensure the robustness of the model, a systematic assessment of various feature selection algorithms could be conducted. Additionally, the model was tested using data from a single tertiary care healthcare institution over one year. Future work should involve testing the model using data from multiple sites and several data-years to enhance its generalizability and reduce the risk of overfitting.\n\nUnderfitting was addressed by using a comprehensive set of features and employing advanced machine learning techniques. The use of a Na\u00efve Bayes model, along with feature selection methods like correlation-based feature subset selection, helped in identifying significant features that contribute to the predictive power of the model. The model's performance, as indicated by the high accuracy and AUC values, suggests that it captures the underlying patterns in the data without being too simplistic.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting, particularly given the high-dimensional feature array and the relatively low percentage of readmission cases in our cohort. One of the primary methods used was feature reduction. We utilized a feature selection approach to identify a subset of highly predictive variables associated with readmitted patients. This involved testing features using a classifier-based method and regression models to assess their predictive value. The feature selection process helped in reducing the dimensionality of the data, thereby mitigating the risk of overfitting.\n\nAdditionally, we implemented an orthogonal validation approach to further refine our model. This involved using logistic regression for orthogonal validation, which helped in ensuring that the selected features were robust and not merely artifacts of the training data. The final composite model was developed using 105 features, which were selected through this rigorous process. This model achieved an AUC of 0.78 and a cross-validation testing accuracy of 83.19%, indicating its effectiveness in predicting readmission rates without overfitting.\n\nWe also employed a 5-fold cross-validation approach to estimate testing accuracies. This technique helps in assessing the model's performance on different subsets of the data, providing a more reliable estimate of its generalization capability. By using cross-validation, we ensured that the model's performance was not overly optimistic and that it could generalize well to new, unseen data.\n\nOverall, these methods\u2014feature reduction, orthogonal validation, and cross-validation\u2014were crucial in preventing overfitting and ensuring the robustness and reliability of our predictive model.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model developed in this study leverages machine learning techniques to predict readmission rates in heart failure patients, utilizing a comprehensive set of features extracted from electronic medical records (EMR). The approach involves EMR-wide feature selection and machine learning to identify novel predictors, making it a data-driven, hypothesis-free method.\n\nThe model is not entirely a black box. While machine learning models, particularly those involving complex algorithms like deep learning, can sometimes be opaque, the methods used here include steps that enhance interpretability. For instance, feature selection was performed using correlation-based feature subset selection, which aims to identify features with significant discriminatory power while being uncorrelated in the feature space. This process helps in understanding which variables are most influential in predicting readmission rates.\n\nAdditionally, the use of logistic regression for orthogonal validation provides a more interpretable model compared to some other machine learning techniques. Logistic regression coefficients can be examined to understand the contribution of each feature to the prediction, offering insights into the factors driving readmission.\n\nPrincipal component analysis (PCA) was also employed to understand the variability of features. PCA reduces the dimensionality of the data while retaining most of the variance, making it easier to visualize and interpret the relationships between different features.\n\nFurthermore, the study highlights specific novel factors identified through feature selection that could help delineate readmission rates associated with heart failure. This indicates that the model not only predicts outcomes but also provides actionable insights into the underlying factors contributing to readmission.\n\nIn summary, while the model benefits from the power of machine learning, it incorporates methods that enhance its interpretability, making it a valuable tool for both prediction and understanding the drivers of readmission in heart failure patients.",
  "model/output": "The model employed in this study is designed for a binary classification task. The classification problem is defined as distinguishing between \"Readmitted\" (RA) patients and \"Not readmitted\" (NonRA) patients. The primary goal is to predict whether a patient will be readmitted based on various features extracted from electronic medical records (EMR).\n\nA Na\u00efve Bayes model was initially used for machine learning, and exploratory data analyses were conducted using Elasticsearch and Kibana. All models were trained using 70% of the dataset and tested on the remaining 30%. Feature selection was performed using correlation-based feature subset selection across two classes. Orthogonal validation of the machine learning models was carried out with logistic regression.\n\nThe final composite model was developed using 105 features, achieving an area under the curve (AUC) of 0.78 and a cross-validation testing accuracy of 83.19%. This model demonstrates the utility of EMR-wide machine learning in predicting readmission rates, showing encouraging results compared to existing predictive models for heart failure readmission rates, which typically have AUCs in the range of 0.6 to 0.7.\n\nThe model's performance was evaluated using ROC curves, which provide a visual representation of the trade-off between the true positive rate and the false positive rate. The ROC curves for logistic regression models and the composite model with 105 features are depicted in Figure 3. The composite model's high accuracy and AUC indicate its effectiveness in identifying key predictors of patient readmission.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the machine learning models used in this study is not publicly released. However, the algorithms employed, such as the Na\u00efve Bayes classifier, are available through the Weka software suite, which provides a programmatic interface in Java. Weka is open-source and can be accessed via its official website. Additionally, the exploratory data analyses were conducted using Elasticsearch and Kibana, both of which are open-source tools available on GitHub. For principal component analyses and visualizations, the Python-based scikit-learn package and matplotlib were utilized, both of which are also open-source and freely available. While the specific implementations and datasets used in this study are not publicly released, the tools and methodologies are accessible through these open-source platforms.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to ensure the robustness and accuracy of the predictive models developed. The dataset was split into training and testing subsets, with 70% of the data used for training the models and the remaining 30% reserved for testing. This split was crucial for assessing the models' performance on unseen data.\n\nTo further validate the models, a 5-fold cross-validation approach was utilized. This technique involves dividing the dataset into five equal parts, or folds, and training the model on four of these folds while testing it on the remaining fold. This process is repeated five times, with each fold serving as the test set once. The results from these five iterations are then averaged to provide a more reliable estimate of the model's performance.\n\nIn addition to cross-validation, orthogonal validation was performed using logistic regression. This method helps to confirm the findings by applying a different statistical approach, thereby enhancing the credibility of the results.\n\nFeature selection was a critical component of the evaluation process. A correlation-based feature subset selection method was employed to identify the most relevant features for classification. This approach aims to find a subset of features that are not only significant in discriminating between the classes but also uncorrelated in the feature space, thereby reducing redundancy and improving model performance.\n\nPrincipal component analysis (PCA) was conducted to understand the variability of the features. This technique helps in dimensionality reduction by transforming the original features into a set of principal components that capture the most significant patterns in the data. The results of PCA were visualized using matplotlib, providing insights into the underlying structure of the data.\n\nThe classification task was defined as a binary classification problem, where patients were categorized as either \"Readmitted\" (RA) or \"Not readmitted\" (NonRA). The Na\u00efve Bayes model, chosen for its simplicity and effectiveness in handling large datasets, was used for this classification task. The model's performance was evaluated using metrics such as accuracy, which was found to be 83.19% in the final composite model.\n\nOverall, the evaluation method combined multiple techniques to ensure the reliability and validity of the predictive models. The use of cross-validation, orthogonal validation, and feature selection, along with PCA, provided a thorough assessment of the models' performance and their potential for predicting patient readmission rates.",
  "evaluation/measure": "In our evaluation, we reported several key performance metrics to assess the effectiveness of our predictive models. The primary metrics we focused on were accuracy and the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve. These metrics are widely recognized and used in the literature for evaluating predictive models, particularly in healthcare settings.\n\nAccuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. It provides a straightforward indication of how often the model's predictions are correct. In our study, we achieved an accuracy of 83.19% for our composite model, which is notably higher than the accuracy rates reported in many existing predictive models for heart failure readmission, which typically range from 60% to 70%.\n\nThe AUC, on the other hand, provides a more comprehensive evaluation of the model's performance by considering the trade-off between sensitivity (true positive rate) and specificity (true negative rate) across all possible classification thresholds. An AUC of 1 indicates a perfect model, while an AUC of 0.5 suggests a model with no discriminative power. Our composite model achieved an AUC of 0.78, which is significantly higher than the AUCs reported in previous studies, which generally fall within the range of 0.6 to 0.7.\n\nIn addition to these primary metrics, we also reported the performance of individual models built using different data modalities, such as diagnoses, procedures, medications, and laboratory measurements. This detailed reporting allows for a nuanced understanding of how different types of data contribute to the overall predictive power of the model. For instance, medications were found to be the most predictive with an accuracy of 81% and an AUC of 0.615, while procedures encoded as binary variables performed poorly with AUCs of less than 0.50.\n\nBy reporting these metrics, we aim to provide a transparent and comprehensive evaluation of our model's performance, making it comparable to other studies in the field. The high accuracy and AUC of our composite model suggest that our EMR-wide predictive modeling approach is effective and has the potential to improve the quality of healthcare delivery by identifying patients at risk of readmission.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison with existing heart failure readmission models to evaluate the performance of our EMR-wide feature selection and machine learning approach. We examined various predictive modeling techniques used in previous research, including Na\u00efve Bayes, Random Forests, logistic regression, Adaboost, and neural networks. For instance, we referenced a study by Hosseinzadeh et al., which showed that Na\u00efve Bayes models outperformed Random Forest models in predicting hospital readmissions using healthcare data from Quebec, Canada. Additionally, we compared our results with those from Duggal et al., who used a diabetes cohort from India and found that Na\u00efve Bayes showed higher readmission-associated savings compared to other models. Furthermore, we noted that Futoma et al. demonstrated similar accuracy rates for Random Forests and deep learning using neural networks with a large patient dataset.\n\nOur composite model, which combines features from various data modalities such as diagnoses, procedures, medications, and lab measurements, achieved an AUC of 0.78 and an accuracy of 83.19%. This performance is notably higher than the AUCs in the range of 0.6\u20130.7 reported in existing predictive models for heart failure readmission rates. The comparison highlights the utility of our data-driven, EMR-wide machine learning approach in improving the prediction of readmission rates. We also performed orthogonal validation methods, which confirmed the robustness of our model's accuracy. Overall, our findings suggest that leveraging EMR-wide data and advanced machine learning techniques can significantly enhance the prediction of heart failure readmission rates compared to traditional methods.",
  "evaluation/confidence": "In our study, we employed several statistical methods to ensure the robustness and significance of our results. We used orthogonal validation approaches, including logistic regression, to confirm the predictive power of our features. This involved testing the significance of individual features using statistical tests such as the Kolmogorov-Smirnov test, t-test, Z-score, and Mann-Whitney test, depending on the data type. These tests helped us to assess the discriminatory power of features across different classes, specifically between readmitted (RA) and not readmitted (NonRA) patients.\n\nWe also utilized 5-fold cross-validation to estimate the testing accuracies of our models. This technique helps in evaluating the model's performance and generalizability by dividing the data into five subsets, training the model on four subsets, and testing it on the remaining one, repeating this process five times.\n\nThe performance metrics, such as accuracy and AUC, were calculated for various models, including those based on diagnoses, procedures, medications, and laboratory values. For instance, the composite model, which combined features from different data modalities, achieved an AUC of 0.78 and an accuracy of 83.19%. These metrics provide a measure of the model's ability to discriminate between readmitted and not readmitted patients.\n\nWhile we did not explicitly state confidence intervals for the performance metrics, the use of cross-validation and statistical tests ensures that our results are robust and that the features identified are significantly associated with readmission rates. The statistical significance of the features was validated through orthogonal methods, providing confidence in the superiority of our approach compared to traditional methods and baselines.",
  "evaluation/availability": "Not enough information is available."
}