{
  "publication/title": "Evaluation of predictive models to determine total morbidity outcome of feedlot cattle based on cohort-level feed delivery data during the first 15 days on feed.",
  "publication/authors": "Heinen L, Lancaster PA, White BJ, Zwiefel E",
  "publication/journal": "Translational animal science",
  "publication/year": "2022",
  "publication/pmid": "36172463",
  "publication/pmcid": "PMC9512095",
  "publication/doi": "10.1093/tas/txac121",
  "publication/tags": "- Feeding patterns\n- Machine learning\n- Predictive analytics\n- Bovine respiratory disease\n- Feedlot cattle\n- Morbidity prediction\n- Data transformation\n- Predictive modeling\n- Animal health\n- Feed intake analysis",
  "dataset/provenance": "The dataset used in this study was sourced from feedlots, encompassing a total of 518 cohorts, which included 56,796 animals. These cohorts varied in terms of sex, age, arrival weight, and arrival time of year over a 2-year period. The data consisted of feed delivery information collected during the first 15 days of the feeding period, along with arrival characteristics of the cattle. This dataset was specifically curated to predict total morbidity outcomes over the entire feeding period.\n\nThe data underwent a rigorous transformation process, including the application of inclusion criteria to ensure completeness and accuracy. Cohorts with missing data or extreme values were removed to minimize data entry errors. Additionally, cohorts designated as hospital pens or those with average arrival weights outside the specified range were excluded. This process resulted in a refined dataset that was used to train and test five predictive models: advanced perceptron, decision forest, logistic regression, neural network, and boosted decision tree.\n\nThe dataset was split into training and testing subsets, with the training set representing 75% of the original data and the testing set representing the remaining 25%. This split was stratified by the prevalence of the outcome of interest, ensuring a balanced representation of high and low morbidity cohorts in both subsets. The models were generated using Microsoft Azure, and their performance was evaluated based on metrics such as accuracy, sensitivity, specificity, positive and negative predictive values, and the area under the receiver operating characteristic curve.\n\nThe dataset utilized in this study builds upon previous work conducted by Rojas et al. and Amrine et al., leveraging the resources available on Azure to develop and evaluate predictive models for cohort-level morbidity in feedlot cattle. The focus on feed delivery data during the initial 15 days of feeding highlights the potential of this information as a significant predictor of overall morbidity outcomes.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a testing set. The training set comprised 75% of the original dataset, while the testing set made up the remaining 25%. This division was stratified by the prevalence of the outcome of interest, ensuring that both sets had a representative distribution of high and low morbidity cohorts. The training set was used to build and train the predictive models, while the testing set was used to evaluate the performance of these models. The final dataset consisted of 518 cohorts, with the training set containing approximately 389 cohorts and the testing set containing around 129 cohorts.",
  "dataset/redundancy": "The dataset was split into training and testing sets to ensure that the models could be trained and evaluated independently. The split was stratified by the prevalence of the outcome of interest, which was the total morbidity category. This means that the distribution of high and low morbidity cohorts was maintained in both the training and testing sets.\n\nThe training set represented 75% of the original dataset, while the testing set represented the remaining 25%. This split was enforced to ensure that the models were trained on a representative sample of the data and that their performance could be evaluated on an independent set of data.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the agricultural industry. The use of a stratified split ensures that the models are trained and evaluated on data that is representative of the overall population, which is a common practice in machine learning.\n\nThe independence of the training and testing sets was enforced by ensuring that there was no overlap between the cohorts in the two sets. This means that the models were trained on one set of data and evaluated on a completely separate set of data, which helps to ensure that the results are generalizable to new, unseen data.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are not new. They are well-established methods in the field of predictive modeling. The algorithms employed include advanced perceptron, neural network, boosted decision tree, decision forest, and logistic regression. These models were chosen based on previous work and the resources available on Azure.\n\nThe advanced perceptron and neural network models are types of artificial neural networks, which are useful for identifying patterns in operational data. However, they are less interpretable in terms of model structure or the importance of predictive variables.\n\nBoosted decision trees and decision forests are classification models that create a series of splits in data based on attributes to minimize entropy in resulting data subsets. These models are known for their ability to handle complex interactions between variables and provide good predictive performance.\n\nLogistic regression is a statistical method often used for binary classification problems. It estimates the probability of an event occurring based on one or more predictor variables.\n\nThe decision to use these specific models was influenced by previous research conducted by Rojas et al. and Amrine et al., as well as the computational resources and tools available in Azure Machine Learning Studio. The study aimed to evaluate the performance of these models in predicting the total morbidity outcome of feedlot cattle based on cohort-level feed delivery data during the first 15 days on feed.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data underwent several transformations and preprocessing steps before being used in the predictive models. Initially, inclusion criteria were applied to ensure that only cohorts with complete arrival and feeding data were considered. Feed delivery data beyond 15 days on feed were excluded, and the remaining data were adjusted for arrival body weight on a dry matter basis. Cohorts with missing data or extreme values for certain arrival characteristics or dry matter intake were removed to reduce data entry errors. Specifically, cohorts were excluded if they were designated as hospital pens, had average arrival weights outside the range of 182 kg to 545 kg, had days on feed outside the range of 0 to 300, had not yet been closed out, or had dry matter intake as a percentage of average arrival body weight outside the range of 0.1% to 5% on any given day within the first 15 days on feed. After applying these exclusion criteria, 12,139 cohorts were removed from the dataset.\n\nData wrangling techniques in R software were then used to create a dataset consisting of one row per cohort. Several additional variables were generated to describe feed delivery characteristics, such as day-to-day changes in feed delivery and rolling averages of feed delivery using 2-to-7-day time spans. The outcome of interest, total morbidity category, was captured as a categorical variable indicating whether the total morbidity percentage was high (greater than or equal to 15%) or low (less than 15%).\n\nBefore model building, the data were split into training and testing sets, stratified by the prevalence of the outcome of interest. The training set represented 75% of the original dataset, while the testing set represented the remaining 25%. This split ensured that the models were trained on a representative sample of the data and evaluated on a separate, unbiased subset. The final dataset consisted of 518 cohorts, encompassing 56,796 animals, with 18.5% of cohorts having high morbidity.",
  "optimization/parameters": "In our study, the number of input parameters used in the models was determined through a series of data transformation and variable creation steps. Initially, we applied inclusion criteria to the data, which involved removing cohorts with incomplete or extreme values for certain arrival characteristics or dry matter intake. This process resulted in the removal of many rows of data, ensuring that only relevant and high-quality data were used for model training.\n\nSeveral additional variables were created using the existing data to describe feed delivery characteristics. These included day-to-day changes in feed delivery and rolling averages of feed delivery using 2-to-7-day time spans. The outcome of interest was captured in a variable called total morbidity category, which described the total morbidity of a cohort of cattle during the entire feeding period. This variable was expressed as a categorical variable, with high morbidity indicating a total morbidity percentage of greater than or equal to 15%, and low morbidity indicating a total morbidity percentage of less than 15%.\n\nFollowing data transformation and variable creation, the final dataset consisted of 518 cohorts with 18.5% of cohorts having high morbidity. The dataset included a variety of variables related to feeding characteristics, arrival characteristics, and the outcome variable. These variables were used as input parameters for the five predictive models that were trained and tested in this study.\n\nThe models selected for investigation in this study were advanced perceptron, neural network, boosted decision tree, decision forest, and logistic regression. The selection of these models was based on previous work conducted by Rojas et al. (2022) and Amrine et al. (2014), as well as the resources available on Azure. The number of input parameters used in each model was determined by the variables included in the final dataset, which were carefully selected and transformed to ensure their relevance and quality.",
  "optimization/features": "The study utilized a comprehensive set of features derived from feed delivery data and arrival characteristics to predict total morbidity in feedlot cattle. The final dataset consisted of 18.5% of cohorts having high morbidity, with a total of 518 cohorts (56,796 animals) included in the analysis.\n\nThe input features were carefully selected and transformed to ensure they were relevant and informative for the predictive models. Several additional variables were created using existing data to describe feed delivery characteristics, such as day-to-day changes in feed delivery and rolling averages of feed delivery using 2-to-7-day time spans. These variables were calculated for their respective increments throughout the 15-day feeding period of interest.\n\nThe dataset included a variety of feeding variables, such as feed intake measured by DMI-BW for each day starting on the day of arrival (0) to day 15, percent change in feed intake measured by DMI-BW between sequential days, and rolling averages in various increments of percent change in DMI-BW. Additionally, arrival characteristics such as arrival date, average arrival weight, sex, and arrival animal count were included.\n\nFeature selection was performed to ensure that only the most relevant variables were used in the predictive models. This process involved removing cohorts with missing data, extreme values, or those designated as hospital pens. The final dataset consisted of 18 variables that populated the dataset, which were used to train and test the five predictive models.\n\nThe feature selection process was conducted using the training set only, ensuring that the testing set remained unbiased and could be used to evaluate the performance of the models accurately. This approach helped to maximize the predictive power of the models while maintaining their generalizability to new, unseen data.",
  "optimization/fitting": "The study employed five different predictive models to determine the ability of feed delivery data from the first 15 days to predict total feeding period morbidity. These models included advanced perceptron, decision forest, logistic regression, neural network, and boosted decision tree. The dataset consisted of 518 cohorts, with a total of 56,796 animals, and was split into training and testing subsets, stratified by the outcome of interest. The training set represented 75% of the original dataset, while the testing set represented the remaining 25%.\n\nGiven the complexity of the models, particularly the neural network and advanced perceptron, there was a potential risk of overfitting. To mitigate this, the dataset was split into training and testing sets, ensuring that the models were evaluated on unseen data. This approach helped in assessing the generalizability of the models. Additionally, the threshold probability for each model was manually adjusted to maximize the F1 score, balancing sensitivity and positive predictive value. This step was crucial in ensuring that the models were not overly complex and were able to generalize well to new data.\n\nThe decision forest and boosted decision tree models, which are based on creating a series of splits in the data, inherently have a mechanism to prevent overfitting through techniques like pruning and limiting the depth of the trees. The logistic regression model, being a simpler model with fewer parameters, was less prone to overfitting. The neural network and advanced perceptron models, while more complex, were evaluated using the testing set to ensure they did not overfit the training data.\n\nUnderfitting was addressed by selecting models that were known to be effective in similar contexts and by ensuring that the models had enough complexity to capture the underlying patterns in the data. The decision forest and boosted decision tree models, for instance, are known for their ability to capture complex relationships in the data. The logistic regression model, while simpler, provided a baseline for comparison. The neural network and advanced perceptron models were chosen for their ability to identify patterns in operational data, although they are more challenging to interpret.\n\nIn summary, the study employed a combination of model complexity and evaluation on unseen data to balance the risk of overfitting and underfitting. The use of multiple models and the adjustment of threshold probabilities ensured that the models were robust and generalizable.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models used in this study vary in their interpretability, ranging from black-box models to more transparent ones. The advanced perceptron and neural network models are considered black-box models. These models are powerful for identifying patterns in operational data but are difficult to interpret in terms of model structure or the importance of predictive variables. They do not provide clear insights into which features are driving the predictions, making it challenging to understand the underlying mechanisms.\n\nOn the other hand, the decision tree, decision forest, and logistic regression models offer more transparency. These models create a series of splits in the data based on attributes to minimize entropy in resulting data subsets, making them easier to interpret. For instance, decision trees and forests can visually represent the decision-making process, showing how different variables contribute to the final prediction. This transparency allows for a better understanding of which feeding characteristics are most predictive of morbidity.\n\nLogistic regression models are also interpretable, as they provide coefficients that indicate the strength and direction of the relationship between each predictor and the outcome. This makes it straightforward to identify which variables are most important in predicting morbidity. Additionally, feature importance obtained from the Microsoft Azure platform for these models highlighted that feeding data, particularly changes in feed delivery early in the feeding period, were among the top predictors. This information can be crucial for feedlot managers looking to implement interventions based on model predictions.",
  "model/output": "The models developed in this study are classification models. They are designed to predict the total morbidity outcome of feedlot cattle cohorts, categorizing them into high (\u2265 15% total morbidity) or low morbidity categories. The models utilize various machine learning algorithms, including advanced perceptron, decision forest, logistic regression, neural network, and boosted decision tree, to classify the cohorts based on their feeding characteristics during the first 15 days on feed and arrival characteristics.\n\nThe performance of these models was evaluated using metrics such as accuracy, sensitivity, and specificity. Sensitivity measures the ability of the models to correctly identify high morbidity cohorts, while specificity measures their ability to correctly identify low morbidity cohorts. The decision forest model, for instance, demonstrated the highest specificity, accurately identifying low morbidity lots with a high success rate. On the other hand, the logistic regression and neural network models showed similar sensitivity, indicating their effectiveness in correctly identifying high morbidity cohorts.\n\nThe output of these models provides valuable insights into the predictive power of feed delivery data in determining the health outcomes of cattle cohorts. By analyzing the changes in feed delivery and other relevant variables, the models can help in early identification of high morbidity cohorts, facilitating timely interventions to improve health and economic outcomes in feedlots.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method involved using a separate testing dataset to assess the performance of the predictive models. The dataset was initially split into training and testing sets, with the training set comprising 75% of the original data and the testing set making up the remaining 25%. This split was stratified by the prevalence of the outcome of interest to ensure a representative distribution of high and low morbidity cohorts in both sets.\n\nThe models were trained using the training set and then evaluated using the testing set. The performance metrics used to evaluate the models included accuracy, sensitivity, specificity, positive and negative predictive values, and the area under the receiver operating characteristic (ROC) curve. These metrics were calculated using confusion matrices generated by each model run in Azure.\n\nTo optimize the models' performance, the threshold probability was manually adjusted for each model to maximize the F1 score, which balances sensitivity and positive predictive value. This adjustment helped in fine-tuning the models to better distinguish between high and low morbidity cohorts.\n\nThe ROC curves for the five predictive models were analyzed to compare their sensitivity and specificity. Models with ROC curves closer to the top left corner of the plot were considered to have better predictive ability. Additionally, the overall accuracy and area under the ROC curve (AUC) values were used to support the conclusions about the models' performance.\n\nThe decision forest model, for instance, demonstrated the highest specificity, accurately identifying low morbidity cohorts, but had lower sensitivity. In contrast, the logistic regression and neural network models showed similar sensitivity and specificity, indicating a balanced ability to correctly identify both high and low morbidity cohorts. The advanced perceptron model, however, exhibited poor performance, as it identified all cohorts as high morbidity, resulting in a sensitivity of 100% but low overall accuracy.",
  "evaluation/measure": "The performance metrics reported in this study include accuracy, sensitivity, specificity, positive and negative predictive values, and the area under the receiver operating characteristic curve (AUC). These metrics were calculated using confusion matrices produced by each model run in Azure.\n\nAccuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, also known as recall or true positive rate, indicates the ability of the model to correctly identify high morbidity cohorts. Specificity, or the true negative rate, shows the model's ability to correctly identify low morbidity cohorts. Positive predictive value (PPV) is the proportion of positive results that are true positives, while negative predictive value (NPV) is the proportion of negative results that are true negatives. The AUC provides an aggregate measure of performance across all classification thresholds.\n\nThese metrics are widely used in the literature for evaluating the performance of predictive models, particularly in medical and biological sciences. They provide a comprehensive view of the model's ability to correctly classify cases, balance between sensitivity and specificity, and handle different types of errors. The use of these metrics ensures that the evaluation is representative and comparable to other studies in the field.",
  "evaluation/comparison": "The study did not involve a direct comparison to publicly available methods on benchmark datasets. Instead, the focus was on evaluating the performance of five specific predictive models\u2014advanced perceptron, decision forest, logistic regression, neural network, and boosted decision tree\u2014using a dataset specific to the research objectives. These models were chosen based on previous work and the resources available on Azure.\n\nA comparison to simpler baselines was not explicitly detailed. However, the evaluation metrics used, such as accuracy, sensitivity, specificity, positive and negative predictive values, and the area under the receiver operating characteristic curve, provided a comprehensive assessment of each model's performance. The decision forest model, for instance, demonstrated the highest specificity, indicating its strong ability to accurately identify low morbidity cohorts. Conversely, the logistic regression and neural network models showed similar sensitivity and specificity, highlighting their balanced performance in predicting high morbidity cohorts.\n\nThe models were evaluated using a testing set that represented 25% of the original dataset, ensuring that the performance metrics were derived from data not used in the training process. This approach helped in assessing the generalizability and robustness of the models. The threshold probability for each model was manually adjusted to maximize the F1 score, balancing sensitivity and positive predictive value. This method ensured that the models were optimized for practical application in predicting total feeding period morbidity.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}