{
  "publication/title": "Predicting osteoporotic fractures post-vertebroplasty: a machine learning approach with a web-based calculator.",
  "publication/authors": "Cai S, Liu W, Cai X, Xu C, Hu Z, Quan X, Deng Y, Yao H, Chen B, Li W, Yin C, Xu Q",
  "publication/journal": "BMC surgery",
  "publication/year": "2024",
  "publication/pmid": "38724895",
  "publication/pmcid": "PMC11080251",
  "publication/doi": "10.1186/s12893-024-02427-x",
  "publication/tags": "- Machine Learning\n- Osteoporotic Vertebral Compression Fractures\n- Predictive Modeling\n- Support Vector Machines\n- SHAP Values\n- Web-Based Calculator\n- Clinical Decision-Making\n- Postoperative Complications\n- Feature Selection\n- Boruta Algorithm\n- Decision Curve Analysis\n- Patient Risk Assessment\n- Medical Data Analysis\n- Statistical Analysis\n- Surgical Outcomes",
  "dataset/provenance": "The dataset used in this study was sourced from a retrospective survey of patients admitted to the Department of Spine Surgery of Liuzhou People\u2019s Hospital affiliated with Guangxi Medical University. The data collection period spanned from June 2016 to June 2018. The study focused on patients who underwent percutaneous vertebroplasty (PVP) surgery. A total of 385 patients met the inclusion criteria, with 58 of these patients experiencing new-onset postoperative osteoporotic vertebral compression fractures (OVCF).\n\nThe dataset includes various patient characteristics and medical information. For each patient, median values and interquartile ranges (IQR) were recorded for age, height, weight, body mass index (BMI), and bone mineral density (BMD). Additionally, the median injection volume of bone cement used during surgery, the presence or absence of cement leakage, the median time from hospitalization to surgery, and the median time from injury to surgery were documented. Other relevant data points included whether patients had received anti-osteoporosis treatment, had multiple vertebral fractures at baseline, or were undergoing steroid therapy.\n\nThis dataset has not been used in previous publications by our team or by the broader community. The study was approved by the Institutional Review Board of Liuzhou People\u2019s Hospital, with the approval number 2020 (KY-E-22\u201301). The requirement for informed consent was waived by the Institutional Review Board.",
  "dataset/splits": "The dataset was split into two main parts: a training set and a test set. The training set included data from June 2016 to June 2017, while the test set covered data from July 2017 to June 2018. This time-series division was chosen to better simulate real-world application scenarios.\n\nThe total number of patients included in the study was 385, with 58 patients experiencing new-onset postoperative osteoporotic vertebral compression fractures (OVCF). The specific distribution of data points between the training and test sets is not explicitly detailed, but the time-based split ensures that the model's performance can be evaluated on data that was not used during its training phase. This approach helps in assessing the model's generalizability and robustness in predicting new OVCF cases.",
  "dataset/redundancy": "The dataset used in this study was split into training and test sets based on time. Specifically, data collected between June 2016 and June 2017 was designated as the training set, while data from July 2017 to June 2018 was used as the test set. This time-series division was employed to better simulate the real-world application of the model, ensuring that the training and test sets are independent.\n\nThe independence of the training and test sets was enforced by using a temporal split, which helps to prevent data leakage and ensures that the model's performance is evaluated on data that it has not seen during training. This approach is particularly useful in medical studies where temporal trends can significantly impact the outcomes.\n\nRegarding the distribution of the dataset, it included a total of 385 patients who met the inclusion criteria, with 58 patients experiencing new-onset postoperative OVCF. The dataset was analyzed using chi-square and independent samples t-tests to compare the differences between the groups with and without new-onset OVCF. The results indicated that there were significant differences in weight, BMI, degree of osteoporosis, and the use of anti-osteoporosis treatment between the two groups. Additionally, the new OVCF group had a higher proportion of patients with multiple vertebral fractures and steroid use.\n\nThe correlation heat map revealed that body weight was correlated with BMI and had some correlation with height. This detailed analysis ensures that the dataset is robust and that the model's predictions are based on relevant and significant variables. The distribution of the dataset is comparable to previously published machine learning datasets in the medical field, focusing on the predictive power of models built from discrete data.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are not new. They are well-established methods commonly employed in predictive modeling. The algorithms include logistic regression, decision tree (DT), random forest (RF), extreme gradient boosting (XGBoost), light gradient boosting machine (LightGBM), elastic network K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and Multi-Layer Perceptual Machines (MLP). These algorithms were selected to comprehensively evaluate their performance on the dataset, ensuring a robust comparison across different model types.\n\nThe choice of these algorithms was driven by their proven effectiveness in handling discrete data and their ability to outperform traditional regression methods in terms of predictive power. The study aimed to identify the most suitable algorithm for predicting new-onset osteoporotic vertebral compression fractures (OVCF) in patients treated with percutaneous vertebroplasty (PVP).\n\nThe algorithms were evaluated using a tenfold cross-validation approach on the training set, which helped in assessing the robustness and generalization ability of the models. The Support Vector Machine (SVM) was ultimately identified as the best-performing model based on the area under the receiver operating characteristic curve (ROC_AUC) and decision curve analysis (DCA).\n\nThe study did not introduce a new machine-learning algorithm. Instead, it focused on applying and validating existing algorithms to a specific medical prediction problem. The results were published in a medical journal, BMC Surgery, as the primary goal was to contribute to the field of spine surgery and clinical decision-making rather than to the development of new machine-learning techniques.",
  "optimization/meta": "The model developed in this study does not function as a meta-predictor. Instead, it employs a single machine-learning algorithm, specifically the Support Vector Machine (SVM), which was identified as the best-performing model through a comprehensive evaluation process.\n\nThe evaluation involved nine different machine-learning algorithms: logistic regression, decision tree (DT), random forest (RF), extreme gradient boosting (XGBoost), light gradient boosting machine (LightGBM), elastic network K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and Multi-Layer Perceptual Machines (MLP). These algorithms were assessed using a tenfold cross-validation approach on the training set to ensure robustness and generalization.\n\nThe SVM model was selected based on its superior performance in distinguishing the risk of novel osteoporotic vertebral compression fractures (OVCF) after percutaneous vertebroplasty. The training data used for this model was independent, as it was divided into training and testing groups using time as a separation criterion. This approach aimed to validate the model's generalizability, although further real-world data validation is recommended.\n\nThe study did not combine predictions from multiple machine-learning algorithms to form a meta-predictor. Instead, it focused on optimizing and interpreting the best-performing single algorithm, SVM, to provide accurate and clinically useful predictions.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms. We began by identifying the most relevant features using the Boruta algorithm, which iteratively removes the least important features, ensuring that only variables with true predictive power are included in the modeling process. This feature selection method compares the importance of real features with shadow features, which are generated by shuffling the values of the real features. This approach helped us highlight significant factors such as BMI, weight, steroid use, length of hospital stay, and surgery duration, which were deemed important for predicting the risk of new osteoporotic vertebral compression fractures (OVCF).\n\nFollowing feature selection, we preprocessed the data to handle missing values and normalize the features. Missing values were imputed using appropriate techniques to maintain the integrity of the dataset. Normalization was performed to ensure that all features contributed equally to the model's predictions, preventing any single feature from dominating due to its scale.\n\nCategorical variables were encoded using techniques such as one-hot encoding, which converts categorical data into a format that can be provided to ML algorithms to do a better job in prediction. Continuous variables were standardized to have a mean of zero and a standard deviation of one, which is essential for algorithms that are sensitive to the scale of the data.\n\nAdditionally, we performed a tenfold cross-validation approach on the training set to ensure the robustness and generalization ability of the models. This method involved dividing the training data into ten subsets, training the model on nine subsets, and validating it on the remaining subset. This process was repeated ten times, with each subset serving as the validation set once, providing a comprehensive evaluation of the model's performance.\n\nIn summary, our data encoding and preprocessing steps involved feature selection using the Boruta algorithm, handling missing values, normalizing continuous variables, encoding categorical variables, and employing tenfold cross-validation. These steps were essential in preparing the data for the machine-learning algorithms, ensuring that the models were robust, generalizable, and capable of making accurate predictions.",
  "optimization/parameters": "In our study, we employed a feature selection technique called Boruta to identify the most relevant features from the training set that significantly contribute to the prediction of new osteoporotic vertebral compression fractures (OVCF). This iterative process ensures that only variables with true predictive power are included in the subsequent modeling, thereby optimizing the number of input parameters (p).\n\nThe selection of p was not arbitrary but rather a result of a systematic approach. We started with a comprehensive set of potential features and iteratively removed the least important ones. This method allowed us to refine the model by retaining only those features that had a substantial impact on the predictive power. As a result, the final model includes a subset of features that are crucial for accurate predictions.\n\nThe exact number of parameters (p) used in the model can vary depending on the dataset and the specific features identified as important by the Boruta algorithm. However, the process ensures that the model is parsimonious, including only the necessary features to maintain robustness and generalization ability. This approach not only improves the model's performance but also enhances its interpretability, making it more practical for clinical applications.",
  "optimization/features": "In our study, we employed a rigorous feature selection process to identify the most relevant variables for predicting novel osteoporotic vertebral compression fractures (OVCF). We utilized Boruta, an iterative feature selection algorithm that removes the least important features progressively. This method ensures that only variables with true predictive power are retained for subsequent modeling.\n\nThe feature selection was conducted exclusively on the training set, ensuring that the selected features are unbiased and generalizable to new, unseen data. This approach helps in building robust prediction models that can effectively distinguish the risk of novel OVCF.\n\nThe specific number of features (f) used as input varies as the Boruta algorithm iteratively refines the feature set. However, the process guarantees that the final set of features is optimized for predictive accuracy and clinical relevance.\n\nNot applicable",
  "optimization/fitting": "In our study, we employed nine different machine learning algorithms to predict the risk of new osteoporotic vertebral compression fractures (OVCF) after percutaneous vertebroplasty. These algorithms included logistic regression, decision tree, random forest, extreme gradient boosting, light gradient boosting machine, elastic network, K-Nearest Neighbors, Support Vector Machines, and Multi-Layer Perceptron. To ensure the robustness and generalization ability of the models, we used a tenfold cross-validation approach on the training set.\n\nThe number of parameters in our models varied depending on the algorithm. For instance, tree-based models like random forest and gradient boosting machines have a large number of parameters due to the numerous decision trees involved. Conversely, simpler models like logistic regression have fewer parameters.\n\nTo address the risk of overfitting, especially in models with a large number of parameters, we implemented several strategies. First, we used tenfold cross-validation, which helps in assessing the model's performance on different subsets of the data and ensures that the model generalizes well to unseen data. Second, we utilized learning curves to monitor the model's performance as the size of the training set increases. This helps in identifying whether the model is overfitting or underfitting. Additionally, we employed the Boruta algorithm for feature selection, which iteratively removes the least important features, ensuring that only variables with true predictive power are included in the modeling process.\n\nTo rule out underfitting, we ensured that our models were complex enough to capture the underlying patterns in the data. We monitored the performance metrics, such as the area under the receiver operating characteristic curve (ROC AUC), to ensure that the models were not too simplistic. Furthermore, we used techniques like SHAP (SHapley Additive exPlanations) values to interpret the model's predictions and understand the contribution of each feature. This helped in fine-tuning the models to ensure they were neither too simple nor too complex.\n\nIn summary, we carefully balanced the complexity of our models to avoid both overfitting and underfitting. By using cross-validation, learning curves, feature selection, and interpretability techniques, we ensured that our models were robust and generalizable.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method used was tenfold cross-validation. This approach involved dividing the training data into ten subsets, training the model on nine of these subsets, and validating it on the remaining one. This process was repeated ten times, with each subset serving as the validation set once. This technique helps to ensure that the model generalizes well to unseen data and does not overfit to the training set.\n\nAdditionally, we utilized the Boruta algorithm for feature selection. Boruta works by iteratively removing the least important features, ensuring that only those with true predictive power are included in the modeling process. This not only simplifies the model but also reduces the risk of overfitting by eliminating irrelevant or noisy features.\n\nFurthermore, we evaluated model performance using learning curves. These curves compare the model's performance on the training set and the test set as the number of training samples increases. A stable performance on both sets indicates that the model generalizes well and does not overfit.\n\nIn summary, our use of tenfold cross-validation, the Boruta algorithm for feature selection, and learning curve analysis collectively helped to prevent overfitting and enhance the reliability of our predictive models.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model developed in this study is not a black box; significant efforts have been made to ensure its transparency and interpretability. To achieve this, several techniques were employed.\n\nFirstly, a web calculator was created to visualize the model's predictions. This tool allows clinicians to input patient characteristics and quickly obtain a prediction of the risk of a novel osteoporotic vertebral compression fracture (OVCF). This interactive approach helps in understanding how different patient features influence the model's output.\n\nSecondly, learning curves were used to assess the model's performance under different training set sizes. This provides insights into how well the model generalizes to new data as more training samples are included.\n\nAdditionally, SHAP (SHapley Additive exPlanations) values were utilized to interpret the model. SHAP values explain the contribution of each feature to the model\u2019s predictions, making it clear how individual patient characteristics affect the risk assessment. For instance, higher weights and longer operative times positively influence the model\u2019s prediction of a higher risk of postoperative complications. The SHAP value plots visually express how each feature shifts the probability from the baseline value, with color coding (red for higher values and blue for lower values) to aid in intuitive interpretation.\n\nFeature importance rankings were also demonstrated, showing which variables have the greatest impact on model predictions. For example, 'Weight' was identified as the most influential feature, followed by 'Steroid use' and 'Anti-osteoporosis medication'. This ranking helps in understanding which patient characteristics are crucial for the model's decisions.\n\nMoreover, the distribution of SHAP values for both categorical and continuous variables was visualized. These plots provide a detailed understanding of how individual values of each feature affect the model output, further enhancing the model's transparency.\n\nIn summary, the model's interpretability is ensured through the use of visual tools, learning curves, SHAP values, and feature importance rankings. These techniques collectively make the model transparent and understandable, aiding clinicians in making informed decisions.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the risk of a novel osteoporotic vertebral compression fracture (OVCF) in patients. The model outputs a probability score indicating the likelihood of a patient experiencing a postoperative OVCF. This probability is derived from various patient-specific features, such as weight, body mass index, length of hospitalization, and details of surgeries and treatments. The model uses machine learning algorithms, with the Support Vector Machine (SVM) being identified as the best-performing model. The output is visualized through a web-based calculator, which allows clinicians to input patient data and receive a predicted risk score. This score helps in making informed clinical decisions regarding the management and treatment of patients at risk for postoperative complications. The model's performance is evaluated using metrics like the area under the receiver operating characteristic curve (ROC AUC) and decision curve analysis (DCA), ensuring both statistical validity and practical clinical utility.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the machine learning models and the web calculator is not explicitly released. However, a web-based calculator has been deployed, which integrates the SHAP values to assign specific predicted importance values to each feature. This calculator is accessible via a web-connected device and allows clinicians to enter patient characteristics to quickly obtain a prediction of the risk of a novel osteoporotic vertebral compression fracture (OVCF). The web calculator can be found at the following URL: https://nicolazhang.shinyapps.io/refracture_shap/.\n\nThe web calculator serves as a practical tool for clinical decision-making, enabling real-time, data-driven decisions during clinical care. It represents a more accurate and cost-effective solution than traditional scoring methods. The calculator is designed to be user-friendly, prompting healthcare providers to enter patient-specific data such as weight, body mass index, length of hospitalization, and details of various surgeries, as well as whether treatments such as anti-osteoporosis medications or steroids were used. After submitting the data, the calculator processes these inputs through a machine learning model and outputs a probability score indicating the risk of a fracture. This predictive probability is a nuanced calculation that takes into account the baseline risk and the individual contributions of each input parameter. The SHAP value plot visually expresses how each feature shifts the probability from the baseline value, providing an intuitive visual interpretation of the data.",
  "evaluation/method": "The evaluation method employed in this study was comprehensive and multi-faceted to ensure the robustness and generalizability of the models. Initially, a tenfold cross-validation approach was used on the training set. This method involved dividing the data into ten subsets, training the model on nine of them, and validating it on the remaining one. This process was repeated ten times, with each subset serving as the validation set once. This technique helped to assess the model's performance and prevent overfitting.\n\nFollowing the cross-validation, the models were further evaluated using an external test set. This step involved performing a ROC_AUC analysis to visually assess the discriminative power of the models. The ROC_AUC metric is crucial as it measures the model's ability to distinguish between different outcomes, providing a clear indication of its predictive accuracy.\n\nThe Support Vector Machine (SVM) model demonstrated the highest AUC, indicating its superior ability to discriminate patient prognosis. This was closely followed by the Light Gradient Boosting Machine (LightGBM) and Random Forest (RF) models, which also showed high predictive accuracy but were slightly less effective than the SVM.\n\nIn addition to these statistical evaluations, the models were assessed for their clinical utility using Decision Curve Analysis (DCA). This method ensured that the selected models were not only statistically valid but also had practical value in actual clinical applications.\n\nTo enhance transparency and interpretability, learning curves and SHAP (SHapley Additive exPlanations) values were utilized. The learning curve revealed the model's performance under different training set sizes, while the SHAP values explained the contribution of each feature to the model\u2019s predictions. This approach helped to understand how the model makes its predictions, making it more interpretable for clinicians.\n\nOverall, the evaluation method combined statistical rigor with practical clinical relevance, ensuring that the models developed are both accurate and useful in real-world settings.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our machine learning models in predicting new osteoporotic vertebral compression fractures (OVCF) after percutaneous vertebroplasty. The primary metric used was the Area Under the Receiver Operating Characteristic Curve (ROC_AUC). This metric is crucial as it provides a comprehensive measure of a model's ability to distinguish between patients at risk of developing new OVCF and those who are not. The ROC_AUC was calculated over tenfold cross-validation to ensure the robustness and generalization ability of the models.\n\nIn addition to ROC_AUC, we utilized Decision Curve Analysis (DCA) to assess the clinical utility of the selected models. DCA is particularly important because it evaluates not only the statistical validity of the models but also their practical value in real-world clinical applications. This ensures that the models we developed are not just theoretically sound but also beneficial for patient care.\n\nThe ROC_AUC and DCA metrics are widely recognized in the literature for evaluating predictive models, making our set of metrics representative and comparable to other studies in the field. These metrics provide a clear and comprehensive evaluation of model performance, ensuring that our findings are both reliable and applicable in clinical settings.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on evaluating the performance of various machine learning algorithms on our specific dataset related to predicting new osteoporotic vertebral compression fractures (OVCF) after percutaneous vertebroplasty.\n\nWe selected nine different machine learning algorithms to fully consider the performance of different types of models on our dataset. These algorithms included logistic regression, decision tree (DT), random forest (RF), extreme gradient boosting (XGBoost), light gradient boosting machine (LightGBM), elastic network K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and Multi-Layer Perceptual Machines (MLP). This approach allowed us to compare the effectiveness of different modeling techniques in predicting the risk of new OVCF.\n\nTo ensure the robustness and generalization ability of the models, we used a tenfold cross-validation approach on the training set. This method helped us to identify the best-performing models and to evaluate their performance in distinguishing the risk of novel OVCF.\n\nIn terms of simpler baselines, we did not explicitly compare our models to simpler baselines such as traditional regression methods. However, it is noted that machine learning algorithms generally outperform traditional regression methods in terms of predictive power when dealing with discrete data. This observation guided our choice of algorithms for the study.\n\nThe performance of the models was evaluated using the ROC_AUC metric, which serves as an important performance metric. After selecting the best models based on ROC_AUC, we further used Decision Curve Analysis (DCA) to evaluate the performance of the models in terms of clinical utility. This ensured that the selected models were not only statistically valid but also had practical value in actual clinical applications.\n\nIn summary, while we did not perform a direct comparison to publicly available methods or simpler baselines on benchmark datasets, our study involved a comprehensive evaluation of multiple machine learning algorithms using cross-validation and clinical utility metrics. This approach allowed us to identify the most effective models for predicting the risk of new OVCF.",
  "evaluation/confidence": "The evaluation of our models involved a rigorous process to ensure confidence in the results. We employed a tenfold cross-validation approach on the training set, which helps to assess the robustness and generalization ability of the models. This method provides a more reliable estimate of model performance by averaging the results over multiple folds, reducing the risk of overfitting and providing a more stable performance metric.\n\nFor performance evaluation, we primarily used the ROC_AUC metric, which serves as a crucial indicator of a model's ability to distinguish between different outcomes. The ROC_AUC values were calculated for each of the nine machine learning algorithms we tested, and the results were compared to identify the best-performing models. The use of ROC_AUC ensures that we are considering both the true positive rate and the false positive rate, providing a comprehensive view of model performance.\n\nIn addition to ROC_AUC, we utilized Decision Curve Analysis (DCA) to evaluate the clinical utility of the selected models. DCA helps to determine the net benefit of using a model for clinical decision-making across different threshold probabilities. This analysis ensures that the models not only perform well statistically but also have practical value in real-world clinical applications.\n\nStatistical significance was assessed using p-values, with a threshold of less than 0.05 considered statistically significant. This threshold was applied to various comparisons, including baseline characteristics and model performance metrics, to ensure that any observed differences were not due to random chance.\n\nThe learning curves were also analyzed to understand the model's performance under different training set sizes. These curves help to visualize how well the model generalizes to unseen data as the amount of training data increases, providing further confidence in the model's robustness.\n\nOverall, the combination of cross-validation, ROC_AUC, DCA, and statistical significance testing ensures that our evaluation is thorough and that the conclusions drawn are reliable and applicable in clinical settings.",
  "evaluation/availability": "Not enough information is available."
}