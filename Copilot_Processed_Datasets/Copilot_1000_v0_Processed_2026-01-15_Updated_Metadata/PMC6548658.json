{
  "publication/title": "Deep Learning Predicts Lung Cancer Treatment Response from Serial Medical Imaging.",
  "publication/authors": "Xu Y, Hosny A, Zeleznik R, Parmar C, Coroller T, Franco I, Mak RH, Aerts HJWL",
  "publication/journal": "Clinical cancer research : an official journal of the American Association for Cancer Research",
  "publication/year": "2019",
  "publication/pmid": "31010833",
  "publication/pmcid": "PMC6548658",
  "publication/doi": "10.1158/1078-0432.ccr-18-2495",
  "publication/tags": "- Deep Learning\n- Biomarkers\n- Survival Prediction\n- Lung Cancer\n- Radiotherapy\n- CT Scans\n- Recurrent Neural Networks\n- Convolutional Neural Networks\n- Kaplan-Meier Curves\n- Prognostic Factors\n- Transfer Learning\n- ImageNet\n- Clinical Endpoints\n- Statistical Analysis\n- Machine Learning in Medicine",
  "dataset/provenance": "The datasets used in this study were sourced from patients treated at a specific institution. Two independent cohorts were analyzed, comprising a total of 268 stage III non-small cell lung cancer (NSCLC) patients. The first dataset, referred to as Dataset A, included 179 consecutive patients who were treated with definitive chemoradiation therapy between 2003 and 2014. This dataset contained a total of 581 CT scans, with an average of 3.2 scans per patient, ranging from 2 to 4 scans. The scans were taken at various time points: pre-treatment and follow-up scans at 1, 3, and 6 months after radiation therapy. The second dataset, Dataset B, consisted of 89 consecutive patients treated with neoadjuvant radiotherapy and chemotherapy prior to surgical resection between 2001 and 2013. This dataset included 178 CT scans with two time points: scans taken prior to radiation therapy and after the completion of radiation therapy.\n\nThe data used in this study has not been previously published or used by the community in other papers. The datasets were specifically curated for this research to evaluate the use of deep learning models in predicting survival and other clinical endpoints in NSCLC patients. The CT scans included in the datasets were acquired as part of standard clinical practice, with variations in contrast administration based on clinical guidelines. The datasets represent a realistic clinical setting, where not all patients received imaging scans at all specified time points.",
  "dataset/splits": "In our study, we utilized two independent datasets, referred to as Dataset A and Dataset B. Dataset A was split into two main parts: a training/tuning set and a test set. The split ratio was 2:1, resulting in 107 patients in the training/tuning set and 72 patients in the test set. Additionally, for robust training, we employed Monte Carlo cross-validation with 10 different splits. Each of these splits further divided the training/tuning set into a 3:2 ratio, creating multiple iterations for training and tuning.\n\nDataset B, which served as an external validation set, was not split further and consisted of 89 patients. This dataset was used to evaluate the generalizability of our models trained on Dataset A.\n\nThe distribution of data points in each split was designed to ensure that the models were trained and validated on diverse and representative samples, allowing for reliable performance evaluation.",
  "dataset/redundancy": "Two independent cohorts were used for this analysis, referred to as Dataset A and Dataset B. Dataset A consisted of 179 patients who were treated with definitive chemoradiation therapy. This dataset was randomly split into a training/tuning set and a test set in a 2:1 ratio, resulting in 107 patients for training/tuning and 72 patients for testing. The test set was independent of the training process, ensuring that the model's performance could be evaluated on unseen data.\n\nTo further validate the model, an additional test was performed on Dataset B, which comprised 89 patients treated with neoadjuvant radiotherapy and chemotherapy prior to surgical resection. This dataset served as an external validation set, providing a range of standard of care treatment protocols.\n\nThe distribution of patient parameters in the training and test sets of Dataset A did not show significant differences, indicating a balanced split. This careful splitting and independent testing approach is crucial for assessing the generalizability and robustness of the deep learning models developed in this study.\n\nThe use of independent test sets and the random splitting of Dataset A aligns with best practices in machine learning, ensuring that the models are evaluated on data that was not used during training. This methodology helps to mitigate overfitting and provides a more reliable assessment of the models' performance.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is convolutional neural networks (CNNs), specifically the ResNet architecture. This is combined with recurrent neural networks (RNNs) using gated recurrent units (GRUs) to handle longitudinal data. The use of transfer learning with weights pre-trained on ImageNet allows the model to leverage features learned from a large dataset of natural images, which is particularly useful given the relatively smaller size of our medical imaging dataset.\n\nThe algorithm is not entirely new, as ResNet and GRUs are well-established architectures in the field of deep learning. However, the application of these architectures to predict survival and treatment response in non-small cell lung cancer (NSCLC) patients using CT scans is novel. The integration of these networks for this specific medical task and dataset is what sets our work apart.\n\nThe focus of our publication is on the application of these machine-learning techniques to a specific medical problem, rather than the development of a new algorithm. Therefore, it is published in a clinical research journal rather than a machine-learning journal. The primary contribution of our work lies in demonstrating the potential of deep learning in improving prognostic assessments in oncology, rather than in the development of new machine-learning algorithms.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The deep learning model is based on a convolutional neural network (CNN) pre-trained on ImageNet, which is then applied to our dataset using transfer learning. The CNN extracts features from CT images at different time points, which are then fed into a recurrent neural network (RNN) for longitudinal analysis. The RNN uses gated recurrent units (GRU) to handle the time domain and missing scans. The final output is a binary classification using a softmax layer.\n\nThe model was trained and evaluated using two datasets. Dataset A consisted of 179 patients treated with definitive radiation therapy, which was split into training/tuning and test sets. The training was performed with Monte Carlo cross-validation using 10 different splits. The test set of 72 patients was independent and not used in the training process. Dataset B, consisting of 89 patients treated with trimodality, served as an external test set. The survival predictions for Dataset B were made using the one-year survival model trained on Dataset A. The independence of the training data is ensured by the use of separate cohorts for training and testing, as well as the external validation set.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure consistency and optimal input for the neural network. CT scans were acquired using standardized protocols, and the input tumor image region was defined at the center of an identified seed point for both pre-treatment and follow-up scans. These seed points were manually defined using 3D Slicer.\n\nDue to variability in slice thicknesses and in-plane resolution, CT voxels were interpolated to a homogeneous resolution of 1 \u00d7 1 \u00d7 1 mm\u00b3. This interpolation was performed using linear and nearest neighbor methods to maintain tumor details without introducing significant perturbations. The interpolation ensured that the input images were stable and consistent for the proposed architecture.\n\nThree axial slices, each measuring 50 \u00d7 50 mm\u00b2, were selected as inputs to the model. These slices were centered on the seed point and included slices 5 mm proximal and 5 mm distal to the seed point. This approach provided the network with sufficient information to learn from while keeping the number of features manageable compared to a full 3D approach. It also reduced GPU memory usage, training time, and the risk of overfitting.\n\nImage augmentation was applied to the training data, including flipping, translation, rotation, and deformation. These augmentations were performed on both pre-treatment and follow-up images to ensure the network could generalize well across different input series. The deformations were kept within the order of millimeters to avoid altering the morphology of the tumor or surrounding tissues noticeably.\n\nThe neural network structure was implemented in Python using Keras with a TensorFlow backend. The base model utilized a ResNet convolutional neural network (CNN) pre-trained on the ImageNet database, which contains over 14 million natural images. This transfer learning approach leveraged the features learned from a large dataset of natural images to improve the performance on the medical imaging data. For each time point input, a separate CNN was defined, allowing the network to handle multiple time points effectively.",
  "optimization/parameters": "In our study, the model utilized a combination of convolutional neural networks (CNNs) and recurrent layers with gated recurrent units (GRUs). The specific number of parameters (p) in the model is not explicitly stated, as it can vary based on the architecture of the CNNs and the configuration of the recurrent layers. However, it is important to note that the CNNs were pre-trained on ImageNet, a dataset containing 14 million 2D color images, which provided a robust foundation for feature extraction. The additional weights following the CNN were randomized at initialization for transfer learning.\n\nThe selection of parameters was guided by the need to handle missing scans and to incorporate time-domain information. Recurrent neural network (RNN) algorithms were employed to allow for the amalgamation of several time points and to learn from samples with missed patient scans at certain time points. The output of the pre-trained network was masked to skip time points when a scan was not available. Averaging and fully connected layers were applied after the GRU, with batch normalization and dropout after each fully connected layer to prevent overfitting. The final softmax layer allowed for binary classification output.\n\nFor the scenario where follow-up scans were not input, the pre-treatment image alone was used, and the recurrent and average pooling layers were replaced by a fully connected layer. This adjustment was necessary because there was only one input time point in this case.\n\nThe model was trained using Monte Carlo cross-validation with 10 different splits, further divided into training and tuning sets in a 3:2 ratio. This approach ensured that the model was robust and generalizable. The training was performed on 107 patients, with class weight balancing, for up to 300 epochs. The model was evaluated on an independent test set of 72 patients, who were not used in the training process. This rigorous training and evaluation process helped in selecting the optimal parameters for the model.",
  "optimization/features": "The input features for our model primarily consist of CT scan images of patients at various time points. Specifically, the model can handle multiple scans, with the architecture designed to input scans at three different time points into three separate convolutional neural networks (CNNs). This allows the model to leverage temporal information.\n\nFeature selection in the traditional sense was not performed, as the model relies on deep learning techniques to automatically extract relevant features from the input images. Instead of manually selecting features, the model uses a pre-trained ResNet CNN, which was initially trained on a large dataset of 2D color images (ImageNet). This transfer learning approach allows the model to benefit from the features learned on a vast and diverse dataset, which are then fine-tuned on our specific medical imaging data.\n\nThe additional weights following the CNN were randomized at initialization, indicating that while the initial layers benefit from pre-trained weights, the later layers are adapted specifically to our task. This setup ensures that the model can learn relevant features directly from the input images without the need for explicit feature selection.\n\nThe model is designed to handle missing scans by using recurrent neural network (RNN) algorithms, specifically gated recurrent units (GRUs). This allows the model to amalgamate information from several time points and learn from samples with missed patient scans at certain time points. The output of the pre-trained network is masked to skip time points when a scan is not available, ensuring that the model can still make predictions even with incomplete data.\n\nIn summary, the input features are the CT scan images at multiple time points, and the model uses deep learning techniques to automatically extract relevant features without the need for traditional feature selection methods. The use of transfer learning and RNNs allows the model to handle missing data and learn from incomplete datasets.",
  "optimization/fitting": "The fitting method employed in our study utilized a deep learning approach with a convolutional neural network (CNN) architecture, specifically ResNet, which is known for its depth and complexity. The number of parameters in the model is indeed much larger than the number of training points, a common scenario in deep learning with medical imaging data.\n\nTo mitigate the risk of overfitting, several techniques were implemented. First, transfer learning was used, where the CNN was pre-trained on a large dataset of natural images (ImageNet) and then fine-tuned on our medical imaging data. This approach leverages the general features learned from a vast amount of data, making the model more robust and less prone to overfitting on the smaller medical dataset. Additionally, data augmentation techniques were applied to the training tumor region, introducing small-scale deformations to artificially increase the diversity of the training set and prevent the model from memorizing the training data.\n\nRegularization methods such as dropout and batch normalization were also incorporated after each fully connected layer. Dropout randomly sets a fraction of input units to zero at each update during training time, which helps prevent overfitting. Batch normalization normalizes the input of each layer to have a mean of zero and a variance of one, which stabilizes and accelerates the training process.\n\nTo ensure that the model was not underfitting, the performance was evaluated on an independent test set that was not used during the training process. The model's ability to generalize to unseen data was assessed using metrics such as the area under the receiver operator characteristic curve (AUC) and the Kaplan-Meier method for survival analysis. The comparable performance on the training and test sets indicates that the model has learned meaningful patterns from the data and is not underfitting.\n\nFurthermore, Monte Carlo cross-validation was performed using 10 different splits of the data, providing a more robust estimate of the model's performance and reducing the risk of underfitting. The use of class weight balancing during training also helped to address any class imbalances in the dataset, ensuring that the model was not biased towards the majority class.",
  "optimization/regularization": "In our study, several regularization techniques were employed to prevent overfitting. Batch normalization was applied after each fully connected layer to stabilize and accelerate the training process. Additionally, dropout was used after each fully connected layer to randomly set a fraction of input units to zero during training, which helps to prevent the network from becoming too reliant on any single feature. These techniques, combined with image augmentation, which included flipping, translation, rotation, and deformation of the training data, helped to improve the model's generalization performance. The image augmentation ensured that the network could learn robust features that were invariant to small variations in the input images.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are not explicitly detailed in the provided text. However, it is mentioned that training was performed with Monte Carlo cross-validation using 10 different splits, with a further 3:2 split of training to tuning data, for up to 300 epochs. This information provides some insight into the optimization process.\n\nRegarding model files and optimization parameters, there is no specific mention of where these can be accessed or under what license. The text does discuss the use of a ResNet convolutional neural network (CNN) pre-trained on the ImageNet database, combined with recurrent neural network (RNN) layers for longitudinal analysis. It also mentions the use of batch normalization and dropout after each fully connected layer to prevent overfitting. However, the exact parameters and configurations for these components are not provided.\n\nFor those interested in replicating or building upon our work, it would be beneficial to have access to the specific hyper-parameters, optimization schedules, and model files. This would allow for a more detailed understanding of the training process and potentially improve the model's performance or adapt it to different datasets. Unfortunately, the provided text does not include information on where or how these resources can be obtained.",
  "model/interpretability": "The model employed in our study is largely a black box, particularly in its intermediate layers. The input and output of the model are intuitive, with the input being CT scans and the output being probabilities of prognosis. However, the hidden layers within the deep learning network are not easily interpretable. This lack of transparency makes it challenging to determine the reasoning behind the network's performance and to identify which specific parameters have a positive or negative impact on the predictions.\n\nTo address this issue and make the model more interpretable, we utilized activation maps. These maps highlight the portions of the image that the network considers most important for its predictions. By visualizing these maps as heat maps over the final convolutional layer, we can gain some insight into what the model is focusing on. This approach helps to capture highly weighted portions of the image with respect to the network\u2019s predictions, providing a way to interpret the abstract features learned by the model.\n\nAdditionally, incorporating domain knowledge into these abstract features is a crucial area for future research. By doing so, we aim to make the automatically learned feature representations more interpretable and clinically relevant. This would allow physicians to better understand the basis for the model's predictions and potentially integrate this information into their decision-making processes.",
  "model/output": "The model is designed for binary classification. It utilizes a final softmax layer to produce a binary classification output. This output is used to predict survival outcomes and other clinical endpoints, such as distant metastasis, progression, and locoregional recurrence. The model's architecture includes convolutional neural networks (CNNs) merged with recurrent neural networks (RNNs), specifically gated recurrent units (GRUs), which handle time-varying inputs. The GRUs take into account the time domain and can manage missing scans, ensuring the network can learn from samples with missed patient scans at certain time points. The output of the pretrained network is masked to skip time points where scans are not available. Averaging and fully connected layers are applied after the GRU, with batch normalization and dropout to prevent overfitting. The model was evaluated on an independent test set for performance, and the two-year overall survival Kaplan-Meier curves were performed with median stratification of low and high mortality risk groups. The model's predictions were also used to categorize pathological response, distinguishing between responders and those with gross residual disease. The area under the receiver operator characteristic curve (AUC) was used to evaluate the model's performance in predicting pathological response. Additionally, the model's predictions were compared to a clinical model involving parameters such as stage, gender, age, tumor grade, performance, smoking status, and clinical tumor size.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the deep learning models involved several rigorous steps to ensure their performance and generalizability. Initially, the models were trained and tuned using a subset of Dataset A, which consisted of patients treated with definitive chemoradiation therapy. This dataset was randomly split into training/tuning and test sets in a 2:1 ratio, ensuring that the test set remained independent and was not used during the training process.\n\nTo evaluate the models' performance, we employed Monte Carlo cross-validation with 10 different splits of the training data, further divided into training and tuning sets in a 3:2 ratio. This approach helped in assessing the models' robustness and stability. The training was conducted for up to 300 epochs with class weight balancing to handle any imbalances in the data.\n\nThe primary evaluation metric was the area under the receiver operator characteristic curve (AUC), which was used to assess the models' ability to distinguish between positive and negative survival outcomes. Additionally, the Wilcoxon rank sums test (Mann\u2013Whitney U test) was employed to evaluate statistical differences between the survival groups.\n\nFor survival analysis, Kaplan-Meier estimates were calculated to compare low and high mortality risk groups, stratified at the median prediction probability derived from the training set. The Log-Rank test was used to control for these comparisons, and hazard ratios were calculated using the Cox Proportional-Hazards Model.\n\nThe models were also evaluated on an independent test set from Dataset A, comprising 72 patients, to assess their performance in predicting overall survival and other clinical endpoints such as distant metastasis, progression, and locoregional recurrence. Furthermore, an additional test was performed on Dataset B, which included patients treated with neoadjuvant radiotherapy and chemotherapy prior to surgical resection. This cohort served as an external validation set to evaluate the models' generalizability across different treatment regimens.\n\nIn summary, the evaluation method involved a combination of cross-validation techniques, independent test sets, and statistical analyses to thoroughly assess the performance and reliability of the deep learning models in predicting clinical outcomes for NSCLC patients.",
  "evaluation/measure": "In the evaluation of our deep learning models, we primarily focused on performance metrics that are widely accepted and representative in the literature for survival analysis and prognostic modeling. For assessing the predictive performance of our models, we utilized the Area Under the Receiver Operating Characteristic Curve (AUC). This metric was chosen for its ability to provide a comprehensive evaluation of the model's performance across all classification thresholds.\n\nWe reported AUC values for various clinical endpoints, including one-year and two-year overall survival, distant metastasis-free survival, progression-free survival, and locoregional recurrence-free survival. These endpoints are critical in evaluating the efficacy of treatment and the prognostic value of our models.\n\nIn addition to AUC, we employed the Wilcoxon rank sums test (also known as the Mann\u2013Whitney U test) to assess statistical differences between positive and negative survival groups. This non-parametric test is robust and suitable for comparing two independent samples, making it an appropriate choice for our datasets.\n\nFor survival analysis, we used Kaplan-Meier estimates to compare low and high mortality risk groups. These groups were stratified based on the median prediction probability derived from the training set. The Log-Rank test was used to control for statistical significance in these comparisons, providing a reliable measure of the differences in survival distributions between the groups.\n\nFurthermore, we calculated Hazard Ratios using the Cox Proportional-Hazards Model to quantify the risk of events such as death, metastasis, progression, and recurrence. This model is standard in survival analysis and allows for the adjustment of multiple covariates, providing a more nuanced understanding of the prognostic factors.\n\nOverall, the set of performance metrics we reported is representative of the current literature in the field of prognostic modeling and survival analysis. These metrics collectively provide a thorough evaluation of our deep learning models' predictive capabilities and their potential clinical utility.",
  "evaluation/comparison": "In our study, we employed a deep learning approach combining Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) to predict survival and other clinical endpoints in non-small cell lung cancer (NSCLC) patients. To evaluate the performance of our models, we conducted several comparisons.\n\nWe compared the performance of our deep learning models with that of radiographic and clinical features. Specifically, we assessed the predictive power of our models against a random forest clinical model that utilized features such as stage, gender, age, tumor grade, performance status, smoking status, and clinical tumor size. This comparison allowed us to gauge the added value of our deep learning approach over traditional clinical and radiographic methods.\n\nAdditionally, we evaluated the generalizability of our models by testing them on an independent dataset (Dataset B), which consisted of patients treated with a different regimen (chemoradiation followed by surgery). This external validation helped us understand how well our models performed in a different clinical context.\n\nFurthermore, we compared the performance of our models at different time points. We observed that the inclusion of follow-up scans at one, three, and six months post-treatment improved the predictive accuracy of our models for two-year overall survival. This trend was also noted for other clinical endpoints, such as one-year survival, metastasis, progression, and locoregional recurrence-free survival.\n\nIn summary, our evaluation included comparisons with simpler baselines and different clinical contexts, providing a comprehensive assessment of our deep learning models' performance.",
  "evaluation/confidence": "The evaluation of our deep learning models included several statistical analyses to ensure the robustness and significance of our results. We used the area under the receiver operator characteristic curve (AUC) to assess the performance of our models in distinguishing between positive and negative survival groups. Additionally, we employed the Wilcoxon rank sums test (Mann\u2013Whitney U test) to evaluate statistical differences between these groups.\n\nFor survival estimates, we utilized the Kaplan-Meier method to compare low and high mortality risk groups, stratified at the median prediction probability of the training set. The Log-Rank test was used to control these comparisons. Hazard ratios were calculated through the Cox Proportional-Hazards Model to further quantify the risk associated with different groups.\n\nOur models demonstrated significant differences in various clinical endpoints, including overall survival, distant metastasis-free survival, progression-free survival, and locoregional recurrence-free survival. These differences were statistically significant at multiple follow-up time points, indicating the reliability of our models' predictions.\n\nMoreover, we compared our deep learning models to a random forest clinical model that included features such as stage, gender, age, tumor grade, performance, smoking status, and clinical tumor size. The deep learning models showed superior performance in predicting survival and prognostic factors, with statistically significant results.\n\nIn summary, our evaluation metrics included confidence intervals and statistical significance tests, ensuring that our claims of superiority over other methods and baselines are well-supported. The use of multiple statistical analyses and comparisons to established clinical models provides a comprehensive assessment of our deep learning approach's effectiveness.",
  "evaluation/availability": "Not enough information is available."
}