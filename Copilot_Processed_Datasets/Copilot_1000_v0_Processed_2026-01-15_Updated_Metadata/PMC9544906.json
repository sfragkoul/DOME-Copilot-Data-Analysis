{
  "publication/title": "An exploratory application of machine learning methods to optimize prediction of responsiveness to digital interventions for eating disorder symptoms.",
  "publication/authors": "Linardon J, Fuller-Tyszkiewicz M, Shatte A, Greenwood CJ",
  "publication/journal": "The International journal of eating disorders",
  "publication/year": "2022",
  "publication/pmid": "35560256",
  "publication/pmcid": "PMC9544906",
  "publication/doi": "10.1002/eat.23733",
  "publication/tags": "- Adherence\n- Digital\n- Eating disorders\n- E-health\n- Engagement\n- Intervention\n- Machine learning\n- Prediction\n- Randomized controlled trial\n- Uptake",
  "dataset/provenance": "The dataset used in this study was derived from digital interventions, focusing on routinely collected baseline data. The sample consisted mostly of well-educated, White women who were highly symptomatic. The specific number of data points is not explicitly stated, but the sample characteristics and outcomes provide some insights into the dataset's scope.\n\nThe rate of uptake for the intervention was 83%, with a 36% adherence rate and a 57% dropout rate. Objective binge eating decreased by an average of six episodes from pre- to post-intervention, indicating some level of engagement and symptom change among participants.\n\nThe dataset included a variety of baseline variables, such as motivation levels, age, overvaluation, eating concerns, and the presence of current or past anxiety disorders. These variables were used to predict engagement outcomes and symptom-level change. The predictive models were generated using eight different classification approaches, including traditional regression, elastic-net penalized regression, support vector machines with various kernels, k-Nearest Neighbor, Classification and Regression Tree (CART), and random forest.\n\nThe dataset was split into training and testing sets multiple times to examine predictive performance and reduce data biases. Sensitivity analyses were conducted to balance outcomes and exclude non-uptake participants, but predictive performance remained poor for engagement outcomes. However, incorporating usage variables improved predictive performance for adherence and dropout.\n\nThe dataset has not been explicitly mentioned as used in previous papers or by the community, but the methods and findings are consistent with studies in related fields, such as depression, schizophrenia, and anxiety. The study highlights the limitations of relying solely on baseline information for predicting success from digital interventions and suggests that other sources of big data may prove more useful.",
  "dataset/splits": "The dataset was split into 100 different training and testing datasets. Each split consisted of 67% of the data for training and 33% for testing. This iterative process was implemented to examine predictive performance and reduce data biases. The models were trained using fivefold cross-validation to select the optimal model. The predictive performance indices were then averaged across all iterations.",
  "dataset/redundancy": "The dataset was split into training and testing subsets to evaluate the predictive performance of various models. Specifically, the full dataset was divided into 100 different training (67%) and testing (33%) datasets. This iterative process was implemented to examine predictive performance and reduce data biases. Each model was generated using the training data and then validated on the testing data. To ensure robustness, models were trained using fivefold cross-validation to select the optimal model. This approach helps in comparing precision and accuracy across different subsets of the data, enhancing the generalizability of the findings.\n\nThe training and test sets were not entirely independent, as they were derived from the same dataset through multiple splits. However, this method allowed for a comprehensive evaluation of model performance across various data configurations. The distribution of the dataset is not explicitly compared to previously published machine learning datasets, but the use of cross-validation and multiple training-testing splits is a standard practice in machine learning to mitigate overfitting and ensure that models generalize well to new data.",
  "dataset/availability": "The data used in this study is not publicly available. However, de-identified data can be accessed by contacting the corresponding author. This approach ensures that the data is shared responsibly while maintaining participant privacy and confidentiality. The decision to not release the data publicly is likely due to ethical considerations and the need to protect sensitive information related to participants' health and personal details. The authors have declared no conflict of interest, which further supports the integrity of the data sharing process.",
  "optimization/algorithm": "The machine-learning algorithms used in this study were classification approaches, specifically traditional regression, elastic-net penalized regression, support vector machine with linear, polynomial, and radial basis function kernels, k-Nearest Neighbor, Classification and Regression Tree (CART), and random forest. These are well-established methods in the field of machine learning and have been extensively used in various predictive modeling tasks.\n\nThe algorithms employed are not new; they are standard techniques that have been widely applied in both machine learning and statistical literature. The choice of these algorithms was driven by their proven effectiveness in handling different types of data and their ability to capture complex patterns. The elastic-net approach, for instance, is particularly useful when the number of predictors is greater than the number of participants, making it a robust choice for our dataset.\n\nThe decision to use these established algorithms in a study focused on digital interventions for eating disorder symptoms was strategic. The primary goal was to evaluate the predictive performance of machine learning techniques compared to traditional methods, rather than to introduce novel algorithms. This approach allows for a clear comparison and assessment of the benefits and limitations of machine learning in the context of digital health interventions.\n\nThe algorithms were implemented using the caret package, which is a widely used tool in the R programming language for training and plotting models. This package provides a consistent interface to various machine learning algorithms, making it easier to compare their performance across different datasets. The use of caret ensures that the models were trained and validated in a systematic and reproducible manner, enhancing the reliability of the findings.",
  "optimization/meta": "The models used in this study did not employ meta-predictors. Instead, they utilized a variety of individual machine learning algorithms to generate predictive models. The specific algorithms included traditional regression, elastic-net penalized regression, support vector machines with different kernels (linear, polynomial, and radial basis function), k-Nearest Neighbor, Classification and Regression Tree (CART), and random forest.\n\nEach of these models was trained and validated independently using an iterative process. The full dataset was split into 100 different training and testing datasets, with each model generated in the training data and then validated in the testing data. This approach ensured that the training data was independent for each iteration, reducing the risk of overfitting and enhancing the generalizability of the results.\n\nThe predictive performance of these models was evaluated using various metrics, such as accuracy, area under the curve (AUC), F1 score, true negatives, false positives, false negatives, and true positives for binary outcomes. For continuous outcomes, metrics like R\u00b2, Root-Mean-Square Error, and Mean Absolute Error were used. These evaluations were averaged across all iterations to provide a comprehensive assessment of each model's performance.",
  "optimization/encoding": "All predictors were centered and standardized before being input into the machine-learning algorithms. This preprocessing step ensures that each predictor contributes equally to the model, preventing variables with larger scales from dominating the learning process. The data was split into training and testing sets multiple times to examine predictive performance and reduce data biases. Specifically, the full dataset was divided into 100 different training (67%) and testing (33%) datasets. Each model was trained using fivefold cross-validation to select the optimal model parameters. This iterative process helped in validating the models and ensuring their robustness. Additionally, sensitivity analyses were conducted using 10 iterations of training-testing splits to further assess the models' performance under different conditions.",
  "optimization/parameters": "In our study, we utilized 36 baseline variables as input parameters for our predictive models. These variables were chosen based on pragmatic considerations, aiming to characterize the sample and include key outcome measures. The selection of these parameters was guided by the goal of generating accurate predictive models for engagement outcomes and symptom-level change in digital interventions.\n\nThe choice of these baseline variables was influenced by their potential relevance to the outcomes of interest. However, it is important to note that the predictive performance of our models was not superior to traditional logistic regression, suggesting that the chosen predictors may not have been sufficiently influential or related to the outcomes. This finding aligns with other studies that have reported small to no incremental benefits of machine learning when restricting analyses to routinely collected baseline data.\n\nFuture research should consider exploring other sources of data that can be easily collected during digital intervention trials. Information on design choices, user experience, perceived usability, and reasons for disengagement may prove useful for generating more accurate predictive models. Additionally, assessing predictors repeatedly throughout the intervention phase may help determine whether poor prediction is a function of measuring these constructs in a static manner and at an inappropriate time.",
  "optimization/features": "In our study, we utilized 36 baseline variables as input features for our predictive models. These features were selected based on pragmatic choices to characterize the sample and those used as key outcome measures. Feature selection was not explicitly performed as part of our modeling process. All selected features were used across all models, and the full set of features was included in both the training and testing datasets during the iterative process of model validation. This approach ensured that the models were evaluated on the same set of predictors throughout the analysis.",
  "optimization/fitting": "The study employed eight different classification approaches to generate predictive models, including traditional regression, elastic-net penalized regression, support vector machines with various kernels, k-Nearest Neighbor, Classification and Regression Tree (CART), and random forest. The models were implemented using the caret package, ensuring robust and standardized procedures.\n\nTo address the potential issue of overfitting, especially when the number of predictors is large relative to the number of training points, several strategies were employed. The full dataset was split into 100 different training (67%) and testing (33%) datasets, and each model was generated in the training data and then validated in the testing data. This iterative process helped to examine predictive performance and reduce data biases. Additionally, models were trained using fivefold cross-validation to select the optimal model, which further mitigated overfitting concerns. The predictive performance indices, such as accuracy, area under the curve (AUC), F1 score, true negatives, false positives, false negatives, and true positives for binary outcomes, and R\u00b2, Root-Mean-Square Error, and Mean Absolute Error for continuous outcomes, were computed and averaged across all iterations. This approach ensured that the models were not overly tailored to the training data but could generalize well to unseen data.\n\nTo rule out underfitting, the study utilized a variety of sophisticated machine learning techniques that are capable of capturing complex patterns in the data. The use of elastic-net penalized regression, support vector machines with different kernels, and random forests, among others, provided a comprehensive set of models that could handle both linear and nonlinear relationships. The iterative process of model validation and the use of cross-validation further ensured that the models were not too simplistic to capture the underlying data patterns. The consistent performance across different models and the comparison with traditional techniques also indicated that the models were appropriately complex to avoid underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our predictive models. One key method used was cross-validation. Specifically, we implemented fivefold cross-validation during the training process to select the optimal model. This technique involves dividing the training data into five subsets, training the model on four of these subsets, and validating it on the remaining subset. This process is repeated five times, with each subset serving as the validation set once. By averaging the results, we obtained a more reliable estimate of the model's performance.\n\nAdditionally, we utilized an iterative process where the full dataset was split into 100 different training (67%) and testing (33%) datasets. Each model was generated in the training data and then validated in the testing data. This approach helped to reduce data biases and provided a more comprehensive evaluation of the models' predictive performance.\n\nFurthermore, we employed regularization techniques such as elastic-net penalized regression. Elastic-net combines the penalties of both L1 (lasso) and L2 (ridge) regularization, which helps to prevent overfitting by adding a penalty term to the loss function. This encourages sparsity in the model coefficients, effectively reducing the complexity of the model and improving its generalization to new data.\n\nAnother regularization method used was the support vector machine (SVM) with different kernels (linear, polynomial, and radial basis function). SVMs are inherently regularized due to their margin maximization principle, which helps to find a balance between maximizing the margin and minimizing the classification error. This regularization helps to prevent overfitting by ensuring that the model generalizes well to unseen data.\n\nOverall, these techniques collectively contributed to the prevention of overfitting and enhanced the reliability and generalizability of our predictive models.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in this study span a range of interpretability levels, from relatively transparent to more opaque. Traditional regression models, including generalized linear models and elastic-net penalized regression, are generally considered transparent. These models provide clear, interpretable coefficients that indicate the direction and magnitude of the relationship between each predictor and the outcome. For instance, in the elastic-net model, the coefficients can directly show how changes in variables like motivation levels or age influence engagement outcomes.\n\nSupport vector machines (SVM), particularly those with linear kernels, also offer a degree of interpretability. The linear SVM model, for example, can be interpreted through its decision boundary, which is a linear combination of the input features. This allows for some understanding of which features are most influential in making predictions. For example, in predicting uptake, motivation levels, age, and overvaluation were identified as key predictors.\n\nIn contrast, models like k-Nearest Neighbors (k-NN) and random forests are more black-box in nature. k-NN makes predictions based on the similarity of new data points to existing ones, which can be difficult to interpret directly. Random forests, while powerful, aggregate the predictions of multiple decision trees, making it challenging to trace back the exact reasoning behind a specific prediction. However, variable importance measures can be extracted from these models to understand which features are most influential. For example, in the random forest model, variables such as age and depressive symptoms were found to be important for predicting adherence.\n\nClassification and Regression Trees (CART) offer a middle ground. While individual trees can be visualized and interpreted, the aggregation of multiple trees in a random forest can obscure this interpretability. Nonetheless, CART models can provide clear decision rules that are easy to understand. For instance, the CART model highlighted variables like age and education status as important for predicting dropout.\n\nOverall, while some models provide clear, interpretable results, others require additional techniques, such as variable importance measures, to gain insights into their decision-making processes. This mix of transparency and opacity allows for a balanced approach to understanding and validating the models' predictions.",
  "model/output": "The models generated were both classification and regression models. For engagement outcomes, which were binary, classification approaches were used. These included traditional regression, elastic-net penalized regression, support vector machines with various kernels, k-Nearest Neighbor, Classification and Regression Tree (CART), and random forest. For continuous outcomes, such as symptom-level change, regression models were employed. The performance of these models was evaluated using metrics appropriate for each type of outcome. For binary outcomes, accuracy, area under the curve (AUC), F1 score, true negatives, false positives, false negatives, and true positives were computed. For continuous outcomes, R-squared, Root-Mean-Square Error, and Mean Absolute Error were used. The models were trained using fivefold cross-validation to select the optimal model, and their performance was averaged across 100 iterations of training and testing splits. Sensitivity analyses were also conducted to further validate the models' performance.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method involved generating predictive models using eight different classification approaches, including traditional regression, elastic-net penalized regression, support vector machines with various kernels, k-Nearest Neighbor, Classification and Regression Tree (CART), and random forest. These models were implemented using the caret package.\n\nTo assess predictive performance and reduce data biases, an iterative process was employed. The full dataset was split into 100 different training (67%) and testing (33%) datasets. Each model was trained on the training data and then validated on the testing data. Fivefold cross-validation was used to select the optimal model within each training set.\n\nFor binary outcomes, such as engagement indices, several metrics were computed: accuracy, area under the curve (AUC), F1 score, true negatives, false positives, false negatives, and true positives. For continuous outcomes, metrics included R\u00b2, Root-Mean-Square Error (RMSE), and Mean Absolute Error (MAE). These performance indices were averaged across all 100 iterations.\n\nSensitivity analyses were conducted using 10 iterations of training-testing splits. For binary outcomes, models were repeated using an artificially oversampled dataset to balance outcomes and by including only participants who accessed the digital intervention. For symptom-level change, models were repeated excluding baseline levels of the modeled outcome. Additional sensitivity analyses were performed using intervention usage pattern data from one randomized controlled trial to assess whether predictive accuracy improved.",
  "evaluation/measure": "In our evaluation, we reported a comprehensive set of performance metrics to assess the predictive models used for both engagement outcomes and symptom-level change. For binary outcomes, such as engagement indices, we computed accuracy, area under the curve (AUC), F1 score, true negatives, false positives, false negatives, and true positives. These metrics provide a thorough evaluation of the models' predictive performance, covering aspects like the overall correctness, the trade-off between sensitivity and specificity, and the precision and recall of the models.\n\nFor continuous outcomes, specifically the change in objective binge eating, we used R-squared (R\u00b2), Root-Mean-Square Error (RMSE), and Mean Absolute Error (MAE). These metrics help in understanding the proportion of variance explained by the models, the average magnitude of the errors, and the absolute errors, respectively. The use of these metrics is consistent with standard practices in the literature, ensuring that our evaluation is representative and comparable to other studies in the field.\n\nAdditionally, we conducted sensitivity analyses to further validate our models. These analyses included using an artificially oversampled dataset to balance outcomes and excluding certain participants to assess the robustness of our models. The performance metrics reported in these analyses provide insights into how well our models generalize to different scenarios and datasets.\n\nOverall, the set of performance metrics we reported is representative of the literature and provides a comprehensive evaluation of our predictive models. This ensures that our findings are reliable and can be compared with other studies in the field.",
  "evaluation/comparison": "In our study, we compared the performance of machine learning (ML) models to traditional statistical approaches. Specifically, we evaluated eight different classification approaches, including traditional regression, elastic-net penalized regression, various support vector machine (SVM) kernels, k-Nearest Neighbor, Classification and Regression Tree (CART), and random forest. These models were implemented using the caret package.\n\nFor engagement outcomes, we found that the predictive performance of ML models was poor and not superior to traditional logistic regression. This was evident from the area under the curve (AUC) values, which were similar across all models. For symptom-level change, ML models explained slightly more variance than traditional generalized linear models, but the difference was minimal.\n\nWe also conducted sensitivity analyses to further compare the models. For binary outcomes, we used an artificially oversampled dataset to balance outcomes and repeated models for participants who accessed their digital intervention. For symptom-level change, we excluded baseline levels of the modeled outcome. These analyses showed that predictive performance remained poor for engagement outcomes and did not significantly improve for symptom-level change.\n\nAdditionally, we incorporated usage variables into our analyses for a subset of participants. This produced strong predictive performance for adherence and dropout across most models, except for the SVM with a radial basis function kernel. However, predictive performance did not improve for symptom-level change.\n\nOverall, our findings suggest that ML models did not confer a significant benefit over traditional approaches when using the predictors chosen for our analyses. This is consistent with other studies that have reported small to no incremental benefits of ML when restricting analyses to routinely collected baseline data.",
  "evaluation/confidence": "The evaluation of our models involved an iterative process where the full dataset was split into 100 different training and testing datasets. Each model was generated in the training data and then validated in the testing data. This process allowed us to compute various performance metrics, including accuracy, area under the curve (AUC), F1 score, true negatives, false positives, false negatives, and true positives for binary outcomes. For continuous outcomes, we computed R\u00b2, Root-Mean-Square Error (RMSE), and Mean Absolute Error (MAE). Each of these performance indices was averaged across all iterations to provide a robust estimate of model performance.\n\nHowever, it is important to note that while we reported mean values for these metrics, we did not provide confidence intervals for them. This means that the variability and uncertainty around these estimates are not explicitly quantified in our results. Additionally, while we compared the performance of different models, we did not conduct statistical tests to determine if the differences in performance were statistically significant. Therefore, we cannot definitively claim that one method is superior to others or to the baselines based on statistical significance.\n\nThe lack of confidence intervals and statistical significance testing is a limitation of our evaluation. Future work could address this by including confidence intervals for performance metrics and conducting statistical tests to compare model performance. This would provide a more comprehensive understanding of the uncertainty and variability in our results, as well as the statistical significance of any observed differences in performance.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, interested parties can contact the corresponding author to gain access to de-identified data if necessary. This approach ensures that the data can be shared responsibly while maintaining participant confidentiality. The decision to not publicly release the raw evaluation files is likely due to ethical considerations and the need to protect sensitive participant information."
}