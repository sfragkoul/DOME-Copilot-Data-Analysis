{
  "publication/title": "Machine-learning based prediction of appendicitis for patients presenting with acute abdominal pain at the emergency department.",
  "publication/authors": "Schipper A, Belgers P, O'Connor R, Jie KE, Dooijes R, Bosma JS, Kurstjens S, Kusters R, van Ginneken B, Rutten M",
  "publication/journal": "World journal of emergency surgery : WJES",
  "publication/year": "2024",
  "publication/pmid": "39716296",
  "publication/pmcid": "PMC11664873",
  "publication/doi": "10.1186/s13017-024-00570-7",
  "publication/tags": "- Acute Abdominal Pain\n- Appendicitis\n- Machine Learning\n- Emergency Department\n- Diagnostic Models\n- XGBoost\n- Medical History\n- Physical Examination\n- Laboratory Tests\n- Clinical Decision Support",
  "dataset/provenance": "The dataset used in this study was sourced from patient records at the Jeroen Bosch Hospital. The initial dataset consisted of 350 cases, but after applying eligibility criteria, such as the availability of complete medical history, physical examination findings, and laboratory test results, the final dataset comprised 336 eligible cases. These cases were selected to include the first series of measurements from each emergency department visit to limit the influence of medications. The data extraction process was performed using CTcue, a privacy-by-design tool that pseudonymizes patient data by redacting personally identifiable information and hashing patient IDs. This ensures that the dataset is anonymized and compliant with privacy regulations.\n\nThe dataset includes various parameters collected during the initial patient evaluation, such as ED intake information, vital signs, medical history, and physical examination findings. Additionally, blood and urine test results from standardized laboratory order sets were collected for these cases. Detailed information on these parameters can be found in the supplementary tables provided with the manuscript.\n\nThe dataset was balanced using subsampling techniques to ensure equal representation of appendicitis cases and other causes of acute abdominal pain. This balancing was crucial for training the machine learning models to differentiate between appendicitis and other conditions with similar clinical presentations. The dataset has not been used in previous publications by the community, but it builds upon established methodologies for data collection and preprocessing in medical research. The annotated data from this study could serve as a valuable basis for developing automated processing techniques for unstructured medical data in future research.",
  "dataset/splits": "The dataset was divided into two primary splits: a training set and a validation set. The training set comprised 80% of the total cases, amounting to 268 data points. Within this training set, there were 133 cases of appendicitis and 135 cases of other acute abdominal pain (AAP) causes. The validation set constituted the remaining 20% of the cases, totaling 68 data points, with an equal distribution of 34 appendicitis cases and 34 other AAP cases.\n\nTo ensure robust model training and hyperparameter tuning, repeated stratified 10-fold cross-validation was employed. This technique preserved the class distribution across the folds, ensuring that each fold had a representative mix of appendicitis and other AAP cases. The mean performance result across these repetitions was used to fine-tune the models. This approach helped in optimizing the models' performance and generalizability.",
  "dataset/redundancy": "The dataset used in this study was collected from 350 patients who presented with acute abdominal pain (AAP) at the emergency department (ED) of a Dutch teaching hospital. The data was pseudonymized and retrospectively gathered, ensuring patient privacy. To limit the influence of medications, only the first series of measurements from each ED visit was extracted.\n\nBalanced subsampling was applied to train the model to differentiate appendicitis from other AAP causes. This technique involved achieving equal numbers of appendicitis and other AAP cases. Among the other AAP cases, those suspected of appendicitis were balanced with those having non-specific or other AAP causes based on initial assessments by primary care physicians or triage nurses upon ED arrival. This approach enhances machine learning (ML) model performance on minority classes by balancing class distributions, adjusting class frequencies without accounting for other parameters. No duplicate cases were introduced into the dataset during this process.\n\nCases were excluded if they had insufficient medical history or physical examination findings or were missing more than 70% of the laboratory test results or vital signs. This threshold was chosen to balance the preservation of enough cases while minimizing missing parameters, resulting in a final dataset of 336 eligible cases.\n\nThe data extraction was performed using a privacy-by-design data extraction tool that automatically pseudonymizes patient data by redacting personally identifiable information and hashing patient IDs. This ensures that the training and test sets are independent and that the distribution of the dataset is representative of the diverse clinical presentations encountered in daily practice. The dataset's composition and preprocessing steps were designed to reflect the real-world variability in AAP presentations, making the ML models applicable to a broad range of patients.",
  "dataset/availability": "The data utilized in this study is provided within the manuscript and supplementary information files. This ensures that the dataset is accessible to the public for further research and validation purposes. Additionally, the code and models developed for this research are available via repositories on GitHub and Figshare. The GitHub repository can be accessed at the provided URL, and the Figshare repository is associated with the DOI mentioned. These repositories contain the necessary tools and information to replicate the study's findings and apply the models to new datasets.\n\nThe data extraction process was performed using CTcue, a privacy-by-design data extraction tool. This tool automatically pseudonymizes patient data by redacting personally identifiable information and hashing patient IDs. This ensures that the data is anonymized and compliant with privacy regulations, allowing for secure and ethical sharing of the dataset.\n\nThe study was conducted according to the Declaration of Helsinki and Guidelines for Good Clinical Practice. The execution of this retrospective observational study of patient records was approved by the local review board of the Jeroen Bosch Hospital. The Medical Research Ethics Committee Brabant waived the need for informed written consent and consent for publication, ensuring that the study adhered to ethical standards and regulations.\n\nIn summary, the data, including the data splits used, is released in a public forum through the manuscript, supplementary information files, and repositories on GitHub and Figshare. The data is anonymized using CTcue, ensuring privacy and compliance with ethical standards. The study was conducted with the necessary approvals and waivers, making the dataset available for public use while maintaining ethical integrity.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is gradient boosting, specifically the eXtreme Gradient Boosting (XGBoost) algorithm. This algorithm is not new; it is a well-established and widely used method in the field of machine learning.\n\nThe choice of XGBoost was driven by its proven effectiveness in handling structured data and its ability to provide robust performance in predictive modeling tasks. While XGBoost is a mature algorithm, it continues to be refined and optimized, making it a reliable choice for our diagnostic model.\n\nThe decision to use XGBoost in a medical context, rather than publishing it in a machine-learning journal, stems from our focus on applying established techniques to solve specific clinical problems. Our primary goal was to develop a practical tool for diagnosing acute appendicitis, leveraging the strengths of XGBoost to enhance diagnostic accuracy in emergency settings. The algorithm's implementation and optimization were tailored to meet the unique requirements of medical data, ensuring that it could effectively support clinical decision-making.",
  "optimization/meta": "The models developed in this study do not function as meta-predictors. They do not use data from other machine-learning algorithms as input. Instead, they rely directly on clinical data such as vital signs, medical history, and physical examination findings. The models, specifically the HIVE and HIVE-LAB models, were trained to optimize the area under the receiver operating characteristic curve (AUROC) using Bayesian optimization through Optuna. This process involved 100 trials to identify the optimal hyperparameter settings. The models were interpreted using SHapley Additive exPlanations (SHAP) values, which calculated the percentage contribution of each parameter to the prediction.\n\nThe training data for these models was collected from a single-center context within a Dutch hospital. This setting involved primary care physicians acting as gatekeepers to emergency department (ED) access, leading to a higher prevalence of serious conditions like appendicitis in the cohort. The generalizability and reliability of these models across diverse clinical environments and patient demographics would require further validation in multi-center studies. The models were designed to enhance the diagnostic process for appendicitis by providing a tool that outperforms traditional methods like the Alvarado scoring system and can aid physicians in making more accurate diagnoses.",
  "optimization/encoding": "The data encoding process involved transforming both binary and nominal parameters into numerical formats suitable for machine learning algorithms. Binary parameters, such as the presence of nausea, were directly encoded. Nominal parameters, like the location of pain, required more sophisticated handling. To manage these, a target-based encoding algorithm known as CatBoost was employed. This algorithm converts categorical variables into numerical representations by leveraging statistical properties derived from the training data. This approach ensures that the encoded data retains meaningful information relevant to the prediction task. The encoding process was crucial for preparing the data for the XGBoost algorithm, which was used to build the machine learning models. By converting all parameters into a consistent numerical format, the models could effectively learn from the data and make accurate predictions about the probability of appendicitis. The use of CatBoost for encoding helped in maintaining the integrity of the categorical information, thereby enhancing the model's performance.",
  "optimization/parameters": "In our study, we utilized a total of 73 parameters for model development. These parameters were derived from medical history and physical examination data, which were initially extracted from emergency department reports in free-text entries. To structure this data, an initial annotation process was conducted by two researchers, who labeled all medical symptoms in 100 cases, resulting in 367 initial labels. These labels were then reviewed by two emergency department physicians for their diagnostic value. Labels deemed clinically unrelated to acute appendicitis were excluded, while others were grouped under overarching labels, reducing the total to 289. This final set of 289 labels was categorized into 73 parameters. These parameters included 50 binary parameters (e.g., presence of nausea) and 23 nominal parameters (e.g., location of pain). Another 236 cases were subsequently annotated using this structured framework.\n\nThe selection of these parameters was a meticulous process aimed at ensuring that only clinically relevant and diagnostically valuable information was included. This approach helped in maintaining the integrity and reliability of the models, as it focused on parameters that had a significant impact on the diagnosis of appendicitis. The use of a structured framework for annotation ensured consistency and reduced the likelihood of missing important clinical details. This method also allowed for the efficient handling of binary and nominal parameters, which were encoded into numerical parameters using a target-based encoding algorithm. This encoding process was crucial for the effective training and optimization of the models, as it enabled the algorithms to handle diverse types of data seamlessly.",
  "optimization/features": "The input features for the models were derived from various sources, including ED intake information, vital signs, medical history, physical examination findings, and laboratory test results. Initially, 367 labels were identified through an annotation process. These labels were reviewed and refined, resulting in a final set of 289 labels, which were then categorized into 73 parameters. These parameters included 50 binary parameters (e.g., presence of nausea) and 23 nominal parameters (e.g., location of pain).\n\nFeature selection was performed to ensure that only clinically relevant parameters were included. Labels with a prevalence of less than 5% were reviewed by ED physicians for their diagnostic value. Those deemed clinically unrelated to acute appendicitis (AAP) causes were excluded, while others were grouped under overarching labels. This process ensured that the final set of features was both comprehensive and relevant to the diagnosis of appendicitis.\n\nThe feature selection process was conducted using the training set only, ensuring that the validation set remained independent and unbiased. This approach helped to preserve the integrity of the model evaluation and to avoid overfitting. The final dataset consisted of 336 eligible cases, with the features being used to train and validate the models.",
  "optimization/fitting": "The models developed in this study utilized the eXtreme Gradient Boosting (XGBoost) algorithm, which is well-suited for handling a large number of parameters relative to the number of training points. The dataset consisted of 336 cases, with 80% used for training and hyperparameter tuning, resulting in 268 training samples. The parameters included 73 features, comprising 50 binary and 23 nominal parameters, which were encoded using the CatBoost algorithm to convert them into numerical representations.\n\nTo address the potential issue of overfitting, several strategies were employed. Firstly, repeated stratified 10-fold cross-validation was used during the training process. This technique ensures that the class distribution is preserved across folds, providing a robust estimate of model performance and reducing the risk of overfitting. Secondly, Bayesian optimization through Optuna was utilized to refine the hyperparameters over 100 trials. This method systematically explores the hyperparameter space to identify the optimal settings, further mitigating the risk of overfitting.\n\nTo prevent underfitting, the models were trained to optimize the area under the receiver operating characteristic curve (AUROC). This metric is particularly useful for evaluating the performance of classification models, especially when dealing with imbalanced datasets. Additionally, the use of SHapley Additive exPlanations (SHAP) values helped in interpreting the models by calculating the percentage contribution of each parameter to the predictions. This ensured that the models were not overly simplistic and captured the necessary complexity of the data.\n\nThe models were also validated on a separate validation set consisting of 68 cases, which was not used during the training process. This independent validation step provided an unbiased assessment of the models' performance, ensuring that they generalized well to new, unseen data. Furthermore, the TRIPOD checklist was followed to enhance the transparency and reproducibility of the study's methodology, adding another layer of rigor to the model development process.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was balanced subsampling, which involved equalizing the number of appendicitis cases and other acute abdominal pain (AAP) cases. This technique helps in mitigating the bias that can occur when dealing with imbalanced datasets, thereby enhancing the model's performance on minority classes.\n\nAdditionally, we utilized Bayesian optimization through Optuna to refine the hyperparameters of our models. This process involved conducting 100 trials to identify the optimal settings, which helped in fine-tuning the models and reducing the risk of overfitting.\n\nFurthermore, we followed the TRIPOD checklist to ensure transparency and rigor in our methodology. This checklist provides guidelines for reporting prediction model studies, which includes recommendations for model validation and assessment of model performance.\n\nWe also employed SHapley Additive exPlanations (SHAP) values to interpret the models. By calculating the percentage contribution of each parameter to the prediction, we gained insights into the model's decision-making process, which is crucial for understanding and validating the model's performance.\n\nThese techniques collectively contributed to the prevention of overfitting and ensured that our models were reliable and generalizable.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available and have been reported in detail. Specifically, the hyperparameters for the models were refined using Bayesian optimization through Optuna, involving 100 trials to identify the optimal settings. These settings are documented in Additional File 2, which provides a comprehensive overview of the hyperparameter configurations for the XGBoost models.\n\nThe model files and optimization parameters are accessible via public repositories. The code and models can be found on GitHub at the repository \"AI-for-Apendicitis\" and on Figshare with the DOI: 10.6084/m9.figshare.27194622. These repositories contain all necessary files and parameters used in the optimization process, ensuring transparency and reproducibility of our work.\n\nThe data used in this study is provided within the manuscript and supplementary information files, making it accessible for further research and validation. The availability of these resources aligns with our commitment to open science and ensures that other researchers can build upon our findings.",
  "model/interpretability": "The models developed in this study, specifically the HIVE and HIVE-LAB models, are not entirely black-box systems. To ensure transparency and interpretability, SHapley Additive exPlanations (SHAP) values were employed. SHAP values provide a way to understand the contribution of each feature to the model's predictions. By calculating the percentage contribution of each parameter using the TreeSHAP algorithm, it becomes possible to interpret how different factors influence the likelihood of appendicitis.\n\nFor instance, SHAP values can reveal which symptoms or vital signs are most indicative of appendicitis. This level of interpretability is crucial in a clinical setting, where understanding the reasoning behind a model's predictions is essential for trust and adoption by healthcare professionals. The use of SHAP values allows clinicians to see not just the output of the model, but also the underlying factors that led to that output, making the models more transparent and actionable in real-world scenarios.",
  "model/output": "The models developed in this study are classification models. Specifically, they are designed to estimate the probability of appendicitis at two key decision points in the emergency department (ED) workup of acute abdominal pain (AAP). The models use the eXtreme Gradient Boosting (XGBoost) algorithm, which is well-suited for classification tasks. The primary output of these models is a probability score indicating the likelihood of appendicitis for each case. This score ranges from 0 to 100, with 0 being 'highly unlikely' and 100 being 'very likely'. The models were trained to optimize the area under the receiver operating characteristic curve (AUROC), a metric commonly used to evaluate the performance of classification models. The models were also interpreted using SHapley Additive exPlanations (SHAP) values, which calculate the percentage contribution of each parameter to the prediction. This approach helps in understanding the factors that influence the model's decisions. The models were validated using a reader study that compared their outcomes with the clinical performance of emergency department physicians, ensuring that the models' outputs are clinically relevant and reliable.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models developed in this study is publicly available. It can be accessed via the GitHub repository. Additionally, the models themselves are available on Figshare. The code and models are released under licenses that allow for their use and further development by the research community. This ensures transparency and reproducibility of the study's findings. The specific details for accessing the code and models are provided in the data availability section of the publication.",
  "evaluation/method": "The evaluation method for our study involved a comprehensive reader study designed to compare the performance of our machine learning models, HIVE and HIVE-LAB, with the clinical assessments of emergency department (ED) physicians. The reader study utilized a validation set consisting of 68 cases, each presented in its original format to mimic the electronic health record system. Three ED physicians, with varying levels of experience (one, five, and ten years post-qualification), independently evaluated each case. They scored the likelihood of appendicitis on a scale from 0 to 100, mirroring the probability output of our models.\n\nThe evaluation process was conducted in two steps. Initially, physicians scored each case based on intake information, vital signs, medical history, and physical examination findings. Subsequently, they adjusted their likelihood scores, if necessary, after reviewing the laboratory test results for the same cases. This two-step evaluation ensured a thorough assessment, reflective of real-world diagnostic practices, and allowed for an evaluation of the added value of laboratory test results.\n\nAdditionally, our models were trained to optimize the area under the receiver operating characteristic curve (AUROC). Hyperparameters were refined using Bayesian optimization through Optuna, involving 100 trials to identify the optimal settings. Model interpretation was performed using SHapley Additive exPlanations (SHAP) values, calculating the percentage contribution of each parameter to the predictions made by the XGBoost models.\n\nThe Alvarado scoring system, also known as MANTRELS, was used as a benchmark for risk stratification of acute appendicitis. This 10-point clinical scoring system helped in comparing the performance of our models against established clinical practices. The determination of 'appendicitis' versus 'other AAP causes' was based on hospitalization, treatment received, and ICD-10 codes, ensuring a robust reference standard for our evaluations.",
  "evaluation/measure": "In our study, we primarily reported the area under the receiver operating characteristic curve (AUROC) as the key performance metric. This metric was chosen because it provides a comprehensive evaluation of the model's ability to distinguish between cases of appendicitis and other causes of acute abdominal pain (AAP). The AUROC values were calculated for both the HIVE and HIVE-LAB models, as well as for the diagnostic performance of emergency department (ED) physicians and the Alvarado scoring system.\n\nThe AUROC metric is widely used in the literature for evaluating diagnostic models, making it a representative and comparable measure. For instance, the HIVE model achieved an AUROC of 0.919, indicating strong discriminative ability. Similarly, the HIVE-LAB model, which includes laboratory parameters, achieved an AUROC of 0.923, showing a slight improvement but not statistically significant when compared to the HIVE model.\n\nIn addition to the AUROC, we also reported sensitivity and specificity thresholds for the Alvarado scoring system. At a threshold of \u2264 4, the system correctly ruled out 56% of patients without appendicitis (specificity of 56%) and had a sensitivity of 88%. At a threshold of \u2265 7, the system correctly identified 27% of patients with appendicitis (sensitivity of 27%) and had a specificity of 100%. These thresholds provide insights into the system's utility in both ruling out and identifying appendicitis.\n\nFurthermore, we conducted statistical comparisons of AUROC values using DeLong\u2019s Test to assess the significance of differences between the models, ED physicians, and the Alvarado scoring system. This approach ensures that our performance evaluations are rigorous and statistically sound.\n\nOverall, the reported metrics are representative of standard practices in the field and provide a clear and comprehensive evaluation of the models' performance.",
  "evaluation/comparison": "In our study, we conducted a comprehensive evaluation of our machine learning models, HIVE and HIVE-LAB, by comparing their performance with established clinical methods and simpler baselines. One of the key comparisons involved the Alvarado scoring system, a widely used clinical tool for assessing the risk of acute appendicitis. This system, also known as MANTRELS, evaluates patients based on symptoms and clinical findings, providing a score that stratifies risk levels.\n\nWe also performed a reader study to compare the outcomes of our models with the clinical performance of emergency department (ED) physicians. This study involved three ED physicians with varying levels of experience who evaluated the same validation set of cases. The physicians scored the likelihood of appendicitis based on intake information, vital signs, medical history, and physical examination findings, and then adjusted their scores after reviewing laboratory test results. This two-step evaluation process allowed us to assess the added value of laboratory tests in the diagnostic process.\n\nIn addition to these clinical comparisons, we ensured that our models were optimized using Bayesian optimization through Optuna, involving 100 trials to identify the best hyperparameters. This rigorous optimization process helped us refine our models to achieve high performance. Furthermore, we used SHapley Additive exPlanations (SHAP) values to interpret the models, calculating the contribution of each parameter to the predictions. This approach provided transparency and insights into how our models made their decisions.\n\nOur models were trained to optimize the area under the receiver operating characteristic curve (AUROC), a standard metric for evaluating the performance of binary classifiers. This focus on AUROC ensured that our models were well-calibrated and capable of distinguishing between cases with and without appendicitis.\n\nOverall, our evaluation included comparisons with both clinical benchmarks and simpler baselines, providing a thorough assessment of our models' effectiveness in diagnosing acute appendicitis.",
  "evaluation/confidence": "The evaluation of our models included the calculation of confidence intervals for the performance metrics. Specifically, the area under the receiver operating characteristic curve (AUROC) values were reported with their respective confidence intervals. This provides a measure of the uncertainty around the point estimates of the AUROC, allowing for a more nuanced interpretation of the model's performance.\n\nStatistical significance was assessed using DeLong's Test, which compares the AUROC values between different models and diagnostic methods. The p-values from these comparisons indicate whether the differences in performance are statistically significant. For instance, the HIVE model showed significantly higher performance than two out of three emergency department (ED) physicians during their initial assessment. Similarly, both the HIVE and HIVE-LAB models demonstrated significantly better performance compared to the Alvarado scoring system. These results suggest that our models are superior to the baseline methods in terms of diagnostic accuracy for appendicitis.\n\nAdditionally, the reader study involving ED physicians provided further validation of our models' performance. The physicians' diagnostic accuracy improved when laboratory test results were included, although this improvement was not statistically significant. This indicates that while laboratory results can be beneficial, the models' performance remains robust even without them. Overall, the statistical analyses support the claim that our models offer a reliable and effective tool for diagnosing appendicitis, outperforming traditional methods and providing a valuable aid to clinical decision-making.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, the data used within the manuscript and supplementary information files is provided. Additionally, the code and models developed for this study are accessible via repositories on GitHub and Figshare. The GitHub repository can be found at the link provided in the manuscript, and the Figshare repository is associated with the DOI also mentioned in the manuscript. These repositories contain the necessary tools and models to replicate the study's findings and evaluations. The specific details about the data and code availability are outlined in the \"Data availability\" section of the publication."
}