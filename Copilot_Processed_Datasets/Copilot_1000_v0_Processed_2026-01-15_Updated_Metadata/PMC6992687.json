{
  "publication/title": "Individual-patient prediction of meningioma malignancy and survival using the Surveillance, Epidemiology, and End Results database.",
  "publication/authors": "Moreau JT, Hankinson TC, Baillet S, Dudley RWR",
  "publication/journal": "NPJ digital medicine",
  "publication/year": "2020",
  "publication/pmid": "32025573",
  "publication/pmcid": "PMC6992687",
  "publication/doi": "10.1038/s41746-020-0219-5",
  "publication/tags": "- Machine Learning\n- Meningioma\n- Predictive Modeling\n- Clinical Outcomes\n- Survival Analysis\n- Malignancy Prediction\n- SEER Database\n- Feature Importance\n- Tumor Size\n- Age at Diagnosis\n- Classi\ufb01cation Performance\n- Electronic Health Records\n- Data Science\n- Medical Imaging\n- Cancer Genomics",
  "dataset/provenance": "The dataset used in this study was sourced from the Surveillance, Epidemiology, and End Results (SEER) program. The latest SEER data release, from November 2017, was queried using SEER*Stat v8.3.5. The data included patients diagnosed with meningioma (WHO ICD-O-3 histology codes 9530-9539) in the brain and spinal cord between 2004 and 2015 across 18 registries in 13 states. Initially, 88,015 patients were identified. However, only the first meningioma recorded in SEER for each patient was included in the analyses. This resulted in a dataset that has been used to develop and validate predictive models of meningioma malignancy and associated survival. The data has also been used in previous epidemiological reports and could lead to the development of new practical diagnostic and prognostic tools in oncology. The dataset is available for download through the SEER program.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a test set. The test set, which was used for unbiased estimation of model performance, consisted of 18,854 patients, representing 30% of the total dataset. The remaining 70%, comprising 43,990 patients, was used for training and cross-validation purposes. Within the training set, a subset of 13,197 cases was specifically allocated for cross-validation and initial model exploration. This cross-validation subset was used to fine-tune the model and ensure its robustness before final evaluation on the test set. The distribution of data points in each split was designed to ensure that the model's performance could be accurately assessed without bias, with the test set providing an independent evaluation of the model's generalizability.",
  "dataset/redundancy": "The dataset used in this study was split into training, cross-validation, and test sets to ensure robust model evaluation. Specifically, 30% of the 62,844 included patients were randomly selected and set aside for use as a test dataset. This test set was sequestered until the final models were developed, allowing for a pseudo-prospective evaluation and reducing bias in the scoring of model performance. The remaining 70% of the data, totaling 43,990 patients, were used for training and cross-validation. Out of these, 13,197 cases were used for cross-validation and initial model exploration.\n\nThe use of a true test set is crucial for unbiased estimation of model performance, as cross-validation alone can lead to artificially inflated classification accuracy due to human or automated optimization of models and parameters. The random assignment of patients to the test set ensures that the training and test sets are independent, which is essential for evaluating the generalizability of the models.\n\nThe distribution of the dataset is notably imbalanced, with approximately 95% of meningiomas being benign. This imbalance is addressed through various techniques, such as random undersampling and the use of weighted scoring metrics like the weighted F1 score. The F1 score is particularly suited for imbalanced datasets as it penalizes misclassifications of the minority class, which in this case are non-benign meningiomas.\n\nThe approach taken in this study is consistent with best practices in machine learning, ensuring that the models are evaluated on data that they have not seen during training. This methodology provides a more reliable assessment of model performance and generalizability, which is critical for developing practical diagnostic and prognostic tools in oncology.",
  "dataset/availability": "All data used in this study are available for download through the Surveillance, Epidemiology, and End Results (SEER) program. The SEER program is a publicly available resource that provides data on cancer statistics in the United States. The data includes patients diagnosed between 2004 and 2015 across 18 registries in 13 states. The data was queried using SEER*Stat v8.3.5 for all cases of meningioma recorded in the brain and spinal cord. The data includes a variety of clinical variables such as age, sex, and tumor size, which were used to develop and validate predictive models of meningioma malignancy and associated survival.\n\nThe data splits used in the study were enforced by randomly assigning 30% of patients to a \"test\" dataset, which was sequestered until the final models were developed. This allowed for pseudo-prospective evaluation of the models and reduced bias in the scoring of model performance. The remaining 70% of the data was used for training and cross-validation. The use of a true \"test\" set allows for unbiased estimation of model performance, which cannot be achieved through cross-validation alone. The data splits were enforced by using the scikit-learn Python module for preprocessing, hyperparameter optimization, cross-validation, and scoring. The data is available for download through the SEER program, and the source code for the meningioma.app is available for download at https://github.com/jeremymoreau/meningioma.",
  "optimization/algorithm": "The machine-learning algorithm class used is an ensemble classifier, specifically a voting ensemble classifier combining balanced logistic regression and balanced random forest base models. This classifier is not entirely new, as it builds upon existing algorithms like the Balanced Random Forest (BRF) and random forest algorithms. The BRF algorithm is an extension of the popular random forest algorithm, which constructs ensembles of decision trees and uses a voting procedure to determine the overall classification decision.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of the study was on applying and validating these models for a specific medical application\u2014predicting meningioma malignancy and survival\u2014rather than on developing a new machine-learning algorithm. The study emphasizes the practical application and validation of these models using clinical data, which is more aligned with medical and computational biology journals. The ensemble classifier was implemented using existing Python packages, such as imbalanced-learn for random undersampling and MLxtend for building the ensemble voting classifier. Additionally, hyperparameter optimization was performed using randomized fivefold stratified K-fold cross-validation, with the weighted F1 score as the primary scoring metric. This approach was chosen to address the class imbalance in the dataset, where approximately 95% of meningiomas were benign. The use of Platt scaling for probability calibration further improved the model's performance.",
  "optimization/meta": "The model employed in this study utilizes a meta-predictor approach, specifically a voting ensemble classifier known as BLR-RF. This classifier combines two base models: balanced logistic regression (BLR) and balanced random forest (BRF). The BLR-RF classifier implements a random undersampling procedure, similar to the BRF algorithm, where each base classifier in the ensemble is trained on a randomly selected class-balanced subsample of the training dataset. This approach helps to address the issue of class imbalance in the dataset, which is crucial given that approximately 95% of meningiomas are benign.\n\nThe training data for the base models is independent, as the dataset was split into training and test sets. Specifically, 30% of the patients were set aside for the test dataset, ensuring that the training and validation processes are unbiased. Additionally, a subset of the training data was used for hyperparameter optimization and cross-validation, further ensuring that the models are evaluated on independent data.\n\nThe use of Platt scaling for probability calibration was applied to a subset of data not used in the initial training, which helps to improve model calibration without compromising the independence of the training data. This two-step process ensures that the final model's performance is robust and generalizable to new, unseen data.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for preparing the dataset for the machine-learning algorithm. Initially, features with low-frequency classes were recoded into more general categories to ensure sufficient examples for training and cross-validation. This step was essential to avoid issues related to data sparsity, which can negatively impact model performance.\n\nAdditionally, features were recoded when it was determined that two or more classes did not provide additional information regarding the outcome variable. For instance, patients with meningiomas on the left versus the right side had equivalent survival outcomes, so these categories were combined.\n\nThe \"Survival months\" variable in SEER was used to define survival, calculated based on the date of diagnosis and the date of last contact. Censoring was based on the \"Vital status recode\" variable in SEER. This approach ensured that the survival data was accurately represented and that censored data was appropriately handled.\n\nRace was recoded into \"white,\" \"black,\" and \"other\" groups. Tumor laterality was recoded as \"bilateral\" (e.g., large meningiomas extending over both hemispheres), \"midline\" (e.g., falcine meningiomas), or \"not bilateral.\" The original SEER coding for surgical procedures was preserved, except for specific codes that were recoded as \"other surgery\" because they accounted for a very small percentage of all surgically treated cases.\n\nThese preprocessing steps were essential for creating a robust and reliable dataset that could be effectively used for training and validating the machine-learning models. By ensuring that the data was well-prepared, we could focus on developing accurate and generalizable models for predicting malignancy and survival outcomes.",
  "optimization/parameters": "In our study, the number of input parameters used in the model was initially quite extensive, drawing from various clinical variables available in the SEER database. However, through a rigorous feature selection process, we identified that tumor size and age at diagnosis were the two most important features. These features were the only ones retained in the final model, significantly simplifying the input parameter set.\n\nThe selection of these parameters was driven by their importance in predicting meningioma malignancy and survival outcomes. We employed techniques such as feature importance analysis and evaluated candidate models using confusion matrices and other performance metrics. This process ensured that the selected parameters were not only relevant but also effective in improving model performance.\n\nThe use of a limited set of clinical variables demonstrates the model's efficiency and practicality, making it more accessible and interpretable for clinicians. This approach also aligns with our goal of developing models that can be easily tested and improved upon, fostering further research and application in clinical settings.",
  "optimization/features": "The input features for our models were carefully selected through a process of exploratory data analysis and consideration of data availability. We initially considered a wide range of potential features, but we excluded variables with large numbers of missing values to ensure robustness. The final set of features included age, tumor size, race, tumor site, and surgery, among others. These features were chosen for their discriminatory capability in relation to the outcomes of interest\u2014malignancy and survival.\n\nFeature selection was indeed performed, and it was conducted using only the training set to prevent data leakage. This approach ensures that the selection process does not inadvertently use information from the test set, which could lead to overoptimistic performance estimates. The selection criteria focused on the availability of data and the ability of each feature to discriminate between different outcomes. For example, tumor size and age at diagnosis were identified as the most important features for the malignancy model.\n\nAdditionally, we recoded certain features to create more general classes where appropriate. This step was taken to ensure that each class had sufficient examples for training and cross-validation. For instance, tumor laterality was recoded into categories such as \"bilateral,\" \"midline,\" or \"not bilateral\" to better capture the relevant information without overfitting to rare categories. Similarly, race was recoded into broader groups like \"white,\" \"black,\" and \"other\" to maintain a balanced dataset.\n\nIn summary, the feature selection process was rigorous and focused on using the training set exclusively. This approach helped us to build models that are generalizable and robust, with a final set of features that are both informative and practical for clinical use.",
  "optimization/fitting": "The fitting method employed in this study involved a voting ensemble classifier combining balanced logistic regression and balanced random forest base models. This approach was chosen to address the significant class imbalance in the dataset, where approximately 95% of meningiomas were benign.\n\nTo mitigate overfitting, several strategies were implemented. First, a random undersampling procedure was used, akin to the Balanced Random Forest algorithm, where each base classifier in the ensemble was trained on a randomly selected class-balanced subsample of the training dataset. This ensured that each bootstrap sample contained roughly equal numbers of benign and non-benign tumors, thereby reducing the risk of the model becoming too specialized to the training data.\n\nAdditionally, hyperparameter optimization was performed using 1000 iterations of randomized fivefold stratified K-fold cross-validation. This technique helps in selecting the best hyperparameters by evaluating the model's performance on multiple subsets of the data, thereby providing a more robust estimate of the model's generalization capability.\n\nA second probability calibration step using Platt scaling was applied with a subset of data not used in the initial training. This step improved the model's calibration, ensuring that the predicted probabilities were well-aligned with the true observed distribution of each class in the test dataset.\n\nTo rule out underfitting, the model's performance was evaluated using a weighted F1 score, which penalizes misclassifications of the minority class (non-benign meningiomas) to a greater extent. This scoring metric is particularly important in imbalanced datasets, as it ensures that the model is not simply predicting the majority class.\n\nFurthermore, the model's performance was assessed using confusion matrices, precision-recall curves, and receiver operating characteristic (ROC) curves. These metrics provided a comprehensive evaluation of the model's ability to correctly classify both benign and non-benign meningiomas, ensuring that the model was neither overfitting nor underfitting the data.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and improve the robustness of our models. One key approach was the use of ensemble methods, specifically a voting ensemble classifier combining balanced logistic regression and balanced random forest models. This ensemble method helps to reduce overfitting by averaging the predictions of multiple models, thereby mitigating the risk of any single model overfitting the training data.\n\nAdditionally, we implemented random undersampling within the balanced random forest algorithm. This involved resampling the non-minority classes so that each bootstrap sample contained roughly equal numbers of benign and non-benign tumors. This technique helps to address class imbalance, which is crucial in our highly imbalanced dataset where approximately 95% of meningiomas are benign.\n\nHyperparameter optimization was performed using 1000 iterations of randomized fivefold stratified K-fold cross-validation. This process helps to ensure that the model generalizes well to unseen data by evaluating performance across multiple splits of the data.\n\nFurthermore, we applied Platt scaling as a second probability calibration step using a subset of data not used in the initial training. This calibration step improves the model's probability estimates, making them more reliable and reducing the risk of overfitting.\n\nThese techniques collectively contribute to the prevention of overfitting and enhance the model's ability to generalize to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available. We performed hyperparameter optimization using 1000 iterations of randomized fivefold stratified K-fold cross-validation. The primary scoring metric for this process was the weighted F1 score, which is implemented in scikit-learn. This approach was chosen to address the imbalanced nature of our dataset, ensuring that misclassifications of the minority class (non-benign meningiomas) were appropriately penalized.\n\nThe source code for our meningioma application, which includes the implementation of these configurations and optimization parameters, is available for download. It can be accessed at https://github.com/jeremymoreau/meningioma. The code is provided under a license that allows for its use and modification, facilitating reproducibility and further research.\n\nAdditionally, the data used in this study is available for download through the SEER program, ensuring that others can replicate our findings and build upon our work. The reporting summary linked to this paper provides further information on the experimental design, including details on the configuration and optimization processes.",
  "model/interpretability": "The models developed in this study are designed with a focus on transparency and interpretability, aiming to make them accessible and understandable to clinicians. Unlike many machine learning models that can be seen as black boxes, our approach provides clear insights into the decision-making process.\n\nOne of the key features of our models is the use of ensemble classifiers, which not only provide binary predictions but also offer probability estimates. This is achieved by calculating the proportion of votes within the ensemble. For instance, if 50 out of 100 base classifiers predict a certain outcome, the predicted probability is 50%. This probabilistic output is crucial for clinicians as it gives a sense of the confidence level behind each prediction.\n\nAdditionally, the models were calibrated to ensure that the predicted probabilities align well with the actual observed outcomes. This calibration step is essential for providing reliable probability estimates, making the models more transparent and trustworthy.\n\nThe importance of individual features in the models is also clearly illustrated. For example, tumor size and age at diagnosis were identified as the two most important features in the malignancy model. This transparency allows clinicians to understand which factors are driving the predictions, making it easier to interpret and trust the model's outputs.\n\nFurthermore, the models' performance is evaluated using various metrics such as sensitivity, specificity, positive predictive value, and negative predictive value. These metrics provide a comprehensive view of the model's performance, helping clinicians to understand its strengths and limitations.\n\nThe web application, meningioma.app, is another example of our commitment to transparency. It allows clinicians to input basic clinical details and obtain straightforward predictions of malignancy and survival. The app uses the same models described in the study, ensuring that the predictions are based on well-validated and transparent methods.\n\nIn summary, the models developed in this study are designed to be transparent and interpretable, providing clinicians with clear insights into the decision-making process. This transparency is achieved through the use of ensemble classifiers, feature importance analysis, performance evaluation metrics, and a user-friendly web application.",
  "model/output": "The models developed in this study are classification models. Specifically, two classifiers were created: one for predicting meningioma malignancy and another for predicting survival outcomes. The malignancy classifier differentiates between benign and non-benign tumors, focusing on binary classification due to the limited set of clinical variables available. The survival model predicts the likelihood of survival based on various clinical features.\n\nFor the malignancy classifier, performance metrics include a weighted F1 score of 0.82, an area under the curve (AUC) of 0.83, a sensitivity of 0.79, and a specificity of 0.75. The positive predictive value (PPV) is 0.14, and the negative predictive value (NPV) is 0.99. Feature importance analysis identified tumor size and age at diagnosis as the most significant features.\n\nThe survival model's performance is evaluated using time-dependent average precision (APt) and area under the receiver operating characteristic curve (AUCt). For 5-year survival, the APt is 0.62, and the AUCt is 0.81. Additionally, a Uno's C-statistic of 0.79 was obtained, indicating good model performance. The log hazard ratios for each feature in the survival model show the proportional impact of each variable on the probability of death.\n\nLearning curves illustrate that model performance improves with an increasing number of training examples, plateauing around 5,000 to 10,000 samples. This suggests that the models are robust and generalizable, as evidenced by their performance across different SEER registries. The models provide valuable predictions for individual patients, complementing traditional epidemiological reports and offering practical tools for clinical use.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the application is available for download. It can be accessed via a public repository on GitHub. This allows users to explore, modify, and run the algorithm as needed. The application is designed to be user-friendly, enabling clinicians to easily test the models and provide feedback for improvement. This open-source approach aims to inspire further development and interest in the possibilities of such tools.",
  "evaluation/method": "The evaluation method for our models involved a combination of cross-validation and a separate test set to ensure unbiased performance estimation. We used 1000 iterations of randomized fivefold stratified K-fold cross-validation for hyperparameter optimization, employing the weighted F1 score as the primary metric. This approach was chosen because it penalizes misclassifications of the minority class, which is crucial given the highly imbalanced nature of our dataset. Additionally, we evaluated candidate models using confusion matrices derived from the cross-validation set, providing a clear representation of true versus predicted classes and rates of true and false positives and negatives.\n\nFor the malignancy model, we utilized a voting ensemble classifier combining balanced logistic regression and balanced random forest base models. This classifier implements a random undersampling procedure to address class imbalance, ensuring that each base classifier in the ensemble is trained on a class-balanced subsample of the training dataset. To improve model calibration, we applied Platt scaling with a subset of data not used in the initial training.\n\nModel scoring for the malignancy model included the F1 score and confusion matrices, as well as precision-recall and receiver operating characteristic (ROC) curves. These metrics were summarized by average precision (AP) and area under the curve (AUC) values. However, it is important to note that AUC can provide overly optimistic estimates of performance in imbalanced datasets, making the precision-recall curve a valuable complement.\n\nFor the survival model, we used Cox\u2019s proportional hazards model, which is well-suited for working with censored survival data and provides easily interpretable prediction probabilities. We reported time-dependent average precision (APt) and area under the curve (AUCt) values using the R implementation in the APtools package, as well as Uno\u2019s C-statistic. These metrics, along with calibration plots of predicted versus observed risk, provided a comprehensive evaluation of model performance. Confidence intervals were computed using bootstrap resampling of the test set with 1000 iterations for precision-recall, ROC, APt, and AUCt values.",
  "evaluation/measure": "In the evaluation of our models, we employed a comprehensive set of performance metrics to ensure a thorough assessment. For the malignancy classifier, we primarily used the weighted F1 score, which is particularly suitable for imbalanced datasets like ours. This metric penalizes misclassifications of the minority class more severely, making it crucial for our imbalanced dataset where classifying all meningiomas as benign could yield high accuracy but would be misleading. Additionally, we utilized confusion matrices to represent true versus predicted classes and to calculate rates of true and false positives and negatives.\n\nTo provide a more nuanced evaluation, we also reported precision-recall and receiver operating characteristic (ROC) curves. The precision-recall curve is especially valuable in imbalanced datasets as it focuses on the performance of the positive class, whereas the ROC curve provides a broader view of the trade-off between sensitivity and specificity. We summarized these curves using the average precision (AP) and the area under the ROC curve (AUC) metrics. However, it is important to note that AUC can sometimes provide overly optimistic estimates in imbalanced datasets, which is why we also evaluated the precision-recall curve.\n\nFor the survival model, we used time-dependent metrics such as the area under the curve (AUCt) and average precision (APt), which are implemented in the R package APtools. These metrics are particularly useful for censored survival data and provide easily interpretable prediction probabilities. We also reported Uno\u2019s C-statistic, which is implemented in the scikit-survival Python package. This statistic, along with time-dependent AP and AUC values, offers a comprehensive evaluation of the model's performance.\n\nFurthermore, we included calibration plots to compare predicted probabilities against the true observed distribution of each class in the test dataset. This helps in assessing how well the predicted probabilities align with the actual outcomes. Confidence intervals for precision-recall, ROC, APt, and AUCt values were computed using bootstrap resampling of the test set with 1000 iterations, ensuring the robustness of our performance metrics.\n\nIn summary, our choice of performance metrics is representative of current best practices in the literature, particularly for handling imbalanced datasets and censored survival data. This comprehensive approach ensures that our models are rigorously evaluated and that their performance is accurately reported.",
  "evaluation/comparison": "In our evaluation process, we conducted a thorough comparison of various models to ensure the robustness and effectiveness of our approach. We performed an exploratory model comparison for initial models after hyperparameter optimization but prior to probability calibration. This comparison included several machine learning algorithms such as K-Nearest Neighbors, Logistic Regression, Random Forests, Balanced Random Forest, Balanced Logistic Regression, and a combination of Balanced Logistic Regression and Random Forests.\n\nFor each model, we generated confusion matrices to illustrate the predicted versus true labels, evaluated on the test set. These matrices were normalized across each row to provide a clear view of the performance metrics. Additionally, we used kernel density plots to show the distribution of predicted probabilities for benign and non-benign meningioma classes. Calibration diagrams were also created to plot predicted probabilities against the true observed distribution of each class in the test dataset.\n\nThese comparisons allowed us to assess the performance of different models and select the most effective ones for further refinement. The use of confusion matrices, kernel density plots, and calibration diagrams provided a comprehensive evaluation of model performance, ensuring that our final models were well-calibrated and accurate.",
  "evaluation/confidence": "Confidence intervals were computed using bootstrap resampling of the test set with 1000 iterations for precision-recall, ROC, APt, and AUCt values. This approach ensures that the performance metrics are robust and provides a measure of uncertainty around the estimates.\n\nThe use of 95% confidence intervals in various figures, such as the receiver operating characteristic curve and precision-recall curve, indicates that the results are statistically significant. These intervals help in understanding the variability and reliability of the model's performance metrics.\n\nAdditionally, the inclusion of chance-level values in the reporting of average precision and AUC metrics further supports the statistical significance of the results. By comparing the model's performance against these baseline values, it is evident that the method outperforms random guessing, reinforcing the claim of superiority over baselines.\n\nThe use of stratified K-fold cross-validation with a weighted F1 score as the primary scoring metric also contributes to the statistical rigor of the evaluation. This method ensures that the model's performance is assessed across different subsets of the data, reducing the risk of overfitting and providing a more reliable estimate of its generalizability.",
  "evaluation/availability": "The raw evaluation files used in this study are available for download through the Surveillance, Epidemiology, and End Results (SEER) program. This program provides a comprehensive source of population-based information, which was instrumental in our research. The data is publicly accessible, allowing other researchers to replicate and build upon our findings. The SEER program ensures that the data is reliable and standardized, making it a valuable resource for medical and epidemiological studies. For those interested in accessing this data, detailed instructions and guidelines are available on the SEER website. This transparency supports the reproducibility of our results and encourages further advancements in the field."
}