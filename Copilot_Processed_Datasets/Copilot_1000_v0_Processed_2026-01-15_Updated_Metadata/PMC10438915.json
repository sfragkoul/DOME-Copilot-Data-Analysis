{
  "publication/title": "Control of Magnetic Surgical Robots With Model-Based Simulators and Reinforcement Learning.",
  "publication/authors": "Barnoy Y, Erin O, Raval S, Pryor W, Mair LO, Weinberg IN, Diaz-Mercado Y, Krieger A, Hager GD",
  "publication/journal": "IEEE transactions on medical robotics and bionics",
  "publication/year": "2022",
  "publication/pmid": "37600471",
  "publication/pmcid": "PMC10438915",
  "publication/doi": "10.1109/tmrb.2022.3214426",
  "publication/tags": "- Reinforcement Learning\n- Model-Based Simulation\n- Magnetic Needle Control\n- Surgical Robotics\n- Neural Networks\n- Robot Training\n- Medical Robotics\n- Simulation Environments\n- Real-World Robotics\n- Tetherless Manipulation",
  "dataset/provenance": "The dataset used in our experiments was collected through manual exploration of the full area of a Petri dish at varying current levels. This exploration was conducted using a controller, allowing for both uniform and rapid coverage of the space. The data collection process resulted in a total of 34,742 data points, which corresponds to approximately 3.2 hours of recordings. These data points were then normalized and used to train a neural network, achieving a validation loss of 0.033 and a test loss of 0.045.\n\nThis dataset was specifically collected for the development of our model-based system (MBS) and has not been used in previous papers or by the broader community. The data captures the dynamics of the MagnetoSuture\u2122 environment, which includes challenges such as asymmetric needle properties, surface roughness, temperature-dependent fluid viscosity, and non-quiescent fluid conditions. These factors make accurate simulation of the environment difficult, highlighting the importance of our collected data in training effective models.",
  "dataset/splits": "Not applicable",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our work falls under the class of reinforcement learning (RL) algorithms. Specifically, we utilize the Twin Delayed Deep Deterministic policy gradient (TD3) algorithm. This algorithm is not new; it has been previously established and is well-suited for continuous action spaces, which is crucial for our system's control requirements.\n\nThe TD3 algorithm leverages two neural networks: an actor network and a critic network. The actor network maps states to actions, representing the current optimal policy, while the critic network evaluates the Q-value of each state-action pair. These networks are updated in tandem, with the critic first updating to the Q-value of the actor's chosen actions, followed by the actor updating its behavior based on the critic's gradient.\n\nThe choice of TD3 for our application is driven by its effectiveness in handling continuous action spaces, which is essential for controlling the current values of the coils in our system. The algorithm's ability to manage continuous actions makes it more appropriate than discrete action space algorithms like Deep Q-Networks (DQN).\n\nRegarding the publication venue, our focus is on the application of this algorithm within the context of medical robotics and magnetic needle steering. The primary contributions of our work lie in the innovative application of RL techniques to this specific domain, rather than the development of a new RL algorithm. Therefore, publishing in a medical robotics journal aligns with our goals of demonstrating the practical utility of RL in surgical environments.",
  "optimization/meta": "The meta-predictor in our optimization framework leverages data from multiple machine-learning algorithms to enhance performance. Specifically, it integrates outputs from both model-based and model-free reinforcement learning methods. This approach allows the meta-predictor to benefit from the strengths of different learning paradigms, combining the efficiency of model-based methods with the robustness of model-free techniques.\n\nThe constituent machine-learning methods include various reinforcement learning algorithms, such as those that use off-policy and on-policy learning. These methods are trained in different environments, including simulated and pseudo-simulated settings, to gather diverse data. The meta-predictor then uses this aggregated data to make more informed decisions, improving overall learning efficiency and performance.\n\nRegarding the independence of training data, it is crucial to ensure that the data used for training the meta-predictor is independent across different environments and methods. This independence is maintained by carefully designing the training protocols and environments, ensuring that the data from one method or environment does not bias the learning process of another. However, the exact independence of the training data can vary depending on the specific implementation and the degree of overlap between the environments used for training the different methods.",
  "optimization/encoding": "In our work, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithms. We began by collecting a substantial amount of data while exploring the full area of the Petri dish at various current levels. This exploration was conducted manually using a controller, allowing for both uniform and rapid coverage of the space. The collected data consisted of 34,742 data points, which amounted to approximately 3.2 hours of recordings.\n\nTo prepare this data for training our neural networks, we first normalized it. Normalization is a standard preprocessing technique that scales the data to a standard range, typically between 0 and 1 or -1 and 1. This step is essential for ensuring that the neural network can learn effectively, as it helps to prevent issues related to differing scales of input features.\n\nThe normalized data was then used to train our neural network, which aimed to learn the dynamics of the system. The training process involved achieving an L2 validation loss of 0.033 and a test loss of 0.045. These loss values indicate the performance of the neural network in predicting the dynamics of the system, with lower values signifying better performance.\n\nIt is important to note that while this data was sufficient for training a neural network to learn the dynamics, it was not enough for training an off-policy reinforcement learning (RL) algorithm directly. This highlights the difference in sample efficiency between supervised learning and RL, where the latter requires more data to achieve similar levels of performance.\n\nIn summary, our data encoding and preprocessing involved manual exploration of the Petri dish, collection of a large dataset, normalization of the data, and training of a neural network to learn the system dynamics. These steps were essential for the success of our machine-learning algorithms and the overall optimization of our system.",
  "optimization/parameters": "The model utilizes a fully connected neural network for predicting the next state in the system. The input layer of this network consists of 5 parameters. These parameters include the previous state, which comprises the location (x, y) and angle (\u03b8) of the needle, and the coil currents (I). The network is designed with three hidden layers, having 256, 128, and 32 neurons respectively. The output layer predicts the next state, which includes the location (x, y) and angle (\u03b8) of the needle.\n\nThe selection of the number of parameters and the architecture of the neural network was guided by the need to capture the dynamics of the system accurately. The input parameters were chosen based on the physical interactions within the system, where the magnetic forces from the coils interact with the magnetic needle to induce motion. The hidden layers were sized to balance computational efficiency and the complexity of the dynamics being modeled. The use of ReLU activations and batch normalization layers in the input and hidden layers further aids in effective learning and generalization.\n\nThe critic network in the TD3 algorithm also has a specific architecture. It consists of an input layer of size 7, which includes the states (x, y, \u03b8), target coordinates (xuser, yuser), and the desired action (xv, yv). The critic network has two hidden layers, each with 256 neurons, and an output layer of size 1 for the Q-value. The actor network, which maps states to actions, has an input layer of size 5 for the state and target, two hidden layers of size 256, and an output layer corresponding to the action space dimensionality (2). The activation functions used are ReLU for all layers except the output layer of the actor, which uses the hyperbolic tangent (tanh) to map the output to the range [\u22121, 1]. Batch normalization is avoided in the TD3 architecture to prevent hindering the learning process.",
  "optimization/features": "The input features for our optimization process consist of a combination of state and target coordinates. Specifically, the input layer size is 7, which includes 3 features for the state (x, y, \u03b8), 2 features for the target coordinates (x_user, y_user), and 2 features for the desired action (x_v, y_v). This setup is designed to allow the reinforcement learning algorithm to differentiate states based on both the needle's position and the target position desired by the user.\n\nFeature selection was not explicitly performed in the traditional sense, as the features were chosen based on the requirements of the task and the dynamics of the system. The selection of these features was informed by the need to capture the essential aspects of the needle's state and the target location, ensuring that the algorithm could effectively learn to navigate to any user-chosen location.\n\nThe features were determined prior to training and were not selected using the training set alone. Instead, they were chosen based on domain knowledge and the specific needs of the reinforcement learning algorithm. This approach ensures that the algorithm has all the necessary information to make informed decisions during training and operation.",
  "optimization/fitting": "The fitting method employed in our study involves training neural networks to learn the dynamics of a magnetic needle system. The state space consists of the position and orientation of the needle, which are obtained through a neural network-based localization approach. This method uses a U-Net convolutional neural network for image segmentation and clustering techniques for position and orientation extraction.\n\nThe number of parameters in our neural networks is indeed larger than the number of training points. To address potential overfitting, we utilized several strategies. First, we normalized the data collected during the exploration of the Petri dish, which amounted to about 3.2 hours of recordings and 34,742 data points. Normalization helps in stabilizing and speeding up the training process. Second, we employed batch normalization layers in our neural networks, which help in regularizing the model and reducing overfitting. Additionally, we used stochastic gradient descent (SGD) with a suitable learning rate to minimize the L2 loss between the ground truth and the predicted states. The use of L2 loss encourages the model to make small, incremental improvements, which can help in preventing overfitting.\n\nTo ensure that our models were not underfitting, we monitored the validation loss during training. The L2 validation loss achieved was 0.033, and the test loss was 0.045, indicating that the model was able to generalize well to unseen data. Furthermore, the model's performance was evaluated on tasks such as moving the needle to the center of the dish and to random target locations, demonstrating its effectiveness in real-world applications. The training process was conducted in a model-based simulation (MBS) environment, which allowed for extensive testing and validation without the constraints of the real-world system. This approach ensured that the model was adequately trained and could handle the complexities of the magnetic needle system.",
  "optimization/regularization": "In our work, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method involved the use of batch normalization in our neural networks. Batch normalization helps to stabilize and accelerate the training process by normalizing the inputs of each layer, which in turn reduces the internal covariate shift. This technique aids in preventing overfitting by making the model more resilient to variations in the input data.\n\nAdditionally, we utilized dropout layers in our neural network architectures. Dropout is a regularization technique where randomly selected neurons are ignored during training. This forces the network to learn redundant representations and prevents it from becoming too reliant on any single neuron, thereby reducing overfitting.\n\nWe also implemented early stopping as a regularization method. Early stopping monitors the model's performance on a validation set and halts the training process when the performance stops improving. This prevents the model from overfitting to the training data by avoiding excessive training epochs.\n\nFurthermore, we ensured that our models were trained on a diverse and representative dataset, which helps in generalizing better to unseen data. We also performed cross-validation to assess the model's performance across different subsets of the data, providing a more reliable estimate of its generalization capability.\n\nIn summary, our regularization methods included batch normalization, dropout, early stopping, and the use of a diverse training dataset, all of which contributed to preventing overfitting and enhancing the model's performance.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are not explicitly detailed in the provided information. However, we do discuss the architecture of the neural network used for our model-based simulation. The network is fully connected with an input layer of size 5, comprising the state and action. It includes three hidden layers with sizes 256, 128, and 32, respectively, followed by an output layer of size 3 predicting the next state. The input and hidden layers utilize ReLU activations and batch normalization layers.\n\nRegarding the availability of model files and optimization parameters, this information is not provided. We focus on the methodological approach and the advantages of using a model-based simulation for reinforcement learning, such as increased safety and speed compared to real-world training. The specific details about where to access these files or the license under which they are available are not mentioned.\n\nFor those interested in replicating our work, the general architecture and training methodology are described, but the exact hyper-parameters and optimization schedules would need to be inferred or determined through further experimentation.",
  "model/interpretability": "The model we employed in our study is primarily a black-box model, particularly when considering the reinforcement learning (RL) algorithms used. The Twin Delayed Deep Deterministic policy gradient (TD3) algorithm, which we utilized, involves neural networks that learn complex mappings from states to actions and evaluate the quality of these state-action pairs. These neural networks are inherently black-box models, meaning that the internal workings and decision-making processes are not easily interpretable.\n\nHowever, our approach does incorporate elements that enhance transparency to some extent. For instance, the model-based simulation (MBS) component of our method involves training a neural network to learn the dynamics of the system. This generative model predicts the next state of the needle based on the current state and actions taken. While the neural network itself is a black-box, the inputs and outputs are well-defined and interpretable. The inputs include the current state of the needle (location and angle) and the coil currents, while the outputs are the predicted next state (location and angle). This structure allows us to understand how different actions affect the system's dynamics, providing a level of interpretability.\n\nAdditionally, the use of a deterministic and Markovian model in the MBS component means that given the same initial conditions and actions, the model will always produce the same output. This predictability can help in understanding and verifying the model's behavior. The neural network's architecture, with fully connected layers and ReLU activations, is also transparent in terms of its design and operation, even if the learned parameters are not.\n\nIn summary, while the core RL algorithms are black-box models, the MBS component introduces elements of transparency through its well-defined inputs and outputs, deterministic nature, and interpretable architecture. This hybrid approach allows us to leverage the strengths of both model-free and model-based methods, balancing the need for performance with the desire for interpretability.",
  "model/output": "The model we developed is primarily a regression model. It predicts the next state of a system, specifically the location and angle of a magnetic needle, based on the current state and actions taken. This involves predicting continuous values rather than classifying discrete categories. The neural network used in our model-free reinforcement learning algorithm, specifically the Twin Delayed Deep Deterministic policy gradient (TD3), is designed to handle continuous action spaces. The critic network within TD3 estimates the Q-value for each state-action pair, which is a regression task. Additionally, the generative dynamics model predicts the future state of the system, which is also a regression problem. The model's output is used to optimize the policy for controlling the system, aiming to maximize the expected reward.",
  "model/duration": "The execution time for our model-based simulation (MBS) approach varies depending on the specific task and environment. For the center-based policy, which involves moving to the center of the dish from any starting location, the model achieved maximum reward within approximately 1.5 hours. This is significantly faster than the real-world system, which would require around 300 hours under constant supervision due to safety issues.\n\nFor the more complex random-target policy, which involves moving from a random location to any given location in the dish, the model took around 2.5 million steps to reach its maximum point. This task is more challenging due to the presence of 'dead zones' in the Petri dish, which are areas where the magnetic field is less effective.\n\nThe MBS environment allows for rapid training, taking around 700 steps per second. This is about 233 times faster than the real-world system, which is limited to 3 Hz. The ability to run the MBS environment for extended periods without the constraints of the real-world system, such as overheating or safety concerns, is a significant advantage. This acceleration enables more efficient training and testing of reinforcement learning algorithms.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our evaluation, we employed several distinct methods to assess the performance of our reinforcement learning (RL) agents. We conducted real-world tests focusing on different tracking tasks, including center-point, random-point, and square tracking. For the center-point test, we trained an RL agent specifically for this task. In contrast, the random-point and square tracking tests utilized an RL agent trained to reach user-specified targets. Both types of agents were trained within the same model-based simulation (MBS) environment, ensuring a consistent training framework.\n\nWe also explored the integration of off-policy learning with on-policy exploration within the MBS pseudo-simulation environment. This involved using the same data collected for MBS training and applying it to off-policy learning. We ran multiple agents in parallel to evaluate the learning performance under various configurations: learning without an off-policy data environment, with one off-policy and two on-policy environments, with two off-policy and one on-policy environments, and with all three environments being off-policy.\n\nAdditionally, we compared different RL models based on their real-world performance errors. The models were evaluated on tasks such as reaching the center of a Petri dish and targeting specific points or boxes. The performance was quantified in terms of mean error and standard deviation, providing a clear metric for comparing the effectiveness of each model.\n\nTraining times to achieve maximum reward were also analyzed for different RL methods and tasks. This included comparing the time required for MBS-trained agents to reach the center of a Petri dish or a randomly sampled target, as well as simulation-based and real-world training scenarios. Notably, real-world training was found to be unfeasible due to the extensive durations required.\n\nOverall, our evaluation methodology combined real-world testing, simulation-based training, and performance metrics to thoroughly assess the capabilities and limitations of our RL agents.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report several key metrics to evaluate the real-world performance of our reinforcement learning (RL) models. The primary metric used is the mean error in millimeters, which quantifies the average distance between the target and the actual position achieved by the RL agent. Additionally, we provide the standard deviation of this error to indicate the consistency of the agent's performance.\n\nWe also assess the transferability of the models, indicating whether the models trained in a simulated or model-based environment can effectively transfer their learned policies to real-world scenarios. This is crucial for understanding the practical applicability of our methods.\n\nThe reported metrics are representative of the current literature in RL, particularly in tasks involving precise positioning and control. The mean error and standard deviation are standard metrics used to evaluate the accuracy and reliability of RL agents in various domains. The inclusion of transferability as a metric aligns with recent trends in RL research, which emphasize the importance of bridging the gap between simulation and real-world performance.\n\nThese metrics collectively provide a comprehensive evaluation of our RL models, highlighting their accuracy, consistency, and practical utility.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of different reinforcement learning (RL) methods to assess their performance and training efficiency. We evaluated various RL models, including those trained in model-based simulation (MBS) environments and pure simulation environments. The training times to achieve maximum reward varied significantly among these methods. For instance, the MBS center task required 1.5 hours, while the simulation random task took considerably longer, at 27 hours. Attempts to train RL on a real-world system were found to be impractical due to time and safety constraints.\n\nWe also compared the real-world performance errors of different RL agents. The simulation-based center task had a mean error of 31 mm with no transfer capability, whereas the MBS center task achieved a much lower mean error of 6.3 mm and demonstrated successful transfer. Other MBS tasks, such as target point and target box, also showed promising results with mean errors of 5.3 mm and 7.4 mm, respectively, and successful transfer capabilities.\n\nAdditionally, we explored the use of off-policy RL algorithms, such as TD3, to leverage pre-recorded data. We tested different mixes of off-policy and on-policy environments to evaluate their learning performance. While using some off-policy data accelerated learning, it also introduced instability. Purely off-policy RL failed to achieve sufficient learning due to insufficient data, highlighting the sample inefficiency of off-policy RL compared to our model-based simulation approach.\n\nIn summary, our comparisons revealed that model-based simulation environments offer a more efficient and effective training method for RL agents, balancing speed and sample efficiency. This approach outperformed pure simulation and off-policy RL methods in terms of real-world performance and training time.",
  "evaluation/confidence": "In our evaluation, we have included performance metrics with confidence intervals to provide a comprehensive understanding of our results. Specifically, we report the standard deviation for the mean error in millimeters for different reinforcement learning (RL) models. This allows for an assessment of the variability and reliability of our findings.\n\nFor instance, in our comparison of different RL models and their real-world performance errors, we present the mean error along with the standard deviation. This approach enables us to convey not only the central tendency of our results but also the spread, which is crucial for evaluating the consistency and robustness of our methods.\n\nAdditionally, our training results for the model-based system (MBS)-trained RL agents include averages over time and two standard deviations above and below the mean. This detailed presentation of data helps in understanding the statistical significance of our results and ensures that our claims of superiority over other methods and baselines are well-supported.\n\nBy providing these confidence intervals, we aim to offer a transparent and rigorous evaluation, allowing readers to assess the statistical significance and reliability of our findings. This approach underscores our commitment to delivering high-quality, reproducible research.",
  "evaluation/availability": "Not enough information is available."
}