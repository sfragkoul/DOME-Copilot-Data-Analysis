{
  "publication/title": "Natural Language Processing to identify pneumonia from radiology reports.",
  "publication/authors": "Dublin S, Baldwin E, Walker RL, Christensen LM, Haug PJ, Jackson ML, Nelson JC, Ferraro J, Carrell D, Chapman WW",
  "publication/journal": "Pharmacoepidemiology and drug safety",
  "publication/year": "2013",
  "publication/pmid": "23554109",
  "publication/pmcid": "PMC3811072",
  "publication/doi": "10.1002/pds.3418",
  "publication/tags": "- pneumonia\n- Natural Language Processing\n- sensitivity\n- specificity\n- validity\n- radiology reports\n- electronic medical records\n- outcome validation\n- medical record review\n- automated methods",
  "dataset/provenance": "The dataset used in our study consists of 93,110 chest radiograph reports. These reports were previously manually reviewed for a study focused on the pneumococcal conjugate vaccine. The dataset includes information about patient age, care setting (outpatient vs. inpatient), and comorbidities such as congestive heart failure, chronic lung disease, and cancer. These comorbidities were defined based on ICD-9 codes from the prior 12 months. The data was sourced from a single healthcare system, which may have implications for the generalizability of our findings. The test set used for validation in our study comprises 5,000 reports, which were oversampled for true pneumonia cases and comorbidities to improve the precision of estimates for patient subgroups. This oversampling was done to ensure that the test set accurately reflects the characteristics of the source population.",
  "dataset/splits": "There are two main data splits in our study: the source dataset and the test set.\n\nThe source dataset consists of 93,110 chest radiograph reports. This dataset was previously manually reviewed for a study of pneumococcal conjugate vaccine. Within this dataset, 26,345 reports were positive for pneumonia, and 66,765 were negative for pneumonia.\n\nThe test set comprises 5,000 reports, which were oversampled for true pneumonia cases and comorbidities to improve the precision of estimates of the test statistics for patient subgroups. In this test set, 2,200 reports were positive for pneumonia, and the remaining 2,800 were negative.\n\nThe distribution of data points in each split varies by patient age, comorbidities, and setting of care. For instance, in both the source dataset and the test set, the majority of reports come from the outpatient setting, with 92% in the test set. The mean age of patients in the test set is 55 years. The prevalence of comorbidities such as congestive heart failure, chronic lung disease, and cancer is also considered in the distribution of data points.",
  "dataset/redundancy": "The dataset used in this study consisted of 93,110 chest radiograph reports, which were previously manually reviewed for a study of pneumococcal conjugate vaccine. This dataset was split into a source dataset and a test set. The test set contained 5,000 reports, which were oversampled for true pneumonia cases and comorbidities to improve the precision of estimates of the test statistics for patient subgroups.\n\nThe test set was designed to be independent of the source dataset. This independence was enforced by ensuring that the test set was a separate subset of the overall dataset, distinct from the reports used in the initial training and validation phases. The distribution of the test set reflects the characteristics of the source population, with a focus on including a higher proportion of pneumonia cases and comorbidities to enhance the robustness of the statistical analyses.\n\nThe distribution of the test set compares favorably to previously published machine learning datasets in the medical field. The oversampling of pneumonia cases and comorbidities ensures that the model's performance can be accurately assessed across different patient subgroups, which is crucial for generalizability and clinical applicability. This approach helps in mitigating the risk of bias and ensures that the model's performance is reliable across various patient demographics and clinical settings.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "Not enough information is available.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding process involved several steps to prepare the radiology reports for analysis using the NLP system, ONYX. Initially, the reports were manually reviewed to create a gold standard for pneumonia identification. This involved detailed instructions to ensure consistency, such as excluding certain types of infiltrates that did not qualify as pneumonia. The inter-rater and intra-rater agreements were high, indicating reliable manual reviews.\n\nThe reports were then processed using ONYX, an open-source NLP system designed to interpret free text and produce structured output. ONYX was trained on documents from the pulmonary domain and helped create training cases. It generated concepts from individual sentences, such as identifying a \"localized infiltrate\" with a specific location from a phrase like \"ill-defined density in the right lower lobe.\"\n\nDecision rules were applied to ONYX's output to classify the reports as consistent with pneumonia, inconsistent with pneumonia, or needing manual review. Two classifiers were developed: one prioritizing accuracy and the other minimizing manual review. The first classifier flagged reports for manual review if they contained inconsistent statements, atelectasis with pneumonia, or state change language. The second classifier used an algorithm called ConText to more accurately detect negated and resolved findings and to distinguish clinical history from current findings.\n\nThe data included information about patients' age, care setting, and comorbidities, which were defined based on ICD-9 codes from the prior 12 months. This information was used to model the sensitivity and specificity of ONYX in relation to these characteristics. The reports were oversampled for pneumonia and comorbidities to improve the precision of estimates for patient subgroups.\n\nIn summary, the data encoding process involved manual review to create a gold standard, training ONYX on pulmonary documents, applying decision rules to classify reports, and incorporating patient characteristics to model the performance of the NLP system.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "Not applicable",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model used in this study, ONYX, is not a black-box system but rather a transparent and interpretable NLP tool. ONYX integrates knowledge about syntax and semantics to interpret free text and produce structured output. This means that the decisions made by ONYX are based on clear, rule-based processes that can be understood and verified by human experts.\n\nFor instance, ONYX can generate concepts from specific phrases in radiograph reports. From the phrase \"ill-defined density in the right lower lobe,\" ONYX produces a concept of \"localized infiltrate\" with a location of \"right lower lobe.\" This transparency allows users to see exactly how the system is interpreting the text and making decisions.\n\nDecision rules are applied to ONYX's output to classify reports as consistent with pneumonia, inconsistent with pneumonia, or needing manual review. These rules are designed based on expert knowledge and can be adjusted or reviewed to ensure they align with clinical standards. For example, reports containing seemingly inconsistent statements about pneumonia, such as infiltrates being both present and absent, are flagged for manual review. This rule-based approach ensures that the classification process is understandable and can be audited.\n\nAdditionally, the use of ConText, an algorithm designed to handle negation and historical information, further enhances the transparency of ONYX. ConText helps to accurately detect negated and resolved findings, distinguishing clinical history from current findings. This makes it clear how the system differentiates between past and present conditions, adding another layer of interpretability.\n\nOverall, the transparency of ONYX allows for a clear understanding of how the model processes and classifies radiograph reports, making it a reliable tool for clinical and research applications.",
  "model/output": "The model in question is a classification model. It is designed to classify chest radiograph reports as either consistent with pneumonia, inconsistent with pneumonia, or requiring manual review. The model uses decision rules applied to the output of an NLP system called ONYX, which interprets free text from the reports. Two classifiers were developed: Classifier 1 prioritizes accuracy, aiming to minimize false positives and false negatives, while Classifier 2 prioritizes efficiency, aiming to reduce the number of reports that need manual review. The model's performance is evaluated using metrics such as sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). The output of the model is used to determine which reports need manual review and which can be automatically classified as showing or not showing pneumonia.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The ONYX system, which we used for identifying pneumonia from free-text radiology reports, is available as open-source software. This availability allows other researchers and institutions to adapt and utilize ONYX for their specific outcomes and study needs. By making ONYX open-source, we aim to facilitate further advancements in natural language processing (NLP) and its application in medical research. The open-source nature of ONYX ensures that the community can benefit from its capabilities, improve upon it, and contribute to its development. This approach aligns with our goal of enhancing the validity and efficiency of medical research through automated extraction of information from electronic medical records.",
  "evaluation/method": "The evaluation of the method involved several steps to ensure its accuracy and reliability. Initially, the NLP tool, ONYX, was applied to a set of 1000 previously manually reviewed reports. This step allowed for targeted training based on ONYX's mistakes, leading to improvements in its processing engine, including the addition of new semantic or syntactic grammar rules.\n\nFollowing this, ONYX's performance was assessed using an independent test set of 5,000 reports. This test set was specifically oversampled for pneumonia cases and comorbidities to enhance the precision of the test statistics for various patient subgroups. The evaluation focused on calculating the proportion of reports that ONYX classified as requiring manual review and then estimating the sensitivity and specificity for the remaining reports.\n\nSensitivity was defined as the proportion of true pneumonia reports that ONYX correctly identified, while specificity was the proportion of non-pneumonia reports that ONYX correctly identified as not showing pneumonia. To understand how these measures varied by patient characteristics, multivariable logistic regression was used. This approach modeled ONYX's true positive rate (sensitivity) and false positive rate (1 minus specificity) as functions of patient age, comorbidities, and care setting. Interaction terms between each characteristic and pneumonia status facilitated the estimation of both true and false positive rates in a single model. Weights were used to account for oversampling, and generalized estimating equations provided standard errors that considered potential correlations between multiple reports from the same patient.\n\nUsing the coefficients from this model, the overall sensitivity and specificity of ONYX were estimated for a population with characteristics similar to the source population. From these estimates, the positive predictive value (PPV) and negative predictive value (NPV) were calculated, assuming the pneumonia prevalence in the source population. This step was crucial because PPV increases with higher prevalence, as the proportion of true positives relative to false positives rises. Conversely, NPV decreases with higher prevalence.\n\nAdditionally, ONYX's performance was evaluated for individual radiologists who had read at least 100 reports from the test set. This evaluation provided insights into how the tool performed across different radiologists, highlighting variations in accuracy. The results showed that Classifier 2, which was tailored to decrease manual review, had markedly higher specificity and PPV compared to Classifier 1. This difference was partly due to a new decision rule that classified certain ambiguous reports as not showing pneumonia. A post hoc analysis explored the factors contributing to Classifier 2's higher specificity by comparing the classifiers' performance on the subset of reports that both could classify and examining Classifier 2's accuracy for the reports that only it could classify.",
  "evaluation/measure": "In our evaluation, we focused on several key performance metrics to assess the effectiveness of our NLP tool, ONYX. The primary metrics reported include sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). Sensitivity measures the proportion of true pneumonia cases that ONYX correctly identified, while specificity measures the proportion of non-pneumonia cases that ONYX correctly identified as not showing pneumonia. PPV indicates the probability that a positive result from ONYX truly indicates pneumonia, and NPV indicates the probability that a negative result from ONYX truly indicates the absence of pneumonia.\n\nThese metrics were chosen because they provide a comprehensive view of ONYX's performance in different scenarios. Sensitivity and specificity are fundamental metrics in evaluating the accuracy of diagnostic tools, and PPV and NPV are crucial for understanding the practical implications of ONYX's results in real-world applications. By reporting these metrics, we aim to provide a clear and representative assessment of ONYX's performance, aligning with standard practices in the literature.\n\nAdditionally, we modeled these measures using multivariable logistic regression to examine how they varied by patient characteristics such as age, comorbidities, and care setting. This approach allowed us to estimate ONYX's overall sensitivity and specificity in a population with similar characteristics to the source population. We also calculated PPV and NPV assuming the prevalence of pneumonia as observed in the source population, which is important because these values change with the prevalence of the condition.\n\nThe results showed that Classifier 2, which was tailored to decrease the amount of manual review, had markedly higher specificity and PPV compared to Classifier 1. This was partly due to a new decision rule that classified certain ambiguous reports as not showing pneumonia. The performance of ONYX was also assessed for individual radiologists who had read at least 100 reports from the test set, providing further insights into its reliability and consistency across different users.\n\nIn summary, the reported metrics are representative of standard evaluations in the field and provide a thorough assessment of ONYX's performance in identifying pneumonia cases from electronic chest radiograph reports.",
  "evaluation/comparison": "Not applicable. The publication does not provide information about comparisons to publicly available methods or simpler baselines on benchmark datasets. The focus is on the development and evaluation of the ONYX NLP tool for identifying pneumonia cases from chest radiograph reports, with comparisons made between two different classifiers tailored for different study needs. The evaluation involves assessing sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) for these classifiers, as well as exploring their performance across different patient characteristics and care settings. The study does not mention the use of benchmark datasets or comparisons to other publicly available methods or simpler baselines.",
  "evaluation/confidence": "The evaluation of ONYX's performance included several key metrics, each accompanied by confidence intervals to indicate the reliability of the estimates. For Classifier 1, which was tailored to maximize accuracy, the sensitivity was reported as 92% with a 95% confidence interval (CI) of 90% to 93%. The specificity was 87% with a 95% CI of 86% to 88%. The positive predictive value (PPV) was 74% with a 95% CI of 72% to 76%, and the negative predictive value (NPV) was 96% with a 95% CI of 96% to 97%.\n\nFor Classifier 2, which aimed to minimize manual review, the sensitivity was 75% with a 95% CI of 72% to 77%. The specificity was 95% with a 95% CI of 94% to 96%. The PPV was 86% with a 95% CI of 83% to 88%, and the NPV was 91% with a 95% CI of 90% to 91%.\n\nThese confidence intervals provide a range within which the true values of the metrics are likely to fall, giving a sense of the precision of the estimates. The use of generalized estimating equations also ensured that the standard errors accounted for potential correlations between multiple reports from the same patient, enhancing the robustness of the statistical analyses.\n\nThe results showed that Classifier 2 had markedly higher specificity and PPV than Classifier 1. This difference was statistically significant and was partly expected due to the new decision rule that classified certain ambiguous reports as not showing pneumonia. The post hoc analysis further explored the factors contributing to Classifier 2's higher specificity, confirming its superior performance in reducing the need for manual review while maintaining a low false positive rate.\n\nOverall, the evaluation demonstrated that ONYX, with both classifiers, performed well in identifying pneumonia cases from electronic chest radiograph reports. The inclusion of confidence intervals and the use of robust statistical methods provide a high level of confidence in the reported performance metrics.",
  "evaluation/availability": "Not enough information is available."
}