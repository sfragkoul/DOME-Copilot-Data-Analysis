{
  "publication/title": "Adding Continuous Vital Sign Information to Static Clinical Data Improves the Prediction of Length of Stay After Intubation: A Data-Driven Machine Learning Approach.",
  "publication/authors": "Casti\u00f1eira D, Schlosser KR, Geva A, Rahmani AR, Fiore G, Walsh BK, Smallwood CD, Arnold JH, Santillana M",
  "publication/journal": "Respiratory care",
  "publication/year": "2020",
  "publication/pmid": "32879034",
  "publication/pmcid": "PMC7906608",
  "publication/doi": "10.4187/respcare.07561",
  "publication/tags": "- Machine Learning\n- Predictive Modeling\n- Vital Signs\n- Mechanical Ventilation\n- Length of Stay\n- Gradient Boosting Trees\n- Feature Selection\n- Unsupervised Learning\n- Affinity Propagation\n- Healthcare Data\n- Electronic Health Records\n- Time-Series Analysis\n- Classification\n- Overfitting\n- Robustness",
  "dataset/provenance": "The dataset used in this study was collected from subjects at Boston Children\u2019s Hospital. The vital signs data were recorded using routine bedside monitors (Philips IntelliVue MP90) and captured at a 5-second frequency using the T3 system. The vital signs considered were heart rate, breathing frequency, and both pulse and SpO2. These signals were chosen for their consistent availability across most subjects and to ensure robustness in the analysis.\n\nThe study included 284 subjects, and the data points for each subject consisted of 69,120 data points within the first 24-hour window after the onset of mechanical ventilation. This time window was selected to represent a small interval of the 24-hour period, ensuring that it would not significantly alter the trends in the vital signs time series. Additionally, this approach allowed for the handling of incomplete data, such as gaps due to routine patient care, through appropriate data imputation methods.\n\nStatic clinical data, including sex, age, pre-ICU admission location, and components of the Pediatric Index of Mortality-2 or Pediatric Index of Mortality-3 score, were also collected from the electronic health records. These data points were used in the second phase of the study to complement the vital signs data.\n\nThe dataset generated and analyzed during this study is not publicly available to protect the privacy of individually identifiable health information. However, the de-identified features used for the predictive algorithms are available from the corresponding authors upon reasonable request. This approach ensures that the data can be used for further research while maintaining the confidentiality of the subjects.",
  "dataset/splits": "In our study, we divided the subjects' data into three distinct splits for each of the three classification scenarios. These splits were training, validation, and evaluation sets. The distribution of data points in each split was as follows: 80% of the data was allocated to the training set, 10% to the validation set, and the remaining 10% to the evaluation set. This division was maintained consistently across all 300 independent experiments conducted for each scenario. The purpose of this split was to ensure a proper distribution of the evaluation of model accuracy and to mitigate overfitting due to specific choices of subject sets. The training set was used to construct the classification model, the validation set was used to optimize the hyperparameters of the classification algorithm, and the evaluation set was used exclusively to quantify the accuracy of the proposed model.",
  "dataset/redundancy": "The dataset used in this study consisted of 284 subjects, with data collected for four vital signs: heart rate, pulse, SpO2, and breathing frequency, recorded during the first 24 hours after the onset of mechanical ventilation. To ensure the validity and robustness of our approach, the subjects' data were divided into three distinct sets: training, validation, and test sets. The split was done according to the following rule: 80% of the data was allocated for training, 10% for validation, and the remaining 10% for evaluation.\n\nTo mitigate overfitting and ensure that the evaluation of model accuracy was properly distributed, a total of 300 independent experiments were conducted. Each experiment used random partitions of subjects for the training, validation, and evaluation sets. This method ensured that the training and test sets were independent, as the data was randomly partitioned in each experiment.\n\nThe same proportion of each class was maintained in the training sets across the different experiments to avoid potentially unbalanced distribution of classes. This approach allowed us to assess the quality of predictions on both classes in all 300 experiments. The training set was used to construct the classification model, the validation set was used to optimize the hyperparameters of the classification algorithm, and the evaluation set was used exclusively to quantify the accuracy of the proposed model.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the healthcare domain. The use of random partitions and the maintenance of class proportions in the training sets are standard practices aimed at ensuring the robustness and generalizability of the models. The high number of independent experiments (300) further strengthens the reliability of the results by providing a comprehensive evaluation of the model's performance across different data splits.",
  "dataset/availability": "The data sets generated and analyzed during the current study are not publicly available to protect the privacy of individually identifiable health information. However, the de-identified features used for the predictive algorithms are available from the corresponding authors on reasonable request. This approach ensures that sensitive patient information remains confidential while still allowing for potential replication or extension of the study by other researchers. The corresponding authors can be contacted via email for access to these de-identified features.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is gradient boosting trees. This approach is not new; it is a well-established technique in the field of machine learning. Gradient boosting trees combine multiple weak learners, typically decision trees, into a single strong learner in an iterative fashion. This method provides an extra degree of freedom in the classic bias-variance trade-off and offers robustness toward overfitting, making it suitable for the problem at hand.\n\nThe reason this algorithm was not published in a machine-learning journal is that our focus was on applying established machine-learning techniques to a specific medical problem\u2014predicting the length of stay (LOS) in the ICU. The innovation lies in the application and adaptation of these techniques to the medical domain, rather than in the development of a new algorithm. Our work emphasizes the practical implementation and evaluation of gradient boosting trees in a real-life healthcare scenario, ensuring that the methodology is scalable, robust, and interpretable. This approach is particularly relevant for clinical settings where computational efficiency and reliability are crucial.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, time series data for vital signs, including heart rate, pulse, SpO2, and breathing frequency, were collected for each patient over a 24-hour period after the onset of mechanical ventilation. These time series data were required to have no missing values for most automatic computational feature extraction techniques. To address any gaps in the data, a data imputation approach based on Gaussian processes was employed. This method systematically calculated imputed values without assuming a specific functional form of the signal, allowing for data-driven and easily implementable imputations. Imputation was performed for time windows of 20 minutes or less, ensuring that the vital signs data were complete and ready for feature extraction.\n\nFeature extraction was then conducted to convert the raw time series data into vectors of statistical features. This process involved summarizing the time series data, capturing correlation structures, distributions, entropy, stationarity, and scaling properties. By projecting the vital signs time series into a feature space, the underlying statistical and temporal behaviors of the vital signs across multiple subjects were captured. This step helped mitigate the impact of errors and outliers in the measurements and facilitated the identification of common properties that might not be apparent from the raw time series data.\n\nThe feature extraction process resulted in a large number of features, which were then subjected to dimensionality reduction techniques. Principal component analysis and the removal of highly linearly correlated features were used to reduce the dimensionality and remove redundant information. Additionally, an unsupervised learning approach based on clustering strategies, specifically the affinity propagation method with the maximal information coefficient as a similarity matrix, was implemented. This approach identified a reduced number of representative features that could characterize most of the input parameter space, making the data more manageable for the subsequent machine-learning steps.\n\nThe preprocessed and encoded data were then used in the model formulation step, where various supervised learning approaches, including logistic regressions, random forests, support vector machines, and gradient boosted trees, were implemented. Ensemble methods such as voting and stacking classifiers were also employed to combine individual classifiers and improve the accuracy of the models. The final models were evaluated on unseen data to assess their predictive performance, ensuring robustness and interpretability in the medical application context.",
  "optimization/parameters": "In our study, we initially considered a large number of features, specifically 13,332, extracted from various time-series data. However, to make our model more practical and efficient, we employed an unsupervised learning approach using affinity propagation for feature selection. This method reduced the number of features to 186, which were then used for prediction. These selected features included statistical representations from heart rate, pulse, SpO2, and breathing frequency signals.\n\nThe selection of these 186 features was driven by the need to balance computational efficiency and predictive power. By clustering similar features together, we ensured that the most relevant and informative features were retained, while redundant or less informative ones were discarded. This approach not only made the model more interpretable but also significantly reduced the time required for feature extraction and processing, making it suitable for real-time applications.\n\nThe final model, therefore, operates with 186 input parameters, which were chosen based on their ability to characterize the input parameter space effectively. This dimensionality reduction step was crucial in developing a scalable and robust model that could be deployed in real-life scenarios.",
  "optimization/features": "In the optimization process, feature selection was indeed performed to identify a subset of useful variables that characterize each individual patient. Initially, a large number of features were extracted from raw data, totaling 13,332 data points per subject. However, to make the model more practical for real-time medical settings, an unsupervised learning approach based on clustering strategies was implemented. This approach used the affinity propagation method with the maximal information coefficient as a similarity matrix. As a result, the number of features was reduced from 13,332 to 186. This reduction allowed for more agile dynamic training and validation of the supervised training step. The feature selection process was conducted using the training set only, ensuring that the evaluation was performed in a strictly out-of-sample fashion. The selected features were then used for the supervised learning study, focusing on maintaining predictive performance while reducing computational effort.",
  "optimization/fitting": "In our study, we addressed the challenge of having a large number of predictors relative to the number of observations, which is a common issue that can lead to overfitting. Specifically, we had 186 predictors and 284 observations/subjects. To mitigate this risk, we employed several strategies.\n\nFirstly, we used a shrinking factor within the gradient boosting tree algorithm. This factor controls the learning rate, allowing for more parsimonious models by reducing the influence of each individual tree. This approach helps in avoiding overfitting by ensuring that the model does not become too complex and thus more likely to fit the noise in the training data.\n\nSecondly, we conducted a robust evaluation design involving 300 independent experiments with random partitions of subjects into training, validation, and test sets. This method ensured that our model's performance was not dependent on specific choices of subject sets, thereby reducing the risk of overfitting.\n\nAdditionally, we maintained the same proportion of each class in the training set across different experiments to avoid unbalanced distribution of classes. This balance is crucial for assessing the quality of predictions on both classes.\n\nTo further validate our model's performance, we evaluated it on both validation and test sets, which showed accurate mean values. This indicated that any slight potential overfitting on the training set did not affect the quality of predictions on unseen patient data.\n\nMoreover, we ensured that the model's performance was consistent across different sets, as evidenced by similar distributions for model accuracy in both validation and test sets. This consistency is a sign of the robustness of our predictive models.\n\nIn summary, through the use of a shrinking factor, a rigorous evaluation design, and balanced class distributions, we effectively managed the risk of overfitting and ensured the reliability and generalizability of our model.",
  "optimization/regularization": "To prevent overfitting, several techniques were employed. A shrinking factor was used within the gradient boosting tree algorithm to control the learning rate and promote more parsimonious models. This factor reduces the influence of each individual tree, allowing subsequent trees to improve the model. Additionally, the data was divided into training, validation, and test sets with 80%, 10%, and 10% splits respectively, and 300 independent experiments were conducted using random partitions to ensure a proper distribution of model accuracy evaluation. This approach helped mitigate overfitting due to specific choices of subject sets. Furthermore, the same proportion of each class was maintained in the training sets across different experiments to avoid unbalanced class distributions and to assess the quality of predictions on both classes. The validation set was used to optimize hyperparameters, while the evaluation set was used exclusively to quantify the accuracy of the proposed model. These measures collectively ensured the robustness and validity of the approach.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed in the supplementary materials. These materials are accessible at the provided URL. The supplementary information includes the specific settings and schedules used for the gradient boosting tree algorithm, which were crucial for achieving the reported performance.\n\nThe supplementary materials also contain the details of the feature selection process, including the affinity propagation approach used to reduce the number of features. This information is essential for replicating the feature selection step of our workflow.\n\nRegarding the model files, while the specific trained models are not publicly available due to privacy concerns related to the individually identifiable health information, the de-identified features used for the predictive algorithms can be obtained from the corresponding authors upon reasonable request. This allows other researchers to replicate the model training and evaluation processes using the same feature set.\n\nThe supplementary materials are available under the terms specified by the journal, ensuring that researchers can access and use the information for further studies or validations. The URL provided in the publication directs readers to the supplementary materials, where they can find comprehensive details about the configurations and parameters used in our study.",
  "model/interpretability": "The model employed in our study is not entirely a black box, as we have taken steps to ensure interpretability. We utilized an unsupervised learning approach based on clustering strategies to identify a reduced number of features that characterize most of the input parameter space. This method, known as affinity propagation, outputs representative features that can be interpreted, unlike projected features obtained from principal component analysis.\n\nFor instance, of the 186 selected features, 48% were extracted from the heart rate signal (27%) or the pulse signal (21%). This means that nearly half of the selected features represent statistical characteristics of the heart rate. Additionally, features extracted from SpO2 and breathing frequency accounted for 22% and 30%, respectively. This breakdown provides a clear understanding of which vital signs are most influential in our model's predictions.\n\nBy focusing on a smaller, interpretable set of features, we aim to provide a general understanding of the families of features that drive our solutions. This approach not only enhances the model's interpretability but also ensures that the results are more transparent and easier to validate in a medical context.",
  "model/output": "The model discussed in this publication is a classification model. It is designed to predict the length of stay (LOS) for patients on mechanical ventilation, specifically distinguishing between prolonged LOS and shorter LOS. The model uses various supervised learning approaches, with a particular focus on gradient boosting trees. This approach is chosen for its robustness and ability to handle overfitting, making it suitable for the classification task at hand. The model's performance is evaluated using metrics such as accuracy and area under the curve (AUC) from receiver operating characteristic (ROC) curves, which are typical for classification problems. The evaluation involves multiple scenarios, including using vital signs time-series features only, static clinical data only, and a combination of both. The results indicate that the model can accurately predict prolonged LOS, demonstrating its effectiveness as a classification model.",
  "model/duration": "The execution time of the model varied depending on the approach used. In the baseline approach, there were no constraints on computational time, allowing for extensive experimentation with various feature selection and modeling techniques. This approach involved processing a large number of features, with the total number of data points from 24 hours of raw data for four different time series collected at 5-second intervals amounting to 69,120 data points. All extracted features from the raw data led to a total of 13,332 data points per subject, which could be time-consuming for real-time decision-making.\n\nIn contrast, the scalable robust approach focused on reducing the number of features to make the model more practical for real-time applications. This approach used affinity propagation, an unsupervised learning method, to reduce the number of features from 13,332 to 186. This dimensionality reduction allowed for more agile dynamic training and validation of the supervised training step, making the model more computationally efficient. The similarity matrix generated in this process showed larger similarities within clusters than across clusters, indicating confidence in the clustering results.\n\nThe gradient boosting tree predictive models were assessed on evaluation sets, and the results were presented as a distribution of accuracies observed in 300 independent experiments. The median model total accuracy was 64% for static clinical data only, 72% for time series data only, and 80% for the combined approach. The area under the curve values for the median model performance were 73% for static clinical data, 83% for vital signs, and 90% for the combined approach. These findings demonstrated that the predictive power of vital sign information alone could outperform the approach based on electronic health record static clinical data, and that combining both data types provided the best results.\n\nThe model's performance was also evaluated for overfitting issues. The observed accuracies on the training set demonstrated the model's ability to fit the data, but the prediction ability on both validation and evaluation sets showed accurate mean values, indicating that any slight overfitting on the training set did not affect the quality of predictions in unseen patient data. The confusion matrix for the combined scenario showed that the models were better at identifying one of the classes, with an average accuracy of 84.0% for predicting a length of stay greater than 4 days and 68.4% for predicting a length of stay less than 4 days. This suggests that while there is room for improvement, the model performs well in predicting prolonged length of stay in subjects on mechanical ventilation.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "To evaluate the performance of our gradient boosting tree models, we employed a rigorous approach involving multiple classification scenarios and extensive experimentation. We divided the subjects' data into three subsets: 80% for training, 10% for validation, and 10% for evaluation. This partitioning was done randomly across 300 independent experiments to ensure a proper distribution of model accuracy and to mitigate overfitting.\n\nFor each experiment, the training set was used to construct the classification model, the validation set was used to optimize the hyperparameters, and the evaluation set was used exclusively to quantify the model's accuracy. This strict out-of-sample evaluation ensured that our models were tested on unseen data, providing a realistic assessment of their predictive performance.\n\nTo further reduce the risk of overfitting, we implemented a shrinking factor within the gradient boosting tree algorithm. This factor controlled the learning rate, favoring more parsimonious models and providing a trade-off between the number of trees and the learning rate. Additionally, we maintained the same proportion of each class in the training sets to avoid unbalanced distributions and to assess the quality of predictions across all classes.\n\nThe evaluation focused on three scenarios: using vital signs time-series features only, using static clinical data only, and using a combination of both. The results showed that the model combining both data types achieved the highest accuracy, indicating that vital signs and static clinical data contain complementary information that enhances model performance. The median model accuracies for the three scenarios were 64%, 72%, and 80%, respectively, demonstrating the superior predictive power of the combined approach.\n\nMoreover, we assessed the models' performance using receiver operating characteristic (ROC) curves and area under the curve (AUC) values. The median AUC values were 73% for static clinical data, 83% for vital signs, and 90% for the combined approach, further confirming the effectiveness of integrating both data types.\n\nTo ensure the robustness of our models, we also examined their performance on validation and test sets, which showed similar distributions of model accuracy. This consistency suggested that our models were not overfitting to the training data and could generalize well to unseen patient data. Overall, our evaluation method provided a comprehensive and reliable assessment of the models' predictive performance, highlighting the potential of combining vital signs and static clinical data for accurate predictions.",
  "evaluation/measure": "In our evaluation, we focused on several key performance metrics to assess the effectiveness of our predictive models. The primary metric reported is the total accuracy, which is the ratio of correct predictions to the total number of observations. This metric provides a straightforward measure of how often the model's predictions match the actual outcomes.\n\nAdditionally, we presented the receiver operating characteristic (ROC) curves and their corresponding area under the curve (AUC) values. The AUC is a critical metric that summarizes the model's ability to distinguish between positive and negative classes across all possible classification thresholds. We reported the 10th, 50th, and 90th percentile ROC curves to provide a comprehensive view of the model's performance, highlighting the worst, median, and best-case scenarios, respectively.\n\nThe median AUC values for the three scenarios\u2014using static clinical data only, vital signs time-series data only, and a combination of both\u2014were 73%, 83%, and 90%, respectively. These metrics indicate that the model combining both data types outperforms models using either data type alone, suggesting that the complementary information from both sources enhances predictive power.\n\nTo ensure the robustness of our findings, we conducted 300 independent experiments with random partitions of subjects into training, validation, and evaluation sets. This approach helps mitigate overfitting and provides a reliable distribution of model performance. The use of a shrinking factor within the gradient boosting tree algorithm further controls the learning rate, promoting more parsimonious models and reducing the risk of overfitting.\n\nOverall, the reported metrics are representative of standard practices in the literature, providing a clear and comprehensive evaluation of the model's predictive performance. The combination of accuracy, ROC curves, and AUC values offers a well-rounded assessment, ensuring that the model's strengths and potential limitations are thoroughly understood.",
  "evaluation/comparison": "Two different strategies were employed to address the classification problem. The first strategy, referred to as the baseline approach, involved experimenting with multiple modeling approaches without constraints on computational power. This approach aimed to identify the classification performance of various methodologies in a research environment, with no limitations on computational time. Multiple supervised learning approaches were implemented, including logistic regressions, random forests, support vector machines, and gradient boosted trees. Ensemble methods, such as voting and stacking classifiers, were also used to combine individual classifiers and improve model accuracy.\n\nThe second strategy focused on developing a workflow suitable for real-life scenarios, achieving comparable performance to the best methodologies from the baseline approach but with significantly less computational effort and greater parsimony. This scalable robust approach used an unsupervised learning method for feature selection, reducing the number of features from 13,332 to 186. This reduction allowed for more efficient training and validation of the supervised learning step. The gradient boosting tree was chosen for its robustness and ability to handle overfitting, providing a good balance between bias and variance.\n\nThe evaluation of the gradient boosting tree involved three scenarios: using vital sign information only, using static clinical data only, and using a combination of both. For each scenario, subjects' data were divided into training, validation, and test sets, with 300 independent experiments conducted to ensure robustness. The results showed that the model combining vital signs and static clinical data provided the best performance, with a median accuracy of 80%. This indicates that both data types contain complementary information that enhances model performance. The approach was designed to be scalable and robust, addressing the need for real-time decision-making in medical settings.",
  "evaluation/confidence": "The evaluation of our approach involved conducting 300 independent experiments using random partitions of subjects for training, validation, and evaluation sets. This extensive testing was designed to ensure a proper distribution of model accuracy evaluation and to mitigate overfitting due to specific choices of subject sets.\n\nFor each of the three classification scenarios\u2014using vital signs time-series features only, using static clinical data only, and using a combination of both\u2014we reported the median model total accuracy. These accuracies were presented as distributions observed in the 300 independent experiments, rather than as single accuracy measurements from a single model. This approach provides a more robust assessment of model performance.\n\nThe distributions of receiver operating characteristic (ROC) curves for the 300 experiments were also presented for the three data type choices. For clarity, the 10th, 50th, and 90th percentile ROC curves were provided, representing the worst, median, and best model performances, respectively. The corresponding area under the curve (AUC) values were labeled as P10, P50, and P90. The median AUC values were 73% for static clinical data, 83% for vital signs, and 90% for the combined approach. These values indicate the reliability and consistency of our model's performance across different experiments.\n\nTo assess the model's robustness and overfitting issues, we compared the performance on the training set with that on the validation and evaluation sets. The observed accuracies on the training set demonstrated the model's ability to fit the data, but the prediction ability on both validation and evaluation sets showed accurate mean values (around 81% and 79%, respectively). This suggests that any slight overfitting on the training set did not affect the quality of predictions on unseen patient data. Additionally, the similar distributions for model accuracy on both validation and test sets indicate the robustness of our predictive models.\n\nThe confusion matrix for the scenario combining both time-series and static clinical data was also computed for the 300 experiments. This matrix showed that our models were better at identifying one of the classes, with an average accuracy of 84.0% for predicting a length of stay (LOS) of 4 days or more, and 68.4% for predicting an LOS of less than 4 days. This indicates that while there is room for improvement, our methodology provides a reliable basis for predicting prolonged LOS in subjects on mechanical ventilation.",
  "evaluation/availability": "The raw evaluation files are not publicly available. This is to protect the privacy of individually identifiable health information. However, the de-identified features used for the predictive algorithms are available from the corresponding authors upon reasonable request. This approach ensures that sensitive patient data remains confidential while still allowing for potential replication or further analysis of the study's methods."
}