{
  "publication/title": "Development and validation of an artificial intelligence model for the early classification of the aetiology of meningitis and encephalitis: a retrospective observational study.",
  "publication/authors": "Choi BK, Choi YJ, Sung M, Ha W, Chu MK, Kim WJ, Heo K, Kim KM, Park YR",
  "publication/journal": "EClinicalMedicine",
  "publication/year": "2023",
  "publication/pmid": "37415843",
  "publication/pmcid": "PMC10319989",
  "publication/doi": "10.1016/j.eclinm.2023.102051",
  "publication/tags": "- Meningitis\n- Autoimmune encephalitis\n- Tuberculosis\n- Neuroinflammation\n- Artificial intelligence\n- Early diagnosis\n- Machine learning\n- Medical data analysis\n- Patient classification\n- Clinical decision support",
  "dataset/provenance": "The dataset used in this study was sourced from patient data from two different institutions. The primary dataset was collected from patients at Sinchon Severance Hospital, while an external validation dataset was developed using patient data from Gangnam Severance Hospital. This external dataset performed well despite having somewhat different cause distributions or feature types, demonstrating the model's reproducibility and lack of overfitting.\n\nThe dataset includes patients with confirmed causes of aetiology, specifically autoimmune, bacterial, tuberculosis, and viral infections. The aetiology was determined based on laboratory test results of cerebrospinal fluid conducted during hospitalization. Additionally, the model was applied to patients whose pathogen was not clearly identified through laboratory tests to predict their suspected cause. The suspected cause was determined by the consensus of three neurologists based on electronic medical records, including suspected diagnosis and treatment information documented by the attending physician at the time of hospitalization, as well as various examination results.\n\nThe test set consisted of 100 samples, with 56 patients from the internal dataset (Sinchon Severance Hospital) and 44 from the external dataset (Gangnam Severance Hospital). The dataset includes a wide range of variables collected within the first 24 hours of hospitalization, such as baseline characteristics, medical history, vital signs, diagnostic modalities, and laboratory findings. Data preprocessing involved normalizing numerical variables, handling missing values, and selecting features based on missing value percentages and correlation coefficients.\n\nThe dataset has not been used in previous papers by the community, but it builds upon established methods and variables commonly used in medical research. The use of electronic medical records and consensus among neurologists ensures the reliability and validity of the dataset. The external validation dataset further strengthens the generalizability of the findings.",
  "dataset/splits": "The dataset was split into multiple parts for training, validation, and testing purposes. Initially, data from patients with confirmed aetiology (283 patients) were used for training the AI models. These patients were hospitalized with meningitis and encephalitis. The most common causes identified were viral infections (187 patients), followed by bacterial (39 patients), tuberculosis (32 patients), and autoimmune (25 patients) causes. Patients with fungal or other infections were excluded due to small sample sizes.\n\nFor the external validation, a separate dataset was developed using patient data from a different institution. This dataset performed well despite having somewhat different cause distributions or feature types. The external validation dataset consisted of 44 patients from Gangnam Severance Hospital.\n\nAdditionally, a test set was constructed with a sample size of 100, consisting of 56 patients from the internal dataset (Sinchon Severance Hospital) and 44 from the external dataset (Gangnam Severance Hospital). This test set was used to compare the performance of the AI model with the clinical judgment of clinicians.\n\nThe performance of the AI model was also evaluated using stratified K-fold cross-validation (K = 2, 4, 5, 10). As the value of K increased, the training size increased, and overall performance tended to improve. Therefore, the final performance comparison of the models was conducted through 10-fold cross-validation.",
  "dataset/redundancy": "The dataset was split into training and validation sets, with an additional separate dataset developed for external validation. This external validation dataset was created using patient data from a different institution, ensuring independence from the training and validation sets. This approach helped to demonstrate the model's reproducibility and generalizability, as it performed well despite having somewhat different cause distributions or feature types compared to the training data.\n\nTo enforce independence, the external validation dataset was sourced from a distinct institution, which inherently had different patient demographics and clinical practices. This method helped to mitigate overfitting and ensured that the model's performance was not solely dependent on the specific characteristics of the training data.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the medical field. The use of an external validation set is a robust approach that is increasingly recognized as essential for validating the performance of machine learning models in healthcare. This method provides a more reliable assessment of the model's ability to generalize to new, unseen data, which is crucial for real-world clinical applications.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study primarily revolves around the use of established machine-learning models, rather than introducing a new algorithm. The models utilized include Extreme Gradient Boosting (XGBoost), Random Forest, Light Gradient Boosting Machine, Category Boosting, K-Nearest Neighbour, Gaussian Naive Bayes, and TabNet. These models are well-known in the machine-learning community and have been extensively studied and applied in various domains.\n\nXGBoost, in particular, is a scalable tree boosting system that has been widely adopted for its efficiency and performance in handling structured/tabular data. It is not a new algorithm; it was introduced by Chen and Guestrin in 2016 and has since become a staple in many machine-learning toolkits. The decision to use XGBoost, along with other established models, was driven by their proven track record in achieving high performance in classification tasks.\n\nThe choice of these models was further validated through a comparative analysis using the PyCaret library, which facilitated the evaluation of various classification models. The models that demonstrated higher performance were selected for further optimization. Regularization techniques, such as L2 regularization for XGBoost, were applied to reduce model complexity. Bayesian optimization was used to fine-tune the model parameters, ensuring that the models were optimized for the specific dataset and task at hand.\n\nThe ensemble model, which combined XGBoost and TabNet, was found to achieve the best performance. The composition of this ensemble model was determined through extensive experimentation, with the optimal ratio identified as 80% XGBoost and 20% TabNet. This approach leveraged the strengths of both models, leading to improved overall performance.\n\nIn summary, the optimization algorithm used in this study relies on well-established machine-learning models and techniques. The focus was on leveraging the strengths of these models and optimizing their parameters to achieve the best possible performance for the given task. The decision to use these models was based on their proven effectiveness and the results of comparative analyses.",
  "optimization/meta": "The model employed in this study incorporates a meta-predictor approach, utilizing data from multiple machine-learning algorithms as input. Specifically, the ensemble model that demonstrated the highest performance was composed of 80% Extreme Gradient Boosting (XGBoost) and 20% TabNet. This combination was identified through a process of varying the ensemble model composition ratio to achieve optimal performance.\n\nThe ensemble model integrates the strengths of different machine-learning methods, including XGBoost, Random Forest, Light Gradient Boosting Machine, and TabNet. These models were selected based on their higher performance as evaluated using functions provided by the PyCaret library in Python.\n\nTo ensure the robustness and generalizability of the model, training data independence was maintained. The model was evaluated using stratified K-fold cross-validation with different values of K (2, 4, 5, 10). This approach helped in assessing the model's performance across different training sizes and ensured that the training data was independent. The final performance comparison was conducted through 10-fold cross-validation, which provided a comprehensive evaluation of the model's effectiveness.\n\nAdditionally, the model's performance was further validated using an external dataset from a different institution. This external validation confirmed that the model was reproducible and not overfitted, as it performed well despite having somewhat different cause distributions or feature types. The use of an external dataset also ensured that the training data remained independent, reinforcing the reliability of the model's predictions.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the quality and reliability of the machine-learning models. We began by collecting a comprehensive set of variables related to baseline characteristics at admission, including age, sex, height, weight, and mental status. Additionally, we gathered medical history data such as seizures, tuberculosis, and the Charlson comorbidity index based on ICD-10 codes. Vital signs like blood pressure, heart rate, respiratory rate, and body temperature within the first 24 hours of hospitalization were also recorded. Diagnostic modalities such as brain computed tomography (CT), chest X-ray (CXR), and EEG results were included, along with laboratory findings from cerebrospinal fluid (CSF), blood, and urine samples.\n\nTo handle missing values, we employed different strategies depending on the type of variable. For categorical variables, missing values were replaced with -1. For numerical features, various imputation techniques were used during the AI process. Variables with a missing value percentage of more than 50% were excluded to maintain data integrity. To ensure inter-variable independence, variables with Pearson correlation coefficients higher than 0.8 were integrated into one. Additionally, variables with a variance inflation factor greater than 10 were eliminated to mitigate multicollinearity issues.\n\nAll numerical variables were normalized using min-max scaling to bring them to a comparable range. During the collection of vital signs, medically impossible values were excluded based on clinical judgment, as they were likely due to typographical errors. The initial value was selected when there were several identical test results within the initial 24 hours.\n\nTo address class imbalance, we utilized class weights in all models and applied various sampling techniques. These included four under-sampling techniques: random sampling, Tomek links, condensed nearest neighbor, and one-sided selection; two over-sampling techniques: synthetic minority over-sampling technique (SMOTE) and adaptive synthetic sampling (ADASYN); and a combination technique called SMOTE-Tomek links. These methods helped to balance the dataset and improve the performance of the models.\n\nIn summary, our data encoding and preprocessing steps involved careful selection and imputation of variables, normalization, and the use of sampling techniques to handle class imbalance. These steps were essential in preparing the data for effective machine-learning model training and evaluation.",
  "optimization/parameters": "In our study, we initially considered 110 variables for model development. However, through a rigorous feature selection process, we excluded variables with more than 50% missing values and those with a variance inflation factor greater than 10 to mitigate multicollinearity. Additionally, variables with Pearson correlation coefficients higher than 0.8 were integrated to ensure inter-variable independence. This process resulted in the selection of 82 variables for model training.\n\nThe selection of these 82 variables was crucial for optimizing model performance. By reducing the number of variables, we aimed to enhance the model's interpretability and generalization capabilities. The final set of variables was chosen based on their relevance and contribution to the prediction task, ensuring that the model could effectively distinguish between different aetiologies.\n\nThe optimization of model parameters was conducted using Bayesian optimization, which helped in fine-tuning the hyperparameters to reduce model complexity and improve performance. This approach ensured that the selected parameters were optimal for the given dataset and prediction task.",
  "optimization/features": "In our study, we initially considered 110 variables as potential input features. To refine this set, we performed a rigorous feature selection process. Variables with a missing value percentage exceeding 50% were excluded to ensure data integrity. To maintain inter-variable independence, we integrated variables that had Pearson correlation coefficients higher than 0.8. Additionally, we eliminated variables with a variance inflation factor greater than 10 to mitigate the issue of multicollinearity. This meticulous process resulted in the selection of 82 features out of the initial 110.\n\nThe feature selection was conducted using the training set only, ensuring that the model's performance on unseen data was not compromised. This approach helped in identifying the most relevant and independent features, thereby enhancing the model's predictive accuracy and generalizability.",
  "optimization/fitting": "In our study, we employed several strategies to address potential overfitting and underfitting issues. Initially, we started with 110 variables, but through a rigorous feature selection process, we reduced this number to 82. This process involved excluding variables with more than 50% missing values, integrating variables with high Pearson correlation coefficients (greater than 0.8) to ensure inter-variable independence, and eliminating variables with a variance inflation factor greater than 10 to mitigate multicollinearity.\n\nTo further prevent overfitting, we utilized regularization techniques, specifically L2 regularization for the XGBoost classification model. This helped in reducing model complexity. Additionally, we employed Bayesian optimization to fine-tune the model parameters, ensuring that the models were not overly complex.\n\nWe also implemented stratified K-fold cross-validation (with K values of 2, 4, 5, and 10) to evaluate model performance. This method helped in assessing how well the models generalize to unseen data. We observed that as the value of K increased, the training size increased, and overall performance tended to improve. Therefore, the final performance comparison of the models was conducted through 10-fold cross-validation.\n\nTo address potential underfitting, we used an ensemble approach. We found that the best performance was achieved by a model composed of 80% XGBoost and 20% TabNet. This combination leveraged the strengths of both models, enhancing the overall predictive power.\n\nMoreover, we handled class imbalance using class weights in all models and applied various under-sampling and over-sampling techniques, including random sampling, Tomek links, condensed nearest neighbor, one-sided selection, synthetic minority over-sampling technique (SMOTE), adaptive synthetic sampling (ADASYN), and SMOTE-Tomek links. These techniques ensured that the models were not biased towards the majority class.\n\nFor the TabNet model, we duplicated the original data 20 times to address the small sample size issue, ensuring that the model had enough data to learn effectively. We also monitored the learning curve plot to stop training at the epoch where the validation loss started to increase while the training loss continued to decrease, preventing overfitting.\n\nIn summary, our approach involved careful feature selection, regularization, parameter optimization, ensemble modeling, and handling class imbalance, all of which helped in mitigating both overfitting and underfitting issues.",
  "optimization/regularization": "In our study, we implemented several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was regularization, specifically L2 regularization for the XGBoost classification model. This technique helps to penalize large coefficients, thereby reducing the model's complexity and preventing it from fitting the noise in the training data.\n\nAdditionally, we employed Bayesian optimization to fine-tune the model parameters. This approach systematically explores the parameter space to find the optimal settings that minimize overfitting while maximizing model performance.\n\nWe also utilized stratified K-fold cross-validation with varying values of K (2, 4, 5, 10). This method ensures that each fold of the validation set is representative of the overall data distribution, providing a more reliable estimate of model performance and helping to detect overfitting.\n\nFurthermore, we handled class imbalance by using class weights in all models. This technique adjusts the model's learning process to pay more attention to the minority classes, thereby improving the model's ability to generalize to imbalanced datasets.\n\nTo address the issue of small sample size, we duplicated the original data 20 times when applying the TabNet model. This augmentation technique helps to increase the effective sample size, making the model less prone to overfitting.\n\nLastly, we monitored the learning curve plots to stop training at the epoch where the validation loss started to increase while the training loss continued to decrease. This early stopping criterion prevents the model from overfitting to the training data by halting the training process at the optimal point.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in Appendix 2. These details include the optimized parameters for each model, which were selected to reduce model complexity through Bayesian optimization. The optimization schedule and model files are not explicitly detailed in the main text, but the performance metrics and validation results are thoroughly documented in the supplementary materials.\n\nThe supplementary materials, including tables and figures, are available for reference. These materials provide insights into the performance of various models, the feature selection process, and the statistical analyses conducted. However, specific model files and detailed optimization schedules are not provided in the main text or supplementary materials.\n\nThe supplementary materials are accessible along with the main publication, but the licensing details for these materials are not specified. For access to the supplementary materials, readers are encouraged to refer to the main publication and its associated resources.",
  "model/interpretability": "The models employed in this study are not entirely transparent and can be considered as black-box models. To address this and enhance interpretability, several model-agnostic methods were utilized. These methods include permutation feature importance (PIMP), local interpretable model-agnostic explanations (LIME), and Shapley additive explanations (SHAP). These techniques help in verifying the explainability of the AI models by providing insights into how different features contribute to the model's predictions.\n\nFor instance, PIMP determines feature importance by randomly shuffling a particular feature and observing the resulting performance loss. This method highlighted that variables such as the previous history of tuberculosis, cerebrospinal fluid (CSF) cell differential ratio, and CSF glucose levels were among the most important features.\n\nLIME, on the other hand, computes model predictions by generating new datasets with features removed one at a time and then calculates the prediction differences. This method revealed that abnormal findings in electroencephalography (EEG) were strongly associated with autoimmune causes and had a significant positive impact.\n\nSHAP, which uses Shapley values from game theory, calculates the contribution of each feature to the prediction result. This method showed that CSF protein, CSF polymorphonuclear cell ratio, previous tuberculosis history, and CSF mononuclear cell ratio had the most considerable prediction power. For example, a decrease in CSF protein indicated a higher likelihood of an autoimmune cause, while abnormal EEG findings showed the opposite trend.\n\nAdditionally, the F score, a method built into XGBoost, was used to measure feature importance. This method is based on the number of times a variable is used to split the data, the number of data points separated by that variable, and the average training loss reduction achieved by using the feature. This provided further insights into the key features driving the model's decisions.\n\nIn summary, while the models themselves are complex and not fully transparent, the use of interpretability methods like PIMP, LIME, and SHAP, along with the F score, allows for a deeper understanding of the features that influence the model's predictions. This enhances the trustworthiness and reliability of the models in practical applications.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the aetiology of encephalitis and meningitis, categorizing patients into four distinct classes: autoimmune, bacterial, tuberculosis, and viral. The model utilizes data collected within the first 24 hours of hospitalization to make these predictions. Various classification algorithms were compared, including Extreme Gradient Boosting (XGBoost), Random Forest, Light Gradient Boosting Machine, Category Boosting, K-Nearest Neighbour, Gaussian Naive Bayes, and TabNet. An ensemble model combining XGBoost and TabNet was also employed to enhance performance. The model's effectiveness was evaluated using metrics such as AUROC, recall, precision, accuracy, and F1 score, with a particular emphasis on the F1 score due to the imbalanced nature of the data. The final performance comparison was conducted through 10-fold cross-validation, and the model's predictions were validated against both internal and external datasets. Additionally, the model's performance was compared with that of clinicians, demonstrating its potential to aid in early treatment selection.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the models involved several rigorous methods to ensure robustness and generalizability. We utilized stratified K-fold cross-validation with varying K values (2, 4, 5, 10) to assess model performance. This approach helped in understanding how the model performs with different training sizes, and we observed that performance generally improved as the value of K increased. Consequently, the final performance comparison was conducted using 10-fold cross-validation.\n\nIn addition to cross-validation, we employed an external validation dataset from a different institution. This dataset had somewhat different cause distributions and feature types, but the model performed well, indicating reproducibility and a lack of overfitting. The external validation set helped in verifying the model's generalizability to new, unseen data.\n\nWe assessed model performance using several classification metrics, including the area under the receiver operating characteristic curve (AUROC), recall, precision, accuracy, and F1 score. Given the imbalanced nature of our data, we placed a high emphasis on the F1 score. The ensemble model, which combined XGBoost and TabNet, achieved the highest F1 score on the external dataset, demonstrating its effectiveness in handling imbalanced data.\n\nWe also compared the AI model's performance with that of clinicians. Three doctors with varying levels of neurological experience were asked to predict aetiologies based on the same data used by the AI models. The AI models, particularly the XGBoost and ensemble models, outperformed the clinicians in all metrics, highlighting the potential of AI in aiding clinical decision-making.\n\nTo further verify the explainability of the AI models, we used model-agnostic methods such as permutation feature importance (PIMP), local interpretable model-agnostic explanations (LIME), and Shapley additive explanations (SHAP). Additionally, we utilized the F score method built into XGBoost to measure feature importance. These methods provided insights into which features were most influential in the model's predictions.\n\nStatistical analysis was performed to ensure the reliability of our findings. We conducted sample size estimation using the Nx-subsampling scheme and found that performance metrics converged with around 80-100 training samples fewer than our actual training sample size. For feature selection, we used the Pearson correlation coefficient and variance inflation factor to ensure inter-variable independence and to address multicollinearity. One-way analysis of variance (ANOVA) and the Pearson \u03c72 test were used to compare continuous and categorical variables, respectively.",
  "evaluation/measure": "In our evaluation, we assessed model performance using a comprehensive set of classification metrics. These included the area under the receiver operating characteristic curve (AUROC), recall, precision, accuracy, and the F1 score. These metrics are widely recognized and used in the literature for evaluating the performance of classification models, ensuring that our evaluation is both thorough and comparable to other studies in the field.\n\nThe AUROC provides a single scalar value that represents the ability of the model to distinguish between classes, making it a crucial metric for evaluating the overall performance of our models. Recall, also known as sensitivity, measures the proportion of actual positives that are correctly identified by the model. Precision, on the other hand, measures the proportion of predicted positives that are actually correct. Accuracy gives an overall measure of the correctness of the model's predictions, while the F1 score is the harmonic mean of precision and recall, providing a balance between the two.\n\nGiven the imbalanced nature of our dataset, we placed a particular emphasis on the F1 score. This metric is especially useful in scenarios where the classes are not equally represented, as it takes into account both the precision and recall, providing a more nuanced evaluation of the model's performance.\n\nIn addition to these metrics, we also used the DeLong test to compare the AUROC among high-performance models, ensuring that any observed differences in performance were statistically significant. This rigorous approach to performance evaluation allows us to confidently assess the strengths and weaknesses of our models, providing a clear picture of their effectiveness in predicting the aetiology of patients.",
  "evaluation/comparison": "In our evaluation, we compared the performance of various classification models using functions provided by the PyCaret library in Python. This comparison included several conventional machine learning models such as Extreme Gradient Boosting (XGBoost), Random Forest, and Light Gradient Boosting Machine. Additionally, we applied the Category Boosting model, which has a similar structure, and models known for their high interpretability, such as K-Nearest Neighbour and Gaussian Naive Bayes.\n\nTo enhance performance, we also applied deep learning models like TabNet. Due to the small sample size, we duplicated the original data 20 times to facilitate training. We experimented with different ensemble model compositions and found that the best performance was achieved with a model composed of 80% XGBoost and 20% TabNet.\n\nWe evaluated all models using stratified K-fold cross-validation with varying values of K (2, 4, 5, 10). Our findings indicated that as the value of K increased, the training size increased, leading to overall performance improvements. Therefore, the final performance comparison of the models was conducted through 10-fold cross-validation.\n\nRegularization techniques, such as L2 for XGBoost classification, were adapted to reduce model complexity. Bayesian optimization was used to choose the model parameters. To handle class imbalance, class weights were used in all models. Various under-sampling and over-sampling techniques, including random sampling, Tomek links, condensed nearest neighbour, one-sided selection, synthetic minority over-sampling technique (SMOTE), and adaptive synthetic sampling (ADASYN), were employed. Additionally, a combination technique called SMOTE-Tomek links was used.\n\nWe also compared our models to the clinical judgment of clinicians. Three doctors with diverse levels of neurological experience were selected to predict aetiologies based on clinical and laboratory data collected within 24 hours of admission. The test set consisted of 100 samples, with 56 patients from the internal dataset and 44 from the external dataset. The performance of both the XGBoost model and the ensemble model exceeded that of the clinicians in all metrics.",
  "evaluation/confidence": "The evaluation of our models included a comprehensive assessment of performance metrics, such as the area under the receiver operating characteristic curve (AUROC), recall, precision, accuracy, and F1 score. These metrics were crucial for evaluating the models' effectiveness, especially given the imbalanced nature of our dataset.\n\nTo ensure the robustness of our findings, we employed the DeLong test to compare the AUROC among high-performance models. This statistical test did not show any significant differences, indicating that the performance metrics were reliable and consistent across different models.\n\nWe also placed a high emphasis on the F1 score due to the imbalanced labels in our data. By replacing missing values of continuous variables with median values and performing oversampling using ADASYN in the ensemble model, we achieved the highest F1 score on the external dataset. This approach helped mitigate the impact of class imbalance and ensured that our model's performance was not skewed by the majority class.\n\nIn addition to these metrics, we conducted post-hoc analyses with Bonferroni correction to examine differences between each pair of groups. This statistical method helped us identify significant differences in clinical variables between the four aetiologies, further validating the model's performance.\n\nOverall, the performance metrics were accompanied by confidence intervals and statistical significance tests, providing a strong basis for claiming the superiority of our models over baselines and other methods. The use of rigorous statistical techniques ensured that our results were reliable and generalizable.",
  "evaluation/availability": "Not enough information is available."
}