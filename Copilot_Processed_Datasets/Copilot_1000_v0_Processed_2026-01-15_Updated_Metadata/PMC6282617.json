{
  "publication/title": "Pheno-Deep Counter: a unified and versatile deep learning architecture for leaf counting.",
  "publication/authors": "Giuffrida MV, Doerner P, Tsaftaris SA",
  "publication/journal": "The Plant journal : for cell and molecular biology",
  "publication/year": "2018",
  "publication/pmid": "30101442",
  "publication/pmcid": "PMC6282617",
  "publication/doi": "10.1111/tpj.14064",
  "publication/tags": "- Leaf counting\n- Deep learning\n- Plant phenotyping\n- Image analysis\n- Data agglomeration\n- Cross-validation\n- Mean squared error\n- Plant species adaptation\n- Neural networks\n- Regression models",
  "dataset/provenance": "The dataset used in our study primarily comes from the CVPPP 2017 dataset, which includes images of various plant species. Specifically, we utilized the A1 set, consisting of 128 images of Arabidopsis thaliana Col-0. This set was used for initial training and validation purposes. Additionally, we incorporated other sets from the CVPPP 2017 dataset, such as A2 (A. thaliana of five genotypes), A3 (tobacco), and A4 (A. thaliana Col-0), to increase data diversity and improve model accuracy.\n\nWe also employed datasets from other sources, including images of komatsuna plants and nocturnal images of Arabidopsis plants. The komatsuna dataset contains 300 RGB images of five different komatsuna plants, captured over a period of 10 days. For the nocturnal images, we selected and annotated a subset of night images, totaling 72 images from 18 plants.\n\nThe datasets were split into training, validation, and testing sets to optimize and evaluate the performance of our model. For example, in the komatsuna case, the dataset was split into a training set of 120 images, a validation set of 60 images, and a testing set of 120 images. This splitting strategy ensured that our model was trained and validated on diverse and representative data, leading to robust performance on unseen testing data.\n\nOur experiments demonstrated the benefit of data agglomeration across different sources, showing that increasing data diversity improves accuracy. This approach allowed our model to generalize better and achieve reliable leaf count predictions across various plant species and imaging conditions.",
  "dataset/splits": "In our experiments, we utilized multiple datasets with varying splits to evaluate the performance and adaptability of our network. For the CVPPP 2017 dataset, we initially pre-trained our network using Arabidopsis plant images from sets A1, A2, and A4. This training dataset was split into three parts: 50% for training, 25% for validation, and 25% for testing. We employed a fourfold cross-validation approach to ensure robust performance evaluation.\n\nFor the tobacco plant dataset, we fine-tuned the pre-trained network using a variable number of training images, specifically 7, 14, 21, and 27 images. The results indicated that increasing the number of training images improved prediction accuracy.\n\nIn the komatsuna dataset, which contained 300 RGB images of five different plants, we split the data as follows: two plants (IDs 00 and 01) for the training set (120 images), one plant (ID 04) for the validation set (60 images), and two plants (IDs 02 and 03) for the testing set (120 images). We progressively increased the size of the training set to 10, 20, 30, and then 40 images per plant, observing that more data led to more accurate predictions.\n\nFor the nocturnal images of Arabidopsis plants, we selected 18 plants and sampled one image per night every other day for 8 days, resulting in 72 images. We pre-trained the network using NIR images and fine-tuned it with 10 plants for training (40 images), 4 plants for validation (16 images), and 4 plants for testing (16 images). This process was repeated four times with different random splits of the training, validation, and testing sets.\n\nAdditionally, we conducted experiments to demonstrate the benefit of data agglomeration. We started with the A1 set of images from the CVPPP 2017 dataset, which included 128 images of A. thaliana Col-0. We split this dataset into 64 images for training, 32 images for validation, and 32 images for testing using a fourfold cross-validation approach. We then added more data from the CVPPP 2017 dataset, including sets A2, A3, and A4, and observed improvements in performance metrics such as MSE and percentage agreement.",
  "dataset/redundancy": "In our study, we employed a rigorous approach to ensure the independence and representativeness of our datasets. For the initial experiments, we used the A1 set of images from the CVPPP 2017 dataset, which includes 128 images of A. thaliana Col-0. These images were split into three subsets using a fourfold cross-validation method. Specifically, 64 images were allocated for training, 32 for validation, and 32 for testing. This split was done randomly to ensure that each subset was representative of the entire dataset.\n\nThe validation set played a crucial role in monitoring the model's performance during training and preventing overfitting. Overfitting occurs when a model memorizes the training data rather than learning to generalize from it. By using a separate validation set, we could adjust the model's parameters to improve its performance on unseen data.\n\nIn subsequent experiments, we expanded the dataset by including additional sets of images from the CVPPP 2017 dataset, such as A2 (A. thaliana of five genotypes), A3 (tobacco), and A4 (A. thaliana Col-0). This data agglomeration approach helped in improving the model's accuracy and robustness. The inclusion of diverse data sources ensured that the model could generalize better across different plant species and imaging conditions.\n\nFor the tobacco plant experiments, we fine-tuned the pre-trained network using a variable number of training images, specifically 7, 14, 21, and 27 images. This progressive increase in training data allowed us to observe the impact of data quantity on the model's performance. Similarly, for the komatsuna case, we used a dataset of 300 RGB images of five different komatsuna plants, split into training, validation, and testing sets. The training set consisted of images from two plants, the validation set from one plant, and the testing set from two plants. This split ensured that the model was evaluated on completely independent data.\n\nThe distribution of our datasets compares favorably with previously published machine learning datasets in the field of plant phenotyping. By including diverse plant species and imaging conditions, we aimed to create a more comprehensive and representative dataset. This approach aligns with the broader goal of developing models that can generalize well across different scenarios, rather than being specialized for a single plant species or imaging setup.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used is deep neural networks, specifically convolutional neural networks (CNNs). The architecture employed is not entirely new, as it builds upon established structures like ResNet50. However, the specific implementation and application to leaf counting in plant phenotyping are novel contributions.\n\nThe decision to publish in a plant science journal rather than a machine-learning journal is driven by the primary focus and impact of the work. The research aims to address a critical challenge in plant phenotyping\u2014the accurate counting of leaves\u2014which is a fundamental trait for understanding plant growth and development. By demonstrating the effectiveness of deep learning in this context, the study contributes significantly to the field of plant science. The use of deep learning is a means to an end, with the end being improved plant phenotyping techniques. Therefore, the target audience and the most relevant community for this work are plant scientists and researchers in agricultural technology, rather than primarily machine-learning specialists.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing for our machine-learning algorithm involved several steps to ensure consistency and quality across different datasets. Initially, images from various sources were normalized to address variations in intensity and size. This was achieved through histogram normalization to standardize illumination and resizing all images to a uniform dimension of 320x320 pixels. For multi-modal images, particularly RGB images, which were too small for the ResNet architecture, we upsampled them to 240x240 pixels. This preprocessing ensured that the images were compatible with the input requirements of our neural network.\n\nTo further enhance the robustness of our model, we employed dataset augmentation techniques. These included random geometrical transformations such as rotations, zoom-ins, and shifts. This augmentation helped the network learn from a more diverse set of data, improving its ability to generalize to unseen images.\n\nAdditionally, we used an early stop criterion during training to prevent overfitting. This involved monitoring the validation error and terminating the training process after 10 epochs when the validation error started to increase. This approach helped in maintaining the model's performance on unseen data.\n\nThe images were then fed into the modality branches of our deep neural network, which were based on the ResNet50 architecture. Each modality branch processed the input images independently, generating a vector representation specific to each input type. These vectors were then combined using an element-wise maximum fusion layer, which retained the most meaningful features from all modalities. The fused features were subsequently passed through a fully connected layer with ReLU activation, leading to the final regression layer that predicted the leaf count. The model was trained to minimize the mean squared error between the predicted leaf count and the ground truth.",
  "optimization/parameters": "The model utilizes a deep network architecture designed for leaf counting, incorporating multiple input modalities such as RGB, NIR, and FMP. The architecture includes several key components that contribute to the total number of parameters.\n\nThe modality branch processes each input independently using the ResNet50 architecture, which is a pre-trained model known for its depth and complexity. Each modality branch ends with a fully connected layer consisting of 1024 neurons, activated by the ReLU function. This layer is crucial for extracting meaningful and discriminative features from the input images.\n\nFollowing the modality branches, the feature fusion layer combines information from all modalities using an element-wise maximum fusion. This layer helps retain the most relevant features from each modality, enhancing the overall performance of the model.\n\nThe regression component of the network takes the fused features and processes them through another fully connected layer with 512 neurons, also activated by the ReLU function. This layer is responsible for relating the fused information to the leaf count. The final output is produced by a single neuron that predicts the number of leaves.\n\nTo prevent overfitting and ensure stable results, an L2 regularizer is applied to the last fully connected layer before the output. The regularization constant is set to 0.02, which helps in controlling the magnitude of the weights and improving the generalization of the model.\n\nThe selection of parameters was guided by the need to balance model complexity and performance. The use of pre-trained parameters from the ResNet50 architecture provides a strong foundation, while the fully connected layers and regularization techniques ensure that the model can effectively learn from the data and generalize to new scenarios. The architecture is designed to be adaptable, allowing for the addition of new modalities without changing the overall structure. This flexibility is crucial for extending the model's applicability to different experimental setups and plant species.",
  "optimization/features": "Up to three different input features are used: RGB, NIR, and FMP. These inputs are processed independently through separate modality branches, each utilizing the ResNet50 architecture. The modality branches generate vector representations specific to each input, ensuring meaningful and discriminative features. Each branch ends with a fully connected layer of 1024 neurons using rectified linear unit (ReLU) non-linearity, which suppresses negative values during feature extraction. The output vector size is consistent regardless of the input image size.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the network is designed to learn and retain the most meaningful features through the modality branches and subsequent fusion layer. The fusion layer combines information from all modalities using an element-wise maximum fusion, which helps in retaining the most useful image features. This approach allows the network to handle any number of inputs and demonstrates the benefit of multi-modal learning for plant phenotyping purposes.",
  "optimization/fitting": "The fitting method employed in our study involved a deep neural network architecture designed to predict leaf counts from plant images. The network was initially pre-trained on a large dataset and then fine-tuned on smaller, specific datasets to adapt to different plant species and imaging conditions.\n\nThe number of parameters in our neural network is indeed much larger than the number of training points, especially when fine-tuning with a limited number of images. To address the risk of over-fitting, several strategies were implemented. First, we used a pre-trained network initialized with parameters from a previous image recognition task, which provided a robust starting point. Second, we applied L2 regularization in the last fully connected layer to prevent the network from learning overly complex patterns. Additionally, we employed dataset augmentation techniques, such as random rotations, zoom-ins, and shifts, to artificially increase the diversity of the training data. This helped the network to generalize better and reduced the risk of over-fitting.\n\nTo ensure that the model was not under-fitting, we monitored the performance on a validation set during training. The early stopping criterion was used to halt training when the validation error started to increase, indicating that the model was beginning to over-fit the training data. This approach helped in finding the optimal point where the model generalized well to unseen data.\n\nOverall, the combination of pre-training, regularization, data augmentation, and early stopping allowed us to effectively manage the balance between over-fitting and under-fitting, resulting in a reliable and accurate leaf counting model.",
  "optimization/regularization": "To prevent overfitting, we employed several strategies during the training of our deep neural network. One key technique involved using a validation set to monitor the model's performance and implement early stopping. This approach allowed us to terminate the training process when the validation error started to increase, indicating that the model was beginning to overfit to the training data.\n\nAdditionally, we utilized an L2 regularizer in the last fully connected layer before the output. This regularization technique helps to prevent the network from learning overly complex patterns by penalizing large weights, thereby promoting simpler and more generalizable models. We set the regularization constant to 0.02 for all experiments.\n\nAnother important strategy was dataset augmentation. By applying random geometrical transformations such as rotations, zoom-ins, and shifts to the training data, we artificially increased the diversity of the training set. This helped the network to learn more robust features and generalize better to new, unseen data, reducing the risk of overfitting.\n\nThese combined techniques ensured that our model remained robust and generalizable, even when trained on relatively small datasets.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we trained our network using a learning rate of 0.0001 and employed an early stop criterion to prevent overfitting, terminating training after 10 epochs when the validation error began to increase. The implementation details, including the use of Keras with a TensorFlow backend and training on a machine equipped with a TITAN X GPU, are also provided.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly detailed in the publication. However, the methods and procedures described are comprehensive enough for replication. The code and additional supporting information, such as evaluation metrics and dataset details, are available in the online version of the article. For specific inquiries about model files or further optimization parameters, readers are encouraged to refer to the supplementary materials or contact the authors directly.\n\nThe publication is open-access, and the methods section provides a clear pathway for reproducing the experiments. The use of open-source libraries like Keras and TensorFlow ensures that the tools required for implementation are freely accessible. Additionally, the acknowledgments section credits Nvidia Corp. for providing the GPU used in the experiments, which is a significant resource for training deep learning models.",
  "model/interpretability": "The model presented in this work is not entirely a black box. While deep learning models are often criticized for their lack of interpretability, our approach incorporates several design choices that enhance transparency.\n\nFirstly, the architecture uses a modality branch based on the ResNet50 architecture, which processes each input modality independently. This design ensures that meaningful and discriminative features are extracted from each modality, such as RGB, NIR, and FMP. By visualizing the activations produced by the network for each modality branch, it is possible to observe that most activations are focused on the region where the plant is located. This indicates that the network is learning to focus on relevant parts of the image for leaf counting.\n\nSecondly, the fusion layer combines features from different modalities, retaining the most useful information from each. This process can be interpreted as the network learning to integrate multimodal information effectively. The regression part then relates this fused information to the leaf count through a non-linear regression, adapting image features while learning the regressor.\n\nAdditionally, the model's adaptability through fine-tuning allows for better interpretability in new contexts. By updating the parameters of a pre-trained network with a few labeled images from a new dataset, the model can achieve excellent performance. This capability demonstrates that the network is not merely memorizing the training data but is learning generalizable features that can be adapted to different plant species and imaging conditions.\n\nFurthermore, the evaluation of the network's adaptivity capabilities on various datasets, such as tobacco plants and komatsuna plants, shows that the model can generalize well to new settings. The error distribution analysis provides insights into the model's performance, indicating that it can achieve high agreement with real leaf counts.\n\nIn summary, while the model leverages the power of deep learning, it incorporates design choices that enhance interpretability. The independent processing of modalities, the fusion of multimodal information, and the adaptability through fine-tuning all contribute to a more transparent and interpretable model.",
  "model/output": "The model is a regression model. It is designed to predict the number of leaves in a plant image, which is a continuous value. The output of the model is a single neuron that provides the predicted leaf count. During training, the mean squared error (MSE) between the predicted leaf count and the ground truth is minimized. The model predicts real numbers, and the leaf count is rounded to the nearest integer only at test time. This approach allows the model to handle the variability in leaf counts more effectively than a classification model would. The regression component of the model is crucial for its ability to adapt to different plant species and imaging conditions, making it a versatile tool for plant phenotyping.",
  "model/duration": "The model was trained using a machine equipped with a TITAN X GPU. The training process was monitored using a validation set and an early stop criterion was applied to prevent overfitting. Training was terminated after 10 epochs, as the validation error began to increase. The specific execution time for training the model was not explicitly mentioned, but the use of a high-performance GPU suggests that the training process was efficient. Fine-tuning and adapting the network to new datasets did not require such powerful hardware, indicating that the model can be adapted to new data relatively quickly.",
  "model/availability": "The source code for our deep learning architecture, Pheno-Deep Counter, is publicly available. It can be downloaded from a repository hosted on Bitbucket. The implementation is open source, allowing researchers and practitioners to use, modify, and distribute the code according to the terms of the license provided in the repository. This open-source approach aims to facilitate the adoption and further development of machine learning-based methods for leaf counting in plant phenotyping. Additionally, the repository includes instructions on how to use the software, making it accessible even to those who may not be familiar with the underlying technology.",
  "evaluation/method": "The evaluation of our approach, PhenoDC, for leaf counting was conducted using a comprehensive set of experiments designed to demonstrate its reliability and performance. We adopted established evaluation metrics, including Difference in Count (DiC), Absolute Difference in Count (|DiC|), Mean Squared Error (MSE), and Percentage Agreement (%). These metrics were chosen for their consensus in the broader community and their ability to quantitatively assess the performance of leaf counting algorithms.\n\nTo ensure robust evaluation, we split our data into training and testing sets. The training set was used to optimize the model parameters, while the testing set, consisting of unseen data, was used to evaluate the model's performance. This approach helps in preventing overfitting and ensures that the model generalizes well to new data.\n\nWe performed a series of experiments to highlight various aspects of our method. One key experiment involved data agglomeration, where we showed that increasing data diversity from different sources improved the accuracy of our predictions. This was demonstrated using the CVPPP 2017 dataset, where adding more data from diverse sources significantly reduced the MSE and increased the percentage agreement.\n\nAdditionally, we evaluated our method on the CVPPP 2017 benchmark dataset, comparing it with state-of-the-art methods. The results, presented in a table, showed that PhenoDC outperformed other deep learning methods and machine-learning algorithms in terms of lower MSE and higher percentage agreement. This comparison was statistically significant, as indicated by a paired t-test.\n\nWe also conducted experiments to assess the flexibility of our network to adapt to different contexts, such as varying plant species and imaging conditions. For instance, we fine-tuned our pre-trained network using a variable number of tobacco and komatsuna plant images, demonstrating that more training data led to better predictions. Similarly, we evaluated our method on nocturnal images of Arabidopsis plants, showing reliable leaf counting even in challenging conditions.\n\nOverall, our evaluation methodology involved a combination of cross-validation, independent dataset testing, and novel experiments designed to push the boundaries of our method's capabilities. This comprehensive approach ensures that PhenoDC is a reliable and versatile tool for leaf counting in various plant phenotyping scenarios.",
  "evaluation/measure": "In our evaluation, we adopted a set of performance metrics that are widely accepted in the community for assessing leaf counting algorithms. These metrics include:\n\n* Difference in count (DiC): This metric calculates the mean and standard deviation of the differences between the predicted leaf counts and the ground truth. The best performance is indicated by values close to 0 for both the mean and standard deviation.\n* Absolute difference in count (|DiC|): Similar to DiC, but it considers the absolute values of the differences. Again, lower mean and standard deviation values are better.\n* Mean squared error (MSE): This metric measures the mean of the squared differences between the predictions and the ground truth. A lower MSE indicates better performance.\n* Percentage agreement (%): This metric represents the percentage of times the predicted leaf count matches the ground truth exactly. A higher percentage indicates better performance.\n\nThese metrics provide a comprehensive evaluation of the algorithm's accuracy and reliability. The use of mean and standard deviation for DiC and |DiC| allows us to assess both the central tendency and the variability of the errors. MSE gives a sense of the magnitude of the errors, while percentage agreement provides a straightforward measure of the algorithm's correctness. Together, these metrics offer a robust evaluation of our approach's performance in leaf counting.",
  "evaluation/comparison": "A comprehensive comparison with publicly available methods was conducted on benchmark datasets, specifically the CVPPP 2017 dataset. This dataset includes various plant species and growth conditions, providing a robust evaluation platform. The comparison involved several state-of-the-art methods, including deep learning approaches for leaf counting and those based on leaf segmentation. Notably, the performance was assessed using established evaluation metrics such as Difference in Count (DiC), Absolute DiC, Mean Squared Error (MSE), and Percentage Agreement.\n\nThe results demonstrated that our method, PhenoDC, outperformed all other methods across multiple datasets, achieving the lowest MSE and highest percentage agreement in many cases. This superior performance was statistically significant, as evidenced by a paired t-test comparing our method with Aich and Stavness (2017). The evaluation also highlighted the flexibility of PhenoDC, showing its ability to adapt to different plant species and imaging modalities.\n\nIn addition to comparing with advanced methods, the evaluation included an analysis of the benefits of data agglomeration. By training on diverse datasets, the model's accuracy improved significantly, reducing the MSE by about 50% and increasing the percentage agreement to 56%. This underscores the importance of data diversity in enhancing model performance.\n\nFurthermore, the evaluation considered the impact of multi-modal learning. Using the dataset from Cruz et al. (2016), which includes images acquired in multiple modalities (RGB, NIR, FMP), the model's performance was assessed. The results showed that multi-modal learning led to improved leaf count predictions, further validating the robustness of PhenoDC.\n\nOverall, the comparison with publicly available methods and the analysis of data agglomeration and multi-modal learning provide strong evidence of the reliability and superiority of PhenoDC in leaf counting tasks.",
  "evaluation/confidence": "The evaluation of our method, PhenoDC, includes several performance metrics such as Difference in Count (DiC), Absolute DiC (|DiC|), Mean Squared Error (MSE), and Percentage Agreement (%). These metrics are presented with mean and standard deviation values, providing a measure of variability and confidence in the results.\n\nStatistical significance is assessed using a paired t-test, which shows statistically significant differences (p-value < 0.0001) when comparing our method with Aich and Stavness (2017). This indicates that the improvements observed are not due to random chance, reinforcing the reliability of our approach.\n\nAdditionally, the results demonstrate that PhenoDC outperforms other state-of-the-art methods across various datasets, achieving the lowest MSE and highest percentage agreement in many cases. This consistent superior performance across different metrics and datasets further supports the confidence in our method's effectiveness.",
  "evaluation/availability": "Not enough information is available."
}