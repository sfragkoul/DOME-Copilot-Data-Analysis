{
  "publication/title": "ECG-based epileptic seizure prediction: Challenges of current data-driven models.",
  "publication/authors": "Kalousios S, M\u00fcller J, Yang H, Eberlein M, Uckermann O, Schackert G, Polanski WH, Leonhardt G",
  "publication/journal": "Epilepsia open",
  "publication/year": "2025",
  "publication/pmid": "39529572",
  "publication/pmcid": "PMC11803288",
  "publication/doi": "10.1002/epi4.13073",
  "publication/tags": "- ECG-based seizure prediction\n- Heart rate variability (HRV)\n- Machine learning\n- Seizure prediction\n- Epilepsy\n- Non-causal analysis\n- Pseudo-prospective evaluation\n- Feature selection\n- Multidimensional scaling (MDS)\n- Receiver operating characteristic (ROC) curve\n- Support vector machine (SVM)\n- Extra Trees classifier\n- Patient-specific models\n- Electrocardiogram (ECG) processing\n- Neurophysiological factors",
  "dataset/provenance": "The dataset used in this study was collected at the neurosurgical epilepsy unit of the University Hospital Carl Gustav Carus in Dresden, Germany, between September 2021 and December 2022. The data consists of ECG recordings from patients with epilepsy (PWE) who were undergoing monitoring with either non-invasive video-electroencephalography (V-EEG) or invasive intracranial EEG (iEEG) for presurgical evaluation. The inclusion criteria for participants were being 18 years of age or older and having more than three recorded epileptic seizures during monitoring, with each seizure at least 2 hours apart. Exclusion criteria included having a vagus nerve stimulator, cardiac pacemaker, or cardiac arrhythmia.\n\nA total of 39 patients were included in the study, comprising 8 women and 31 men, with a median age of 38 years. The dataset comprises 252 seizures that were eligible for analysis, with patients experiencing a median of five seizures. For each seizure, a 1-hour segment of the ECG signal preceding seizure onset was analyzed. The ECG signals were preprocessed and features were extracted from overlapping three-minute windows with a 10-second stride. Windows containing artifacts were excluded from further analyses. The extracted features belong to the time and frequency domains, with a considerable part obtained by non-linear analyses, including recurrent quantification analysis. A table listing all 97 features can be found in the supplementary material.\n\nThe dataset has not been used in previous papers by the community.",
  "dataset/splits": "In our study, two distinct experimental designs were employed, each involving different data splits.\n\nFor the non-causal experiment, a leave-one-out cross-validation (LOOCV) approach was used. This method involved creating a unique data split for each seizure, treating each seizure as a separate event. Consequently, the number of data splits was equal to the number of seizures in the dataset. Each split consisted of a single seizure as the test set and the remaining seizures as the training set. This process was repeated for each seizure, ensuring that every seizure was used once as a test set.\n\nIn the causal experiment, a pseudo-prospective testing approach was utilized. This design involved creating multiple data splits based on the chronological order of seizures. For each seizure, past seizures were used as the training set, and the current seizure served as the test set. This process was repeated for each seizure, starting from the third seizure onwards, as at least two past seizures were required for training. Therefore, the number of data splits was equal to the total number of seizures minus two. The distribution of data points in each split varied, with the training set size increasing as more seizures were included in the chronological order.\n\nAdditionally, the causal experiment was subdivided into three complexity levels. At the first level, all features were used for training. The second level introduced feature selection, retaining only the top features based on their importance. The third level combined feature selection with hyperparameter optimization. Each complexity level involved the same number of data splits, but the feature sets and hyperparameters varied across the splits.\n\nIn summary, the non-causal experiment involved a single data split per seizure, while the causal experiment involved multiple data splits based on the chronological order of seizures, with varying complexity levels. The distribution of data points in each split was determined by the number of seizures in the dataset and the specific experimental design.",
  "dataset/redundancy": "In our study, we employed two distinct experimental designs to analyze the dataset: a non-causal analysis and a pseudo-prospective analysis. For the non-causal analysis, we used a leave-one-out cross-validation (LOOCV) approach. This method involved treating each seizure as a separate event, ensuring that the chronological order of seizures was disregarded. This design allowed us to exhaustively test all unique feature combinations and fit and test models for each patient, aiming to approximate the upper limit of data separability using predefined training labels.\n\nIn the pseudo-prospective analysis, we adopted a more realistic approach by considering each seizure as an independent test set, while past data served as the training set. This method maintained the temporal sequence of events, ensuring that the training and test sets were independent. To optimize models and enhance performance, we subdivided this experiment into three complexity levels. The first level used all features for training in each iteration with a support vector machine (SVM) classifier. The second level introduced feature selection, retaining only features that surpassed the 90th percentile of the Extra Trees classifier's Gini Index from the training set. The third level combined feature selection with hyperparameter optimization through a grid search in a nested LOOCV, repeated in each iteration. This design ensured that any potential information leakage was excluded.\n\nThe distribution of our dataset compares favorably with previously published machine learning datasets in the field of seizure prediction. Our approach of using LOOCV in the non-causal experiment allowed for a comprehensive evaluation of feature combinations, while the pseudo-prospective design provided a more practical assessment of model performance. The results from these experiments highlight the challenges and variability in seizure prediction, aligning with findings from other studies that emphasize the importance of temporal dynamics and feature selection in improving predictive accuracy.",
  "dataset/availability": "Not applicable.",
  "optimization/algorithm": "The optimization algorithm employed in our study utilizes a support vector machine (SVM) classifier, which is a well-established machine-learning algorithm class. The SVM is not a new algorithm; it has been extensively used in various fields, including bioinformatics and medical research, due to its effectiveness in handling high-dimensional data and its ability to generalize well to unseen data.\n\nThe SVM classifier used in our work is not novel and has been previously applied in similar studies. The choice of SVM was driven by its proven performance in classification tasks, particularly in scenarios involving complex and high-dimensional datasets, such as those encountered in seizure prediction using heart rate variability (HRV) features.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of our study is on the application of machine-learning techniques to seizure prediction rather than the development of new machine-learning algorithms. Our primary contribution lies in the innovative application of existing machine-learning methods to a specific medical problem, namely the prediction of seizures using HRV data. The SVM, along with other techniques like the Extra Trees classifier and multidimensional scaling, was selected for its robustness and suitability for the task at hand. The emphasis is on the practical implementation and evaluation of these methods in a clinical context, rather than the theoretical advancements in machine-learning algorithms.",
  "optimization/meta": "The experiments conducted did not involve a meta-predictor. Instead, the study employed a pseudo-prospective testing approach where each seizure was considered an independent test set, with past data assigned to the training set. The models were optimized through different complexity levels, incorporating feature selection and hyperparameter optimization.\n\nThe first level of complexity used a support vector machine (SVM) classifier with all features for training in each iteration. The second level introduced feature selection, choosing features that surpassed the 90th percentile of the Extra Trees classifier's Gini Index from the training set. The third level combined feature selection with hyperparameter optimization through a grid search in a nested leave-one-out cross-validation (LOOCV), repeated in each iteration.\n\nThe study utilized an Extra Trees classifier for dimensionality reduction and feature selection, and an SVM classifier with a radial basis function (RBF) kernel for classification. The results were averaged over five iterations due to the stochastic nature of the feature selection strategy, ensuring that any potential information leakage was excluded. The performance was evaluated using the area under the curve (AUC) of the receiver operating characteristic (ROC), with improvement over chance (IoC) accepted if the majority of iterations achieved better than chance classification performance for each seizure.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps. Initially, windows containing 10-second segments previously flagged as artifacts were excluded from further analyses. The extracted features belonged to the time and frequency domains, with a significant portion obtained through non-linear analyses, including recurrent quantification analysis. A total of 97 features were considered, with a detailed list available in the supplementary material.\n\nFor the non-causal experiment, an Extra Trees classifier was employed for dimensionality reduction, retaining only the top 10 features based on their Gini Index. This step was crucial due to the exponential increase in computational complexity with the number of input features. The performance was evaluated using leave-one-out cross-validation (LOOCV), treating seizures as separate events.\n\nIn the pseudo-prospective experiment, the data was preprocessed to ensure that past seizures served as the training set, while the next chronological seizure served as the test set. This approach was repeated for each patient, with the number of iterations depending on the total number of seizures. Feature selection was performed in the second and third complexity levels, where only features surpassing the 90th percentile of the Extra Trees classifier's Gini Index from the training set were selected. Additionally, hyperparameter optimization was conducted through a grid search in a nested LOOCV for the third complexity level.\n\nThe data was further visualized using multidimensional scaling (MDS) to map high-dimensional observations onto a lower-dimensional space, preserving the original similarities or dissimilarities in the data. This technique was particularly useful for identifying differences in heart rate variability (HRV) between interictal and preictal intervals.\n\nOverall, the data encoding and preprocessing steps were designed to optimize the performance of the machine-learning algorithms while ensuring the integrity and relevance of the features used for classification.",
  "optimization/parameters": "In our study, we utilized a range of features derived from heart rate variability (HRV) data, which included time domain, frequency domain, and non-linear analyses features. Initially, we had 97 features. To manage the computational complexity, we employed an Extra Trees classifier for dimensionality reduction, retaining only the top 10 features based on their Gini Index. These top features were then used to create unique combinations, with the number of features in each combination ranging from 3 to 10, resulting in 968 unique combinations. The selection of these features and their combinations was driven by the goal of optimizing the model's performance in distinguishing between interictal and preictal states.\n\nThe model parameters were further optimized through a grid search in a nested leave-one-out cross-validation (LOOCV) process, which was repeated for each patient. This approach ensured that the model parameters were tuned to maximize the area under the receiver operating characteristic curve (ROC-AUC) for each unique combination of features. The final model parameters were selected based on their performance in the cross-validation process, aiming to achieve the highest average ROC-AUC.",
  "optimization/features": "In our study, we initially extracted a total of 97 features from the data, which belong to the time and frequency domains, as well as non-linear analyses, including recurrent quantification analysis. To manage the computational complexity, we employed an Extra Trees classifier for dimensionality reduction, retaining only the top 10 features based on their Gini Index.\n\nFeature selection was performed using the training set only. In the non-causal experiment, we conducted a recursive search to identify the feature combination yielding the highest area under the receiver operating characteristic curve (ROC-AUC) scores for each patient. In the causal experiment, feature selection was integrated into the pseudo-prospective testing approach. At the second complexity level, features surpassing the 90th percentile of the Extra Trees classifier's Gini Index from the training set were selected. At the third complexity level, feature selection was combined with hyperparameter optimization through a grid search in a nested leave-one-out cross-validation (LOOCV), repeated in each iteration.\n\nThe number of features used as input varied depending on the experiment and the complexity level. In the non-causal experiment, the selected combinations comprised an average of 3.9 features. In the causal experiment, the number of features ranged from a minimum of 3 to a maximum of 10, with 968 unique combinations being tested.",
  "optimization/fitting": "In our study, we employed a robust methodology to address potential issues of overfitting and underfitting. The number of parameters was indeed larger than the number of training points, particularly in the non-causal experiment where we exhausted all unique feature combinations. To mitigate overfitting, we utilized leave-one-out cross-validation (LOOCV), which ensures that each seizure is used as a test set exactly once, while the remaining seizures form the training set. This approach helps in assessing the model's performance on unseen data and reduces the risk of overfitting.\n\nAdditionally, in the pseudo-prospective experiment, we implemented a pseudo-prospective testing approach where each seizure was considered an independent test set, and past data were used for training. This method simulates real-world conditions more closely and helps in evaluating the model's generalizability. We further subdivided this experiment into three complexity levels, incorporating feature selection and hyperparameter optimization to enhance performance and prevent overfitting.\n\nTo rule out underfitting, we ensured that our models were sufficiently complex to capture the underlying patterns in the data. The use of an Extra Trees classifier for feature selection and an SVM with a radial basis function (RBF) kernel allowed us to handle non-linear relationships effectively. Moreover, the recursive feature selection process helped in identifying the most relevant features, thereby improving the model's performance without underfitting the data.\n\nIn summary, our approach combined rigorous cross-validation techniques, feature selection, and hyperparameter optimization to balance the trade-off between overfitting and underfitting, ensuring that our models were both generalizable and capable of capturing the complexities in the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was feature selection, which involved creating unique combinations of features and testing them with leave-one-out cross-validation (LOOCV) using a support vector machine (SVM) classifier. This process helped in identifying the most relevant features and reducing the dimensionality of the data, thereby mitigating the risk of overfitting.\n\nAdditionally, we utilized a pseudo-prospective testing approach, where each seizure was considered an independent test set, and past data was used for training. This method ensured that the models were evaluated in a more realistic and temporally consistent manner, reducing the likelihood of overfitting to the training data.\n\nIn the third complexity level of our pseudo-prospective testing, we combined feature selection with hyperparameter optimization through a grid search in a nested LOOCV. This nested approach further helped in preventing overfitting by ensuring that the hyperparameters were optimized in a way that generalizes well to unseen data.\n\nMoreover, due to the stochastic nature of the feature selection strategy, the latter two complexity levels were conducted five times for each patient. The results were averaged, and improvement over chance (IoC) classification score was accepted only if the majority of the five iterations achieved better than chance classification performance for each seizure. This repetitive evaluation process added another layer of robustness to our models, ensuring that the results were not due to random fluctuations.\n\nOverall, these techniques collectively helped in preventing overfitting and enhancing the generalizability of our models.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the publication. Specifically, we employed an Extra Trees classifier for feature selection, retaining the top 10 features based on the Gini Index. For the Support Vector Machine (SVM) classifier, we utilized a radial basis function (RBF) kernel. The optimization process involved a grid search for hyperparameter tuning within a nested leave-one-out cross-validation (LOOCV) framework.\n\nThe experimental design included three complexity levels for pseudo-prospective testing. At the first level, all features were used for training in each iteration. The second level introduced feature selection, retaining only features that surpassed the 90th percentile of the Extra Trees classifier's Gini Index. The third level combined feature selection with hyperparameter optimization through a grid search in a nested LOOCV, repeated for each iteration.\n\nDue to the stochastic nature of the feature selection strategy, the latter two complexity levels were conducted five times for each patient, and the results were averaged. The improvement over chance (IoC) classification score was accepted if the majority of the five iterations achieved better-than-chance classification performance for each seizure.\n\nAll analyses were conducted using Python (version 3.9.7), with custom algorithms and open-source libraries, including Neurokit2 (version 2.1) for HRV extraction and Scikit-learn (version 1.1.2) for machine-learning models. The specific configurations and parameters are described in the methods section of the paper, ensuring reproducibility.\n\nThe model files and detailed optimization schedules are not explicitly provided in the publication. However, the methods and parameters used are thoroughly described, allowing interested researchers to replicate the experiments. The use of open-source libraries and standard machine-learning techniques ensures that the configurations are accessible and can be implemented by others in their own research.",
  "model/interpretability": "The models employed in this study are not entirely black-box, as they incorporate techniques that enhance interpretability. The Extra Tree classifier, for instance, was used for dimensionality reduction and feature selection. This classifier provides a measure of feature importance based on the Gini Index, which indicates the normalized total reduction criterion during feature split. This allows for the identification of the top features that contribute most to the model's predictions, making it possible to understand which features are driving the classification decisions.\n\nAdditionally, multidimensional scaling (MDS) was utilized to visualize complex heart rate variability (HRV) data in a more familiar 2D space. This technique preserves the original similarities or dissimilarities in the data, allowing for the formulation of hypotheses based on visual observations. By mapping high-dimensional observations onto a lower-dimensional space, MDS helps in interpreting the relationships and patterns within the data.\n\nThe use of support vector machines (SVM) with a radial basis function (RBF) kernel also contributes to interpretability to some extent. While SVMs are generally considered more interpretable than some other machine learning models, the decision boundary created by the SVM can provide insights into how the model distinguishes between different classes. The features selected by the Extra Tree classifier and used in the SVM can be examined to understand their impact on the classification outcomes.\n\nFurthermore, the study correlated the results with clinical metadata, which adds another layer of interpretability. By examining how the model's performance relates to clinical factors, such as sleep-wake cycles and seizure onset zones, it is possible to gain a deeper understanding of the underlying physiological processes and how they influence the model's predictions.\n\nIn summary, while the models used in this study are not entirely transparent, they incorporate several techniques that enhance interpretability. The use of feature importance measures, dimensionality reduction, data visualization, and correlation with clinical metadata all contribute to a better understanding of the model's decisions and the underlying data patterns.",
  "model/output": "The model employed in this study is a classification model. Specifically, it uses a Support Vector Machine (SVM) with a radial basis function (RBF) kernel for distinguishing between different classes. The primary goal of the model is to predict epileptic seizures based on ECG data. The performance of the model is evaluated using the area under the receiver operating characteristic curve (ROC-AUC), which measures the model's ability to distinguish between two classes. The model's performance is considered an improvement over chance if the AUC value is significantly greater than that of a naive predictor. The experiments conducted include both non-causal and causal analyses, with the non-causal experiment achieving an average ROC-AUC of 0.823, indicating strong classification performance. The causal experiment, which uses a pseudo-prospective testing approach, shows more varied results, with average ROC-AUC scores around 0.56 across different complexity levels. The model's output is used to visualize complex heart rate variability (HRV) data in a more familiar 2D space, aiding in the formulation of hypotheses based on these observations.",
  "model/duration": "The execution time for the models varied depending on the experiment design. In the non-causal experiment, the process involved fitting and testing numerous models for each patient, exhausting all unique feature combinations. This recursive search and leave-one-out cross-validation approach was computationally intensive, but it allowed us to identify the best-performing feature combinations for each patient.\n\nIn the pseudo-prospective experiment, the execution time was influenced by the complexity level. At the first level, the model was trained with all features using an SVM classifier. The second level introduced feature selection, which added some overhead but helped in improving the model's performance. The third level combined feature selection with hyperparameter optimization through a grid search, which was the most time-consuming due to the nested leave-one-out cross-validation.\n\nThe stochastic nature of the feature selection strategy required repeating the latter two complexity levels five times for each patient, and the results were averaged. This approach ensured that the models were robust and that the improvement over chance classification score was reliable.\n\nOverall, the execution time was significant, but it was necessary to ensure that the models were thoroughly evaluated and optimized. The use of custom algorithms and open-source libraries, including Neurokit 2 for HRV extraction and Scikit-learn for machine-learning models, facilitated the implementation and execution of the models.",
  "model/availability": "The source code for the models and algorithms used in this study is not publicly released. However, the analyses were conducted using Python, version 3.9.7, with custom algorithms and open-source libraries. Specifically, Neurokit2, version 2.1, was used for heart rate variability (HRV) extraction, and Scikit-learn, version 1.1.2, was utilized for machine-learning models. These libraries are freely available and can be accessed through their respective repositories. The custom algorithms developed for this study are not provided as standalone software or executable files. Additionally, there is no web server, virtual machine, or container instance available for running the algorithm.",
  "evaluation/method": "The evaluation of the methods employed in this study involved two distinct experiments: a non-causal analysis and a causal analysis using a pseudo-prospective testing approach.\n\nFor the non-causal experiment, a leave-one-out cross-validation (LOOCV) strategy was utilized. This method involved treating each seizure as a separate event and recursively searching for the feature combination that yielded the highest area under the receiver operating characteristic curve (ROC-AUC) scores for each patient. An Extra Trees classifier was used for dimensionality reduction, retaining only the top 10 features based on their Gini Index. This approach aimed to examine the separability of the training data without considering the temporal order of the seizures.\n\nIn the causal experiment, a pseudo-prospective testing approach was adopted. This method involved successively considering each seizure as an independent test set, while past data was assigned to the training set. The experiment was subdivided into three complexity levels. The first level used a support vector machine (SVM) classifier with all features for training in each iteration. The second level introduced feature selection, retaining only features that surpassed the 90th percentile of the Extra Trees classifier's Gini Index from the training set. The third level combined feature selection with hyperparameter optimization through a grid search in a nested LOOCV, repeated in each iteration. This approach aimed to optimize models and enhance performance while excluding any potential information leakage.\n\nThe performance of the models was evaluated using the area under the curve (AUC) of the receiver operating characteristic (ROC), which measures a model's ability to distinguish between two classes. The ROC curve expresses the relationship between the true positive rate (sensitivity) and the false positive rate (1\u2014specificity) at various classification thresholds. The improvement over chance (IoC) was defined as the model's performance if the chance of a naive predictor achieving an equal or better score than the tested model was less than 5% (p < 0.05). This threshold-free metric was chosen to simplify model comparison, as it summarizes multiple threshold-dependent metrics into one.\n\nAll analyses were conducted with Python (v. 3.9.7), using custom algorithms and open-source libraries, including Neurokit 2 (v. 2.1) for heart rate variability (HRV) extraction and Scikit-learn (v. 1.1.2) for machine-learning models. The results of the non-causal experiment showed an average ROC-AUC of 0.823 (\u00b10.12), with IoC classification scores achieved in 82.5% of the seizures. The causal experiment, however, demonstrated more variability in performance, with average ROC-AUC scores around 0.56 and IoC classification rates ranging from 45.4% to 49.4%. The wide dispersion in scores highlighted the challenges in achieving consistent performance across different patients and testing methods.",
  "evaluation/measure": "In our study, we primarily focused on the area under the curve (AUC) of the receiver operating characteristic (ROC) as our key performance metric. The ROC curve illustrates the relationship between the true positive rate (sensitivity) and the false positive rate (1\u2014specificity) across various classification thresholds. This threshold-free metric was chosen to simplify model comparison, as it consolidates multiple threshold-dependent metrics into a single value.\n\nThe AUC provides a measure of a model's ability to distinguish between two classes. Specifically, when classifying a finite number of samples, the AUC values obtained by a naive predictor should follow a normal distribution with a mean of 0.5. A model's performance is considered an improvement over chance (IoC) if the probability of a naive predictor achieving an equal or better score than the tested model is less than 5% (p < 0.05). Therefore, a model's performance is defined as IoC if the AUC value is significantly greater than that of a naive predictor.\n\nIn addition to the AUC, we also reported the standard deviation (SD) of the AUC scores to provide an indication of the variability in performance across different patients or experiments. This metric helps to understand the consistency and reliability of the model's predictions.\n\nOur choice of metrics is representative of the literature in the field of seizure prediction. The AUC is a widely accepted and commonly used metric for evaluating the performance of classification models, particularly in medical and biological research. It provides a comprehensive assessment of a model's discriminative power and is less sensitive to class imbalances compared to other metrics, such as accuracy. By reporting the AUC and its associated SD, we aim to provide a clear and transparent evaluation of our models' performance, enabling meaningful comparisons with other studies in the field.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of different methods to evaluate their performance in seizure prediction. We employed two main experimental designs: a non-causal experiment and a pseudo-prospective experiment. The non-causal experiment involved a leave-one-out cross-validation (LOOCV) approach, where we exhaustively tested all unique feature combinations using a support vector machine (SVM) classifier. This method allowed us to approximate the upper limit of data separability using predefined training labels. The average ROC-AUC score across patients was 0.823, indicating a high level of performance.\n\nIn contrast, the pseudo-prospective experiment aimed to simulate a more realistic scenario by considering causality. We tested three levels of complexity: using all features, feature selection (FS), and FS combined with hyperparameter optimization. The average ROC-AUC score across all three levels was approximately 0.56, which is closer to the chance level. This discrepancy highlights the challenges in achieving high performance in causal studies compared to non-causal ones.\n\nWe also compared the performance of different feature selection and optimization strategies within the pseudo-prospective framework. Despite efforts to optimize models through FS and hyperparameter tuning, there was no significant uplift in performance. This suggests that the inherent complexity and variability in the data pose substantial challenges for seizure prediction.\n\nAdditionally, we visualized the dispersion in scores, noting that patients with a small number of seizures tended to contribute to both extremes of the AUC dispersion. Patients with a seizure number above the median demonstrated more consistent scores, with some achieving high IoC classification rates across different methodologies.\n\nIn summary, our comparison of methods revealed that while non-causal approaches can achieve high performance, causal studies face significant challenges. The wide dispersion in scores and the lack of clear superiority among different pseudo-prospective testing methods underscore the need for more robust and probabilistic approaches in seizure prediction.",
  "evaluation/confidence": "The evaluation of our methods involved several key performance metrics, primarily focusing on the area under the curve (AUC) of the receiver operating characteristic (ROC) and the improvement over chance (IoC) classification scores. The AUC provides a threshold-free metric that summarizes the model's ability to distinguish between two classes, offering a comprehensive view of performance across various classification thresholds.\n\nFor the non-causal experiment, the average ROC-AUC was 0.823 with a standard deviation of 0.12, indicating a relatively consistent performance across patients. The IoC classification scores were achieved in 82.5% of the seizures, suggesting that the model's performance was significantly better than chance. This high percentage of IoC classifications underscores the model's reliability and effectiveness in distinguishing between interictal and preictal intervals.\n\nIn the pseudo-prospective experiment, the average ROC-AUC score was approximately 0.56, with IoC classifications ranging from 45.4% to 49.4% across different complexity levels. While this performance is lower than the non-causal experiment, it still demonstrates an improvement over chance. The wide dispersion in scores, with some patients scoring below chance level and others nearly perfectly, highlights the variability in individual patient responses. However, patients with a higher number of seizures in the test set showed more consistent performance, with two patients achieving \u2265 75% IoC classification across different methodologies.\n\nStatistical significance was assessed to ensure that the observed performance was not due to random fluctuations. A model's performance was defined as IoC if the chance of a naive predictor achieving an equal or better score was less than 5% (p < 0.05). This stringent criterion ensures that the reported improvements are statistically significant and not merely artifacts of the data.\n\nOverall, the evaluation metrics provide a robust assessment of the models' performance, with clear indications of statistical significance and improvement over chance. The results demonstrate the potential of the methods for seizure prediction, albeit with room for further optimization and refinement.",
  "evaluation/availability": "The raw evaluation files are not shared. The data availability statement explicitly mentions that research data are not shared. Therefore, the evaluation files used in the study are not publicly available."
}