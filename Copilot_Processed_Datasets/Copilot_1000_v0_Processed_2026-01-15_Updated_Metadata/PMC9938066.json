{
  "publication/title": "A proof of concept reinforcement learning based tool for non parametric population pharmacokinetics workflow optimization.",
  "publication/authors": "Otalvaro JD, Yamada WM, Hernandez AM, Zuluaga AF, Chen R, Neely MN",
  "publication/journal": "Journal of pharmacokinetics and pharmacodynamics",
  "publication/year": "2023",
  "publication/pmid": "36478350",
  "publication/pmcid": "PMC9938066",
  "publication/doi": "10.1007/s10928-022-09829-5",
  "publication/tags": "- Pharmacokinetics\n- Reinforcement learning\n- Automation\n- Modeling process\n- Model building\n- Population pharmacokinetics\n- Machine learning\n- Pharmacometric modeling\n- Drug development\n- Optimization algorithms\n- Non-parametric modeling\n- Computational pharmacology\n- Pharmacometric workflow\n- Model selection\n- Parameter estimation",
  "dataset/provenance": "The dataset used in this study was previously published and simulated. It was initially described and modeled using NPAG by another source. This dataset was chosen because it includes a combination of unimodality, bimodality, and outliers, which provided a robust test for the reinforcement learning agent. Additionally, since it is a simulated dataset, the original distribution was known, allowing for a clear evaluation of the agent's performance. The dataset was restricted to structural models with one and two compartments, with the coefficients of the error polynomial fixed to their original values. No covariates were included in this analysis. The total number of data points is not explicitly stated, but the dataset was sufficient to run 1000 episodes with 30 actions per episode, resulting in a theoretical maximum of 30,000 calls to NPOD.",
  "dataset/splits": "Not applicable",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The raw data, logs, and source code used in our study are publicly available on our GitHub repository. This includes all the necessary information to replicate our findings and build upon our work. The repository can be accessed at https://github.com/Siel/Vial. The data is provided under a license that allows for open access and use, ensuring that researchers and practitioners can utilize it for further studies or applications. By making our data and code publicly available, we aim to promote transparency and reproducibility in our research. This approach not only facilitates verification of our results but also encourages collaboration and innovation within the scientific community.",
  "optimization/algorithm": "The optimization algorithm employed in our study falls under the class of reinforcement learning (RL) algorithms. Specifically, we utilized the SARSA (State-Action-Reward-State-Action) algorithm, which is a well-established method within the RL framework. This algorithm was chosen for its suitability in handling the iterative process of model building in population pharmacokinetics.\n\nThe SARSA algorithm is not new; it has been in use for several years. However, its application in the context of automating non-parametric population pharmacokinetics workflow optimization is novel. This specific use case has not been extensively explored in previous research, particularly since 2013. Our implementation serves as a proof of concept, demonstrating how RL can be integrated into the pharmacometrics field to automate parts of the model-building process.\n\nThe decision to publish this work in a pharmacokinetics journal rather than a machine-learning journal is driven by the primary focus of our research. Our aim is to introduce pharmacometrists to the potential of RL in their field, providing them with a practical tool to enhance their workflow. While the RL algorithm itself is not new, its application in this specific domain is innovative and addresses a gap in the current methodologies used in pharmacokinetics. By presenting this work in a pharmacokinetics journal, we hope to reach the intended audience of pharmacometrists and encourage further exploration and development of RL techniques in this area.",
  "optimization/meta": "Not applicable",
  "optimization/encoding": "The data encoding for the machine-learning algorithm involved defining discrete state and action representations. For the state definition, given the limitation of having a discrete state, the actual parameter ranges values did not matter. Instead, a key was used to differentiate different states based on the current value of the ranges and the number of compartments used. Specific strings were defined to describe the state in one and two compartments scenarios, using symbols to represent upper and lower bounds.\n\nThe action space was also discretized. For a one-compartment model, actions included increasing or decreasing the lower and upper bounds of parameters like ke and v, as well as switching to a two-compartment model. For a two-compartment model, additional actions were defined for parameters like kcp and kpc. The step sizes for increasing and decreasing actions were set to ensure consistency and finiteness of the action-state space.\n\nThe reward function was defined based on the log-likelihood returned by the Maximum Likelihood methods. For the first run in each episode, the reward was equal to the likelihood achieved by taking the first action. Subsequent rewards were calculated as the difference between the current and previous likelihoods. This approach ensured that the total sum of discounted rewards was not dependent on the number of steps in each episode. The reward function was designed to encourage exploration by preferring non-previously explored actions over previously non-effective ones.",
  "optimization/parameters": "In our study, the number of parameters used in the model varies depending on the structural model being considered. For a one-compartment model, there are two primary parameters: the elimination rate constant (ke) and the volume of distribution (v). When switching to a two-compartment model, additional parameters are introduced, specifically the inter-compartmental clearance rates (kcp and kpc).\n\nThe selection of these parameters is driven by the need to capture the essential pharmacokinetic processes. The one-compartment model is suitable for drugs that exhibit simple pharmacokinetic behavior, where the drug is assumed to be uniformly distributed in a single compartment. In contrast, the two-compartment model is used for drugs that exhibit more complex behavior, such as those that distribute into both central and peripheral compartments.\n\nThe decision to include or exclude certain parameters is based on the model's ability to fit the data and the likelihood of the model. The agent, using the SARSA algorithm, adjusts the bounds of these parameters to optimize the model's performance. This adjustment process involves increasing or decreasing the lower and upper bounds of each parameter, thereby exploring the parameter space to find the best fit.\n\nThe agent's actions are discrete, with specific steps defined for increasing or decreasing the bounds of each parameter. For example, the increase step size is set to 10%, while the decrease step size is fixed to ensure consistency between opposing actions. This discrete approach helps in managing the finite action-state space, making the optimization process more tractable.\n\nIn summary, the model parameters are selected based on the structural complexity required to accurately represent the pharmacokinetic behavior of the drug. The agent's actions focus on adjusting the bounds of these parameters to optimize the model's fit to the data.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The fitting method employed in this work involves a non-parametric population pharmacokinetics (pop-PK) approach, which does not assume a specific family of statistical distributions for model parameters. This method requires searching for both the best-fit model structure and the best-fit model parameter ranges.\n\nThe optimization process often relies on a trial-and-error phase, where the modeler makes changes to the model, fits it again, and evaluates the results to decide if further changes are needed. To automate this process, a reinforcement learning (RL) algorithm, specifically SARSA, was used. This algorithm helps in navigating the discrete action space, which includes actions like increasing or decreasing the bounds of parameters and switching between model structures (e.g., one-compartment vs. two-compartment models).\n\nThe reward function for the RL agent is defined based on the log-likelihood returned by the Maximum Likelihood method. The initial reward is set to the likelihood achieved by the first action, and subsequent rewards are calculated as the difference between the current and previous likelihoods. This approach ensures that the total sum of discounted rewards is not dependent on the number of steps in each episode.\n\nTo address the potential issue of overfitting, future work suggests developing a new reward function that considers multiple elements such as fitness, number of parameters, execution time, and complexity. This would involve using criteria like the Akaike Information Criterion or Bayesian Information Criterion to balance the number of parameters and model complexity. Additionally, splitting the data into training and evaluation sets and rewarding the agent based on the evaluation set performance can help prevent overfitting.\n\nUnderfitting is mitigated by the extensive search process conducted by the RL agent, which explores a wide range of model structures and parameter ranges. The agent's actions, such as adjusting parameter bounds and switching between model structures, ensure that the model is thoroughly explored and optimized. The use of a Sobol sequence for initial support points and an interior point method for weight optimization further enhances the robustness of the fitting process.\n\nThe training process for the RL agent involved 1000 episodes with 30 actions per episode, taking approximately 5.5 hours on a machine with an AMD Ryzen 5950x. After training, the obtained Q matrix can be used to fit the dataset multiple times with minimal effort. This approach demonstrates the potential of RL in automating the non-parametric model-building process, although further advancements and optimizations are needed to overcome current limitations.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and optimization parameters used in our study are detailed within the publication. Specifically, the SARSA algorithm was configured with parameters such as alpha (0.9), epsilon (1), and a time horizon of 30 actions per episode over 1000 episodes. The initial state for the environment was set with a two-compartment model and specific parameter ranges, deliberately deviated from the true values to challenge the agent.\n\nThe raw data, logs, and source code, including the model files and optimization parameters, are available on our GitHub repository. This repository provides access to all the necessary files and scripts used in our experiments, allowing for reproducibility and further exploration. The repository is licensed under terms that permit open access and use, ensuring that researchers can build upon our work.\n\nFor those interested in the detailed implementation and results, the GitHub repository serves as a comprehensive resource. It includes the complete codebase, data logs, and any additional materials needed to replicate the experiments or adapt the methods for other studies. The availability of these resources underscores our commitment to transparency and collaboration within the scientific community.",
  "model/interpretability": "The model presented in this work is not a black box. It is designed to be interpretable and transparent, allowing pharmacometricians to understand the decision-making process of the reinforcement learning agent. The model's actions and state representations are discrete and well-defined, making it easier to trace the agent's decisions.\n\nFor instance, the agent has specific actions related to adjusting the bounds of parameters such as ke (elimination rate constant) and v (volume of distribution). These actions include increasing or decreasing the lower and upper bounds of these parameters. Additionally, the agent can switch between one-compartment and two-compartment models, which are standard in pharmacokinetics. This switching action is explicit and can be easily understood by pharmacometricians familiar with compartmental models.\n\nThe state definition is also clear. The states are represented by strings that encode the current values of the parameter ranges and the number of compartments used. For example, a state might be represented as \"f# ke; # v; \" ke; \" vg00\" for a one-compartment model, where \"#\" and \"\" represent the lower and upper bounds, respectively. This string-based representation allows for a straightforward interpretation of the model's state at any given time.\n\nFurthermore, the reward function is based on the likelihood achieved by the model, which is a well-understood metric in pharmacometrics. The reward is calculated as the difference between the current and previous likelihoods, providing a clear incentive for the agent to improve the model's fit to the data. This transparency in the reward function helps pharmacometricians understand why certain actions were taken by the agent.\n\nIn summary, the model's design ensures that it is not a black box. The discrete actions, clear state representations, and interpretable reward function make it possible for pharmacometricians to follow the agent's decision-making process and understand the rationale behind its actions. This transparency is crucial for building trust in the model and for integrating it into the pharmacometric workflow.",
  "model/output": "The model presented in this work is a reinforcement learning (RL) based tool designed for non-parametric population pharmacokinetics (pop-PK) workflow optimization. It is not a traditional classification or regression model but rather an autonomous algorithm that guides or independently performs sections of the non-parametric pop-PK model selection process.\n\nThe RL agent interacts with an interface to the Non-Parametric Optimal Design algorithm, which performs the parameter optimization. The agent's goal is to find the structural model that maximizes the final likelihood for a specific pharmacokinetic dataset. The output of the model is a set of best-fit models based on quantitative measures, specifically the likelihood achieved by the actions taken by the agent.\n\nThe agent's performance is evaluated based on the likelihood and the number of support points, which are used to index the Q matrix. The reward function is defined as the difference between the current obtained likelihood and the previous one, encouraging improvements over the last reward obtained. This approach ensures that the total sum of discounted rewards is not dependent on the number of steps in each episode.\n\nThe agent was configured to run for 1000 episodes with a limit of 30 actions per episode. The total amount of time used to train the agent was 5.5 hours, although it is believed that this time can be further improved. The framework provided could allow the integration of more actions, such as adding or removing covariates, non-linear compartments, or executing secondary analysis.\n\nThe model's output is a set of best-fit models that can be further evaluated by pharmacometricians for qualitative measures of goodness of fit. The agent's ability to automate the iterative model building process allows for more efficient use of time, promoting and leveraging the pharmacometrician's creativity and insight during the pop-PK task.",
  "model/duration": "The model's execution time for completing 1000 episodes, with each episode consisting of 30 actions, was approximately 5.5 hours. This duration was recorded using a machine equipped with an AMD Ryzen 5950x processor. The time investment was primarily due to the need to run full optimization routines for each step within every episode. This process is computationally intensive, especially when dealing with the complexities of non-parametric population pharmacokinetics workflow optimization. However, it's important to note that once the training is complete, the resulting Q matrix can be reused to fit the dataset multiple times with minimal additional effort. This reuse can significantly reduce the time required for subsequent analyses. Future improvements, such as implementing distributed computing, reducing convergence requirements for Maximum Likelihood methods, or transitioning to Deep Reinforcement Learning techniques, could further optimize the training process and reduce the overall execution time.",
  "model/availability": "The source code for the implementation described in this publication is publicly available. It can be found on our GitHub repository. The repository contains the raw data, logs, and the source code necessary to reproduce the results and run the algorithm. This allows other researchers to use, modify, and build upon our work. The specific URL for the repository is provided in the publication. The code is released under a permissive license, which allows for free use, modification, and distribution, subject to the terms of the license. This ensures that the community can benefit from and contribute to the development of this reinforcement learning approach for non-parametric population pharmacokinetics model building.",
  "evaluation/method": "The evaluation of the method involved multiple criteria, with the primary focus on the final likelihood obtained, as this was the metric used to encode the reward. The evaluation considered the final number of support points, the shape of the distribution, and technical elements such as the total amount of time spent in the training process. The method was tested over 1000 episodes, each consisting of up to 30 actions, resulting in a theoretical maximum of 30,000 calls to the NPOD algorithm. However, due to the exploration phase and the density of support points, not all episodes were completed, leading to fewer than 1000 episodes in the evaluation. The total execution time for the episodes was 5.5 hours using a machine with an AMD Ryzen 5950x processor. The final reward obtained by the agent matched the reward achieved by a pharmacometrician to the second decimal digit. The optimization routine resulted in 45 final support points and selected a model with two compartments over one with only one. The distribution of the parameters ke and V obtained by the RL agent was also analyzed and compared to the original simulated subjects and support points from a previous study.",
  "evaluation/measure": "The performance of the reinforcement learning (RL) agent was evaluated using several key metrics to ensure a comprehensive assessment. The primary metric reported is the final likelihood obtained, as this directly relates to the reward encoded in the RL algorithm. This metric is crucial because it indicates how well the model fits the data, which is a fundamental goal in pharmacokinetics and pharmacodynamics.\n\nIn addition to the final likelihood, the number of support points was also considered. This metric provides insight into the complexity and flexibility of the model, as a higher number of support points can indicate a more detailed and potentially more accurate model.\n\nThe shape of the distribution of the parameters was another important metric. This helps in understanding the stability and reliability of the parameter estimates, as a well-defined distribution suggests that the parameters are consistently estimated across different runs.\n\nTechnical elements such as the total amount of time spent in the training process were also taken into account. This is particularly relevant given the computational intensity of the RL approach, and it provides a practical measure of the efficiency of the method.\n\nThe execution time to finish the episodes was reported as 5.5 hours, using a machine with an AMD Ryzen 5950x. This gives a clear indication of the computational resources required for the training process.\n\nThe final reward obtained by the agent was compared to the reward obtained by a pharmacometrician, and it was found to be equal to the second decimal digit. This comparison highlights the effectiveness of the RL agent in achieving results comparable to human experts.\n\nThe optimization routine resulted in 45 final support points and selected a model with two compartments over a one-compartment model. This decision aligns with the goal of finding the most informative model, as indicated by the likelihood and other performance metrics.\n\nOverall, the set of metrics reported provides a thorough evaluation of the RL agent's performance. The focus on likelihood, number of support points, distribution shape, and execution time ensures that both the accuracy and efficiency of the method are assessed. This approach is representative of the literature in the field, where similar metrics are commonly used to evaluate the performance of pharmacometric models.",
  "evaluation/comparison": "Not applicable.",
  "evaluation/confidence": "The evaluation of the performance in this study primarily focuses on the final likelihood obtained, as this was the metric used to encode the reward. The results indicate that the final reward achieved by the reinforcement learning (RL) agent matched the reward obtained by a pharmacometrician to the second decimal digit. This suggests a high level of accuracy in the optimization process conducted by the RL agent.\n\nThe study also considers other technical elements such as the total amount of time spent in training, which was 5.5 hours using a machine with an AMD Ryzen 5950x. The optimization routine resulted in 45 final support points and selected a model with two compartments over one with only one. The distribution of key parameters (ke and V) obtained by the RL agent is visually represented in a figure, providing a clear comparison with the original simulated subjects and the support points obtained in a previous study.\n\nHowever, specific details about confidence intervals for the performance metrics or statistical significance tests comparing the RL method to other baselines are not provided. The evaluation emphasizes the practical performance and the ability of the RL agent to match the results of a human expert, but it does not delve into the statistical rigor required to claim superiority over other methods. Therefore, while the results are promising, the lack of detailed statistical analysis limits the confidence in asserting the method's superiority over other approaches.",
  "evaluation/availability": "The raw data, logs, and source code used in our evaluation are publicly available. They can be accessed through our GitHub repository. This includes all the necessary files to replicate the evaluation process and understand the results presented in our study. The repository is open to the public, allowing for transparency and reproducibility of our work."
}