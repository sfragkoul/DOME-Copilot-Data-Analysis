{
  "publication/title": "Predicting the bioactivity of 2-alkoxycarbonylallyl esters as potential antiproliferative agents against pancreatic cancer (MiaPaCa-2) cell lines: GFA-based QSAR and ELM-based models with molecular docking.",
  "publication/authors": "Oyeneyin OE, Obadawo BS, Olanrewaju AA, Owolabi TO, Gbadamosi FA, Ipinloju N, Modamori HO",
  "publication/journal": "Journal, genetic engineering & biotechnology",
  "publication/year": "2021",
  "publication/pmid": "33689046",
  "publication/pmcid": "PMC7947105",
  "publication/doi": "10.1186/s43141-021-00133-2",
  "publication/tags": "- 2-alkoxycarbonyl esters\n- Computer-aided drug design\n- Genetic function approximation\n- Extreme learning machine\n- Epidermal growth factor receptor\n- Molecular docking\n- Pancreatic cancer\n- Antiproliferative agents\n- MiaPaCa-2 cell lines\n- Quantitative structure-activity relationship",
  "dataset/provenance": "The dataset used in this study consists of a series of twenty-four compounds of 2-alkoxycarbonylallyl esters, which were collected from the literature. These compounds are known for their potential anticancer properties. The curative activities of these compounds against MIAPaCa-2 cancer cell lines were given in IC50 (\u03bcM) and were converted into their corresponding pIC50 values (i.e., - log IC50 = pIC50) to fit a range of values and to suit a normal distribution curve. The experimental activities, pIC50 values, and the structures of the compounds are presented in a table within the publication.\n\nThe 2D structures of the compounds were drawn using ChemDraw Ultra 12.0 and saved as cdx files. These files were then moved to the Spartan 14 software for optimization using molecular mechanics with the molecular mechanics force field (MMFF) to generate their most stable conformers. This was followed by a restricted hybrid Hartree-Fock\u2014density functional theory self-consistent field (HF-DFT SCF) calculation using Pulay\u2019s DIIS and geometric direct minimization with the 6-31G* basis set. The optimized molecular structures were saved as sdf files.\n\nThe optimized compounds were then subjected to PaDEL-Descriptor software version 2.20 to calculate the 1D, 2D, and 3D descriptors. After removing salts, detecting tautomers, and retaining the file names as molecule names, a total of 1875 descriptors were generated and saved as a Microsoft Excel Comma Separated Value (csv) file. These descriptors were used to develop the QSAR models for predicting the bioactivity of the molecules under investigation.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a testing set. The data was randomized before splitting to ensure an even distribution of data points and to improve computational efficiency. The ratio of the training set to the testing set was 4:1. This means that 80% of the data was used for training, while the remaining 20% was used for testing.\n\nThe training dataset was employed to generate weights, which were then validated using the testing set of data. This approach helps in assessing the model's performance and generalization capabilities.\n\nThe training set was also used for internal validation, while the testing set was used for external validation. The internal validation involved using parameters such as Friedman\u2019s lack of fit (LOF), the correlation coefficient (R2), adjusted R2, and the cross-validation coefficient (Q2cv) to evaluate the model's fitness and reliability. The external validation further confirmed the model's predictive accuracy using the testing set.\n\nIn addition to the training and testing sets, there was also a mention of an external validation set, which included specific compounds used to validate the built model. This external validation set was a subset of the testing set and was used to assess the model's performance on unseen data.",
  "dataset/redundancy": "The dataset was initially randomized to ensure an even distribution of data points, which improves computational efficiency. This randomization was crucial for the subsequent separation of the dataset into training and testing sets in a 4:1 ratio. The training dataset, comprising 80% of the data, was used to generate weights for the model, while the testing set, making up the remaining 20%, was employed for validation. This approach ensures that the training and test sets are independent, reducing the risk of data leakage and overfitting.\n\nThe randomization process helps in mitigating any potential biases that might arise from the order or grouping of the data. By ensuring that the data points are evenly distributed, the model's performance is more likely to generalize well to new, unseen data. This method of data splitting is consistent with best practices in machine learning, where the goal is to create robust models that can reliably predict outcomes on independent test sets.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets, which often use similar ratios for training and testing splits. This standard practice helps in maintaining consistency and comparability across different studies, allowing for a more objective evaluation of model performance. The use of a 4:1 split is particularly effective in scenarios where the dataset is sufficiently large, ensuring that the model has enough data to learn from while still having a meaningful test set to evaluate its performance.",
  "dataset/availability": "The data utilized in this study, including the specific splits used for training and testing, are not publicly released. The dataset consists of descriptors and experimental activities of investigated compounds, which were initially randomized and then separated into training and testing sets in a 4:1 ratio. This randomization process was implemented to ensure an even distribution of data points and to improve the computational efficiency of the algorithm. The training dataset was used to generate weights, which were subsequently validated using the testing set. However, the actual dataset and its splits are not made available in a public forum, and there is no specific license or enforcement mechanism mentioned for accessing this data.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is feed-forward neural networks, specifically the Extreme Learning Machine (ELM). ELM is characterized by its single hidden layer and the random determination of biases and weights connecting the hidden and input layers. This approach leads to fast convergence and high training speed, maintaining a simple structure that enhances computational efficiency and robustness.\n\nThe ELM algorithm is not new; it has been established in the field of machine learning. The choice to use ELM in this context is driven by its efficiency and effectiveness in handling the specific tasks of predicting bioactivity and molecular interactions. The focus of this publication is on applying ELM to the domain of quantitative structure-activity relationship (QSAR) studies and molecular docking, rather than on the development of new machine-learning algorithms. Therefore, it is published in a journal that aligns with the application domain, which is genetic engineering and biotechnology, rather than a machine-learning journal.",
  "optimization/meta": "The model developed in this study does not function as a meta-predictor. Instead, it relies on a single machine-learning approach, specifically the Extreme Learning Machine (ELM) method. This method was implemented using MATLAB, where the descriptors and experimental activities of the investigated compounds were randomized before being split into training and testing sets in a 4:1 ratio. This randomization ensures an even distribution of data points, enhancing computational efficiency.\n\nThe training dataset was used to generate weights, which were then validated using the testing set. The procedures for the computational implementation of the ELM-based models involved initial random number generation to control the biases and weights linking the hidden layers. The model's performance was evaluated using various statistical parameters, including the correlation coefficient (R2), adjusted R2, and cross-validation coefficient (Q2cv). These parameters confirmed the model's reliability and stability in predicting the activity of new compounds.\n\nThe descriptors used in the model were normalized and pre-treated to filter out redundant and highly correlated data, reducing collinearity and improving model performance. The dataset was divided using Kennard and Stone\u2019s algorithm, ensuring that the training and test sets were independent. The training set, comprising 70% of the data, was used to build and internally validate the model, while the test set, comprising 30% of the data, was used for external validation.\n\nIn summary, the model is based on a single ELM approach, with clear procedures for data randomization, normalization, and independent training and testing sets, ensuring robust and reliable predictions.",
  "optimization/encoding": "The data encoding process involved several steps to ensure the descriptors were suitable for the machine-learning algorithm. Initially, the 2D structures of the compounds were drawn using ChemDraw Ultra 12.0 and saved in cdx format. These structures were then optimized using Spartan 14 software with molecular mechanics and a restricted hybrid Hartree-Fock\u2014density functional theory self-consistent field calculation. This optimization generated the most stable conformers of the molecules, which were then saved in sdf format.\n\nFollowing optimization, the compounds were processed using PaDEL-Descriptor software version 2.20 to calculate 1D, 2D, and 3D descriptors. This software generated a total of 1875 descriptors, which were saved as a Microsoft Excel Comma Separated Value (csv) file. The descriptors were then normalized using the formula X = (X1 - Xmin) / (Xmax - Xmin), where X1 is the descriptor's value for each molecule, and Xmin and Xmax are the minimum and maximum values for each descriptor. This normalization step was crucial for filtering out redundant and highly correlated data, reducing collinearity, and improving the model's performance.\n\nThe pre-treated dataset was then split into training and test sets using Kennard and Stone\u2019s algorithm. The training set, comprising 70% of the data (16 compounds), was used to build and internally validate the model. The remaining 30% of the data (8 compounds) were used for external validation of the model. This division ensured that the model was trained on a representative subset of the data and validated on an independent set, enhancing the reliability and generalizability of the results.",
  "optimization/parameters": "In our study, the number of descriptors used in the model, denoted as p, varied depending on the specific model being developed. The descriptors were selected from a pool of candidates, and their number was optimized during the model development process. The selection of descriptors was guided by the need to filter out redundant data, highly correlated data, and reduce collinearity, thereby improving the performance of the model.\n\nThe descriptors used in the final model included ATSC3c, MATS5p, minHBint5, and ETA_Shape_P. These descriptors were chosen based on their ability to represent the physiochemical properties of the compounds under investigation. The specific number of descriptors used in the final model can be found in the detailed description of the model's internal validation parameters, where the number of descriptors is explicitly mentioned in the context of the Friedman\u2019s lack of fit (LOF) calculation.\n\nThe selection process involved normalizing the descriptors and pre-treating the data using specialized software. This ensured that the descriptors were appropriately scaled and that any redundant or highly correlated data was removed. The final set of descriptors was then used to build the model, with the biological activities (pIC50) serving as the dependent variable. The model's performance was evaluated using various statistical parameters, including the correlation coefficient (R2), adjusted R2, and the cross-validation coefficient (Q2cv), to ensure its reliability and stability.",
  "optimization/features": "The input features for the models were molecular descriptors. These descriptors were initially normalized and pre-treated to filter out redundant and highly correlated data, thereby reducing collinearity and improving model performance. The descriptors used in the final model included ATSC3c, MATS5p, minHBint5, and ETA_Shape_P. These descriptors were selected based on their significance in the model, as indicated by their mean effect values. Feature selection was performed using the training set only, ensuring that the model's performance on the test set was not influenced by the feature selection process. The number of features used as input in the final model is four.",
  "optimization/fitting": "The fitting method employed in this study utilized the Genetic Function Approximation (GFA) method within Material Studio 2017. This approach was chosen to build a robust model by fixing biological activities (pIC50) as the dependent variable and physiochemical properties (descriptors) as the independent variables.\n\nTo address the potential issue of over-fitting, several validation techniques were implemented. The dataset was divided into training and test sets using Kennard and Stone\u2019s algorithm, ensuring that 70% of the data was used for training and 30% for external validation. This division helps in assessing the model's generalizability to unseen data.\n\nInternal validation was performed using parameters such as Friedman\u2019s lack of fit (LOF), the correlation coefficient (R2), adjusted R2, and the cross-validation coefficient (Q2cv). The LOF score was used to evaluate the fitness of the models, with a lower standard error of estimation (SEE) indicating a better fit. The R2 value, which measures the proportion of variance explained by the model, was expected to be close to 1 for a good model. The adjusted R2 accounts for the number of descriptors, providing a measure of the model's reliability and stability. The Q2cv value was used to cross-validate the model's strength, ensuring that it performs well on both training and validation sets.\n\nExternal validation was conducted using the R2predicted value, which is a commonly used parameter to validate the model's predictive power. The R2test value was calculated to assess the model's performance on the test set, ensuring that it generalizes well to new data.\n\nAdditionally, the variance inflation factor (VIF) was used to measure multicollinearity among the descriptors. A VIF value between 1 and 5 was considered acceptable, ensuring that the descriptors did not correlate too strongly with each other, which could lead to over-fitting. The mean effect of each descriptor was also calculated to assess its significance in the model.\n\nIn summary, the fitting method involved rigorous validation techniques to rule out both over-fitting and under-fitting. The use of internal and external validation parameters, along with the assessment of multicollinearity, ensured that the model was robust and reliable.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our model. One key method involved the normalization and pretreatment of descriptors. This process filtered out redundant and highly correlated data, reducing collinearity and thereby improving the model's performance. By normalizing the descriptors, we ensured that each descriptor contributed equally to the model, preventing any single descriptor from dominating the predictions.\n\nAdditionally, we used the Friedman's lack of fit (LOF) metric to evaluate the fitness of our models. This metric helps in assessing how well the model generalizes to new data by penalizing models that fit the training data too closely, which is a common sign of overfitting. A lower LOF score indicates a better-fitting model that is less likely to overfit.\n\nThe cross-validation coefficient (Q2cv) was also utilized to validate the strength of our QSAR model. Cross-validation is a powerful technique for assessing the model's performance on different subsets of the data, providing a more reliable estimate of its generalization capability. A higher Q2cv value suggests that the model performs well on unseen data, reducing the risk of overfitting.\n\nFurthermore, the adjusted R-squared (R2adj) value was calculated to measure the reliability and stability of the model. Unlike the standard R-squared, which can increase with the addition of more descriptors, the adjusted R-squared accounts for the number of predictors in the model, providing a more accurate measure of the model's fit. This helps in ensuring that the model is not overly complex and is less likely to overfit.\n\nThe variance inflation factor (VIF) was used to measure the multicollinearity among the descriptors. A VIF value between 1 and 5 is considered acceptable, indicating that the descriptors are not too highly correlated with each other. This helps in maintaining the stability of the model and preventing overfitting due to redundant information.\n\nOverall, these techniques collectively contributed to the prevention of overfitting, ensuring that our model is robust, reliable, and capable of making accurate predictions on new data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in detail within the publication. Specifically, the process of selecting the optimum activation function and corresponding hidden nodes is described, including the range within which hidden nodes were optimized. The activation functions considered include the sine function, hardlim function, sigmoid function, and triangular basis function, among others. The hidden nodes were optimized within a search space of 1 to 100.\n\nThe computational implementation of the proposed ELM-based models was carried out using MATLAB. The steps involved in the optimization process, such as the initial random number generation, computation of the hidden layer output matrix, and output weight computation using the least square solution, are all detailed. The evaluation of the developed models was performed using a testing set of data, and the performance was assessed using four different performance measuring parameters: MAE, RMSE, CC, and MAPD. The models with the lowest error and highest correlation coefficient were saved as the best models, and their hyper-parameters and weights were also saved to ensure reproducibility.\n\nThe specific configurations and parameters used in the molecular docking and ADME/Tox screening are also provided. For instance, the 3D crystal structure of the epidermal growth factor receptor (EGFR) was downloaded from the protein data bank, and the receptor was prepared using the Protein Preparation Wizard. The lead molecules were prepared using ligand preparation in Schr\u00f6dinger Suite 2017-1 with an OPLS3 force field. The ionization state was generated using Epik 2.2 in Schr\u00f6dinger Suite at pH 7.0 \u00b1 2.0.\n\nRegarding the availability of model files and optimization parameters, the details provided in the publication are sufficient for reproducibility. The steps and parameters are clearly outlined, and the use of standard software and force fields ensures that the methods can be replicated by other researchers. However, specific model files or scripts are not directly provided in the text, but the information given is comprehensive enough to guide the replication of the study.",
  "model/interpretability": "The models developed in this study, particularly the ELM-based models, are not entirely transparent and can be considered somewhat black-box in nature. These models, including the ELM-Sine and ELM-Sig models, utilize complex neural network architectures that do not readily provide interpretable insights into how specific descriptors influence the predicted activity.\n\nHowever, some level of interpretability can be inferred through statistical analyses and validation metrics. For instance, the mean effect of descriptors on the activity (pIC50) of the compounds was calculated, revealing the significance of each descriptor. The order of decreasing effect was found to be ETA_Shape_P > ATSC3c > MATS5p > minHBint5. This indicates that ETA_Shape_P has the most significant impact on the activity, followed by ATSC3c, MATS5p, and minHBint5.\n\nAdditionally, the variance inflation factor (VIF) was used to measure multicollinearity among descriptors, ensuring that there is no excessive intercorrelation that could bias the predictions. The low correlation values (\u2264 0.5) among most descriptors suggest that the model's predictions are not biased by redundant information.\n\nThe Pearson's correlation coefficients for the descriptors also support the model's reliability, as they indicate that the descriptors do not correlate strongly with each other, thereby reducing the risk of overfitting.\n\nFurthermore, the high values of the correlation coefficient (R2) for both the training set (0.9929) and the test set (0.8397) confirm that the model can successfully predict the activity of new compounds. This correlation with experimental activity provides some level of interpretability, as it shows that the model's predictions are aligned with observed data.\n\nIn summary, while the ELM-based models are not fully transparent, various statistical analyses and validation metrics offer insights into the model's behavior and the significance of different descriptors. This partial interpretability helps in understanding the model's predictions and its reliability in predicting the activity of new compounds.",
  "model/output": "The model developed is a regression model. It predicts the activity of compounds, specifically the pIC50 value, which is a measure of the potency of a substance in inhibiting a specific biological or biochemical function. The model uses various molecular descriptors to make these predictions. The performance of the model is evaluated using metrics such as Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Correlation Coefficient (CC), and Mean Absolute Percentage Deviation (MAPD). These metrics indicate how well the predicted activities align with the experimental activities, confirming the model's effectiveness in regression tasks. The model's output weights are computed using the least square solution, further emphasizing its regression nature. Additionally, the model's performance is assessed on both training and testing datasets, with a focus on minimizing errors and maximizing correlation, which are typical goals in regression modeling.",
  "model/duration": "The execution time for the model was not explicitly mentioned. However, the computational efficiency was improved through the randomization of descriptors and experimental activities, which enhanced the even distribution of data points. This step likely contributed to reducing the overall execution time. The model was implemented in MATLAB, and the procedures involved several steps, including random number generation, optimization of activation functions and hidden nodes, computation of hidden layer output matrix, and output weight computation. These steps were followed by the evaluation of the developed models using a testing set of data. The performance of the model was assessed using metrics such as MAE, RMSE, CC, and MAPD, which helped in identifying the best models characterized by the lowest error and highest correlation coefficient. The hyper-parameters and weights of the best model were saved to ensure reproducibility of the results.",
  "model/availability": "The source code for the models developed in this study is not publicly released. The modeling and simulation were implemented using MATLAB, but the specific scripts and algorithms used are not available for public access.\n\nThe computational details involve the use of several software tools and methods. For instance, the descriptors were normalized and pre-treated using the Data Pre-treatment software. The dataset was split into training and test sets using Kennard and Stone\u2019s algorithm. The model was built using Material Studio 2017 software, employing the Genetic Function Approximation (GFA) method. Additionally, the Extreme Learning Machine (ELM)-based models were developed and evaluated using MATLAB.\n\nFor molecular docking and ADME/Tox screening, the Protein Preparation Wizard and LigPrep in Schr\u00f6dinger Suite 2017-1 were used. The 3D crystal structure of the epidermal growth factor receptor (EGFR) was downloaded from the Protein Data Bank, and the ligands were prepared using LigPrep with an OPLS3 force field. Epik 2.2 in Schr\u00f6dinger Suite was used to generate the ionization state of the ligands.\n\nWhile the specific software and tools used are mentioned, the actual code or executable files are not provided. Therefore, researchers interested in replicating or building upon this work would need to use similar software and methods described in the study.",
  "evaluation/method": "The evaluation of the method involved several steps to ensure the robustness and reliability of the model. Initially, the dataset was divided into training and test sets using Kennard and Stone\u2019s algorithm, with 70% of the data used for training and 30% for testing. This division helps in assessing the model's performance on unseen data.\n\nFor internal validation, the training set was used to build the model, and various parameters were evaluated. The correlation coefficient (R2) was calculated to measure the goodness of fit, with values close to 1 indicating a strong model. Additionally, the adjusted R2 was considered to account for the number of descriptors, providing a measure of the model's reliability and stability. The cross-validation coefficient (Q2cv) was also computed to assess the model's predictive power through cross-validation techniques.\n\nExternal validation was performed using the test set, with the R2predicted value being a key parameter. This value indicates how well the model generalizes to new, unseen data. A high R2predicted value close to 1 suggests that the model is stable and reliable for predicting the activity of new compounds.\n\nFurthermore, the model's performance was evaluated using additional statistical analyses such as the variance inflation factor (VIF) and mean effect (ME). These analyses help in understanding the influence of each descriptor on the model's predictions and ensure that there is no bias or multicollinearity among the descriptors.\n\nThe experimental activities were plotted against the predicted activities for both the training and test sets, confirming the model's ability to accurately predict the activity of new compounds. The high correlation coefficients for both sets further validate the model's predictive power and stability.",
  "evaluation/measure": "The performance of the developed models was assessed using four different metrics: Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Correlation Coefficient (CC), and Mean Absolute Percentage Deviation (MAPD). These metrics provide a comprehensive evaluation of the models' predictive accuracy and reliability.\n\nMAE measures the average magnitude of the errors in a set of predictions, without considering their direction. It gives an idea of how wrong the predictions are on average. RMSE, on the other hand, measures the square root of the average of squared differences between predicted and observed values. It gives more weight to larger errors, making it a good measure of accuracy when large errors are particularly undesirable.\n\nCC indicates the strength and direction of a linear relationship between predicted and experimental values. A CC value closer to 1 indicates a stronger linear relationship. MAPD measures the accuracy of predictions as a percentage of the actual values, providing an intuitive understanding of the prediction error relative to the size of the values being predicted.\n\nThe use of these four metrics ensures a thorough evaluation of the models' performance. MAE and RMSE provide insights into the magnitude of prediction errors, while CC assesses the correlation between predicted and experimental values. MAPD offers a percentage-based measure of prediction accuracy, making it easier to interpret the practical significance of the errors.\n\nThese metrics are widely used in the literature for evaluating predictive models, making our evaluation representative and comparable to other studies in the field. The combination of these metrics allows for a robust assessment of the models' performance, ensuring that they are reliable and accurate for predicting the activity of new compounds.",
  "evaluation/comparison": "In our study, we compared the performance of our developed models with other established methods to ensure their robustness and accuracy. Specifically, we evaluated the ELM-based models against a QSAR-based model. The ELM-Sine model demonstrated superior performance with improvements of 10.41%, 19.30%, 0.33%, and 11.93% in terms of MAE, RMSE, CC, and MAPD, respectively. This comparison highlights the intrinsic capability of the ELM algorithm to approximate non-linear and complex relationships between descriptors and the target variable.\n\nAdditionally, we validated our models using external datasets to assess their generalizability. The ELM-Sig model, in particular, showed persistent closeness with the measured values, indicating its reliability and effectiveness in predicting bioactivity. The superiority of the ELM-based models over the QSAR model can be attributed to the ELM algorithm's ability to handle complex data relationships efficiently.\n\nWe also performed molecular docking studies to compare the lead compounds with a standard drug, chlorambucil. The lead compounds showed better docking scores and inhibitory potentials, further validating the predictive power of our models. The ADME/Tox properties of these compounds were also evaluated, confirming their suitability as potential drug candidates according to Lipinski's rule of five.\n\nIn summary, our models were rigorously compared against established methods and simpler baselines, demonstrating their superior performance and reliability in predicting the potential activity of 2-alkoxycarbonylallyl esters as anticancer agents against MIAPaCa-2 pancreatic cancer cell lines.",
  "evaluation/confidence": "The evaluation confidence of the models presented in this work is supported by several statistical analyses and validation parameters. The performance metrics, such as MAE, RMSE, CC, and MAPD, were used to compare the developed models. The ELM-Sig model demonstrated superior performance with significant improvements over the ELM-Sine and QSAR models. For instance, the ELM-Sig model showed a performance improvement of 4.09% and 14.92% in MAE compared to ELM-Sine and QSAR models, respectively. Similarly, it exhibited enhancements of 9.53% and 30.68% in RMSE.\n\nThe statistical significance of these results is further validated by the low values of residual activity, indicating a high correlation between experimental and predicted activities. The R2 value for the predicted set (R2_pred) was 0.7560, which meets the minimum recommended value for validating a built model. Additionally, the high correlation coefficients (R2) for the training set (0.9929) and test set (0.8397) confirm the model's ability to predict the activity of new compounds accurately.\n\nThe applicability domain of the model was evaluated using the leverage approach, which identified only one influential compound with a leverage value greater than the warning value. This suggests that the model is robust and not overly influenced by outliers.\n\nFurthermore, the mean effect of the descriptors and the variance inflation factor (VIF) were analyzed to ensure that the descriptors do not correlate with one another, thereby avoiding bias in the predictions. The low correlation values (\u2264 0.5) among most descriptors support the model's reliability.\n\nIn summary, the performance metrics are statistically significant, and the model's superiority over other methods is well-supported by rigorous validation parameters and statistical analyses.",
  "evaluation/availability": "The evaluation process for the models developed in this study involved several key steps and metrics to ensure robustness and reliability. The models were evaluated using performance measuring parameters such as Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Correlation Coefficient (CC), and Mean Absolute Percentage Deviation (MAPD). The best models were selected based on the lowest error rates and highest correlation coefficients.\n\nThe raw evaluation files, including the experimental and predicted activities, as well as the residuals, are not explicitly mentioned as being publicly available. Therefore, it is not clear if these files can be accessed by the broader scientific community. The study does not provide specific details on where or how these evaluation files might be released, nor does it mention any licensing information related to their availability.\n\nFor those interested in replicating or building upon the findings, the hyper-parameters of the best models and the weights were saved to ensure reproducibility of the results. This suggests a commitment to transparency and the potential for others to verify the outcomes. However, without explicit information on the availability of the raw evaluation files, it is challenging to provide a definitive answer on their accessibility."
}