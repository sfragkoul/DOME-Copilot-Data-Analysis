{
  "publication/title": "Artificial intelligence improves the accuracy of residents in the diagnosis of hip fractures: a multicenter study.",
  "publication/authors": "Sato Y, Takegami Y, Asamoto T, Ono Y, Hidetoshi T, Goto R, Kitamura A, Honda S",
  "publication/journal": "BMC musculoskeletal disorders",
  "publication/year": "2021",
  "publication/pmid": "33941145",
  "publication/pmcid": "PMC8091525",
  "publication/doi": "10.1186/s12891-021-04260-2",
  "publication/tags": "- Artificial Intelligence\n- Deep Learning\n- Hip Fractures\n- Medical Imaging\n- Diagnostic Accuracy\n- Computer-Aided Diagnosis\n- Orthopedics\n- Machine Learning\n- Radiology\n- Clinical Diagnosis",
  "dataset/provenance": "The dataset used in this study was sourced from three hospitals located in Aichi Prefecture, Japan: Gamagori City Hospital, Tsushima City Hospital, and Nagoya Daini Red Cross Hospital. These hospitals provided a diverse range of care settings, including both urban and rural areas.\n\nThe dataset consisted of 5295 cases of femoral neck fractures or femoral trochanteric fractures, diagnosed using plain X-rays, CT, or MRI between 2009 and 2019. Patients aged 20 years and older were included. The dataset also included cases with various complications such as hip implants on the opposite side, complicated pubic or sciatic fractures, osteoarthritis of the hip, spine implants, and pathologic fractures due to metastatic cancer.\n\nAfter excluding certain cases, such as periprosthetic fractures and images where bilateral hips were not included within the image range, a total of 5242 AP pelvic X-rays from 4851 cases were included. These images were further divided into two sets per image, resulting in 5242 images containing the fracture site and 5242 images without the fracture site. This process yielded a total of 10,484 images used for machine learning.\n\nThe dataset was divided into three sets: a training dataset, a validation dataset, and a test dataset. The training dataset consisted of 8484 images, the validation dataset had 1000 images, and the test dataset also had 1000 images. This division ensured that the model was trained, validated, and tested on separate sets of data, enhancing the robustness and generalizability of the results.",
  "dataset/splits": "The dataset was divided into three distinct splits: a training dataset, a validation dataset, and a test dataset. Each split contained a balanced number of images from both non-fractured and fractured sides.\n\nThe training dataset comprised 8,484 images, with an equal number of non-fractured and fractured side images, totaling 4,242 images for each category. The validation dataset consisted of 1,000 images, evenly split between non-fractured and fractured sides, with 500 images in each category. Similarly, the test dataset also contained 1,000 images, with 500 images from non-fractured sides and 500 images from fractured sides.\n\nThese splits were created to ensure that the deep learning algorithm could be trained, validated, and tested effectively, providing a robust evaluation of its performance in diagnosing hip fractures.",
  "dataset/redundancy": "The dataset used in this study was divided into three distinct sets: a training dataset, a validation dataset, and a test dataset. The training dataset consisted of 8,484 images, with an equal number of non-fracture and fracture side images (4,242 each). The validation and test datasets each contained 1,000 images, again with an equal split between non-fracture and fracture side images (500 each).\n\nTo ensure independence between the training and test sets, the images were randomly divided. This random division helped to prevent any overlap between the datasets, ensuring that the model's performance could be accurately evaluated on unseen data. The images were extracted from a larger pool of 10,484 images, which were generated from approximately 5,000 cases. This large and diverse dataset is a key strength, as it includes images from multiple institutions with varying imaging formats and conditions.\n\nThe distribution of our dataset is notably larger and more diverse compared to many previously published machine learning datasets for hip fracture diagnosis. Most studies have used data from a single institution, limiting the variability in imaging conditions and patient demographics. In contrast, our multi-center dataset includes images from different radiographic equipment and file formats, enhancing the generalizability of our findings. This approach aligns with the understanding that large and diverse datasets are crucial for the success of machine learning algorithms, particularly in medical imaging.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used is a deep convolutional neural network. Specifically, the EfficientNet-B4 model, which is a pre-trained ImageNet model, was employed. This model is not new; it has been previously developed and is widely recognized in the field of machine learning.\n\nThe choice to use this established model was driven by its proven effectiveness in image classification tasks. The EfficientNet-B4 model was selected for its balance of accuracy and efficiency, making it suitable for the complex task of diagnosing hip fractures from X-ray images.\n\nThe decision to use a well-known model rather than developing a new one was strategic. The focus of this study was on applying advanced machine learning techniques to improve diagnostic accuracy in medical imaging, rather than innovating in the field of machine learning algorithms themselves. By leveraging an existing, robust model, the research could concentrate on the specific application and its clinical implications.\n\nThe EfficientNet-B4 model was implemented using the PyTorch 1.3 and Fast.ai 1.0 frameworks, which provided the necessary tools and libraries to train and evaluate the model effectively. This approach ensured that the study could benefit from the latest advancements in deep learning while maintaining a clear focus on its primary objective: enhancing the diagnostic capabilities of less experienced clinicians.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding process involved converting images extracted from the DICOM server into 3-channel, 8-bit JPEG images. These images were then resized to 380 \u00d7 380 pixels. The window level was not adjusted for any of the images. Each image was annotated with a rectangle that encompassed the entire fracture site. To generate non-fractured side data, a vertical dividing line was placed with a 50-pixel margin from the rectangle, and the images without the rectangle were used. This process resulted in 5242 images that did not contain the fracture site. Similarly, the images containing the rectangle of the same size were used as the fracture side data, yielding another 5242 images with the fracture site. In total, 10,484 images were prepared for machine learning. These images were then randomly divided into three datasets: a training dataset with 8484 images, a validation dataset with 1000 images, and a test dataset with 1000 images. This division ensured a balanced representation of both fractured and non-fractured sides across all datasets.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "The input features for the deep learning algorithm consisted of X-ray images of the hip, specifically focusing on the femur. The dataset included 5242 AP pelvic X-rays from 4851 cases, which were divided into two images per case, resulting in a total of 10,484 images used for machine learning. These images were preprocessed by converting them into 3-channel, 8-bit JPEG format and resizing them to 380 \u00d7 380 pixels. The images were not adjusted for window level, and each image was annotated with a rectangle that included the entire fracture site.\n\nFeature selection was not explicitly mentioned as a separate step in the process. Instead, the images were directly used as input features for the deep learning model. The dataset was divided into training, validation, and test sets, ensuring that the model's performance was evaluated on unseen data. The training dataset consisted of 8484 images, the validation dataset had 1000 images, and the test dataset also had 1000 images. This division ensured that the model's performance was assessed on data that was not used during the training phase, providing a robust evaluation of its generalization capabilities.",
  "optimization/fitting": "The deep learning model employed in this study utilized an EfficientNet-B4 architecture, which is a pre-trained ImageNet model. This model is known for its efficiency in balancing network depth, width, and resolution, allowing it to scale effectively with a large number of parameters relative to the training dataset.\n\nThe dataset consisted of 10,484 images, which were derived from 5,242 AP pelvic X-rays. These images were split into training, validation, and test sets, ensuring a robust evaluation of the model's performance. The training set included 8,484 images, the validation set 1,000 images, and the test set another 1,000 images. This division helped in mitigating overfitting by providing a separate validation set to tune hyperparameters and a test set to evaluate the final model performance.\n\nTo further address overfitting, techniques such as data augmentation and dropout were likely employed, although not explicitly detailed. Data augmentation involves creating modified versions of the training images to increase the diversity of the training set without actually collecting new data. Dropout is a regularization technique where randomly selected neurons are ignored during training, which helps prevent the model from becoming too reliant on specific patterns in the training data.\n\nThe model's performance metrics, including accuracy, sensitivity, specificity, and the area under the curve (AUC), were evaluated on the test set. The high performance on the test set, with an accuracy of 96.1%, sensitivity of 95.2%, specificity of 96.9%, and an AUC of 0.99, indicates that the model generalizes well to unseen data, suggesting that overfitting was effectively managed.\n\nUnderfitting was addressed by using a pre-trained model, EfficientNet-B4, which has been trained on a large and diverse dataset (ImageNet). This pre-training provides the model with a good starting point, allowing it to learn more complex features from the specific dataset used in this study. Additionally, the use of a deep convolutional neural network architecture ensures that the model has the capacity to learn intricate patterns in the data, reducing the risk of underfitting.\n\nIn summary, the model's architecture, the use of a pre-trained network, and the division of the dataset into training, validation, and test sets all contributed to effectively managing both overfitting and underfitting. The high performance metrics on the test set further validate the model's robustness and generalizability.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we utilized an EfficientNet-B4 model, which is a pre-trained ImageNet model, implemented using the PyTorch 1.3 and Fast.ai 1.0 frameworks. The model was trained on a large dataset consisting of 10,484 images, which were derived from 5,242 AP pelvic X-rays. These images were split into training, validation, and test datasets to ensure robust evaluation.\n\nThe optimization parameters, including learning rates, batch sizes, and other relevant hyper-parameters, are not explicitly listed in the main text but are implied through the use of standard practices and the frameworks mentioned. The performance metrics, such as accuracy, sensitivity, specificity, F-value, and AUC, are thoroughly reported, providing a clear picture of the model's effectiveness.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly provided in the publication. However, the methods and frameworks used are well-documented and widely available, allowing for reproducibility by researchers familiar with deep learning techniques. The publication is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction, provided appropriate credit is given to the original authors.\n\nFor those interested in accessing the specific model files or detailed optimization parameters, it would be advisable to contact the corresponding author or refer to supplementary materials that may be available through the publishing platform.",
  "model/interpretability": "The model employed in this study is based on a deep learning algorithm, which is often associated with the \"black box problem.\" This means that the model cannot explicitly express the feature quantities it uses for decision-making, making the reasons for its judgments unclear to humans. However, to address this issue, we utilized Gradient-weighted Class Activation Mapping (Grad-CAM). This technique allows for the visualization of class-discriminative regions on X-rays, revealing the location of the diagnosis. For instance, when the model identifies a fracture, Grad-CAM highlights the relevant area on the X-ray, providing a visual explanation of the model's decision. This helps in understanding where the model recognizes the fracture, although it may only show a rough area rather than the exact fracture line. The heat maps generated by Grad-CAM were consistent with the fracture sites indicated by orthopedic surgeons, demonstrating the model's ability to focus on relevant regions. This approach enhances the interpretability of the model, making it more transparent and trustworthy for clinical use.",
  "model/output": "The model developed is a classification model. It is designed to diagnose the presence or absence of hip fractures in X-ray images. The model uses a deep convolutional neural network approach, specifically an EfficientNet-B4 model, which is a pre-trained ImageNet model. The output of the model is a binary classification indicating whether a fracture is present or not. Additionally, the model provides a heat map using Gradient-weighted Class Activation Mapping (Grad-CAM) to visualize the areas of the X-ray image that the model focuses on for its diagnosis. This heat map helps in understanding the basis for the model's classification decision. The performance metrics evaluated include accuracy, sensitivity, specificity, F-value, and the area under the curve (AUC), all of which are typical for classification tasks.",
  "model/duration": "The average inference time per image, including the generation of heat maps, was approximately 1.17 seconds. This calculation was derived from the average time per image of the test dataset divided by the total calculation time for 1000 images. This efficiency is crucial for practical clinical application, ensuring that the system can provide rapid and reliable assistance to clinicians.",
  "model/availability": "The source code for the deep learning algorithm developed in this study is not publicly released. However, the algorithm was implemented using open-source frameworks, specifically Pytorch 1.3 and Fast.ai 1.0. The model used is an EfficientNet-B4, which is a pre-trained ImageNet model.\n\nThe algorithm was run using a Digital Imaging and Communications in Medicine (DICOM) image display software program from Toshiba Medical Systems Corporation. This software was used as the image reference software program for the study.\n\nThe algorithm was tested and validated using a controlled experiment with clinicians, where 31 residents diagnosed 300 hip fracture images with and without the aid of the CAD system. The results showed a significant improvement in diagnostic accuracy when the CAD system was used.\n\nThe performance of the deep learning algorithm was evaluated using metrics such as accuracy, sensitivity, specificity, F-value, and area under the curve (AUC). The algorithm achieved high performance in these metrics, indicating its effectiveness in diagnosing hip fractures.\n\nThe study also used gradient-weighted class activation mapping (Grad-CAM) to visualize the basis for the deep learning algorithm's diagnosis of a fracture. This visualization provided evidence about where the AI recognized the fracture, addressing the \"black box problem\" associated with AI-based diagnostics.",
  "evaluation/method": "The evaluation method for our study involved several key steps to assess the performance and effectiveness of the computer-aided diagnosis (CAD) system for hip fractures. Initially, we evaluated the deep learning algorithm using a test image dataset, calculating metrics such as accuracy, sensitivity, specificity, F-value, and the area under the receiver operating characteristic (ROC) curve (AUC). These metrics were chosen to provide a comprehensive understanding of the algorithm's performance.\n\nTo validate the heatmap generated by the CAD system, we used Gradient-weighted Class Activation Mapping (Grad-CAM). We selected 40 images, 20 with fractures and 20 without, from the test dataset that the algorithm correctly diagnosed. The assessor evaluated the consistency between the high signal intensity regions on the heatmap and the actual fracture sites on the X-rays, using sensitivity and specificity as measures. The intra-observer correlation for this evaluation was found to be perfect, with a kappa value of 1.0.\n\nIn addition to these technical evaluations, we conducted a controlled experiment with clinicians to assess the practical impact of the CAD system. Thirty-one residents, young doctors within two years of graduation, participated in the study. They were presented with 300 randomly selected hip fracture images and asked to diagnose the presence or absence of fractures both with and without the aid of the CAD system. The sequence involved the clinicians first diagnosing the images independently, then receiving visual hints from the CAD system, and finally diagnosing the images again based on these hints. This process was repeated for all 300 images.\n\nThe diagnostic accuracy of the residents was compared with and without the use of the CAD system. We analyzed the accuracy, sensitivity, and specificity for both first-year and second-year residents. Statistical analyses were performed using the EZR software program, with Fisher's exact test for categorical variables and the Wilcoxon signed-rank test for non-normally distributed diagnostic accuracy data. A p-value of less than 0.05 was considered statistically significant. This comprehensive evaluation method ensured that both the technical performance of the CAD system and its practical utility in a clinical setting were thoroughly assessed.",
  "evaluation/measure": "In our study, we evaluated the performance of our deep learning algorithm using a comprehensive set of metrics to ensure a thorough assessment. The primary metrics reported include accuracy, sensitivity, specificity, and the F-value. These metrics provide a well-rounded view of the algorithm's performance by considering true positives, true negatives, false positives, and false negatives.\n\nAccuracy measures the overall correctness of the algorithm, indicating the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, also known as recall, assesses the algorithm's ability to identify positive cases correctly, which is crucial for detecting fractures. Specificity evaluates the algorithm's capability to correctly identify negative cases, ensuring that non-fracture images are accurately classified.\n\nThe F-value, or F1 score, is the harmonic mean of precision and recall, providing a single metric that balances both concerns. This is particularly useful when dealing with imbalanced datasets, as it gives a more nuanced view of the algorithm's performance beyond simple accuracy.\n\nAdditionally, we calculated the receiver operating characteristic (ROC) curve and measured the area under the curve (AUC). The ROC curve plots the true positive rate against the false positive rate at various threshold settings, providing a visual representation of the algorithm's diagnostic ability. The AUC quantifies the overall ability of the algorithm to discriminate between positive and negative cases, with a value of 1 indicating perfect discrimination.\n\nThese metrics are widely used in the literature and are considered representative of the standard evaluation practices in the field. By including accuracy, sensitivity, specificity, the F-value, and the AUC, we ensure that our evaluation is comprehensive and comparable to other studies in the domain. This approach allows for a robust assessment of the algorithm's performance and its potential clinical utility.",
  "evaluation/comparison": "Not applicable. The study focused on evaluating the performance of a deep learning algorithm for hip fracture detection and the effectiveness of a CAD system in a clinical setting. It did not involve comparisons to publicly available methods or simpler baselines on benchmark datasets. Instead, the evaluation centered on the accuracy, sensitivity, specificity, and other metrics of the developed algorithm and its impact on clinicians' diagnostic accuracy. The study also compared the diagnostic performance of residents with and without the aid of the CAD system, highlighting improvements in accuracy, sensitivity, and specificity when using the CAD system. Additionally, the study conducted a literature review to contextualize the performance of the developed CAD system against other AI-based systems for hip fracture diagnosis.",
  "evaluation/confidence": "The evaluation of the deep learning algorithm included several performance metrics, all of which have confidence intervals. The accuracy was reported as 96.1% with a 95% confidence interval (CI) of 94.9 to 97.3. Sensitivity was 95.2% (95% CI: 93.9, 96.5), and specificity was 96.9% (95% CI: 95.8, 98.0). The F-value was 0.961 (95% CI: 0.950, 0.972). The area under the curve (AUC) for the receiver operating characteristic (ROC) curve was 0.99 (95% CI: 0.98, 1.00).\n\nStatistical significance was assessed using various tests. Fisher\u2019s exact test was used for categorical variables, and the Shapiro-Wilk test was used to check the normality of the distribution of diagnostic accuracy. Since the data did not show a normal distribution, the Wilcoxon signed-rank test was employed. P-values of less than 0.05 were considered statistically significant.\n\nThe diagnostic accuracy of clinicians with and without the aid of the computer-aided diagnosis (CAD) system was also evaluated. The residents' mean diagnostic accuracy significantly improved with the aid of the CAD system. The accuracy increased from 84.7% (95% CI: 82.2, 87.2) without aid to 91.2% (95% CI: 89.6, 92.8) with aid, with a p-value of less than 0.01. Sensitivity improved from 83.4% (95% CI: 83.4, 90.6) to 90.6% (95% CI: 83.9, 97.3), and specificity increased from 88.7% (95% CI: 82.1, 95.3) to 93.4% (95% CI: 89.5, 97.3), both with p-values of less than 0.01.\n\nThese results indicate that the deep learning algorithm and the CAD system provide statistically significant improvements in diagnostic accuracy, sensitivity, and specificity. The confidence intervals and p-values support the claim that the method is superior to baseline performance.",
  "evaluation/availability": "Not enough information is available."
}