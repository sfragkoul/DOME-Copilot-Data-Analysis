{
  "publication/title": "Uncovering hub genes and immunological characteristics for heart failure utilizing RRA, WGCNA and Machine learning.",
  "publication/authors": "Tu D, Xu Q, Zuo X, Ma C",
  "publication/journal": "International journal of cardiology. Heart & vasculature",
  "publication/year": "2024",
  "publication/pmid": "38371312",
  "publication/pmcid": "PMC10869931",
  "publication/doi": "10.1016/j.ijcha.2024.101335",
  "publication/tags": "- Heart failure\n- Robust rank aggregation\n- Weighted gene co-expression network analysis\n- Machine learning\n- Immune infiltration\n- Diagnostic markers\n- Gene expression\n- Bioinformatics\n- Cardiovascular diseases\n- Gene enrichment analysis",
  "dataset/provenance": "The datasets utilized in this study were obtained from the NCBI Gene Expression Omnibus (GEO) database. Specifically, six datasets were included based on certain eligibility criteria: GSE16499, GSE26887, GSE42955, GSE57338, GSE76701, and GSE79962. These datasets consist of microarray data from heart samples of heart failure (HF) patients and non-failing donors (NFDs), with a minimum of three samples investigated in each dataset. The series matrix files and corresponding platform files were downloaded as CSV files from GEO.\n\nAdditionally, GSE116250 was selected as the external validation dataset. This dataset includes 50 HF patients (13 with ischemic cardiomyopathy and 37 with dilated cardiomyopathy) and 14 NFDs.\n\nThe final merged dataset, after correcting for batch effects, comprised 259 samples, including 124 HF patients (64 with ischemic cardiomyopathy and 60 with dilated cardiomyopathy) and 135 NFDs. This comprehensive collection of datasets allowed for a robust analysis of gene expression patterns associated with heart failure.",
  "dataset/splits": "In our study, we utilized multiple datasets to ensure the robustness and generalizability of our findings. We included six datasets from the NCBI Gene Expression Omnibus (GEO) database for our primary analysis. These datasets were filtered based on specific eligibility criteria, including the organism being Homo sapiens, the type of experiments performed, and the minimum number of samples investigated. The datasets included were GSE16499, GSE26887, GSE42955, GSE57338, GSE76701, and GSE79962. These datasets collectively contained heart samples from heart failure (HF) patients and non-failing donors (NFDs), with a minimum of three samples per dataset.\n\nAdditionally, we used an external validation dataset, GSE116250, which included 50 HF patients (13 with ischemic cardiomyopathy and 37 with dilated cardiomyopathy) and 14 NFDs. This external dataset was crucial for validating the diagnostic model we developed.\n\nThe final merged dataset, after correcting for batch effects, consisted of 259 samples. This merged dataset included 124 HF patients (64 with ischemic cardiomyopathy and 60 with dilated cardiomyopathy) and 135 NFDs. This comprehensive approach allowed us to identify hub genes associated with HF and construct a reliable diagnostic model.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The datasets used in this study were obtained from the NCBI Gene Expression Omnibus (GEO) database, which is a public forum. The specific datasets included in our study are GSE16499, GSE26887, GSE42955, GSE57338, GSE76701, and GSE79962. Additionally, GSE116250 was selected as the external validation dataset. These datasets are publicly available and can be accessed via the GEO database using the provided accession numbers. The data includes expression profiling by array performed with heart samples from heart failure patients and non-failing donors, with a minimum of three samples investigated in each dataset.\n\nThe series matrix files of these datasets and their corresponding platform files were downloaded as CSV files from GEO. The datasets were filtered according to specific eligibility criteria, including the organism being set as \"Homo sapiens\" and the experiments being performed with heart samples. The data is available under the terms and conditions specified by the GEO database, which typically includes proper citation and acknowledgment of the original authors and the database.\n\nTo ensure the integrity and reproducibility of our analysis, we followed standard procedures for data processing and normalization. This included column normalization and log-transformation using the Bioconductor package limma. The datasets were then integrated using the Robust Rank Aggregation (RRA) method to identify the most significant differentially expressed genes. The final merged dataset consisted of 259 samples, including 124 heart failure patients and 135 non-failing donors. This dataset was used for further analysis, including weighted gene co-expression network analysis (WGCNA) and machine learning methods to identify diagnostic markers and construct a diagnostic model for heart failure.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are primarily regularization techniques and feature selection methods. These include LASSO (Least Absolute Shrinkage and Selection Operator) regression, ridge regression, elastic net regression, and support vector machine-recursive feature elimination (SVM-RFE). These algorithms are well-established in the field of machine learning and are commonly used for feature selection and model optimization.\n\nThe algorithms employed are not new; they have been extensively used in various bioinformatics and machine learning applications. The choice of these algorithms is driven by their effectiveness in identifying critical diagnostic markers from high-dimensional data, which is a common challenge in biomedical research.\n\nThe focus of this study is on the application of these machine-learning techniques to identify diagnostic markers for heart failure, rather than the development of new algorithms. Therefore, the publication is more aligned with the field of biomedical research and heart failure studies, where the practical application and validation of these methods are of primary interest. The algorithms were selected for their robustness and ability to handle the complexities of biological data, ensuring that the diagnostic markers identified are reliable and clinically relevant.",
  "optimization/meta": "The model employs a meta-predictor approach, integrating multiple machine-learning algorithms to identify diagnostic markers for heart failure (HF). The meta-predictor combines outputs from several feature selection methods, including best subset regression, LASSO regression, ridge regression, elastic net regression, and support vector machine-recursive feature elimination (SVM-RFE).\n\nBest subset regression identifies the optimal feature subset based on classification performance. Regularization techniques such as LASSO, ridge, and elastic net regression are used to detect genes that significantly contribute to HF diagnosis. These models are evaluated using root mean squared error (RMSE) in an internal validation dataset. SVM-RFE further refines the selection by ranking genes through recursive feature elimination.\n\nThe intersection of the best-performing models from these methods\u2014best subset regression, LASSO regression, and SVM-RFE\u2014determines the final diagnostic markers. This approach ensures that the selected markers are robust and reliable, leveraging the strengths of multiple machine-learning techniques.\n\nRegarding the independence of training data, the final merged dataset is divided into training and internal validation datasets at a 7:3 ratio, ensuring that the same percentage of HF patients is maintained in both subsets. This division helps prevent overfitting and ensures that the models are trained and validated on independent data. Additionally, ten-fold cross-validation is employed to further validate the model's performance.",
  "optimization/encoding": "In our study, the data encoding and preprocessing were crucial steps to ensure the robustness and accuracy of our machine-learning models. We began by obtaining publicly available heart failure (HF) microarray datasets from the NCBI Gene Expression Omnibus (GEO) database. These datasets were filtered based on specific criteria, including the organism being Homo sapiens and the inclusion of heart samples from both HF patients and non-failing donors (NFDs).\n\nThe raw datasets underwent several preprocessing steps using the Bioconductor package limma. This included column normalization and log-transformation to stabilize variance and make the data more suitable for analysis. Differentially expressed genes (DEGs) were then identified using the limma package, generating ranked lists of upregulated and downregulated genes based on their fold changes.\n\nTo integrate the results from multiple datasets, we employed the Robust Rank Aggregation (RRA) method. This approach combines the differential expression rankings of each gene across all datasets and performs a hypothesis test based on the ranking vector. Genes with an absolute fold change (logFC) greater than 1 and an adjusted p-value less than 0.05 were selected as significantly differentially expressed.\n\nBefore merging the datasets, we corrected for batch effects using the ComBat function from the sva package. This step was essential to minimize experimental variance and ensure consistency across the datasets. Principal component analysis (PCA) was conducted before and after correction to visualize the effectiveness of batch effect removal.\n\nFor the weighted gene co-expression network analysis (WGCNA), we selected genes with the top 25% variation across samples to ensure heterogeneity and accuracy in the co-expression network analysis. Genes with a height value greater than 60 were considered outliers and excluded. We then calculated the adjacency matrix from the soft thresholding power \u03b2 and converted it into a topological overlap matrix (TOM). Hierarchical clustering and dynamic tree cutting were used to divide genes into different modules, and modules with highly correlated eigengenes were merged with a merge height of 0.2.\n\nKey genes were defined based on gene significance (GS) and module membership (MM) within the target modules. The intersection of genes identified through the RRA and WGCNA methods was considered critical genes in HF. These critical genes were further analyzed using various machine-learning techniques, including best subset regression, LASSO regression, ridge regression, elastic net regression, and support vector machine-recursive feature elimination (SVM-RFE). The final diagnostic markers, FCN3 and SMOC2, were identified through these rigorous preprocessing and encoding steps, ensuring the reliability and validity of our findings.",
  "optimization/parameters": "In our study, we employed several machine learning techniques to identify diagnostic markers for heart failure (HF). Initially, we used best subset regression to determine the optimal feature subset, which resulted in a model with six features: AEBP1, FCN3, MYH6, NPPA, SGPP2, and SMOC2. This model exhibited the lowest Bayesian Information Criterion (BIC) score, indicating it was the most parsimonious and accurate among the tested models.\n\nTo further refine our selection, we applied regularization techniques, including LASSO, ridge, and elastic net regression. These methods helped in identifying the most contributory genes to the diagnosis of HF. The performance of these models was evaluated using the root mean squared error (RMSE) in the internal validation dataset. Among these, LASSO regression was found to be the optimal model, as it yielded the lowest RMSE.\n\nAdditionally, we utilized the Support Vector Machine-Recursive Feature Elimination (SVM-RFE) algorithm, which identified two critical genes as diagnostic markers. By intersecting the results from best subset regression, LASSO regression, and SVM-RFE, we ultimately determined FCN3 and SMOC2 as the key diagnostic markers for HF.\n\nThe selection of these parameters was rigorous and involved multiple validation steps to ensure robustness and accuracy. The final model incorporated these two markers, demonstrating superior diagnostic accuracy compared to traditional biomarkers like B-type natriuretic peptide (BNP). This approach not only reduced model complexity but also enhanced the model's predictive power, making it a reliable tool for HF diagnosis.",
  "optimization/features": "In the optimization process, feature selection was indeed performed to identify the most relevant diagnostic markers for heart failure. Initially, a comprehensive set of features was considered. However, to enhance model performance and reduce complexity, several feature selection methods were employed.\n\nThe best subset regression method was used to identify the feature subset that exhibited optimal classification performance. Additionally, regularization techniques such as LASSO regression, ridge regression, and elastic net regression were applied to detect the genes that contributed most to the diagnosis. These models were established on the training dataset, ensuring that the feature selection process did not introduce bias from the validation data.\n\nFurthermore, the support vector machine-recursive feature elimination (SVM-RFE) method was adopted for classification analysis of the selected biomarkers. The intersection of the best subset regression, the best-performing model of regularization technique, and SVM-RFE was considered to determine the final set of diagnostic markers.\n\nAs a result of this rigorous feature selection process, two critical genes, FCN3 and SMOC2, were identified as diagnostic markers. These markers were then used to construct a diagnostic model for heart failure, demonstrating superior diagnostic accuracy compared to traditional biomarkers like BNP.",
  "optimization/fitting": "In our study, we employed several machine learning techniques to identify diagnostic markers for heart failure (HF). Given the high-dimensional nature of gene expression data, the number of features (genes) was indeed much larger than the number of training samples. To address potential overfitting, we implemented several strategies.\n\nFirstly, we divided the final merged dataset into training and internal validation datasets at a ratio of 7:3, ensuring the same percentage of HF patients in both subsets. This split allowed us to train our models on one portion of the data and evaluate their performance on an independent portion, helping to mitigate overfitting.\n\nSecondly, we utilized ten-fold cross-validation during the training process. This technique involves partitioning the training data into ten subsets, training the model on nine subsets, and validating it on the remaining subset. This process is repeated ten times, with each subset serving as the validation set once. Cross-validation provides a more robust estimate of model performance and helps to ensure that the model generalizes well to unseen data.\n\nAdditionally, we employed regularization techniques such as LASSO (Least Absolute Shrinkage and Selection Operator), ridge regression, and elastic net. These methods introduce a penalty term to the loss function, which shrinks the coefficients of less important features towards zero, effectively reducing the model's complexity and preventing overfitting.\n\nTo further validate our models, we used the root mean squared error (RMSE) in the internal validation dataset as a performance metric. By comparing the RMSE of different models, we selected the one with the best performance, ensuring that it generalizes well to new data.\n\nRegarding underfitting, we used feature selection methods such as best subset regression and support vector machine-recursive feature elimination (SVM-RFE). These methods help identify the most relevant features, improving the model's ability to capture the underlying patterns in the data. Moreover, the use of multiple machine learning techniques and the integration of results from different methods helped to enhance the robustness of our diagnostic model, reducing the risk of underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting when applying regularization methods. Firstly, we divided the final merged dataset into a training dataset and an internal validation dataset at a ratio of 7:3, ensuring that the same percentage of heart failure (HF) patients was maintained in both datasets. This split allowed us to train our models on one subset of the data and evaluate their performance on a separate subset, reducing the risk of overfitting.\n\nAdditionally, we utilized ten-fold cross-validation during the training process. This technique involves dividing the training dataset into ten subsets, training the model on nine of these subsets, and validating it on the remaining subset. This process is repeated ten times, with each subset serving as the validation set once. Cross-validation helps to ensure that the model generalizes well to unseen data by providing a more robust estimate of its performance.\n\nWe applied three regularization techniques: LASSO (Least Absolute Shrinkage and Selection Operator) regression, ridge regression, and elastic net regression. These methods introduce a penalty term to the loss function, which helps to shrink the coefficients of less important features, thereby reducing model complexity and preventing overfitting. The performance of these models was evaluated using the root mean squared error (RMSE) in the internal validation dataset, which provided an unbiased estimate of the model's predictive accuracy.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model developed in this study is not a blackbox model. It is designed to be transparent and interpretable, providing clear insights into the relationship between the diagnostic markers and heart failure (HF).\n\nThe model incorporates a nomogram, which is a graphical representation of a predictive model. This nomogram allows for the visualization of the relationship between the expression levels of the diagnostic markers (FCN3 and SMOC2) and the probability of HF. By using the nomogram, clinicians can easily determine the likelihood of HF based on the expression levels of these markers.\n\nAdditionally, the model includes a forest plot from a multivariate logistic regression analysis. This plot clearly shows the independent association of each diagnostic marker with HF, providing a transparent view of how each marker contributes to the prediction.\n\nThe use of machine learning techniques such as best subset regression, LASSO regression, ridge regression, elastic net regression, and SVM-RFE ensures that the selected diagnostic markers are robust and relevant. These methods help in identifying the most significant features, reducing model complexity, and improving accuracy.\n\nFurthermore, the model's performance is evaluated using receiver operating characteristic (ROC) curves and calibration plots. These evaluations provide a clear understanding of the model's discriminatory capability and its fit between predicted and observed probabilities.\n\nOverall, the transparency of the model is achieved through the use of visual tools like nomograms and forest plots, as well as rigorous statistical and machine learning techniques. This ensures that the model is not only accurate but also interpretable, making it a valuable tool for clinical decision-making.",
  "model/output": "The model developed in this study is a classification model, specifically designed for diagnostic purposes. It is a multivariate logistic regression model that predicts the probability of heart failure (HF) based on the expression levels of two diagnostic markers, FCN3 and SMOC2. The model's performance was evaluated using receiver operating characteristic (ROC) curves, which are commonly used to assess the diagnostic ability of binary classifiers. The area under the curve (AUC) values for the model were 0.985 in the merged dataset and 0.976 in the external validation dataset, indicating strong discriminatory capability. Additionally, a nomogram was constructed to visualize the model, further emphasizing its use as a diagnostic tool. The calibration plots also showed good agreement between predicted and observed probabilities, reinforcing the model's reliability as a classifier for HF.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several rigorous steps to ensure the robustness and accuracy of the diagnostic model for heart failure (HF). Initially, the final merged dataset was divided into a training dataset and an internal validation dataset at a ratio of 7:3, maintaining the same percentage of HF patients in both subsets. This split was crucial to prevent model overfitting and to assess the model's performance on unseen data.\n\nTen-fold cross-validation was employed during the training phase to further validate the model's performance. This technique involves dividing the training dataset into ten subsets, training the model on nine subsets, and validating it on the remaining subset. This process is repeated ten times, with each subset serving as the validation set once. The average performance across these ten iterations provides a reliable estimate of the model's generalization capability.\n\nThree feature selection methods were utilized to identify the most relevant diagnostic markers: best subset regression, regularization techniques (ridge, elastic net, and lasso), and support vector machine-recursive feature elimination (SVM-RFE). The performance of these models was evaluated using the root mean squared error (RMSE) in the internal validation dataset. The model with the best performance was selected, and the genes from this model were considered potential diagnostic markers.\n\nThe diagnostic model was constructed using a binary logistic regression model, which was then visualized using a nomogram. The discriminatory capability of the model was evaluated using the receiver operating characteristic (ROC) curve, and its calibration was assessed using a calibration plot. These evaluations were performed in both the final merged dataset and an external validation dataset to ensure the model's generalizability and reliability.\n\nAdditionally, immune infiltration analysis was conducted to evaluate the immune activities involved in HF. Single-sample gene set enrichment analysis (ssGSEA) was used to investigate the varying degrees of infiltration of 29 immune signatures in HF patients and non-failing donors (NFDs). A correlation heatmap was generated to illustrate the correlation between immune cell types and immune-related functions. Principal component analysis (PCA) clustering analysis was also performed on the immune infiltration matrix data to visualize the differences between HF patients and NFDs.\n\nIn summary, the evaluation method involved a comprehensive approach that included cross-validation, internal and external validation datasets, and multiple feature selection techniques. This ensured that the diagnostic model for HF was robust, accurate, and generalizable.",
  "evaluation/measure": "In the evaluation of our diagnostic model for heart failure (HF), several performance metrics were reported to ensure a comprehensive assessment. The primary metric used was the Area Under the Receiver Operating Characteristic Curve (AUC), which provides a measure of the model's ability to distinguish between HF patients and non-HF individuals. The AUC was reported for both the merged dataset and an external validation dataset, demonstrating the model's robustness and generalizability.\n\nAdditionally, calibration plots were generated using the GiViTI calibration belt to assess the agreement between predicted probabilities and observed outcomes. This metric is crucial for evaluating the model's reliability and ensuring that the predicted risks are accurate.\n\nThe diagnostic accuracy of the nomogram, which was constructed based on the logistic regression analysis, was also compared to the traditional biomarker B-type natriuretic peptide (BNP). This comparison highlighted the superior performance of our model in diagnosing HF.\n\nFurthermore, the root mean squared error (RMSE) was used to evaluate the performance of different regularization techniques (LASSO, ridge, and elastic net) in the internal validation dataset. This metric helped in selecting the optimal model for feature selection.\n\nOverall, the reported metrics are representative of standard practices in the literature for evaluating diagnostic models. They provide a thorough assessment of the model's discriminatory power, calibration, and accuracy, ensuring that the findings are reliable and applicable in clinical settings.",
  "evaluation/comparison": "In our study, we employed a combination of robust bioinformatics methods and machine learning techniques to identify diagnostic markers for heart failure (HF). We utilized the Robust Rank Aggregation (RRA) method to integrate results from multiple datasets, addressing the limitation of conventional differentially expressed genes (DEGs) analysis. This approach allowed us to identify genes with robust differential expression across various studies.\n\nAdditionally, we used Weighted Gene Co-expression Network Analysis (WGCNA) to detect co-expression modules and hub genes in HF. WGCNA helped us dissect genes into network modules and find those significantly associated with clinical traits. The intersection of genes identified through RRA and WGCNA was considered critical for HF.\n\nFor feature selection in machine learning, we employed best subset regression and three regularization methods: LASSO regression, ridge regression, and elastic net. These models were established on a training dataset and evaluated using the root mean squared error (RMSE) in an internal validation dataset. Furthermore, we used the Support Vector Machine-Recursive Feature Elimination (SVM-RFE) method for classification analysis. The intersection of best subset regression, SVM-RFE, and the best-performing regularization model was considered potential diagnostic markers of HF.\n\nWhile we did not perform a direct comparison to publicly available methods on benchmark datasets, our approach integrated multiple robust techniques to enhance the reliability of our findings. We also did not compare our methods to simpler baselines, as our focus was on leveraging advanced bioinformatics and machine learning tools to identify critical genes and construct a diagnostic model for HF. The combination of RRA, WGCNA, and machine learning methods provided a comprehensive framework for exploring novel diagnostic markers associated with HF.",
  "evaluation/confidence": "In our study, we employed several statistical methods to ensure the robustness and significance of our findings. For differential expression gene (DEG) analysis, we used independent two-sample t-tests for normally distributed data with equal variance, and Mann-Whitney non-parametric tests otherwise. To control for multiple testing, we adjusted p-values using the Benjamini-Hochberg method. Genes were considered significantly differentially expressed if they had an absorbance fold change (logFC) greater than 1 and an adjusted p-value less than 0.05.\n\nFor correlation analyses, we calculated Pearson correlation coefficients for normally distributed variables and Spearman rank correlation otherwise. We interpreted correlation coefficients as follows: values between 0 and 0.3 indicated negligible correlation, 0.3 to 0.5 indicated low correlation, 0.5 to 0.7 indicated moderate correlation, 0.7 to 0.9 indicated high correlation, and 0.9 to 1 indicated very high correlation. We defined genes with an absolute correlation coefficient greater than 0.5 and a p-value less than 0.05 as diagnostic marker-related genes.\n\nIn our machine learning models, we used root mean squared error (RMSE) as a goodness-of-fit indicator to evaluate model performance. We also performed ten-fold cross-validation to prevent model overfitting. The intersection of genes identified by best subset regression, support vector machine-recursive feature elimination (SVM-RFE), and the best-performing regularization technique (LASSO, ridge, or elastic net) were considered potential diagnostic markers.\n\nTo evaluate the diagnostic model, we used receiver operating characteristic (ROC) curves to assess discriminatory capability and calibration plots to evaluate calibration. The model's performance was validated in both the merged dataset and an external validation dataset. Statistical significance was determined using p-values, with significance levels indicated as follows: *p < 0.05, **p < 0.01, ***p < 0.001.",
  "evaluation/availability": "The raw evaluation files used in our study are publicly available. They were obtained from the NCBI Gene Expression Omnibus (GEO) database. Specifically, the datasets used include GSE16499, GSE26887, GSE42955, GSE57338, GSE76701, and GSE79962. These datasets were filtered based on specific eligibility criteria, such as being from \"Homo sapiens\" and including heart samples from both heart failure patients and non-failing donors. Additionally, GSE116250 was selected as the external validation dataset.\n\nThe series matrix files of these datasets and their corresponding platform files were downloaded as CSV files from the GEO database. These files are available for public use under the terms and conditions specified by the GEO database, which typically include proper citation of the original studies and compliance with any relevant data usage policies. The datasets can be accessed through the GEO database website, ensuring transparency and reproducibility of our research findings."
}