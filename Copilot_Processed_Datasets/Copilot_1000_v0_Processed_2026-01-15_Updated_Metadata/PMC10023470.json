{
  "publication/title": "Validation of a targeted metabolomics panel for improved second-tier newborn screening.",
  "publication/authors": "Mak J, Peng G, Le A, Gandotra N, Enns GM, Scharfe C, Cowan TM",
  "publication/journal": "Journal of inherited metabolic disease",
  "publication/year": "2023",
  "publication/pmid": "36680545",
  "publication/pmcid": "PMC10023470",
  "publication/doi": "10.1002/jimd.12591",
  "publication/tags": "- Metabolomics\n- Newborn screening\n- Machine learning\n- False positives\n- Inborn metabolic disorders\n- Targeted metabolomics\n- Random Forest classification\n- Metabolic profiling\n- Biomarkers\n- Chromatographic separation",
  "dataset/provenance": "The dataset used in this study was obtained from the California Biobank Program (CBP) under SIS request 886. The data consists of newborn screening samples, specifically dried blood spots, from 178 cases with true-positive screens for four conditions: GA1, MMA, OTCD, or VLCADD. Additionally, the dataset includes 613 false-positive screens for these conditions and 92 true-negative controls.\n\nThe initial untargeted metabolomics analysis yielded approximately 9,000 ion features. These features were ranked using a Random Forest (RF) classification algorithm to identify the most important metabolites for disease classification. Following this, a targeted metabolomics approach was employed, focusing on a 121-metabolite panel. This panel was used to further analyze 285 screen-positive cases, aiming to improve the discrimination between true- and false-positive cases.\n\nThe data used in this study has not been previously published or used by the community in the same context. The California Department of Public Health is not responsible for the results or conclusions drawn by the authors of this publication. However, the data can be obtained by others after submitting a new request to the CBP coordinator. Requests for data should be directed to CaliforniaBiobank@cdph.ca.gov.",
  "dataset/splits": "Not applicable.",
  "dataset/redundancy": "Not applicable",
  "dataset/availability": "The data used in this study were obtained from the California Biobank Program under a specific request. The California Department of Public Health is not responsible for the results or conclusions drawn by the authors of this publication. The data can be obtained by others after submitting a new request to the California Biobank Program coordinator. Requests for data should be directed to CaliforniaBiobank@cdph.ca.gov. This process ensures that the data is accessible to other researchers while maintaining the necessary oversight and control over its distribution.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is Random Forest (RF). This is a well-established ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n\nThe Random Forest algorithm is not new; it has been extensively used and validated in various fields, including metabolomics and newborn screening. The algorithm was introduced by Leo Breiman in 2001 and has since become a standard tool in machine learning due to its robustness and ability to handle complex datasets.\n\nThe reason this algorithm was not published in a machine-learning journal is that our focus is on applying established machine-learning techniques to improve the accuracy and efficiency of newborn screening for metabolic disorders. The innovation lies in the application of these techniques to a specific medical problem rather than in the development of a new algorithm. Our work demonstrates how Random Forest can be effectively used to reduce false positives in newborn screening, which is a significant contribution to the field of medical diagnostics.",
  "optimization/meta": "The model employed in this study does not function as a meta-predictor. Instead, it relies on a single machine-learning algorithm, specifically Random Forest (RF), to classify samples. The RF algorithm is used to analyze targeted metabolomics data, which includes either analyte peak areas or response ratios for 285 screen-positive cases. This approach aims to improve discrimination between true- and false-positive cases without compromising sensitivity.\n\nThe RF classifier was trained using data from an expanded metabolite panel, and its performance was evaluated using leave-one-out cross-validation repeated 20 times. This method ensures that the model's stability and reproducibility are assessed rigorously. The RF algorithm's output is visualized using the Mean Decrease in Accuracy (MDA) to identify the importance of each metabolite in the classification process.\n\nThe training data for the RF model consists of targeted metabolomics data from screen-positive cases, and the independence of this data is maintained through the leave-one-out cross-validation process. This ensures that each sample is used once as a testing case, while the remaining samples are used for training, thereby preserving the independence of the training data.",
  "optimization/encoding": "For the machine-learning algorithm, the data was initially processed using Progenesis for untargeted studies. This software was employed to open and align raw data, pick peaks, and export a dataset of ion features and their intensities in each sample. Internal standards were utilized for retention time alignment but not for data normalization. The data from both positive- and negative-ionization acquisition modes were then merged into a master dataset. To ensure data quality, ion features in the quality control samples with a coefficient of variation (CV) of 30% or less were retained for further analysis.\n\nFor targeted studies, Targetlynx XS was used to open the raw data, review peak integration, and export a dataset of compounds and their intensities in each sample. When a matching isotopic internal standard was available, the peak area was normalized to the internal standard peak area. For compounds without a matching internal standard, the responses were reported using raw peak area unless otherwise specified.\n\nThe Random Forest (RF) classification algorithm was then applied to identify features that differed most between true- and false-positive cases. The importance of each metabolite to the classification was visualized using the Mean Decrease in Accuracy (MDA), which lists features in order of their importance to the model. This output was manually reviewed to select features that clearly differed between true- and false-positives. Compounds were identified using an in-house library that included MS1, MS2, and retention time data for a given chromatography method. Only those with tier 1 matches were carried forward for clinical test translation.\n\nFor the validation study, RF classification was applied using leave-one-out cross-validation (LOOCV). This method involves using one sample as the testing set and all others as the training set to build the model for each disorder. This process was iterated for all samples, ensuring that each sample was used as the testing set once. Only RF assignments from the testing cases were used for the final outcome prediction. Positive predictive values (PPV) with 100% classification sensitivity and area under the receiver operating characteristic curves (AUC) were estimated from the testing results. To demonstrate robustness, LOOCV was repeated 20 times, and the median PPV and AUC results were reported. The RF classification analyses were conducted using R software version 4.1.3 with the randomForest and pROC packages. Additionally, principal component analyses (PCA) and heatmaps with Ward\u2019s hierarchical clustering using Euclidean distance were created using Metaboanalyst. Boxplots were generated using Microsoft Excel.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "In our study, we utilized a comprehensive set of features for input into our machine learning models. Initially, we performed untargeted metabolomics, which yielded approximately 9,000 ion features. However, not all of these features were used directly in our final analysis. We employed a feature selection process to identify the most relevant metabolites for classification. This process involved using Random Forest (RF) to rank the importance of each ion feature and manually reviewing the output to select features that clearly differed between true- and false-positive cases.\n\nFor our targeted studies, we focused on a more refined set of metabolites. We prioritized a list of 121 metabolites, which included known disease markers from primary screening, metabolites identified through untargeted metabolomics, and isobaric species revealed by chromatographic separation. This targeted approach allowed us to mimic a second-tier screening environment and rigorously evaluate our metabolite panel.\n\nFeature selection was performed using the training set only, ensuring that the selection process did not introduce bias from the testing set. This approach helped us to identify the most important analytes driving classification, such as 3-hydroxyglutaric acid for GA1, methylmalonic acid for MMA, and specific long-chain acylcarnitines for VLCADD. The selected features were then used to train our RF classifiers, which demonstrated significant reductions in false-positive rates for the conditions studied.",
  "optimization/fitting": "Not applicable",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One of the primary methods used was leave-one-out cross-validation (LOOCV). This technique involves using one sample as the testing set and all other samples as the training set, iteratively building the model for each disorder. This process was repeated for all samples, ensuring that every sample was used as the testing set exactly once. By doing so, we could estimate the performance of our Random Forest (RF) models more accurately and reduce the risk of overfitting.\n\nAdditionally, we repeated the LOOCV process 20 times to further evaluate the stability and reproducibility of our models. This multiple iteration approach helped in assessing the variation in the area under the receiver operating characteristic curves (AUC) and ensured that our results were not due to random chance.\n\nFurthermore, we manually reviewed the output from the RF models, specifically the Mean Decrease in Accuracy (MDA) scores, to select features that clearly differed between true- and false-positive cases. This step helped in focusing on the most relevant features, thereby reducing the complexity of the model and mitigating the risk of overfitting.\n\nIn summary, our approach included LOOCV, repeated cross-validation, and manual feature selection to prevent overfitting and enhance the reliability of our classification models.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model employed in this study is not a black box. It utilizes Random Forest (RF), a classification algorithm that combines the output of many random decision trees. This approach allows for interpretability through the visualization of feature importance. Specifically, the importance of each metabolite to the classification is visualized using the Mean Decrease in Accuracy (MDA), a score chart that lists features in order of their importance to the model. This method provides a clear and transparent way to understand which metabolites are most critical for distinguishing between true- and false-positive cases.\n\nFor example, in the analysis of GA1, 3-hydroxyglutaric acid was identified as the most important metabolite, indicating that its removal would substantially decrease the classification accuracy. Similarly, for OTCD, metabolites like proline, citrulline, and uridine showed clear separation between true- and false-positive cases, supporting the RF classification accuracy. This transparency allows researchers to manually review the output and select features that clearly differ between true- and false-positives, ensuring that the model's decisions are understandable and verifiable.",
  "model/output": "The model employed in our study is a classification model. We utilized Random Forest (RF), a classification algorithm that combines the output of many random decision trees. This model was used to identify features that differed most between true- and false-positive cases in our metabolomics data. The RF model was applied to classify samples in the validation study, with the performance estimated using leave-one-out cross-validation (LOOCV). This process involved using one sample as testing and all others as training to build the model for each disorder, iterating for all samples. The final outcome prediction was based on RF assignments from testing cases, not from training. The model's performance was evaluated using positive predictive values (PPV) with 100% classification sensitivity and area under the receiver operating characteristic curves (AUC). The robustness of the model was demonstrated by repeating LOOCV 20 times and reporting the median PPV and AUC results. The RF classification analyses were conducted using R software with the randomForest and pROC packages.",
  "model/duration": "The execution time for the targeted metabolomics analysis was optimized for a rapid and high-throughput process. Specifically, each sample was analyzed using a liquid chromatography-tandem mass spectrometry (LC-MS/MS) method with a runtime of just 3 minutes per sample. This efficient runtime highlights the applicability of the method for newborn screening settings, where quick turnaround times are crucial. The combination of targeted metabolomics and machine learning allowed for the effective classification of true- and false-positive cases, demonstrating the feasibility of integrating this approach into clinical workflows.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The method was evaluated using a robust approach that included both untargeted and targeted metabolomics studies. For untargeted studies, Progenesis was used to process raw data, pick peaks, and export datasets of ion features and their intensities. Internal standards were used for retention time alignment, and quality control samples with a coefficient of variation (CV) of 30% or less were retained for analysis. Random Forest (RF), a classification algorithm, was employed to identify features that differed most between true- and false-positive cases. The importance of each metabolite to classification was visualized using the Mean Decrease in Accuracy (MDA) score chart. Features that clearly differed between true- and false-positives were manually reviewed and selected.\n\nFor targeted studies, Targetlynx XS was used to process raw data, review peak integration, and export datasets of compounds and their intensities. Peak areas were normalized to internal standard peak areas when available. RF classification was applied to classify samples in the validation study, with performances estimated using leave-one-out cross-validation (LOOCV). This process involved using one sample as testing and all others as training to build the model for each disorder, iterating for all samples. Only RF assignments from testing cases were used for final outcome prediction. Positive predictive values (PPV) with 100% classification sensitivity and area under the receiver operating characteristic curves (AUC) were estimated from testing results. To ensure robustness, LOOCV was repeated 20 times, and median PPV and AUC results were reported.\n\nPrincipal component analyses (PCA) and heatmaps with Ward\u2019s hierarchical clustering using Euclidean distance were created using Metaboanalyst. Boxplots were generated using Microsoft Excel to visualize the data. The evaluation demonstrated that the expanded metabolite panel contained necessary markers to reduce false positive cases for multiple disorders, showing clear separation between true- and false-positive cases in heatmap analysis. The RF classifier reduced the number of false positives significantly for various conditions, with AUC ranging from 0.93 to 1.00 and PPV ranging from 36% to 100%. The stability and reproducibility of the classifier models were confirmed through multiple iterations of LOOCV.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our machine learning approach for classifying true- and false-positive cases in newborn screening for metabolic disorders. The primary metrics reported include Positive Predictive Value (PPV) and the Area Under the Receiver Operating Characteristic Curve (AUC).\n\nPPV is calculated as the ratio of true positives (TP) to the sum of true positives and false positives (FP), providing a measure of the proportion of positive results that are actual positives. We achieved high PPV values, with the most notable being 100% for OTCD, indicating perfect classification for this condition. For other disorders, PPV ranged from 36% to 100%, demonstrating varying levels of classification accuracy.\n\nAUC is a measure of the model's ability to distinguish between true-positive and false-positive cases. It ranges from 0 to 1, with higher values indicating better performance. Our models showed strong discriminative power, with AUC values ranging from 0.93 to 1.00. This indicates that our approach effectively separates true-positive cases from false-positive ones.\n\nTo ensure the robustness of our results, we performed leave-one-out cross-validation (LOOCV) 20 times and reported the median PPV and AUC results. This method involves using one sample as the testing set and all others as the training set, iterated for all samples. The stability and reproducibility of our models were further validated by the low standard deviation of AUC results, with the largest being 0.007 for GA1.\n\nThese metrics are representative of those commonly used in the literature for evaluating classification models in metabolomics and newborn screening studies. They provide a comprehensive assessment of our model's performance, highlighting its effectiveness in reducing false positives while maintaining high sensitivity.",
  "evaluation/comparison": "Not applicable.",
  "evaluation/confidence": "The evaluation of our method's performance included several key metrics, and we took steps to ensure the confidence in these results. We used leave-one-out cross-validation (LOOCV) to estimate the performance of our Random Forest (RF) models. This process was repeated 20 times to assess the stability and reproducibility of our results. For each disorder, we reported the median Positive Predictive Value (PPV) and Area Under the Receiver Operating Characteristic Curve (AUC) from these iterations. The largest standard deviation of AUC results was 0.007 for GA1, indicating good stability.\n\nThe statistical significance of our findings was demonstrated through the reduction in false positives. For example, RF reduced the number of false positives by 83% for GA1, 84% for MMA, 100% for OTCD, and 51% for VLCADD, all while maintaining 100% sensitivity. The AUC ranged from 0.93 to 1.00, and PPV ranged from 36% to 100%, depending on the disorder. These metrics suggest that our method is superior in discriminating between true- and false-positive cases compared to traditional screening methods.\n\nAdditionally, the heatmap analysis with Ward\u2019s hierarchical clustering revealed clear separation between true- and false-positive cases for GA1, VLCADD, and OTCD. This visual evidence supports the quantitative metrics, providing further confidence in the method's effectiveness. The use of isotopic internal standards and processing samples in a single batch also helped to minimize batch effects and reduce analytic variability, enhancing the reliability of our results.",
  "evaluation/availability": "Not enough information is available."
}