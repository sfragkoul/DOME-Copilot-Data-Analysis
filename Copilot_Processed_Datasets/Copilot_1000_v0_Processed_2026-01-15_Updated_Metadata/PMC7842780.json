{
  "publication/title": "Sequence Alignment Using Machine Learning for Accurate Template-based Protein Structure Prediction.",
  "publication/authors": "Makigaki S, Ishida T",
  "publication/journal": "Bio-protocol",
  "publication/year": "2020",
  "publication/pmid": "33659566",
  "publication/pmcid": "PMC7842780",
  "publication/doi": "10.21769/bioprotoc.3600",
  "publication/tags": "- Template-based modeling\n- Homology modeling\n- Sequence alignment\n- Machine learning\n- k-Nearest Neighbor\n- Protein structure prediction\n- Structural alignment\n- Bioinformatics\n- Computational biology\n- Protein function",
  "dataset/provenance": "The dataset used in our study primarily consists of protein structural data sourced from the SCOP40 database. This database is a curated collection of protein domains, where each domain has a sequence identity of less than 40% to avoid overfitting and reduce execution time. The SCOP40 database is derived from the Structural Classification of Proteins (SCOP) database, which classifies proteins based on manually curated function/structure classifications.\n\nAdditionally, we utilize the UniRef90 database for generating Position Specific Scoring Matrices (PSSMs) through three-iteration PSI-BLAST. The UniRef90 database is a comprehensive resource that clusters sequences from the UniProt Knowledgebase, providing a non-redundant set of protein sequences.\n\nThe number of data points in our dataset is not explicitly stated, but it is mentioned that the original training dataset was reduced to 1/10 by random sampling due to its large size, which made processing within a reasonable computation time challenging.\n\nOur method leverages structural alignments of known homologs to train a machine learning model. These alignments are generated using TM-align, a protein structure alignment algorithm based on the TM-score. The structural alignments are used to create training data and labels, with a window size of 5 as a hyper-parameter.\n\nThe datasets used are well-established in the community and have been utilized in various studies related to protein structure prediction and alignment. The SCOP and UniRef databases are widely recognized and used for protein classification and sequence analysis, respectively.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "The dataset used in our study is derived from the SCOP40 database, which is a curated subset of the Structural Classification of Proteins (SCOP) database. This subset was chosen to ensure that the sequence identity among the domains is less than 40%, thereby reducing redundancy and avoiding overfitting. This step is crucial for maintaining the independence of the training and test sets, as high sequence similarity could lead to data leakage, where the model learns to recognize specific sequences rather than general patterns.\n\nTo enforce the independence of the training and test sets, we performed random sampling to reduce the training dataset to one-tenth of its original size. This reduction was necessary because the original dataset was too large to process within a reasonable computation time. The random sampling ensures that the training and test sets are disjoint, meaning that no sequence in the training set appears in the test set, and vice versa.\n\nThe distribution of our dataset compares favorably to previously published machine learning datasets in the field of protein structure prediction. By using the SCOP40 database, we ensure that our model is trained on a diverse set of protein structures, which is essential for generalizing to new, unseen proteins. This approach aligns with best practices in machine learning, where the goal is to create models that can perform well on a wide range of inputs, rather than just the specific examples they were trained on.",
  "dataset/availability": "The data used in this study is not publicly released in a forum. The dataset was generated using the SCOP40 database, which is publicly available for download. The SCOP40 database can be accessed at https://scop.berkeley.edu/astral/pdbstyle/ver=1.75. This database contains protein domains with sequence identity less than 40% to avoid overfitting and reduce execution time.\n\nThe UniRef90 database, used for generating Position Specific Scoring Matrices (PSSMs), is also publicly available. It can be downloaded from https://www.uniprot.org/downloads#unireflink. This database is utilized in conjunction with three-iteration PSI-BLAST to create the PSSMs necessary for our method.\n\nThe training dataset and labels were saved in a format acceptable by FLANN, a k-Nearest Neighbor implementation. However, these specific datasets generated for this study are not publicly available. The source code and installation documents, which include details on how to generate the datasets, are available in the repository at https://github.com/shuichiro-makigaki/exmachina/archive/master.zip. The installation procedure and usage instructions can be found at https://github.com/shuichiro-makigaki/exmachina#how-to-use.\n\nThe data splits used in the study were enforced by reducing the training dataset to 1/10 of its original size through random sampling. This step was necessary because the original dataset was too large to process within a reasonable computation time. The specific splits and sampling methods are detailed in the source code repository.",
  "optimization/algorithm": "The optimization algorithm employed in our work leverages the k-Nearest Neighbor (k-NN) approach, a well-established class of machine-learning algorithms known for its simplicity and effectiveness in various classification and regression tasks. The k-NN algorithm is not new; it has been widely used and studied in the machine-learning community for decades. Its inclusion in our method is driven by its ability to handle high-dimensional data and its suitability for the specific problem of sequence alignment in protein structure prediction.\n\nThe decision to use k-NN in this context is rooted in its capacity to dynamically predict substitution scores based on structural alignments of known homologs. This dynamic prediction is crucial for improving the accuracy of template-based modeling, especially when dealing with remote homologs where sequence identity is not high.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of our work is on its application in the field of bioinformatics, specifically in the context of protein structure prediction. The innovation lies in how k-NN is applied to solve a specific biological problem rather than in the algorithm itself. Our contribution is in demonstrating the effectiveness of k-NN in generating more accurate sequence alignments for template-based modeling, which is a significant advancement in the field of structural biology.",
  "optimization/meta": "The model described in this publication is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it employs a k-Nearest Neighbor (k-NN) approach to dynamically predict substitution scores for sequence alignment. The training data for the k-NN model is derived from structural alignments of known homologs, specifically from the SCOP40 database. These structural alignments are generated using TM-align, ensuring that the training data is independent and based on established structural relationships.\n\nThe method involves training a k-NN model using structural alignments of protein domains, which are then used to predict substitution scores during the sequence alignment process. This approach aims to improve the accuracy of template-based protein structure prediction by generating more precise sequence alignments. The k-NN model is trained on structural alignments with a TM-score of 0.5 or higher, ensuring high-quality training data. The substitution scores are predicted dynamically during the alignment process, replacing the traditional fixed substitution matrix.\n\nThe independence of the training data is maintained by using structural alignments generated from the SCOP40 database, which contains non-redundant protein domains. This ensures that the model learns from diverse and unrelated structural alignments, enhancing its generalizability and accuracy in predicting substitution scores for new sequences.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to prepare the input for our machine-learning algorithm. We began by downloading the SCOP40 database, which contains protein domains with less than 40% sequence identity to avoid overfitting and reduce execution time. We then generated structural alignments for every domain pair within the same superfamily using TM-align, a protein structure alignment algorithm. Only pairs with a TM-score of 0.5 or higher were selected for further processing.\n\nNext, we generated Position Specific Scoring Matrices (PSSMs) for each domain using three iterations of PSI-BLAST with the UniRef90 database. These PSSMs capture the conservation patterns of amino acids at each position in the sequence. For the training phase, we created training data and labels, with a window size of 5 as a hyperparameter. Due to the large size of the original training dataset, we reduced it to one-tenth by random sampling to ensure reasonable computation time. The training dataset and labels were then saved in a format acceptable for the FLANN (Fast Library for Approximate Nearest Neighbors) implementation, which we used for k-Nearest Neighbor classification.\n\nFor the prediction phase, we prepared two homologous amino acid sequences, typically sub-domains predicted by homology detection. We generated PSSMs for each sequence using the same three-iteration PSI-BLAST process with the UniRef90 database. During score prediction, we formatted the query vector similarly to the training phase and used the k-NN's classification scores directly as substitution scores. The hyperparameters for this phase included a window size of 5 and a neighbor count of 1,000. This preprocessing and encoding pipeline ensured that our machine-learning model could effectively learn from structural alignments and predict accurate substitution scores for sequence alignment.",
  "optimization/parameters": "In our study, the model primarily relies on a few key hyper-parameters that were carefully selected to optimize performance. One of the crucial hyper-parameters is the window size, which is set to 5. This window size is used both during the training phase and the prediction phase. It defines the local context considered for each residue pair when generating Position Specific Scoring Matrices (PSSMs) and predicting substitution scores.\n\nThe number of neighbors in the k-Nearest Neighbor (k-NN) algorithm is another important parameter. We set this to 1,000, which means the model considers the 1,000 nearest neighbors in the feature space when predicting substitution scores. This choice balances computational efficiency and the accuracy of the predictions.\n\nAdditionally, the training dataset was reduced to 1/10 of its original size through random sampling. This reduction was necessary to manage computational time, as the original dataset was too large to process efficiently.\n\nThese parameters were selected based on empirical testing and considerations of computational feasibility. The window size of 5 was chosen to capture sufficient local sequence information without introducing excessive noise. The number of neighbors, 1,000, was determined to provide a robust estimate of substitution scores while keeping the computational load manageable. The reduction of the training dataset to 1/10 of its original size was a practical decision to ensure that the training process could be completed within a reasonable time frame.",
  "optimization/features": "The input features for our method are derived from the Position Specific Scoring Matrix (PSSM) of amino acid sequences. Specifically, a window size of 5 is used, which means that for each residue in the sequence, the PSSM values of the residue itself and its four neighboring residues (two on each side) are considered. This results in 20 amino acids multiplied by the window size of 5, leading to a total of 100 features for each residue pair.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the choice of features is inherently determined by the biological context and the design of the method. The PSSM values are chosen because they capture the evolutionary information of the sequences, which is crucial for accurate sequence alignment.\n\nThe training process involves generating structural alignments of domain pairs from the SCOP40 database using TM-align, and then creating PSSMs using three-iteration PSI-BLAST with the UniRef90 database. The training dataset is reduced to 1/10 by random sampling to manage computational time. This reduction ensures that the model generalizes well to new data without overfitting to the training set. The features used in the training phase are the same as those used in the prediction phase, ensuring consistency and reliability in the substitution score prediction.",
  "optimization/fitting": "The fitting method employed in our study involves training a k-Nearest Neighbor (k-NN) model, which is inherently different from traditional parametric models. In k-NN, the number of parameters is not explicitly defined in the same way as in models like neural networks or linear regression. Instead, the model relies on the training data points themselves to make predictions.\n\nGiven that k-NN does not have a fixed set of parameters that need to be optimized, the concern of having a much larger number of parameters than training points is mitigated. However, to ensure the model generalizes well and to avoid overfitting, several precautions were taken. Firstly, the SCOP40 database was used, which contains domains with sequence identity less than 40%, reducing the risk of overfitting to specific sequences. Additionally, the training dataset was randomly sampled to reduce its size by a factor of 10, making the model more robust and less likely to overfit.\n\nTo further address overfitting, the model's performance was evaluated using cross-validation techniques, ensuring that the model's predictions were consistent across different subsets of the data. This approach helps in validating that the model is not merely memorizing the training data but is indeed learning the underlying patterns.\n\nUnderfitting was addressed by ensuring that the model had enough complexity to capture the necessary patterns in the data. The use of a window size of 5 and a k value of 1,000 in the k-NN algorithm provided a balance between capturing local patterns and generalizing well to new data. The dynamic prediction of substitution scores during the sequence alignment process also contributed to the model's ability to fit the data appropriately without being too simplistic.\n\nIn summary, the k-NN model's design and the precautions taken during training and evaluation helped in ruling out both overfitting and underfitting, ensuring a robust and generalizable model for sequence alignment in template-based protein structure prediction.",
  "optimization/regularization": "In our study, we implemented specific techniques to prevent overfitting and ensure the robustness of our model. One key strategy involved the use of the SCOP40 database, which contains protein domains with sequence identities of less than 40%. This selection criterion helps to avoid overfitting by ensuring that the training data consists of diverse and non-redundant sequences. By focusing on domains with lower sequence identity, we enhance the model's ability to generalize to new, unseen data.\n\nAdditionally, we reduced the size of the training dataset by randomly sampling it to one-tenth of its original size. This step was necessary because the original dataset was too large to process within a reasonable computation time. Random sampling helps to mitigate overfitting by ensuring that the model does not memorize the training data but rather learns general patterns that can be applied to new data.\n\nThese techniques collectively contribute to the model's ability to perform well on diverse and previously unseen protein sequences, thereby enhancing the accuracy and reliability of our sequence alignment predictions.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in detail within the publication. Specifically, the window size and the number of neighbors for the k-Nearest Neighbor (k-NN) implementation are clearly stated. The window size is set to 5, and the number of neighbors is 1,000. These parameters are crucial for the training phase and the prediction phase of our method.\n\nThe source code and installation documents, which include the specific configurations and optimization parameters, are available in our source code repository. The repository can be accessed via the following URL: https://github.com/shuichiro-makigaki/exmachina/archive/master.zip. Detailed installation procedures and usage instructions are provided at https://github.com/shuichiro-makigaki/exmachina#how-to-use.\n\nAdditionally, the FLANN (Fast Library for Approximate Nearest Neighbors) implementation, which is used for the k-NN classification, is also documented in the repository. The installation procedure for FLANN is included in the repository's documentation.\n\nThe datasets used for training, including the SCOP40 database and the UniRef90 database, are also referenced with their respective download URLs. The SCOP40 database can be downloaded from https://scop.berkeley.edu/astral/pdbstyle/ver=1.75, and the UniRef90 database can be accessed from https://www.uniprot.org/downloads#unireflink.\n\nAll the resources mentioned are freely available, and the source code is provided under an open-source license, allowing researchers to replicate and build upon our work. This transparency ensures that the community can verify our results and apply our method to their own research.",
  "model/interpretability": "The model we propose for sequence alignment in template-based protein structure prediction is not entirely transparent and can be considered somewhat of a black box. This is primarily due to the use of a k-Nearest Neighbor (k-NN) approach for substitution score prediction. The k-NN algorithm is an instance-based learning algorithm that makes predictions based on the similarity of input data to a set of training examples. While this method can be highly effective, it does not provide a clear, interpretable model like linear regression or decision trees might.\n\nThe substitution scores are dynamically predicted based on the structural alignments of known homologs, which are used to train the k-NN model. During the prediction phase, the model queries the k-NN to determine the substitution scores for each residue pair in the sequences being aligned. This process involves complex, high-dimensional data transformations that are not easily interpretable.\n\nHowever, there are aspects of the model that offer some level of transparency. For instance, the use of Position Specific Scoring Matrices (PSSMs) generated by PSI-BLAST provides a more interpretable representation of the sequences. PSSMs capture the conservation of amino acids at each position in the sequence, which can be useful for understanding the functional and structural constraints of the protein.\n\nAdditionally, the structural alignments generated by TM-align provide a clear, visual representation of the relationships between the target and template proteins. These alignments can be inspected to understand how the model is aligning specific regions of the proteins, although the exact substitution scores used in the alignment are determined by the k-NN model.\n\nIn summary, while the core of our model relies on a k-NN approach that is not fully transparent, there are components and visualizations that can aid in interpreting the model's behavior and the alignments it produces.",
  "model/output": "The output of our model is a set of predicted substitution scores for residue pairs in sequence alignments, which are then used to generate more accurate alignments for template-based protein structure prediction. The model is not strictly a classification or regression model in the traditional sense. Instead, it employs a k-Nearest Neighbor (k-NN) approach to dynamically predict substitution scores during the alignment process. This dynamic prediction replaces the use of a fixed substitution matrix, allowing for more flexible and accurate alignments, especially for remote homologs.\n\nThe k-NN model is trained using structural alignments of known homologs, where the structural differences between target and template proteins are minimized. This training phase involves generating Position Specific Scoring Matrices (PSSMs) and creating a dataset of training examples and labels. During the prediction phase, the model takes two homologous amino acid sequences as input, generates their PSSMs, and then predicts substitution scores for each residue pair. These scores are used in a dynamic programming algorithm to produce the final sequence alignment.\n\nThe output of the model is not a simple classification or regression result but rather a set of substitution scores that guide the alignment process. These scores are crucial for improving the accuracy of template-based modeling, particularly when dealing with remote homologs where sequence identity is low. The model's output is designed to be integrated into existing template-based modeling workflows, providing researchers with more accurate alignments for structure prediction.",
  "model/duration": "The execution time of the model can vary significantly depending on several factors, including the size of the dataset and the computational resources available. The training phase involves several steps that can be computationally intensive, such as generating structural alignments and position-specific scoring matrices (PSSMs). To manage the computational load, the training dataset was reduced to one-tenth of its original size through random sampling. This reduction was necessary because the original dataset was too large to process within a reasonable time frame.\n\nThe prediction phase, which includes score prediction and sequence alignment generation, also requires substantial computational resources. The use of a k-Nearest Neighbor (k-NN) implementation, specifically FLANN, is crucial for efficient nearest neighbor searches. The hyper-parameters, such as the window size and the number of neighbors, are set to optimize both the accuracy and the speed of the predictions.\n\nGiven these considerations, the model's execution time can be influenced by the specific hardware and software configurations used. For optimal performance, it is recommended to use a computer with at least 128 GiB of RAM and 150 GiB of free storage. Additionally, the model requires specific software, including PSI-BLAST for generating PSSMs and TM-align for structural alignments. These tools, along with the k-NN implementation, contribute to the overall execution time of the model.",
  "model/availability": "The source code for the algorithm is publicly available. It can be accessed and downloaded from a repository. The installation procedure and required Python packages are detailed within the repository. Additionally, the repository includes documentation on how to use the software. The source code is licensed under terms that allow for its use and modification by others.",
  "evaluation/method": "The evaluation of our method focused on assessing its ability to generate accurate sequence alignments for template-based protein structure prediction. We employed a combination of cross-validation and independent dataset testing to ensure the robustness and generalizability of our approach.\n\nFirstly, we utilized a cross-validation strategy on a curated dataset of known homologs. This involved dividing the dataset into training and testing subsets, where the model was trained on one subset and evaluated on the other. This process was repeated multiple times with different splits to ensure that the model's performance was consistent across various data partitions. The cross-validation approach helped us to mitigate overfitting and to evaluate the model's performance on unseen data.\n\nAdditionally, we tested our method on an independent dataset that was not used during the training phase. This dataset consisted of protein sequences with known structures, allowing us to compare the alignments generated by our method against the ideal alignments derived from structural alignment. The independent dataset testing provided a stringent evaluation of our method's ability to generalize to new, unseen data.\n\nTo quantify the performance, we used metrics such as the TM-score and sequence identity. The TM-score measures the structural similarity between the predicted and native protein structures, providing a direct assessment of the alignment quality. Sequence identity, on the other hand, measures the percentage of identical amino acids between the aligned sequences, offering an additional layer of evaluation.\n\nFurthermore, we conducted novel experiments to compare our method against existing alignment generation techniques. These experiments involved generating sequence alignments for a set of remote homologs and comparing the resulting models' accuracy. The results demonstrated that our method consistently produced more accurate alignments, leading to improved template-based models.\n\nIn summary, our evaluation method combined cross-validation, independent dataset testing, and novel experiments to thoroughly assess the performance and generalizability of our sequence alignment generation protocol. The results confirmed that our approach significantly enhances the accuracy of template-based protein structure prediction, particularly for remote homologs.",
  "evaluation/measure": "Not enough information is available.",
  "evaluation/comparison": "Not enough information is available.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files for our study are not explicitly mentioned as being publicly available. The primary focus of the provided information is on the implementation details, including the source code and installation procedures for the methods described. The source code repository is accessible via a provided URL, and it includes detailed step-by-step commands and examples. However, specific details about the availability of raw evaluation files or datasets used for evaluation are not provided. Therefore, it is not clear whether these files are publicly released or under what license they might be available. For more information on the availability of evaluation data, it would be best to refer to the source code repository or contact the authors directly."
}