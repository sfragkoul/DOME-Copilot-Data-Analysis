{
  "publication/title": "Long-Term Predictive Modelling of the Craniofacial Complex Using Machine Learning on 2D Cephalometric Radiographs.",
  "publication/authors": "Myers M, Brown MD, Badirli S, Eckert GJ, Johnson DH, Turkkahraman H",
  "publication/journal": "International dental journal",
  "publication/year": "2025",
  "publication/pmid": "39757033",
  "publication/pmcid": "PMC11806318",
  "publication/doi": "10.1016/j.identj.2024.12.023",
  "publication/tags": "- Artificial intelligence\n- Machine learning\n- Cephalometric analysis\n- Craniofacial complex\n- Growth and development\n- Orthodontics\n- Predictive modeling\n- Long-term growth\n- Skeletal relationships\n- Dental relationships",
  "dataset/provenance": "The dataset for this study was sourced from the American Association of Orthodontists Foundation (AAOF) Craniofacial Legacy Collection. This collection includes data from various longitudinal growth studies, such as the Bolton Brush Growth, Burlington Growth, Denver Growth, Fels Longitudinal, Forsyth Twin, Iowa Growth, Matthews Growth, Michigan Growth, and Oregon Growth studies. The dataset consists of lateral cephalometric radiographs (LCRs) from 301 subjects, with an equal representation of males and females. These subjects had skeletal Class I, II, or III relationships and had LCRs available at both pre-puberty (T1) and post-puberty (T2) time points. The mean age at T1 was 11.0 years, and at T2, it was 18.0 years. The dataset was carefully curated to exclude subjects with craniofacial anomalies, prior orthodontic treatment, missing teeth (excluding third molars), or incomplete records. This ensures a high-quality and standardized sample for the study. The dataset has not been used in previous publications by the authors, but it has been utilized by the orthodontic community for various research purposes. The AAOF Craniofacial Legacy Collection is a valuable resource that provides a comprehensive set of growth data, enabling robust analysis and modeling of craniofacial growth patterns.",
  "dataset/splits": "The dataset was divided into two primary splits: a training set and a testing set. The training set consisted of 240 subjects, while the testing set comprised 61 subjects. The training set included records from both the initial time point (T1) and the follow-up time point (T2), allowing the machine learning algorithms to learn from the complete data. In contrast, the testing set only included the T1 timepoint, enabling the models to predict the T2 outcomes independently. This approach ensured that the models could be evaluated on their ability to make accurate predictions based on initial data alone. Additionally, a 5-fold cross-validation approach was applied to the training set to enhance the robustness of the models and address potential biases introduced by random splits. This technique involved using different subsets of the training data for validation and training in each iteration, providing a more reliable assessment of model performance and improving the generalizability of the predictions.",
  "dataset/redundancy": "The dataset used in this study was divided into two independent sets: a training set and a testing set. The training set consisted of 240 subjects, while the testing set included 61 subjects. This split was done randomly to ensure that the models could be trained on a substantial amount of data while also having a separate set to evaluate their performance.\n\nThe independence of the training and testing sets was enforced by ensuring that the testing set only included data from the T1 timepoint. This allowed the models to predict the T2 outcomes independently, without any overlap or contamination from the training data. This approach helps in assessing the generalizability and robustness of the models.\n\nRegarding the distribution, the dataset included a diverse sample of both genders and all types of malocclusions. This diversity enhances the generalizability of the findings, as it covers a wide range of craniofacial growth patterns. The inclusion of subjects with skeletal Class I, II, and III relationships further ensures that the models can handle various types of malocclusions, making the results more applicable to a broader population.\n\nCompared to previously published ML datasets in this field, this study stands out due to its focus on long-term growth changes over an 8-year period. Most prior research has concentrated on shorter prediction intervals, such as 2 or 4 years. Additionally, the comprehensive set of cephalometric linear and angular measurements used in this study provides a more holistic understanding of craniofacial growth dynamics. This broader perspective is crucial for developing accurate and reliable predictive models for orthodontic interventions.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study were traditional regression models, specifically Lasso, Random Forest, and Support Vector Regression (SVR). These algorithms are well-established and widely used in the field of machine learning.\n\nLasso, or Least Absolute Shrinkage and Selection Operator, is a type of linear regression that includes a regularization term to penalize the absolute size of the regression coefficients. This helps in feature selection by driving some coefficients to zero.\n\nRandom Forest is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees. It is known for its ability to handle large datasets and high-dimensional spaces, and it reduces overfitting by averaging the results of multiple trees.\n\nSupport Vector Regression (SVR) is a type of Support Vector Machine (SVM) used for regression tasks. It works by finding a hyperplane that best fits the data within a specified margin of error. SVR is particularly effective for small datasets and complex regression tasks.\n\nThese algorithms are not new; they have been extensively studied and applied in various domains. The choice to use these established algorithms was driven by their proven effectiveness in handling different types of data and their ability to capture both linear and non-linear relationships. The focus of this study was on applying these algorithms to predict long-term growth-related changes in skeletal and dental relationships within the craniofacial complex, rather than developing a new machine-learning algorithm. Therefore, publishing in a machine-learning journal was not the primary goal. Instead, the results were presented in the context of orthodontics and craniofacial growth prediction, highlighting the practical applications of these algorithms in a clinical setting.",
  "optimization/meta": "Not applicable. The study does not discuss the use of a meta-predictor. The research utilized three traditional regression models: Lasso, Random Forest, and Support Vector Regression (SVR). These models were trained and tested independently on the dataset, which was divided into training and testing sets. The training set included records from both T1 and T2 timepoints, while the testing set only included T1 timepoint data to predict T2 outcomes. The models were evaluated using various statistical methods, including mean absolute errors, intraclass correlation coefficients, and repeated measures ANOVA, to compare their performance. However, there is no indication that the study employed a meta-predictor that combines the outputs of these models or uses data from other machine-learning algorithms as input.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithms involved several steps to ensure the quality and consistency of the input data. Initially, any magnification errors in the cephalometric radiographs that could not be corrected by adjusting the DPI to its pre-set value were discarded. This step was crucial to maintain the accuracy of the measurements.\n\nFor intra-reliability testing, a subset of 20 images was randomly selected and re-traced by the researcher. This process helped in evaluating the reproducibility of the measurements, with intraclass correlation coefficients (ICCs) used to assess the consistency. Descriptive statistics, including mean, standard deviation, minimum, and maximum values, were calculated for the sample data at two time points, T1 (pre-pubertal) and T2 (post-pubertal).\n\nThe dataset was then randomly divided into two parts: 240 subjects for training and 61 subjects for testing. For the training set, records from both T1 and T2 were provided to the machine-learning algorithms to enable model training. In contrast, for the testing set, only the T1 timepoint data was used, allowing the models to predict the T2 outcomes independently. This approach ensured that the models could generalize well to new, unseen data.\n\nThree traditional regression models were utilized: Lasso, Random Forest, and Support Vector Regression (SVR). Linear Regression with L1 (Lasso) and L2 (Ridge) regularizers was employed to investigate possible linear associations in the data. For non-linear relationships, SVR and Random Forest were used. SVR, a kernel-based algorithm, was particularly suitable for small datasets and complex regression tasks, while Random Forest, an ensemble of decision trees, helped mitigate overfitting and improve prediction accuracy.\n\nTo further ensure the robustness of the models, a 5-fold cross-validation approach was applied to both the training and testing sets. This technique involved using different subsets of the data for validation and training in each iteration, providing a more reliable assessment of model performance and enhancing the generalizability of the predictions. All experiments were conducted using Python 3.7.9 and Scikit-learn version 1.0.2 in the Spyder 4.1.5 environment.",
  "optimization/parameters": "In our study, the number of input parameters (p) used in the model varied depending on the specific measurement being predicted. The models utilized pre-pubertal values of various measurements and sex as the most important predictors. However, the complexity of the relationships differed among measurements. For instance, maxillary length, mandibular length, and upper incisor inclination exhibited complexity in terms of the number of measurements utilized in their predictive algorithms.\n\nThe selection of parameters was informed by the feature importance analysis, which assessed the weights assigned to variables by the Lasso model. This analysis highlighted the interdependence between maxillary and mandibular relationships and the reliance of dental relationships on other dental measures. The models were trained using records from both pre-pubertal (T1) and post-pubertal (T2) timepoints, allowing them to learn the most relevant features for prediction. The use of cross-validation techniques, including 5-fold cross-validation, further ensured the robustness of the models and the generalizability of the predictions.",
  "optimization/features": "The study utilized 16 linear and angular measurements as input features for the predictive models. These features were derived from lateral cephalometric radiographs and included various skeletal and dental relationships.\n\nFeature selection was implicitly performed through the use of different machine learning algorithms. The Lasso model, in particular, is known for its ability to perform feature selection by shrinking the coefficients of less important features to zero. This process was conducted using the training set only, ensuring that the feature importance was assessed independently of the testing data.\n\nThe feature importance analysis revealed that pre-pubertal values of the measurements and sex were consistently the most important predictors of post-pubertal values. This indicates that the selected features were effective in capturing the relevant information for predicting long-term growth changes in the craniofacial complex.",
  "optimization/fitting": "The study utilized a dataset of 301 subjects, which was divided into 240 subjects for training and 61 subjects for testing. This division was done randomly, ensuring that the models were trained on a substantial amount of data while also having a separate set for unbiased evaluation.\n\nThe number of parameters in the models was not excessively large compared to the number of training points. To mitigate overfitting, several techniques were employed. Firstly, a 5-fold cross-validation approach was applied to the training set. This method involved splitting the training data into five subsets, training the model on four subsets, and validating it on the remaining subset. This process was repeated five times, with each subset serving as the validation set once. This technique helped in ensuring that the model generalizes well to unseen data.\n\nAdditionally, regularization techniques were used in the linear regression models. Lasso (L1 regularization) and Ridge (L2 regularization) were employed to penalize large coefficients, thereby preventing the model from becoming too complex and overfitting the training data.\n\nFor non-linear models, such as Support Vector Regression (SVR) and Random Forest, the study utilized these algorithms due to their robustness in handling complex relationships within the data. SVR, being a kernel-based algorithm, constructs a hyperplane that maximizes the margin between predicted and actual values, which is particularly effective for small datasets and complex regression tasks. Random Forest, an ensemble of decision trees, helps in mitigating overfitting by averaging the predictions of multiple trees, thus improving prediction accuracy for non-linear relationships.\n\nTo address potential biases introduced by random splits, a k-fold cross-validation approach was also applied to the training set, with k = 5. This technique ensured that different subsets of the training data were used for validation and training in each iteration, providing a more reliable assessment of model performance and enhancing the generalizability of the predictions.\n\nUnderfitting was addressed by using a variety of models, including both linear and non-linear methods. The study employed Linear Regression for linear associations, while SVR and Random Forest were used for data that deviated from linear trajectories. This approach ensured that the models could capture both simple and complex relationships within the dataset, thereby avoiding underfitting.\n\nIn summary, the study employed a combination of cross-validation techniques, regularization methods, and a diverse set of models to ensure that the predictions were robust, generalizable, and neither overfitted nor underfitted to the training data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was regularization, specifically L1 (Lasso) and L2 (Ridge) regularizers in the context of Linear Regression. These regularizers help to penalize large coefficients, thereby reducing the complexity of the model and preventing it from fitting the noise in the training data.\n\nAdditionally, we utilized non-linear methodologies such as Support Vector Regression (SVR) and Random Forest. SVR, being a kernel-based algorithm, constructs a hyperplane that maximizes the margin between predicted and actual values, which is particularly effective for small datasets and complex regression tasks. Random Forest, an ensemble of decision trees, was employed to mitigate overfitting by averaging the results of multiple trees, thus improving prediction accuracy for non-linear relationships within the dataset.\n\nTo further ensure the robustness of the models, we applied a 5-fold cross-validation approach. This technique involves dividing the dataset into five subsets, training the model on four subsets, and validating it on the remaining subset. This process is repeated five times, with each subset serving as the validation set once. This method provides a more reliable assessment of model performance and enhances the generalizability of the predictions.\n\nMoreover, we conducted a k-fold cross-validation approach with k = 5 on the training set. This technique enabled the use of different subsets of the training data for validation and training in each iteration, providing a more comprehensive evaluation of model performance and reducing the risk of overfitting.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in this study are not entirely black-box, as we have utilized techniques to assess and interpret feature importance. Specifically, the Lasso model provided insights into the most influential predictors by analyzing the weights assigned to variables. In nearly all cases, pre-pubertal values of the measurements and sex emerged as the most important predictors of post-pubertal values. This transparency is crucial for understanding the underlying factors driving the predictions.\n\nFor instance, the Lasso model consistently highlighted the significance of pre-pubertal measurements and sex in predicting various craniofacial outcomes. This aligns with established clinical findings, where early craniofacial dimensions and sex-related growth patterns play pivotal roles. The model's ability to identify these key features enhances its interpretability and clinical relevance.\n\nAdditionally, the interdependence between maxillary and mandibular relationships was evident, showcasing the intricate connections within the craniofacial complex. Dental relationships, on the other hand, predominantly relied on other dental measures for prediction. This nuanced understanding of feature importance helps in validating the models' outputs and ensures that the predictions are not only statistically sound but also clinically meaningful.\n\nThe use of multiple machine learning models, including Lasso, Random Forest, and Support Vector Regression, further aids in interpretability. Each model brings unique strengths: Lasso's ability to identify key features, Random Forest's capacity to capture complex interactions, and SVR's suitability for small datasets and complex regression tasks. This complementary nature ensures that both linear and non-linear relationships are effectively captured, providing a robust framework for predictive modeling.\n\nIn summary, while the models are sophisticated, the techniques used to assess feature importance make them more transparent. This transparency is essential for clinical applications, where understanding the underlying factors driving predictions is crucial for informed decision-making.",
  "model/output": "The models employed in this study are regression models. Specifically, three traditional regression models were utilized: Lasso, Random Forest, and Support Vector Regression (SVR). These models were used to predict long-term growth changes in skeletal and dental relationships within the craniofacial complex. The dataset was divided into training and testing sets, with the training set providing records from both pre-puberty (T1) and post-puberty (T2) timepoints to enable model training. The testing set, however, only included the T1 timepoint, allowing the models to predict the T2 outcomes independently. The performance of these models was evaluated using various metrics, including mean absolute errors (MAEs), intraclass correlation coefficients (ICCs), and t-tests. Additionally, the percentage of subjects with predicted growth measurements within specified margins of error was calculated, along with 95% confidence intervals. The results indicated that the models showed moderate to good correlation in their predictions, with varying degrees of accuracy across different measurements.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models and algorithms used in this study is not publicly released. The experiments were conducted using Spyder 4.1.5, utilizing the programming language Python 3.7.9 and Scikit-learn version 1.0.2. However, no specific method to run the algorithm, such as an executable, web server, virtual machine, or container instance, has been made available to the public. Therefore, the models and their implementations are not accessible for external use or verification.",
  "evaluation/method": "The evaluation of the method involved several rigorous steps to ensure the robustness and accuracy of the models. The dataset was randomly divided into two subsets: 240 subjects for training and 61 subjects for testing. This split allowed for a comprehensive assessment of the models' performance on unseen data.\n\nFor the testing set, the 95% confidence interval (CI) for the percentage of subjects with predicted growth measurements within a 2 mm or 2\u00b0 margin of error was calculated. This interval had a width of 25% or less, extending from 37.5% to 62.5%, if the percentage was 50%. Similarly, the 95% CI for the intraclass correlation coefficients (ICCs) had a width of 0.19, extending from 0.69 to 0.88, if the ICC was 0.80. These intervals provided a clear understanding of the models' predictive accuracy and reliability.\n\nThe training set included records from both the pre-pubertal (T1) and post-pubertal (T2) timepoints, enabling the machine learning algorithms to learn from the complete dataset. In contrast, the testing set only included the T1 timepoint, allowing the models to predict the T2 outcomes independently. This approach simulated a real-world scenario where predictions are made based on initial data without access to future measurements.\n\nThree traditional regression models were utilized: Lasso, Random Forest, and Support Vector Regression (SVR). Linear Regression with L1 (Lasso) and L2 (Ridge) regularizers was employed to investigate possible linear associations. For data that deviated from linear trajectories, non-linear methodologies such as SVR and Random Forest were used. SVR, a kernel-based algorithm, was particularly suitable for small datasets and complex regression tasks. Random Forest, an ensemble of decision trees, helped mitigate overfitting and improve prediction accuracy for non-linear relationships.\n\nA 5-fold cross-validation was applied to the test set to identify the optimal model parameters. Additionally, a k-fold cross-validation approach (with k = 5) was applied to the training set to ensure the robustness of the models and address potential biases introduced by random splits. This technique involved using different subsets of the training data for validation and training in each iteration, providing a more reliable assessment of model performance and enhancing the generalisability of the predictions.\n\nStatistical analysis included calculating the difference between predicted and actual measurements, summarizing mean absolute errors (MAEs), and using paired t-tests to detect consistent differences (bias). Repeated measures analysis of variance (ANOVA) was used to compare the three algorithms for differences between predicted and actual measurements. Bland-Altman plots and ICCs were used to evaluate the agreement between predicted and actual measurements. The percentage of subjects with predicted growth measurements within 2 mm or 2\u00b0, and 3 mm or 3\u00b0 margins of error were calculated, along with 95% confidence intervals. McNemar\u2019s tests were used to compare the algorithms for differences in the percentage of subjects within these margins of error. All tests were conducted with a 2-sided 5% significance level, and analyses were performed using SAS version 9.4.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the accuracy and reliability of our machine learning models in predicting long-term growth changes in craniofacial structures. The primary metrics reported include Mean Absolute Error (MAE), Intraclass Correlation Coefficients (ICCs), and results from paired t-tests.\n\nMAE was calculated to quantify the average magnitude of errors between the predicted and actual measurements, providing a clear indication of the models' predictive accuracy. This metric is crucial for understanding the average deviation of predictions from the true values, and it was reported for various craniofacial measurements, including incisor positions and angular measurements.\n\nICCs were utilized to assess the agreement between the predicted and actual measurements. These coefficients range from 0 to 1, with higher values indicating better agreement. Our results showed moderate to good correlation for most measurements, underscoring the models' reliability in predicting craniofacial growth.\n\nPaired t-tests were conducted to determine if there was a consistent difference (bias) between the predicted and actual measurements. This statistical test helped us identify any systematic errors in the models' predictions, ensuring that the differences were not due to random variation.\n\nAdditionally, we used repeated measures analysis of variance (ANOVA) to compare the three algorithms (Lasso, Random Forest, and Support Vector Regression) for differences between the predicted and actual measurements. This analysis provided insights into the performance variability across different models and measurements.\n\nBland-Altman plots were employed to visually evaluate the agreement between the predicted and actual measurements, offering a graphical representation of the data's distribution and potential outliers.\n\nThe percentage of subjects with predicted growth measurements within clinically acceptable margins of error (2 mm or 2\u00b0, and 3 mm or 3\u00b0) was also calculated. This metric is particularly relevant for clinical applications, as it indicates the proportion of predictions that fall within acceptable ranges for orthodontic interventions.\n\nMcNemar\u2019s tests were used to compare the three algorithms for differences in the percentage of subjects with predicted growth measurements within the specified clinical thresholds. This statistical test helped us determine if there were significant differences in performance between the models.\n\nOverall, the set of metrics reported in our study is representative of standard practices in the literature, ensuring a thorough and comprehensive evaluation of the models' performance. These metrics provide a robust framework for assessing the accuracy, reliability, and clinical relevance of our predictive models.",
  "evaluation/comparison": "The evaluation of the models involved a comparison of three different algorithms: Lasso, Random Forest, and Support Vector Regression (SVR). These algorithms were chosen to cover both linear and non-linear relationships within the dataset. The comparison was conducted using repeated measures analysis of variance (ANOVA) to assess differences between the predicted and actual measurements. This statistical method allowed for a thorough evaluation of how each algorithm performed across various craniofacial measurements.\n\nThe results of the repeated measures ANOVA indicated statistically significant differences between the models for several key measurements. For instance, SVR showed significantly lower absolute differences than Lasso and Random Forest for the mandible to cranial base angle. Conversely, Random Forest had higher absolute differences than Lasso and SVR for measurements such as the mandibular plane angle, maxilla to mandible angle, lower incisor position, and posterior face height. These findings highlight the strengths and weaknesses of each algorithm in predicting different aspects of craniofacial growth.\n\nThe comparison also included an assessment of feature importance, which was analyzed using the weights assigned to variables by the Lasso model. This analysis revealed that pre-pubertal values of the measurements and sex were consistently the most important predictors of post-pubertal values. However, there were exceptions, such as upper incisor position and posterior face height, where other factors played a more significant role. This detailed examination of feature importance provided insights into the key factors driving the predictive accuracy of the models.\n\nIn summary, the methods comparison involved a rigorous evaluation of three different algorithms using statistical techniques to identify significant differences in performance. The results provided a comprehensive understanding of how each algorithm performed across various measurements and highlighted the importance of different features in predicting craniofacial growth.",
  "evaluation/confidence": "The evaluation of our models included several performance metrics, each accompanied by confidence intervals to provide a measure of uncertainty. For instance, the 95% confidence interval for the percentage of subjects with predicted growth measurements within a 2 mm or 2\u00b0 margin of error had a width of 25%, extending from 37.5% to 62.5% if the percentage is 50%. Similarly, the 95% confidence interval for the intraclass correlation coefficients (ICCs) had a width of 0.19, extending from 0.69 to 0.88 if the ICC is 0.80.\n\nStatistical significance was assessed using various tests. A paired t-test was used to determine if there was a consistent difference (bias) between the predicted and actual measurements. Repeated measures analysis of variance (ANOVA) was employed to compare the three algorithms (Lasso, Random Forest, and SVR) for differences between the predicted and actual measurements. McNemar\u2019s tests were used to compare the algorithms for differences in the percentage of subjects with predicted growth measurements within specified clinical thresholds.\n\nThe results indicated that most measurements showed no statistically significant difference between the actual and predicted values (P > .05), except for specific cases such as the lower incisor to mandibular plane angle for all three methods, and the mandibular plane angle and lower incisor inclination for SVR (P < .05). This suggests that while the models generally performed well, there were instances where the predictions deviated significantly from the actual measurements.\n\nOverall, the performance metrics and statistical tests provided a comprehensive evaluation of the models' accuracy and reliability. The inclusion of confidence intervals and significance testing ensured that the claims of superiority over baselines and other methods were supported by robust statistical evidence.",
  "evaluation/availability": "Not applicable."
}