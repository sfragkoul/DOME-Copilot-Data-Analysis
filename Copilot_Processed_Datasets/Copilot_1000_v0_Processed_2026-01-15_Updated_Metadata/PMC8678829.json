{
  "publication/title": "Detection of abnormal left ventricular geometry in patients without cardiovascular disease through machine learning: An ECG-based approach.",
  "publication/authors": "Angelaki E, Marketou ME, Barmparis GD, Patrianakos A, Vardas PE, Parthenakis F, Tsironis GP",
  "publication/journal": "Journal of clinical hypertension (Greenwich, Conn.)",
  "publication/year": "2021",
  "publication/pmid": "33507615",
  "publication/pmcid": "PMC8678829",
  "publication/doi": "10.1111/jch.14200",
  "publication/tags": "- Machine Learning\n- Random Forest\n- Cardiovascular Disease\n- Echocardiography\n- Feature Importance\n- SHAP Values\n- Medical Diagnosis\n- Data Classification\n- Hypertension\n- Predictive Modeling",
  "dataset/provenance": "The dataset used in this study was derived from electrocardiogram (ECG) measurements. These measurements were based on representative complexes of 1-second duration for each lead, produced by averaging over 10 seconds. The dataset was split into a training set (80%) and a test set (20%). The test set consisted of data that the model had not seen during training and was used exclusively for performance evaluation of the final model. The reported metrics are based on the test set.\n\nThe initial dataset consisted of 60 features, but through feature selection, the model was trained on 32 features. This selection process involved eliminating irrelevant features and correcting for high correlation among some of the features, as assessed by Pearson's correlation test. A Pearson coefficient greater than 0.90 was used as the threshold for removal.\n\nThe dataset included a variety of clinical, anthropometric, and ECG data. Some of the features used as inputs to the machine learning model included sex, age, history of hypertension, body mass index (BMI) class, height, body surface area (BSA), and various ECG measurements such as P wave duration, QRS interval duration, QT-interval corrected for heart rate, and others.\n\nThe dataset was stratified for sex, NG class, CR+LVH class, and BMI class while splitting, ensuring that the training and test sets had the same proportions of these features as the original dataset. This stratification helped in maintaining the representativeness of the dataset across different subsets.\n\nThe dataset was not explicitly mentioned to have been used in previous papers or by the community. However, the methods and features used are consistent with standard practices in the field of machine learning and cardiology. The dataset was specifically curated for this study to address the research questions and hypotheses being investigated.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a test set. The original dataset was partitioned such that 80% of the data was allocated to the training set, and the remaining 20% was designated for the test set. The test set comprised data that the model had not encountered during the training phase, ensuring an unbiased evaluation of the model's performance. The reported metrics and performance results are based on the test set.\n\nStratification was applied during the splitting process to maintain the same proportions of key features\u2014such as sex, NG class, CR+LVH class, and BMI class\u2014in both the training and test sets as in the original dataset. This approach helped to ensure that the model's performance was evaluated on a representative sample of the data.",
  "dataset/redundancy": "The dataset was divided into a training set and a test set. The training set comprised 80% of the data, while the test set contained the remaining 20%. The test set was specifically designed to include data that the model had not encountered during the training phase. This ensured that the test set was independent of the training set, allowing for an unbiased evaluation of the model's performance.\n\nTo maintain the integrity of the dataset distribution, stratification was employed. This process ensured that the training and test sets had the same proportions of key features as the original dataset. Specifically, stratification was applied based on sex, NG class, CR+LVH class, and BMI class. This approach helped to preserve the representativeness of the dataset, ensuring that the model's performance metrics were reliable and generalizable.\n\nThe distribution of the dataset in this study is comparable to other machine learning datasets in the medical field, where maintaining the balance of key features is crucial for accurate model evaluation. By using stratification, we aimed to mitigate the risk of overfitting and ensure that the model's predictions were robust and applicable to real-world scenarios.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used is ensemble learning, specifically the random forest (RF) method. This is a well-established technique in the field of machine learning and is not a new algorithm. Random forests are known for their robustness and ability to handle complex datasets, making them suitable for a variety of applications, including medical research.\n\nThe random forest algorithm was chosen after evaluating other relevant approaches, such as boosting algorithms and support vector classifiers (SVCs). The random forest method was selected because it provided the best performance for the dataset in question and offered interpretability through SHAP (SHapley Additive exPlanations) tools. This interpretability is particularly important in medical applications, where understanding the factors contributing to predictions is crucial.\n\nThe decision to use random forests was supported by their effectiveness in handling smaller datasets and their ability to provide feature importance measures. These measures help in understanding which features drive the model's predictions, which is essential for medical research. The random forest algorithm was implemented using scikit-learn, an open-source Python package for machine learning. This package is widely used and trusted in the machine learning community, ensuring the reliability and reproducibility of the results.\n\nThe random forest method was optimized using cross-validation with grid search to tune hyperparameters and minimize overfitting. The out-of-bag (oob) validation procedure was used to select the number of trees, with the highest number of trees chosen based on research recommendations. This approach ensures that the model is both accurate and generalizable to new data.",
  "optimization/meta": "The model employed in our study is a Random Forest (RF), which is an ensemble-based supervised machine learning algorithm. It consists of a collection of de-correlated decision trees. Each decision tree performs a series of binary decisions, or splits, by selecting a subgroup of input features, effectively trying out different feature orders and combinations. The probabilities estimated by each tree are aggregated, and the class with the highest probability is predicted.\n\nThe RF model does not use data from other machine-learning algorithms as input. Instead, it directly uses the original dataset, which was split into a training set (80%) and a test set (20%). The test set consisted of data the model had not seen during training and was used exclusively for performance evaluation of the final model.\n\nThe RF model was chosen after evaluating other relevant approaches, such as a Boosting algorithm (specifically, Catboost) and a Support Vector Classifier (SVC). Catboost obtained an accuracy of 83%, while the SVC produced a mean accuracy of 83% and an AUC(ROC) value of 0.80, both models on the test set for NG vs. CR+LVH. However, the RF was the best-performing model for our dataset and also provided interpretability through the SHAP tools.\n\nThe training data for the RF model was independent, as stratification for sex, NG class, CR+LVH class, and BMI class was done while splitting the dataset. This ensured that the training and test sets had the same proportions of these features as the original dataset. The reported performance results and feature importance graphs are based on the test set to avoid inflating the importance of some features that might not be as important in predicting the outcome.",
  "optimization/encoding": "In our study, the data encoding and preprocessing were crucial steps to ensure the machine-learning algorithm could effectively learn from the input features. We began by extracting additional ECG waveform measurements from 1-second representative complexes for each lead, which were produced by the electrocardiograph. These measurements included areas under curves, slopes, and peak heights of curves, among others.\n\nTo handle the high dimensionality of the data, we performed feature selection. This involved eliminating irrelevant features and correcting for high correlation among some of the features using Pearson's correlation test. We retained only one of two correlated features, keeping the one that provided the most information and reducing redundancy.\n\nThe dataset was split into a training set (80%) and a test set (20%). The test set consisted of data that the model had not seen during training, ensuring an unbiased evaluation of the model's performance. Stratification was done for sex, NG class, CR+LVH class, and BMI class to maintain the same proportions of these features in both the training and test sets as in the original dataset.\n\nFor the machine-learning model, we used a random forest (RF) algorithm, which is an ensemble-based supervised learning method. The RF algorithm consists of a collection of de-correlated decision trees, each performing a series of binary decisions by selecting subgroups of input features. This approach allows the model to handle smaller datasets effectively through a technique called bootstrap aggregating, or bagging, which trains multiple trees on overlapping, randomly selected subsets of the data.\n\nHyperparameter tuning was performed using cross-validation with grid search to optimize the model parameters and minimize overfitting. The number of trees in the random forest was selected using the model's internal out-of-bag (oob) validation procedure, which provides an estimate of the model's performance on unseen data.\n\nFeature importance was assessed using SHAP (SHapley Additive exPlanations), a game-theoretic approach that provides insight into which features drive the model's predictions. SHAP values were computed to explain the output of the machine-learning model, connecting optimal credit allocation with local explanations using Shapley values from game theory.\n\nIn summary, the data encoding and preprocessing involved extracting relevant ECG features, performing feature selection to reduce dimensionality, splitting the dataset into training and test sets with stratification, and using a random forest algorithm with hyperparameter tuning and SHAP for feature importance assessment. These steps ensured that the machine-learning model could effectively learn from the input data and provide reliable predictions.",
  "optimization/parameters": "In our study, the model was initially trained using a set of 60 features. However, through a feature selection process, we reduced this number to 32 features. This selection was based on eliminating irrelevant features and correcting for high correlation among some of the features, as assessed by Pearson's correlation test. A Pearson coefficient greater than 0.90 was used as the threshold for removal, ensuring that only the most relevant features were retained. This dimensionality reduction helped in improving the model's performance and interpretability.\n\nThe final model was trained on these 32 selected features, which included a mix of clinical, anthropometric, and ECG data. This careful selection process ensured that the model was not overfitted and could generalize well to new, unseen data. The features were chosen to provide a comprehensive view of the patient's health status, including factors like age, BMI class, hypertension history, and various ECG measurements. This approach allowed us to build a robust model that could accurately classify patients into different categories based on their left ventricular geometry.",
  "optimization/features": "The model was initially trained using 60 features. Feature selection was performed to reduce the dimensionality of the feature space. This process involved eliminating irrelevant features and correcting for high correlation among some of the features, as assessed by Pearson's correlation test. A Pearson coefficient threshold of greater than 0.90 was used for feature removal. As a result, the final model was trained on 32 features. Feature selection was conducted using the training set only.",
  "optimization/fitting": "The fitting method employed in our study utilized a Random Forest (RF) algorithm, which is an ensemble-based supervised machine learning technique. This method consists of a collection of de-correlated decision trees, each performing a series of binary decisions by selecting subgroups of input features. The RF algorithm aggregates the probabilities from all the trees to yield the predicted class label.\n\nTo address the potential issue of overfitting, especially given the relatively large number of features compared to the number of training points, we implemented several strategies. Firstly, we used bootstrap aggregating, or bagging, which trains multiple trees on overlapping, randomly selected subsets of the data. This technique helps to reduce overfitting by ensuring that each tree is trained on a different subset of the data.\n\nAdditionally, we employed cross-validation with grid search for hyperparameter tuning. This approach helps to overcome the problem of overfitting by systematically working through multiple combinations of parameter tunes, and determining the best combination of hyperparameters that generalizes well to unseen data.\n\nFor model selection, specifically choosing the number of trees, we utilized the internal out-of-bag (oob) validation procedure. The oob error was plotted for a wide range of tree number values, and we selected the highest number of trees, as recommended by some researchers. This procedure allows for training and cross-validation in one pass, ensuring that the model generalizes well to the test set.\n\nFurthermore, we optimized model parameters by minimizing the RF\u2019s built-in out-of-bag error estimate, which is almost identical to that obtained by N-fold cross-validation. This method ensures that the model is not overfitting to the training data.\n\nTo rule out underfitting, we carefully selected and engineered features that were relevant to the problem at hand. We started with an initial set of 60 features and reduced it to 32 features through feature selection, eliminating irrelevant features and correcting for high correlation among some of the features. This process ensured that the model had enough relevant information to make accurate predictions without being too simplistic.\n\nIn summary, the combination of bagging, cross-validation with grid search, oob validation, and careful feature selection helped us to effectively manage and mitigate both overfitting and underfitting in our model.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning model. One of the key methods used was cross-validation with grid search. This approach helps to optimize hyperparameters and reduces the risk of overfitting by evaluating the model's performance on multiple subsets of the data.\n\nAdditionally, we utilized the out-of-bag (oob) validation procedure, which is an internal validation method specific to random forests. This technique allows for the estimation of the model's performance without the need for a separate validation set, thereby making efficient use of the available data.\n\nFurthermore, we implemented bootstrap aggregating, or bagging, which involves training multiple decision trees on overlapping, randomly selected subsets of the data. This method helps to improve the model's generalization ability by reducing variance and preventing overfitting.\n\nTo select the optimal number of trees in our random forest model, we plotted the oob error for a wide range of tree numbers and chose the configuration that minimized this error. This process ensured that our model was neither too simple nor too complex, striking a balance that enhanced its predictive accuracy and generalization to unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we employed a random forest (RF) model, and the process involved hyperparameter tuning using cross-validation with grid search. This approach aimed to mitigate overfitting by systematically working through multiple combinations of parameter tunes.\n\nFor model selection, particularly in determining the number of trees, we utilized the internal out-of-bag (oob) validation procedure. This method allowed us to plot the oob error across a range of tree numbers, ultimately leading us to choose the highest number of trees, a strategy supported by some researchers as the optimal approach.\n\nThe specific details of the hyper-parameter configurations, including the ranges tested and the final selected values, are provided in the methods section of the paper. Additionally, the optimization parameters and the process for feature selection, such as the use of a Pearson coefficient threshold for feature removal, are thoroughly documented.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly provided in the publication. However, the methods and configurations described are reproducible using standard machine learning libraries, such as scikit-learn, which is an open-source Python package. This ensures that researchers can implement and validate our findings using the same or similar tools.\n\nNot sure about the license under which the model files or optimization parameters would be shared, as this information is not specified in the provided context.",
  "model/interpretability": "Our model, a Random Forest (RF), is not a black box but rather provides a level of transparency through several interpretability techniques. One key method we employed is SHAP (SHapley Additive exPlanations), a game-theoretic approach that explains the output of any machine learning model. SHAP values help in understanding the contribution of each feature to the model's predictions, both globally and locally. This allows us to see not only which features are important overall but also how specific features influence individual predictions.\n\nFor instance, in our binary classification task of distinguishing between normal geometry (NG) and combined concentric remodeling (CR) and left ventricular hypertrophy (LVH), SHAP values revealed that hypertension has a strong positive effect on being classified as CR+LVH. Additionally, age plays a significant role, with a notable cutoff around 65 years, where the risk dynamics differ between men and women. These insights were visualized using SHAP summary plots, which show the effect of each feature on the model's output.\n\nFurthermore, we used feature importance graphs to provide a clearer picture of each feature's contribution. We calculated mean decrease in impurity (MDI) and permutation importance, but we found SHAP values to be more accurate for both global and local feature importance. This approach allowed us to identify key features such as age, hypertension status, and QTc duration, which significantly impact the model's predictions.\n\nIn summary, our model's transparency is enhanced by the use of SHAP values and feature importance graphs, which provide clear and actionable insights into the factors driving our predictions. This level of interpretability is crucial, especially in medical applications, where understanding the underlying patterns is often as important as the model's predictive performance.",
  "model/output": "The model developed is a classification model. It was trained to categorize individuals into different classes based on their health conditions. Specifically, the model was designed to handle three types of classification tasks:\n\n1. Binary classification to distinguish between normal individuals (NG) and those with either concentric remodeling (CR) or left ventricular hypertrophy (LVH).\n2. Binary classification to identify individuals who have already developed LVH, separating them from those who are either normal or have CR.\n3. Multiclass classification to categorize individuals into one of three classes: NG, CR, or LVH.\n\nThe model's performance was evaluated using metrics such as accuracy, sensitivity, specificity, area under the receiver operating characteristic curve (AUC/ROC), and area under the precision-recall curve (AUC/PR). For instance, in the binary classification task of NG vs. CR+LVH, the model achieved an accuracy of 87%, with a sensitivity of 97% and a specificity of 75%. The AUC/ROC for this task was 0.91, and the AUC/PR was 0.89.\n\nThe model utilized a random forest (RF) algorithm, which is an ensemble-based supervised machine learning method. This approach involves training multiple decision trees on overlapping, randomly selected subsets of the data, a technique known as bootstrap aggregating or bagging. The RF model was chosen for its robustness and ability to provide interpretability through tools like SHAP (SHapley Additive exPlanations), which help in understanding the contribution of individual features to the model's predictions.\n\nIn summary, the model is a classification model designed to categorize individuals into different health classes based on various clinical and echocardiographic features. It demonstrated strong performance in distinguishing between the specified classes, with the random forest algorithm being the selected method due to its accuracy and interpretability.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our machine learning model involved several key steps and metrics to ensure its robustness and generalizability. Initially, the dataset was split into a training set (80%) and a test set (20%). The test set, which the model had not seen during training, was used exclusively for performance evaluation. This approach helped in assessing how well the model generalizes to new, unseen data.\n\nWe employed multiple metrics to evaluate the model's performance, including accuracy, sensitivity, specificity, area under the receiver operating characteristic curve (AUC/ROC), and area under the precision-recall curve (AUC/PR). These metrics were calculated for different classification tasks: binary classification (e.g., NG vs. CR+LVH), binary classification focusing on LVH (NG+CR vs. LVH), and multiclass classification (NG vs. CR vs. LVH).\n\nFor hyperparameter tuning, we used cross-validation with grid search. This method helps in selecting the optimal parameters for the model by systematically working through multiple combinations of parameter tunes. Additionally, we utilized the out-of-bag (oob) validation procedure, which is an internal validation technique specific to random forests. This procedure allows for training and cross-validation in one pass, providing a reliable estimate of the model's performance.\n\nTo address data imbalance, particularly in the binary classification task for NG+CR vs. LVH, we performed oversampling using the Random Over Sampler. This technique helps in balancing the dataset, ensuring that the model does not become biased towards the majority class.\n\nWe also visualized feature importance using SHAP (SHapley Additive exPlanations) values. SHAP provides a game-theoretic approach to explain the output of the model, offering insights into which features drive the model's predictions. This method was particularly useful in understanding the contribution of individual features and their interactions in predicting the outcomes.\n\nIn summary, our evaluation method involved a rigorous process of splitting the dataset, using multiple performance metrics, employing cross-validation and oversampling techniques, and leveraging SHAP for feature importance visualization. These steps ensured that our model was thoroughly evaluated and validated for its predictive performance and interpretability.",
  "evaluation/measure": "In our evaluation, we reported several key performance metrics to comprehensively assess the effectiveness of our model. These metrics include accuracy, sensitivity, specificity, area under the receiver operating characteristic curve (AUC/ROC), and area under the precision-recall curve (AUC/PR). These metrics were chosen because they provide a well-rounded view of the model's performance across different aspects.\n\nAccuracy measures the overall correctness of the model's predictions, indicating the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, also known as recall, evaluates the model's ability to identify positive cases correctly, which is crucial for detecting conditions like LVH. Specificity assesses the model's ability to correctly identify negative cases, ensuring that the model does not falsely classify individuals as having a condition when they do not.\n\nThe AUC/ROC curve provides a visual representation of the model's ability to distinguish between classes across all classification thresholds, with a higher AUC indicating better performance. Similarly, the AUC/PR curve is particularly useful for imbalanced datasets, as it focuses on the performance of the model in terms of precision and recall, providing a more nuanced view of the model's effectiveness in identifying positive cases.\n\nThese metrics are widely used in the literature and are considered representative of model performance in medical and classification tasks. By reporting these metrics, we aim to provide a clear and comprehensive evaluation of our model's performance, allowing for comparisons with other studies and ensuring transparency in our results.",
  "evaluation/comparison": "In our study, we compared the performance of our selected machine learning model, the Random Forest (RF), with other relevant approaches to ensure its robustness and effectiveness. Specifically, we evaluated a Boosting algorithm and a Support Vector Classifier (SVC). The Boosting algorithm, implemented using CatBoost, achieved an accuracy of 83% on the test set for the binary classification task of distinguishing between normal geometry (NG) and combined categories of concentric remodeling (CR) and left ventricular hypertrophy (LVH). Similarly, the SVC, using the scikit-learn package, produced a mean accuracy of 83% and an AUC(ROC) value of 0.80 for the same classification task. These comparisons were crucial in validating our choice of the RF model, which not only outperformed these alternatives with an accuracy of 87% but also provided better interpretability through SHAP tools. This thorough evaluation underscores the superiority of the RF model in handling our dataset and offers insights into its generalizability and reliability in medical applications.",
  "evaluation/confidence": "The evaluation of our machine learning model focused on several key performance metrics, including accuracy, sensitivity, specificity, area under the receiver operating characteristic curve (AUC/ROC), and area under the precision-recall curve (AUC/PR). These metrics were calculated for different classification tasks, such as binary classification (e.g., NG vs. CR+LVH) and multiclass classification (e.g., NG vs. CR vs. LVH).\n\nConfidence intervals for these performance metrics were not explicitly provided in the results. However, the model's performance was assessed using a test set that consisted of 20% of the original dataset, which had not been seen during the training phase. This approach ensures that the reported metrics reflect the model's generalizability to new, unseen data.\n\nStatistical significance was not directly addressed in the context of comparing our method to others or baselines. However, the use of cross-validation with grid search for hyperparameter tuning and the internal out-of-bag (oob) validation procedure for model selection suggests a rigorous approach to model evaluation. These techniques help to mitigate overfitting and ensure that the model's performance is robust and reliable.\n\nThe model's performance was further validated through the use of SHAP (SHapley Additive exPlanations) values, which provide a game-theoretic approach to explaining the output of the machine learning model. SHAP values offer insights into the importance of individual features and their interactions, enhancing the interpretability of the model's predictions. This is particularly important in medical applications, where understanding the underlying patterns is often as crucial as the model's predictive performance.\n\nIn summary, while confidence intervals and explicit statistical significance tests were not provided, the evaluation process included robust techniques such as cross-validation, out-of-bag validation, and the use of SHAP values for feature importance. These methods collectively support the confidence in the model's performance and its potential superiority over other approaches.",
  "evaluation/availability": "Not enough information is available."
}