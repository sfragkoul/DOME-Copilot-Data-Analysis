{
  "publication/title": "Novel verbal fluency scores and structural brain imaging for prediction of cognitive outcome in mild cognitive impairment.",
  "publication/authors": "Clark DG, McLaughlin PM, Woo E, Hwang K, Hurtz S, Ramirez L, Eastman J, Dukes RM, Kapur P, DeRamus TP, Apostolova LG",
  "publication/journal": "Alzheimer's & dementia (Amsterdam, Netherlands)",
  "publication/year": "2016",
  "publication/pmid": "27239542",
  "publication/pmcid": "PMC4879664",
  "publication/doi": "10.1016/j.dadm.2016.02.001",
  "publication/tags": "- Alzheimer's disease\n- Mild cognitive impairment\n- Verbal fluency\n- Machine learning\n- Predictive modeling\n- Graph theory\n- Lexical frequency\n- Semantic similarity\n- Dementia conversion\n- Brain imaging\n- Natural language processing\n- Independent components analysis\n- Clustering\n- Switching\n- Cross-validation",
  "dataset/provenance": "The dataset used in this study was sourced from the UCLA Imaging and Genetic Biomarkers for Alzheimer\u2019s Disease (ImaGene) study. This prospective study enrolled and followed individuals recruited from two primary sources: referring neurologists and the ongoing longitudinal database study of the Alzheimer\u2019s Disease Research Center (ADRC) at UCLA. Participants were required to be at least 50 years old, able to independently carry out daily activities, and score at least 21 on the Mini-Mental State Examination (MMSE). The study included individuals diagnosed with Mild Cognitive Impairment (MCI) based on Petersen criteria, as well as cognitively normal participants. Exclusion criteria included concurrent medical problems affecting cognition, history of substance abuse, neurological or psychiatric illnesses, contraindications to MRI, significant brain changes, and sensory impairments interfering with cognitive testing.\n\nA total of 158 individuals met the inclusion criteria. Among these, 70 MCI participants were classified as \"amnestic\" due to poor memory performance, with varying degrees of impairment in other cognitive domains. The remaining 37 MCI participants were classified as nonamnestic, with impairments primarily in nonmemory domains such as executive function, language, visuospatial abilities, and attention. The dataset includes detailed clinical and cognitive examinations, blood samples, and MRI scans for each participant. The diagnoses for each subject were determined through a consensus process involving neurologists, neuropsychologists, and other key study personnel. This dataset has not been used in previous publications by the community.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is ensemble learning, which combines predictions from multiple classifiers to improve overall performance. Specifically, we employed an ensemble of classifiers with four different architectures: random forests of conditional trees, support vector machines, naive Bayes, and multilayer perceptrons. This approach allows us to leverage the strengths of various algorithms and achieve more robust predictions.\n\nThe algorithm used is not entirely new, as it builds upon established machine-learning techniques. However, the specific application and integration of these methods for predicting dementia conversion in mild cognitive impairment (MCI) patients are novel. The focus of our work is on the clinical application rather than the development of a new machine-learning algorithm per se.\n\nThe reason this work was published in a medical journal rather than a machine-learning journal is that the primary contribution lies in the clinical application and the development of new predictive scores for verbal fluency tasks. The machine-learning techniques used are well-known and have been extensively studied in the literature. Our innovation comes from applying these techniques to a specific medical problem, namely the prediction of dementia conversion in MCI patients, and from developing novel verbal fluency scores that enhance the predictive power of the models. The clinical relevance and potential impact on early detection and intervention for Alzheimer's disease are the key aspects that make this work suitable for publication in a medical journal.",
  "optimization/meta": "The model employed in this study is indeed a meta-predictor, utilizing an ensemble of individual classifiers. This ensemble consists of classifiers with four different architectures: random forests of conditional trees, support vector machines, naive Bayes, and multilayer perceptrons. Each architecture contributes multiple classifiers to the ensemble, with one classifier trained using the left-in data and ten additional classifiers trained using bootstrap samples of the left-in data. This results in a total of 44 classifiers.\n\nThe predictions from each classifier in the ensemble are obtained for both the training data and the left-out data points. These predictions are then combined linearly using sparse logistic regression, also known as LASSO (least absolute shrinkage and selection operator) regression. This approach helps in producing a lower variance model while automatically performing variable selection.\n\nThe training data for the sparse logistic regression model includes the ensemble predictions on the training data, with conversion as the outcome variable. The left-out data points, for which the ensemble has also generated predictions, are then entered into this sparse logistic regression model to obtain the final prediction. This method ensures that the training data for the meta-predictor is independent, as it relies on predictions from classifiers trained on different subsets of the data.",
  "optimization/encoding": "In our study, we employed a comprehensive approach to data encoding and preprocessing to ensure the robustness of our machine-learning algorithms. We began by computing a variety of fluency scores, both traditional and novel, along with brain imaging measurements. These scores were then consolidated into a single data matrix. Any missing values within this matrix were imputed using the mean of the nonmissing values for that particular score.\n\nTo enhance the predictive power of our models, we developed a large set of novel predictive scores for five fluency tasks, which included categories like animals, vegetables, and letters F, A, and S. These scores were derived from various methods, including clustering, switching, and independent components analysis (ICA). Additionally, we created scores based on fundamental lexical qualities such as syllable counts and word frequencies. These lexical measures were utilized in multiple ways, such as calculating averages, sums, or metric ranges for a given fluency word list.\n\nWe also leveraged graph theory to develop several novel scores. In this approach, words in each list were treated as nodes in a network, and weighted graphs were created by assigning numerical values to the edges between nodes. These values represented the semantic, orthographic, or phonologic similarity between connected words. From these weighted graphs, we derived various scores, some directly and others through conversion into unweighted graphs using similarity thresholds.\n\nFor the machine-learning process, we performed five analyses, each using a different subset of the available scores. These subsets included raw scores, brain imaging measures, a combination of raw and brain scores, novel fluency-derived scores, and a combination of novel and brain scores. Demographic variables such as age, sex, and education were included in all analyses.\n\nTo assess the quality of each classifier, we used leave-one-out cross-validation. This method involved constructing a separate classifier for each participant left out and then using that classifier to predict whether the left-out participant would convert to dementia. During each cross-validation loop, we undertook three key steps: variable selection, training of an ensemble of classifiers, and combination of the ensemble predictions through sparse logistic regression. This approach allowed us to develop a model with high predictive accuracy while automatically performing variable selection.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the analysis and the variable selection process. We employed an ensemble of classifiers consisting of four different architectures: random forests of conditional trees, support vector machines, naive Bayes, and multilayer perceptrons. For each architecture, one classifier was trained using the left-in data, and ten additional classifiers were trained using bootstrap samples of the left-in data, resulting in a total of 44 classifiers.\n\nVariable selection was performed using the random forests algorithm, which was run on the training data set with 400 trees. The importance of each variable was calculated and converted to z-scores. Separate thresholds were selected for each analysis through an iterative search over thresholds between 0 and 2.0. This process ensured that only the most relevant variables were included in the final model, thereby controlling the number of parameters.\n\nThe final predictions were obtained by combining the predictions from each classifier in the ensemble using sparse logistic regression. This method, also known as LASSO (least absolute shrinkage and selection operator) regression, automatically performs variable selection and produces a lower variance model. The sparse logistic regression model was trained using the ensemble predictions on the training data as the predictor variables, further refining the set of parameters used in the model.\n\nThe specific number of parameters (p) selected for each analysis can be found in the supplementary tables, which detail the variables selected during the different cross-validation loops. The importance values assigned to each variable from all cross-validation iterations are listed in Table 3, providing insight into the relevance of each parameter in the model.",
  "optimization/features": "In our study, we utilized a comprehensive set of features derived from verbal fluency tasks and brain imaging data. The specific number of features (f) used as input varied depending on the analysis, as we employed a rigorous feature selection process.\n\nFeature selection was indeed performed to identify the most relevant variables for our predictive models. This process involved running the random forests algorithm on the training data set with 400 trees, calculating importance for each variable, and converting these importance values to z-scores. Separate thresholds were selected for each analysis through an iterative search over thresholds between 0 and 2.0. This ensured that only the most informative features were retained for model training.\n\nTo maintain the integrity of our cross-validation process, feature selection was conducted using only the training set within each fold. This approach helped to prevent data leakage and ensured that the selected features were truly indicative of the underlying patterns in the data. By focusing on the most relevant features, we aimed to enhance the predictive performance of our ensemble classifiers.",
  "optimization/fitting": "In our study, we employed an ensemble of classifiers, which included random forests of conditional trees, support vector machines, naive Bayes, and multilayer perceptrons. This ensemble approach allowed us to leverage the strengths of multiple algorithms, thereby enhancing the robustness and accuracy of our predictions.\n\nThe number of parameters in our model was indeed larger than the number of training points, which could potentially lead to overfitting. To mitigate this risk, we utilized sparse logistic regression, also known as LASSO (Least Absolute Shrinkage and Selection Operator) regression. This technique not only reduces the variance of the model but also automatically performs variable selection, ensuring that only the most relevant features are included. By doing so, we effectively controlled for overfitting.\n\nTo further ensure the reliability of our model, we employed cross-validation techniques. Specifically, we ran the random forests algorithm on the training data set to calculate the importance of each variable. These importance values were then converted to z-scores, and thresholds were selected iteratively to optimize the model's performance. This process helped in identifying the most significant variables and prevented the model from becoming too complex.\n\nAdditionally, we trained multiple classifiers using bootstrap samples of the left-in data, which helped in creating a diverse set of models. The predictions from these classifiers were then combined linearly using sparse logistic regression, providing a final prediction for the left-out data point. This ensemble method ensured that the model was not underfitting by capturing a wide range of patterns in the data.\n\nIn summary, our approach involved using an ensemble of classifiers, sparse logistic regression for variable selection, and cross-validation techniques to balance the trade-off between overfitting and underfitting. These methods collectively ensured that our model was both robust and accurate in predicting the outcomes.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method used was sparse logistic regression, also known as LASSO (Least Absolute Shrinkage and Selection Operator) regression. This technique is particularly effective in reducing model variance by performing automatic variable selection, which helps in mitigating overfitting. By incorporating sparse logistic regression, we were able to create a more parsimonious model that generalizes better to unseen data.\n\nAdditionally, we utilized an ensemble of classifiers, which included random forests of conditional trees, support vector machines, naive Bayes, and multilayer perceptrons. This ensemble approach helps to average out the errors of individual classifiers, thereby reducing the risk of overfitting. Each classifier in the ensemble was trained using bootstrap samples of the data, further enhancing the model's ability to generalize.\n\nVariable selection was another crucial step in our process. We ran the random forests algorithm on the training data to calculate the importance of each variable. These importance values were then converted to z-scores, and thresholds were selected iteratively to ensure that only the most relevant variables were included in the final model. This rigorous variable selection process helped in reducing the complexity of the model and preventing overfitting.\n\nMoreover, we employed a cross-validation method to evaluate the performance of our classifiers. This technique involves splitting the data into multiple folds and training the model on different subsets of the data, which helps in assessing the model's performance on unseen data and ensures that it generalizes well.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the publication. Specifically, we employed an ensemble of classifiers with four different architectures: random forests of conditional trees, support vector machines, naive Bayes, and multilayer perceptrons. The variable selection process involved running the random forests algorithm on the training dataset with 400 trees and calculating importance for each variable. These importance values were then converted to z-scores, and thresholds were selected iteratively.\n\nThe ensemble predictions were combined using sparse logistic regression, which is also known as LASSO (least absolute shrinkage and selection operator) regression. This method helps in producing a lower variance model while automatically performing variable selection. The sparse logistic regression model was trained using conversion as the outcome variable and the ensemble predictions on the training data as the predictor variables.\n\nAll analyses were performed in R, utilizing additional libraries such as party, e1071, and RSNNS. The specific configurations and parameters for these models are described in the methods section of the paper. However, the exact model files and optimization schedules are not explicitly provided in the publication. For detailed information on the implementation and access to the code, interested parties may need to contact the authors directly.\n\nThe publication itself is available under standard academic publishing licenses, which typically allow for personal use and scholarly sharing, but specific details on the availability of the code and models would require direct inquiry.",
  "model/interpretability": "The model employed in this study is not a black box but rather a combination of interpretable components and techniques. The ensemble of classifiers includes architectures such as random forests of conditional trees, support vector machines, naive Bayes, and multilayer perceptrons, each of which has its own interpretability characteristics. For instance, random forests and decision trees are generally considered interpretable because they provide a clear structure of decision rules. Support vector machines, while less interpretable than decision trees, can still offer insights through their support vectors. Naive Bayes classifiers are also interpretable as they provide probabilities based on Bayes' theorem.\n\nThe variable selection process involved running the random forests algorithm to calculate importance for each variable, which was then converted to z-scores. This method helps in understanding which variables are most influential in the model's predictions. Additionally, the use of sparse logistic regression for combining ensemble predictions further aids in interpretability by performing automatic variable selection, reducing the model to a lower variance form.\n\nThe novel scores derived from verbal fluency tasks, such as measures of coherence, lexical frequency, and graph theoretical measures like algebraic connectivity and transitivity, provide clear examples of how the model incorporates interpretable features. These scores are based on well-defined linguistic and mathematical principles, making it easier to understand their contribution to the model's predictions.\n\nMoreover, the inclusion of demographic variables like age, although selected in only a few cross-validation loops, adds another layer of interpretability. The model's performance is assessed using ROC curves and various metrics like accuracy, sensitivity, and specificity, which are standard and interpretable measures in the field of machine learning.\n\nIn summary, the model's design and the techniques used ensure a level of transparency, allowing for a clear understanding of how different variables and classifiers contribute to the final predictions.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the conversion from mild cognitive impairment (MCI) to clinical Alzheimer\u2019s disease (AD). The model uses ensemble classifiers, which combine predictions from multiple individual classifiers to make a final prediction. These classifiers are trained on various groups of scores, including traditional raw scores and novel scores derived from verbal fluency tasks, as well as structural brain measurements from MRI scans. The outcome variable for the model is the conversion to AD, making it a classification problem rather than a regression problem. The performance of the model is evaluated using metrics such as the area under the receiver operating characteristic curve (AUC), sensitivity, specificity, and accuracy, which are typical for classification tasks.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the custom Python software used to calculate raw and novel scores on verbal fluency word lists is not publicly released. This software utilized the NetworkX Python library for similarity measurements between words.\n\nThe analyses were performed in R, with additional libraries including party, e1071, and RSNNS. However, the specific scripts or code used for these analyses are not made available to the public.\n\nThe methods described in the publication can be replicated using the mentioned libraries and tools, but the exact implementations are not provided. Therefore, while the general approach is detailed, the specific software and algorithms are not publicly accessible.",
  "evaluation/method": "The evaluation of our method involved a rigorous cross-validation approach to ensure the robustness and generalizability of our findings. We utilized receiver operating characteristic (ROC) curves to assess the performance of five ensemble classifiers. The area under the ROC curve (AUC) was reported for each classifier, providing a comprehensive measure of their discriminative ability. Additionally, we defined the optimal cut point for each ROC curve as the threshold that maximized the F-measure, which is the harmonic mean of sensitivity and positive predictive value. This approach allowed us to balance the trade-off between sensitivity and specificity.\n\nFor each classifier, we measured key performance metrics at the optimal cut point, including accuracy, sensitivity, specificity, negative predictive value, and positive predictive value. This detailed evaluation provided a thorough understanding of how well each classifier performed in distinguishing between different conditions.\n\nThe classifiers were trained using various combinations of novel verbal fluency scores, raw scores, and brain imaging data. The performance of these classifiers was compared to identify the most effective approach for predicting outcomes. The novel verbal fluency scores, in particular, demonstrated superior performance, achieving the highest AUC and sensitivity, which is crucial for developing an effective screening test. The raw plus brain ensemble showed the highest specificity, indicating its strength in correctly identifying true negatives.\n\nOverall, the evaluation method ensured that our findings were reliable and that the classifiers could be effectively applied in practical settings. The use of cross-validation and multiple performance metrics provided a comprehensive assessment of the classifiers' capabilities, highlighting the potential of novel verbal fluency scores in early detection and prediction of cognitive decline.",
  "evaluation/measure": "In our study, we evaluated the performance of our ensemble classifiers using several key metrics to ensure a comprehensive assessment. The primary metric reported is the Area Under the Receiver Operating Characteristic Curve (AUC), which provides a single scalar value that summarizes the performance of the classifier across all classification thresholds. This metric is crucial as it reflects the classifier's ability to distinguish between the positive and negative classes.\n\nIn addition to AUC, we also reported sensitivity (also known as recall or true positive rate), specificity (true negative rate), negative predictive value (NPV), positive predictive value (PPV), and accuracy. These metrics were measured at the optimal cut point, which was defined as the threshold that maximized the F-measure. The F-measure is the harmonic mean of sensitivity and PPV, providing a balance between these two important aspects of classifier performance.\n\nThe set of metrics we used is representative of the literature in the field of machine learning and medical diagnostics. AUC is widely used because it provides a robust measure of classifier performance, especially when dealing with imbalanced datasets. Sensitivity and specificity are essential for understanding the classifier's performance in identifying true positives and true negatives, respectively. NPV and PPV are important for assessing the reliability of the classifier's predictions in clinical settings. Accuracy gives an overall measure of the classifier's correctness.\n\nBy reporting these metrics, we aim to provide a thorough evaluation of our classifiers' performance, ensuring that our findings are both reliable and comparable to other studies in the field. This comprehensive approach allows for a better understanding of the strengths and limitations of our models, contributing to the ongoing efforts in developing effective predictive tools for medical diagnostics.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, we focused on evaluating the predictive power of novel verbal fluency scoring methods and structural brain imaging measures for predicting cognitive and functional decline in patients with mild cognitive impairment (MCI).\n\nWe compared the performance of five ensemble classifiers, each utilizing different combinations of raw scores, novel fluency scores, and brain imaging measures. The classifiers included combinations of raw scores, brain imaging data, novel fluency scores, and a combination of novel fluency scores and brain imaging data. This approach allowed us to assess the relative contributions of these different types of data to the predictive models.\n\nThe performance of these classifiers was evaluated using receiver operating characteristic (ROC) curves and various metrics such as area under the curve (AUC), sensitivity, specificity, and positive predictive value. The classifier trained solely on novel verbal fluency scores yielded the best AUC of 0.872, indicating strong predictive power. This classifier could be thresholded to achieve a sensitivity of 1.00 with a specificity of 0.675.\n\nIn addition to comparing different data types, we also evaluated the importance of various predictor variables within the novel fluency score subset. Measures of coherence, lexical frequency, and graph theoretical measures such as algebraic connectivity, radius, and transitivity were found to be particularly important. These variables were selected based on their cumulative importance values across cross-validation iterations.\n\nWhile we did not compare our methods to simpler baselines in a traditional sense, the use of raw scores and brain imaging data alone served as a baseline for evaluating the added value of the novel fluency scores. The results showed that the novel fluency scores significantly improved predictive accuracy compared to using raw scores or brain imaging data alone.\n\nOverall, our study highlights the value of brief, information-dense cognitive tests from which many potential predictor variables can be extracted. The findings contribute to the literature on machine learning for predicting outcomes in MCI by demonstrating the utility of novel verbal fluency scoring methods.",
  "evaluation/confidence": "The evaluation of our classifiers involved several performance metrics, including the area under the receiver operating characteristic curve (AUC), sensitivity, specificity, negative predictive value (NPV), positive predictive value (PPV), and accuracy. These metrics were calculated for each of the five ensemble classifiers.\n\nThe AUC values, which provide a single scalar value that summarizes the performance of the classifier across all classification thresholds, were reported for each classifier. The AUC for the classifier trained with novel verbal fluency scores was 0.872, which was significantly better than the classifier trained with raw scores (AUC 0.719, P < .05 using the DeLong test). This statistical significance indicates a strong confidence in the superiority of the novel scores classifier over the raw scores classifier.\n\nThe optimal cut point for each receiver operating characteristic (ROC) curve was defined as the threshold that maximized the F-measure, which is the harmonic mean of sensitivity and positive predictive value. This approach ensures that the reported performance metrics are based on a well-defined and consistent threshold.\n\nThe sensitivity and specificity of the classifiers were also evaluated at the optimal cut point. The novel scores classifier achieved a sensitivity of 1.00 and a specificity of 0.675, demonstrating its ability to correctly identify all positive cases while maintaining a reasonable level of specificity.\n\nIn summary, the performance metrics for our classifiers were rigorously evaluated, and the results were statistically significant. The novel verbal fluency scores classifier showed superior performance compared to the other classifiers, providing a high level of confidence in its effectiveness.",
  "evaluation/availability": "Not applicable"
}