{
  "publication/title": "Direct Comparison of the Prediction of the Unbound Brain-to-Plasma Partitioning Utilizing Machine Learning Approach and Mechanistic Neuropharmacokinetic Model.",
  "publication/authors": "Kosugi Y, Mizuno K, Santos C, Sato S, Hosea N, Zientek M",
  "publication/journal": "The AAPS journal",
  "publication/year": "2021",
  "publication/pmid": "34008121",
  "publication/pmcid": "PMC8131289",
  "publication/doi": "10.1208/s12248-021-00604-x",
  "publication/tags": "- BCRP\n- brain-to-plasma unbound concentration ratio\n- in silico\n- machine learning\n- MDR1\n- Neuropharmacokinetics\n- Drug Discovery\n- Central Nervous System\n- Blood-Brain Barrier\n- Predictive Modeling",
  "dataset/provenance": "The dataset used in this study consists of 640 proprietary compounds obtained from Takeda Pharmaceutical Company. These compounds were experimentally analyzed for various properties, including in vivo Kp, in vitro brain tissue and plasma protein binding, and efflux activities of MDR1 and BCRP. The dataset covers a broad chemical diversity, representing a wide range of physicochemical properties. This includes molecular weight (M.W.) ranging from 187 to 555, clogP from 0.291 to 5.20, and topological polar surface area (TPSA) from 12.0 to 157. The efflux ratios (ER) in MDR1 cells range from 0.563 to 151, in BCRP cells from 0.522 to 100, and Kp,uu,brain from 0.00354 to 2.82. The data for both Kp,uu,brain and ER in MDCK-MDR1 and MDCK-BCRP cells were log-transformed to reduce unequal error variances. The dataset was split using three validation approaches: cluster-split, time-split, and an external dataset of 34 commercially available compounds. The cluster-split method used molecular fingerprints with a specified Tanimoto coefficient to divide the data into training and test sets with an 80:20 ratio. The time-split method divided the data by the date of assay, with 80% of the data before a certain date used for training and 20% after that date used for testing. The external dataset was used to assess model performance on compounds not included in the training or test sets.",
  "dataset/splits": "Three validation approaches were applied to investigate the model performance. The first method involved clustering the dataset using molecular fingerprints with a specified Tanimoto coefficient, splitting the data into training and test sets with an 80:20 ratio based on property values. This approach aimed to estimate predictivity when compounds with similar scaffolds were included in both training and test datasets.\n\nThe second method employed time-split validation to estimate predictivity for new chemical entities likely to be investigated in the future. The total dataset was split by the date of assay, with 80% of the data before a certain date assigned to the training set and the remaining 20% after that date assigned to the test set.\n\nThe third method utilized 34 commercially available compounds as an external dataset to assess model performance. This external dataset provided an additional layer of validation, ensuring the model's robustness and generalizability to compounds not included in the initial training or test sets.\n\nThe cluster-split training and test sets showed similar distributions of log-transformed brain-to-plasma partition coefficient (log(Kp,uu,brain)), indicating a reasonable splitting method. However, molecular weight (M.W.), calculated logP (clogP), and topological polar surface area (TPSA) in the time-split test sets tended to have higher values than those in the time-split training sets. This suggests that the time-split test set may contain compounds with different physicochemical properties, providing a more challenging validation scenario.",
  "dataset/redundancy": "The datasets were split using three different validation approaches to thoroughly investigate model performance. The first method involved clustering the dataset using molecular fingerprints with a specified Tanimoto coefficient. This clustering was followed by splitting the data into training and test sets with an 80:20 ratio, depending on the property value. This approach ensured that compounds with similar scaffolds were included in both the training and test datasets, allowing for an estimation of predictivity when similar compounds are present in both sets.\n\nThe second method employed time-split validation. This involved splitting the total dataset by the date of the assay, with 80% of the data before a certain date assigned to the training set and the remaining 20% after that date assigned to the test set. This method aimed to estimate the predictivity for new chemical entities that chemists might investigate in the future.\n\nThe third method utilized an external dataset consisting of 34 commercially available compounds. This dataset was used to assess model performance independently of the internal datasets.\n\nThe training and test sets were designed to be independent through these splitting methods. The cluster-split training and test sets showed similar distributions of log-transformed brain-to-plasma partition coefficient (log(Kp,uu,brain)), indicating that the splitting method was reasonable. However, molecular weight (M.W.), calculated logP (clogP), and topological polar surface area (TPSA) in the time-split test sets tended to have higher values than those in the time-split training sets. This suggests that the time-split test sets might include compounds with slightly different physicochemical properties, which is beneficial for assessing the model's generalizability to new, potentially more complex compounds.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in terms of chemical diversity and coverage of physicochemical properties. The study covered a broad range of molecular weights (187 to 555), clogP values (0.291 to 5.20), and TPSA values (12.0 to 157), ensuring that the models were trained and tested on a diverse set of compounds. This diversity is crucial for developing robust predictive models that can generalize well to new, unseen compounds.",
  "dataset/availability": "Not applicable.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are random forest regression (RF) and Gaussian process (GP) models. These are well-established classes of machine-learning algorithms that are widely used in various predictive modeling tasks.\n\nThe random forest regression algorithm is a flexible and easy-to-use machine learning method that often produces good results even without extensive hyper-parameter tuning. It is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the mean prediction of the individual trees. This approach helps to reduce overfitting and improve the generalization of the model.\n\nThe Gaussian process model is a non-parametric, probabilistic model that is particularly useful for regression tasks. It provides a distribution over functions, allowing for uncertainty quantification in predictions. The GP model used in this study involves optimizing four hyperparameters: \u03b81, \u03b82, \u03b83, and \u03b3i. These hyperparameters control various aspects of the model, such as the overall scale of the property values, the length scale parameters for each descriptor, and the variance of the assumed noise in the data. Hyperparameter tuning for the GP model was conducted using conjugate gradient optimization.\n\nBoth RF and GP are not new algorithms; they have been extensively studied and applied in various fields, including drug discovery and pharmacokinetics. The choice of these algorithms for this study is likely due to their robustness and effectiveness in handling complex, high-dimensional data, which is typical in cheminformatics and pharmacokinetics research. The focus of this publication is on the application of these algorithms to predict unbound brain-to-plasma partitioning (Kp,uu,brain), rather than on the development of new machine-learning algorithms. Therefore, it is appropriate that the study is published in a journal focused on pharmacokinetics and drug discovery, rather than a machine-learning journal.",
  "optimization/meta": "The models used in this study do not constitute a meta-predictor. Instead, they are individual machine learning models that were built and evaluated separately. The models used include random forest regression (RF) and Gaussian process (GP) models. These models were trained and tested using specific datasets, and their performance was assessed based on statistical indexes such as the coefficient of determination (R2) and residual mean squared error (RMSE).\n\nThe RF model used 400 random forest trees for each model, and the GP model involved the optimization of four hyperparameters. The descriptors used in these models were ranked based on their importance in the Gaussian process optimization (GPOPT) model for predicting Kp,uu,brain.\n\nThe study also compared the performance of these machine learning models with a neuroPK model, which is a translational model that incorporates scaling factors for MDR1 and BCRP activities. The neuroPK model was found to have better predictivity for external compounds that are outside the chemical space of the training set, while the machine learning models performed better for compounds with similar chemical properties to those in the training set.\n\nIn summary, the models used in this study are not meta-predictors but rather individual machine learning models that were built and evaluated separately. The performance of these models was compared with a neuroPK model, and the results suggested that the choice of model may depend on the specific application and the availability of data.",
  "optimization/encoding": "The data encoding and preprocessing involved several steps to ensure the quality and relevance of the descriptors used in the machine learning models. Descriptors with a standard deviation less than 0.0005 were excluded to remove those with minimal variability. Additionally, descriptors represented by less than 4% of compounds were excluded to focus on more prevalent features. To handle multicollinearity, if the pairwise correlation between any two descriptors exceeded 0.95, the descriptor of the pair with the lowest correlation with the target variable (Y column) was excluded. This process helped in retaining only the most informative and independent descriptors.\n\nThe dataset consisted of 640 proprietary compounds with experimentally determined in vivo Kp, in vitro brain tissue and plasma protein binding, and efflux activities of MDR1 and BCRP. The dataset covered a broad chemical diversity, representing a wide range of physicochemical properties, efflux ratios (ER), and brain-to-plasma partition coefficients (Kp,uu,brain). The data for both Kp,uu,brain and ER in MDCK-MDR1 and MDCK-BCRP cells did not follow a normal distribution and were log-transformed to reduce unequal error variances. This transformation ensured that the models could better capture the underlying patterns in the data.\n\nThree validation approaches were employed to assess model performance: cluster-split, time-split, and an external dataset of 34 commercially available compounds. The cluster-split method involved clustering the dataset using molecular fingerprints with a specified Tanimoto coefficient and splitting it into training and test sets with an 80:20 ratio based on property values. The time-split method involved splitting the dataset by the date of assay, with 80% of the data before a certain date used for training and the remaining 20% for testing. These approaches helped in evaluating the models' predictivity for compounds with similar scaffolds and new chemical entities, respectively.",
  "optimization/parameters": "In the optimization process, four hyperparameters were used in the Gaussian process model. These hyperparameters are \u03b81, \u03b82, \u03b83, and \u03b3i (where i ranges from 1 to K). \u03b81 represents the overall scale for the property values, while \u03b3i are length scale parameters, each corresponding to a specific descriptor. \u03b82 accounts for an overall constant shift in the function away from zero, and \u03b83 describes the variance of the assumed noise in the data. The selection of these parameters was guided by the need to optimize the model's performance, with hyperparameter tuning of \u03b3i conducted using conjugate gradient optimization.",
  "optimization/features": "The input features for the models were carefully selected through a process of descriptor exclusion. Descriptors with a standard deviation less than 0.0005 were excluded. Additionally, descriptors represented by less than 4% of compounds were removed. Furthermore, if the pairwise correlation between any two descriptors exceeded 0.95, the descriptor of the pair with the lowest correlation with the target variable (Y column) was excluded. This process ensured that only the most relevant and non-redundant features were used as input for the models.\n\nThe number of features (f) used as input is not explicitly stated, but it is clear that feature selection was performed. This selection process was conducted using the training set only, ensuring that the test set remained unbiased and that the models' performance could be accurately evaluated. The descriptors ranked in the top 20 of feature importance used in the Gaussian process optimization (GPOPT) model for Kp,uu,brain prediction are summarized in supplementary materials.",
  "optimization/fitting": "The models employed in this study, specifically random forest regression (RF) and Gaussian process (GP), are designed to handle a variety of data complexities. For the RF models, 400 trees were used, which helps in reducing overfitting by averaging the results of multiple decision trees. This ensemble approach ensures that the model generalizes well to unseen data.\n\nFor the GP models, hyperparameter tuning was conducted using conjugate gradient optimization. This method optimizes four hyperparameters: \u03b81 (overall scale for property values), \u03b82 (constant shift), \u03b83 (variance of the assumed noise), and \u03b3i (length scale parameters for each descriptor). The optimization process ensures that the model captures the underlying patterns in the data without overfitting. The use of a small value for \u03b3i indicates that differences in the corresponding descriptor significantly influence property values, which helps in fine-tuning the model's sensitivity to important features.\n\nTo rule out underfitting, the models were evaluated using multiple validation approaches, including cluster-split, time-split, and external dataset validation. These methods ensure that the models are tested on diverse and representative datasets, confirming their ability to generalize beyond the training data. The performance metrics, such as the coefficient of determination (R2) and residual mean squared error (RMSE), were calculated to assess the models' predictive accuracy. Additionally, the percentage of correct answers within 2-fold variability was evaluated, providing a robust measure of the models' performance.\n\nIn summary, the use of ensemble methods in RF and rigorous hyperparameter tuning in GP, along with comprehensive validation strategies, ensures that the models are neither overfitted nor underfitted. This approach guarantees that the models are reliable and accurate in predicting the desired outcomes.",
  "optimization/regularization": "In our study, several regularization methods were employed to prevent overfitting and ensure the robustness of our models. Firstly, we applied descriptor exclusion rules to filter out less informative or redundant features. Descriptors with a standard deviation below a certain threshold were removed, as were those represented by less than a specific percentage of compounds. Additionally, if the pairwise correlation between any two descriptors exceeded a certain value, the descriptor with the lowest correlation to the target variable was excluded. This helped in reducing multicollinearity and focusing on the most relevant features.\n\nFor the Gaussian process (GP) model, hyperparameter tuning was conducted using conjugate gradient optimization. This process involved optimizing four hyperparameters, including the overall scale for property values, length scale parameters for each descriptor, and a constant shift. The optimization aimed to find the best combination of hyperparameters that minimized the model's error, thereby improving its generalization capability.\n\nIn the random forest (RF) regression, we used an ensemble of 400 trees. This approach inherently provides regularization by averaging the predictions of multiple trees, which helps to reduce overfitting compared to a single decision tree. The use of a large number of trees ensures that the model captures the underlying patterns in the data without memorizing the noise.\n\nOverall, these regularization techniques were crucial in enhancing the predictive performance and reliability of our models.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, for the Gaussian process (GP) model, four hyperparameters were optimized: \u03b81, \u03b82, \u03b83, and \u03b3i. The overall scale for the property values is given by \u03b81, and the \u03b3i are a set of length scale parameters, one for each descriptor. An overall constant shift in the function away from zero is given by \u03b82. The variance of the assumed noise is described by hyperparameter \u03b83. Hyperparameter tuning of \u03b3i was conducted by conjugate gradient optimization (GPOPT).\n\nFor the random forest (RF) model, we used 400 random forest trees for each model. The concept and detailed implementation of the GP method for regression problems were described in a previous study. The descriptors ranked in the top 20 of feature importance used in the GPOPT model for Kp,uu,brain prediction are summarized in supplementary materials.\n\nThe model evaluation metrics, such as the coefficient of determination (R2) and residual mean squared error (RMSE), are also provided. These metrics were calculated using specific equations detailed in the publication.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly provided in the publication. The focus is on the methods and results obtained from the models rather than the distribution of the model files themselves. For access to the specific model files and optimization parameters, it would be necessary to contact the authors directly or refer to any supplementary materials that may accompany the publication. The publication itself does not specify a license for the use of the reported configurations or parameters.",
  "model/interpretability": "The models employed in this study include both random forest regression (RF) and Gaussian process (GP) models, which are generally considered to be more interpretable than many other machine learning algorithms. RF, in particular, is known for its ability to provide insights into feature importance. In our study, the top 20 features with the lowest length scale were extracted in both RF and GP models, indicating which descriptors were most influential in the predictions. This allows for a clear understanding of which molecular properties are driving the model's predictions.\n\nThe GP model, while more complex, also offers some level of interpretability through its hyperparameters. For instance, the overall scale for the property values is given by \u03b81, and the \u03b3i parameters represent length scales for each descriptor, indicating how sensitive the model is to changes in those descriptors. This provides a transparent view of how different features contribute to the model's outputs.\n\nAdditionally, the incorporation of MDR1 and BCRP efflux activities into the models adds another layer of interpretability. These activities are ranked among the top features in terms of importance, highlighting their significant role in predicting Kp,uu,brain. This transparency is crucial for understanding the biological relevance of the model's predictions and for validating the model against known biological mechanisms.\n\nIn summary, while the models used are sophisticated, they offer several avenues for interpretability. The feature importance rankings in RF and the hyperparameter interpretations in GP, along with the inclusion of biologically relevant descriptors like MDR1 and BCRP activities, make these models more transparent and easier to interpret.",
  "model/output": "The models developed in this study are regression models. Specifically, random forest regression (RF) and Gaussian process (GP) models were employed. These models were used to predict the unbound brain-to-plasma partitioning (Kp,uu,brain) values, as well as the efflux activities of MDR1 and BCRP. The performance of these models was evaluated using statistical indexes such as the coefficient of determination (R2) and the residual mean squared error (RMSE). The models were built using a variety of descriptors and hyperparameters, with the goal of achieving accurate and robust predictions. The incorporation of MDR1 and BCRP efflux activities significantly improved the predictive performance of the models, particularly in the time-split dataset. The highest R2 value of 0.602 was obtained using the GP model that included both MDR1 and BCRP activities. Overall, the regression models demonstrated superior performance compared to the neuroPK model for the same dataset.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the models involved several approaches to assess their performance comprehensively. Three main validation methods were employed: cluster-split validation, time-split validation, and external dataset validation.\n\nIn the cluster-split validation, the dataset was divided using molecular fingerprints and a specified Tanimoto coefficient. This method ensured that compounds with similar scaffolds were included in both the training and test sets, allowing for an estimation of the model's predictivity for structurally similar compounds. The dataset was split in an 80:20 ratio based on property values.\n\nTime-split validation was used to estimate the model's predictivity for new chemical entities. The dataset was split by the date of assay, with 80% of the data before a certain date used for training and the remaining 20% after that date used for testing. This approach simulated the scenario where chemists might investigate new compounds in the future.\n\nAdditionally, an external dataset consisting of 34 commercially available compounds was used to assess the model's performance. This provided an independent evaluation of the model's generalizability to compounds not included in the original dataset.\n\nThe predictive performance of the models was evaluated using statistical indexes such as the coefficient of determination (R2) and the residual mean squared error (RMSE). These metrics were calculated using log-transformed values of the observed and predicted data. The percentage of correct answers was also evaluated by comparing the number of predicted values within 2-fold variability of the observed values. Paired t-tests of squared errors were used to assess statistical differences between the machine learning and neuroPK models.\n\nOverall, these validation methods provided a robust evaluation of the models' performance, ensuring that they could generalize well to new and structurally diverse compounds.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the predictive capabilities of our models. The primary metrics reported are the coefficient of determination (R2) and the residual mean squared error (RMSE). These metrics were chosen for their ability to provide a comprehensive evaluation of model performance.\n\nR2 indicates the proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, where a value closer to 1 indicates a better fit of the model to the data. RMSE, on the other hand, measures the average magnitude of the errors between predicted and observed values, providing an indication of the model's accuracy. Lower RMSE values signify better model performance.\n\nAdditionally, we evaluated the percentage of predictions that fell within a 2-fold variability of the observed values. This metric is particularly useful in assessing the practical applicability of the models, as it provides insight into how often the predictions are within a reasonable range of the actual values.\n\nThese metrics are widely used in the literature for evaluating predictive models in pharmacokinetics and related fields. They offer a balanced view of model performance, capturing both the overall fit (R2) and the accuracy of individual predictions (RMSE and 2-fold variability). By reporting these metrics, we aim to provide a transparent and comprehensive assessment of our models' predictive capabilities, allowing for meaningful comparisons with other studies in the field.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of different modeling approaches to predict the brain-to-plasma unbound partition coefficient (Kp,uu,brain). We evaluated the performance of machine learning models, specifically random forest regression (RF) and Gaussian process optimization (GPOPT), against a neuroPK model.\n\nTo assess the predictive performance, we used three validation approaches: cluster-split, time-split, and an external test set. The cluster-split method involved dividing the dataset based on molecular fingerprints, ensuring that compounds with similar scaffolds were included in both training and test sets. This approach allowed us to estimate the predictivity for compounds with similar chemical structures. The time-split validation involved splitting the dataset by the date of assay, simulating the prediction of new chemical entities that might be investigated in the future. This method helped us evaluate the model's ability to generalize to unseen data.\n\nFor the machine learning models, we developed predictors for efflux activities of MDR1 and BCRP using transcellular transport data from MDCK-MDR1 and MDCK-BCRP cells. The GPOPT models showed R2 values of 0.581 for MDR1 and 0.499 for BCRP in the cluster-split test set, indicating a good fit to the data. Additionally, a significant percentage of compounds were predicted within a 2-fold range of the observed values, demonstrating the models' accuracy.\n\nWe also compared the performance of the neuroPK model against the machine learning models. The neuroPK model showed better prediction performance based on R2 and RMSE metrics. Notably, the neuroPK model utilized MDR1 and BCRP variables to establish in vitro to in vivo correlations (IVIVE), which are crucial for translating predictions across species. This advantage makes the neuroPK model widely applicable over structurally diverse datasets, whereas machine learning models tend to perform best with compounds similar to those in the training set.\n\nIn summary, our comparison involved rigorous validation methods and demonstrated that the neuroPK model offers superior predictivity and broader applicability compared to machine learning models for predicting Kp,uu,brain.",
  "evaluation/confidence": "The evaluation of our models involved several statistical measures to assess their performance and confidence. We used the coefficient of determination (R2) and the residual mean squared error (RMSE) as primary metrics. These metrics were calculated using logarithmic transformations of the observed and predicted values to ensure robustness and to handle the variability in the data.\n\nTo determine the statistical significance of our results, paired t-tests of squared errors were employed. This approach allowed us to compare the performance of our neuroPK models against machine learning models, providing a clear indication of whether the differences in performance were statistically significant.\n\nAdditionally, we reported confidence intervals for key parameters, such as \u03b1 and \u03b2, in the neuroPK model. These intervals give a range within which the true values of these parameters are likely to fall, adding another layer of confidence to our findings.\n\nThe use of cluster-split and time-split validation methods further enhanced the reliability of our results. The cluster-split method ensured that compounds with similar scaffolds were included in both training and test sets, while the time-split method assessed the model's ability to predict new chemical entities. Both methods showed that our neuroPK model performed well across different validation approaches, indicating its robustness and generalizability.\n\nIn summary, the performance metrics used in our study are supported by statistical significance tests and confidence intervals, providing a strong basis for claiming the superiority of our neuroPK model over traditional machine learning approaches.",
  "evaluation/availability": "Not enough information is available."
}