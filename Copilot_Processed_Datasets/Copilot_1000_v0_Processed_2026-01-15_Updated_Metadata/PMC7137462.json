{
  "publication/title": "Predicting Inpatient Length of Stay After Brain Tumor Surgery: Developing Machine Learning Ensembles to Improve Predictive Performance.",
  "publication/authors": "Muhlestein WE, Akagi DS, Davies JM, Chambless LB",
  "publication/journal": "Neurosurgery",
  "publication/year": "2019",
  "publication/pmid": "30113665",
  "publication/pmcid": "PMC7137462",
  "publication/doi": "10.1093/neuros/nyy343",
  "publication/tags": "- Machine Learning\n- Length of Stay\n- Predictive Modeling\n- Neurosurgery\n- Clinical Outcomes\n- Data Analysis\n- Ensemble Models\n- Medical Research\n- Patient Outcomes\n- Health Informatics",
  "dataset/provenance": "The dataset used in our study is sourced from two large, publicly available databases. For the training and internal validation of our ensemble model, we utilized the National Inpatient Sample (NIS) database, covering admissions from 2002 to 2011. The NIS is the largest all-payer inpatient database in the United States, representing approximately 8 million hospital stays from around 1,000 hospitals. This database is designed to approximate a 20% stratified sample of non-federal U.S. hospitals.\n\nFor external validation, we employed data from the American College of Surgeons National Surgical Quality Improvement Program (NSQIP) database, specifically for the years 2012 and 2013. The NSQIP database collects data on randomly selected surgical patients from over 400 academic and private hospitals across the United States. We chose years outside the range included in the training database to ensure no overlap between the training and validation datasets.\n\nIn total, we screened 79,742,743 admissions from the NIS and 11,953,75 admissions from the NSQIP. From these, 41,222 admissions met the criteria for inclusion in the training dataset, and 4,592 admissions were included in the external validation dataset. Both datasets are de-identified and publicly available, and their use was considered exempt from Institutional Review Board review.\n\nThe NIS and NSQIP databases have been widely used in previous research and by the community for various studies in healthcare outcomes and quality improvement. Our study leverages the strengths of these datasets to build and validate a machine learning ensemble model for predicting length of stay following craniotomy for brain tumor.",
  "dataset/splits": "In our study, we utilized two primary datasets: the National Inpatient Sample (NIS) and the National Surgical Quality Improvement Program (NSQIP). The NIS dataset, spanning from 2002 to 2011, was used for training and internal validation. We screened approximately 79 million admissions and identified 41,222 eligible admissions for inclusion in the training dataset. For internal validation, we employed a 5-fold cross-validation approach. This involved dividing the remaining data into 5 mutually exclusive folds, with each fold used once for validation and the remaining 4 used together as training. Additionally, 20% of the training dataset was randomly selected as a holdout set, which was excluded from training to provide an additional layer of internal validation.\n\nThe NSQIP dataset, covering the years 2012 to 2013, was used for external validation. We screened around 1.2 million admissions and included 4,592 eligible admissions in the external validation dataset. This dataset served as an entirely separate database to evaluate the ensemble's ability to generalize to new data, ensuring that our model's performance was assessed on a truly external and independent sample. The use of both internal and external validation datasets allowed us to robustly assess the model's generalizability and performance.",
  "dataset/redundancy": "The datasets used in this study were carefully selected and split to ensure independence between the training and validation sets. The training dataset was derived from the National Inpatient Sample (NIS) database, covering the years 2002 to 2011. This dataset includes approximately 8 million hospital stays from around 1000 hospitals, representing a 20% stratified sample of non-federal US hospitals.\n\nFor external validation, the American College of Surgeons National Surgical Quality Improvement Program (NSQIP) database was used, specifically for the years 2012 and 2013. This choice was made to ensure no overlap between the training and validation datasets, thereby guaranteeing their independence. The NSQIP database collects data from over 400 academic and private hospitals across the United States.\n\nTo further enforce the independence of the datasets, a holdout set was created by randomly selecting 20% of the training dataset, which was excluded from the training process. This holdout set was used for internal validation to assess the model's performance on unseen data.\n\nThe distribution of the datasets differs from previously published machine learning datasets in that they are derived from large, publicly available, de-identified databases specifically focused on surgical outcomes. The NIS and NSQIP databases are comprehensive and include a wide range of preoperative patient variables, making them suitable for modeling complex outcomes such as length of stay (LOS) following craniotomy for brain tumor.\n\nThe NIS dataset, with its extensive coverage of hospital stays, provides a robust training set, while the NSQIP dataset, with its prospective data collection, offers a rigorous external validation set. This approach ensures that the model's generalizability is thoroughly tested across different populations and time periods.",
  "dataset/availability": "The datasets used in this study are publicly available and deidentified, ensuring compliance with privacy regulations. The training dataset was derived from the National Inpatient Sample (NIS) database, which spans from 2002 to 2011. The NIS is the largest publicly available all-payer inpatient database in the United States, representing approximately 8 million hospital stays from around 1,000 hospitals. It is designed to approximate a 20% stratified sample of nonfederal U.S. hospitals.\n\nFor external validation, data from the American College of Surgeons National Surgical Quality Improvement Program (NSQIP) database was used, specifically for the years 2012 to 2013. The NSQIP is a multi-institutional program that prospectively collects data on randomly selected surgical patients from over 400 academic and private hospitals across the United States.\n\nBoth the NIS and NSQIP databases are accessible to the public, and their use in this study was considered exempt from Institutional Review Board review due to their deidentified nature. The data splits used for training and validation were clearly defined: the NIS dataset was used for training and internal validation, while the NSQIP dataset was used for external validation. This separation ensures that the validation process is robust and that the model's generalizability can be assessed on an entirely different dataset.",
  "optimization/algorithm": "The machine-learning algorithm class used is an ensemble model, which combines multiple algorithms to improve predictive performance. This approach leverages the strengths of various machine-learning techniques, including support vector machines, artificial neural networks, k-nearest neighbors, generalized additive models, tree-based models, linear models, and others. The ensemble model was chosen to handle the complexity of predicting length of stay (LOS) following craniotomy for brain tumor, as it allows for the integration of diverse inputs and the recognition of complex, nonlinear relationships.\n\nThe specific ensemble method employed is not entirely new but represents a novel application in the context of neurosurgical outcomes research. The technique involves systematically ranking, selecting, and combining multiple machine-learning algorithms to build a robust predictive model. This approach has been used in other fields but has not been extensively applied to neurosurgical data.\n\nThe reason this ensemble method was not published in a machine-learning journal is that the focus of the study is on its application in clinical medicine, specifically in predicting LOS after brain tumor surgery. The primary goal is to demonstrate the utility of machine learning in improving patient outcomes and providing clinical insights, rather than introducing a new machine-learning algorithm. The study aims to bridge the gap between theoretical machine-learning techniques and practical clinical applications, highlighting the potential of these methods in real-world medical settings.",
  "optimization/meta": "The model employs a meta-predictor approach, leveraging the strengths of multiple machine-learning algorithms to enhance predictive accuracy. This ensemble model is constructed by first evaluating and ranking the predictive abilities of a broad range of machine-learning algorithms. The top-performing algorithms are then combined to form the ensemble, which benefits from the complementary strengths of these individual algorithms.\n\nThe ensemble model includes algorithms that were identified as the most predictive through a direct comparison of their performance. These algorithms were optimized within each fold by creating additional sub-fold training/validation splits, ensuring that the best hyperparameters were selected for each algorithm. The ensemble itself was then trained and cross-validated in a similar manner to the individual algorithms, using the root mean square logarithmic error (RMSLE) as the validation metric.\n\nTo ensure the independence of the training data, the ensemble model was internally validated using a holdout dataset that had not been seen during the training process. This holdout dataset was treated as a single sample of data, and no confidence intervals were calculated for it. Additionally, the ensemble model was externally validated using a completely separate database derived from the National Surgical Quality Improvement Program (NSQIP), further confirming the generalizability of the model to never-before-seen data. This rigorous validation process ensures that the training data for the ensemble model is independent and that the model's predictions are robust and reliable.",
  "optimization/encoding": "For the machine-learning algorithm, data encoding and preprocessing were crucial steps to ensure the models could effectively learn from the input features. Numerical data were standardized by subtracting the mean and dividing by the standard deviation, which helped to bring all features to a similar scale. Missing numerical values were imputed using the median value for the respective variable, and a new binary variable was created to indicate the imputation. This approach allowed algorithms that cannot handle missing data to still be trained, while those that can leverage imputation could do so.\n\nCategorical data were handled differently depending on the type of algorithm. For linear algorithms, one-hot encoding was used to transform categorical variables into multiple binary columns. Missing categorical values were treated as a separate category, ensuring that no information was lost. For tree-based algorithms, categorical data were encoded using randomly assigned integers, which is a method that these algorithms can handle efficiently.\n\nThis preprocessing pipeline ensured that the data was in a suitable format for a wide range of machine-learning algorithms, allowing for comprehensive model training and validation. The use of different encoding methods for linear and tree-based algorithms maximized the performance and applicability of the models.",
  "optimization/parameters": "A total of 26 different variables were considered as input parameters for the model. These variables included a variety of preoperative patient characteristics available in both the training and validation datasets, such as age, race, sex, specific neurosurgical diagnosis, preoperative comorbidities, admission quarter, and whether the surgery was emergent or nonemergent. The selection of these variables was based on their availability and relevance to the outcome being modeled, which was the length of stay (LOS) in the hospital following craniotomy for brain tumor. The variables were chosen to capture a broad range of factors that could potentially influence LOS, ensuring that the model had a comprehensive set of inputs to make accurate predictions.",
  "optimization/features": "In our study, we considered a total of 26 different variables as input features. These variables were selected based on their availability in both the training and validation datasets and included a variety of preoperative patient characteristics such as age, race, sex, specific neurosurgical diagnosis, preoperative comorbidities, admission quarter, and whether the surgery was emergent or non-emergent.\n\nFeature selection was not explicitly performed as a separate step. Instead, we included all relevant variables that were available in both datasets. This approach ensured that the features used in the model were consistent across the training and validation datasets, which is crucial for the generalizability of the model.\n\nThe selection of these features was done using the training set only, as the validation set was kept separate to ensure an unbiased evaluation of the model's performance. This approach helps to prevent data leakage and ensures that the model's performance on the validation set is a true reflection of its ability to generalize to new, unseen data.",
  "optimization/fitting": "The fitting method employed in this study involved a rigorous process to ensure that the model neither overfitted nor underfitted the data. To address the potential issue of overfitting, especially given the large number of parameters relative to the training points, several techniques were utilized.\n\nFirst, model hyperparameters were optimized within each fold by creating an additional sub-fold training/validation split. This approach allowed for the testing of each combination of hyperparameters within this sub-fold setup, ensuring that the optimal hyperparameters were determined without overfitting to the training data.\n\nAdditionally, cross-validation scores were calculated using the root mean square logarithmic error (RMSLE) across five validation folds. This metric was chosen because it penalizes large errors less when both predicted and actual lengths of stay (LOS) are very large, thereby providing a more robust measure of model performance.\n\nTo further validate the model, an ensemble approach was used, combining the algorithms with the highest cross-validation scores. This ensemble was then trained and cross-validated in the same manner as the individual algorithms, providing an additional layer of validation.\n\nInternal validation was conducted by calculating the RMSLE for predictions made on a never-before-seen holdout dataset. This step ensured that the model's performance was not overly tailored to the training data.\n\nFollowing internal validation, the ensemble was trained on 100% of the NIS database, and the fully trained model was externally validated with the NSQIP database. This external validation provided an independent assessment of the model's generalizability to new data.\n\nTo rule out underfitting, the model's performance was evaluated using lift charts, which visually demonstrated the accuracy of predicted LOS relative to actual LOS. These charts showed that the model's predictions were generally within a reasonable range of the actual values, indicating that the model was not underfitting the data.\n\nOverall, the combination of cross-validation, ensemble methods, and external validation ensured that the model was neither overfitting nor underfitting the data, providing a robust and generalizable prediction of LOS following craniotomy for brain tumor.",
  "optimization/regularization": "In our study, we employed regularization techniques to prevent overfitting. Specifically, we utilized regularized linear and logistic regression methods. These methods incorporate mechanisms to minimize overfitting by shrinking or eliminating large regression coefficients. Examples of these techniques include ridge regularization, lasso regularization, and elastic net regularization, which combines both ridge and lasso regularizations. These approaches help to ensure that our models generalize well to new, unseen data by avoiding the pitfalls of overfitting to the training dataset.",
  "optimization/config": "The hyperparameter configurations and optimization schedule used in our study are detailed within the publication. We employed a 5-fold cross-validation approach, where each fold was further divided into sub-folds for hyperparameter tuning. This process ensured that the optimal hyperparameters were identified for each algorithm within the ensemble. The specific details of the hyperparameters and the optimization process are described in the methods section.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly provided in the publication. The focus was on the methodology and results rather than the distribution of specific model files. However, the software used for model training and validation, DataRobot version 3.0, is mentioned, and it is commercially available. The use of this software implies that similar models could be trained using the described methods and datasets, but the exact model files are not shared publicly.\n\nThe publication does not specify the licensing terms for the methods or the software used. DataRobot is a commercial product, and its licensing would be subject to the terms and conditions set by DataRobot, Inc. For those interested in replicating the study, it would be necessary to obtain access to the DataRobot software and adhere to its licensing agreements.",
  "model/interpretability": "The model employed in this study is not a black box, as it utilizes techniques that allow for interpretability. Several methods were used to ensure that the underlying mechanisms driving the predictions could be understood.\n\nOne of the key techniques used was permutation importance analysis. This method helps identify the relative importance of various risk factors in determining extended length of stay (LOS). By permuting the values of each variable and observing the change in model performance, it is possible to rank the variables based on their impact on the predictions. This provides a clear indication of which factors are most influential.\n\nAdditionally, partial dependence plots were utilized to visualize how changes in a single variable affect the model's predictions. These plots dynamically depict the impact of a variable across its entire range of values, offering more insight than static regression coefficients. This approach allows for a deeper understanding of how individual variables contribute to the model's outcomes.\n\nMoreover, the use of regularized linear and logistic regression models, which are inherently interpretable, contributes to the transparency of the ensemble. These models provide coefficients that can be examined to understand the direction and magnitude of the relationship between predictors and the outcome.\n\nIn summary, the model incorporates several interpretability techniques, ensuring that the predictions are not only accurate but also understandable. This transparency is crucial for clinical applications, where it is essential to trust and explain the model's decisions.",
  "model/output": "The model developed in this study is a regression model. It is designed to predict the length of stay (LOS) in the hospital following craniotomy for brain tumor. The output of the model is a continuous variable representing the predicted LOS, rather than a categorical classification. The model was trained and validated using root mean square logarithmic error (RMSLE) as the primary metric, which is a common evaluation measure for regression tasks. The ensemble model combines multiple machine learning algorithms to improve the accuracy of the LOS predictions. The strongest predictors of increased LOS identified by the model include non-elective surgery, preoperative pneumonia, sodium abnormality, weight loss, and non-White race. These insights can help clinicians and hospital administrators make informed decisions to improve patient care and resource allocation.",
  "model/duration": "The execution time for the model varied depending on the specific algorithm and the tasks performed. Some algorithms, such as support vector machines and artificial neural networks, required significant processing power and were slow to train. This was due to their ability to model complex, nonlinear relationships between inputs and outputs. In contrast, linear models like ordinary least squares were fast to train. To manage the computational demands, a tradeoff was made between algorithm training run time and the number of cross-validation folds. A 5-fold cross-validation was used, which allowed for efficient training while still providing robust estimates of the ensemble's generalizability. Additionally, the final model was evaluated against a separate database, the NSQIP, to ensure its ability to generalize to new data. The use of an ensemble of algorithms also helped to mitigate some of the individual training times, as the best-performing algorithms were combined to form a more efficient and accurate model.",
  "model/availability": "The software used for model training and validation in this study was provided by DataRobot, Inc. Specifically, DataRobot version 3.0 from Boston, Massachusetts, was utilized. This software encompasses the necessary tools and algorithms to facilitate the training, validation, and optimization of machine learning models. The software includes capabilities for hyperparameter tuning, cross-validation, and ensemble model creation, which were integral to the development and evaluation of the predictive models discussed in the study. The use of this software ensured that the models were trained and validated in a robust and systematic manner, leveraging the strengths of multiple machine learning algorithms to achieve accurate predictions.",
  "evaluation/method": "The evaluation of the method involved a comprehensive approach to ensure the robustness and generalizability of the model. Initially, we employed a 5-fold cross-validation strategy, where the dataset was divided into five mutually exclusive folds. Each fold was used once for validation while the remaining four were used for training. This process was repeated five times, providing five independent estimates of the model's ability to generalize to unseen data. The root mean square logarithmic error (RMSLE) was used as the validation metric, as it penalizes large errors less when both predicted and actual length of stay (LOS) are very large, compared to when they are small.\n\nTo further validate the model, we used a holdout dataset that was never used in the training process. This internal validation step provided an additional layer of assurance that the model could perform well on data it had not seen during training.\n\nBeyond internal validation, we conducted external validation using an entirely separate database, the National Surgical Quality Improvement Program (NSQIP). This dataset was chosen because it is significantly different from the training dataset, the National Inpatient Sample (NIS), in terms of patient characteristics and average LOS. The NSQIP dataset includes admissions from 2012 to 2013, ensuring no overlap with the training data from 2002 to 2011. The model's performance on the NSQIP dataset demonstrated its ability to generalize to new, external data, which is crucial for real-world applicability.\n\nAdditionally, we generated lift charts to visualize the accuracy of the ensemble model's predictions. These charts ranked and divided the ensemble predictions into 10 bins, or deciles, and calculated the average predicted and actual LOS for each bin. This visualization helped to graphically represent how well the model's predictions aligned with actual outcomes.\n\nPermutation importance analyses were also conducted to understand the relative importance of various variables in the model's predictions. This involved retraining the model with permuted variables and comparing the change in RMSLE to determine the impact of each variable.\n\nOverall, the evaluation method combined cross-validation, holdout validation, external validation, and detailed analyses to ensure the model's reliability and generalizability.",
  "evaluation/measure": "In our study, we primarily used the root mean square logarithmic error (RMSLE) as our validation metric. RMSLE was chosen because it penalizes large errors less when both predicted and actual length of stay (LOS) are very large, compared to when they are small. This makes it a suitable metric for our problem, as it better handles the variability in LOS. RMSLE can be interpreted as the standard deviation of the log of unexplained variance, or more simply, it indicates that the model\u2019s prediction is usually within e to the power of the calculated RMSE times the true value.\n\nWe calculated RMSLE scores for each of the 5 validation folds used in our cross-validation process. The closer the RMSLE value is to 0, the more accurate the model, with RMSLE=0 denoting zero error. Additionally, we calculated the RMSLE for predictions made on a never-before-seen holdout dataset as an additional internal validation step. Following this, the ensemble model was externally validated using the NSQIP database, and a single RMSLE value was reported for this external validation.\n\nTo visualize the performance of our ensemble model, we generated lift charts. These charts divide the ensemble predictions into 10 equal bins, or deciles, and plot the average predicted LOS against the average actual LOS for each decile. This provides a graphical representation of the model's accuracy in predicting LOS.\n\nWe also performed permutation importance analyses to understand the relative importance of different variables in our model. This involved retraining the ensemble on data where the values of a specific variable were randomly permuted, and then comparing the change in RMSLE to the original model. This helped us rank the variables by their importance to the model's predictions.\n\nPartial dependence plots were used to visualize how the model's predictions change in response to variations in a single variable. These plots provide a dynamic view of the model's behavior, unlike static regression coefficients.\n\nIn summary, our performance measures include RMSLE for both internal and external validation, lift charts for visualizing prediction accuracy, permutation importance for variable ranking, and partial dependence plots for understanding variable impacts. These metrics provide a comprehensive evaluation of our model's performance and generalizability.",
  "evaluation/comparison": "A comparison to publicly available methods was not explicitly performed on benchmark datasets. However, the study employed a unique approach by evaluating and ranking the predictive abilities of a broad range of machine learning algorithms. The top-performing algorithms were then combined into an ensemble model, leveraging the complementary strengths of multiple algorithms. This method allowed for a comprehensive comparison within the study itself, rather than relying on external benchmark datasets.\n\nA comparison to simpler baselines was not explicitly mentioned. The focus was on building and validating a guided machine learning ensemble to predict length of stay (LOS) in hospital for patients following craniotomy for brain tumor. The ensemble model was trained and internally validated using the National Inpatient Sample (NIS) database, and then externally validated using the National Surgical Quality Improvement Program (NSQIP) database. This approach ensured that the model's performance was assessed against real-world data, providing a robust evaluation of its predictive capabilities.",
  "evaluation/confidence": "The evaluation of our model's performance involved calculating the root mean square logarithmic error (RMSLE) for both internal and external validation datasets. For the internal validation, which used the NIS database, the RMSLE was reported with a 95% confidence interval (CI), specifically .555 (95% CI, .553-.557). This indicates a high level of precision in our internal validation results.\n\nHowever, for the external validation using the NSQIP database, the RMSLE was reported as .631 without a confidence interval. This is because the NIS holdout and the NSQIP dataset were treated as single samples of data, and thus no confidence intervals were calculated for these external validation results.\n\nThe statistical significance of our model's performance was demonstrated through the use of cross-validation and external validation. The internal validation involved five non-overlapping cross-validation folds, providing five independent estimates of the model's ability to generalize to unseen data. Additionally, the model was validated on an entirely separate database, the NSQIP, which is considered the gold standard for model validation. The fact that the model maintained a modest level of performance on this external dataset, despite differences in the populations represented, suggests that the results are statistically significant and that the model is robust and generalizable.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The evaluation process involved internal and external validation datasets derived from the National Inpatient Sample (NIS) and the National Surgical Quality Improvement Program (NSQIP), respectively. These datasets are large, deidentified databases that contain sensitive patient information. As such, they are not released publicly to maintain patient privacy and comply with regulatory requirements. The NIS and NSQIP databases are accessible through specific channels and with appropriate permissions, typically for research purposes. The evaluation metrics and results, such as the root mean square logarithmic error (RMSLE) and lift charts, are presented in the publication to provide transparency and reproducibility of the findings. However, the actual data used for evaluation is not made available to the public."
}