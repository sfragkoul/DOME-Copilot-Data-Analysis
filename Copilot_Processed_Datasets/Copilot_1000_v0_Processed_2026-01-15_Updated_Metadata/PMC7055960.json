{
  "publication/title": "Predicting novel substrates for enzymes with minimal experimental effort with active learning.",
  "publication/authors": "Pertusi DA, Moura ME, Jeffryes JG, Prabhu S, Walters Biggs B, Tyo KEJ",
  "publication/journal": "Metabolic engineering",
  "publication/year": "2017",
  "publication/pmid": "29030274",
  "publication/pmcid": "PMC7055960",
  "publication/doi": "10.1016/j.ymben.2017.09.016",
  "publication/tags": "- Machine learning\n- Active learning\n- Enzyme promiscuity\n- Support vector machines\n- Metabolic engineering\n- Computational biology\n- Biocatalysis\n- Enzyme characterization\n- Substrate prediction\n- Chemical diversity",
  "dataset/provenance": "The dataset for this study was primarily sourced from the BRENDA online enzyme database, supplemented with additional data from published literature and experimental results. The enzymes investigated include 2-succinyl-5-enolpyruvyl-6-hydroxy-3-cyclohexene-1-carboxylic acid synthase (MenD) from Escherichia coli, carboxylic acid reductase (Car) from Nocardia iowensis, amino acid ester hydrolase (AAEH) from Xanthomonas citri, and 4-hydroxyacetophenone monooxygenase (HAPMO) from Pseudomonas putida.\n\nPositive training data consists of compounds listed as substrates of an enzyme from one organism for a specific EC number in BRENDA, with additional data from literature and experiments. Negative training data, consisting of inactive compounds, were obtained through a manual search of the primary literature. These inactive compounds often contain the functional group required by the enzymatic activity under investigation, ensuring a higher degree of certainty compared to randomly selected compounds.\n\nThe study also utilized the ZINC database, version 12, to rank compounds for active learning. This database provided a large and diverse set of biochemically relevant chemicals, allowing for the selection of compounds that could potentially add informative chemical features to the models.\n\nThe datasets for MenD, Car, AAEH, and HAPMO were compiled to develop SVM classification models. The number of data points varied for each enzyme, with the datasets being relatively small and homogeneous in some cases, which limited their predictive power for diverse chemical spaces. However, the active learning approach was employed to strategically expand the available pool of compounds, both active and inactive, to improve the models' performance.\n\nThe datasets used in this study were not extensively used in previous papers or by the community, as the focus was on demonstrating the efficacy of active learning in predicting enzyme substrate promiscuity. The study aimed to show that active learning could prioritize new compounds for testing, efficiently expanding the domain of applicability of the classifier.",
  "dataset/splits": "In our study, we employed a cross-validation approach to evaluate the predictive performance of our Support Vector Machine (SVM) models. For each dataset, we created multiple data splits to ensure robust and reliable results. Specifically, we sampled approximately 60% of the molecules for training the SVM, while the remaining 40% were used as a test set to measure the model's accuracy. This procedure was repeated 1,000 times for each enzyme to generate a comprehensive set of results. The final estimated accuracy for each dataset was calculated by averaging the results from these 1,000 iterations.\n\nThe datasets we analyzed included compounds for four different enzymes: MenD, Car, HAPMO, and AAEH. For each enzyme, the distribution of data points in the training and test sets remained consistent across the 1,000 iterations, with approximately 60% of the data used for training and 40% for testing. This approach ensured that our models were evaluated on a diverse range of compounds, providing a thorough assessment of their predictive performance.\n\nIn addition to the standard training and test splits, we also explored the use of active learning to strategically select compounds for training. This involved generating learning curves that tracked the increase in classifier accuracy as additional compounds were added to the training set. The active learning method demonstrated a significantly faster gain in accuracy compared to random selection, particularly for the MenD and AAEH datasets. This indicates that active learning can be an effective tool for efficiently sampling diverse chemical spaces and improving the predictive power of SVM models.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm presented in this work is based on support vector machines (SVMs), a well-established class of machine-learning algorithms. SVMs are particularly effective for classification tasks, making them suitable for predicting enzyme promiscuity.\n\nThe specific algorithm developed, SimAL, is not entirely new but represents an innovative application of existing techniques. It combines cheminformatics with SVM learning to predict an enzyme\u2019s promiscuity with fewer experimental observations. This approach judiciously resolves conflicts in existing data and improves predictive power, which has been experimentally validated.\n\nThe reason this algorithm was not published in a machine-learning journal is that the primary focus of this work is on its application in metabolic engineering and enzyme characterization. The development and validation of SimAL are presented in the context of its utility for understanding and predicting enzyme promiscuity, which is crucial for designing novel metabolic pathways and biosynthetic routes. The emphasis is on the practical implications and experimental validation of the algorithm rather than the theoretical advancements in machine learning.",
  "optimization/meta": "Not applicable. The publication focuses on using support vector machines (SVMs) for predicting enzyme substrate promiscuity and employs active learning to strategically select compounds for testing. The methodology does not involve a meta-predictor that combines outputs from multiple machine-learning algorithms. Instead, it relies on SVM models trained on diverse compound datasets to make predictions. Active learning is used to enhance the efficiency of selecting informative compounds for further experimentation, but it does not constitute a meta-predictor framework. The training data for the SVM models is compiled from existing studies and is used to develop and validate the models' predictive capabilities. The independence of the training data is not a primary concern in this context, as the focus is on leveraging available data to improve the models' accuracy and efficiency in predicting enzyme promiscuity.",
  "optimization/encoding": "In our study, the data encoding process involved several key steps to prepare the chemical structure data for machine learning models. Compound names were first converted to SMILES representations using the MolConvert Calculator Plugin. This was followed by structure canonicalization and transformation using the Standardizer. InChI representations of molecules were generated and read using the RDKit open-source cheminformatics platform. These string representations were then converted into 2D chemical fingerprints with integer counts, which were used to develop the machine learning models.\n\nThe fingerprinting process utilized the SMARTS queries available in the OpenBabel FP4 fingerprint implementation. Additional queries for Cahn-Ingold-Prelog chirality assignments were carried out using RDKit. Cofactors, which were not considered in the reactions, were removed from the dataset using a hand-curated set of cofactor pair rules. The centroid of the training sets was defined as the average of all the fingerprints in the set.\n\nFor visualization and clustering, t-distributed stochastic neighbor embeddings (t-SNE) were generated using the scikit-learn and matplotlib Python libraries. The t-SNE algorithm is a stochastic clustering method that attempts to preserve the distance between data points in a higher-dimensional space while embedding them in a lower-dimensional space. This method ensures that proximal compounds are more similar than distant compounds, although distances between distant clusters of similar compounds may not be comparable due to the stochastic nature of the algorithm.",
  "optimization/parameters": "In our study, the model utilized several key parameters to optimize its performance. The choice of kernel function was crucial, and we selected a radial basis function kernel, which is well-suited for handling non-linear data. This kernel implicitly maps the input data into a high-dimensional feature space, facilitating better separation of classes.\n\nFor the active learning component, we employed an information density approach to rank compounds based on their potential to enhance the model's predictive power. This approach involves calculating an information density score for each compound, which requires selecting an uncertainty metric, a similarity metric, and a tunable parameter. In our case, the uncertainty metric was chosen to be the logarithm of the reciprocal absolute value of the fingerprint count. The similarity metric used was the Tanimoto coefficient, adapted for count fingerprints. The tunable parameter \u03b2 was set to 1.\n\nAdditionally, the decision function incorporated a threshold \u03c4, which was also set to the Tanimoto coefficient adapted for count fingerprints. This threshold helps in determining the significance of each compound's contribution to the model.\n\nThe model's accuracy was evaluated using learning curves, which demonstrate how the accuracy grows as a function of the number of training examples. We generated these curves for enzyme-substrate datasets to observe the behavior of the SVM models as more data points were used for training. The process involved dividing the dataset into training and test sets, initially training the models with a small subset of data, and then iteratively adding compounds to the training set. For each addition, the models were retrained, and the accuracy was recalculated. This procedure was repeated for 1,000 iterations to ensure robustness.\n\nIn summary, the model's parameters were carefully selected to optimize its performance in ranking compounds and improving predictive accuracy through active learning. The use of a radial basis function kernel, the Tanimoto coefficient for similarity and threshold, and the information density approach with a specific uncertainty metric and tunable parameter were key components in achieving this optimization.",
  "optimization/features": "In our study, we utilized chemical fingerprints to represent compounds, which typically consider more than 300 features for a compound set. This high-dimensional chemical space allows for a comprehensive analysis of various chemical features.\n\nFeature selection was implicitly performed through the use of Support Vector Machines (SVMs) and active learning. The SVMs were trained to identify the most discriminatory features between active and inactive compounds. The active learning process further refined this by prioritizing compounds that contained features likely to inform the model, thereby indirectly selecting the most relevant features.\n\nThe feature selection process was conducted using the training set only, ensuring that the model's performance on unseen data was not compromised. This approach allowed us to focus on the most informative features, such as carbon-carbon double bonds and aldehydes for MenD, and nitrogen-containing groups for Car, which are highly correlated with the activity of the compounds.",
  "optimization/fitting": "In our study, we employed Support Vector Machines (SVMs) for classification tasks, which inherently have a regularization parameter to control the trade-off between achieving a low training error and a low testing error. This helps in mitigating over-fitting, especially when the number of parameters (features) is much larger than the number of training points.\n\nTo further address over-fitting, we utilized an active learning approach. This method selectively chooses the most informative compounds to add to the training set, ensuring that the model learns from a diverse and representative subset of the data. By doing so, we enhance the model's generalization capability, reducing the risk of over-fitting to the training data.\n\nAdditionally, we generated learning curves to monitor the model's performance as the size of the training set increases. These curves demonstrate how the accuracy of the SVM grows with more training examples, providing insights into whether the model is under-fitting or over-fitting. In our experiments, we observed that the models trained with actively selected data reached their maximum accuracy with fewer compounds compared to those trained with randomly selected data. This indicates that the active learning approach effectively utilizes the available data, minimizing both over-fitting and under-fitting.\n\nMoreover, we performed cross-validation by dividing the dataset into training and test sets multiple times. This process helps in assessing the model's performance on unseen data, ensuring that it generalizes well beyond the training examples. The consistent improvement in accuracy with actively selected data further supports the effectiveness of our approach in avoiding under-fitting.\n\nIn summary, through the use of regularization, active learning, learning curves, and cross-validation, we have taken measures to ensure that our models neither over-fit nor under-fit the data. These techniques collectively contribute to the robustness and reliability of our SVM models in predicting enzyme-substrate interactions.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method involved the use of a radial basis function kernel in our Support Vector Machine (SVM) models. This kernel function allows the input data to be implicitly mapped into a high-dimensional feature space, which is particularly useful for capturing non-linear relationships in the data.\n\nAdditionally, we utilized recursive feature elimination to approximate feature importance. This process helps in identifying and retaining the most discriminatory chemical moieties, thereby reducing the complexity of the model and mitigating the risk of overfitting.\n\nFurthermore, we implemented active learning techniques to efficiently sample diverse chemical spaces. This approach ensures that the models are trained on a representative subset of the data, which helps in generalizing better to unseen compounds. The active learning method selects compounds that are likely to add the most information to the classifier, thereby improving the model's accuracy with fewer compounds.\n\nCross-validation was another crucial technique we employed. We performed cross-validation by sampling approximately 60% of the molecules for training the SVM and using the remaining compounds as a test set. This procedure was repeated 1,000 times for each enzyme, and the final estimated accuracy was calculated by averaging the results. This rigorous validation process helps in assessing the model's performance and ensuring that it generalizes well to new data.\n\nIn summary, our study incorporated several overfitting prevention techniques, including the use of a radial basis function kernel, recursive feature elimination, active learning, and cross-validation. These methods collectively contribute to the robustness and reliability of our models.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in this study are primarily Support Vector Machines (SVMs) with a radial basis function kernel, which are generally considered black-box models. This is because the feature weights in SVM models using non-linear kernel functions cannot be directly computed. However, we approximated feature importance using recursive feature elimination to calculate a ranking coefficient. This process involves calculating a ranking coefficient that indicates the impact of removing a feature or set of features on the model's objective function. By normalizing this coefficient, we can represent the percent change in the objective function, providing insights into which features are most important for the model's predictions.\n\nFor example, in the case of the MenD classifier, carbon-carbon double bonds and aldehydes were identified as highly important features. This is because these features are strongly correlated with the activity of substrates in the dataset. Similarly, the Car classifier identified nitrogen-containing groups as most discriminatory, as these features are generally found in the inactive compounds. These examples illustrate how, despite the black-box nature of the SVM with a non-linear kernel, we can still gain interpretability by approximating feature importance.\n\nAdditionally, the active learning approach used in this study further enhances interpretability. By selecting compounds that maximize the information gained, the model can adapt to new data and provide insights into which chemical features are most relevant for predicting enzyme activity. For instance, in the case of MenD, the active learning algorithm prioritized compounds with aliphatic aldehydes containing double bonds, helping to resolve whether double bonds alone make a compound inactive or if it is the lack of an aldehyde. This demonstrates how the model can evolve and provide clearer interpretations as more data is introduced.",
  "model/output": "The model employed in our study is a classification model, specifically a Support Vector Machine (SVM). SVMs are particularly well-suited for classification tasks, where the goal is to predict discrete labels or categories. In our case, the SVM models were used to classify compounds as either active or inactive substrates for various enzymes.\n\nThe choice of SVM was driven by its ability to handle high-dimensional spaces and its effectiveness in capturing complex patterns in the data. We utilized a radial basis function (RBF) kernel, which allows the input data to be implicitly mapped into a high-dimensional feature space. This is crucial for capturing non-linear relationships in the data, which are common in chemical datasets.\n\nThe performance of these SVM models was evaluated using cross-validation, where approximately 60% of the molecules were used for training, and the remaining compounds served as a test set. This process was repeated 1,000 times for each enzyme to ensure robust estimates of model accuracy. The final estimated accuracy was calculated by averaging the results across these iterations.\n\nAdditionally, we explored the use of active learning to enhance the predictive power of our models with minimal experimental burden. Active learning involves strategically selecting compounds that maximize the information gained from each experiment, thereby improving the model's accuracy more efficiently than random sampling.\n\nIn summary, our models are classification models designed to predict whether a given compound will be an active or inactive substrate for specific enzymes. The use of SVMs with an RBF kernel, combined with active learning techniques, allows us to achieve high accuracy and efficiency in our predictions.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved several key steps to assess the performance and effectiveness of the active learning approach for predicting enzyme substrate promiscuity.\n\nInitially, learning curves were generated to track the increase in classifier accuracy as additional compounds were added to the training set. This process involved dividing the dataset into a training set (60% of the data) and a test set (the remaining data). Two support vector machine (SVM) models were used: one trained with data selected by active learning and the other with data selected at random. The accuracy of each model was calculated after each compound was added to the training set, and this procedure was repeated 1000 times to obtain an average accuracy value at each training set size.\n\nTo further evaluate the method, an active learning algorithm was allowed to select compounds from a large and diverse database (ZINC) that contained relevant reactive atoms for the enzymes under investigation. The algorithm ranked these compounds based on their potential to add information to the classifier. The top 10% of the compounds prioritized by active learning were then compared to an equal number of randomly selected compounds to assess the enrichment of informative chemical features.\n\nExperimental validation was also conducted. Two sets of four compounds were selected from the active learning-prioritized lists for one of the enzymes (MenD) and subjected to activity assays. The results of these assays were used to refine the SVM model, and the improved model was then tested on additional compounds to demonstrate its ability to uncover new potential reactions.\n\nAdditionally, the chemical features that had the greatest importance to the models were analyzed to ensure they aligned with biochemical knowledge of the datasets. This involved generating SVMs using the full datasets for each enzyme and calculating the impact of each chemical feature in distinguishing between active and inactive substrates.\n\nOverall, the evaluation method combined computational analysis, experimental validation, and biochemical knowledge to thoroughly assess the effectiveness of the active learning approach in predicting enzyme substrate promiscuity.",
  "evaluation/measure": "In our evaluation, we primarily focused on the predictive performance of our Support Vector Machine (SVM) models using cross-validation. The key performance metric reported is the estimated accuracy, denoted as amax. This metric was calculated by averaging the results of 1,000 iterations, where each iteration involved sampling approximately 60% of the molecules for training and using the remaining compounds as a test set.\n\nThe amax values for the different datasets were as follows:\n\n* Car dataset: 75% \u00b1 0.6%\n* MenD dataset: 79% \u00b1 0.8%\n* HAPMO dataset: 76% \u00b1 0.6%\n* AAEH dataset: 81% \u00b1 0.6%\n\nThese accuracy measures provide a clear indication of how well our models perform on held-out data, which is crucial for assessing their generalizability. The reported metrics are standard in the field of machine learning and are commonly used to evaluate the performance of predictive models, particularly in the context of chemical and biological datasets.\n\nAdditionally, we analyzed the importance of chemical features in our models. We identified the chemical features that had the greatest impact on distinguishing between active and inactive substrates for each enzyme. This analysis is summarized in a table, highlighting the most informative features for each dataset. For example, carbon-carbon double bonds and aldehydes were found to be highly important for the MenD classifier, while nitrogen-containing groups were crucial for the Car classifier.\n\nThe set of metrics reported is representative of the literature in this field. Accuracy is a widely used metric for evaluating the performance of predictive models, and the use of cross-validation ensures that our results are robust and not overly optimistic. The analysis of feature importance also aligns with common practices in the literature, providing insights into the chemical features that drive the predictions of our models.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, we focused on evaluating the performance of our active learning approach in the context of enzyme promiscuity prediction. We compared the active learning method to a random selection baseline to demonstrate its effectiveness in improving classifier accuracy with fewer compounds.\n\nWe generated learning curves to track the increase in classifier accuracy as additional compounds were added to the training set. This allowed us to observe that active learning methods approached maximum accuracy more quickly than random selection. For the MenD and AAEH datasets, active learning achieved the maximum average accuracy with significantly fewer compounds than were available for selection.\n\nAdditionally, we compared the active learning approach to a dissimilarity-based method. This comparison showed that active learning not only selects diverse compounds but also prioritizes those that maximize information content, rather than just selecting outliers. This was evident in the enrichment of both present and absent features in the top-ranked compounds selected by active learning, unlike the dissimilarity-based approach which only enriched for absent features.\n\nWe also experimentally validated the active learning approach by selecting compounds from the prioritized lists and performing activity assays. The results confirmed that the top-ranked compounds were more informative about the chemical space and improved the prediction accuracy of the SVM model.\n\nIn summary, while we did not compare our method to other publicly available methods on benchmark datasets, we did perform comparisons to simpler baselines and demonstrated the effectiveness of active learning in efficiently sampling diverse chemical spaces for enzyme promiscuity prediction.",
  "evaluation/confidence": "The evaluation of our method includes performance metrics with confidence intervals. For instance, the estimated accuracy (amax) for different datasets is provided with standard deviations, such as 75% \u00b1 0.6% for the Car dataset and 79% \u00b1 0.8% for the MenD dataset. These intervals give a sense of the variability and reliability of the accuracy measurements.\n\nStatistical significance is assessed using Barnard\u2019s exact test at a significance level of \u03b1 = 0.05, adjusted with the Bonferroni correction. This ensures that the results are robust and that the features identified as enriched are genuinely significant. For example, features like 1,3-tautomers for Car and aldehydes for MenD have very low p-values, indicating strong statistical significance.\n\nAdditionally, the learning curves for MenD and AAEH show a significantly faster gain in accuracy when using active learning compared to random selection. This difference is statistically significant, demonstrating that active learning is superior in these cases. However, for Car and HAPMO, no significant advantage was observed with active learning, likely due to the homogeneity of the datasets.\n\nThe experimental validation further supports the statistical significance of our findings. Compounds selected from the top 10% of active learning rankings caused large shifts in importance scores for discriminatory features, indicating that these compounds were more informative. Moreover, the refined SVM model identified new potential substrates, which were experimentally confirmed, reinforcing the method's effectiveness.",
  "evaluation/availability": "Not enough information is available."
}