{
  "publication/title": "A Cautionary Note on Predicting Social Judgments from Faces with Deep Neural Networks.",
  "publication/authors": "Keles U, Lin C, Adolphs R",
  "publication/journal": "Affective science",
  "publication/year": "2021",
  "publication/pmid": "34966898",
  "publication/pmcid": "PMC8664800",
  "publication/doi": "10.1007/s42761-021-00075-5",
  "publication/tags": "- Affective Science\n- Social Judgments\n- Facial Features\n- Machine Learning\n- Regression Models\n- Cross-Prediction\n- Social Attributes\n- Human Ratings\n- Feature Spaces\n- Variance Analysis\n- Semi-Partial Correlation\n- Generalizability\n- Out-of-Sample Testing\n- Studio Portraits\n- Ambient Photos\n- Regularization\n- Bootstrap Procedure\n- Permutation Analysis\n- False Discovery Rate\n- Linear Regression",
  "dataset/provenance": "The dataset used in this study was sourced from publicly available datasets. The training dataset consisted of 183 studio portraits of neutral, frontal, white faces from the Chicago Face Database. This database originally contained social attribute ratings for 597 portraits of neutral, frontal faces from four races, but only the white faces were used in this study. The database provides ratings by human subjects on 15 social attributes using a 1\u20137 Likert scale. The social attribute \"unusual\" was excluded because it was not rated in any of the out-of-sample test datasets.\n\nThe test datasets included ratings for various social attributes from independent sets of human subjects for faces that were different from those used in the training set. These datasets encompassed studio portraits as well as ambient photos taken in real-world contexts. The test datasets included:\n\n* The Lin et al. (2021) dataset, which included ratings for 100 studio portraits of frontal, neutral, white faces on 100 social attributes.\n* The Oh et al. (2020) dataset, which included ratings for 66 studio portraits of frontal, neutral, white faces on 14 social attributes.\n* The Walker et al. (2018) dataset, which included ratings for 40 studio portraits of frontal, neutral, white faces on seven social attributes.\n* The Oosterhof and Todorov (2008) dataset, which included ratings for 300 computer-generated frontal, neutral, white faces on nine social attributes.\n* The White et al. (2017) dataset, which originally included ratings for 1224 ambient photos of individuals of various races on five social attributes. Only 504 photos of white individuals were used in this study.\n\nAll faces in the training and test datasets were limited to white faces, and the effects of race and context were beyond the scope of this study. The data used in this research were from publicly available datasets and previously published studies.",
  "dataset/splits": "In our study, we utilized multiple datasets to train and test our models. The training dataset consisted of 183 studio portraits of neutral, frontal, white faces, each represented by various features in the respective facial feature space. These images were drawn from the Chicago Face Database.\n\nFor testing, we employed five out-of-sample datasets, each containing ratings on various social attributes from independent sets of human subjects. These datasets included:\n\n1. **Lin et al. (2021) dataset**: This included ratings for 100 studio portraits, with 60 novel faces, on 100 social attributes.\n2. **Oh et al. (2020) dataset**: This included ratings for 66 novel studio portraits on 14 social attributes.\n3. **Walker et al. (2018) dataset**: This included ratings for 40 novel studio portraits on seven social attributes.\n4. **Oosterhof and Todorov (2008) dataset**: This included ratings for 300 computer-generated faces on nine social attributes.\n5. **White et al. (2017) dataset**: This originally included ratings for 1224 ambient photos, but we used a subset of 504 photos of white individuals on five social attributes.\n\nThe distribution of data points in each test dataset varied, with some datasets containing studio portraits and others containing ambient photos taken in real-world contexts. The social attributes rated in the test datasets were not always identical to those in the training dataset, so we computed prediction accuracy for attributes that were the same or semantically highly similar to those in the training dataset.",
  "dataset/redundancy": "The datasets used in this study were split into training and test sets to ensure independence. The training dataset consisted of 183 face images, each represented by various features in a specific facial feature space. The models were then tested on five out-of-sample, independent datasets that are publicly available. These test datasets were selected to sample social judgments from different types of faces, including studio portraits of frontal, neutral faces, computer-generated faces, and ambient photos taken under unconstrained conditions.\n\nTo enforce independence, all faces in the training and test datasets were limited to white faces, and the effects of race and context were not considered. The test datasets included a mix of studio portraits and ambient photos, which varied in viewpoint, facial expression, and background. This variation ensured that the test sets were distinct from the training set, providing a robust evaluation of the models' generalizability.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the field of facial attribute prediction. The use of multiple, independent test datasets with varied conditions helps to mitigate overfitting and ensures that the models can generalize well to new, unseen data. This approach is crucial for assessing the reliability and validity of the models in real-world applications.",
  "dataset/availability": "The data utilized in this study are sourced from publicly available datasets. These datasets can be accessed through the links provided in the cited papers. This ensures that the data is readily available to the public for verification and further research. The data includes various social attribute ratings for different sets of facial images, which were used to train and test our models. The specific datasets used include studio portraits, computer-generated faces, and ambient photos, all of which are publicly accessible. This transparency in data availability is crucial for reproducibility and for allowing other researchers to build upon our findings. The data is made available under the terms that allow for use, sharing, adaptation, distribution, and reproduction, as long as appropriate credit is given to the original authors and the source. This approach aligns with the principles of open science, promoting collaboration and further advancements in the field.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is linear regression, specifically L2-regularized linear regression, also known as ridge regression. This method was chosen to fit a set of model weights separately for each social attribute, optimally mapping facial features onto human subjects\u2019 social judgments from faces.\n\nThe algorithm used is not new. Ridge regression has been established in the field for some time, with its foundations laid by Hoerl and Kennard in 1970. The decision to use this well-known method was driven by its effectiveness in handling multicollinearity and overfitting, which are common issues in high-dimensional data like facial features. The focus of this study is on the application of these algorithms to predict social judgments from facial features, rather than the development of new machine-learning algorithms. Therefore, it is appropriate for this work to be published in a journal focused on affective science rather than a machine-learning journal.",
  "optimization/meta": "The models employed in this study are not meta-predictors. They are regularized linear regression models, specifically ridge regression, LASSO regression, and ordinary least squares (OLS) regression. These models were used to predict social judgments from facial features. The features used in these models come from three distinct spaces: a pre-trained deep convolutional neural network (DCNN) for face identification, a pre-trained DCNN for object recognition, and facial geometry features.\n\nThe training data for these models consisted of neutral, frontal images of white faces from the Chicago Face Database, along with corresponding human ratings for various social attributes. The models were then tested on five out-of-sample datasets, which included different types of face images and social attribute ratings from independent samples of human subjects.\n\nThe choice of ridge regression as the primary method was based on its superior prediction accuracies across social attributes and test datasets compared to LASSO regression and OLS regression. The regularization parameter for ridge and LASSO regression was selected using a cross-validation procedure, while OLS regression did not require this step.\n\nIn summary, the models do not use data from other machine-learning algorithms as input. They are standalone linear regression models trained on specific facial feature spaces to predict social judgments from faces. The training data is independent, as it comes from a separate dataset (the Chicago Face Database) and is used to predict ratings in different, out-of-sample test datasets.",
  "optimization/encoding": "In our study, the data encoding and preprocessing involved several key steps to prepare the facial images for the machine-learning algorithm. Initially, the output from the block5_conv2 layer of a deep convolutional neural network (DCNN) had a volume shape of 14 \u00d7 14 \u00d7 512, which was then flattened into a 100,352-dimensional feature vector. This vector represented each face image with 100,352 features.\n\nGiven the high dimensionality, principal component analysis (PCA) was employed to reduce the dimensionality and redundancy of these features. The goal was to retain a smaller number of principal components (PCs) from the 100,352 features. To avoid biasing the PCs of the test datasets with the variance from the training dataset, PCA was performed using a larger and more comprehensive set of faces. Specifically, face images of 426 white adults with neutral expressions, aggregated from three publicly available face databases, were used. The optimal number of PCs was determined based on their performance in predicting social judgments from the faces in the model training dataset, which consisted of 183 studio portraits from the Chicago Face Database. The first 26 PCs were found to offer the best average prediction accuracy across all 14 social attributes and were used to represent the DCNN-Object features in subsequent analyses.\n\nIn addition to the DCNN features, a complementary set of interpretable face features was used. These features included 30 physical and geometric characteristics of the face, such as skin luminance, eye dimensions, and facial proportions. These features were automatically measured using pre-trained models of facial landmark detection and face parsing. The facial landmark detection model, built using an ensemble-of-regression-trees approach and trained on the IBUG 300-W dataset, estimated the location of 68 key points on each face image. The face parsing model, implemented with a BiSeNet architecture and trained on the CelebAMask-HQ dataset, segmented each face image into various facial parts like the skin area, eyes, and nose. These automated methods allowed for the extraction of 30 facial-geometry features that closely imitated the manually measured features provided in the Chicago Face Database.\n\nThe final step in data encoding involved fitting L2-regularized linear regression (ridge regression) models to map the facial features onto human subjects\u2019 social judgments. Cross-validation was used to determine the optimal regularization parameter for ridge regression, ensuring that the models were robust and generalizable. This preprocessing and encoding pipeline enabled the effective use of both high-dimensional DCNN features and interpretable facial-geometry features in predicting social judgments from facial images.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the feature space. For the DCNN-Object features, we used principal component analysis (PCA) to reduce the dimensionality of the 100,352-dimensional feature vectors. The optimal number of principal components (PCs) was determined based on their performance in predicting social judgments from faces. Specifically, we found that the first 26 PCs offered the best average prediction accuracy across all social attributes. Therefore, 26 parameters were used for the DCNN-Object features.\n\nFor the Facial-Geometry features, we automatically measured 30 physical and geometric features that closely imitate the manually measured features provided in the Chicago Face Database. Thus, 30 parameters were used for the Facial-Geometry features.\n\nThe selection of the number of parameters was based on the performance of the models in predicting social judgments. For the DCNN-Object features, we increased the number of PCs from 10 to 110 with a step size of one and fit ridge regression models to predict the ratings of the faces. The number of PCs that provided the best average prediction accuracy across all social attributes was selected. For the Facial-Geometry features, the number of parameters was determined based on the availability of automatically measurable features that closely imitate the manually measured features.",
  "optimization/features": "In our study, we utilized three distinct types of facial features as inputs for our models. The first type, derived from a deep convolutional neural network (DCNN), initially produced a high-dimensional feature vector of 100,352 features per face image. To manage this dimensionality, we employed principal component analysis (PCA) to reduce the features. We determined that the first 26 principal components (PCs) offered the best average prediction accuracy across all social attributes. Therefore, these 26 PCs were used to represent the DCNN-Object features in our subsequent analyses.\n\nThe second type of features consisted of 30 physical and geometric facial features. These features were automatically measured using pre-trained models for facial landmark detection and face parsing. The features included measurements such as nose width, eye height, face length, and facial width-to-height ratio, among others.\n\nFeature selection was performed using PCA for the DCNN-derived features. To prevent biasing the PCs of the test datasets with the variance in the training dataset, PCA was performed using a larger and more comprehensive set of faces from publicly available databases. The optimal number of PCs was determined based on their performance for predicting social judgments from the faces in the model training dataset.\n\nFor the facial-geometry features, no further feature selection was performed beyond the initial set of 30 features that were automatically measured. These features were chosen based on their relevance to social perception, as indicated by previous research.\n\nIn summary, the input features for our models consisted of 26 PCs derived from the DCNN features and 30 facial-geometry features. Feature selection using PCA was performed using a separate, comprehensive set of faces to ensure unbiased results.",
  "optimization/fitting": "In our study, we employed L2-regularized linear regression, also known as ridge regression, to fit models that mapped facial features to human social judgments. This method is particularly useful when the number of parameters is much larger than the number of training points, which was the case in our analysis.\n\nTo address the risk of overfitting, we utilized cross-validation. Specifically, we randomly split the training dataset into 80% training and 20% validation samples for 2,000 iterations. At each iteration, we fitted models using a range of regularization parameters and evaluated their performance on the validation set. The regularization parameter that minimized the average error across all iterations was selected, and the model weights were refit with this optimal parameter using the entire training dataset. This procedure helped to ensure that our models generalized well to unseen data.\n\nTo further validate our approach, we repeated the procedure using additional evaluation metrics, including the coefficient of determination (R2) and the root mean square error (RMSE). The results corroborated those obtained using mean squared error (MSE), providing additional confidence in our model selection.\n\nMoreover, we compared ridge regression with other linear regression methods, such as LASSO regression and ordinary least squares (OLS) regression. Ridge regression consistently provided the best predictions across social attributes and test datasets. LASSO regression yielded similar prediction accuracies, but OLS regression performed worse due to multicollinearity in the features. This comparison underscored the effectiveness of ridge regression in handling high-dimensional data and mitigating overfitting.\n\nTo rule out underfitting, we ensured that our models were sufficiently complex to capture the underlying patterns in the data. The use of a large number of features and the application of regularization techniques allowed us to balance model complexity and generalization performance. Additionally, the variance partitioning analysis helped us understand the unique and shared explained variance between different feature spaces, further validating the robustness of our models.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. We utilized L2-regularized linear regression, also known as ridge regression, which helps to mitigate overfitting by adding a penalty term to the loss function. This penalty term constrains the magnitude of the model coefficients, thereby reducing the complexity of the model and preventing it from fitting the noise in the training data.\n\nTo determine the optimal regularization parameter for ridge regression, we employed a cross-validation procedure. Specifically, the training dataset was randomly split into 80% training and 20% validation samples for 2,000 iterations. At each iteration, a range of regularization parameters were used to fit models to the training part, and each fitted model was used to predict the human ratings of the faces in the validation part. This procedure yielded a model accuracy per regularization parameter per iteration per social attribute, assessed with the mean squared error (MSE). The regularization parameter that minimized the average error across all iterations was selected, and the model weights were refit with this optimal regularization parameter using the entire training dataset.\n\nAdditionally, we repeated this procedure using evaluation metrics beyond MSE, including the coefficient of determination (R2) and the root mean square error (RMSE). The results corroborated those obtained using MSE. Furthermore, we compared the performance of ridge regression with other linear regression methods, such as LASSO regression and ordinary least squares (OLS) regression. Ridge regression provided the best predictions across social attributes and test datasets, indicating its effectiveness in preventing overfitting and generalizing well to unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we employed ridge regression with cross-validation to determine the optimal regularization parameter. The training dataset was split into 80% training and 20% validation samples for 2,000 iterations. At each iteration, a range of regularization parameters (n = 30, log-spaced between 1 and 100,000) were used to fit models to the training part, and each fitted model was used to predict the human ratings of the faces in the validation part. This procedure yielded a model accuracy per regularization parameter per iteration per social attribute, assessed with the mean squared error (MSE).\n\nThe optimal regularization parameter that minimized the average error across all iterations was selected, and the model weights were refit with this optimal regularization parameter using the entire training dataset. We also repeated this procedure using evaluation metrics in addition to MSE, including the coefficient of determination (R2) and the root mean square error (RMSE). The results corroborated those using MSE.\n\nThe model files and optimization parameters are not explicitly provided in the publication. However, the methods and procedures used are thoroughly described, allowing for replication of the study. The datasets used, including the Chicago Face Database and other out-of-sample test datasets, are publicly available. The specific details of the datasets and their availability can be found in the references cited within the publication.\n\nThe license under which the datasets and methods are available is not specified in the provided context. However, it is common for academic publications to use datasets that are either in the public domain or available under licenses that permit academic use and replication of studies. For specific licensing information, one would need to refer to the original sources of the datasets and any associated documentation.",
  "model/interpretability": "The models employed in this study are not entirely black-box systems. While the deep convolutional neural networks (DCNNs) used for feature extraction are complex and may be considered somewhat opaque, the subsequent steps involving these features are more interpretable.\n\nThe features extracted by the DCNN-Identity network are more focused on identity-specific information, such as identity and gender, which are directly relevant to social judgments from faces. This makes the DCNN-Identity features more interpretable in the context of predicting social judgments. On the other hand, the DCNN-Object network extracts features that are more general to the images, including color and parts of the images, which are less directly related to social judgments.\n\nThe use of regularized linear regression models to map these features onto human social judgments provides a level of transparency. The regression models allow for the examination of the weights assigned to different features, indicating their importance in predicting social judgments. This means that the contribution of each feature to the final prediction can be analyzed, making the model more interpretable.\n\nAdditionally, the variance partitioning analysis helps to understand the unique and shared variance explained by different feature spaces. This analysis provides insights into how different types of features contribute to the predictions, further enhancing the interpretability of the models.\n\nThe cross-prediction analysis also offers insights into the specificity of the models' predictions. By examining how models trained on one social attribute perform on predicting other social attributes, we can understand the generalizability and specificity of the learned representations. This analysis reveals that the models' predictions are not attribute-specific, indicating that they capture broader, correlated social judgments.\n\nIn summary, while the feature extraction process involves complex neural networks, the subsequent modeling and analysis steps provide a level of interpretability. The use of regression models and variance partitioning analysis allows for the examination of feature importance and the shared variance among different feature spaces, making the models more transparent and interpretable.",
  "model/output": "The model employed in our study is a regression model, specifically a linear regression model. We utilized L2-regularized linear regression, also known as ridge regression, to fit a set of model weights for each social attribute. This approach optimally mapped facial features onto human subjects' social judgments from faces. The model was trained using cross-validation to determine the optimal regularization parameter, ensuring robust and accurate predictions. Additionally, we explored other linear regression methods, such as LASSO regression and ordinary least squares (OLS) regression, to compare their performance. However, ridge regression provided the best predictions across social attributes and test datasets. The model's output is continuous ratings for social attributes, rather than discrete classifications. This allows for a nuanced understanding of how facial features influence social judgments. The predictions were evaluated using metrics such as the coefficient of determination (R2), root mean square error (RMSE), and Spearman rank-order correlation, which assessed the model's accuracy and generalizability across different datasets and attributes.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for all analysis procedures is publicly available. It can be accessed via the Open Science Framework (OSF) at the provided link. The code is shared under a license that permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source. A link to the Creative Commons license is also provided for further details. This ensures that other researchers can replicate the study, build upon the methods, or apply them to new datasets.",
  "evaluation/method": "The evaluation method employed in this study was comprehensive and multifaceted, ensuring the robustness and generalizability of the models. The models were initially trained on the Chicago Face Database, which consisted of 183 face images, each represented by various features. To assess the models' performance, five out-of-sample datasets were used. These datasets included a variety of face types, such as studio portraits, computer-generated faces, and ambient photos, ensuring that the models were tested on diverse and realistic conditions.\n\nThe evaluation process involved several key steps. First, the optimal regularization parameter was selected using cross-validation to minimize the average error across all iterations. This parameter was then used to refit the model weights with the entire training dataset. Additionally, evaluation metrics such as the coefficient of determination (R2) and the root mean square error (RMSE) were used alongside mean squared error (MSE) to select the regularization parameter, with results corroborating those obtained using MSE.\n\nTo predict ratings of social attributes for novel faces in each test dataset, the final fitted model for each social attribute was utilized. In cases where the test datasets did not include ratings for the exact same social attributes as the training set, models for semantically similar or opposite social attributes were used. A bootstrap procedure was employed to estimate the prediction accuracy of each model on each test dataset. This involved randomly sampling the face images and their ratings 10,000 times with replacement and computing the Spearman rank-order correlation between the resampled predicted and resampled human ratings per social attribute.\n\nThe Spearman rank-order correlation was chosen to assess model accuracy because it is more reliable than raw rating values, especially when ratings are collected on different scales. The mean prediction accuracy for each social attribute was obtained by averaging the accuracies across bootstrap iterations. For datasets with a large number of ambient photos, one image per identity was randomly sampled at each bootstrap iteration to prevent bias.\n\nTo determine the statistical significance of the mean prediction accuracy and estimate the chance threshold, a permutation analysis was conducted. This involved shuffling the ratings in the test datasets across face images and computing the Spearman correlation between the predicted and permuted ratings for each social attribute. This procedure was repeated 10,000 times to generate an empirical null distribution of correlations, under the null hypothesis that there is no relationship between facial features and social judgments. The chance threshold was set at the 95th percentile of this distribution, and the permutation p-value for each social attribute was defined as the proportion of null correlations that were greater than or equal to the observed prediction accuracy. P-values were corrected for multiple comparisons using the false discovery rate (FDR) procedure.\n\nTo ensure the robustness of the findings, the analysis was repeated using different linear regression methods, including LASSO regression and ordinary least square regression (OLS), in addition to ridge regression. The same cross-validation procedure was used to select the optimal regularization parameter for LASSO regression. The results indicated that ridge regression provided the best predictions across social attributes and test datasets.\n\nIn summary, the evaluation method involved a rigorous process of cross-validation, bootstrap sampling, permutation analysis, and the use of multiple regression techniques to ensure the models' accuracy, robustness, and generalizability across different face types and social attributes.",
  "evaluation/measure": "In the evaluation of our models, several performance metrics were employed to ensure a comprehensive assessment of their predictive accuracy. The primary metric used was the Spearman rank-order correlation, which measures the rank-order correlation between predicted and actual human ratings of social attributes. This metric was chosen because it is more reliable than raw rating values, especially when ratings are collected on different scales across datasets.\n\nIn addition to Spearman's correlation, other evaluation metrics were considered, including the coefficient of determination (R2) and the root mean square error (RMSE). These metrics were used to select the optimal regularization parameter during model training, and the results corroborated those obtained using the mean squared error (MSE).\n\nTo assess the statistical significance of the prediction accuracy, a permutation analysis was conducted. This involved shuffling the ratings in the test datasets and computing the Spearman correlation between the predicted and permuted ratings. This procedure was repeated 10,000 times to generate an empirical null distribution, from which the chance threshold and permutation p-values were determined. The p-values were corrected for multiple comparisons using the false discovery rate (FDR) procedure.\n\nThe set of metrics used is representative of common practices in the literature, ensuring that our evaluation is robust and comparable to other studies in the field. The use of Spearman's correlation, in particular, aligns with the need to handle different rating scales and focuses on the rank order of faces based on social attributes, which is a more reliable metric. The inclusion of R2 and RMSE, along with the permutation analysis, provides a thorough evaluation of model performance and statistical significance.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the performance of different linear regression methods to predict social judgments from facial features. We compared ridge regression, LASSO regression, and ordinary least squares (OLS) regression. The comparison was conducted across various social attributes and test datasets.\n\nRidge regression was found to provide the best predictions, with mean prediction accuracies measured by Spearman's \u03c1 of 0.552 \u00b1 0.197 for DCNN-Identity models, 0.430 \u00b1 0.218 for DCNN-Object models, and 0.385 \u00b1 0.213 for Facial-Geometry models. LASSO regression yielded similar prediction accuracies to ridge regression, while OLS regression performed worse, particularly for DCNN-Identity and Facial-Geometry models due to multicollinearity in the features.\n\nThe comparison was performed on out-of-sample test datasets that included studio portraits and ambient photos of faces, ensuring a robust evaluation across different types of facial images. The test datasets were selected to sample social judgments from various face types, including studio portraits, computer-generated faces, and ambient photos taken under unconstrained conditions.\n\nWe also addressed potential concerns about the superior performance of DCNN-Identity models. To ensure that the performance was not merely due to the larger number of features, we applied principal component analysis (PCA) on the DCNN-Identity features and used only the first 30 principal components for model fitting. Additionally, we used features from a different DCNN for face identification to confirm that the superior performance was not idiosyncratic to the specific network architecture.\n\nIn summary, our evaluation included a thorough comparison of different regression methods and feature spaces, ensuring that the best-performing models were identified and validated across diverse datasets and social attributes.",
  "evaluation/confidence": "To evaluate the confidence in our results, several statistical methods were employed. First, a bootstrap procedure was used to estimate the prediction accuracy of each model on each test dataset. This involved randomly sampling the face images and their ratings 10,000 times with replacement, and computing the Spearman rank-order correlation between the resampled predicted and resampled human ratings per social attribute. The mean prediction accuracy for each social attribute was obtained by averaging the accuracies across these bootstrap iterations.\n\nTo assess the statistical significance of the mean prediction accuracy, a permutation analysis was performed. This generated an empirical null distribution of correlations for each social attribute and test dataset separately. At each permutation iteration, the ratings in a test dataset were shuffled across face images, and the Spearman correlation between the predicted and permuted ratings was computed. This procedure was repeated 10,000 times to obtain a distribution of correlations under the null hypothesis that there is no relationship between facial features and social judgments from faces. The chance threshold was determined by taking the 95th percentile of the empirical null distribution. The permutation p-value for each social attribute was defined as the proportion of the null correlations that were greater than or equal to the observed prediction accuracy. These p-values were corrected for multiple comparisons across the predicted social attributes using the false discovery rate (FDR) procedure.\n\nAdditionally, the performance metrics included mean bootstrap residual cross-prediction accuracy, bootstrap standard deviation, and significance levels computed via permutation tests and FDR corrected. This comprehensive statistical approach ensures that the results are robust and that the claims of superiority over other methods and baselines are statistically significant.",
  "evaluation/availability": "All data used in the evaluation process are from publicly available datasets. These datasets can be accessed via the links provided in the cited papers. The analysis codes used for the evaluation are also publicly available. They can be found at the provided URL. The article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source. The images or other third-party material in this article are included in the article's Creative Commons license, unless indicated otherwise. If material is not included in the article's Creative Commons license and the intended use is not permitted by statutory regulation or exceeds the permitted use, permission must be obtained directly from the copyright holder."
}