{
  "publication/title": "Antibody attributes that predict the neutralization and effector function of polyclonal responses to SARS-CoV-2.",
  "publication/authors": "Natarajan H, Xu S, Crowley AR, Butler SE, Weiner JA, Bloch EM, Littlefield K, Benner SE, Shrestha R, Ajayi O, Wieland-Alter W, Sullivan D, Shoham S, Quinn TC, Casadevall A, Pekosz A, Redd AD, Tobian AAR, Connor RI, Wright PF, Ackerman ME",
  "publication/journal": "BMC immunology",
  "publication/year": "2022",
  "publication/pmid": "35172720",
  "publication/pmcid": "PMC8851712",
  "publication/doi": "10.1186/s12865-022-00480-w",
  "publication/tags": "- SARS-CoV-2\n- IgG\n- IgM\n- IgA\n- Neutralization\n- Effector function\n- Antibody\n- Machine learning\n- COVID-19\n- Humoral response",
  "dataset/provenance": "The dataset used in this study was sourced from convalescent subjects who had been infected with SARS-CoV-2. The infection was confirmed through nasopharyngeal swab PCR. Plasma samples were collected from subjects at Johns Hopkins Medical Institutions (JHMI), while serum samples were collected from subjects at Dartmouth-Hitchcock Medical Center (DHMC). These samples were obtained approximately one month after symptom onset or the first positive PCR test, depending on the severity of the disease.\n\nThe dataset includes samples from 126 convalescent subjects from JHMI, 20 convalescent subjects from DHMC, and 15 na\u00efve subjects from the New Hampshire area, who served as negative controls. The na\u00efve subjects' seronegative status was confirmed by clinical laboratory assays.\n\nThe data has been used in previous research, including a study published in 2021. The dataset and associated materials are available to the community for further analysis and research. The specific datasets used and analyzed in this study can be accessed at the following GitHub repositories: [Acker manLab/ Butler_ et_ al_ COVID_ 2020](https://github.com/Acker manLab/Butler_et_al_COVID_2020) and [Acker manLab/ Natarajan_ et_ al_ COVID_ 2021](https://github.com/Acker manLab/Natarajan_et_al_COVID_2021).",
  "dataset/splits": "In our study, we utilized two primary cohorts for our analysis. The discovery cohort consisted of 126 adult convalescent plasma donors diagnosed with SARS-CoV-2 infection. These donors were from the Baltimore, MD, and Washington DC areas, and are referred to as the JHMI cohort. The validation cohort comprised 20 SARS-CoV-2 convalescent individuals from the Hanover, New Hampshire area, known as the DHMC cohort. Additionally, samples from 15 na\u00efve subjects collected from the New Hampshire area early in the pandemic served as negative controls.\n\nThe JHMI cohort was used for the initial modeling and feature selection process. We employed a five-fold cross-validation technique within this cohort to tune the lambda parameter and minimize mean squared error. This process was repeated 200 times to ensure the robustness of our findings. The final model was selected based on the median mean squared error obtained from these repeated runs.\n\nThe DHMC cohort was used as a validation set to assess the generalizability of our models. This cohort offered rigor due to differing demographics, sample collection methods, and handling processes compared to the JHMI cohort. The validation process involved comparing the model's predictions on the DHMC cohort to the observed functional activities.\n\nIn summary, our dataset splits included a discovery cohort of 126 samples, a validation cohort of 20 samples, and an additional 15 negative control samples. The discovery cohort was further divided into five folds for cross-validation, and the entire process was repeated 200 times to ensure the reliability of our models.",
  "dataset/redundancy": "The datasets used in this study were split into a discovery cohort and a validation cohort. The discovery cohort comprised 126 adult convalescent plasma donors diagnosed with SARS-CoV-2 infection from the Baltimore, MD, and Washington DC area. The validation cohort consisted of 20 SARS-CoV-2 convalescent individuals from the Hanover, New Hampshire area. This split was designed to ensure independence between the training and test sets, with the validation cohort offering rigor through differing demographics, sample collection methods, and handling processes.\n\nThe discovery cohort, referred to as the JHMI cohort, was used to develop and train the models. The validation cohort, known as the DHMC cohort, was used to assess the generalizability of the models. The independence of these sets was enforced by selecting cohorts from different geographical locations with varying demographics and sample handling procedures. This approach aimed to mimic real-world variability and ensure that the models could perform well across different populations and conditions.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the field of immunology. The use of diverse cohorts with differing characteristics enhances the robustness of the models, making them more applicable to a broader range of scenarios. The rigorous selection process and the inclusion of negative controls further strengthen the reliability of the findings.",
  "dataset/availability": "The datasets used and analyzed in our study are publicly available. They can be accessed at the following GitHub repositories:\n\n* https://github.com/AckermanLab/Butler_et_al_COVID_2020\n* https://github.com/AckermanLab/Natarajan_et_al_COVID_2021\n\nThese repositories contain the data splits used in our research, ensuring transparency and reproducibility. The data is shared under a license that allows for open access and use by the research community, fostering further collaboration and validation of our findings. The availability of these datasets enables other researchers to build upon our work, conduct independent analyses, and contribute to the broader scientific understanding of the topics we explored.",
  "optimization/algorithm": "The machine-learning algorithm class used is regularized generalized linear modeling. This approach was chosen for its prior success in identifying interpretable factors that contribute to functional activity while avoiding overfitting. The lambda parameter was tuned using five-fold cross-validation to minimize mean squared error. A process of 200-times repeated modeling was used to investigate the potential of different combinations of biophysical features for modeling. The final model was selected based on the median mean squared error obtained among the repeated runs in the cohort. The selected features and their coefficients were reported at a value of lambda at which median model performance fell one standard error above the minimum to optimize generalizability and provide more regularization to the model.\n\nThis algorithm is not new. It is a well-established method in the field of machine learning and statistics. The reason it was not published in a machine-learning journal is that the focus of the study was on applying this method to understand the humoral response features that may drive complex antibody functions and to enable robust predictions from surrogate measures in the context of SARS-CoV-2. The innovation lies in the application of this method to a specific biological problem rather than in the development of a new algorithm.",
  "optimization/meta": "The model employed in this study does not function as a meta-predictor. Instead, it utilizes a regularized generalized linear modeling approach to predict antibody functions. This approach was chosen for its ability to identify interpretable factors that contribute to functional activity while avoiding overfitting. The model was trained using Fc Array features to predict each antibody function with minimal mean squared error.\n\nThe process involved five-fold cross-validation to evaluate generalizability within the JHMI cohort. Additionally, models were trained on permuted functional data to establish robustness. The cross-validated models demonstrated similar accuracy when applied to held-out subjects and when predicting effector function and neutralization activity in the validation cohort (DHMC).\n\nThe model consistently selected a subset of features for each function, with frequently contributing features being primarily related to spike recognition and driven by IgG and Fc\u03b3R-binding antibodies. The use of regularization helped to simplify the models, improving interpretability at the cost of eliminating highly correlated features. This approach ensures that the selected variables are meaningful and reduces the risk of overfitting, thereby enhancing the model's generalizability.",
  "optimization/encoding": "The data encoding process involved utilizing biophysical features of antibodies to predict various functional responses. These features were derived from an Fc Array, which measures antibody characteristics such as Fc domain properties and antigen-specificity. The data was pre-processed to include measurements of antibody effector functions, including antibody-mediated phagocytosis, cytotoxicity, complement deposition, and neutralization potency. The goal was to create a comprehensive profile of the polyclonal antibody response raised by natural SARS-CoV-2 infection. This profile was then used to train machine learning models, specifically regularized generalized linear models, to predict antibody functions with minimal mean squared error. The models were evaluated using five-fold cross-validation within the discovery cohort and validated against an independent cohort to ensure generalizability. The selected features for each functional response were those that appeared frequently in repeated modeling, indicating their importance in predicting antibody activities.",
  "optimization/parameters": "In our study, the lambda parameter (\u03bb) was tuned using five-fold cross-validation to minimize mean squared error (MSE). This process was repeated 200 times to explore different combinations of biophysical features for modeling. The final model was selected based on the median MSE obtained from these repeated runs in the JHMI cohort. The selected features and their coefficients were reported at a value of \u03bb where the median model performance was one standard error above the minimum. This approach aimed to optimize the generalizability of the model and provide more regularization.\n\nThe model consistently selected a subset of features for each function, with frequently contributing features being primarily related to spike recognition and driven by IgG and Fc\u03b3R-binding antibodies. The magnitudes of feature contributions were evaluated, and a representative model for each function demonstrated the identity and relative coefficients of the contributing features. Despite their sparseness compared to control antigens, the models relied almost exclusively on antibody responses to the SARS-CoV-2 spike.",
  "optimization/features": "In our study, we utilized a comprehensive set of biophysical features derived from the Fc Array to model antibody functions. The specific number of features (f) used as input varied depending on the modeling process, as we employed a feature selection method to identify the most relevant predictors.\n\nFeature selection was indeed performed to enhance the model's generalizability and interpretability. This process involved using a regularized generalized linear modeling approach with L1-penalization (LASSO), which helped to eliminate variables that were less relevant to the outcome. By imposing a penalty on the absolute value of the feature coefficients, we aimed to reduce overfitting and reinforce the model's performance.\n\nThe feature selection was conducted using the training set only, specifically the JHMI cohort. We employed a process of 200-times repeated modeling with five-fold cross-validation to investigate the potential of different combinations of biophysical features. The final model was selected based on the median mean squared error (MSE) obtained among the repeated runs in the JHMI cohort. This approach ensured that the selected features and their coefficients were robust and generalizable. The lambda parameter (\u03bb) was tuned to optimize the model's performance, with the selected features and their coefficients reported at a value of \u03bb where the median model performance fell one standard error above the minimum. This strategy balanced model simplicity and biological relevance, providing insights into the key features driving antibody functions.",
  "optimization/fitting": "The fitting method employed in this study utilized a regularized generalized linear modeling approach, specifically designed to handle a large number of parameters relative to the number of training points. This method was chosen to predict antibody functions using Fc Array features, aiming to minimize mean squared error (MSE) and avoid overfitting.\n\nTo address the potential issue of overfitting, regularization through L1-penalization (LASSO) was applied. This technique imposes a penalty on the absolute value of the feature coefficients, effectively reducing the complexity of the model by eliminating variables that are less relevant to the outcome. This process helps in selecting a subset of features that are most predictive of the functional responses, thereby enhancing the model's generalizability.\n\nThe lambda parameter (\u03bb), which controls the amount of regularization, was tuned using five-fold cross-validation to minimize MSE. This cross-validation process was repeated 200 times to ensure robustness and to investigate the potential of different combinations of biophysical features. The final model was selected based on the median MSE obtained from these repeated runs, with the selected features and their coefficients reported at a value of \u03bb where the median model performance fell one standard error above the minimum. This approach optimizes the model's generalizability and provides more regularization.\n\nAdditionally, a permutation test procedure was conducted where the penalized multivariate regression was performed against randomized functional outcomes in the JHMI cohort, repeated 200 times. This step further validated the model's robustness and ensured that the selected features were genuinely contributing to the predictive power of the model, rather than being artifacts of the data.\n\nTo rule out underfitting, the model's performance was evaluated in terms of the degree of correlation between predicted and observed activity for a representative cross-validation replicate. This evaluation allowed for better visualization of model performance and ensured that the model was not too simplistic to capture the underlying patterns in the data. The cross-validated models trained on diverse data subsets showed similar accuracy when applied to held-out subjects as when used to predict effector function and neutralization activity observed in the validation cohort (DHMC). This consistency across different datasets indicates that the model is neither overfitting nor underfitting.",
  "optimization/regularization": "In our study, we employed regularization techniques to prevent overfitting and enhance the generalizability of our models. Specifically, we used L1-penalization, also known as the Least Absolute Shrinkage and Selection Operator (LASSO). This method imposes a penalty on the absolute value of the feature coefficients, effectively shrinking some coefficients to zero. This process helps in eliminating variables that are less relevant to the outcome, thereby simplifying the model and reducing the risk of overfitting. The lambda parameter (\u03bb) was tuned using five-fold cross-validation to minimize mean squared error (MSE). We conducted a process of 200-times repeated modeling to explore the potential of different combinations of biophysical features. The final model was selected based on the median MSE obtained from these repeated runs. Additionally, we reported the selected features and their coefficients at a value of \u03bb where the median model performance fell one standard error above the minimum. This approach optimized the generalizability of the model and provided more regularization.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are not explicitly detailed in the main text. However, the lambda parameter (\u03bb) was tuned using five-fold cross-validation to minimize mean squared error (MSE). This process was repeated 200 times to investigate the potential of different combinations of biophysical features for modeling. The final model was selected based on the median MSE obtained among the repeated runs in the JHMI cohort. The selected features and their coefficients were reported at a value of \u03bb where the median model performance fell one standard error above the minimum. This approach was taken to optimize the generalizability and provide more regularization to the model.\n\nThe datasets used and analyzed in this study are available at the following GitHub repositories: [Butler et al. COVID 2020](https://github.com/AckermanLab/Butler_et_al_COVID_2020) and [Natarajan et al. COVID 2021](https://github.com/AckermanLab/Natarajan_et_al_COVID_2021). These repositories likely contain the necessary files and parameters used for the optimization process, although the specific licensing details are not provided here. For more information on the availability and terms of use, please refer to the repositories directly.\n\nThe supplementary information, including additional tables and figures, is available online at the provided DOI link. This supplementary material may contain further details on the optimization parameters and configurations used in the study.",
  "model/interpretability": "The model employed in this study is not a black-box model. Instead, it is designed to be interpretable, allowing for a clear understanding of the contributing factors to the predicted outcomes. This interpretability is achieved through the use of regularization techniques, which simplify the model by selecting a subset of features that are most relevant to the prediction task. This process helps to eliminate highly correlated features, making the model more interpretable at the cost of potentially obscuring some biological mechanisms.\n\nThe model consistently selects a subset of features for each functional response, with frequently contributing features being more likely to have higher coefficients. These features are primarily related to spike recognition and are driven by IgG and Fc\u03b3R-binding antibodies. This consistency across repeated modeling indicates that the selected features are robust and not merely artifacts of the specific dataset or modeling process.\n\nFor example, the final models predictive of each function rely almost exclusively on antibody responses to the SARS-CoV-2 spike. The identity and relative coefficients of the contributing features are presented in a representative model for each function, providing a clear view of how different biophysical features contribute to the predictions. This transparency allows researchers to understand the underlying mechanisms and the importance of each feature in driving the functional responses.\n\nAdditionally, the use of hierarchical clustering and correlation analysis further enhances the interpretability of the model. These methods help to visualize the relationships between different antibody features and their contributions to the predicted outcomes, making it easier to identify key drivers of functional responses.",
  "model/output": "The model employed in our study is a regression model, specifically a regularized generalized linear modeling approach. This method was chosen to predict various antibody functions using Fc Array features, with the goal of minimizing mean squared error. The model was trained and validated using datasets from two cohorts: the Johns Hopkins Medical Institutions (JHMI) cohort for training and the Dartmouth Hitchcock Medical Center (DHMC) cohort for validation. The performance of the model was evaluated using metrics such as mean squared error and correlation between predicted and observed responses. The model consistently selected a subset of features for each function, with frequently contributing features being primarily related to spike recognition and driven by IgG and Fc\u03b3R-binding antibodies. Regularization was applied to enhance the model's predictive quality, simplifying the resulting models and improving their interpretability. However, this approach may also obscure potential biological mechanisms due to the trade-off between model simplification and the elimination of highly correlated features.",
  "model/duration": "The execution time for the model involved several steps. The lambda parameter was tuned using five-fold cross-validation to minimize mean squared error. This process was repeated 200 times to investigate the potential of different combinations of biophysical features for modeling. Additionally, a permutation test procedure was performed 200 times against randomized functional outcomes. These repeated modeling processes were conducted to ensure the robustness and generalizability of the model. The specific duration for each step is not detailed, but the repeated nature of the modeling and validation processes indicates a significant investment of computational time.",
  "model/availability": "Not applicable",
  "evaluation/method": "The evaluation method employed in this study involved a rigorous process to ensure the robustness and generalizability of the models. The lambda parameter (\u03bb) was tuned using five-fold cross-validation to minimize mean squared error (MSE). This process was repeated 200 times to explore various combinations of biophysical features for modeling. The final model was selected based on the median MSE obtained from these repeated runs within the Johns Hopkins Medical Institutions (JHMI) cohort. The selected features and their coefficients were reported at a \u03bb value where the median model performance was one standard error above the minimum. This approach aimed to optimize generalizability and provide more regularization to the model.\n\nAdditionally, a permutation test procedure was conducted. This involved performing penalized multivariate regression against randomized functional outcomes in the JHMI cohort, also repeated 200 times. This step helped to validate the significance of the selected features. A correlation network was constructed using the biophysical features that were consistently selected throughout the repeated modeling process. This network analysis provided insights into the relationships between different features.\n\nThe study also utilized surrogate functional assays, which offered advantages in terms of throughput and reproducibility. However, it is acknowledged that these assays may have limitations in biological relevance. Future work may involve comparing these assays with more biologically relevant ones to gain deeper insights into the mechanisms underlying the observed correlations. The evaluation method thus combined statistical rigor with biological relevance to provide a comprehensive assessment of the models.",
  "evaluation/measure": "In our study, we primarily focused on mean squared error (MSE) as our key performance metric. This metric was used to evaluate the accuracy of our models in predicting antibody functions based on biophysical features. To ensure the robustness and generalizability of our models, we employed a five-fold cross-validation process. This approach helped us to minimize MSE and select the optimal lambda (\u03bb) parameter, which is crucial for regularization in our models.\n\nAdditionally, we conducted a permutation test procedure where the penalized multivariate regression was performed against randomized functional outcomes. This step was essential to validate the robustness of our models and to ensure that the selected features were indeed relevant to the observed outcomes.\n\nTo further assess model performance, we evaluated the degree of correlation between predicted and observed activity. This visualization allowed us to better understand how well our models were performing in predicting antibody functions.\n\nThe use of MSE as a primary performance metric is consistent with standard practices in the field of machine learning and statistical modeling. It provides a clear and quantifiable measure of model accuracy, making it a reliable metric for comparing different models and feature combinations. The combination of cross-validation, permutation testing, and correlation analysis ensures that our models are not only accurate but also robust and generalizable to new datasets.",
  "evaluation/comparison": "In our study, we employed a regularized generalized linear modeling approach to predict antibody functions using Fc Array features. This method was chosen for its prior success in identifying interpretable factors that contribute to functional activity while avoiding overfitting. To ensure the robustness and generalizability of our models, we utilized five-fold cross-validation within the JHMI cohort. This process helped us evaluate the models' performance and avoid overfitting to the training data.\n\nWe also compared our models to those trained on permuted functional data. This comparison established the robustness of our models by demonstrating that they could accurately predict antibody functions even when the data was shuffled, indicating that the models were not merely memorizing the training data but were capturing genuine underlying patterns.\n\nAdditionally, we validated our models using an independent cohort from Dartmouth Hitchcock Medical Center (DHMC). This validation step was crucial as it allowed us to assess whether our models could generalize to new, unseen data. The models trained on diverse data subsets from the JHMI cohort showed similar accuracy when applied to held-out subjects and when used to predict effector function and neutralization activity observed in the DHMC cohort.\n\nWhile we did not explicitly compare our methods to publicly available benchmark datasets or simpler baselines, the use of cross-validation and an independent validation cohort provides strong evidence of our models' robustness and generalizability. The consistent selection of a subset of features related to spike recognition and driven by IgG and Fc\u03b3R-binding antibodies further supports the biological relevance of our findings.",
  "evaluation/confidence": "The evaluation of our model's performance involved a rigorous process to ensure confidence in the results. We employed five-fold cross-validation to tune the lambda parameter, aiming to minimize mean squared error (MSE). This process was repeated 200 times to thoroughly investigate the potential of different combinations of biophysical features for modeling. The final model was selected based on the median MSE obtained from these repeated runs, ensuring robustness and reliability.\n\nTo optimize generalizability and provide more regularization, we reported the selected features and their coefficients at a value of lambda where the median model performance fell one standard error above the minimum. This approach helps in balancing model complexity and performance, making the results more trustworthy.\n\nIn addition, we conducted a permutation test procedure where penalized multivariate regression was performed against randomized functional outcomes. This was done 200 times in a repeated fashion to assess the statistical significance of our findings. The correlation network was also constructed using biophysical features that were repeatedly selected within the repeated modeling process, further validating the consistency and reliability of our results.\n\nThe use of standard error and repeated modeling ensures that our performance metrics are accompanied by confidence intervals, providing a clear understanding of the variability and reliability of our results. The statistical significance of our method's superiority over others and baselines is supported by the extensive cross-validation and permutation testing, making our claims robust and defensible.",
  "evaluation/availability": "The datasets used and analyzed in our study are publicly available. They can be accessed via GitHub repositories. Specifically, the data is available at the following links:\n\n* https://github.com/AckermanLab/Butler_et_al_COVID_2020\n* https://github.com/AckermanLab/Natarajan_et_al_COVID_2021\n\nThese repositories contain the raw evaluation files necessary for reproducing our findings. The data is released under terms that allow for its use in research and analysis, facilitating further studies and collaborations."
}