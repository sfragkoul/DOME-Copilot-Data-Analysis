{
  "publication/title": "AI Estimation of Gestational Age from Blind Ultrasound Sweeps in Low-Resource Settings.",
  "publication/authors": "Pokaprakarn T, Prieto JC, Price JT, Kasaro MP, Sindano N, Shah HR, Peterson M, Akapelwa MM, Kapilya FM, Sebasti\u00e3o YV, Goodnight W 3rd, Stringer EM, Freeman BL, Montoya LM, Chi BH, Rouse DJ, Cole SR, Vwalika B, Kosorok MR, Stringer JSA",
  "publication/journal": "NEJM evidence",
  "publication/year": "2022",
  "publication/pmid": "36875289",
  "publication/pmcid": "PMC9980216",
  "publication/doi": "10.1056/evidoa2100058",
  "publication/tags": "- Ultrasound\n- Gestational Age\n- Deep Learning\n- Biometry\n- Obstetrical Care\n- Low-Resource Settings\n- Neural Network\n- Pregnancy Dating\n- Medical Imaging\n- AI in Healthcare",
  "dataset/provenance": "The dataset for this study was sourced from participants with viable singleton pregnancies enrolled between September 2018 and June 2021. The data collection involved four research sites, resulting in 4695 participants contributing a total of 8775 ultrasound studies. After applying participant-level and visit-level exclusions, the dataset was divided into training, tuning, and test sets.\n\nThe training and tuning sets comprised 3509 participants, contributing 5958 study scans. These scans included 109,806 blind sweeps, comprising 21,264,762 individual image frames. The test sets included 716 participants in the main test set, 47 in the IVF test set, and 249 in the novice test set. Each participant in the test sets contributed only a single study scan, chosen at random from all the scans they contributed.\n\nThe data used in this study is unique and has not been previously published or used by the community in the same manner. The dataset is particularly large, with over 21 million individual image frames, which is significantly larger than most prior high-profile applications of deep learning to medical imaging. This extensive dataset was crucial for training the deep-learning model to estimate gestational age accurately.",
  "dataset/splits": "We created five nonoverlapping data splits to develop and evaluate our deep learning model. Two of these splits were used for training and tuning the model, while the remaining three were used for testing its performance.\n\nThe training sets comprise all participants who remained after the creation of the test sets. These were split randomly, by participant, in a 4:1 ratio, resulting in a main training set with 2807 participants and a tuning set with 702 participants.\n\nThe three test sets were created first. The IVF test set includes 47 participants who conceived through in vitro fertilization. The novice test set contains 249 participants who had at least one study visit with sweeps collected by a novice user on a low-cost device. The main test set was selected at random from the remaining eligible participants, totaling 716 participants.\n\nSome participants contributed more than one study scan, but in such cases, only a single study scan was selected at random for inclusion in their respective test set. The distribution of participants in the test sets varied by geography, with the IVF test set and a portion of the main test set coming from North Carolina, and the novice test set and the remaining portion of the main test set coming from Zambia.",
  "dataset/redundancy": "The datasets were split into two training sets and three test sets. The training sets were used to develop and tune the deep learning model, while the test sets were used to assess its performance. The test sets included participants who had at least one study with both blind sweeps and sonographer-acquired biometry available, and whose gestational age was established by a prior scan or in vitro fertilization (IVF). The IVF test set comprised all participants who conceived by IVF, the novice test set included participants with scans collected by novice users on low-cost devices, and the main test set was a random selection from the remaining eligible participants.\n\nThe training and test sets are independent. This independence was enforced by ensuring that participants contributed only a single study scan to their respective test set, selected at random. Additionally, after creating the test sets, all remaining participants were randomly allocated into the main training set and the tuning set in a 4:1 ratio.\n\nRegarding the distribution compared to previously published machine learning datasets, this study's approach to dataset splitting and independence is designed to ensure robust model evaluation. The use of distinct test sets for different conditions (IVF, novice users, and a general main test set) allows for a comprehensive assessment of the model's performance across various scenarios. This methodology is crucial for validating the model's generalizability and reliability, which is a key consideration in medical applications.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is deep learning. Specifically, we employed a deep learning model to estimate gestational age from ultrasound sweeps. This approach leverages neural networks to analyze vast amounts of image data, enabling the model to learn complex patterns and features that are indicative of gestational age.\n\nThe deep learning algorithm used is not entirely new, as deep learning techniques have been widely applied in medical imaging and other fields. However, the application of deep learning to estimate gestational age from blind ultrasound sweeps is innovative and tailored to address specific challenges in obstetrical care, particularly in low-resource settings.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of our research is on its application in obstetrics rather than the development of new machine-learning techniques. Our primary goal was to demonstrate the practical utility of deep learning in improving gestational age estimation, which is crucial for providing quality obstetrical care. The model's performance was evaluated in the context of clinical relevance, showing superior accuracy compared to traditional biometry methods. This makes our work more aligned with medical and clinical research journals, where the impact on healthcare outcomes is of paramount importance.",
  "optimization/meta": "Not enough information is available.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for developing and tuning our deep learning model. We began by collecting ultrasound study scans, which included both biometry measurements and blind sweep cineloop videos. These videos were free-hand sweeps across the gravid abdomen, captured in multiple directions and configurations. Each video was approximately 10 seconds long and was collected using both a commercial ultrasound machine and a low-cost, battery-powered device.\n\nThe preprocessing steps involved several key actions. First, we ensured that each study scan contained at least two blind sweep cineloops. Scans that were uninterpretable due to missing image metadata or conducted before 9 weeks of gestation were excluded. This was necessary because early scans were too infrequent to allow effective model training.\n\nFor the blind sweep videos, we standardized the collection process to ensure consistency. This included setting specific depth parameters based on the participant's symphysial-fundal height measurement. The videos were then segmented into individual image frames, resulting in a large dataset of 21,264,762 frames from 109,806 blind sweeps.\n\nThe ground-truth gestational age was established using the best available method for each participant. This included prior ultrasound scans or in vitro fertilization (IVF) data, which provided a reliable benchmark for comparing the model's estimates. The gestational age was set according to prevailing care practices in each country. In North Carolina, it was determined using fetal biometry from the first scan and the reported last menstrual period (LMP). In Zambia, it was assigned solely based on the results of the first scan, as the LMP was less reliable.\n\nThe data was then split into training, tuning, and test sets. The test sets included participants with at least one study with both blind sweeps and sonographer-acquired biometry available. The training sets comprised all remaining participants, split randomly in a 4:1 ratio into a main training set and a tuning set. This division ensured that the model was trained on a diverse and representative dataset, allowing for robust performance evaluation.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "The input features for the deep-learning model consist of blind sweep cineloop videos. These videos are used directly as input without explicit feature selection. The model processes these videos to provide a gestational age estimate as output. The number of features is not explicitly stated as a fixed number since the input is a video, which inherently contains a vast amount of data. The model architecture and its constituent parts are designed to handle this type of input directly, extracting relevant features internally during the training process.",
  "optimization/fitting": "The deep learning model developed in this study utilized a substantial amount of data to ensure robust training and generalization. The training sets comprised a large number of study scans and individual image frames, which helped to mitigate the risk of overfitting. Specifically, the training sets included 5958 study scans comprising 109,806 blind sweeps and 21,264,762 individual image frames. This extensive dataset provided a rich source of information for the model to learn from, reducing the likelihood of overfitting.\n\nTo further address the potential for overfitting, the data was split into training and tuning sets in a 4:1 ratio. This split allowed for the model to be trained on a large dataset while also being tuned and validated on a separate subset of the data. The tuning set served as an independent validation set, helping to ensure that the model generalized well to unseen data.\n\nIn terms of underfitting, the model's architecture and the comprehensive nature of the training data helped to capture the complexities of the gestational age estimation task. The model was designed to receive blind sweep cineloop videos as input and provide a gestational age estimate as output, leveraging the rich information contained in these videos. The use of a deep learning approach, which can learn hierarchical features from the data, further helped to mitigate underfitting.\n\nAdditionally, the model's performance was assessed on multiple test sets, including the main test set, the IVF test set, and the novice test set. These test sets provided a rigorous evaluation of the model's performance, ensuring that it generalized well to different scenarios and populations. The model's superior performance in these test sets, as evidenced by lower mean absolute errors and higher proportions of correctly classified study scans, indicated that it was neither overfitted nor underfitted to the training data.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters are provided in the supplementary materials. Specifically, details including preprocessing steps, training procedures, and parameters are available in Supplementary Appendix Section 1 and Figure S1. These resources are accessible to the public and can be used by researchers and practitioners interested in replicating or building upon the work. The supplementary materials are intended to ensure transparency and reproducibility, allowing others to understand and utilize the methods and findings presented in the study.",
  "model/interpretability": "The model developed in this study is a deep-learning model, which inherently operates as a black box. This means that the internal workings of the model are not easily interpretable. The model processes a vast number of ultrasound image frames to make predictions about gestational age, but the specific features or patterns it uses to make these predictions are not explicitly known. The nature of deep-learning algorithms allows them to incorporate many facets of the available data to accomplish their tasks, rather than relying on specific, predefined features. While this approach enables the model to achieve high accuracy, it does not provide clear, transparent examples of how it arrives at its predictions. The model's performance is validated through comparisons with ground-truth gestational ages and biometry, but the exact mechanisms by which it processes the input data remain opaque.",
  "model/output": "The model developed is a regression model designed to estimate gestational age from ultrasound sweeps. It predicts a continuous value representing the gestational age in days, rather than classifying the data into discrete categories. This approach allows for a more precise assessment of gestational age, which is crucial for obstetrical care.\n\nThe model's performance is evaluated using metrics such as mean absolute error (MAE) and root mean squared error (RMSE), which are standard for regression tasks. These metrics provide a clear indication of the model's accuracy in predicting gestational age compared to ground-truth values established through reliable methods.\n\nIn various test sets, including the main test set, IVF test set, and novice test set, the model consistently demonstrates superior performance compared to traditional biometry methods. For instance, in the main test set, the model achieves an MAE of 3.9 days, which is significantly lower than the 4.7 days achieved by biometry. This indicates that the model provides more accurate gestational age estimates.\n\nThe model's regression nature allows it to handle the complexity of ultrasound data, incorporating a vast number of image frames to make precise predictions. This is particularly beneficial in low-resource settings where access to high-quality ultrasound equipment and trained sonographers may be limited. The model's ability to perform well even with data from untrained users and low-cost devices further underscores its potential impact on improving obstetrical care in such environments.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the deep learning model involved creating multiple datasets to assess its performance comprehensively. Two training sets were developed to tune the model, while three test sets were used to evaluate its predictive performance. The test sets included participants with specific criteria, such as those who conceived via in vitro fertilization (IVF) and those whose ultrasound scans were conducted by novice users on low-cost devices. The main test set was selected randomly from the remaining eligible participants.\n\nThe model's predictive performance was assessed by comparing its estimates of gestational age with the previously established ground-truth gestational age. The absolute difference between these estimates was used to calculate the absolute error of the prediction. The mean absolute error (MAE) and root mean squared error were reported for both the model and biometry. A paired t-test was used to compare the mean of the pairwise difference between the model's absolute error and biometry's absolute error. A negative mean of this pairwise difference, with a 95% confidence interval that does not include zero, would indicate that the AI model is statistically superior to biometry dating.\n\nThe evaluation also involved comparing the model's MAE with that of biometry across different subsets, including geography (Zambia vs. North Carolina) and trimester. The empirical cumulative distribution function (CDF) was plotted to compare the proportion of study scans where the absolute error was less than 7 or 14 days for the model versus biometry. The McNemar test was used to compare these proportions, and Wald-type 95% confidence intervals for the difference in proportions were computed.\n\nFor the novice test set, the diagnostic accuracy of the last menstrual period (LMP) reported at the first patient visit was also presented, as this is relevant for implementing the technology in low-resource settings. No formal statistical analysis plan was made for this diagnostic study. The primary outcome focused on comparing the model versus biometry in the main test set and IVF test set. Point estimates and 95% confidence intervals were provided for secondary and exploratory endpoints, without adjustments for multiple comparisons.",
  "evaluation/measure": "In the evaluation of our model's performance, we employed several key metrics to assess its accuracy and reliability in estimating gestational age. The primary metric reported is the mean absolute error (MAE), which measures the average absolute difference between the model's predictions and the ground-truth gestational age. We also report the standard error (SE) associated with the MAE to provide an indication of the variability in our estimates. Additionally, we calculated the root mean squared error (RMSE) to give a sense of the magnitude of the errors.\n\nTo compare the model's performance with traditional biometry methods, we used paired t-tests to assess the mean of the pairwise differences between the model's absolute errors and biometry's absolute errors. This allowed us to determine if the model's errors were significantly different from those of biometry. We also computed 95% confidence intervals (CIs) for these differences to understand the precision of our estimates.\n\nWe further analyzed the empirical cumulative distribution function (CDF) of the absolute errors to compare the proportion of study scans where the absolute error was within 7 or 14 days for both the model and biometry. This analysis was complemented by McNemar tests to statistically compare these proportions, along with Wald-type 95% CIs for the differences in proportions.\n\nIn addition to these metrics, we evaluated the model's performance across different subsets of the data, including geography (Zambia vs. North Carolina) and trimester (early, mid, and late pregnancy). This subset analysis helped us understand how the model performs in different contexts and stages of pregnancy.\n\nThe reported metrics are representative of standard practices in the literature for evaluating gestational age estimation models. The use of MAE, RMSE, and paired t-tests is common in similar studies, ensuring that our evaluation is comparable to other works in the field. The inclusion of CDF analysis and subset evaluations further enriches our understanding of the model's performance and its potential real-world applications.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our deep-learning model's performance against established biometry methods. We assessed the predictive performance of both approaches by comparing their estimates with the ground-truth gestational age, using metrics such as mean absolute error (MAE) and root mean squared error. We performed statistical tests, including paired t-tests and McNemar tests, to evaluate the significance of the differences observed.\n\nWe compared the model's MAE with that of biometry across the overall test datasets and specific subsets, such as geography (Zambia vs. North Carolina) and trimester. Additionally, we plotted the empirical cumulative distribution function (CDF) for the absolute errors produced by the model and biometry. This allowed us to compare the proportion of study scans where the absolute error was within 7 or 14 days for both methods.\n\nFor the novice test set, we also compared the model's performance with the last menstrual period (LMP) reported at the first patient visit, which is a relevant comparator in low-resource settings. This comparison included calculating the MAE and the proportion of study scans correctly classified within 7 and 14 days.\n\nOur primary outcome focused on comparing the model's performance against biometry in the main and IVF test sets. We did not perform a comparison with publicly available methods on benchmark datasets, nor did we compare against simpler baselines. The evaluation was designed to assess the model's superiority over biometry and LMP in specific clinical contexts.",
  "evaluation/confidence": "The evaluation of our model's performance includes several key metrics, each accompanied by confidence intervals to provide a measure of uncertainty. The mean absolute error (MAE) and root mean squared error (RMSE) are reported with standard errors (SE), offering a clear view of the variability in our estimates. For instance, in the main test set, the model's MAE is 3.9\u00b10.12 days, while biometry's MAE is 4.7\u00b10.15 days, with a statistically significant difference of \u22120.8 days (95% CI, \u22121.1 to \u22120.5; P<0.001). This indicates that our model's performance is not only superior but also statistically significant.\n\nWe employed paired t-tests to assess the mean of the pairwise differences between the model's absolute error and biometry's absolute error. The null hypothesis was that the mean of this pairwise difference is zero. A negative mean of the pairwise difference whose 95% confidence interval does not include zero would indicate statistical superiority. This was indeed the case, reinforcing the model's superiority over biometry.\n\nAdditionally, we used the McNemar test to compare the proportion of study scans in which the absolute error was less than 7 or 14 days for the model versus biometry. The results showed that the model outperformed biometry in both classification windows, with statistically significant differences. For example, 86.0% of study scans were correctly classified within 7 days by the model, compared to 77.0% by biometry (difference, 9.1 percentage points; 95% CI, 5.7 to 12.5 percentage points; P<0.001).\n\nIn the novice test set, while the model and biometry performed similarly, the model was clearly superior when compared to the last menstrual period (LMP). The MAE for the model was 4.9\u00b10.29 days, compared to 17.4\u00b11.17 days for the LMP (difference, \u221212.7 days; 95% CI, \u221215.0 to \u221210.3). The proportion of study scans correctly classified within 7 days was also substantially higher for the model than for the LMP (75.1% vs. 40.1%; difference, 36.1 percentage points; 95% CI, 28.0 to 44.2 percentage points).\n\nOverall, the confidence intervals and statistical tests employed in our evaluation provide a robust measure of the model's performance and its superiority over existing methods. The results are statistically significant, allowing us to confidently claim that our model offers improved accuracy in gestational age estimation.",
  "evaluation/availability": "The data sets used for this research will be made available to other investigators upon request through a third-party data-sharing platform. Access is subject to specific terms around acceptable use and attribution."
}