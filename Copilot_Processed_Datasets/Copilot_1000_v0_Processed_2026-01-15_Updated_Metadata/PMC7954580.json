{
  "publication/title": "A Hybrid Machine-Learning-Based Method for Analytic Representation of the Vocal Fold Edges during Connected Speech.",
  "publication/authors": "Yousef AM, Deliyski DD, Zacharias SRC, de Alarcon A, Orlikoff RF, Naghibolhosseini M",
  "publication/journal": "Applied sciences (Basel, Switzerland)",
  "publication/year": "2021",
  "publication/pmid": "33717604",
  "publication/pmcid": "PMC7954580",
  "publication/doi": "10.3390/app11031179",
  "publication/tags": "- Vocal Fold Segmentation\n- High-Speed Videoendoscopy\n- Connected Speech\n- Machine Learning\n- K-means Clustering\n- Glottal Area Detection\n- Image Gradient\n- Color HSV System\n- Active Contour Model\n- Phonatory Processes",
  "dataset/provenance": "The dataset used in this study was obtained from a vocally normal adult participant during the recitation of the \"Rainbow Passage.\" The data was recorded using a custom-built color high-speed videoendoscopy (HSV) system. The recording was conducted at the Center for Pediatric Voice Disorders, Cincinnati Children\u2019s Hospital Medical Center, and approved by the Institutional Review Board.\n\nThe recording length was approximately 29.14 seconds, consisting of 116,543 video frames. Each frame had a resolution of 256 x 256 pixels, and the video was saved as an uncompressed 24-bit RGB AVI file. The data was captured using a FASTCAM SA-Z color high-speed camera set at 4000 frames per second with a 12-bit color image sensor and 64 GB of cache memory. The camera was coupled with a 3.6 mm Olympus ENF-GP Fiber Rhinolaryngoscope and a 300 W xenon light source.\n\nThis dataset has not been used in previous papers by the community. The study aims to present a new spatio-temporal technique for automatically segmenting vocal fold edges in HSV data during connected speech, utilizing this specific dataset. The focus is on developing an automated method that does not require manual labeling of data points, making it efficient for analyzing large datasets.",
  "dataset/splits": "Not applicable.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data from this study is not publicly available in a forum. However, it can be made available upon request to the corresponding author. To access the data, an appropriate data sharing agreement must be executed. This ensures that the data is used responsibly and in accordance with the guidelines set by the authors and the institutions involved. The data sharing agreement helps to enforce the proper use of the data, maintaining the integrity and confidentiality of the information.",
  "optimization/algorithm": "The optimization algorithm employed in our work is based on unsupervised machine learning, specifically the k-means clustering technique. This method is well-established and widely used in image analysis for partitioning data into clusters with similar characteristics.\n\nThe k-means clustering algorithm is not new; it has been extensively utilized in various fields, including image segmentation. The choice to use k-means clustering in our study was driven by its effectiveness in grouping pixels based on their features, which is crucial for identifying the glottal area in high-speed videoendoscopy (HSV) data.\n\nGiven that k-means clustering is a mature and widely recognized technique, it was not necessary to publish it in a machine-learning journal. Instead, our focus was on applying this established method to a novel problem: the segmentation of vocal fold edges in HSV data during connected speech. This application required adapting and integrating k-means clustering with active contour modeling (ACM) to enhance the accuracy and robustness of edge detection.\n\nThe hybrid approach we developed combines the strengths of k-means clustering for initial segmentation with the precision of ACM for final edge detection. This combination addresses the limitations of ACM, such as sensitivity to contour initialization and computational cost, making it a powerful tool for analyzing vocal fold dynamics in connected speech.",
  "optimization/meta": "The model presented in this work is a hybrid approach that combines an unsupervised machine-learning technique with an active contour modeling (ACM) method. Specifically, the k-means clustering algorithm, a type of unsupervised machine-learning method, is used to group pixels in high-speed videoendoscopy (HSV) kymograms. This clustering step helps in identifying the glottal area by grouping similar pixels together, which are then used to initialize the contour for the ACM.\n\nThe k-means clustering algorithm processes the HSV kymograms by considering various features, such as the intensities of the red and green channels and the image gradient. These features are fed into the clustering algorithm to determine the most effective combination for accurately representing the vocal fold edges. The clustering results provide an initial contour that is then refined using the ACM technique.\n\nThe ACM method is an iterative energy minimization technique used for edge detection. It requires an initial contour near the edges of interest in an image. By using the results from the k-means clustering as the initial contour, the ACM can more accurately detect the glottal edges during vocal fold vibration.\n\nThe hybrid approach leverages the strengths of both methods: the k-means clustering provides a robust initialization for the ACM, while the ACM refines the edge detection with high precision. This combination ensures that the model can handle the complexities of HSV data during connected speech, providing accurate and reliable edge detection.\n\nThe training data for the k-means clustering is independent of the ACM process. The clustering algorithm groups pixels based on their features without any prior labeling or supervision, making it an unsupervised learning method. This independence ensures that the initial contours provided to the ACM are based on the intrinsic properties of the data, rather than any pre-existing labels or annotations.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to prepare the high-speed video (HSV) recordings for machine learning analysis. Initially, temporal segmentation was employed to automatically extract the timestamps of vibratory onsets and offsets of vocalized segments. This step was followed by denoising and motion compensation across video frames to accurately track the vocal folds within a defined window.\n\nThe HSV frames were then cropped based on the center and size of the motion window, focusing on the vibrating vocal folds. Kymograms, which are spatial-temporal plots, were extracted at various cross-sections along the anteroposterior length of the vocal folds from these cropped frames. The y-axis of each kymogram represents the left-right dimension of the video frame, while the x-axis denotes time, measured in frame numbers.\n\nTo mitigate noise, each kymogram was smoothed using a moving average 1D filter with a window size of 5 pixels along the y-axis. This process involved calculating the average intensity of the four neighboring pixels (two above and two below) for each pixel in the kymogram. Additionally, a Tukey window function with a window size of 15 pixels was applied to give higher weights to the middle section of the kymograms, where the glottal area is located, and lower weights to the top and bottom sections.\n\nFor feature extraction, we focused on the red and green color channels, excluding the blue channel due to its high noise level and lack of essential information. Three features were extracted: two intensity features based on the red and green channel pixel intensities, and a gradient feature. The gradient feature was computed along the x- and y-axes with a step size of 8 pixels, and an overall gradient magnitude was calculated by taking the square root of the sum of squared gradients in both directions.\n\nThese preprocessing steps ensured that the kymograms were ready for feature extraction and subsequent machine learning analysis, enabling accurate vocal fold edge representation.",
  "optimization/parameters": "In our study, we utilized three features as input parameters for our machine learning model. These features were selected based on their relevance to the task of vocal fold edge representation. The first two features were the intensity values of the red and green channels in the kymogram images. These intensities were chosen because the glottal areas, which are our regions of interest, typically have lower intensities compared to the surrounding laryngeal tissues. However, relying solely on these intensity features was not sufficient due to the high level of noise in the video data and the presence of dark pixels in non-glottal regions.\n\nTo address this, we introduced a third feature: the image gradient. The gradient feature helps in detecting the edges of the glottal area by capturing the contrast between the intensity of the glottis and the surrounding regions. This feature was computed along both the x and y axes of the kymogram, and an overall gradient magnitude was calculated by combining these gradients.\n\nThe selection of these three features was done through an iterative process where different combinations and numbers of features were tested. It was determined that using the red and green channel intensities along with the gradient feature provided the most accurate representation of the vocal fold edges. This combination allowed the model to better distinguish between the glottal and non-glottal areas, even in the presence of noise and varying lighting conditions.",
  "optimization/features": "In the optimization process, three features were used as input for the machine learning algorithm. These features included the intensities of the red and green channels, as well as the image gradient. The blue channel was excluded due to its high noise level and lack of essential information.\n\nFeature selection was indeed performed to determine the most effective combination of features for accurate vocal fold edge representation. Various subsets of features were tested to identify the optimal combination. The selection process involved evaluating different numbers and combinations of the aforementioned features to ensure the best performance of the machine learning method.\n\nThe feature selection process was conducted using the available data, ensuring that the chosen features would effectively distinguish the glottal area from the surrounding tissues in the kymograms. The combination of red and green channel intensities along with the gradient feature was found to provide the most accurate results, as it captured more information about the glottal area and improved the clustering performance.",
  "optimization/fitting": "Not applicable.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model presented in this work is not a black box. It combines machine learning techniques with active contour modeling, providing a transparent and interpretable approach to vocal fold edge detection in high-speed videoendoscopy (HSV) data during connected speech.\n\nThe machine learning component of the model uses k-means clustering, an unsupervised learning technique. This method groups pixels in the HSV kymograms into clusters based on their features, such as red and green channel intensities and image gradients. The clustering results are visually represented in scatter plots and binary labeled kymograms, making it clear how the data points are grouped. For instance, the glottal area cluster is shown with red diamonds, and the non-glottal cluster with blue circles. This visual representation allows for easy interpretation of which features contribute to the identification of the glottal area.\n\nAdditionally, the active contour modeling (ACM) technique used in the hybrid approach provides a clear and interpretable method for edge detection. The ACM algorithm iteratively deforms a contour to fit the edges of the glottal area, and this process can be visualized and understood. The initial contour is provided by the clustering results, and the final detected edges are shown in the original kymogram, making it evident how the model identifies the vocal fold edges.\n\nThe combination of these techniques ensures that the model's decisions are transparent and interpretable. The use of visual representations, such as scatter plots and labeled kymograms, allows researchers and clinicians to understand how the model identifies the glottal area and detects the vocal fold edges. This transparency is crucial for building trust in the model's results and for facilitating its use in clinical settings.",
  "model/output": "The model presented in this work is a classification model. It employs an unsupervised machine learning approach, specifically k-means clustering, to group pixels in high-speed videoendoscopy (HSV) kymograms into distinct clusters. The primary goal is to classify pixels into two categories: those belonging to the glottal area and those that do not. This classification is crucial for accurately detecting the edges of the vocal folds during connected speech.\n\nThe k-means clustering technique is used to identify and separate the glottal area from the rest of the image. Different subsets of features, such as red and green channel intensities and image gradients, are fed into the machine learning algorithm to determine the most effective combination for accurate vocal fold edge representation. The results demonstrate that using the gradient along with red and green channel intensities as features significantly improves the clustering performance, making it easier to distinguish between the glottal and non-glottal areas.\n\nThe output of the clustering process is then used to initialize an active contour model (ACM), which further refines the detection of the vocal fold edges. This hybrid approach combines the strengths of both unsupervised machine learning and active contour modeling, resulting in a robust and efficient method for vocal fold edge detection in HSV data during connected speech. The final output is a precise representation of the vocal fold edges, which can be used for further analysis of vocal fold vibrations and phonatory processes.",
  "model/duration": "The execution time of the proposed hybrid method varied depending on the quality and complexity of the high-speed videoendoscopy (HSV) data. The method involved several steps, including temporal segmentation, motion compensation, kymogram extraction, and the application of the hybrid spatial segmentation technique.\n\nThe k-means clustering technique, which is a key component of the hybrid method, was implemented for each kymogram. The clustering process involved feeding different subsets of features into the machine learning algorithm to determine the optimal combination for accurate vocal fold edge representation. This step was computationally intensive and required a significant amount of time, especially when dealing with large datasets.\n\nThe active contour modeling (ACM) method, which was used in conjunction with the clustering technique, also contributed to the overall execution time. ACM is an iterative energy minimization technique that requires initializing a deformable contour near the edges of interest in an image. The iterative nature of ACM means that it can take a relatively long time to converge, particularly when the analysis is done at all cross-sections of the vocal folds for each vocalization.\n\nDespite these computational demands, the hybrid method was designed to be more efficient than the ACM method alone. By using unsupervised machine learning techniques to initialize the ACM, the hybrid method was able to reduce the overall execution time while improving the accuracy of vocal fold edge detection.\n\nIn summary, while the execution time of the hybrid method varied depending on the specific characteristics of the HSV data, it was generally more efficient than the ACM method alone. The method's ability to accurately detect vocal fold edges during connected speech makes it a valuable tool for the objective analysis of vocal function.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the hybrid method involved several steps to ensure its accuracy and robustness. The method was tested on high-speed videoendoscopy (HSV) data recorded from a vocally normal adult during the reading of the \"Rainbow Passage.\" The performance of the hybrid method was assessed through visual inspection of the detected vocal fold edges in the HSV kymograms of different vocalization segments.\n\nOut of 76 vocalizations, the visual inspection demonstrated that the developed hybrid technique successfully captured the glottal edges for 74 vocalizations with an error of less than \u00b11 pixel. This resulted in a high accuracy of 97.4% in vocal fold edge representation using the hybrid method for HSV data during connected speech.\n\nThe hybrid method's performance was compared with that of the active contour model (ACM) alone. The comparison was conducted on both decent quality kymograms and those with dim lighting and degraded qualities. The results revealed a significant improvement in edge detection by the hybrid method over using the ACM alone, particularly in lower quality kymograms. This indicated that the proposed hybrid method was less vulnerable to image noise compared to the ACM, which struggled to detect edges in the presence of significant noise.\n\nAdditionally, the computational cost of the hybrid method was found to be half that of the ACM technique, making it more efficient. The segmented edges in the kymograms, extracted at different vocal fold cross-sections, were registered back to the HSV spatial frames to detect the vocal fold edges in each individual HSV frame. This process further validated the method's ability to accurately represent the vocal fold edges during connected speech.",
  "evaluation/measure": "The performance of the proposed hybrid method was primarily evaluated through visual inspection of the detected vocal fold edges in high-speed video (HSV) kymograms. This method successfully captured the glottal edges for 74 out of 76 vocalizations, yielding a high accuracy of 97.4% in vocal fold edge representation during connected speech. This accuracy was determined by assessing the error in edge detection, which was found to be less than \u00b11 pixel for the successful vocalizations.\n\nThe performance of the hybrid method was also compared with the active contour model (ACM) alone. The comparison was conducted on both decent quality kymograms and those with dim lighting and degraded qualities. The hybrid method demonstrated a significant improvement in edge detection, particularly in lower quality kymograms, where the ACM struggled. This indicates that the hybrid method is less vulnerable to image noise and provides more robust edge detection.\n\nAdditionally, the computational cost of the hybrid method was noted to be half of that of the ACM technique, highlighting its efficiency. The hybrid method's ability to handle challenging HSV data obtained using a color camera suggests its potential for even higher accuracy and performance when applied to less challenging monochromatic images.\n\nThe visual inspection method served as a reference for validating the performance of the developed technique, as there is no known gold-standard accurate method to fully capture the vocal fold edges from HSV data during connected speech. The hybrid method's performance was also compared with a previously developed ACM method, which detected the glottal edges accurately in 88% of the vocalizations in the same HSV sample. This comparison further underscores the superiority of the hybrid method.\n\nIn summary, the performance metrics reported include accuracy in edge detection, robustness to image noise, and computational efficiency. These metrics are representative of the method's effectiveness in accurately segmenting vocal fold edges in HSV data during connected speech, addressing limitations of previous methods and paving the way for future applications in vocal fold dynamics analysis.",
  "evaluation/comparison": "The evaluation of the proposed hybrid method involved a comprehensive comparison with existing techniques to assess its performance and robustness. Specifically, the hybrid method was compared against the active contour model (ACM) approach, which was previously developed for vocal fold segmentation during connected speech. This comparison was conducted using high-speed video (HSV) data, focusing on both high-quality and challenging kymograms with dim lighting and degraded quality.\n\nThe hybrid method demonstrated significant improvements in edge detection, particularly in lower-quality kymograms. This enhancement was attributed to the hybrid method's reduced vulnerability to image noise compared to the ACM alone, which struggled to detect edges in the presence of significant noise. Additionally, the computational cost of the hybrid method was found to be half that of the ACM technique, indicating a more efficient approach.\n\nThe performance of the hybrid method was further validated through visual inspection of detected vocal fold edges in HSV kymograms of different vocalization segments of the \"Rainbow Passage.\" Out of 76 vocalizations, the hybrid method successfully captured the glottal edges for 74 vocalizations with an error of less than \u00b11 pixel, achieving a high accuracy of 97.4%. This performance was superior to the previously developed ACM method, which accurately detected glottal edges in 88% of the vocalizations.\n\nThe comparison highlighted the hybrid method's advantages in terms of accuracy, robustness, and computational efficiency, making it a promising tool for automated analysis and measurement of vocal fold dynamics during connected speech. The method's ability to handle challenging HSV data obtained using a color camera also paves the way for its implementation on less challenging monochromatic images, potentially leading to even higher accuracy and performance.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files from this study are not publicly available. However, data from this study can be made available upon request to the corresponding author. This process involves executing an appropriate data sharing agreement. The data sharing agreement ensures that the data is used responsibly and in accordance with ethical guidelines. This approach allows for controlled access to the data, ensuring that it is used for legitimate research purposes while protecting the privacy and rights of the participants involved."
}