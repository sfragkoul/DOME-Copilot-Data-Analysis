{
  "publication/title": "Application of deep convolutional neural networks in classification of protein subcellular localization with microscopy images.",
  "publication/authors": "Xiao M, Shen X, Pan W",
  "publication/journal": "Genetic epidemiology",
  "publication/year": "2019",
  "publication/pmid": "30614068",
  "publication/pmcid": "PMC6416075",
  "publication/doi": "10.1002/gepi.22182",
  "publication/tags": "- Convolutional Neural Networks\n- Deep Learning\n- Image Classification\n- Transfer Learning\n- Microscopy Images\n- Protein Localization\n- Machine Learning\n- Computer Vision\n- Data Imbalance\n- Feature Extraction\n- Residual Networks\n- Ensemble Methods\n- High-Throughput Screening\n- Subcellular Localization\n- Image Segmentation\n- Predictive Accuracy\n- Model Robustness\n- Hyperparameter Tuning\n- Gradient Boosting\n- Random Forests",
  "dataset/provenance": "The dataset used in our study consists of segmented high-throughput microscopy image data. The original data, before segmentation or cropping, was obtained from Chong et al. (2015) and is stored in the CYCLoPs database. This dataset was then processed by P\u00e4rnamaa and Parts (2017) to create 64x64 pixel patches centered on a single cell from the microscopy pictures. As a result, some images may contain surrounding cells with similar fluorescent patterns.\n\nThe dataset includes specific fluorescent patterns of cells that inform the subcellular locations. The red and green channels in an image mark the cell body as background and track the protein location, respectively. The dataset is divided into 65,000 training, 12,500 validation, and 12,500 test single-cell microscopy images. These images are categorized into 12 localization classes, which include cell periphery, cytoplasm, endosome, endoplasmic reticulum, Golgi, mitochondrion, nuclear periphery, nucleolus, nucleus, peroxisome, spindle pole, and vacuole. The distribution of these classes is imbalanced.\n\nAdditionally, we reproduced the CNN on a similar dataset where each microscopy image was not segmented into one cell. This dataset, processed by Kraus et al. (2017) from the same source as ours, includes two more quality control classes, divides the \"spindle pole\" class into two, and separates the \"vacuole membrane\" class into \"vacuole\" and \"vacuole membrane.\" This results in 19 localization classes. The training, validation, and test datasets for this similar dataset include 21,882, 4,916, and 4,224 cell images, respectively.\n\nFurthermore, we demonstrated the transfer learning efficiency of the CNN on a substantially different cell image dataset. This dataset, provided by Kraus et al. (2017) and taken from Yofe et al. (2016), features proteins of interest dyed in red fluorescence instead of green. This dataset is less suitable for automated analysis due to some clustered and overlapping cells in many images. It includes only 11 localization classes: ER, Bud, Bud Neck, Cell Periphery, Cytosol, Mitochondria, Nuclear Periphery, Nucleus, Punctate, Vacuole, and Vacuole Membrane.",
  "dataset/splits": "The dataset used in this study consists of three main splits: training, validation, and test sets. Each split contains a specific number of single-cell microscopy images, which are categorized into 12 subcellular localization classes.\n\nThe training set comprises 65,000 images. This is the largest split and is used to train the models, allowing them to learn the patterns and features necessary for accurate classification.\n\nThe validation set includes 12,500 images. This split is used during the training process to tune hyperparameters and select the best-performing model iterations. It helps in preventing overfitting by providing an unbiased evaluation of the model's performance on unseen data.\n\nThe test set also contains 12,500 images. This split is used to evaluate the final performance of the trained models. It provides an unbiased assessment of how well the models generalize to new, unseen data.\n\nThe distribution of data points across these splits is imbalanced, with varying numbers of images for each of the 12 localization classes. For instance, classes like \"Cytoplasm\" and \"Nucleolus\" have a higher number of images, while classes like \"Peroxisome\" and \"Golgi\" have fewer images. This imbalance is reflected across all three splits, requiring careful consideration in the model training and evaluation processes.",
  "dataset/redundancy": "The datasets used in this study were split into three distinct sets: training, validation, and test sets. The training set consisted of 65,000 images, the validation set had 12,500 images, and the test set also contained 12,500 images. These splits were designed to ensure that the training and test sets were independent, which is crucial for evaluating the performance of the models.\n\nTo enforce the independence of the datasets, the images were carefully selected and segmented from high-throughput microscopy data. The data before segmentation or cropping came from a previously published study, and the images were cropped into 64x64 pixel patches centered on a single cell. This process helped to minimize overlap and ensure that the images in the different sets were distinct.\n\nThe distribution of the datasets across different subcellular localization classes was imbalanced, which is a common challenge in biological image analysis. The classes included categories such as cell periphery, cytoplasm, endosome, endoplasmic reticulum, Golgi, mitochondrion, nuclear periphery, nucleolus, nucleus, peroxisome, spindle pole, and vacuole. This imbalance was addressed by evaluating the models with an increasing equal number of samples per class during the transfer learning experiments.\n\nCompared to previously published machine learning datasets, the datasets used in this study are notable for their size and the complexity of the images. The use of high-throughput microscopy data and the focus on subcellular localization make these datasets particularly challenging and relevant for advancing the field of computational biology. The careful splitting and independent nature of the datasets ensure that the results are robust and generalizable.",
  "dataset/availability": "The data used in this study is not publicly available in a forum. However, the code and fitted models used for the analysis are available for public download on GitHub. This allows other researchers to replicate the results and apply the methods to their own datasets. The specific link for the code and models is https://github.com/menglix/CNNsCelImages. The data itself consists of segmented high-throughput microscopy image data, which was obtained from previous studies and processed accordingly. The datasets include images with specific fluorescent patterns that inform subcellular locations, with different numbers of localization classes depending on the dataset. The training, validation, and test datasets are described in detail in the study, ensuring transparency in the data splits used.",
  "optimization/algorithm": "The optimization algorithm used in our work is not a new machine-learning algorithm. Instead, we employed well-established convolutional neural networks (CNNs), specifically VGG-type CNNs and variations of ResNet, which are widely recognized in the field of deep learning. These CNNs are part of the broader class of neural networks that have been extensively studied and applied in various domains, including computer vision and image classification.\n\nThe choice to use these established CNNs was driven by their proven effectiveness in handling complex image data, which is crucial for our genetic and computational biology applications. The VGG-type CNNs, with their typical structure, demonstrated high classification accuracy on raw image data, outperforming traditional machine learning methods such as random forests and XGBoost. Similarly, ResNets showed improvements in both training speed and accuracy, making them suitable for our needs.\n\nGiven that these CNNs are not novel algorithms, publishing them in a machine-learning journal would not have been appropriate. Instead, our focus was on applying these established methods to a specific domain\u2014genetics and computational biology\u2014and demonstrating their superior performance in this context. This approach aligns with the goals of our study, which is to facilitate the application of CNNs in these fields by providing our computer code and fitted models for public download.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were designed to facilitate the application of convolutional neural networks (CNNs) and other machine learning methods to cell image data. The dataset consisted of segmented high-throughput microscopy images, which were cropped into 64x64 pixel patches centered on a single cell. These images were obtained from the CYCLoPs database and were preprocessed to highlight specific fluorescent patterns that inform subcellular locations. The red and green channels in the images marked the cell body as background and tracked the protein location, respectively.\n\nFor the CNN models, minimal preprocessing was applied to the raw image data. The pixel values of the training images were subtracted by their sample mean, which helped in standardizing the input data. This preprocessing step was crucial for enhancing the performance of CNNs, as it allowed the models to focus on the relevant features in the images without being distracted by variations in brightness or contrast.\n\nIn addition to CNNs, we also implemented traditional machine learning methods such as random forests, XGBoost, linear discriminant analysis, K-nearest neighbor, linear support vector machine, and lasso logistic regression. For these methods, the image data needed to be vectorized before use. This involved converting the 2D image data into a 1D array of pixel values, which could then be fed into the machine learning algorithms. The vectorization process ensured that the traditional classifiers could handle the image data in a format suitable for their respective algorithms.\n\nTo further enhance the performance of the traditional machine learning methods, we leveraged the feature extraction capabilities of CNNs. By replacing the last fully-connected layer of the DeepYeast CNN with random forests or XGBoost, we were able to obtain better performance efficiently. This approach allowed us to combine the strengths of CNNs in feature representation with the robustness of traditional machine learning methods.\n\nOverall, the data encoding and preprocessing steps were designed to maximize the performance of both CNNs and traditional machine learning methods on the cell image dataset. The minimal preprocessing applied to the raw image data for CNNs, along with the vectorization of image data for traditional classifiers, ensured that the models could effectively learn from the data and achieve high predictive accuracy.",
  "optimization/parameters": "In our study, we utilized two primary convolutional neural network architectures: VGG-19 and an 11-layered baseline CNN. The VGG-19 model, known for its depth and complexity, comprises 19 weight layers and utilizes an input size of 224\u00d7224\u00d73. This architecture is characterized by a series of convolutional layers followed by max-pooling layers, culminating in fully connected layers. The total number of parameters in the VGG-19 model is approximately 144 million.\n\nIn contrast, the 11-layered baseline CNN, which we refer to as the VGG-type CNN or DeepYeast, has a more streamlined structure with 11 weight layers and an input size of 64\u00d764\u00d73. This model also features convolutional and max-pooling layers, but it includes fully connected layers with dropout regularization to prevent overfitting. The number of parameters in this model is significantly lower, totaling around 3.1 million.\n\nThe selection of the number of parameters in each model was guided by the architectural design principles of the respective networks. For VGG-19, the high number of parameters is a result of its deep structure, which includes multiple convolutional layers with increasing numbers of filters. This design allows the model to capture complex features from the input data. On the other hand, the 11-layered baseline CNN was designed to be more efficient, with a reduced number of parameters to balance computational cost and performance.\n\nAdditionally, we implemented Residual Neural Networks (ResNets) with varying depths and widths to explore their performance. The ResNet architectures, such as ResNet-18 and ResNet-50, incorporate shortcut connections that facilitate the training of very deep networks. These models were designed to have a comparable number of parameters to their VGG-type counterparts but with improved optimization properties.\n\nIn summary, the number of parameters in our models was determined by the architectural choices made during the design of the VGG-19, 11-layered baseline CNN, and ResNet models. These choices were influenced by the need to balance model complexity, computational efficiency, and performance on the specific tasks at hand.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "In our study, we employed various convolutional neural network (CNN) architectures, including VGG-type models, ResNets, and others like Inception-ResNet V2 and Capsule Networks. The number of parameters in these models can indeed be quite large, especially in deeper and wider networks. For instance, models like Inception-ResNet V2 and Capsule Networks have a substantial number of parameters, which can exceed the number of training points in our dataset.\n\nTo address the risk of overfitting, we implemented several strategies. Firstly, we used a validation dataset to monitor the model's performance during training and selected the model iteration with the best validation accuracy. This approach helps in ensuring that the model generalizes well to unseen data. Additionally, we employed data augmentation techniques, such as random cropping and normalizing image pixels, which significantly improved the model's performance and helped in preventing overfitting. For some models, we also used transfer learning, where we fine-tuned pre-trained models on our dataset, which further aided in mitigating overfitting.\n\nTo rule out underfitting, we experimented with different architectures and depths of networks. We observed that deeper ResNets generally performed better, indicating that the model complexity was sufficient to capture the underlying patterns in the data. Moreover, the use of advanced optimizers like Adam, along with techniques like identity shortcuts, contributed to better convergence and improved performance, ensuring that the models were not too simplistic to capture the necessary features.\n\nIn summary, while some of our models had a large number of parameters, we employed validation-based early stopping, data augmentation, transfer learning, and advanced optimization techniques to prevent overfitting. Conversely, by experimenting with different network architectures and depths, we ensured that our models were complex enough to avoid underfitting.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and ensure the robustness of our convolutional neural networks (CNNs). One of the key methods used was dropout, which randomly sets a fraction of input units to zero at each update during training time. This helps to prevent overfitting by ensuring that the network does not become too reliant on any single neuron.\n\nAdditionally, we utilized data augmentation techniques such as random rotation, shifting, and reflection of the images. This process artificially increases the size of the training dataset by creating modified versions of the existing images, which helps the model to generalize better to new, unseen data.\n\nWe also experimented with different optimizers, including the Adam optimizer, which adapts the learning rate for each parameter and has been shown to converge faster and perform better in many scenarios compared to traditional stochastic gradient descent (SGD).\n\nFurthermore, we implemented early stopping, a technique where the training process is halted when the performance on a validation set stops improving. This prevents the model from continuing to train and potentially overfitting to the training data.\n\nLastly, we ensured that our models were not excessively deep or wide, which can also help in preventing overfitting. We compared various architectures, including VGG-type CNNs and ResNets, and found that simpler models often performed just as well or better than more complex ones for our specific problem.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are indeed available. To facilitate the applications of CNNs in genetics and computational biology, we have made our computer code and fitted models publicly accessible. These resources can be downloaded from our GitHub repository. The repository is open-source, allowing researchers and practitioners to explore, use, and build upon our work. This transparency ensures that others can replicate our results, further validate our findings, and apply our methods to their own research.",
  "model/interpretability": "The models we employed, particularly the convolutional neural networks (CNNs) like VGG-type and ResNets, are generally considered black-box models. This means that while they can achieve high predictive accuracy, the internal workings and the specific features they use for decision-making are not easily interpretable. The complexity of these deep learning models makes it challenging to trace back how a particular output was derived from the input data.\n\nHowever, efforts have been made to visualize and interpret the features extracted by these CNNs. For instance, we visualized the activation in the first and last convolutional layers of a CNN. This approach, while not providing a full interpretation of the model's decisions, offers some insight into which parts of the input images the model focuses on. By applying techniques such as gradient ascent and generating activation heatmaps, we can see which regions of the images are most influential in the model's predictions. These visualizations help in understanding the model's behavior to some extent, even if they do not fully explain the underlying decision-making process.\n\nAdditionally, we explored the use of CNNs as feature extractors for other classifiers like random forests and XGBoost. By replacing the last fully-connected layer of the CNN with these classifiers, we aimed to leverage the feature representation learned by the CNN to improve the performance of traditional machine learning methods. This approach indirectly provides some interpretability, as the features extracted by the CNN can be analyzed and understood in the context of the subsequent classifiers.\n\nIn summary, while our models are primarily black-box, we have taken steps to enhance their interpretability through visualization techniques and by integrating them with more interpretable machine learning methods. These efforts help in gaining some insights into the model's decision-making process, even if a complete transparency is not achieved.",
  "model/output": "The model is designed for classification tasks. It was applied to a dataset of cell images, aiming to predict the correct class labels for the images. The performance of the model was evaluated using test accuracy, which measures the proportion of correctly predicted class labels out of the total number of test samples. Various models were compared based on their test accuracy on a dataset consisting of 12,500 images. The models included different types of convolutional neural networks (CNNs) such as VGG-type models, ResNets, and other state-of-the-art architectures like Inception-ResNet V2 and Capsule Networks. Additionally, traditional machine learning methods like random forests, XGBoost, linear discriminant analysis, K-nearest neighbor, support vector machines, and lasso logistic regression were also evaluated for comparison. The results showed that CNNs consistently outperformed other classifiers on raw image data, achieving higher predictive accuracy. The classification accuracy was measured on the test data, and the models were assessed based on their ability to correctly predict the class labels of the images.",
  "model/duration": "In our study, we observed varying execution times across different models and methods. For instance, the 11-layer VGG-type model, which we refer to as the DeepYeast model, required approximately 6 hours of training time. This timeframe was consistent whether data augmentation was applied or not. ResNet models, such as Res18 and Res50, also exhibited different training durations. Res18 took about 2.45 hours, while Res50 required a more substantial 12.75 hours. Ensemble methods like Random Forest and XGBoost were generally faster, with Random Forest completing in around 2 hours and XGBoost in 10 hours. Traditional machine learning algorithms, such as Linear Discriminant Analysis, were notably quicker, finishing in just 16 minutes. However, other methods like K-Nearest Neighbor and Support Vector Machine took considerably longer, around 18 hours. It is important to note that the training times for convolutional neural networks (CNNs) were measured on a server equipped with a GPU, which significantly accelerated the process compared to CPU-based training. Additionally, we experimented with different optimizers and data augmentation techniques, which slightly affected the training times but generally maintained the models' high performance. Overall, the execution times varied widely depending on the model's complexity and the computational resources available.",
  "model/availability": "To facilitate the application of Convolutional Neural Networks (CNNs) in genetics and computational biology, the computer code and fitted models used in our study are available for public download. This release aims to support further research and practical implementations in these fields. The code can be accessed via a public repository, ensuring that other researchers and practitioners can reproduce our results and build upon our work. The specific details for accessing the code and models are provided in the repository, which includes instructions on how to use the software and the licensing terms that govern its use. This open access to our computational tools is intended to promote transparency, reproducibility, and collaboration within the scientific community.",
  "evaluation/method": "The evaluation of our methods involved several approaches to ensure robustness and generalizability. For the convolutional neural networks (CNNs), we trained models for 65,000 iterations and used a validation dataset to select the model iteration with the best validation accuracy. This helped in tuning the models effectively.\n\nTo assess the performance of transfer learning, we applied a pre-trained 11-layered CNN to a different dataset. This dataset had proteins dyed in red fluorescence and included 11 localization classes. To mitigate the impact of imbalanced classes, we evaluated the network with an increasing equal number of samples per class, ranging from 1 to 500. For each sample size, we bootstrapped 15 times from the original dataset and averaged the performance to ensure reliability.\n\nWe also compared the performance of CNNs with traditional machine learning methods, including random forests, extreme gradient boosting (XGBoost), linear discriminant analysis, K-nearest neighbor, linear support vector machine, and lasso logistic regression. These classifiers were implemented using Python's Scikit-learn, except for lasso logistic regression, which was done using the R glmnet package. Tuning parameters for these methods were chosen based on the validation dataset.\n\nAdditionally, we evaluated the robustness of CNNs by perturbing the training data. We randomly assigned incorrect class labels to 5% or 10% of the training images in each class category and observed that the CNNs maintained high test accuracy, demonstrating their resilience.\n\nThe evaluation also included assessing the training speed and accuracy of different CNN architectures, such as VGG-type CNNs and various ResNet structures. We found that ResNets generally improved both training speed and accuracy compared to other methods.\n\nOverall, the evaluation methods included a combination of validation dataset selection, transfer learning experiments, comparisons with traditional classifiers, and robustness tests to ensure comprehensive assessment of the models' performance.",
  "evaluation/measure": "In our evaluation, we primarily focused on predictive accuracy as our key performance metric. This metric is reported as the correct prediction rate, which provides a clear indication of how well our models are performing in classifying the raw image data.\n\nWe assessed the accuracy of our models on a test dataset consisting of 12,500 images. This dataset was derived from segmented high-throughput microscopy images, which were cropped to 64x64 pixel patches centered on a single cell. The images were labeled with one of 12 localization classes, such as cell periphery, cytoplasm, and nucleus, among others.\n\nThe accuracy rates for our convolutional neural networks (CNNs) ranged from 0.819 to 0.891, demonstrating a consistently higher performance compared to other machine learning methods. For instance, random forests and XGBoost, which are known for their strong classification performance, achieved accuracies of 0.596 and 0.679, respectively. This comparison highlights the superior capability of CNNs in handling raw image data, even when the class sizes are imbalanced.\n\nAdditionally, we evaluated the robustness of our CNNs by perturbing the training data. We randomly assigned incorrect class labels to 5% or 10% of the training images and observed that the VGG-type CNNs still maintained high test accuracies of 0.828 and 0.801, respectively. This robustness is crucial for real-world applications where data may not always be perfectly labeled.\n\nWe also considered the training time as an important performance measure. Training CNNs, especially those with deeper architectures like the 11-layered VGG-type CNNs, can be time-consuming. For example, it took approximately 6 hours to train on a GPU server, compared to 2 or 10 hours for ensemble methods on a CPU server. However, the improved accuracy and robustness of CNNs justify the additional training time.\n\nIn summary, our performance measures are representative of the state-of-the-art in the field. We focused on predictive accuracy as the primary metric, supported by evaluations of robustness and training time. These metrics provide a comprehensive view of the effectiveness and efficiency of our CNNs in genetic and computational biology applications.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of various methods to assess their performance on our cell image dataset. We implemented and evaluated several state-of-the-art convolutional neural networks (CNNs), including Inception-ResNet V2 and CapsNet, alongside traditional machine learning methods. For the traditional methods, we included random forests and extreme gradient boosting (XGBoost), which are known for their strong classification performance. Additionally, we implemented standard classifiers such as linear discriminant analysis, K-nearest neighbor, linear support vector machine, and lasso logistic regression.\n\nTo ensure a fair comparison, we followed established implementation details for the CNNs and adapted architecture parameters to suit our image dimensions and dataset size. For the traditional machine learning methods, we tuned parameters based on a validation dataset, which was sufficiently large to avoid the need for cross-validation.\n\nWe also explored feature extraction using CNNs, leveraging their ability to represent features effectively. By replacing the last fully-connected layer of our DeepYeast model with random forests or XGBoost, we compared the performance of these classifiers when initialized with CNN-extracted features against their performance when applied from scratch.\n\nFurthermore, we utilized transfer learning with a pre-trained VGG-19 model on ImageNet to extract features from our DeepYeast training data. This approach allowed us to benefit from the features learned by VGG-19 on a large and diverse dataset, potentially improving the performance of our classifiers.\n\nIn summary, our evaluation included a thorough comparison with publicly available methods and simpler baselines, providing a comprehensive assessment of the performance of different approaches on our cell image dataset.",
  "evaluation/confidence": "The performance metrics presented in this study include confidence intervals, specifically the standard error of the mean accuracy. This is illustrated in the transfer learning accuracy with increasing sample sizes, where bars show \u00b1 1 standard error of the mean accuracy. This provides a measure of the variability and reliability of the accuracy estimates.\n\nStatistical significance is addressed through bootstrapping experiments. For instance, in the transfer learning evaluation, the network was assessed with an increasing equal number of samples per class, and for each sample size, the performance was bootstrapped 15 times from the original dataset. This approach helps to ensure that the results are robust and not due to random chance.\n\nAdditionally, the comparison of different methods includes a range of predictive accuracies, which allows for a clear distinction between the performance of various classifiers. The consistent higher predictive accuracy of CNNs, particularly VGG-type CNNs and ResNets, compared to traditional machine learning methods, suggests that these models are statistically superior. The use of validation datasets and the selection of model iterations with the best validation accuracy further support the reliability of the results.\n\nOverall, the methods used to evaluate the performance of the CNNs provide a high level of confidence in the superiority of these models over traditional classifiers.",
  "evaluation/availability": "Not applicable"
}