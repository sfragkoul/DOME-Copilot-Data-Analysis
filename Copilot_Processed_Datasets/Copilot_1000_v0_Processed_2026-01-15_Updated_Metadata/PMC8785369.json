{
  "publication/title": "COVID-19 CT image recognition algorithm based on transformer and CNN.",
  "publication/authors": "Fan X, Feng X, Dong Y, Hou H",
  "publication/journal": "Displays",
  "publication/year": "2022",
  "publication/pmid": "35095128",
  "publication/pmcid": "PMC8785369",
  "publication/doi": "10.1016/j.displa.2022.102150",
  "publication/tags": "- COVID-19\n- CT images\n- Feature extraction\n- Transformer\n- Convolutional Neural Networks\n- Bi-branch network\n- Global features\n- Local features\n- Image classification\n- Medical imaging\n- Deep learning\n- Feature fusion\n- Model evaluation\n- Data augmentation\n- Performance metrics",
  "dataset/provenance": "The dataset used in our study is the COVIDx-CT dataset, which is a benchmark CT image dataset. This dataset was derived from CT imaging data collected by the China National Bioinformatics Center. It includes a total of 194,922 images from 3,745 patients, with ages ranging from 0 to 93 years, and a median age of 51 years. The dataset has been strongly clinically verified.\n\nThe COVIDx-CT dataset categorizes chest CT volumes into three different infection types: COVID-19, common pneumonia, and normal control. This dataset has been utilized in various studies and by the community to develop and evaluate models for detecting and classifying COVID-19 from CT images. The dataset's extensive size and clinical verification make it a valuable resource for research in this area.",
  "dataset/splits": "The dataset used in this study is the COVIDx-CT, a benchmark CT image dataset. This dataset is divided into three main splits: training, validation, and testing.\n\nThe training set consists of 143,778 images, which includes 35,996 normal images, 25,496 images of common pneumonia, and 82,286 images of COVID-19.\n\nThe validation set comprises 25,486 images, with 11,842 normal images, 7,400 images of common pneumonia, and 6,244 images of COVID-19.\n\nThe testing set contains 25,658 images, distributed as 12,245 normal images, 7,395 images of common pneumonia, and 6,018 images of COVID-19.\n\nThe dataset was enhanced through data augmentation techniques, such as rotating images at different angles (15\u00b0, 45\u00b0, 90\u00b0, and 180\u00b0), to expand the sample size and improve the model's performance. This augmentation helps in training the Transformer-based model, which requires a large amount of data to achieve better results.",
  "dataset/redundancy": "The dataset used in this study is the COVIDx-CT, a benchmark CT image dataset. This dataset includes 194,922 images from 3,745 patients, with ages ranging from 0 to 93 years, and a median age of 51 years. The dataset has been clinically verified and includes three types of infections: COVID-19, common pneumonia, and normal control.\n\nThe dataset was split into three parts: training, validation, and testing. The training set consists of 143,778 images, the validation set has 25,486 images, and the test set contains 25,658 images. The distribution of the dataset across these splits is as follows:\n\n* Training set: 35,996 normal, 25,496 pneumonia, 82,286 COVID-19\n* Validation set: 11,842 normal, 7,400 pneumonia, 6,244 COVID-19\n* Test set: 12,245 normal, 7,395 pneumonia, 6,018 COVID-19\n\nThe training and test sets are independent. This independence was enforced by ensuring that the images from the same patient do not appear in both the training and test sets. This approach helps to prevent data leakage and ensures that the model's performance is evaluated on truly unseen data.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets for COVID-19 diagnosis. The dataset is large and diverse, covering a wide range of ages and infection types. This diversity helps to improve the model's generalization ability and robustness. Additionally, the dataset has been clinically verified, ensuring the quality and reliability of the data.",
  "dataset/availability": "The data used in this study is publicly available through the COVIDx-CT dataset, which is a benchmark CT image dataset. This dataset was derived from CT imaging data collected by the China National Bioinformatics Center. It includes a substantial number of images from a diverse patient population, with strong clinical verification.\n\nThe dataset is segmented into three types: normal, common pneumonia, and COVID-19. The distribution of these types is clearly defined, with specific counts for training, validation, and testing sets. This segmentation ensures that the data is systematically organized and readily accessible for research purposes.\n\nThe dataset is enhanced through data augmentation techniques, such as rotating images at different angles (15\u00b0, 45\u00b0, 90\u00b0, and 180\u00b0), to expand the sample size and improve the model's training effectiveness. This enhancement is crucial for achieving better results, especially when training models that require large amounts of data.\n\nRegarding the availability and licensing, the data is part of a broader initiative to make COVID-19-related research freely accessible. Specifically, permissions have been granted for unrestricted research re-use and analyses in any form or by any means, with acknowledgment of the original source. This ensures that the data can be used widely in the scientific community without legal barriers.\n\nThe enforcement of these permissions is tied to the ongoing activity of the COVID-19 resource center. As long as this resource center remains active, the data will continue to be available under these terms. This approach facilitates open access and collaboration, which are essential for advancing research during the pandemic.",
  "optimization/algorithm": "The optimization algorithm employed in our study is the stochastic gradient descent algorithm with momentum. This is a well-established optimization technique commonly used in training deep learning models. It is not a new algorithm, having been extensively used and validated in numerous machine learning and deep learning applications.\n\nThe choice of this algorithm is driven by its effectiveness in handling large-scale datasets and its ability to converge efficiently, which is crucial for training complex models like the bi-branch parallel network structure we proposed. The momentum term helps accelerate convergence by adding a fraction of the previous update to the current update, which can smooth out updates and help navigate through local minima more effectively.\n\nGiven that our focus is on the application of this algorithm within the context of medical image processing, particularly for COVID-19 CT image recognition, it is appropriate to publish this work in a journal that emphasizes the intersection of deep learning and medical imaging rather than a purely machine-learning focused journal. This allows us to highlight the practical implications and improvements in diagnostic accuracy that our model provides, which is of significant interest to the medical and healthcare communities.",
  "optimization/meta": "The model described in this publication is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it is a parallel bi-branch model called Trans-CNN Net, which combines a Transformer module and a Convolutional Neural Network (CNN) module. The Transformer branch extracts global features from the input CT images, while the CNN branch extracts local features. These features are then fused bi-directionally to improve classification accuracy. The model is trained on the COVIDx-CT dataset, which includes CT images corresponding to three different infection types: COVID-19, common pneumonia, and normal control. The training data is independent and consists of CT images that have been preprocessed through data augmentation techniques such as rotation. The model's performance is evaluated using various metrics, including specificity, sensitivity, accuracy, precision, and F1 score.",
  "optimization/encoding": "In our study, the data encoding and preprocessing involved several key steps to ensure the model could effectively learn from the CT images. Initially, the dataset was expanded through data enhancement techniques. This involved rotating the images at various angles, specifically 15\u00b0, 45\u00b0, 90\u00b0, and 180\u00b0, to increase the sample size and diversity of the training data. This augmentation helped in making the model more robust and generalizable.\n\nThe enhanced data was then normalized to ensure that the pixel values were within a consistent range, which is crucial for the training process. Normalization helps in stabilizing and speeding up the training of neural networks.\n\nThe dataset used in this study was the COVIDx-CT, a benchmark CT image dataset derived from CT imaging data collected by the China National Bioinformatics Center. This dataset includes a large number of images categorized into three types: COVID-19, common pneumonia, and normal control. The dataset was segmented into training, validation, and testing sets to evaluate the model's performance accurately.\n\nFor the model input, the images were converted into vectors representing the entire picture. This allowed the Transformer module to leverage its global receptive field, enabling it to extract global features effectively. Concurrently, the Convolutional Neural Network (CNN) module was used to extract local features from the images. The combination of these two modules in a bi-branch parallel network structure allowed for a comprehensive feature extraction process.\n\nThe bi-directional feature fusion structure was designed to integrate the features extracted from both branches. This cross-fusion approach ensured that the model could utilize both local and global features, enhancing its classification accuracy. The fusion of features from different scales provided a richer and more comprehensive representation of the input data, which is essential for accurate diagnosis.\n\nIn summary, the data encoding and preprocessing involved data augmentation through rotation, normalization, and the use of a bi-branch parallel network structure combining Transformer and CNN modules. This approach ensured that the model could effectively learn from the CT images and achieve high classification accuracy.",
  "optimization/parameters": "In our study, the model we proposed, Trans-CNN Net, utilizes approximately 92.6 million parameters. The selection of these parameters was carefully considered to balance the model's capacity to learn complex features from the data while maintaining computational efficiency.\n\nThe parameter count was influenced by the architecture of the model, which combines the strengths of both Convolutional Neural Networks (CNNs) and Transformers. The CNN component, based on an improved VGG19 network, contributes significantly to the parameter count due to its deep structure and multiple convolutional layers. The Transformer component, while adding to the parameter count, provides the model with the ability to capture global features effectively.\n\nThe choice of parameters was also guided by the need to pre-train the model on a large dataset like ImageNet, which requires a substantial number of parameters to achieve good feature extraction and generalization. Additionally, the bi-branch parallel structure and the bi-directional feature fusion mechanism further contribute to the parameter count but are essential for enhancing the model's performance by integrating local and global features.\n\nDuring the training process, we used a batch size of 128 and an initial learning rate of 0.0005, which were selected through experimentation to ensure stable and efficient convergence. The model converged at the 48th epoch, indicating that the chosen parameters and training settings were effective in optimizing the model's performance.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "Not applicable.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One of the primary methods used was data augmentation. We expanded the dataset by rotating the images at different angles, specifically 15\u00b0, 45\u00b0, 90\u00b0, and 180\u00b0. This augmentation helped in increasing the diversity of the training data, making the model more generalizable and less likely to overfit to the specific patterns in the original dataset.\n\nAdditionally, we utilized a pre-trained model on the ImageNet dataset. This pre-training provided the model with a strong feature extraction capability, which not only accelerated the convergence speed during training but also helped in capturing more generalized features. The pre-trained model served as a robust starting point, reducing the risk of overfitting to the specific nuances of our dataset.\n\nFurthermore, we implemented a bi-branch parallel network structure that combined the strengths of both Convolutional Neural Networks (CNNs) and Transformers. This architecture allowed for the extraction of both local and global features, enhancing the model's ability to recognize patterns at different scales. The bi-directional feature fusion structure within this network further ensured that useful feature information was not lost as the network layers increased, thereby maintaining the model's performance and generalization capabilities.\n\nThe use of these techniques collectively contributed to the model's ability to achieve high accuracy and robustness, as evidenced by the experimental results. The model demonstrated strong performance in classifying COVID-19, common pneumonia, and normal images, with an overall accuracy of approximately 97%.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, the initial learning rate was set to 0.0005, and the optimization algorithm employed was the stochastic gradient descent with momentum. The batch size was set to 128, and the model converged at the 48th epoch. These details are provided to ensure reproducibility of our experiments.\n\nRegarding model files and optimization parameters, they are not explicitly made available in the publication. However, the implementation was conducted using PyTorch 1.7.0, CUDA 11.0, and cuDNN 7.6, which are standard libraries in the machine learning community. The pre-training was performed on the ImageNet dataset, a widely used benchmark in the field.\n\nFor access to the specific model files and optimization parameters, readers are encouraged to contact the authors directly. The publication is licensed under terms that allow for unrestricted research re-use and analysis, with proper acknowledgment of the original source. This ensures that the community can build upon our work while adhering to ethical and legal standards.",
  "model/interpretability": "In our study, we have taken steps to ensure that our model, Trans-CNN Net, is not a black box. To achieve this, we employed Grad-weighted Class Activation Mapping (Grad-CAM). This technique allows us to visualize the key feature areas that our model uses for classification. By doing so, we can gain insights into which parts of the CT images are most influential in the model's decision-making process. This visualization helps in understanding the model's focus and ensures that it is not merely relying on spurious correlations.\n\nFor instance, the Grad-CAM results, as shown in the relevant figures, highlight the specific regions in the CT images that the model considers important for distinguishing between different types of infections, such as COVID-19, common pneumonia, and normal cases. This transparency is crucial for building trust in the model, especially in medical applications where interpretability is paramount. By making the model's decision-making process more understandable, we can better validate its effectiveness and reliability in clinical settings.",
  "model/output": "The model is designed for classification tasks, specifically for identifying different types of pneumonia from CT images. It categorizes images into three classes: normal, common pneumonia, and COVID-19 pneumonia. The model's performance is evaluated using metrics such as specificity, sensitivity, accuracy, precision, and F1 score, which are typical for classification problems. The confusion matrix and other evaluation indices further confirm that the model's output is a classification result.\n\nThe model achieves high accuracy in classifying these categories, with approximately 95% accuracy for common pneumonia and 96% for COVID-19, resulting in an overall accuracy of about 97%. This indicates that the model is effective in distinguishing between the different types of pneumonia and normal images. The use of a bi-branch feature fusion framework, combining CNN and Transformer, enhances the model's ability to extract both local and global features, leading to improved classification performance. The experimental results demonstrate that this approach outperforms traditional methods like ResNet-152 and Deit-B, showcasing the model's robustness and accuracy in classification tasks.",
  "model/duration": "The model was trained using a 64-bit operating system Ubuntu-18.04.1, with a 3-card parallel training setup on Intel(R) Xeon(R) CPU E 5 \u2013 2695 and NVIDIA TITAN V high-performance GPUs. Each graphics card had a storage capacity of 12 GB and 16 GB memory. The model converged at the 48th epoch. The optimization algorithm used was the random momentum gradient descent algorithm, with a batch setting of 128. The initial learning rate was set to 0.0005. The training process was conducted under Pytorch 1.7.0, CUDA 11.0, and CUDNN 7.6. The model was pre-trained on the ImageNet dataset.\n\nThe specific execution time for the model to run is not provided, but the convergence at the 48th epoch gives an indication of the training duration. The use of high-performance GPUs and a parallel training setup suggests that the model was designed to handle large datasets efficiently. The convergence point at the 48th epoch indicates that the model reached its optimal performance within a reasonable number of epochs, leveraging the computational power of the GPUs and the parallel training setup.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our proposed model, Trans-CNN Net, involved several key steps and metrics to ensure its effectiveness and accuracy. We utilized a benchmark CT image dataset known as COVIDx-CT, which includes a substantial number of images from patients with varying infection types, including COVID-19, common pneumonia, and normal controls. This dataset was segmented into training, validation, and testing sets to facilitate robust evaluation.\n\nTo enhance the dataset, we employed data augmentation techniques such as rotating images at different angles (15\u00b0, 45\u00b0, 90\u00b0, and 180\u00b0). This augmentation helped in expanding the sample size and improving the model's generalization capabilities. The augmented data was then normalized to ensure consistency in training.\n\nThe model was trained using a 64-bit Ubuntu-18.04.1 operating system on a high-performance setup that included Intel(R) Xeon(R) CPU E5-2695 and NVIDIA TITAN V GPUs. The training process was conducted under PyTorch 1.7.0, CUDA 11.0, and cuDNN 7.6, with a pre-trained model on the ImageNet dataset. The initial learning rate was set to 0.0005, and the optimization algorithm used was stochastic gradient descent with momentum. The batch size was set to 128, and the model converged at the 48th epoch.\n\nDuring the evaluation, we measured the model's performance using five key indicators: specificity, sensitivity, accuracy, precision, and F1 score. These metrics were calculated based on true positive (TP), false positive (FP), true negative (TN), and false negative (FN) values. Specificity reflects the model's ability to identify normal images, while sensitivity measures the percentage of true positive COVID-19 cases correctly recognized. Accuracy indicates the overall correctness of the model's classifications, precision describes the percentage of predicted positive cases that are actual positives, and the F1 score provides a harmonic mean of precision and sensitivity, offering a balanced measure of the model's performance.\n\nThe confusion matrix obtained from the testing phase showed that the model achieved approximately 95% accuracy in classifying common pneumonia and about 96% accuracy in recognizing COVID-19 cases, resulting in an overall accuracy of around 97%. This performance was compared with other models, such as ResNet-152 and Deit-B, demonstrating that Trans-CNN Net outperformed them in terms of classification accuracy on the COVIDx-CT dataset.\n\nIn summary, the evaluation method involved a comprehensive approach that included data augmentation, robust training procedures, and the use of multiple performance metrics to ensure the reliability and effectiveness of the Trans-CNN Net model.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our proposed model. These metrics include specificity, sensitivity, accuracy, precision, and the F1 score. Each of these metrics provides a unique perspective on the model's performance, ensuring a thorough assessment.\n\nSpecificity measures the model's ability to correctly identify normal images, which is crucial for distinguishing between healthy and infected cases. Sensitivity, also known as recall, indicates the proportion of true positive COVID-19 cases that are correctly identified by the model. Accuracy reflects the overall correctness of the model's classifications, providing a general measure of performance. Precision assesses the accuracy of the positive predictions made by the model, indicating how many of the predicted COVID-19 cases are actually true positives. The F1 score is the harmonic mean of precision and sensitivity, offering a balanced view of the model's performance, especially when dealing with imbalanced datasets.\n\nThese metrics are widely used in the literature for evaluating medical image classification models, making our evaluation representative and comparable to other studies in the field. By reporting these metrics, we aim to provide a clear and comprehensive understanding of our model's capabilities in detecting COVID-19 from CT images.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of our proposed method with several existing methods to evaluate its performance. We used benchmark datasets, specifically the COVIDx-CT dataset, which is a well-established benchmark for CT image data related to COVID-19. This dataset includes a large number of images from patients with varying infection types, providing a robust basis for comparison.\n\nOur proposed model, Trans-CNN Net, was compared against various other models, including traditional CNN architectures like ResNet-152 and transformer-based models like Deit-B. The comparison was based on multiple performance metrics, such as specificity, recall, F1 score, precision, and accuracy. The results, as shown in the relevant tables, demonstrate that our model outperforms these existing methods in most of the evaluated metrics.\n\nAdditionally, we compared our model with simpler baselines to ensure that the improvements were not merely due to the complexity of the model but rather due to its architectural advantages. For instance, we compared against models like K-ELM, Wang, Li, and Mangal, which represent different levels of complexity and approaches to the problem. The comparison showed that our model consistently achieved higher accuracy and other performance metrics, indicating its superiority.\n\nFurthermore, we visualized the key feature areas of model classification using Grad-weighted Class Activation Mapping (Grad-CAM). This visualization helped us understand the model's focus areas and verify its validity. The results showed that our model effectively captures relevant features for accurate classification.\n\nIn summary, our comparison with publicly available methods and simpler baselines on benchmark datasets confirms the effectiveness and superiority of our proposed Trans-CNN Net model.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, the COVID-19-related research content, including the evaluation results, is freely accessible in publicly funded repositories such as PubMed Central and the WHO COVID database. These repositories allow for unrestricted research re-use and analyses with proper acknowledgment of the original source. The permissions for this access are granted for free as long as the COVID-19 resource center remains active."
}