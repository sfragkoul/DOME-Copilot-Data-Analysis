{
  "publication/title": "Development and implementation of an artificial intelligence-enhanced care model to improve patient safety in hospital wards in Spain.",
  "publication/authors": "Huete-Garcia A, Rodriguez-Lopez S",
  "publication/journal": "Acute and critical care",
  "publication/year": "2024",
  "publication/pmid": "39558593",
  "publication/pmcid": "PMC11617847",
  "publication/doi": "10.4266/acc.2024.00759",
  "publication/tags": "- AI-enhanced care model\n- Hospital wards\n- Critical patients\n- Early warning systems\n- mNEWS2-Lab\n- Machine learning in healthcare\n- Patient monitoring\n- Clinical data analysis\n- Predictive modeling\n- Healthcare improvement",
  "dataset/provenance": "The dataset utilized in this study was sourced from the electronic health records of adult patients admitted to hospital wards. The study included data collected both pre-implementation and post-implementation of the mNEWS2-Lab protocol. Specifically, the pre-implementation data was gathered from April 3 to April 26, 2018, while the post-implementation data spanned from December 1, 2018, to December 1, 2021. Additionally, the AI-enhanced system was implemented on December 15, 2023, with data on its effectiveness collected from December 15, 2023, to April 15, 2024.\n\nThe total number of patients analyzed in this study was 3,790, with an average age of 65 \u00b1 21 years. Among these patients, 56.3% were female, and 60.1% were surgical patients. During the study period, there were 94 critical events (CEs), accounting for 3.7% of the cohort.\n\nThe dataset included a variety of key variables such as vital signs (e.g., oxygen saturation, respiratory rate, blood pressure, heart rate, temperature), critical laboratory parameters (e.g., white blood cell count, C-reactive protein, procalcitonin, troponin T, pH, partial pressure of carbon dioxide, lactate), and other variables like level of consciousness and need for oxygen therapy. These variables were used to develop and validate the mNEWS2-Lab scale, which aims to enhance the detection and care of critical patients in hospital wards.\n\nThe dataset has not been used in previous papers by the community, as this study represents a novel implementation and enhancement of the mNEWS2-Lab protocol with AI integration. The focus was on retrospective analysis of anonymized clinical data, ensuring that the study adhered to ethical guidelines and did not require informed consent from patients. The research was conducted with an exemption from IRB/IACUC approval, given the nature of the data and the absence of direct patient interventions.",
  "dataset/splits": "The dataset was divided into two primary splits: a training set and a testing set. The training set comprised 70% of the data, while the testing set contained the remaining 30%. This split was designed to train and validate the models effectively.\n\nThe dataset was further analyzed in different phases: pre-implementation, post-implementation, and post-AI enhancement. During the pre-implementation phase, 358 patients were analyzed. Post-implementation, the dataset included 3,117 patients. After integrating the AI-enhanced system, data from 315 patients were collected and analyzed.\n\nThe distribution of data points across these phases reflects the varying durations of data collection. The pre-implementation phase had a smaller cohort due to a shorter monitoring period, while the post-implementation phase had a larger cohort due to an extended data collection period. The post-AI enhancement phase had a moderate number of patients, providing valuable initial insights into the system's performance.",
  "dataset/redundancy": "The dataset used in this study was split into training and testing sets to facilitate the development and validation of the AI models. Specifically, 70% of the data was allocated for training the models, while the remaining 30% was reserved for testing their performance. This split ensures that the training and test sets are independent, which is crucial for obtaining unbiased performance metrics.\n\nTo enforce the independence of the training and test sets, standard practices were followed. These include ensuring that no data from the test set was used during the training phase. This separation helps in evaluating the model's generalizability and its ability to perform well on unseen data.\n\nThe distribution of the dataset in this study compares favorably with previously published machine learning datasets in healthcare. The focus on vital signs, critical laboratory parameters, and other relevant variables ensures that the dataset is comprehensive and representative of the clinical scenarios encountered in hospital wards. This approach aligns with best practices in the field, where the goal is to create robust models that can be reliably applied in real-world settings.",
  "dataset/availability": "The data used in this study is not publicly available. The study focused on a retrospective analysis of anonymized clinical data previously collected in electronic records. This data was not released in a public forum due to privacy and ethical considerations. The research was conducted with an exemption from IRB/IACUC approval, as it did not involve direct interventions with patients or experimental procedures with human subjects. The data was collected from adult patients admitted to hospital wards, with specific inclusion and exclusion criteria applied to ensure the relevance and integrity of the dataset. The codebase, including preprocessing scripts, model training code, and deployment scripts, is available on GitHub under an open-access license. This promotes transparency and enables other researchers to replicate and build upon this work. However, the actual patient data remains confidential and is not shared publicly.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages several well-established machine-learning techniques, rather than introducing a new algorithm. The primary classes of machine-learning algorithms used include logistic regression, decision trees, neural networks, and ensemble models.\n\nLogistic regression was utilized as a baseline model to predict binary outcomes based on various predictors. This method is widely recognized for its simplicity and effectiveness in binary classification tasks.\n\nDecision trees were employed to split data into branches based on input features, facilitating the interpretation and visualization of decisions. This approach is particularly useful for understanding the decision-making process within the model.\n\nNeural networks, specifically deep learning models, were used to capture complex non-linear relationships among variables. These models were developed and trained using TensorFlow and Keras, which are high-level libraries known for their ability to handle intricate neural network architectures and large datasets.\n\nEnsemble models were created by combining multiple algorithms to improve overall performance. This technique leverages the strengths of each individual algorithm, resulting in a more robust and accurate predictive model.\n\nGiven that these algorithms are established and widely used in the machine-learning community, there was no need to publish them in a machine-learning journal. Instead, our focus was on applying these techniques to enhance the AI-enhanced care model in hospital wards, demonstrating their effectiveness in a real-world healthcare setting.",
  "optimization/meta": "The AI-enhanced care model in hospital wards employs an ensemble approach, which can be considered a form of meta-predictor. This method combines multiple machine learning algorithms to improve overall performance by leveraging the strengths of each individual algorithm.\n\nThe ensemble models used in this study include logistic regression, decision trees, and neural networks. Logistic regression serves as a baseline model to predict the probability of binary outcomes based on one or more predictors. Decision trees split data into branches based on input features, facilitating the interpretation and visualization of decisions. Neural networks, developed using TensorFlow and Keras, capture complex non-linear relationships among variables.\n\nThe training data for these models is split into training (70%) and testing (30%) sets to ensure that the models are trained and validated independently. This split helps to prevent overfitting and ensures that the models generalize well to new, unseen data. The use of ensemble models allows for a more robust and accurate prediction of critical events in hospitalized patients, enhancing the overall effectiveness of the AI-enhanced care model.",
  "optimization/encoding": "Data encoding and preprocessing were crucial steps in preparing the dataset for the machine-learning algorithms. The process began with data cleaning, which involved removing duplicate records and handling missing values through mean or median imputation techniques. This ensured that the dataset was free from inconsistencies and that missing data did not bias the model.\n\nNormalization was the next step, where variables were scaled to ensure all features were within the same range. Min-max normalization was used to enhance the performance of the machine-learning models by standardizing the data.\n\nClass balancing was addressed using the Synthetic Minority Over-sampling Technique (SMOTE) to prevent the model from being biased toward the majority class. SMOTE generated synthetic samples for the minority class, thereby creating a more balanced dataset.\n\nThe dataset was then split into training (70%) and testing (30%) sets to train and validate the model. This split ensured that the model could be evaluated on unseen data, providing a more accurate assessment of its performance.\n\nSeveral machine-learning algorithms were employed, including logistic regression, decision trees, neural networks, and ensemble models. Logistic regression was used as a baseline model to predict the probability of binary outcomes based on one or more predictors. Decision trees were utilized for their ability to split data into branches based on input features, facilitating the interpretation and visualization of decisions. Neural networks, built using TensorFlow and Keras, captured complex non-linear relationships among variables. Ensemble models combined several algorithms to improve overall performance by leveraging the strengths of each.\n\nThe models were evaluated using metrics such as precision, sensitivity (recall), specificity, and the area under the ROC curve (AUROC). Precision measured the proportion of all positive predictions that were correct. Sensitivity assessed the model's ability to correctly identify critical events (CEs). Specificity evaluated the model's ability to correctly identify non-CEs. The AUROC provided an overall measure of the model's ability to distinguish between classes.",
  "optimization/parameters": "In our study, the model utilized a comprehensive set of input parameters to ensure robust and accurate predictions. The parameters were carefully selected based on their clinical relevance and statistical significance in predicting critical events (CEs). The key variables included vital signs such as oxygen saturation (SpO2), respiratory rate, blood pressure, heart rate, and temperature. Additionally, critical laboratory parameters such as white blood cell count, C-reactive protein, procalcitonin, troponin T, pH, partial pressure of carbon dioxide (pCO2), and lactate were incorporated. Other important variables considered were the level of consciousness, need for oxygen therapy, and the intervention group.\n\nThe selection of these parameters was guided by a multivariate prediction model that identified strong predictors of CE risk. Variables like SpO2, respiratory rate, need for oxygen therapy, blood pressure, heart rate, level of consciousness, and temperature emerged as significant contributors to the model's predictive power. This selection process ensured that the model was both clinically meaningful and statistically sound, enhancing its ability to detect and respond to critical patient conditions effectively. The total number of parameters (p) used in the model was determined through a rigorous statistical analysis, ensuring that only the most relevant and impactful variables were included.",
  "optimization/features": "The input features used in our AI-enhanced care model in hospital wards encompass a comprehensive set of variables that are crucial for predicting critical events (CEs). These features are categorized into three main groups: vital signs, laboratory parameters, and other clinical variables.\n\nThe vital signs include oxygen saturation as measured by pulse oximetry (SpO2), respiratory rate, blood pressure, heart rate, and temperature. These physiological measurements are routinely collected during patient monitoring and provide essential insights into a patient's health status.\n\nLaboratory parameters considered in the model include elevated white blood cell count, C-reactive protein, procalcitonin, troponin T, pH, partial pressure of carbon dioxide (pCO2), and lactate. These biomarkers are indicative of various physiological processes and can signal potential complications or deteriorations in a patient's condition.\n\nOther variables incorporated into the model are the level of consciousness, the need for oxygen therapy, and the occurrence of critical events. These variables add contextual information that is vital for a holistic assessment of patient risk.\n\nFeature selection was performed to ensure that only the most relevant variables were included in the model. This process was conducted using the training set only, adhering to best practices in machine learning to prevent data leakage and maintain the integrity of the model's performance evaluation. The selection process involved statistical analyses, including Fisher\u2019s chi-square test, to identify variables that significantly contribute to the prediction of CEs. This rigorous approach ensures that the model is both efficient and effective in detecting high-risk patients.",
  "optimization/fitting": "The fitting method employed in this study involved several machine learning models, each with its own set of parameters and techniques to address overfitting and underfitting.\n\nFor logistic regression, a baseline model was used to predict binary outcomes. This model is relatively simple and has a limited number of parameters, reducing the risk of overfitting. To further mitigate overfitting, regularization techniques such as L1 and L2 regularization were applied. These techniques add a penalty to the loss function, discouraging the model from fitting the noise in the training data.\n\nDecision trees were used for their interpretability and ability to handle non-linear relationships. To prevent overfitting, pruning techniques were employed, which involve removing parts of the tree that do not provide power in predicting target variables. Additionally, the maximum depth of the trees was limited, ensuring that the model did not become too complex.\n\nNeural networks, built using TensorFlow and Keras, were used to capture complex non-linear relationships. These models have a large number of parameters, which can lead to overfitting. To address this, dropout layers were included, which randomly set a fraction of input units to zero at each update during training time. This helps prevent overfitting by ensuring that the model does not rely too heavily on any single neuron. Early stopping was also used, where training was halted when the model's performance on a validation set stopped improving.\n\nEnsemble models were created by combining multiple algorithms to leverage their strengths. Techniques such as stacking and bagging were used to improve overall performance. These methods help in reducing both overfitting and underfitting by averaging the predictions of multiple models, thereby smoothing out the individual biases and variances.\n\nTo address class imbalance, the synthetic minority over-sampling technique (SMOTE) was implemented. This technique generates synthetic samples for the minority class, helping the model to learn from a more balanced dataset and reducing the risk of underfitting the minority class.\n\nThe dataset was split into training (70%) and testing (30%) sets. This split ensured that the model was trained on a sufficient number of samples while still having a robust testing set to evaluate its performance. Cross-validation techniques were also employed to ensure that the model's performance was consistent across different subsets of the data.\n\nIn summary, various techniques were used to address overfitting and underfitting, including regularization, pruning, dropout layers, early stopping, and ensemble methods. These strategies ensured that the models were robust and generalizable to new data.",
  "optimization/regularization": "In our study, we implemented several techniques to prevent overfitting and ensure the robustness of our models. One key method we used was data preprocessing, which included data cleaning to remove duplicate records and handle missing values through mean or median imputation. This step helped to ensure that our models were trained on clean and consistent data, reducing the risk of overfitting to noise or artifacts.\n\nAnother crucial technique we employed was normalization. We scaled our variables using min-max normalization to ensure that all features were within the same range. This process is essential for enhancing the performance of machine learning models and preventing them from being biased towards features with larger scales.\n\nTo address class imbalance, which can lead to overfitting to the majority class, we used the Synthetic Minority Over-sampling Technique (SMOTE). SMOTE generates synthetic samples for the minority class, helping to balance the dataset and ensuring that our models could generalize better to all classes.\n\nIn addition to these preprocessing steps, we utilized ensemble models. By combining multiple models, we leveraged the strengths of each algorithm, which helped to improve overall performance and reduce the risk of overfitting. Ensemble methods are particularly effective in capturing diverse patterns in the data, leading to more robust and generalizable predictions.\n\nFurthermore, we split our dataset into training (70%) and testing (30%) sets. This split allowed us to train our models on a substantial portion of the data while reserving a separate set for validation. This approach helped us to assess the models' performance on unseen data, providing a more accurate measure of their generalization capabilities.\n\nOverall, these techniques collectively contributed to the prevention of overfitting, ensuring that our models were reliable and effective in predicting critical health events in hospitalized patients.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are all available and accessible to the public. This transparency is a key aspect of our work, ensuring that other researchers can replicate and build upon our findings.\n\nAll the code, including preprocessing scripts, model training code, and deployment scripts, is hosted on GitHub under an open-access license. This platform allows for easy access and collaboration, promoting reproducibility in our research. The use of open-source frameworks and tools further supports this goal.\n\nAdditionally, the software tools and libraries utilized in the development of our AI models are all open-access. This includes Python, Pandas, NumPy, Scikit-learn, TensorFlow/Keras, and the imbalanced-learn library. These tools were chosen for their extensive support and community contributions, which facilitate transparency and reproducibility.\n\nFor the deployment of our trained AI models, we used Flask, a lightweight web framework, and Docker, which ensures consistent performance across different environments by packaging the code and dependencies together. These tools are widely used and well-documented, making it straightforward for others to implement similar systems.\n\nIn summary, all the necessary configurations, schedules, and parameters are publicly available, and the use of open-source tools and platforms ensures that our work can be easily replicated and verified by the scientific community.",
  "model/interpretability": "The AI-enhanced care model in hospital wards incorporates several machine learning techniques, each with varying degrees of interpretability. The model is not entirely a black box; certain components are designed to be transparent and interpretable.\n\nLogistic regression, one of the models used, is inherently interpretable. It provides coefficients that indicate the strength and direction of the relationship between each predictor variable and the outcome. For instance, if the coefficient for a variable like respiratory rate is positive, it suggests that higher respiratory rates are associated with an increased likelihood of a critical event. This transparency allows healthcare professionals to understand the contribution of each variable to the model's predictions.\n\nDecision trees are another key component of the model. These trees split data into branches based on input features, making them highly interpretable. Each split in the tree represents a decision based on a specific variable, such as oxygen saturation or heart rate. This visual representation allows clinicians to trace the decision-making process and understand why a particular prediction was made. For example, a decision tree might show that patients with a low oxygen saturation level and a high heart rate are more likely to experience a critical event.\n\nEnsemble models, which combine multiple algorithms, can be less interpretable than individual models like logistic regression or decision trees. However, techniques such as feature importance scores can be used to provide insights into which variables are most influential in the ensemble's predictions. For instance, if the ensemble model identifies respiratory rate and blood pressure as the most important features, clinicians can understand that these variables play a crucial role in the model's decision-making process.\n\nIn summary, while some aspects of the AI-enhanced care model are complex and may not be fully transparent, significant efforts have been made to incorporate interpretable components. Logistic regression and decision trees provide clear, understandable insights into the model's predictions, ensuring that healthcare professionals can trust and act upon the model's recommendations.",
  "model/output": "The model developed in this study is primarily a classification model. It is designed to predict the occurrence of critical events (CEs) in hospital wards. The model uses various machine learning algorithms, including logistic regression, decision trees, neural networks, and ensemble models, to classify patients based on their risk of experiencing a CE. The output of the model is a binary classification, indicating whether a patient is likely to experience a CE or not. The performance of the model is evaluated using metrics such as precision, sensitivity (recall), specificity, and the area under the ROC curve (AUROC), which are standard for classification tasks. The model's ability to distinguish between patients who will experience a CE and those who will not is crucial for early detection and intervention, thereby improving patient outcomes.\n\nThe model was trained and validated using a dataset split into training (70%) and testing (30%) sets. Data preprocessing steps, including data cleaning, normalization, and class balancing using SMOTE, were employed to ensure the model's robustness and accuracy. The integration of the model with the mNEWS2-Lab system aims to enhance the detection and care of critical patients, providing timely alerts and interventions. The deployment of the model using open-source frameworks like Flask, Docker, and GitHub ensures its accessibility and reproducibility, promoting transparency and enabling further research and development.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the AI models developed in this study is publicly available. It is hosted on GitHub, an open-access platform that promotes transparency and reproducibility. The repository includes preprocessing scripts, model training code, and deployment scripts, allowing other researchers to replicate and build upon this work.\n\nTo ensure the models can run consistently across different environments, Docker was used to containerize the application. This packaging includes the code and all necessary dependencies, making it easier to deploy and run the models in various settings.\n\nAdditionally, the models were deployed using Flask, a lightweight web framework. This framework enables the integration of the AI models into existing hospital information systems, facilitating their practical application in clinical settings. The use of open-source tools and frameworks ensures that the models are accessible and can be easily adapted for use in different healthcare environments.",
  "evaluation/method": "The evaluation method for the AI-enhanced care model involved a comprehensive assessment of various machine learning models using several key metrics. The models were trained on a dataset that was split into 70% for training and 30% for testing. This split ensured that the models were evaluated on data they had not seen during training, providing a robust measure of their generalizability.\n\nSeveral metrics were used to evaluate the performance of the models:\n\n* Precision: This metric measured the proportion of all positive predictions that were correct. It indicated how often the model was correct when it predicted a critical event.\n* Sensitivity (Recall): This metric assessed the model's ability to correctly identify critical events. It measured the proportion of actual critical events that were correctly predicted by the model.\n* Specificity: This metric evaluated the model's ability to correctly identify non-critical events. It measured the proportion of actual non-critical events that were correctly predicted by the model.\n* Area Under the Receiver Operating Characteristic Curve (AUROC): This metric provided an overall measure of the model's ability to distinguish between critical and non-critical events. A higher AUROC indicated better performance.\n\nThe models evaluated included logistic regression, decision trees, neural networks, and ensemble models. Each model was assessed using the aforementioned metrics to determine its effectiveness in predicting critical events. The neural networks, in particular, showed high performance with an AUROC of 0.92, indicating excellent discriminative ability. Ensemble models, which combined multiple algorithms, also showed variable results depending on the combination of models and simulated data.\n\nAdditionally, the impact of the AI-enhanced system was evaluated by comparing the incidence of critical events before and after the implementation of the mNEWS2-Lab system and its AI enhancement. The results showed a significant reduction in the incidence of critical events, demonstrating the effectiveness of the AI-enhanced care model in improving patient outcomes. The economic cost analysis further supported these findings, showing substantial annual savings due to the reduced incidence of critical events.",
  "evaluation/measure": "In our evaluation of the AI-enhanced care model, we focused on several key performance metrics to assess the effectiveness of our models. These metrics include the Area Under the Receiver Operating Characteristic Curve (AUROC), precision (also known as Positive Predictive Value, PPV), sensitivity (or recall), and specificity.\n\nThe AUROC provides a comprehensive measure of the model's ability to distinguish between positive and negative cases across all threshold levels. Precision indicates the proportion of true positive predictions among all positive predictions made by the model. Sensitivity measures the model's ability to correctly identify critical events, while specificity reflects its ability to correctly identify non-critical events.\n\nThese metrics were chosen because they offer a balanced view of the model's performance, covering both the true positive and true negative rates. This set of metrics is widely used in the literature for evaluating machine learning models in healthcare, ensuring that our evaluation is both rigorous and comparable to other studies in the field.\n\nAdditionally, we evaluated the relative risk and risk reduction to understand the impact of our AI-enhanced system on patient outcomes. This includes calculating the relative risk of critical events before and after the implementation of the AI-enhanced system, as well as the percentage reduction in risk.\n\nBy reporting these metrics, we aim to provide a clear and comprehensive assessment of our model's performance, demonstrating its effectiveness in improving patient care and outcomes.",
  "evaluation/comparison": "In our study, we employed several machine learning models to evaluate their performance in predicting critical events (CEs) in hospital wards. To ensure a comprehensive comparison, we included both simple and complex models.\n\nWe started with logistic regression as a baseline model. This choice was motivated by its simplicity and interpretability, providing a foundational benchmark against which more complex models could be compared. Logistic regression helped us understand the basic relationships between predictors and the outcome, offering a clear starting point for our analysis.\n\nNext, we utilized decision trees, which allowed us to explore more intricate decision-making processes. Decision trees are advantageous for their ability to handle non-linear relationships and interactions between variables, making them a suitable intermediate step between logistic regression and more advanced techniques.\n\nFor capturing complex non-linear relationships, we implemented neural networks using TensorFlow and Keras. These deep learning models are particularly effective for handling large datasets and intricate patterns, providing a robust comparison to the simpler models.\n\nAdditionally, we employed ensemble models, which combine multiple algorithms to improve overall performance. By leveraging the strengths of different models, ensemble methods often achieve higher accuracy and robustness, making them a crucial part of our comparative analysis.\n\nTo address class imbalance in our dataset, we used the synthetic minority over-sampling technique (SMOTE) via the imbalanced-learn library. This step was essential for ensuring that our models were not biased toward the majority class, thereby providing a fair comparison across different methodologies.\n\nIn summary, our evaluation included a range of models from simple to complex, ensuring a thorough comparison of their predictive capabilities. This approach allowed us to identify the most effective methods for early CE detection in hospital wards.",
  "evaluation/confidence": "The evaluation of the AI-enhanced care model in hospital wards, particularly the mNEWS2-Lab system, includes several key performance metrics that demonstrate its effectiveness and reliability. The system's sensitivity is 89.7%, indicating a high ability to correctly identify patients at risk for critical events (CEs). The specificity is 97.9%, which means it has a low rate of false positives, crucial for preventing unnecessary interventions. The positive predictive value is 75.5%, and the negative predictive value is 99.2%, further validating the scale's reliability in clinical settings.\n\nThe area under the receiver operating characteristic curve (AUROC) for the mNEWS2-Lab scale is 0.938, showing excellent discriminative ability. This metric is essential for ensuring that the scale can be confidently used to guide clinical decision-making and resource allocation.\n\nThe AI-enhanced system, particularly neural networks, showed an AUROC of 0.92, outperforming traditional logistic regression (AUROC of 0.85) and decision trees (AUROC of 0.88). This improvement highlights the superior predictive performance of AI models in detecting CEs.\n\nStatistical significance is noted in the reduction of CEs, with a relative risk reduction of 65% post-implementation of the mNEWS2-Lab system and an additional 10% reduction after AI enhancement. These results are statistically significant (P<0.001), confirming the system's effectiveness in improving patient outcomes.\n\nThe economic analysis also supports the system's viability, showing substantial annual savings and cost-effectiveness. The initial implementation costs are offset by reduced ICU admissions and shorter hospital stays, making it a sound investment for healthcare facilities.\n\nIn summary, the performance metrics, including sensitivity, specificity, AUROC, and economic analyses, all indicate high confidence in the mNEWS2-Lab system's ability to enhance patient care and outcomes. The statistical significance of the results further supports the claim that the AI-enhanced system is superior to traditional methods.",
  "evaluation/availability": "The evaluation availability for this study is designed to ensure transparency and reproducibility. All the software tools and libraries used in the development of the AI model are open-access. The codebase, including preprocessing scripts, model training code, and deployment scripts, is available on GitHub under an open-access license. This promotes transparency and enables other researchers to replicate and build upon this work. The use of open-source frameworks and tools, such as Flask for deploying machine learning models as web services and Docker for containerizing the application, ensures that the model can run consistently across different environments. This approach allows for the easy accessibility and verification of the evaluation processes and results."
}