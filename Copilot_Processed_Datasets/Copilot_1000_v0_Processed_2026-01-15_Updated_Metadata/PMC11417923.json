{
  "publication/title": "Diabetic Retinopathy Features Segmentation without Coding Experience with Computer Vision Models YOLOv8 and YOLOv9.",
  "publication/authors": "Rizzieri N, Dall'Asta L, Ozoli\u0146\u0161 M",
  "publication/journal": "Vision (Basel, Switzerland)",
  "publication/year": "2024",
  "publication/pmid": "39311316",
  "publication/pmcid": "PMC11417923",
  "publication/doi": "10.3390/vision8030048",
  "publication/tags": "- YOLOv8\n- YOLOv9\n- Diabetic Retinopathy\n- Retinal Lesion Detection\n- Computer Vision\n- Object Detection\n- Medical Imaging\n- Deep Learning\n- Optic Disc Detection\n- Microaneurysms Detection\n- Hemorrhages Detection\n- Mean Average Precision\n- Precision\n- Recall\n- F1 Score\n- Confusion Matrix\n- False Positives\n- False Negatives\n- Image Annotation\n- Dataset Preparation\n- Data Augmentation\n- Telemedicine\n- Home-Based Examinations\n- Glaucoma Detection\n- Vertical Cup-to-Disc Ratio\n- Adaptive Histogram Equalization\n- RGB Images\n- Image Pre-processing\n- Fundus Photographs\n- Computer-Assisted Diagnosis\n- Optometry\n- Medical Professionals\n- Research\n- API Services\n- Online Systems\n- Annotation Platform\n- Computer Vision Models\n- DR Lesion Detection\n- Messidor Database\n- DDR Dataset\n- EyePacs Dataset\n- Image Segmentation\n- Object Localization\n- Object Classification\n- Real-Time Detection\n- Performance Metrics\n- Model Evaluation\n- Experimental Results\n- Medical Image Analysis\n- Early Disease Detection\n- Population-Based Screening\n- Clinical Practice\n- Prevention and Treatment Outcomes\n- Medical Retina Experts\n- Computer-Assisted Lesion Segmentation\n- Image Classification\n- Pixel-Level Labeling\n- International DR Classification Standards\n- Deep Neural Networks\n- Overfitting\n- Standardized Datasets\n- Hyperparameter Adjustment\n- Coding Expertise\n- Non-Programming Experts\n- Low-Cost API Services\n- Education-Based Software\n- Healthcare Services\n- Innovation in Healthcare\n- Progress in Computer-Assisted Diagnoses",
  "dataset/provenance": "The dataset used in this study was sourced from the Messidor dataset, which is a well-recognized online database for healthcare research purposes. This dataset contains 1200 retinal fundus RGB color numerical images of the posterior pole, comprising 540 healthy images and 660 images of diabetic retinopathy (DR) patients. The images are available in resolutions of 1440 \u00d7 960, 2240 \u00d7 1488, or 2304 \u00d7 1536 pixels, with 800 images taken with pupil dilation and 400 without. These images were captured using a color video 3CCD camera on a Topcon TRC NW6 non-mydriatic retinographer with a 45-degree field of view. Each image is accompanied by an Excel file containing medical diagnoses.\n\nFor our research, we meticulously selected 100 DR fundus RGB images from the Messidor dataset. These images were chosen based on their gradability and varying DR severity levels, specifically from levels 2 to 3 according to the reference scale. Each selected image was annotated with pixel-level and bounding-box annotations to highlight the desired features, such as microaneurysms (MAs), hemorrhages (HEMOs), and exudates (EXs). This annotated dataset was then used to train and evaluate the performance of YOLOv8 and YOLOv9 models in detecting these DR lesions.\n\nThe Messidor dataset has been widely used in the community for studies on computer-assisted diagnoses of DR. However, it is known to be prone to overfitting when training with deep neural networks. To mitigate this, we ensured the quality and reliability of our research by using standardized datasets where precise descriptions of available annotations are well-defined. An example of such a dataset is the DDR, which provides 757 annotated DR images for both image classification and lesion detection, with four pixel-level labeled classes. This dataset has been extensively used in research focused on computer-assisted lesion segmentation.",
  "dataset/splits": "The dataset used in this research was initially composed of 100 retinal fundus images from the Messidor dataset. To enhance the diversity of the dataset and mitigate overfitting, data augmentation techniques were applied. This process involved image resizing to a resolution of 640\u00d7640 pixels, auto-orientation, and a 4x4 tiling grid. Additionally, horizontal and vertical flipping, as well as 90-degree rotations in both clockwise and counter-clockwise directions, were performed. As a result, the dataset was expanded to 4256 images.\n\nThe augmented dataset was then divided into two primary splits: training and validation. The training set comprised 94% of the images, totaling approximately 4000 images. The remaining 6% of the images, around 256, were allocated to the validation set. This split ensures that the model is trained on a substantial amount of data while also having a sufficient number of images to validate its performance accurately.",
  "dataset/redundancy": "The dataset used in this study was derived from the Messidor dataset, which contains 1200 retinal fundus images, including 540 healthy images and 660 images of diabetic retinopathy (DR) patients. The Messidor dataset was chosen for its comprehensive annotations and high-quality images, making it suitable for training and validating machine learning models.\n\nTo ensure the quality and reliability of our research, we meticulously selected 100 DR fundus images from the Messidor dataset. These images were chosen based on their gradability and varying DR severity levels, specifically from levels 2 to 3 according to the reference scale. This selection process aimed to create a diverse and representative dataset for training and evaluating our models.\n\nThe selected images were annotated with pixel-level and bounding-box annotations for the desired features, including optic discs (ODs), microaneurysms (MAs), hemorrhages (HEMOs), and exudates (EXs). These annotations were crucial for training the YOLOv8 and YOLOv9 models to accurately detect and segment these features in retinal images.\n\nTo prevent overfitting and ensure the robustness of our models, we applied data augmentation techniques. The original dataset of 100 images was augmented to create a larger dataset of 4256 images. This augmentation process included image resizing to a resolution of 640\u00d7640 pixels, auto-orientation, tiling, and various transformations such as horizontal and vertical flipping, and 90-degree rotations. The augmented dataset was then split into training and validation sets, with 94% of the images used for training and 6% for validation. This split ensured that the training and validation sets were independent, reducing the risk of data leakage and overfitting.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets for DR detection. The Messidor dataset is widely recognized and has been used extensively in research focused on computer-assisted diagnoses of DR. By selecting a subset of images with varying DR severity levels and applying rigorous data augmentation techniques, we ensured that our dataset was diverse and representative, enhancing the generalizability of our findings.",
  "dataset/availability": "The dataset used in this study is derived from the Messidor dataset, which is publicly available for healthcare research purposes. The Messidor dataset contains 1200 eye fundus RGB color numerical images of the posterior pole, with 540 healthy images and 660 diabetic retinopathy (DR) images. These images were collected using a color video 3CCD camera on a Topcon TRC NW6 non-mydriatic retinographer with a 45\u00b0 field of view. The dataset includes images in TIFF format along with an Excel file containing medical diagnoses.\n\nFor our specific research, we meticulously selected 100 DR fundus RGB images from the Messidor dataset. These images were annotated with pixel-level and bounding-box annotations for desired features such as microaneurysms (MAs), hemorrhages (HEMOs), and exudates (EXs). The annotations were performed by medical experts who graded the DR severity into four levels based on the presence and amount of MAs, HEMOs, and neovascularization (NV). The macular edema risk was graded according to hard exudates.\n\nThe dataset was augmented to increase diversity and avoid overfitting. Techniques such as image resizing to a resolution of 640\u00d7640 pixels, auto-orientation, tiling, and various flipping and rotation methods were applied. This resulted in a dataset of 4256 images, with 94% used for training and 6% for validation.\n\nThe Messidor dataset is available under a license that permits its use for research purposes. The specific license details can be found on the Messidor website. To ensure the quality and reliability of our research, we corrected potential sources of errors in the dataset at the beginning of this research using the additional material supplied on the Messidor website. This included deleting image duplicates and adjusting inconsistent grades.\n\nThe dataset, including the data splits used, is not released in a public forum by us. However, the Messidor dataset, from which our dataset is derived, is publicly available and can be accessed through its official website. The license for the Messidor dataset allows for its use in research, ensuring that the data can be utilized by other researchers while adhering to the terms and conditions specified in the license.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the YOLO (You Only Look Once) architecture, which is a state-of-the-art AI architecture known for its efficiency in object detection tasks. YOLO is characterized by its ability to perform object localization and classification in a single forward pass, making it highly suitable for real-time detection and segmentation.\n\nThe specific versions of YOLO used in our research are YOLOv8 and YOLOv9. YOLOv8 is noted for its improved performance in detecting different-sized objects and its increased identification efficiency compared to previous versions. It has been widely used in various research studies due to its faster speed and higher accuracy.\n\nYOLOv9, on the other hand, introduces several novel features aimed at addressing challenges in deep learning, such as information loss across multiple layers. It implements Programmable Gradient Information (PGI), which helps preserve essential data throughout the network's depth, ensuring more reliable gradient generation and model convergence. This version also incorporates Reversible Functions (RFs) to mitigate data degradation and maintain critical information for object detection tasks.\n\nThe reason these algorithms were not published in a machine-learning journal is that they are part of the ongoing development and improvement of the YOLO architecture by Ultralytics. The focus of our study is on applying these advanced object detection algorithms to the specific task of detecting diabetic retinopathy (DR) lesions in retinal fundus photographs. Our contributions lie in demonstrating the feasibility and performance of these models in a medical imaging context, rather than in the development of the algorithms themselves.",
  "optimization/meta": "The model discussed in this publication does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on its own architecture and training process to achieve object detection tasks.\n\nThe YOLOv9 model incorporates several innovative components to enhance its performance. These include the Programmable Gradient Information (PGI) framework, which consists of a main branch for inference, an auxiliary reversible branch for reliable gradient calculation, and multi-level auxiliary information. Additionally, the model uses Reversible Columns (RevCol) and a unique design called GELAN, which fits within the PGI framework. These components work together to preserve essential data across the network's depth, ensuring more reliable gradient generation, convergence, and overall performance.\n\nThe training process involves data augmentation techniques to increase dataset diversity and avoid overfitting. The dataset used for training and validation is clearly defined, with 94% of the images used for training and 6% for validation. This ensures that the training data is independent and not reused for validation, maintaining the integrity of the evaluation process.\n\nThe performance of the model is evaluated using several metrics, including Average Precision (AP), Mean Average Precision (mAP), Precision (P), Recall (R), Accuracy (Acc), F1 score, and Intersection over Union (IoU). These metrics provide a comprehensive assessment of the model's ability to detect and classify objects accurately.\n\nIn summary, the model does not operate as a meta-predictor but rather leverages its own advanced architecture and training methods to achieve superior object detection performance. The training data is independent, and the evaluation metrics are well-defined to ensure a thorough assessment of the model's capabilities.",
  "optimization/encoding": "The data encoding and preprocessing steps were designed to enhance the performance of the machine-learning algorithms used in our study. We began by collecting retinal fundus photographs from diabetic retinopathy (DR) patients, specifically using the Messidor dataset. This dataset consists of 1200 eye fundus RGB color images, with varying resolutions and conditions. To ensure data quality, we corrected potential errors, removed duplicates, and adjusted inconsistent grades.\n\nWe selected 100 DR fundus RGB images from the Messidor dataset, focusing on images with varying DR severity levels. Each image was annotated at the pixel level and with bounding boxes to highlight key features such as microaneurysms (MAs), hemorrhages (HEMOs), and exudates (EXs), as well as the optic disc (OD). These annotations were crucial for training and evaluating our models.\n\nData augmentation techniques were employed to increase the diversity of our dataset and to prevent overfitting. This included image resizing to a resolution of 640\u00d7640 pixels, auto-orientation, and tiling the images into a 4x4 grid. Additionally, we applied horizontal and vertical flipping, as well as 90-degree rotations in both clockwise and counter-clockwise directions, and upside-down flipping. These augmentations resulted in a dataset of 4256 images, with 94% used for training and 6% for validation.\n\nThe batch size for training was set to 8, which is smaller than the default value of 32. This decision was made to balance computational resources and the need for faster convergence. Each model was trained for 100 epochs, with an early-stopping mechanism in place to halt training if the model did not improve for ten consecutive epochs.\n\nImage preprocessing was kept minimal to assess the models' performance with unmodified RGB images. However, we recognize the potential benefits of more advanced preprocessing techniques, such as green channel extraction and adaptive histogram equalization, for future research. These techniques could enhance the contrast of red lesions, making them more distinguishable in the images.",
  "optimization/parameters": "In our research, we utilized two specific variants of the YOLOv9 model: the compact (c) and extensive (e) versions. The compact version employs approximately 27.9 million parameters, while the extensive version uses around 60.5 million parameters. These parameter counts were determined based on the available iterations of the YOLOv9 model at the outset of our work.\n\nThe selection of these variants was influenced by the need to balance computational efficiency and model performance. The compact version offers a more lightweight solution, suitable for environments with limited computational resources, while the extensive version provides enhanced detection capabilities at the cost of increased computational demand.\n\nAdditionally, we compared these models with various versions of the YOLOv8 architecture, which includes nano, small, medium, large, and extra-large variants. The parameter counts for these models range from 3.4 million for the nano version to 71.8 million for the extra-large version. This comparison allowed us to evaluate the trade-offs between model size, computational requirements, and detection performance across different architectures.",
  "optimization/features": "The input features for our models consisted of images from a digital dataset containing diabetic retinopathy (DR) lesions and optic disc (OD) annotations. Specifically, the dataset included microaneurysms (MAs), hemorrhages (HEMOs), and exudates (EXs), along with OD annotations from the Messidor database.\n\nThe images were preprocessed through data augmentation techniques to increase dataset diversity and avoid overfitting. This included image resizing to a resolution of 640\u00d7640 pixels, auto-orientation, and various transformations such as tiling, horizontal and vertical flipping, and rotations. The final augmented dataset consisted of 4256 images, with 94% used for training and 6% for validation.\n\nFeature selection in the traditional sense was not performed, as the input features were the raw pixel values of the images. However, the data augmentation process can be seen as a form of implicit feature selection, as it enhances the diversity and robustness of the input data, effectively selecting and emphasizing relevant features for the models.\n\nThe data augmentation was applied to the entire dataset before splitting it into training and validation sets. Therefore, the augmentation process did not use the training set exclusively, ensuring that the validation set remained independent and unbiased. This approach helped in generalizing the model's performance across different variations of the input images.",
  "optimization/fitting": "In our study, we employed several strategies to address potential overfitting and underfitting issues. Initially, we started with a dataset of 100 images, which might seem small for training complex models. To mitigate overfitting, we applied extensive data augmentation techniques. These included image resizing to 640x640 pixels, auto-orientation, tiling, and various transformations such as horizontal and vertical flipping, and 90-degree rotations. This augmentation increased our dataset to 4256 images, significantly enhancing its diversity.\n\nWe used a batch size of 8, which is smaller than the default of 32. This choice was made to balance computational resources and the need for sufficient updates to the model parameters. We trained each model for 100 epochs but implemented an early-stopping mechanism. If the model did not improve for ten consecutive epochs, training was halted, indicating convergence and preventing overfitting.\n\nTo evaluate model performance, we used a validation set comprising 6% of the total images. Performance metrics such as Average Precision (AP), Mean Average Precision (mAP), Precision, Recall, Accuracy, F1 score, and Intersection over Union (IoU) were calculated. These metrics provided a comprehensive evaluation of the model's ability to detect objects accurately and avoid false positives and negatives.\n\nThe confusion matrix was used to analyze the detection results, with a fixed confidence limit of 0.25. This limit helped in filtering out low-confidence bounding boxes, ensuring that the model's predictions were reliable. The confusion matrix also allowed us to identify and address specific types of errors, such as false positives and false negatives, for different classes of objects.\n\nBy carefully monitoring these metrics and employing data augmentation and early stopping, we ensured that our models neither overfitted nor underfitted the data. The use of a validation set and comprehensive performance metrics further validated the robustness of our approach.",
  "optimization/regularization": "In our research, we implemented several techniques to prevent overfitting and ensure the robustness of our models. One of the primary methods we used was data augmentation. We started with a dataset of 100 images and applied various augmentation techniques to increase the diversity of the dataset. This included image resizing to a resolution of 640\u00d7640 pixels, auto-orientation, and a 4x4 tiling. Additionally, we performed horizontal and vertical flipping, as well as 90-degree rotations in both clockwise and counter-clockwise directions, and upside-down rotations. These augmentations helped to create a more extensive and varied dataset, consisting of 4256 images, which significantly reduced the risk of overfitting.\n\nAnother technique we employed was early stopping. During the training phase, we monitored the model's performance and set an early-stop parameter. If the model did not show improvement for ten consecutive epochs, the training process was halted. This approach ensured that the model did not continue to train unnecessarily, which could lead to overfitting.\n\nWe also used a relatively small batch size of 8 during training. While the default batch size is typically larger, we chose a smaller size to balance the need for computational resources and the risk of overfitting. A smaller batch size can help the model generalize better to unseen data.\n\nFurthermore, we divided our dataset into training and validation sets, with 94% of the images used for training and 6% for validation. This split allowed us to evaluate the model's performance on data it had not seen during training, providing a more accurate assessment of its generalization capabilities.\n\nBy combining these techniques, we were able to effectively mitigate the risk of overfitting and ensure that our models performed well on both training and validation datasets.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our research are detailed within the publication. Specifically, we discussed the data augmentation techniques applied, including image resizing to 640\u00d7640 pixels, auto-orientation, tiling, and various flipping and rotation methods. These steps were crucial in increasing the diversity of our dataset and preventing overfitting. The batch size was set to 8, significantly smaller than the default of 32, to manage computational resources effectively while ensuring model convergence. We trained each model for 100 epochs with an early-stop mechanism that halted training if the model did not improve for ten consecutive epochs.\n\nRegarding the availability of model files and optimization parameters, the specific files and detailed parameters are not directly provided in the text. However, the methods and configurations described can be replicated based on the information given. For those interested in accessing the exact model files or more detailed optimization parameters, it would be advisable to contact the authors directly or refer to any supplementary materials that may accompany the publication.\n\nThe license under which these configurations and schedules are made available is not specified in the text. Typically, academic publications allow for the reproduction of methods and results for research purposes, but for specific licensing details, one would need to refer to the journal's policies or contact the authors.",
  "model/interpretability": "The models we employed, specifically YOLOv8 and YOLOv9, are generally considered to be black-box models. These models, particularly in their deep learning architectures, are known for their complexity and the lack of transparency in how they make predictions. The internal workings of these models involve numerous layers and parameters, making it challenging to interpret the exact decision-making process.\n\nHowever, YOLOv9 introduces several architectural components that aim to enhance interpretability to some extent. For instance, the Programmable Gradient Information (PGI) framework in YOLOv9 includes a main branch for inference, an auxiliary reversible branch for reliable gradient calculation, and multi-level auxiliary information. This design helps in preserving essential data across the network's depth, ensuring more reliable gradient generation and convergence. The reversible functions within YOLOv9 ensure that no information is lost during data transformation, which can aid in understanding how the model processes input data.\n\nAdditionally, the Path Aggregation Network (PAN) and Reversible Columns (RevCol) in YOLOv9 contribute to a more structured flow of information, which can be beneficial for interpretability. The PAN helps in aggregating features from different layers, providing a clearer path for feature extraction and integration. The RevCol ensures that the transformations applied to the data are reversible, maintaining the integrity of the input information throughout the network.\n\nWhile these architectural enhancements do not make the model fully transparent, they do provide some insights into how the model processes and utilizes data. This can be particularly useful for understanding the model's behavior and improving its performance. However, for a more detailed interpretation of the model's decisions, further techniques such as gradient-based methods or attention mechanisms may be required.",
  "model/output": "The model discussed in this publication is primarily a classification model. It is designed to detect and classify specific features in retinal images, such as the optic disc (OD), microaneurysms (MA), and hemorrhages (HEMO). The model variants, including YOLOv8 and YOLOv9, are evaluated using metrics like Average Precision (AP) and mean Average Precision (mAP), which are typical for object detection and classification tasks. The experiments involve detecting these features with varying levels of accuracy, precision, recall, and F1 scores, indicating that the model's output is categorical rather than continuous. This confirms that the model is used for classification purposes, aiming to identify and differentiate between different types of lesions and structures in retinal images.\n\nThe model's performance is assessed through various experiments, where different variants are tested for their ability to detect OD, MA, and HEMO. The results are presented in tables, showing the AP and mAP values for each class, which are key metrics in classification tasks. Additionally, the model's precision, recall, and F1 scores are provided, further supporting the classification nature of the model. The experiments also include the exclusion of certain classes, such as MA, to improve the detection of other features like HEMO, which is a common approach in refining classification models.\n\nIn summary, the model is a classification model designed to detect and classify specific features in retinal images. The evaluation metrics and experimental results confirm that the model's output is categorical, focusing on identifying and differentiating between different types of lesions and structures in the retina.",
  "model/duration": "The execution time for the models varied depending on the specific variant used. For the YOLOv8 variants, the smallest variant, YOLOv8-s, was particularly efficient, completing the task in less than ten minutes. This variant not only maintained high accuracy but also kept the computational load minimal, making it suitable for applications where quick processing is essential. Other variants, such as the nano, medium, large, and extra-large, had different performance characteristics, but the small variant stood out for its balance of speed and accuracy. For YOLOv9, specific execution times were not detailed, but the focus was on the performance metrics rather than the time taken for execution.",
  "model/availability": "The models discussed in this publication, specifically YOLOv8 and YOLOv9, are powerful tools available as online systems. These systems not only offer an annotation platform for creating desired datasets for various computer vision tasks but also provide a series of working computer vision models. These models are accessible as API services, enabling optometrists, medical professionals, and researchers to utilize them without needing extensive coding expertise.\n\nThe availability of these models as API services allows private practices and public hospital teams to create image datasets and establish platforms for objectively evaluating medical images, particularly those from diabetic retinopathy (DR) patients. This accessibility is crucial for enhancing patient care and healthcare services, including telemedicine and home-based examinations, while keeping costs low.\n\nThe use of these models as API services means that the source code is not directly released to the public. However, the functionality is made available through these APIs, allowing users to integrate the models into their workflows seamlessly. This approach ensures that the models can be used by a wide range of professionals, including those who may not have the technical skills to implement and run the models from scratch.\n\nThe models are designed to be versatile and adaptable, making them suitable for various applications in computer-assisted diagnoses of DR. By providing these models as API services, the focus is on making the technology accessible and user-friendly, rather than on the intricacies of the underlying code. This strategy aligns with the goal of empowering professionals to enhance their diagnostic capabilities and improve patient outcomes.",
  "evaluation/method": "The evaluation method employed in our study involved a comprehensive assessment of various YOLO models, specifically YOLOv8 and YOLOv9, using a range of performance metrics. The models were evaluated on a validation dataset, which was derived from an augmented dataset of retinal images. The augmentation process included resizing images to 640x640 pixels, applying auto-orientation, tiling, and various transformations such as flipping and rotations. This resulted in a dataset of 4256 images, with 94% used for training and 6% for validation.\n\nThe models were trained for 100 epochs with a batch size of 8, and an early-stop mechanism was implemented to halt training if no improvement was observed for ten consecutive epochs. This approach ensured efficient use of computational resources while allowing the models to converge effectively.\n\nSeveral key metrics were used to evaluate the models' performance:\n\n* Average Precision (AP): This metric computes the area under the Precision-Recall curve, providing a single value that encapsulates the model\u2019s precision and recall performance.\n* Mean Average Precision (mAP): This extends the concept of AP by calculating the average AP values across multiple object classes, offering a comprehensive evaluation of the model\u2019s performance.\n* Precision (P) and Recall (R): Precision quantifies the proportion of true positives among all positive predictions, while Recall calculates the proportion of true positives among all actual positives.\n* Accuracy (Acc): This metric measures how often a model correctly predicts the outcome.\n* F1 score: This is the harmonic mean of Precision and Recall, providing a balanced assessment of a model\u2019s performance.\n* Intersection over Union (IoU): This metric estimates the similarity between two sets of samples by calculating the ratio between the area of overlap and the area of the union of the predicted bounding boxes and the ground truth bounding boxes.\n\nThe evaluation involved comparing different variants of YOLOv8 (nano, small, medium, large, extra-large) and YOLOv9 (compact, extended) models. The results were presented in tables, highlighting the best-performing models based on metrics such as AP and mAP. For instance, the YOLOv8-small model achieved an AP of 0.982 for OD detection, which is competitive with other state-of-the-art models. The YOLOv9-extended model showed promising results in detecting MAs, HEMOs, and EXs, with an mAP of 0.359.\n\nIn summary, the evaluation method was rigorous and involved a combination of data augmentation, thorough training protocols, and a comprehensive set of performance metrics to assess the models' effectiveness in detecting retinal features.",
  "evaluation/measure": "In our evaluation, we employed a comprehensive set of performance metrics to assess the effectiveness of our models. These metrics include Average Precision (AP), Mean Average Precision (mAP), Precision (P), Recall (R), Accuracy (Acc), and the F1 score. Additionally, we utilized the Intersection over Union (IoU) to estimate the similarity between predicted and ground truth bounding boxes.\n\nAP computes the area under the Precision-Recall curve, providing a single value that encapsulates the model's precision and recall performance. mAP extends this concept by averaging the AP values across multiple object classes, offering a comprehensive evaluation of the model's performance. This metric is particularly useful for comparing different models or versions of the same model on the same task, and it is commonly used in computer vision research.\n\nPrecision quantifies the proportion of true positives among all positive predictions, indicating the model's ability to avoid false positives. Recall, on the other hand, measures the proportion of true positives among all actual positives, assessing the model's capability to detect all instances of a class. Both metrics are calculated for each class by applying the relevant formulas to each image.\n\nAccuracy measures how often the model correctly predicts the outcome, providing a straightforward assessment of the model's performance. The F1 score, which is the harmonic mean of Precision and Recall, offers a balanced evaluation of the model's performance while considering both false positives and false negatives.\n\nThe IoU is used to estimate the similarity between two sets of samples by calculating the ratio between the area of overlap and the area of the union of the predicted bounding boxes and the ground truth bounding boxes. An IoU of 0.5 was selected to calculate the proposed model\u2019s performance and compare the results with other works from the literature.\n\nThese metrics are representative of the standards used in the literature for evaluating object detection models. They provide a thorough assessment of the model's performance, covering various aspects such as precision, recall, accuracy, and the balance between these factors. By using these metrics, we ensure that our evaluation is comprehensive and comparable to other studies in the field.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of our proposed models, YOLOv8 and YOLOv9, with various publicly available methods using benchmark datasets. This comparison included models such as YOLOv3, YOLOv5, DeepLab-v3+, SSD, Faster RCNN, Mask R-CNN, and others, which were evaluated on datasets like DRISHTI-GS, ORIGA, DDR, EyePacs, and Messidor. The performance metrics used for this comparison included Average Precision (AP) and mean Average Precision (mAP) for different retinal features such as Optic Disc (OD), Microaneurysms (MA), Hemorrhages (HEMO), and Exudates (EX).\n\nWe also compared our models with simpler baselines like ResNet and DenseNet, which showed that our models, particularly YOLOv8, performed competitively in OD detection. For instance, YOLOv8 achieved an AP of 0.982 for OD detection, which is second only to DenseNet (AP of 1.000) and YOLOv3 (AP of 0.990), but better than ResNet (AP of 0.956).\n\nAdditionally, we evaluated the performance of different variants of YOLOv8, including nano, small, medium, large, and extra-large, on a validation dataset. The small variant of YOLOv8 showed promising results with an mAP of 0.265 and AP values of 0.506 for MA and 0.584 for HEMO, indicating its potential for accurate and objective detection of these retinal features.\n\nOur comparison also highlighted the challenges in MA detection, where our models, YOLOv8-nano and YOLOv9-extended, achieved higher APs of 0.182 and 0.205, respectively, compared to other neural networks. This demonstrates the effectiveness of our approaches in detecting MA, even without complex image pre-processing.\n\nIn summary, our evaluation involved a thorough comparison with publicly available methods and simpler baselines on benchmark datasets, providing a robust assessment of our models' performance in retinal feature detection.",
  "evaluation/confidence": "The evaluation of our models involved the use of a fixed confidence limit of 0.25, which is in line with the default inference configurations of YOLOv8 and YOLOv9. This confidence limit is crucial as it filters out bounding boxes with low confidence scores through a Non-Maximum Suppression algorithm, which disregards detected objects with an Intersection over Union (IoU) less than the defined threshold. The impact of this confidence limit is directly reflected in the results obtained from background false positives (FPs) and background false negatives (FNs).\n\nThe performance metrics presented, such as Precision, Recall, accuracy, and F1 score, were calculated for the variants of the models with the highest mean Average Precision (mAP). These metrics provide a comprehensive evaluation of the models' performance. However, specific confidence intervals for these metrics were not explicitly detailed in the provided results. The statistical significance of the results was not explicitly discussed, but the comparison of different model variants and their performance metrics suggests a rigorous evaluation process.\n\nThe confusion matrices generated from the experiments provide a detailed breakdown of the models' performance, showing the number of true positives (TPs), false positives (FPs), and false negatives (FNs) for each class. This information is essential for understanding the models' strengths and areas for improvement. For instance, the highest incidence of background FNs was observed in microaneurysms (MAs), followed by hemorrhages (HEMOs) and optic discs (ODs). This indicates that the models struggle more with detecting MAs, which is a known challenge in medical imaging analysis.\n\nIn summary, while the performance metrics provide a clear indication of the models' capabilities, the lack of explicit confidence intervals and statistical significance tests means that further analysis would be required to claim superiority over other methods and baselines with high confidence. The results are promising, particularly for the detection of ODs and HEMOs, but there is room for improvement, especially in the detection of MAs.",
  "evaluation/availability": "Not enough information is available."
}