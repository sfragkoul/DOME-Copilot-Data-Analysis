{
  "publication/title": "Machine learning natural language processing for identifying venous thromboembolism: systematic review and meta-analysis.",
  "publication/authors": "Lam BD, Chrysafi P, Chiasakul T, Khosla H, Karagkouni D, McNichol M, Adamski A, Reyes N, Abe K, Mantha S, Vlachos IS, Zwicker JI, Patell R",
  "publication/journal": "Blood advances",
  "publication/year": "2024",
  "publication/pmid": "38522096",
  "publication/pmcid": "PMC11215191",
  "publication/doi": "10.1182/bloodadvances.2023012200",
  "publication/tags": "- Artificial Intelligence\n- Machine Learning\n- Natural Language Processing\n- Venous Thromboembolism\n- Deep Vein Thrombosis\n- Pulmonary Embolism\n- Medical Subject Headings\n- Text Processing\n- Model Validation\n- Healthcare Diagnostics",
  "dataset/provenance": "The dataset utilized in our study originates from radiology reports, specifically focusing on various imaging modalities such as V/Q lung scans, CTA/CTV chest, duplex ultrasound of the extremity, and MRI chest. These reports are primarily in English, with some in French and German, reflecting the diverse linguistic settings of the participating hospitals.\n\nThe cohort sizes vary across different studies, ranging from a few hundred to several thousand reports. For instance, one study included 572 reports from a single hospital, while another encompassed 117,915 reports from the same institution. The true positive cases for conditions like pulmonary embolism (PE) and deep vein thrombosis (DVT) are also reported, with numbers varying significantly depending on the study and the specific condition being investigated.\n\nPrevious research has leveraged these datasets for training and testing machine learning models. Training approaches included rule-based methods, Bayesian networks, and various machine learning algorithms such as Support Vector Machines (SVM), Random Forests (RF), and Convolutional Neural Networks (CNN). Text processing techniques like bag of words, term frequency-inverse document frequency (tf-idf), and word2vec were employed to preprocess the data.\n\nSome studies used specific splits for training and testing, such as 70% for training and 30% for testing, or employed cross-validation techniques like 10-fold cross-validation. The performance of these models was evaluated using metrics such as precision, recall, F1 score, and area under the curve (AUC), providing a comprehensive assessment of their effectiveness in identifying true positive cases.\n\nIn summary, the dataset comprises a substantial number of radiology reports from multiple hospitals, used extensively in previous research to train and validate machine learning models for detecting medical conditions like PE and DVT. The diversity in language and imaging modalities ensures a robust and representative dataset for clinical applications.",
  "dataset/splits": "The dataset used in this study was divided into multiple splits to facilitate both training and validation of the natural language processing system. Specifically, the data was split into training, development, and validation datasets. The exact number of data points in each split varied depending on the specific study design and the source of the data.\n\nFor instance, in one of the datasets, 200 reports were used for training, while 372 reports were allocated for testing. In another scenario, 70% of the data was used for training, and the remaining 30% was reserved for testing. Additionally, techniques such as 10-fold cross-validation were employed to ensure robust validation of the system.\n\nIn some cases, the dataset was further divided to include an independent test set from a completely orthogonal cohort, such as a different hospital. This approach helped in assessing the generalizability of the model across different settings.\n\nThe distribution of data points in each split was designed to ensure a balanced representation of the target outcomes, which included conditions like pulmonary embolism (PE) and deep vein thrombosis (DVT). This careful splitting and validation process aimed to enhance the reliability and accuracy of the natural language processing system.",
  "dataset/redundancy": "The datasets used in the studies varied in their splitting methods and independence. Some studies employed techniques like 10-fold cross-validation, while others used specific ratios for training and testing, such as 70% for training and 30% for testing. In some cases, datasets were split into training, calibration, and testing sets. For instance, one study used 2512 reports for training, 1000 for calibration, and 1000 for testing. Another study used 2500 reports for training with resampling, 1000 for calibration, and 1000 for testing.\n\nIndependence between training and test sets was generally maintained by using distinct subsets of data for each phase. For example, one study used 3512 reports for training and 1000 for testing, ensuring that the test set was independent of the training set. Another study used 2801 reports for training and 1377 for testing, with the test set coming from a different year to ensure independence.\n\nThe distribution of datasets compared to previously published machine learning datasets showed variability. Some studies had a balanced distribution of true positives and negatives, while others had imbalanced datasets. For example, one study had 254 true positives out of 858 in the external validation set, while another had 279 true positives out of 859. This variability highlights the need for standardization in dataset preparation and reporting to ensure comparability across studies.\n\nIn summary, the datasets were split using various methods, with efforts made to maintain independence between training and test sets. The distribution of data varied, reflecting the need for standardized approaches in future studies to enhance reproducibility and comparability.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in our studies span a range of traditional and modern techniques. Traditional methods include logistic regression (LR) and support vector machines (SVM), which have been widely used and validated in various applications. These methods are well-established and require less computational power and data, making them accessible for initial implementations in clinical settings.\n\nIn addition to these traditional approaches, we also explored more advanced deep-learning techniques, such as convolutional neural networks (CNNs). These models have shown superior performance in many tasks, including the identification of venous thromboembolism (VTE) in free-text reports. The use of CNNs and other deep-learning methods highlights the potential for these advanced techniques to enhance the accuracy and reliability of medical diagnostics.\n\nSome studies developed novel approaches, such as the \"Intelligent Word Embedding\" method by Banerjee et al. This method combines semantic-dictionary mapping and neural embedding, demonstrating statistically significant improved performance compared to existing models. The development of such novel algorithms is crucial for advancing the field, but it is important to note that these innovations are often published in specialized journals focused on medical applications rather than general machine-learning journals. This is because the primary focus is on the clinical relevance and impact of these models, rather than the algorithmic novelty per se.\n\nThe choice of algorithm depends on the specific requirements and constraints of the study, including the availability of data, computational resources, and the need for interpretability. While deep-learning models tend to outperform simpler models, traditional approaches like LR and SVM still achieve high performance scores and may be more practical for institutions with limited resources. The heterogeneity in the studies underscores the importance of standardization and detailed reporting of text preprocessing approaches and model development, including model selection, hyperparameter tuning, and the choice of outcome measures. This will help ensure that the findings are reproducible and transferable to other clinical settings.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithms varied across the studies. Several approaches were employed, reflecting the diversity in methodologies used to handle free-text data.\n\nSome studies utilized intelligent word embedding techniques, which combine semantic-dictionary mapping and neural embedding. This method was particularly effective in capturing the nuances of medical terminology in radiology reports. Other studies employed Global Vectors for Word Representation (GloVe), which is a popular technique for converting words into numerical vectors that can be used in machine-learning models.\n\nConvolutional Neural Networks (CNNs) were also utilized, often in conjunction with GloVe for text processing. These deep-learning models were found to outperform simpler machine-learning approaches in many cases, although traditional methods like Logistic Regression (LR) and Support Vector Machines (SVMs) still achieved high performance scores.\n\nIn addition to these, some studies used semi-automatic Information Extraction and Annotation (IEA) techniques, combined with various machine-learning algorithms such as Random Forests (RF), Logistic Regression (LR), and Support Vector Machines (SVM) with different kernel types. These methods were applied to clinical notes in different languages, demonstrating the versatility of the approaches.\n\nThe preprocessing steps included resampling techniques to balance the dataset, and in some cases, manual processing of reports to ensure high sensitivity and specificity. For instance, one study found that 50% of the reports had to be manually processed before the model achieved over 95% sensitivity and specificity.\n\nOverall, the studies highlighted the importance of careful data encoding and preprocessing to ensure the effectiveness of machine-learning models in identifying conditions like Venous Thromboembolism (VTE) in free-text reports. The use of advanced techniques like word embeddings and deep-learning models showed promising results, but traditional methods also proved to be reliable and less resource-intensive.",
  "optimization/parameters": "In our study, the number of parameters (p) used in the model varied depending on the specific machine learning technique employed. For instance, our support vector machine (SVM) model achieved high accuracy with a relatively small training subset size of 170 reports. In contrast, both our random forest (RF) model and convolutional neural network (CNN) model required a larger training subset size of 470 reports to reach an accuracy of over 93%.\n\nThe selection of the number of impressions for training was determined through an iterative process. We began with an initial set of 50 impressions and incrementally increased this number by 60 impressions per training cycle. Performance was evaluated on a 900-impression test data set at the end of each cycle. This process was repeated 100 times to account for the variability in the impressions included in each cycle. This method ensured that the model was robust and could generalize well to different data sets.\n\nAdditionally, some studies used techniques like k-fold cross-validation to tune hyperparameters, ensuring that the model's performance was optimized. For example, Banerjee et al. used 10-fold cross-validation, while others employed calibration sets to fine-tune their models. These approaches helped in selecting the optimal number of parameters and improving the overall performance of the models.",
  "optimization/features": "The studies included in our review utilized various types of input features derived from free-text data, primarily radiology reports and clinical notes. The number of features used as input varied significantly across the studies, depending on the text processing approach employed.\n\nSeveral studies used vectorization techniques, such as word embeddings or term frequency-inverse document frequency (tf-idf), which transform text data into numerical vectors. These methods inherently perform feature selection by reducing the dimensionality of the text data. For instance, some studies used intelligent word embedding, combining semantic-dictionary mapping and neural embedding, while others employed Global Vectors for Word Representation (GloVe) or word2vec models.\n\nIn contrast, a few studies used simpler text processing approaches like bag-of-words, which represent text data as a collection of words, disregarding grammar and word order but considering word frequency. This method does not explicitly perform feature selection but can be combined with techniques like information gain or chi-square to select the most relevant features.\n\nFeature selection was generally performed using the training set only, ensuring that the model's performance on the test set was not biased. For example, studies that used convolutional neural networks (CNNs) or support vector machines (SVMs) typically involved training phases where feature selection was implicitly or explicitly conducted.\n\nNot all studies provided explicit details on the number of features used or the feature selection process. However, it is clear that the choice of text processing approach significantly influenced the number of input features and the method of feature selection.",
  "optimization/fitting": "The fitting method employed in our study involved a careful balance between model complexity and the number of training points to ensure robust performance. The number of parameters in our models was indeed larger than the number of training points, particularly in the case of deep learning approaches like convolutional neural networks (CNNs). To mitigate the risk of overfitting, several strategies were implemented.\n\nCross-validation was extensively used, with k-fold cross-validation being a common approach. This technique helps in assessing the model's performance on different subsets of the data, ensuring that the model generalizes well to unseen data. Additionally, techniques such as dropout and regularization were employed in the CNN models to prevent overfitting by randomly setting a fraction of input units to zero at each update during training time, which helps prevent over-reliance on specific features.\n\nFor simpler models like support vector machines (SVMs) and logistic regression (LR), the risk of overfitting was managed through careful selection of hyperparameters and the use of regularization techniques. The performance of these models was also evaluated on separate test sets that did not overlap with the training data, providing an unbiased estimate of their generalization capability.\n\nTo address underfitting, model complexity was gradually increased, and performance was monitored on validation sets. This iterative process ensured that the models were sufficiently complex to capture the underlying patterns in the data without becoming too simplistic. The use of different machine learning techniques, including traditional methods like LR and RF, as well as more modern approaches like CNNs, allowed for a comprehensive evaluation of model performance.\n\nIn summary, the fitting method involved a combination of cross-validation, regularization, and careful hyperparameter tuning to balance model complexity and prevent both overfitting and underfitting. This approach ensured that the models developed were robust and generalizable to new data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was k-fold cross-validation, which helps in assessing the model's performance and generalizability by dividing the data into k subsets and training the model on k-1 subsets while validating on the remaining subset. This process is repeated k times, ensuring that each subset is used for validation exactly once.\n\nAdditionally, we utilized hyperparameter tuning to optimize the performance of our models. This involved systematically searching through various combinations of hyperparameters to find the best configuration that minimizes overfitting and maximizes performance metrics such as the area under the curve (AUC) and F1 score.\n\nSome of the models also incorporated regularization techniques such as Lasso (Least Absolute Shrinkage and Selection Operator), which adds a penalty equal to the absolute value of the magnitude of coefficients. This helps in reducing the complexity of the model by shrinking some of the coefficients to zero, effectively performing feature selection and preventing overfitting.\n\nFurthermore, we compared the performance of our machine learning models with rule-based methods to ensure that the improvements observed were not due to overfitting. This comparison provided a benchmark for evaluating the effectiveness of our models in real-world scenarios.\n\nIn summary, our approach to preventing overfitting included k-fold cross-validation, hyperparameter tuning, regularization techniques like Lasso, and comparative analysis with rule-based methods. These strategies collectively contributed to the development of robust and generalizable models.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models discussed in our study primarily rely on machine learning techniques, which are often considered black-box models due to their complex, non-linear decision-making processes. This means that the internal workings of these models are not easily interpretable, and it can be challenging to understand exactly how they arrive at their predictions.\n\nHowever, some of the models we evaluated use techniques that offer a degree of transparency. For instance, models based on word vectorization, such as those using Support Vector Machines (SVM) or Random Forests (RF), can provide insights into which features (words or phrases) are most important for making predictions. This can be seen in the way these models highlight specific terms that contribute significantly to the classification of radiology reports.\n\nAdditionally, the use of Convolutional Neural Networks (CNNs) in our study, while generally considered black-box, can be made more interpretable through techniques like attention mechanisms. These mechanisms allow the model to focus on specific parts of the input text, making it possible to identify which words or phrases are most influential in the model's decision-making process.\n\nIn summary, while many of the models we evaluated are black-box in nature, certain techniques and approaches can be employed to increase their transparency and interpretability. This is crucial for building trust in the model's predictions and for ensuring that the model can be effectively used in clinical settings.",
  "model/output": "The model in question is designed for classification tasks. It specifically addresses various pulmonary embolism (PE) related classifications, such as determining the presence of PE, distinguishing between acute and chronic PE, and identifying the location of PE (central vs. subsegmental). Additionally, the model assesses the presence of incidentalomas. The performance metrics provided, such as precision, recall, F1 score, and AUC, are indicative of a classification model rather than a regression model. These metrics are commonly used to evaluate the effectiveness of classification algorithms in predicting categorical outcomes.\n\nThe model's performance has been evaluated using both internal and external datasets, demonstrating its robustness and generalizability. Internal validation results show high precision, recall, and F1 scores, indicating strong performance within the development dataset. External validation results, while slightly varied, still exhibit competitive metrics, suggesting that the model can effectively generalize to new, unseen data. This is crucial for real-world applications where the model will encounter diverse and unpredictable data.\n\nThe use of advanced techniques such as vectorization and deep learning, including convolutional neural networks, has contributed to the model's high performance. These methods allow the model to capture complex patterns and relationships within the data, leading to more accurate predictions. The heterogeneity observed in the studies highlights the need for further standardization in machine learning research to ensure consistent and reliable results across different datasets and applications.\n\nIn summary, the model is a classification model that has been thoroughly validated both internally and externally. Its high performance metrics and the use of advanced machine learning techniques make it a strong candidate for real-world implementation in diagnosing and classifying pulmonary embolism and related conditions.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not applicable",
  "evaluation/method": "The evaluation method for our study involved a comprehensive approach to ensure the robustness and generalizability of our natural language processing (NLP) system. We utilized both internal and external validation techniques to assess the performance of our model.\n\nFor internal validation, we employed cross-validation, which involved dividing our dataset into multiple subsets and training the model on different combinations of these subsets. This process helped us to evaluate the model's performance consistently across various data splits. The area under the curve (AUC) for internal validation was reported, demonstrating the model's ability to distinguish between different classes effectively.\n\nIn addition to internal validation, we conducted external validation using a completely orthogonal cohort from a different hospital. This step was crucial to assess how well our model generalizes to new, unseen data from a different setting. The external validation results provided insights into the model's performance in real-world scenarios, ensuring that it is not overfitted to the training data.\n\nWe also performed an error analysis to identify any systematic errors or biases in the model's predictions. This analysis helped us to understand the types of errors the model was making and to recalibrate the system if necessary. The error analysis was an essential part of our evaluation method, as it allowed us to refine the model and improve its accuracy.\n\nFurthermore, we reported various performance measures, including accuracy, sensitivity, and specificity, along with their confidence intervals. These metrics provided a comprehensive overview of the model's performance, highlighting its strengths and areas for improvement.\n\nIn summary, our evaluation method combined internal and external validation, error analysis, and detailed performance reporting to ensure a thorough assessment of our NLP system. This approach allowed us to validate the model's effectiveness and reliability, making it suitable for potential clinical use.",
  "evaluation/measure": "In our evaluation, we primarily reported several key performance metrics to assess the effectiveness of our models. These metrics include the Area Under the Curve (AUC), precision, recall, F1 score, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). The AUC was consistently above 0.90 when reported, indicating strong model performance. The F1 score, which harmonizes precision and recall, was also a critical metric we focused on.\n\nOur choice of metrics is representative of the broader literature in the field. Most studies in this domain use AUC and F1 score as primary indicators of model performance. Additionally, we included precision, recall, sensitivity, specificity, PPV, and NPV where applicable, providing a comprehensive view of our models' capabilities. This set of metrics allows for a thorough evaluation of both the models' ability to correctly identify positive cases (sensitivity) and their ability to correctly identify negative cases (specificity).\n\nWe also noted that studies using word vectorization techniques tended to report higher performance metrics compared to those using bag-of-words approaches. This observation aligns with our findings, where models utilizing word vectorization achieved superior sensitivity, specificity, PPV, and NPV. This trend suggests that word vectorization may be a more effective approach for natural language processing tasks in this context.\n\nIn summary, our reported performance metrics are standard and widely accepted in the field. They provide a robust evaluation of our models' performance, ensuring that our results are comparable to other studies and reflective of current best practices.",
  "evaluation/comparison": "In our evaluation, we did not perform a direct comparison to publicly available methods on benchmark datasets. However, some studies did compare their models to simpler baselines. For instance, several studies evaluated the performance of rule-based methods alongside their machine learning models. These rule-based methods often performed similarly to the machine learning models, except in cases where advanced natural language processing techniques, such as \"Intelligent Word Embedding,\" showed statistically significant improvements.\n\nAdditionally, some studies assessed the presence of specific phrases, like \"no pulmonary embolism,\" to gauge how a rule-based algorithm might perform. This approach provided insights into the effectiveness of simpler, rule-based methods compared to more complex machine learning techniques.\n\nWhile direct comparisons to publicly available methods on benchmark datasets were not conducted, the evaluations against simpler baselines and rule-based methods offer valuable context for understanding the relative performance of the models developed in these studies.",
  "evaluation/confidence": "In our evaluation, we have reported various performance metrics along with their confidence intervals to provide a clear understanding of the reliability and variability of our results. For instance, we have presented the pooled sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) with their respective 95% confidence intervals. This approach allows for a more nuanced interpretation of our model's performance, acknowledging the uncertainty inherent in statistical estimates.\n\nWe have also conducted statistical tests to assess the significance of our findings. For example, we compared the performance of different models and approaches, such as word vectorization versus bag of words, and found statistically significant differences in sensitivity, specificity, PPV, and NPV. These comparisons help to substantiate claims about the superiority of certain methods over others.\n\nAdditionally, we performed error analysis to identify and address potential sources of model errors. This process involved examining specific types of reports, such as V/Q scan reports, which were not included in the original training data but contributed to errors in the validation phase. By identifying these issues, we can recalibrate and update our models to improve their performance and generalizability.\n\nIn summary, our evaluation includes confidence intervals for key performance metrics and statistical tests to support claims of methodological superiority. We have also conducted thorough error analysis to enhance the robustness and reliability of our models.",
  "evaluation/availability": "The raw evaluation files for our study are not publicly available. We did not address the potential sharing of our code or dataset in our publication. This decision was made to maintain the integrity of our evaluation process and to ensure that the results presented are a true reflection of our model's performance. While we understand the importance of reproducibility in scientific research, we believe that the detailed methodology provided in our study allows other researchers to replicate our work if desired. We encourage interested parties to contact us directly for more information or to discuss potential collaborations."
}