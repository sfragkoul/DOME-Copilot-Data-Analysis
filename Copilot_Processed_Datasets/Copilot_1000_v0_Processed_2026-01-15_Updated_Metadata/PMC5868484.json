{
  "publication/title": "Applying Machine Learning to Workers' Compensation Data to Identify Industry-Specific Ergonomic and Safety Prevention Priorities: Ohio, 2001 to 2011.",
  "publication/authors": "Meyers AR, Al-Tarawneh IS, Wurzelbacher SJ, Bushnell PT, Lampl MP, Bell JL, Bertke SJ, Robins DC, Tseng CY, Wei C, Raudabaugh JA, Schnorr TM",
  "publication/journal": "Journal of occupational and environmental medicine",
  "publication/year": "2018",
  "publication/pmid": "28953071",
  "publication/pmcid": "PMC5868484",
  "publication/doi": "10.1097/jom.0000000000001162",
  "publication/tags": "- Machine Learning\n- Workers' Compensation\n- Occupational Safety\n- Ergonomics\n- Injury Prevention\n- Data Analysis\n- Bayesian Techniques\n- Workplace Injuries\n- Industry-Specific Hazards\n- Surveillance Systems",
  "dataset/provenance": "The dataset utilized in this study originates from the Ohio Bureau of Workers\u2019 Compensation (OHBWC) workers' compensation data, covering the period from 2001 to 2011. This dataset is particularly valuable because it includes detailed information on all insured employers and employees in Ohio, providing a comprehensive view of occupational injuries within the state. The OHBWC insures approximately two-thirds of Ohio workers, focusing on small and medium-sized employers, which makes the data highly representative of the state's workforce.\n\nThe dataset comprises over one million claims, which were analyzed to identify industry-specific ergonomic and safety prevention priorities. This extensive dataset allowed for a robust analysis of injury trends and the development of targeted prevention strategies. The data includes unstructured narrative text from first reports of injury, which was processed using an auto-coding program to categorize and analyze the information effectively.\n\nPrevious studies and public health practitioners have successfully used similar datasets and methodologies. For instance, the SAS auto-coding program and word probabilities from unstructured narrative text have been applied to first reports of injury data, demonstrating their utility in identifying injury patterns and prevention needs. The consistency of the findings with national data from the Bureau of Labor Statistics (BLS) and other state-specific data further validates the reliability and relevance of the OHBWC dataset.",
  "dataset/splits": "The dataset was split into multiple parts for training and testing purposes. Initially, 9600 claims were randomly selected, with 800 claims chosen per month, evenly distributed between lost-time and medical-only claims across all years. This set was used to train and test the auto-coder. The process involved splitting these 9600 claims into a training set of 8600 and a prediction set of 1000 claims. This splitting process was repeated 25 times to ensure robustness. After testing, the entire set of 9600 manually coded claims was used as the training set to auto-code over 1.2 million other claims in the database. Additionally, claims with rare or under-represented diagnosis categories were manually coded to improve the auto-coder's accuracy. The distribution of data points in each split was designed to ensure a balanced representation of different types of claims, with an emphasis on both lost-time and medical-only claims.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not applicable.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is Bayesian machine learning. This technique was employed to calculate the probability that a given claim belongs to each intervention category by considering words from the unstructured accident narrative and words from the diagnosis category descriptions. The probabilities were based on word frequencies associated with each intervention category in the training set.\n\nThe algorithm used is not entirely new, as Bayesian machine learning techniques have been previously applied in various fields, including injury surveillance studies. However, the specific application and modifications made to the auto-coding program for this study are unique. The program was tailored to improve manual coding and auto-coding efficiency and accuracy across all NORA industry sectors and all diagnosis categories.\n\nThe reason the algorithm was not published in a machine-learning journal is likely because the focus of this study was on applying machine learning to occupational injury surveillance, rather than developing a new machine-learning algorithm. The primary goal was to leverage existing machine learning techniques to automatically code intervention categories for a large dataset of workers' compensation claims, thereby enhancing the efficiency and accuracy of injury surveillance systems. The study's contributions lie in the practical application of machine learning to a specific domain, rather than the innovation of a new algorithm.",
  "optimization/meta": "The model described in the publication does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The auto-coding program employed a Bayesian machine learning technique to calculate the probability that a given claim belongs to each intervention category. This was achieved by considering words from the unstructured accident narrative and words from the diagnosis category descriptions. The probabilities were based on word frequencies associated with each intervention category in the training set.\n\nThe training and testing process involved using 8600 manually coded claims to train the auto-coder and 1000 randomly selected claims to test its accuracy. This process was repeated 25 times to ensure robustness. The training data was independent, as it consisted of manually coded claims that were not used in the testing phase. This independence is crucial for evaluating the model's performance accurately.\n\nThe model's performance was evaluated by comparing the auto-coded intervention categories against the manually coded, \"gold standard,\" category values. The overall percent agreement, and intervention category specific sensitivities and positive predictive values were averaged across the 25 iterations. This approach ensured that the model's predictions were reliable and consistent.",
  "optimization/encoding": "The data encoding process involved utilizing unstructured accident narratives and diagnosis category descriptions to train a Bayesian machine learning model. The model calculated the probability of a claim belonging to each intervention category by analyzing word frequencies associated with each category in the training set. This approach allowed the model to consider the context and specific words used in the narratives to make accurate predictions.\n\nTo enhance the accuracy and efficiency of the auto-coding process, several modifications were implemented. Rule-based techniques were applied to reduce misclassifications by excluding diagnosis categories that were not part of the ERGO case definition. For instance, if the Optimal RTW diagnosis category was excluded from the ERGO case definition, the claim was reclassified as either OTH or STF based on the highest probability score. This step ensured that the model's predictions were more aligned with the predefined case definitions.\n\nThe training set consisted of 8600 manually coded claims, while 1000 randomly selected claims were used for testing the model's accuracy. This process was repeated 25 times to ensure robustness, and the overall percent agreement, as well as intervention category-specific sensitivities and positive predictive values (PPVs), were averaged across these iterations.\n\nAdditionally, the auto-coder was trained to handle rare or under-represented diagnosis categories by overriding auto-coded intervention categories for manually coded claims in these cases. This step was crucial for maintaining the model's accuracy, especially for categories that were challenging to auto-code due to their rarity or complexity.\n\nOverall, the data encoding process involved a combination of manual coding, rule-based techniques, and machine learning to ensure that the auto-coder could accurately classify claims into the appropriate intervention categories. This approach not only improved the efficiency of the coding process but also ensured that the results were reliable and consistent.",
  "optimization/parameters": "Not applicable",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The fitting method employed in this study utilized a Bayesian machine learning technique to calculate the probability that a given claim belongs to each intervention category. This approach considered words from unstructured accident narratives and words from the diagnosis category descriptions, basing probabilities on word frequencies associated with each intervention category in the training set.\n\nThe training set consisted of 8600 manually coded claims, which were used to train the auto-coder. To ensure the robustness of the model, the process of randomly splitting the 9600 claims into a training set of 8600 and a prediction set of 1000 claims was repeated 25 times. The overall percent agreement, and intervention category specific sensitivities and positive predictive values (PPVs) were averaged across these iterations. This repetitive process helped in mitigating overfitting by ensuring that the model generalizes well to unseen data.\n\nTo address the potential issue of overfitting, where the model might perform well on training data but poorly on new data, several measures were taken. Firstly, the use of a large and diverse training set of 8600 claims helped in capturing a wide range of scenarios. Secondly, the model's performance was validated using a separate test set of 1000 claims, which was not used during training. This validation step ensured that the model's predictions were reliable and not merely memorizing the training data.\n\nUnderfitting, where the model is too simple to capture the underlying patterns in the data, was addressed by the complexity of the Bayesian machine learning technique. This method is capable of handling a large number of parameters by leveraging the probabilistic framework to make informed predictions. Additionally, the model was refined through iterative testing and manual coding adjustments, ensuring that it could accurately classify claims into the appropriate intervention categories.\n\nIn summary, the fitting method involved a robust Bayesian machine learning approach, extensive training and validation processes, and iterative refinements to balance the trade-off between overfitting and underfitting. This ensured that the model was both complex enough to capture the necessary patterns and generalizable to new data.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model employed in this study is not a black box. It utilizes a Bayesian machine learning technique, which inherently provides a level of transparency. This approach calculates the probability that a given claim belongs to each intervention category by considering the frequencies of specific words from the unstructured accident narratives and the diagnosis category descriptions. The probabilities are derived from the word frequencies associated with each intervention category in the training set.\n\nFor instance, if a claim narrative contains words frequently associated with ergonomic issues, the model will assign a higher probability to the ergonomic intervention category. Similarly, if the narrative includes terms commonly linked to slips, trips, or falls, the model will increase the probability for the slip/trip/fall category. This word-frequency-based approach allows for a clear understanding of how the model makes its predictions, making it more interpretable than many other machine learning models.\n\nAdditionally, the model's performance was evaluated using a large dataset of manually coded claims, ensuring that the auto-coder's predictions align closely with human expertise. This manual coding process involved consensus coding by at least two coders, further enhancing the transparency and reliability of the model's outputs. The use of rule-based techniques for auto-coding also helps in reducing misclassifications by excluding diagnosis categories that do not fit the case definitions, providing another layer of interpretability.",
  "model/output": "The model developed for this study is a classification model. It utilizes a Bayesian machine learning technique to categorize workers' compensation claims into specific intervention categories. The model calculates the probability that a given claim belongs to each intervention category by analyzing words from the unstructured accident narratives and words from the diagnosis category descriptions. These probabilities are based on word frequencies associated with each intervention category in the training set. The model was trained using 8,600 manually coded claims and tested on 1,000 randomly selected claims to evaluate its accuracy. The process of splitting the claims into training and testing sets was repeated 25 times to ensure robustness. The final model was then used to auto-code over 1.2 million claims in the database, significantly reducing the time required for manual coding. The output of the model provides intervention categories for each claim, which can be used for further analysis and surveillance of occupational injuries.",
  "model/duration": "The auto-coding process for over 1.2 million claims was completed in less than 3 hours. This efficiency was achieved after the model was trained using 9600 manually coded claims. The manual coding rate was approximately 2.2 claims per minute, which would have required about 4.5 person-years to code the same number of claims manually. The significant reduction in time highlights the effectiveness of the auto-coding program in handling large datasets efficiently.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method for the auto-coding program involved a rigorous process to ensure its accuracy and reliability. A total of 9600 manually coded claims were used, with 8600 claims designated for training the auto-coder and 1000 claims reserved for testing its accuracy. The testing process was repeated 25 times, each time randomly splitting the 9600 claims into a training set of 8600 and a prediction set of 1000 claims. This approach allowed for the calculation of overall percent agreement and intervention category-specific sensitivities and positive predictive values (PPVs), which were then averaged across the 25 iterations.\n\nThe auto-coder's performance was assessed by comparing its predictions against the manually coded, \"gold standard,\" category values. The results showed that the auto-coding program correctly predicted 90% of the claims, which is slightly lower than the 93.8% agreement between manual coders after consensus coding. Sensitivity was consistent across the three intervention categories (ERGO, STF, OTH), but PPVs varied more significantly among them.\n\nAdditionally, the auto-coder's performance was enhanced by incorporating rule-based techniques to reduce misclassifications. For instance, when the Optimal RTW diagnosis category was excluded from the ERGO case definition, the ERGO probability was set to zero, and the claim was classified based on the highest probability score for either OTH or STF. This adjustment was necessary for a small fraction of claims but ensured more accurate categorization.\n\nAfter the initial testing, the auto-coder was used to process over 1.2 million claims in the claims database. The auto-coded intervention categories were overridden for manually coded claims with rare or under-represented diagnosis categories to maintain high accuracy. This step was crucial for ensuring that the auto-coder's predictions were reliable even for less common diagnoses.\n\nThe evaluation method demonstrated the effectiveness of the auto-coding program in handling large datasets efficiently while maintaining a high level of accuracy. The use of cross-validation and the comparison with manually coded claims provided a robust assessment of the auto-coder's performance, ensuring its reliability for real-world applications.",
  "evaluation/measure": "The performance of the auto-coding program was evaluated using several key metrics. The overall percent agreement between the auto-coder and manual coders was reported, showing that the auto-coding program correctly predicted 90% of the claims. This is compared to a 93.8% agreement rate between manual coders after consensus coding, indicating that the auto-coder's performance is quite close to that of human coders.\n\nSensitivity, which measures the ability of the auto-coder to correctly identify true positive cases, was found to be essentially the same across all three intervention categories: ergonomic (ERGO), soft tissue (STF), and other (OTH), with values around 89%. This consistency suggests that the auto-coder performs well in identifying claims across different types of interventions.\n\nPositive predictive values (PPVs), which indicate the proportion of positive identifications that are true positives, varied more among the intervention categories. The PPV for the ERGO category was notably higher than for the STF category, by approximately 9%. This variation in PPV highlights differences in the auto-coder's accuracy depending on the intervention type, with ERGO claims being more reliably identified.\n\nThe auto-coder's performance was also assessed using a Bayesian machine learning technique, which considered word frequencies from unstructured accident narratives and diagnosis category descriptions. This method allowed for the calculation of probabilities that a given claim belongs to each intervention category, enhancing the accuracy of the auto-coding process.\n\nAdditionally, the auto-coder's performance was tested using a large dataset of manually coded claims, with the process repeated multiple times to ensure robustness. The results were averaged across these iterations to provide a reliable estimate of the auto-coder's accuracy.\n\nOverall, the reported metrics provide a comprehensive evaluation of the auto-coder's performance, demonstrating its effectiveness in accurately classifying claims by intervention category. The use of sensitivity and PPV, along with overall agreement rates, aligns with standard practices in the literature for evaluating the performance of automated coding systems.",
  "evaluation/comparison": "A comparison to publicly available methods was not explicitly mentioned. However, the auto-coding program's performance was evaluated against manually coded claims, which can be considered a form of benchmarking. The auto-coding program achieved a 90% correct prediction rate, compared to a 93.8% agreement rate between manual coders after consensus coding. This indicates that the auto-coding program performed nearly as well as human coders.\n\nRegarding simpler baselines, the auto-coding program used a Bayesian machine learning technique, which is a more complex method compared to simpler baselines like rule-based systems or keyword matching. The use of such an advanced technique suggests that the focus was on achieving high accuracy rather than comparing against simpler baselines. The program's modifications, such as applying case definitions using rule-based techniques, further improved its accuracy and efficiency.\n\nThe evaluation process involved training the auto-coder with 8600 manually coded claims and testing it with 1000 randomly selected claims. This process was repeated 25 times to ensure the robustness of the results. The overall percent agreement, as well as intervention category-specific sensitivities and positive predictive values, were averaged across these iterations. This rigorous evaluation process demonstrates the reliability and validity of the auto-coding program.\n\nIn summary, while a direct comparison to publicly available methods or simpler baselines was not performed, the auto-coding program's performance was thoroughly evaluated against manually coded claims. The use of advanced machine learning techniques and rigorous evaluation processes ensures the program's accuracy and reliability.",
  "evaluation/confidence": "Evaluation Confidence\n\nThe evaluation of the auto-coding program's performance was conducted using a robust methodology that included confidence intervals for the metrics. The process involved randomly splitting a dataset of 9600 claims into a training set of 8600 and a prediction set of 1000 claims, repeated 25 times. This approach allowed for the calculation of average percent agreement and intervention category-specific sensitivities and positive predictive values (PPVs) across the iterations, providing a measure of variability and confidence in the results.\n\nThe auto-coding program demonstrated high accuracy, with an overall percent agreement of 90% compared to manual coding. Sensitivity was consistent across intervention categories, around 89.6%, indicating the program's reliability in identifying true positives. However, PPVs varied more, with the ergonomic (ERGO) category showing a 9% higher PPV than the slip/trip/fall (STF) category. This variability suggests that while the program is generally accurate, there are differences in its performance across categories.\n\nStatistical significance was not explicitly mentioned for the comparison of the auto-coding program to manual coding or other baselines. However, the high agreement rates and consistent sensitivity across categories suggest that the method is superior to random guessing and likely to be statistically significant. The use of repeated measures and averaging across iterations further supports the reliability of the findings.\n\nIn summary, the evaluation provides confidence in the auto-coding program's performance, with metrics that include measures of variability and consistency. The results indicate that the method is accurate and reliable for auto-coding workers' compensation claims by intervention category.",
  "evaluation/availability": "The raw evaluation files used in this study are not publicly available. The study utilized a proprietary, private, administrative database designed for claims management, which is maintained by the Ohio Bureau of Workers\u2019 Compensation (OHBWC). This database contains sensitive information related to workers' compensation claims and is not released to the public due to privacy and confidentiality concerns.\n\nThe data linkage process involved combining records from OHBWC-insured, private employers with information from the Ohio unemployment insurance (UI) agency. This integration allowed for the calculation of claim rates by the number of employees or estimated full-time equivalents (FTEs) by industry codes. However, the specific datasets and evaluation files generated during this process are not accessible to the general public.\n\nFor researchers or practitioners interested in similar analyses, it is recommended to contact the OHBWC or relevant regulatory bodies to inquire about data access protocols and potential collaborations. Additionally, the methods and techniques described in the study can be replicated using other datasets that adhere to similar privacy and confidentiality standards."
}