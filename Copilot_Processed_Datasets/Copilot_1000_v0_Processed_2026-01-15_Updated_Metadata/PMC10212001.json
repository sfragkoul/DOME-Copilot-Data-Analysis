{
  "publication/title": "Using machine learning to predict antibody response to SARS-CoV-2 vaccination in solid organ transplant recipients: the multicentre ORCHESTRA cohort.",
  "publication/authors": "Giannella M, Huth M, Righi E, Hasenauer J, Marconi L, Konnova A, Gupta A, Hotterbeekx A, Berkell M, Palacios-Baena ZR, Morelli MC, Tam\u00e8 M, Busutti M, Potena L, Salvaterra E, , Feltrin G, Gerosa G, Furian L, Burra P, Piano S, Cillo U, Cananzi M, Loy M, Zaza G, Onorati F, Carraro A, Gastaldon F, Nordio M, Kumar-Singh S, Ba\u00f1o JR, Lazzarotto T, Viale P, Tacconelli E",
  "publication/journal": "Clinical microbiology and infection : the official publication of the European Society of Clinical Microbiology and Infectious Diseases",
  "publication/year": "2023",
  "publication/pmid": "37150358",
  "publication/pmcid": "PMC10212001",
  "publication/doi": "10.1016/j.cmi.2023.04.027",
  "publication/tags": "- Machine Learning\n- Antibody Response\n- SARS-CoV-2 Vaccination\n- Solid Organ Transplant Recipients\n- Predictive Modeling\n- Clinical Covariates\n- Logistic Regression\n- Gradient Boosting Machine\n- Data Analysis Workflow\n- Immunosuppressive Drugs",
  "dataset/provenance": "The dataset for this study was sourced from clinical charts and hospital electronic records of solid organ transplant (SOT) recipients who underwent SARS-CoV-2 vaccination. The cohort was recruited from six hospitals: five in Italy (Bologna, Verona, Padova, Vicenza, and Treviso) and one in Seville, Spain. Participants were enrolled between March 1, 2021, and December 31, 2021, and followed up until January 31, 2022. The database was locked on March 1, 2022, after a thorough review to ensure data integrity.\n\nThe data were collected anonymously and managed using the REDCap electronic data capture tools, hosted by the Interuniversity Consortium CINECA. This platform facilitated the secure and efficient handling of the data, ensuring compliance with privacy standards.\n\nThe specific number of data points is not explicitly stated, but the study involved a multicentre cohort, indicating a substantial number of participants. Previous research, such as the work by Alejo et al., has utilized similar clinical factors, including age, type of transplant, time from transplant to first dosage, types of immunosuppressive drugs, type of mRNA vaccine received, and graft failure. These factors were also considered in our models, aligning with community standards for predicting antibody responses in SOT recipients.",
  "dataset/splits": "The dataset was split into training and testing sets. The model was trained using data from four centers and tested on an unseen validation test center. The specific number of data points in each split is not detailed, but the training set consisted of data from multiple centers, while the testing set was from a single, separate center. This approach ensures that the model's performance is evaluated on data it has not seen during training, providing a robust assessment of its generalization capabilities. The distribution of data points across the splits is not explicitly described, but the use of multiple centers for training suggests a diverse and representative sample.",
  "dataset/redundancy": "The datasets were split into training and testing sets to evaluate the performance of the machine learning models. The training set consisted of data from four centers, while the test set was an unseen validation set from a different center. This approach ensured that the training and test sets were independent, helping to assess the generalizability of the models.\n\nTo enforce independence, we used data from distinct centers for training and testing. This method is crucial for validating the models' performance on new, unseen data, which is essential for real-world applications. The distribution of the datasets was designed to reflect the variability in clinical settings, similar to previously published machine learning datasets in the field.\n\nThe use of bootstrap sampling, with 200 samples, further enhanced the robustness of our analysis by providing means and confidence intervals for key prediction performance measures. This statistical technique helps in understanding the variability and reliability of the models' predictions.",
  "dataset/availability": "The data used in this study is not publicly available. The study involved a cohort of solid organ transplant recipients undergoing SARS-CoV-2 vaccination, enrolled across six hospitals in Italy and Spain between March 2021 and December 2021. Data sources included clinical charts and hospital electronic records, which were gathered anonymously and managed using the REDCap electronic data capture tools hosted at the Interuniversity Consortium CINECA. The database was locked on March 1, 2022, after a careful revision for incongruent or missing data. The data was used to build descriptive statistics and to determine the covariate structure for the prediction of the antibody response. Various machine learning algorithms were employed to assess the effects of covariates on the antibody response and to predict the antibody response. The study involved training the model with data from four centers and testing it on an unseen validation test center, with 200 bootstrap samples run to obtain means and confidence intervals for key prediction performance measures.",
  "optimization/algorithm": "The optimization algorithm employed in this study leverages various machine learning techniques to evaluate predictive performance. The algorithms used are well-established in the field of machine learning, focusing on classification tasks. These include methods that are commonly utilized for their robustness and effectiveness in handling complex datasets.\n\nThe machine-learning algorithms used are not new; they are standard approaches that have been extensively studied and applied in numerous research and practical applications. These algorithms have been chosen for their proven ability to handle the specific challenges presented by the data, particularly in the context of antibody response prediction.\n\nGiven that the algorithms are not novel, the decision to publish in a scientific journal rather than a machine-learning-specific journal is driven by the focus of the research. The primary objective of this study is to apply these algorithms to a specific biological problem, namely the prediction of antibody responses. The findings and methodologies are thus more aligned with the interests and readership of scientific journals that focus on immunology, biology, or related fields. This ensures that the research reaches the audience most likely to benefit from and build upon these results.",
  "optimization/meta": "The models developed in this study do not use data from other machine-learning algorithms as input. Instead, they rely on easy-to-obtain clinical covariates such as age, type of transplant, time from transplant to first dosage, types of immunosuppressive drugs, type of mRNA vaccine received, and graft failure.\n\nThe machine-learning methods used include logistic regression (LR), k-nearest neighbors (KNN), ordinal logistic regression (OLR), and tree-based methods such as the Gradient Boosting Machine (GBM) and Bagged Trees (BT). These methods were trained and tested on different datasets to assess their predictive power for antibody response (AbR) after a booster dose.\n\nThe training data for these models was obtained from four centers, while the testing was conducted on an unseen validation test center. This approach helps ensure that the models are evaluated on independent data, reducing the risk of overfitting and improving the generalizability of the results. The use of 200 bootstrap samples for different machine-learning algorithms further enhances the robustness of the key prediction performance measures.",
  "optimization/encoding": "In our study, the data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms. Initially, we constructed descriptive statistics to understand the covariate structure, which was essential for predicting the antibody response and examining the effects of covariates on this response.\n\nTo assess these effects, we employed an ordinal logistic regression. For predicting the antibody response, we extended the covariates by including their pairwise interaction terms. This approach allowed the model to capture more complex relationships between the variables.\n\nThe dataset was split into training and testing sets, with the model being trained on data from four centers and tested on an unseen validation center. This strategy helped in evaluating the model's generalizability and performance on new, unseen data.\n\nWe conducted 200 bootstrap samples using various machine-learning algorithms. This process enabled us to obtain robust estimates of the key prediction performance measures, including means and confidence intervals. These performance measures were crucial for evaluating the reliability and accuracy of our models.\n\nIn summary, the data encoding involved creating descriptive statistics, using ordinal logistic regression, and extending covariates with pairwise interaction terms. The preprocessing included splitting the data into training and testing sets and performing bootstrap sampling to ensure the robustness of our predictions.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the specific machine learning algorithm employed. We initially considered a set of clinical covariates that were easy to obtain, such as age, type of transplant, time from transplant to first dosage, types of immunosuppressive drugs, type of mRNA vaccine received, and graft failure. These covariates were used to build descriptive statistics and determine the covariate structure for predicting the antibody response.\n\nTo predict the antibody response, we extended the covariates by their pairwise interaction terms. This extension was done to capture more complex relationships between the variables. The final set of parameters included both the original covariates and their interaction terms.\n\nThe selection of parameters was guided by the goal of developing a prediction model based on clinically relevant and easily obtainable data. We aimed to include variables that could provide meaningful insights into the antibody response without relying on extensive or invasive measurements. The choice of covariates was informed by prior research and clinical expertise, ensuring that the model was practical and applicable in real-world settings.\n\nWe trained the model using data from four centers and tested it on an unseen validation test center. This approach helped us assess the generalizability of the model and ensure that it performed well across different populations. Additionally, we ran 200 bootstrap samples of different machine learning algorithms to obtain means and confidence intervals for key prediction performance measures, further validating the robustness of our parameter selection process.",
  "optimization/features": "The input features used in our study were clinical covariates that are easily obtainable. These features included age, type of transplant, time from transplant to the first dosage, types of immunosuppressive drugs, type of mRNA vaccine received, and graft failure. We did not perform explicit feature selection beyond choosing these clinically relevant variables. The selection of these features was based on domain knowledge and prior research, ensuring that they were relevant to the prediction of antibody response. The training of our models was conducted using these selected features, and the performance was evaluated on an unseen validation test center to ensure generalizability.",
  "optimization/fitting": "The fitting method employed in our study involved training various machine learning models using a dataset of solid organ transplant (SOT) recipients to predict antibody response (AbR) to SARS-CoV-2 vaccination. The models included logistic regression (LR), k-nearest neighbors (KNN), ordinal logistic regression (OLR), and tree-based methods such as gradient boosting machines (GBM).\n\nThe number of parameters in some models, particularly the tree-based methods, could indeed be larger than the number of training points, which might lead to overfitting. To mitigate this, we employed cross-validation techniques to optimize the hyperparameters of these models. This process helped in selecting the best parameters that generalize well to unseen data, rather than just fitting the training data. Additionally, we evaluated the models on a separate validation cohort to assess their generalization performance. The results indicated that tree-based methods like GBM showed signs of overfitting, as evidenced by their high performance on the training set but poorer performance on the validation set.\n\nTo rule out underfitting, we ensured that the models were complex enough to capture the underlying patterns in the data. For instance, logistic regression and ordinal logistic regression were chosen for their ability to handle binary and ordinal outcomes, respectively. The inclusion of pairwise interaction terms in the covariates further enhanced the models' capacity to capture complex relationships. The performance metrics, such as balanced accuracy (BA) and the area under the precision-recall curve (PRAUC), were used to evaluate the models' predictive power. The models demonstrated moderate to good performance, indicating that they were neither too simple nor too complex for the data at hand.\n\nIn summary, the fitting method involved a balance between model complexity and generalization performance. Cross-validation and evaluation on a separate validation cohort were crucial in ensuring that the models did not overfit or underfit the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One of the key methods used was cross-validation, specifically through the optimization of hyperparameters. This process involved running 200 bootstrap samples of different machine learning algorithms to obtain means and confidence intervals for key prediction performance measures. This approach helped in assessing the models' performance more reliably and in reducing the risk of overfitting to the training data.\n\nAdditionally, we compared various models, including logistic regression (LR), k-nearest neighbors (KNN), and tree-based methods like Gradient Boosting Machines (GBM). The performance of these models was evaluated using metrics such as balanced accuracy (BA) and the area under the precision-recall curve (PRAUC). We found that tree-based methods, such as GBM, tended to overfit the training data, as indicated by their high performance on the training set but poorer generalization to the validation cohort. In contrast, models like LR did not exhibit such overfitting, suggesting a more balanced performance across different datasets.\n\nFurthermore, we extended the covariates by their pairwise interaction terms to enhance the predictive power of our models. This technique allowed us to capture more complex relationships in the data, potentially improving the models' ability to generalize to unseen data. Overall, these regularization methods helped in developing more reliable and generalizable prediction models for antibody response to SARS-CoV-2 vaccination in solid organ transplant recipients.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models we employed in our study span a range of interpretability levels. Logistic regression (LR) and ordinal logistic regression (OLR) are inherently more transparent and interpretable. These models provide clear parameter estimates that indicate the direction and strength of the relationship between each predictor variable and the outcome. For instance, in our LR model, coefficients for variables like age, type of transplant, and time from transplant to first dosage offer direct insights into how these factors influence antibody response (AbR).\n\nIn contrast, tree-based methods such as Gradient Boosting Machines (GBM) and Bagged Trees are considered black-box models. These models are more complex and do not provide straightforward interpretations of individual predictors. Instead, they rely on ensembles of decision trees, making it challenging to trace the exact contribution of each variable to the final prediction. However, techniques like feature importance scores can be used to gain some insights into which variables are most influential in these models.\n\nOverall, while LR and OLR offer clear, interpretable results, tree-based methods provide robust predictive performance but at the cost of interpretability.",
  "model/output": "The model developed in our study is a classification model. It is designed to predict the antibody response, specifically to classify individuals as having a positive or negative antibody response based on certain thresholds. The model uses various machine learning algorithms to make these predictions, and its performance is evaluated using metrics such as accuracy, sensitivity, and the area under the receiver operator curve (AUROC). These metrics are computed for out-of-sample predictions, ensuring that the model's performance is assessed on unseen data. The model incorporates covariates and their pairwise interaction terms to enhance predictive accuracy. Different algorithms were tested, and their performance was averaged over 200 bootstrap samples to obtain reliable means and confidence intervals for key prediction performance measures. This approach helps in understanding the model's ability to generalize to new, unseen data.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our evaluation, we employed a robust methodology to ensure the reliability and generalizability of our models. We initially built descriptive statistics to determine the covariate structure for predicting antibody responses and examining the effects of these covariates. To assess these effects, we utilized ordinal logistic regression.\n\nFor predicting antibody responses, we extended the covariates by including their pairwise interaction terms. We trained our models using data from four centers and tested them on an unseen validation test center to evaluate their performance on new, unseen data.\n\nTo obtain comprehensive performance metrics, we ran 200 bootstrap samples using various machine learning algorithms. This approach allowed us to calculate means and confidence intervals for key prediction performance measures, providing a detailed understanding of our models' accuracy and reliability.\n\nAdditionally, we evaluated the specificity of our models' predictions, both in-sample and out-of-sample, averaged over 200 runs. This involved computing the true negative rate at different antibody level thresholds, which helped us understand how well our models could correctly identify individuals without a positive antibody response.",
  "evaluation/measure": "In our evaluation, we assessed the predictive power of the machine learning models using several performance metrics. We primarily focused on balanced accuracy (BA) and the area under the precision-recall curve (PRAUC) as our main decision criteria. These metrics were chosen because they provide a comprehensive view of the models' performance, especially in imbalanced datasets, which is common in medical research.\n\nBalanced accuracy is particularly useful because it takes into account both sensitivity and specificity, ensuring that the model performs well across all classes. This is crucial in our context, where predicting both positive and negative antibody responses is important.\n\nIn addition to BA and PRAUC, we also reported other evaluation measures such as accuracy, sensitivity, and specificity. These metrics were depicted in supplementary figures, providing a detailed view of the models' performance across different thresholds.\n\nTo ensure our results are comparable with prior research, we also reported the area under the receiver operating characteristic curve (AUROC). This metric is widely used in the literature and allows for a direct comparison with other studies. The AUROC values for our top-performing models were competitive with those reported in similar studies, indicating that our models are robust and reliable.\n\nOverall, the set of metrics we reported is representative and aligns with the standards in the field. This comprehensive evaluation ensures that our findings are both valid and comparable to other research in this area.",
  "evaluation/comparison": "In our study, we compared various machine learning models to predict antibody response to SARS-CoV-2 vaccination in solid organ transplant recipients. We evaluated several algorithms, including logistic regression (LR), k-nearest neighbors (KNN), ordinal logistic regression (OLR), and tree-based methods such as Gradient Boosting Machines (GBM) and Bagged Trees (BT).\n\nWe assessed the performance of these models using metrics like balanced accuracy (BA) and the area under the precision-recall curve (PRAUC). For instance, at an antibody response (AbR) threshold of 45, LR showed a BA of 0.66 and a PRAUC of 0.37, while KNN had a BA of 0.65 and a PRAUC of 0.36. OLR performed slightly lower with a BA of 0.63 and a PRAUC of 0.34. Tree-based methods like GBM and BT exhibited different strengths, performing well in predicting positive AbR but less effectively in predicting negative AbR.\n\nWe also compared our findings with prior research, specifically the work by Alejo et al., who used a GBM to predict antibody responses. Their model showed good performance in the training set but moderate performance in the external test set. In contrast, our LR model was most accurate in predicting negative AbR, while tree-based models tended to overfit the training data, leading to poor generalization.\n\nAdditionally, we ran 200 bootstrap samples of different machine learning algorithms to obtain means and confidence intervals for key prediction performance measures. This approach helped us to ensure the robustness of our comparisons and to identify the most reliable models for predicting antibody responses.\n\nWe did not perform a direct comparison to publicly available benchmark datasets, as our focus was on the specific context of solid organ transplant recipients and the clinical covariates relevant to this population. However, our evaluation included a range of models, from simpler baselines like LR to more complex algorithms like GBM, providing a comprehensive assessment of their predictive capabilities.",
  "evaluation/confidence": "In the evaluation of our machine learning algorithms, we have taken steps to ensure the reliability and statistical significance of our results. For each performance metric presented, we have included 95% confidence intervals. These intervals provide a range within which the true performance metric is expected to lie, with 95% confidence. This approach allows us to assess the precision of our estimates and to understand the variability in the performance of the algorithms across different runs.\n\nThe confidence intervals are particularly important when comparing the performance of different algorithms. By examining whether the intervals overlap, we can determine the statistical significance of the differences observed. If the intervals do not overlap, it suggests that the differences in performance are likely to be statistically significant, indicating that one algorithm may indeed be superior to others.\n\nWe have averaged the performance metrics over 200 runs to ensure that our results are robust and not dependent on the specifics of any single run. This extensive averaging helps to smooth out any random variations and provides a more stable estimate of the true performance.\n\nIn summary, the inclusion of 95% confidence intervals and the averaging over multiple runs allow us to make confident claims about the performance and superiority of our machine learning algorithms. This rigorous evaluation approach ensures that our findings are statistically significant and reliable.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study's data was gathered anonymously and managed using the REDCap electronic data capture tools hosted at the Interuniversity Consortium CINECA. The data sources were clinical charts and hospital electronic records from six hospitals in Italy and Spain. The database was locked after careful revision for incongruent or missing data. The study does not mention the public release of raw evaluation files or specify any licensing terms for data access."
}