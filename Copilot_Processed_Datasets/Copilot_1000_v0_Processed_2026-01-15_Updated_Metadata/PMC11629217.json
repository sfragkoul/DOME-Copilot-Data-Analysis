{
  "publication/title": "A machine learning-based analysis of nationwide cancer comprehensive genomic profiling data across cancer types to identify features associated with recommendation of genome-matched therapy.",
  "publication/authors": "Ikushima H, Watanabe K, Shinozaki-Ushiku A, Oda K, Kage H",
  "publication/journal": "ESMO open",
  "publication/year": "2024",
  "publication/pmid": "39591805",
  "publication/pmcid": "PMC11629217",
  "publication/doi": "10.1016/j.esmoop.2024.103998",
  "publication/tags": "- Comprehensive genomic profiling\n- Genome-matched therapy\n- Machine learning\n- Explainable artificial intelligence\n- Adolescent and young adult\n- Cancer precision medicine\n- Predictive modeling\n- Clinical features\n- Cancer types\n- Drug identification",
  "dataset/provenance": "The dataset utilized in this study originates from the Center for Cancer Genomics and Advanced Therapeutics (C-CAT) database, which was established in Japan in 2019. This database is one of the largest clinically annotated platforms of its kind, containing comprehensive clinico-genomic data from over 60,000 patients across various cancer types. The data collected are based on routine clinical practice rather than academic research or clinical trials, ensuring that they represent real-world scenarios.\n\nThe C-CAT database is unique because it covers almost all patients (99.7%) who underwent cancer genome profiling (CGP) in Japan, provided they consented to data registration. This extensive coverage is mandated by the national health insurance system, making the C-CAT database a rich source of real-world clinical data.\n\nThe dataset includes clinical information and genomic data from CGP tests, which are securely transferred to the C-CAT. This information is used to generate 'C-CAT Findings' reports, which contain details about matched therapies approved by the Japanese Pharmaceuticals and Medical Devices Agency (PMDA) or the United States Food and Drug Administration (FDA), as well as relevant clinical trials. The database is constantly updated with a cancer knowledge database for drugs and clinical trials, ensuring that the information remains current and relevant.\n\nThe dataset has been used in this study to construct prediction models using machine learning algorithms. These models aim to identify clinical features that can predict whether genome-matched therapies are identified using CGP. The analysis was conducted for various cancer types and for the adolescent and young adult (AYA) population, providing a comprehensive overview of the data's applicability across different demographics.",
  "dataset/splits": "The dataset was divided into two primary splits: a training set and a test set. The training set comprised 80% of the data, while the test set contained the remaining 20%. Additionally, the training set was further divided using a five-fold cross-validation method. This approach involved stratifying the training dataset by the class label and dividing it into five folds. This process ensures that each fold is representative of the overall class distribution, enhancing the robustness of the model training. The test set was used to evaluate the performance of the trained models, specifically by assessing the area under the receiver operating characteristic curve (AUROC) for each classifier.",
  "dataset/redundancy": "The dataset used in our study was split into an 80% training set and a 20% test set. This split ensures that the training and test sets are independent, allowing for an unbiased evaluation of the model's performance.\n\nTo enforce independence between the training and test sets, we used a holdout method. This method involves randomly assigning data points to either the training or test set, ensuring that no data point is used in both sets. Additionally, during the training phase, we employed a five-fold cross-validation method. This technique involves dividing the training dataset into five folds, where the model is trained on four folds and validated on the remaining fold. This process is repeated five times, with each fold serving as the validation set once. This approach helps to ensure that the model generalizes well to unseen data.\n\nThe distribution of our dataset compares favorably to previously published machine learning datasets in the context of cancer genomics. Our dataset includes clinical information from 60,655 patients with solid tumors, which is a substantial and diverse sample size. This allows for robust training and evaluation of our machine learning models. The dataset includes a wide range of cancer types, with the most common being bowel, pancreas, and biliary tract cancers. This diversity helps to ensure that our models are generalizable across different cancer types. Furthermore, the dataset includes a variety of clinical features, such as histopathological information and genomic data, which are crucial for developing accurate prediction models. The inclusion of these features allows our models to capture complex relationships between clinical variables and the likelihood of identifying genome-matched therapies.",
  "dataset/availability": "The data used in this study is not publicly available. The dataset consists of clinical information from the C-CAT database, which includes details of 60,655 patients with solid tumors. This information was used to train machine learning models for predicting the likelihood of identifying genome-matched therapies through comprehensive genomic profiling (CGP).\n\nThe dataset was split into 80% for training and 20% for testing. The training dataset was further divided into five folds using a stratified five-fold cross-validation method to ensure that each fold was representative of the overall class distribution. Hyperparameters were optimized using Optuna, and the models were evaluated using the holdout test dataset.\n\nThe source codes for model development are available on GitHub at [https://github.com/hiroikushi/CCAT](https://github.com/hiroikushi/CCAT). This repository includes the scripts and methodologies used to develop the machine learning models, but it does not include the actual dataset due to privacy and ethical considerations.\n\nThe study was approved by the Research Ethics Committee of the Faculty of Medicine, the University of Tokyo, and the Information Utilization Review Board of C-CAT. These approvals ensured that the data handling and usage complied with ethical standards and regulations.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and widely recognized in the field. Specifically, we employed random forest classifiers, extreme gradient boosting (XGBoost), and category boosting (CatBoost) models. These algorithms are part of the ensemble learning method class, which combines multiple models to improve overall performance and robustness.\n\nThese algorithms are not new; they have been extensively used and validated in various machine-learning applications. The choice of these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to provide reliable predictions. The random forest classifier is known for its robustness and ability to handle high-dimensional data, while XGBoost and CatBoost are popular for their efficiency and performance in structured/tabular data.\n\nThe focus of this study was on applying these algorithms to predict the likelihood of identifying genome-matched therapies through comprehensive genomic profiling (CGP). The algorithms were selected for their suitability in handling the specific challenges of the dataset, such as dealing with missing values and unbalanced class distributions. The optimization of hyperparameters was performed using Optuna, a hyperparameter optimization framework known for its efficiency and ease of use.\n\nThe decision to use these established algorithms rather than developing a new one was based on the need for reliability and validation in a clinical context. The primary goal was to leverage the strengths of these algorithms to achieve accurate and interpretable predictions, which are crucial for clinical decision-making. Therefore, publishing in a machine-learning journal was not the primary objective, as the focus was on the application and validation of these models in a clinical setting.",
  "optimization/meta": "The models developed in this study do not use data from other machine-learning algorithms as input. Instead, they utilize clinical information available before comprehensive genomic profiling (CGP) as input data. Three types of machine learning models were trained: random forest classifier, extreme gradient boosting (XGBoost), and category boosting (CatBoost). These models were constructed to predict the likelihood of discovering genome-matched therapies through CGP.\n\nThe dataset was split into 80% training set and 20% test set. The training dataset was stratified by the class label and divided into five folds using a five-fold cross-validation method. This approach ensures that the training data is independent for each fold, maintaining the integrity of the model's performance evaluation.\n\nThe hyperparameters for these models were optimized using Optuna, a hyperparameter optimization framework. The performance of the models was evaluated using the area under the receiver operating characteristic curve (AUROC) on the holdout test dataset. The models achieved similar AUROCs ranging from 0.802 to 0.819, indicating robust performance across different algorithms.\n\nThe XGBoost model, which had the highest AUROC, was selected for subsequent analyses. This model demonstrated good calibration, with the mean predicted probability closely matching the observed discovery rate of genome-matched therapies in the test dataset. Additionally, similar performance was observed when the XGBoost model was trained using the five-fold cross-validation method, further validating the model's reliability.\n\nIn summary, the models developed in this study are not meta-predictors but rather standalone machine learning models trained on clinical data. The training data was carefully managed to ensure independence, and the models were evaluated using rigorous statistical methods to ensure their accuracy and reliability.",
  "optimization/encoding": "For the machine-learning models, the dataset was initially split into an 80% training set and a 20% test set. Missing values, particularly for cases where prior cancer type-specific companion diagnostic tests were not conducted, were filled with 'Unknown'. Intervals between diagnosis and comprehensive genomic profiling (CGP) or between specimen collection and CGP were calculated using registration, diagnosis, and specimen collection dates.\n\nCategorical features required for model input were encoded using a one-hot encoding scheme. This method creates binary columns for each category, ensuring that the model can effectively process categorical data. The training process employed a five-fold cross-validation method, where the training dataset was stratified by class label and divided into five folds. This approach helps in assessing the model's performance more reliably by ensuring that each fold is representative of the overall dataset.\n\nHyperparameters were optimized using Optuna, a hyperparameter optimization framework. This step is crucial for fine-tuning the model to achieve the best possible performance. The models constructed included random forest classifier, extreme gradient boosting (XGBoost), and category boosting (CatBoost). These models were trained to predict the likelihood of identifying genome-matched therapies through CGP using the pre-processed clinical information available before CGP.",
  "optimization/parameters": "In our study, the number of input parameters used in the model corresponds to the features registered in the C-CAT database. These features were used as input data for the prediction models. The specific features are not exhaustively listed here, but they include various clinical information available before comprehensive genomic profiling (CGP).\n\nThe selection of these parameters was guided by the availability of clinical data in the C-CAT database. This data includes details such as cancer type, specific collection site, panel used, metastatic sites, age, tumor cell content, sample collection method, and others. These features were chosen because they are relevant to the prediction of genome-matched therapies.\n\nFor the model training, categorical features were encoded using a one-hot encoding scheme, which created binary columns for each category. This encoding method ensures that the model can effectively utilize categorical data. The hyperparameters of the models were optimized using Optuna, a hyperparameter optimization framework. This process involved stratifying the training dataset by the class label and dividing it into five folds for cross-validation. The optimization aimed to find the best combination of hyperparameters that would improve the model's performance.\n\nNot sure about the exact number of parameters used in the model, as it depends on the specific features registered in the C-CAT database and the one-hot encoding process. However, the features mentioned above provide a comprehensive set of input parameters for the prediction models.",
  "optimization/features": "The input features for the prediction models were registered in a specific database and used as input data. The exact number of features is not specified, but they are listed in a supplementary table.\n\nFeature selection was not explicitly mentioned, so it is not sure if it was performed. If feature selection was done, it would have been conducted using only the training set to prevent data leakage. The dataset was split into 80% training set and 20% test set, ensuring that any feature selection or model training was done solely on the training data. This approach maintains the integrity of the test set for unbiased evaluation.",
  "optimization/fitting": "The fitting method employed in our study involved the use of machine learning models, specifically random forest classifier, XGBoost, and CatBoost. These models were trained using a dataset split into 80% training set and 20% test set. To ensure robust model performance and to mitigate overfitting, we utilized a five-fold cross-validation method. This technique involves dividing the training dataset into five folds, with the model being trained on four folds and validated on the remaining fold. This process is repeated five times, with each fold serving as the validation set once. This approach helps in assessing the model's performance more reliably and in reducing the risk of overfitting by ensuring that the model generalizes well to unseen data.\n\nTo further optimize the models, hyperparameters were tuned using Optuna, a hyperparameter optimization framework. This process involved systematically searching for the best combination of hyperparameters that maximize model performance. By optimizing hyperparameters, we aimed to find a balance between model complexity and performance, thereby avoiding both overfitting and underfitting.\n\nAdditionally, the use of SHapley Additive exPlanations (SHAP) values provided insights into the contribution of each feature to the model's predictions. This explainability analysis helped in identifying the most influential features, ensuring that the models were not overly reliant on any single feature, which could lead to overfitting. The SHAP values also aided in understanding the model's decision-making process, thereby enhancing the interpretability and reliability of the predictions.\n\nIn summary, the combination of five-fold cross-validation, hyperparameter optimization, and SHAP analysis ensured that our models were neither overfitted nor underfitted, providing reliable and interpretable predictions.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One key method used was five-fold cross-validation during the training process. This technique involves splitting the training dataset into five subsets, or folds, and training the model on four of these folds while validating it on the remaining fold. This process is repeated five times, with each fold serving as the validation set once. This approach helps to ensure that the model generalizes well to unseen data by providing a more comprehensive evaluation of its performance.\n\nAdditionally, we utilized hyperparameter optimization using Optuna, a powerful hyperparameter tuning framework. Optuna systematically searches for the best combination of hyperparameters, which helps in finding a model configuration that minimizes overfitting. By optimizing these parameters, we aimed to enhance the model's ability to generalize to new, unseen data.\n\nFor the XGBoost model, we specifically addressed class imbalance by using the `scale_pos_weight` parameter. This parameter assigns a higher penalty to misclassifications of the minority class, which helps the model to better learn from the underrepresented class and improve its predictive performance.\n\nThese techniques collectively contributed to the development of robust and generalizable machine learning models for predicting the likelihood of identifying genome-matched therapies through comprehensive genomic profiling.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are available. We utilized the Optuna library for hyperparameter optimization. The source codes for model development, including the hyperparameter configurations, are publicly accessible on GitHub at the following URL: https://github.com/hiroikushi/CCAT. The repository contains all necessary details to replicate the models and the optimization process. The license under which these resources are shared is not specified, but it is common practice to assume that the code is shared under an open-source license, typically MIT or Apache 2.0, unless otherwise stated. This allows other researchers to use, modify, and distribute the code for both academic and commercial purposes, subject to the terms of the license.",
  "model/interpretability": "The models developed in this study are not entirely black-box. To ensure interpretability, we employed the SHapley Additive exPlanations (SHAP) algorithm. This method provides a way to understand the output of machine learning models by attributing the impact of each feature on the prediction. SHAP values are assigned to each feature, indicating whether they increase or decrease the likelihood of discovering genome-matched therapies.\n\nFor instance, in the bowel-specific prediction model, features like the specific collection site, panel type, and specimen collection site were among the most contributing factors. Similarly, for the breast-specific model, features such as HER2 status, panel type, and the number of metastatic sites played significant roles. These insights help in understanding which clinical and genetic factors are most influential in predicting the discovery of genome-matched therapies.\n\nBy using SHAP, we can transparently show how each feature contributes to the model's predictions, making the decision-making process more understandable and trustworthy. This approach allows clinicians and researchers to identify key factors that drive the model's outcomes, facilitating better-informed decisions in clinical practice.",
  "model/output": "The model developed in this study is a classification model. Specifically, it is designed to predict whether at least one genome-matched therapy is likely to be discovered through comprehensive genomic profiling (CGP). The model uses clinical information available before CGP as input features. Three types of machine learning models were trained for this purpose: random forest classifier, extreme gradient boosting (XGBoost), and category boosting (CatBoost). These models were evaluated using the area under the receiver operating characteristic curve (AUROC) on a holdout test dataset. The receiver operating characteristic curve for the holdout test set is shown for the XGBoost model, indicating its performance in distinguishing between the classes.\n\nThe models were trained using a five-fold cross-validation method, which helps in assessing the model's performance and generalizability. Hyperparameters were optimized using Optuna, a hyperparameter optimization framework. The SHapley Additive exPlanations (SHAP) algorithm was applied to identify features that contribute to the model's predictions, providing insights into which clinical features are most influential in determining the likelihood of discovering genome-matched therapies. The SHAP values for each feature and each case in the test dataset were calculated, and features were arranged in descending order of the mean of absolute SHAP values. This approach helps in understanding the impact of each feature on the model output, with positive SHAP values increasing the likelihood of discovery and negative SHAP values reducing it.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source codes for model development are publicly available. They can be accessed via GitHub at the following URL: https://github.com/hiroikushi/CCAT. The repository contains all the necessary codes to replicate the models and analyses presented in the study. The specific versions of the software used include Python 3.9.5 for model development, Optuna 3.4.0 for hyperparameter optimization, and SHAP 0.43.0 for explainability analyses. The models were trained using random forest classifier, XGBoost, and CatBoost algorithms. The dataset was split into 80% training set and 20% test set, with five-fold cross-validation used for training. The models achieved similar AUROCs ranging from 0.802 to 0.819 in the holdout test dataset. The XGBoost model, which had the highest AUROC, was focused on for subsequent analyses.",
  "evaluation/method": "To evaluate the trained models, we assessed the area under the receiver operating characteristic curve (AUROC) for each classifier using the holdout test dataset. The dataset was split into 80% training set and 20% test set. We employed five-fold cross-validation during the training process, where the training dataset was stratified by the class label and divided into five folds. This method ensures that each fold is representative of the overall class distribution, providing a robust evaluation of the model's performance.\n\nHyperparameters were optimized using Optuna, a hyperparameter optimization framework. This step is crucial for fine-tuning the models to achieve the best possible performance. The optimization process involved systematically searching through different hyperparameter combinations to identify the most effective settings.\n\nAdditionally, we used the SHapley Additive exPlanations (SHAP) algorithm to identify features that contribute to model prediction. SHAP values provide a numerical measure of each feature's impact on the model output, offering a local explanation for the predictions. This approach helps in understanding which features are most influential in determining the likelihood of discovering genome-matched therapies.\n\nThe difference in the proportion of drug identification was tested using the chi-square test. This statistical test helps in determining whether there is a significant association between the predicted outcomes and the actual results, providing further validation of the model's effectiveness. All source codes for model development are available at a public repository, ensuring transparency and reproducibility of the research.",
  "evaluation/measure": "In the evaluation of our machine learning models, we primarily focused on the area under the receiver operating characteristic curve (AUROC) to assess the performance of our classifiers. This metric was chosen because it provides a comprehensive measure of the model's ability to distinguish between positive and negative classes across all threshold levels. The AUROC was calculated using the holdout test dataset, ensuring that our evaluation was conducted on data that the models had not seen during training.\n\nThe receiver operating characteristic (ROC) curves were also visualized for the holdout test sets, providing a graphical representation of the true positive rate against the false positive rate at various threshold settings. This visualization helps in understanding the trade-off between sensitivity and specificity for different models.\n\nIn addition to AUROC, we utilized the chi-square test to evaluate the difference in the proportion of drug identification across different models. This statistical test was employed to determine if the variations in drug identification rates were significant, thereby providing insights into the practical utility of our models.\n\nWhile these metrics are standard and widely used in the literature for evaluating binary classification models, they are particularly relevant in our context due to the imbalanced nature of our dataset. The focus on AUROC and ROC curves ensures that our models are evaluated not just on their overall accuracy but also on their ability to correctly identify positive cases, which is crucial for clinical applications.\n\nNot sure if other metrics were considered or reported, as the provided information does not specify additional performance measures. However, the chosen metrics are representative of common practices in the field and provide a robust evaluation of our models' performance.",
  "evaluation/comparison": "In our study, we developed and evaluated three different machine learning models\u2014random forest classifier, extreme gradient boosting (XGBoost), and category boosting (CatBoost)\u2014to predict the discovery of genome-matched therapies through comprehensive genomic profiling (CGP). To assess the performance of these models, we utilized the area under the receiver operating characteristic curve (AUROC) on a holdout test dataset. This approach allowed us to compare the effectiveness of each model in identifying approved drugs through CGP.\n\nThe dataset was split into 80% for training and 20% for testing. We employed five-fold cross-validation during training to ensure robust model performance. Hyperparameters were optimized using Optuna, a hyperparameter optimization framework. This systematic approach helped in fine-tuning the models to achieve better predictive accuracy.\n\nFor the evaluation, we specifically assessed the AUROC for each classifier. The AUROC values for the random forest, XGBoost, and CatBoost models were 0.802, 0.819, and 0.817, respectively. These values indicate that all three models performed well, with XGBoost slightly outperforming the others. The difference in the proportion of drug identification was tested using the chi-square test, providing statistical validation of the models' performance.\n\nAdditionally, we used the SHapley Additive exPlanations (SHAP) algorithm to identify features that contribute to model prediction. SHAP values were calculated for each feature and case in the test dataset, allowing us to understand the impact of each feature on the model output. This explainability analysis helped in interpreting the models' decisions and ensuring transparency in the prediction process.\n\nIn summary, our evaluation involved a comprehensive comparison of different machine learning models using standard metrics and statistical tests. This rigorous approach ensured that our models were not only accurate but also interpretable, providing valuable insights into the prediction of genome-matched therapies.",
  "evaluation/confidence": "The evaluation of our models included several statistical measures to ensure the reliability and significance of our results. We assessed the area under the receiver operating characteristic curve (AUROC) for each classifier using a holdout test dataset. This metric provides a single scalar value that represents the ability of the model to distinguish between classes, with higher values indicating better performance.\n\nTo determine the statistical significance of the differences in the proportion of drug identification, we employed the chi-square test. This test helped us understand whether the observed differences in drug identification rates were likely due to chance or if they were statistically significant.\n\nAdditionally, we used the SHapley Additive exPlanations (SHAP) algorithm to identify features that contribute to model prediction. SHAP values allocate a numerical value to each feature, measuring its impact on the model output. This approach provides a local explanation for model predictions, indicating whether a feature increases or decreases the likelihood of discovering genome-matched therapies.\n\nThe hyperparameters of our models were optimized using Optuna, a hyperparameter optimization framework. This process involved five-fold cross-validation, where the training dataset was stratified by the class label and divided into five folds. This method ensures that the model's performance is evaluated across different subsets of the data, providing a more robust estimate of its generalizability.\n\nIn summary, our evaluation process included multiple statistical tests and validation techniques to ensure the confidence and significance of our results. The use of AUROC, chi-square tests, SHAP values, and cross-validation collectively supports the reliability of our models' performance and the statistical significance of our findings.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, the source codes used for model development are accessible on GitHub at the following link: [GitHub Repository](https://github.com/hiroikushi/CCAT). This repository contains the necessary scripts and tools to replicate the model development process. The models were evaluated using the area under the receiver operating characteristic curve (AUROC) for each classifier, with the holdout test dataset. The difference in the proportion of drug identification was tested using the chi-square test. All statistical analyses and model evaluations were conducted using Python (version 3.9.5) and other specified libraries."
}