{
  "publication/title": "An intelligence design for detection and classification of COVID19 using fusion of classical and convolutional neural network and improved microscopic features selection approach.",
  "publication/authors": "Amin J, Anjum MA, Sharif M, Saba T, Tariq U",
  "publication/journal": "Microscopy research and technique",
  "publication/year": "2021",
  "publication/pmid": "33964096",
  "publication/pmcid": "PMC8237066",
  "publication/doi": "10.1002/jemt.23779",
  "publication/tags": "- ensemble methods\n- entropy\n- fusion\n- hand crafted features\n- healthcare\n- public health\n- U-Net\n- COVID-19\n- deep learning\n- medical imaging",
  "dataset/provenance": "The datasets used in our research are publicly available and have been utilized in previous studies. The first dataset, referred to as Dataset I, is the COVID-19 segmentation data, which includes 829 CT image slices from 40 patients. This dataset contains 373 positive cases of COVID-19. The second dataset, Dataset II, consists of Chinese Hospital CT imaging data from 216 patients, comprising 316 positive and 700 negative slices. The third dataset, Dataset III, includes data from 30 patients at POF Hospital, with 2000 positive slices out of a total of 4500. The ground truth for these datasets was created by classified radiologists. The data that support the findings of this study are openly available in the COVID-CT-dataset, accessible at https://covid-19.conacyt.mx/jspui/handle/1000/4157.",
  "dataset/splits": "In our study, we utilized three benchmark datasets to evaluate the proposed research. For Dataset I, which consists of COVID-19 segmentation data, we performed a 10-fold cross-validation. This means the data was divided into 10 sets, with one set used for testing and the remaining nine for training in each iteration. This process was repeated 10 times, ensuring that each set was used for testing exactly once. Dataset I includes 829 slices, with 456 negative images and 373 positive images.\n\nFor Dataset II, which contains Chinese Hospital CT imaging data of 216 patients, and Dataset III, which includes 30 patients from POF Hospital, the specific split details are not provided. However, Dataset II consists of 316 positive and 700 negative slices, while Dataset III includes 2000 positive slices out of 4,500. The distribution of data points in these datasets is not explicitly detailed, but the total number of images and the classification outcomes are provided for each dataset.",
  "dataset/redundancy": "The datasets used in our research were split using a 10-fold cross-validation approach. This method involves dividing the entire dataset into 10 subsets. In each iteration of the cross-validation, one subset is used for testing, while the remaining nine subsets are used for training. This process is repeated 10 times, with each subset serving as the test set exactly once. This ensures that every data point gets to be in the test set once and in the training set nine times, providing a robust evaluation of the model's performance.\n\nThe training and test sets are independent in each fold of the cross-validation. This independence is enforced by the cross-validation process itself, which randomly assigns data points to the 10 subsets. This random assignment helps to ensure that the model is not biased towards any particular subset of the data and that it generalizes well to unseen data.\n\nRegarding the distribution of the datasets, we used three benchmark datasets. The first dataset, COVID-19 segmentation data, consists of 829 slices of CT images from 40 patients, with 373 positive cases. The second dataset, from a Chinese Hospital, includes 1,016 slices from 216 patients, with 316 positive and 700 negative slices. The third dataset, from POF Hospital, contains 4,500 slices from 30 patients, with 2,000 positive slices. The distribution of positive and negative cases in these datasets is comparable to previously published machine learning datasets in the field of medical imaging, ensuring that our results are relevant and applicable to similar studies.",
  "dataset/availability": "The data that support the findings of this study are openly available in the COVID-CT-dataset. This dataset can be accessed at the following URL: https://covid-19.conacyt.mx/jspui/handle/1000/4157. The dataset includes CT scan images related to COVID-19, which were used for both segmentation and classification tasks in our research. The availability of this dataset ensures reproducibility and allows other researchers to validate and build upon our findings. The dataset is provided under terms that allow for academic and research use, facilitating collaboration and further advancements in the field.",
  "optimization/algorithm": "The optimization algorithm used in our research is not a novel machine-learning algorithm. Instead, we employed well-established ensemble learning techniques to enhance the performance of our COVID-19 detection and classification system. Specifically, we utilized three types of ensemble classifiers: boosted trees, bagged trees, and RUSBoosted trees. These methods are widely recognized in the machine-learning community for their effectiveness in improving predictive accuracy and robustness.\n\nThe boosted tree classifier we used is based on the AdaBoost algorithm, which combines multiple weak decision trees to create a strong predictive model. The bagged tree classifier employs the bagging technique, which involves training multiple decision trees on different subsets of the data and then aggregating their predictions. The RUSBoosted tree classifier is an extension of AdaBoost that addresses class imbalance by using random undersampling.\n\nThese ensemble methods were chosen because they have proven to be effective in handling complex datasets and improving the generalization performance of machine-learning models. By combining the strengths of multiple weak learners, these ensemble techniques can achieve higher accuracy and better robustness compared to individual models.\n\nThe reason these algorithms were not published in a machine-learning journal is that they are established techniques that have been extensively studied and validated in the literature. Our focus was on applying these well-known methods to the specific problem of COVID-19 detection and classification, rather than developing new machine-learning algorithms. The innovation in our work lies in the application of these techniques to medical imaging data and the integration of hand-crafted and deep features for improved performance.",
  "optimization/meta": "The model employs an ensemble learning approach, which can be considered a form of meta-predictor as it uses the outputs of multiple machine-learning algorithms as input. The ensemble consists of three types of tree-based classifiers: boosted trees, bagged trees, and RUSBoosted trees. These classifiers are combined to create a powerful prediction model.\n\nThe boosted tree classifier uses the AdaBoost ensemble approach with decision trees as the base learner. The bagged tree classifier employs the Bag ensemble approach. The RUSBoosted tree classifier uses the RUSBoost ensemble approach, which is designed to handle imbalanced datasets.\n\nRegarding the independence of the training data, the experiments were conducted using 10-fold cross-validation. This method involves dividing the entire dataset into 10 subsets, where one subset is used for testing and the remaining nine are used for training. This process is repeated 10 times, ensuring that each subset is used for testing exactly once. This approach helps to ensure that the training data is independent for each fold, providing a robust evaluation of the model's performance.",
  "optimization/encoding": "In our study, the data encoding and preprocessing involved several steps to ensure the machine-learning algorithms could effectively learn from the input images. Initially, various hand-crafted features were extracted from the input images. These features included NHr, SFTA, HOG, LBP, and SURF. Each of these features was selected based on its ability to capture different aspects of the image data.\n\nThe length of the extracted features varied, with NHr features having a length of 128, HOG features having a length of 8,100, SFTA features having a length of 22, SURF features having a length of 96, and deep features from ResNet101 and Inceptionv3 each having a length of 1,000.\n\nTo reduce dimensionality and focus on the most informative features, entropy was used as a selection criterion. This involved calculating the entropy for each feature and selecting the top-scoring features. For example, 15 NHr features were selected out of 128, 3,935 HOG features out of 8,100, 15 SFTA features out of 22, 90 SURF features out of 96, and 100 deep features from each of the deep learning models.\n\nThe selected features were then concatenated horizontally to form a fused feature vector. This vector had a length of 4,256, with the last column representing the class labels. The fused feature vector was then passed to the ensemble classifiers for training and prediction.\n\nAdditionally, deep features were extracted using Deep Convolutional Neural Networks (DCNNs) such as ResNet101 and Inceptionv3. These deep features were also selected based on entropy and fused with the hand-crafted features to create a comprehensive feature set. The final fused features were used to train the ensemble classifiers, which included boosted trees, bagged trees, and RUSBoosted trees. This approach ensured that the machine-learning algorithms had access to a rich and informative set of features, enhancing their ability to accurately classify the input data.",
  "optimization/parameters": "In our study, we employed a U-Net model for segmentation tasks, and the selection of input parameters was crucial for optimizing its performance. The model was configured with an input size of 224x224 pixels. The encoder depth was set to 3, which refers to the number of downsampling operations in the U-Net architecture. This depth was chosen after extensive experimentation to balance between capturing sufficient contextual information and computational efficiency.\n\nThe model was trained using the stochastic gradient descent with momentum (sgdm) optimizer, which is known for its stability and effectiveness in training deep neural networks. The learning rate was set to 0.001, a value that was determined through empirical testing to provide a good trade-off between convergence speed and stability. The number of epochs, or complete passes through the training dataset, was set to 40. This number was selected based on validation performance, ensuring that the model had enough iterations to learn the relevant features without overfitting.\n\nThese parameters\u2014encoder depth, optimizer, learning rate, and number of epochs\u2014were chosen through a systematic process of experimentation and validation. The selected configuration demonstrated superior training accuracy compared to other tested parameters, as detailed in our experiments. This careful tuning of input parameters was essential for achieving high-performance segmentation results in our study.",
  "optimization/features": "In the \"Input Features\" subsection of the \"Optimization\" section, the input features used for classification are a result of a comprehensive process involving extraction, selection, and fusion of both hand-crafted and deep features.\n\nThe hand-crafted features extracted include noise to harmonic ratio (NHr), histogram of oriented gradients (HOG), local binary patterns (LBPs), statistical features of texture attributes (SFTA), and speeded up robust features (SURF). Additionally, deep features are extracted using pre-trained deep learning models, specifically ResNet101 and InceptionV3.\n\nThe lengths of the extracted features are as follows: 128 for NHr, 8,100 for HOG, 22 for SFTA, 96 for SURF, 1,000 for ResNet101, and 1,000 for InceptionV3. Feature selection is performed using entropy to select the best features based on the maximum score. The selected features are 15 NHr features out of 128, 3,935 HOG features out of 8,100, 15 SFTA features out of 22, 90 SURF features out of 96, and 100 deep features out of 1,000 for both ResNet101 and InceptionV3.\n\nThe selected features are then concatenated horizontally to form the final fused feature vector, which has a length of 4,256. The last column of this vector represents the class labels. This final feature vector, along with the class labels, is passed to the ensemble classifiers for COVID-19 classification.\n\nFeature selection was performed using the training set only, ensuring that the model's performance is evaluated on unseen data during the testing phase. This approach helps in preventing data leakage and ensures the robustness of the classification model.",
  "optimization/fitting": "The fitting method employed in our study involved a careful balance to avoid both overfitting and underfitting. The number of parameters in our models was indeed larger than the number of training points, which is a common scenario in deep learning and ensemble methods. To mitigate the risk of overfitting, we utilized several strategies.\n\nFirstly, we employed cross-validation, specifically 10-fold cross-validation, to ensure that our models were generalizing well to unseen data. This technique involves dividing the dataset into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. This approach helps in assessing the model's performance more reliably and reduces the risk of overfitting.\n\nSecondly, we used ensemble methods, which combine multiple weak learners to create a strong predictive model. The ensemble classifiers included boosted trees, bagged trees, and RUSBoosted trees. These methods inherently reduce overfitting by averaging the predictions of multiple models, thereby smoothing out the noise and focusing on the underlying patterns in the data.\n\nAdditionally, we performed extensive hyperparameter tuning. For instance, we experimented with different optimizers such as sgdm, adam, and rmsProp, and selected the one that provided the best training accuracy. We also varied the number of epochs and learning rates to find the optimal settings. The parameters that yielded the highest training accuracy were chosen for further experiments, ensuring that the model was neither too complex nor too simple.\n\nTo rule out underfitting, we ensured that our models had sufficient capacity to learn the underlying patterns in the data. This was achieved by using deep learning architectures like U-Net for segmentation and deep convolutional neural networks (DCNNs) for feature extraction. These models are known for their ability to capture complex patterns in the data. Furthermore, we used a combination of hand-crafted and deep features, which provided a rich representation of the data, helping the models to learn effectively.\n\nIn summary, by using cross-validation, ensemble methods, and careful hyperparameter tuning, we were able to strike a balance between overfitting and underfitting, ensuring that our models generalized well to new data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting. One of the key methods used was dropout. Specifically, a 50% dropout layer was incorporated in the fourth stage of the encoder section of our U-Net model. This dropout layer randomly sets half of the neurons to zero during training, which helps to prevent the model from becoming too reliant on any single neuron and thus reduces overfitting.\n\nAdditionally, we utilized cross-validation to ensure the robustness of our model. In our classification experiments, we performed 10-fold cross-validation, where the data was divided into 10 sets. One set was used for testing while the remaining nine were used for training. This process was repeated 10 times, with each set being used as the test set once. This approach helps to ensure that the model generalizes well to unseen data and is not merely memorizing the training data.\n\nFurthermore, we experimented with different optimizers and learning rates to find the optimal configuration that minimized overfitting. For instance, we tested optimizers such as SGDM, Adam, and RMSProp with varying numbers of epochs and learning rates. The SGDM optimizer with 40 epochs and a learning rate of 0.001 was found to provide the best training accuracy, indicating effective regularization.\n\nThese techniques collectively contributed to the model's ability to generalize well to new, unseen data, thereby reducing the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule are reported in the publication. The specific parameters used for the U-Net model, including input size, encoder depth, maximum epochs, and learning rate, are detailed in the text. Additionally, the selection of optimal parameters for the segmentation model is discussed, with different optimizers and their respective epochs and learning rates being evaluated. The best-performing configuration, which includes an encoder depth of 3, the sgdm optimizer, 40 epochs, and a learning rate of 0.001, is highlighted.\n\nRegarding model files and optimization parameters, the publication does not explicitly mention the availability of these files. However, the data used for evaluation is openly available in the COVID-CT-dataset, accessible at https://covid-19.conacyt.mx/jspui/handle/1000/4157. This dataset includes CT images with ground truth, which can be used to replicate the experiments and optimize the model further.\n\nThe license under which the dataset is available is not specified in the provided text. For detailed information on the license, one would need to refer to the dataset's repository or the associated documentation.",
  "model/interpretability": "The model employed in this study is not entirely a black box, as it incorporates several interpretable components and techniques. The segmentation model used is a modified U-Net, which is a type of convolutional neural network (CNN) known for its architecture that facilitates interpretability. The U-Net architecture includes skip connections that allow the model to combine low-level and high-level features, making it easier to understand how different layers contribute to the final segmentation output.\n\nIn the classification experiments, hand-crafted features such as NHr, SFTA, HOG, LBP, and SURF are extracted and fused. These features are well-documented in the literature and have clear interpretations. For instance, HOG (Histogram of Oriented Gradients) captures edge or gradient structure, which is intuitive for understanding how the model differentiates between different regions in the images. Similarly, LBP (Local Binary Patterns) encodes local texture information, providing a clear basis for feature interpretation.\n\nThe deep features extracted using DCNNs like ResNET101 and Inception v3 are also interpretable to some extent. These networks are designed to capture hierarchical features, starting from simple edges and textures in the lower layers to more complex patterns in the higher layers. By visualizing the activations of these layers, one can gain insights into what the model is focusing on at different stages of processing.\n\nThe ensemble classifiers used, such as boosted trees, bagged trees, and RUSBoosted trees, are also interpretable. Decision trees, which form the basis of these ensemble methods, are inherently interpretable as they can be visualized and understood in terms of decision rules. The ensemble methods combine multiple decision trees to improve robustness and accuracy, but the individual trees remain interpretable.\n\nOverall, while the deep learning components of the model may introduce some level of complexity, the use of interpretable features and ensemble methods ensures that the model's decisions can be understood and explained. This transparency is crucial for applications in medical imaging, where interpretability is essential for clinical acceptance and trust.",
  "model/output": "The model is primarily designed for classification tasks. It focuses on segmenting and classifying healthy and COVID-19 affected regions in medical images, specifically CT scans. The segmentation part of the model identifies the affected lung regions, while the classification part determines whether these regions indicate COVID-19 or not.\n\nThe classification is performed using ensemble methods that combine multiple decision tree classifiers, such as boosted trees, bagged trees, and RUSBoosted trees. These classifiers are trained on fused features, which include both hand-crafted features and deep features extracted from convolutional neural networks. The model's performance is evaluated using metrics like sensitivity, specificity, and accuracy, which are standard for classification tasks.\n\nThe experiments conducted demonstrate the model's effectiveness in classifying COVID-19 cases with high accuracy across different datasets. For instance, the RUSBoosted tree ensemble model achieved an accuracy of 98.17% on the COVID-19 segmentation dataset and 98.97% on the Chinese Hospital dataset. These results indicate that the model is well-suited for classification tasks in the context of COVID-19 diagnosis.",
  "model/duration": "The execution time for the ensemble classifiers varied depending on the specific model used. The boosted tree model took approximately 380.3 seconds to train. The bagged tree model had a significantly shorter training time, completing in about 64.689 seconds. The RUSBoosted tree model required around 189.98 seconds for training. These times reflect the computational effort involved in building the ensemble classifiers, which combine multiple weak decision tree classifiers to create a robust prediction model. The differences in training times can be attributed to the varying complexities and learning rates of the individual models.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The proposed method was evaluated using three benchmark datasets, each containing CT images related to COVID-19. The datasets included COVID-19 segmentation data, Chinese Hospital CT imaging data, and POF Hospital CT imaging data. These datasets were used to assess the performance of the segmentation and classification models.\n\nTwo main experiments were conducted. The first experiment focused on pixel-based segmentation results, which were evaluated against ground truth data. The second experiment involved classification using hand-crafted fused features and a combination of hand-crafted and deep features. For this experiment, a 10-fold cross-validation approach was employed, where the data was divided into 10 sets. One set was used for testing while the remaining nine were used for training, and this process was repeated for all 10 iterations.\n\nThe performance of the proposed approach was assessed using various metrics, including Intersection over Union (IoU), accuracy, sensitivity, and specificity. These metrics provided a comprehensive evaluation of the model's effectiveness in both segmentation and classification tasks. The segmentation model was trained using different optimizers and epochs, with the stochastic gradient descent with momentum (sgdm) optimizer and 40 epochs yielding the highest training accuracy. The classification experiment utilized ensemble methods, including boosted trees, bagged trees, and RUSBoosted trees, to achieve robust prediction outcomes. The results demonstrated the model's capability to accurately segment and classify COVID-19 affected lung regions in CT images.",
  "evaluation/measure": "In our evaluation, we employed several performance metrics to comprehensively assess the effectiveness of our proposed models. These metrics include the Intersection over Union (IoU), Accuracy, Sensitivity, and Specificity. IoU is a crucial metric for evaluating the segmentation performance, particularly in medical imaging, as it measures the overlap between the predicted and ground truth regions. Accuracy provides an overall measure of the correct predictions made by the model, while Sensitivity (also known as Recall) and Specificity focus on the model's ability to correctly identify positive and negative cases, respectively.\n\nThese metrics are widely used in the literature for evaluating segmentation and classification tasks in medical imaging. They provide a robust and representative set of measures to assess the performance of our models. The IoU is particularly important for segmentation tasks, as it directly measures the quality of the segmentation by comparing the predicted regions to the ground truth. Accuracy, Sensitivity, and Specificity are essential for classification tasks, as they provide insights into the model's performance across different classes.\n\nIn addition to these metrics, we also used cross-validation to ensure the robustness of our results. Specifically, we employed 10-fold cross-validation, which involves dividing the dataset into 10 subsets, training the model on 9 subsets, and testing it on the remaining subset. This process is repeated 10 times, with each subset serving as the test set once. This approach helps to mitigate overfitting and provides a more reliable estimate of the model's performance.\n\nOverall, the set of performance metrics we used is representative of the standards in the field and provides a comprehensive evaluation of our models' effectiveness in both segmentation and classification tasks.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of our proposed method with several publicly available methods using benchmark datasets. Specifically, we evaluated our approach against four recent frameworks that utilize different pretrained Convolutional Neural Network (CNN) models for classifying COVID-19 CT images. These frameworks include those by Burgos-Artizzu, Ewen and Khan, Wang et al., and Yang et al. The existing methods achieved a maximum accuracy of 0.89. In contrast, our method, which combines hand-crafted and deep features with prominent feature selection using entropy, demonstrated a significant improvement, achieving an accuracy of 0.9852 for the classification of COVID-19 lung CT images.\n\nOur evaluation involved benchmark datasets, including the COVID-19 segmentation dataset, the Chinese Hospital CT imaging dataset, and the POF Hospital dataset. These datasets provided a robust foundation for assessing the performance of our model against established baselines. The results showed that our ensemble learning classifiers, particularly the boosted tree and bagged tree models, outperformed the simpler baselines in terms of accuracy, sensitivity, and specificity.\n\nThe comparison highlighted the effectiveness of our approach in accurately segmenting and classifying COVID-19 affected lung regions. The fusion of hand-crafted and deep features, along with the use of ensemble learning techniques, proved to be more effective than relying solely on hand-crafted features or simpler CNN models. This comprehensive evaluation underscores the potential of our method for medical applications, particularly in the analysis of informative features for disease classification.",
  "evaluation/confidence": "In our study, we evaluated the performance of our proposed method using several benchmark datasets and reported various performance metrics such as accuracy, sensitivity, specificity, and IoU. However, we did not explicitly provide confidence intervals for these metrics. The results were obtained through cross-validation, specifically 10-fold cross-validation, which helps in assessing the model's performance and generalizability.\n\nThe statistical significance of our results was not explicitly tested against other methods or baselines in terms of formal statistical tests. However, the superior performance of our method, as indicated by the high accuracy and other metrics, suggests that it is effective. For instance, our method achieved an accuracy of 99.09% on the COVID-19 segmentation dataset using the Boosted Tree ensemble, which is notably higher than the accuracies reported by other recent studies.\n\nWhile we did not perform formal statistical significance tests, the consistent high performance across different datasets and the comparison with existing methods provide strong evidence of the superiority of our approach. The use of ensemble learning with hand-crafted and deep features fusion further enhances the robustness and reliability of our results.",
  "evaluation/availability": "The data that support the findings of this study are openly available in the covid-CT-dataset. This dataset can be accessed at the following URL: https://covid-19.conacyt.mx/jspui/handle/1000/4157. The availability of this dataset ensures that other researchers can replicate and build upon the results presented in this study."
}