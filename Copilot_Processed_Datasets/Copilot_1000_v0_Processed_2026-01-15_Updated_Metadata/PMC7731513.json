{
  "publication/title": "Machine learning and individual variability in electric field characteristics predict tDCS treatment response.",
  "publication/authors": "Albizu A, Fang R, Indahlastari A, O'Shea A, Stolte SE, See KB, Boutzoukas EM, Kraft JN, Nissim NR, Woods AJ",
  "publication/journal": "Brain stimulation",
  "publication/year": "2020",
  "publication/pmid": "33049412",
  "publication/pmcid": "PMC7731513",
  "publication/doi": "10.1016/j.brs.2020.10.001",
  "publication/tags": "- Transcranial direct current stimulation (tDCS)\n- Machine learning\n- Support vector machines (SVM)\n- Finite element modeling (FEM)\n- Working memory\n- Older adults\n- Cognitive training\n- Brain stimulation\n- Prognostic labeling\n- Behavioral response prediction\n- Electrode placement\n- Current intensity\n- Cross-validation\n- Neuroimaging\n- Statistical analysis",
  "dataset/provenance": "The dataset used in this study was sourced from a phase-II pilot clinical trial. This trial employed a randomized, triple-blinded design, ensuring that the assessor, interventionist, and participant were all unaware of the treatment assignments. The trial was registered under the identifier NCT02137122.\n\nThe dataset consists of structural imaging and behavioral data from fourteen healthy older adults who received active-tDCS stimulation. The participants had a mean age of approximately 73.57 years, with a standard deviation of 7.84 years. The group was balanced in terms of gender, with seven females and seven males. All participants were screened for eligibility based on specific inclusion criteria detailed in a prior manuscript.\n\nThe study protocol adhered to the Declaration of Helsinki and was approved by the University of Florida\u2019s Institutional Review Board. Informed written consent was obtained from all participants prior to any study procedures.\n\nThis dataset builds upon previous work from our research group, which demonstrated improvements in working memory in older adults following tDCS paired with cognitive training. The current study aims to apply machine learning and finite element modeling (FEM) to the same dataset to identify the key determinants of treatment response. Specifically, a Support Vector Machine (SVM) algorithm was used to investigate the contributions of current intensity and direction, as well as their interaction, for predicting working memory improvements.",
  "dataset/splits": "The dataset was split using a two-level nested stratified cross-validation procedure. This involved an outer cross-validation loop consisting of seven iterations, where in each iteration, six folds were used as training data and one fold was left out as test data. This resulted in seven data splits, each containing one test fold and six training folds.\n\nThe dataset consisted of fourteen participants, so each test fold contained data from two participants, while each training fold contained data from the remaining twelve participants. This approach ensured that each participant's data was used as test data exactly once and as training data six times.\n\nThe distribution of data points in each data split was balanced, with each test fold containing an equal number of responders and non-responders. This was achieved by stratifying the data based on the participant classes before splitting. The responder class had an average performance increase of 22%, while the non-responder class had an average increase of 9% in the two-back working memory task.",
  "dataset/redundancy": "The dataset used in this study was sourced from a phase-II pilot clinical trial with a randomized, triple-blinded design. The study involved fourteen healthy older adults who received active-tDCS stimulation. To ensure the robustness and generalizability of our machine-learning model, we employed a two-level nested cross-validation procedure. This approach involved splitting the data into K-folds, with K set to 7 in this case. The outer cross-validation loop consisted of K iterations, where in each iteration, K-1 folds were used as training data, and a single fold was left out as test data. This ensured that the training and test sets were independent in each iteration.\n\nWithin the outer loop, an inner cross-validation loop was performed on the training data to optimize the hyper-parameter C. This nested structure helped to avoid overfitting and provided a more reliable estimate of model performance. The decision function was then used to make predictions on the held-out test data. This method of cross-validation is a standard approach in the statistical literature and is widely used to provide unbiased generalizability to new data samples.\n\nThe distribution of our dataset compares favorably to previously published machine-learning datasets in the context of tDCS and cognitive training. However, it is important to note that the limited number of data points (n = 14) may affect the generalizability of the model. To address this, future work will use a larger and more heterogeneous tDCS clinical trial dataset (n = 160), which is near completion. This larger dataset will provide more robust validation of our findings and enhance the reliability of our model.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is Support Vector Machine (SVM). This is a well-established algorithm in the field of machine learning, known for its effectiveness in classification tasks. The specific implementation used is LIBSVM, which is a popular library for SVM.\n\nThe SVM algorithm is not new; it has been extensively studied and applied in various domains, including bioinformatics, image recognition, and text classification. The choice of SVM for this study is driven by its ability to handle high-dimensional data and its robustness in finding optimal hyperplanes that separate different classes.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of this study is on its application in the context of transcranial direct current stimulation (tDCS) and its effects on working memory in older adults. The primary contribution of this work lies in the novel application of SVM to analyze current density patterns derived from finite element models (FEM) to predict tDCS treatment responses. The study aims to provide insights into the critical components of tDCS dosing parameters, rather than introducing a new machine-learning algorithm.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The model is a Support Vector Machine (SVM) that directly uses current density values from MRI data to classify responders and non-responders to tDCS treatment. The SVM algorithm is used to find the optimal hyperplane that separates the two classes with the maximal margin. The training data consists of current density maps from participants, and the classes are determined by performance changes on a working memory task. The data points are independently and identically distributed, meeting the statistical assumptions required for the SVM algorithm. The model's performance is evaluated using a two-level nested stratified cross-validation procedure, which ensures that the training and test data are independent in each iteration. This approach helps to provide unbiased generalizability to new data samples and avoids overfitting.",
  "optimization/encoding": "Current density values were initially computed in each participant's native brain space. These values were then masked using individual participants\u2019 white and gray matter voxels to ensure that current values were restricted within brain regions only, thereby reducing the number of features submitted into the classifier. This masking step was crucial for focusing the analysis on relevant brain areas and minimizing irrelevant data.\n\nThe masked values were subsequently transformed into MNI (Montreal Neurological Institute) space using the University of Florida Aging Brain-587 tissue probability map and SPM\u2019s normalize function. This transformation step was essential for standardizing the data across participants, enabling more accurate comparisons and analyses. A quality check was performed by computing the median values of current density (J) before and after the transformation, confirming the high correlation (r = 0.998) and thus the reliability of the transformation process.\n\nFeature selection was performed on the training data to further reduce the number of trained features, addressing the high dimensionality of MRI data. One popular method used was filtering features via voxel-wise t-tests between classes to select current elements with a significant group-level difference (p < 0.01) as features for the subsequent prediction step. This step ensured that only the most relevant features were used in the machine-learning algorithm, improving its efficiency and accuracy.\n\nThe Support Vector Machine (SVM) algorithm was employed to classify responders from non-responders. SVM is a machine-learning algorithm that searches for the optimal hyperplane separating two classes with the maximal margin, assuming independently and identically distributed (iid) data. The LIBSVM tool was used to optimize the objective function, which included a penalty parameter on the training error. A linear kernel was generated to facilitate the classification process. Model performance was evaluated using ten permutations of two-level nested stratified cross-validation, ensuring robust and unbiased performance metrics. This involved splitting the data into K-folds (K = 7) and performing an outer cross-validation loop consisting of K iterations, with each iteration using K-1 folds for training and one fold for testing. An inner cross-validation loop was performed on the training data to optimize the hyper-parameter C. Predictions of held-out test data were made using the decision function derived from the training process.",
  "optimization/parameters": "In our study, the number of parameters used in the model was initially high due to the high dimensionality of MRI data. To manage this, feature selection was performed on the training data to reduce the number of trained features. This process involved filtering features via voxel-wise t-tests between classes to select current elements with a significant group-level difference (p < 0.01) as features for the subsequent prediction step. This method helped in identifying the most relevant features, thereby optimizing the number of parameters used in the model. The final model was trained on all fourteen current density maps to derive overall classification weights, which were then applied to independent data from new subjects to predict their tDCS response classification.",
  "optimization/features": "The study initially dealt with high-dimensional MRI data, which necessitated feature selection to reduce the number of trained features. This selection process was performed exclusively on the training data. One popular method employed for feature selection was filtering features via voxel-wise t-tests between classes. This approach aimed to select current elements with a significant group-level difference (p < 0.01) as features for the subsequent prediction step. The goal was to identify the most relevant features that could effectively distinguish between responders and non-responders to the treatment. By focusing on the training data, the feature selection ensured that the model's performance was evaluated on unseen data, thereby maintaining the integrity of the validation process. This method helped in mitigating the risk of overfitting and enhanced the model's generalizability to new, independent datasets.",
  "optimization/fitting": "The study utilized a Support Vector Machine (SVM) algorithm to classify treatment responders and non-responders based on current density values derived from finite element modeling (FEM). The number of parameters in the SVM model was indeed much larger than the number of training points, given the high dimensionality of MRI data. To address the risk of overfitting, a two-level nested cross-validation procedure was employed. This approach involved an outer cross-validation loop with K-folds (K=7), where K-1 folds were used for training and one fold was left out for testing. Within the training data, an inner cross-validation loop was performed to optimize the hyper-parameter C, ensuring that the model's performance was evaluated on independent test samples.\n\nAdditionally, 10 permutations of the cross-validation were conducted to assess the retest reliability of the models. The average accuracy and confidence intervals across these permutations were used to estimate model performance on novel datasets. The results showed a 95% confidence interval of [85.03\u201387.83%] with an average accuracy of 86.42%, indicating robust performance and a small standard deviation.\n\nTo further mitigate overfitting, feature selection was performed on the training data. Voxel-wise t-tests were used to filter features, selecting current elements with significant group-level differences (p < 0.01) for the prediction step. This reduced the number of trained features and helped in focusing on the most relevant data.\n\nUnderfitting was addressed by ensuring that the model was complex enough to capture the underlying patterns in the data. The use of a linear kernel in the SVM algorithm allowed for the identification of an optimal hyperplane that separates the two classes with maximal margin. The decision function was then applied to the held-out test data to make predictions.\n\nOverall, the combination of nested cross-validation, feature selection, and the use of a linear kernel in the SVM algorithm helped in balancing the trade-off between overfitting and underfitting, ensuring that the model generalized well to new data.",
  "optimization/regularization": "To prevent overfitting in our study, we employed two-level nested cross-validation. This technique helps to increase the number of independent test samples and assess the retest reliability of the models. By using nested cross-validation, we aimed to avoid overestimating the performance due to \"double-dipping\" in the choice of model hyperparameters. This method involves an outer loop for model validation and an inner loop for hyperparameter tuning, ensuring that the model's performance is evaluated on unseen data.\n\nAdditionally, we used 10 permutations to assess the retest reliability of these models. The average accuracy and confidence intervals across these permutations were used to estimate model performance on novel datasets. This approach provides a robust measure of the model's performance and helps to mitigate the risk of overfitting.\n\nFurthermore, we utilized feature selection techniques to reduce the dimensionality of the data and focus on the most relevant features. This step is crucial in preventing overfitting, especially when dealing with high-dimensional data like MRI scans. By selecting features that show significant group-level differences, we ensure that the model is trained on the most informative data points, enhancing its generalizability to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are not explicitly detailed in the provided text. However, it is mentioned that a linear kernel was generated for the Support Vector Machine (SVM) using a specific function, and the model performance was evaluated across ten permutations of two-level nested stratified cross-validation. The optimal hyper-parameter C was determined through an inner cross-validation loop.\n\nRegarding the availability of model files and optimization parameters, there is no information provided about where these can be accessed or under what license. The text does not mention any repositories or links where the model files or optimization parameters are stored.\n\nThe study does mention the use of specific tools and software, such as SPSS Statistics 25 and MATLAB 2019b, for statistical analyses, but it does not provide details on how to access the configurations or parameters used within these tools for our specific study.\n\nIn summary, while some details about the optimization process are provided, specific hyper-parameter configurations, optimization schedules, model files, and optimization parameters are not readily available or discussed in the text.",
  "model/interpretability": "The model employed in this study is not a black box. It utilizes a Support Vector Machine (SVM), which is inherently interpretable due to its linear nature when using a linear kernel. The SVM finds an optimal hyperplane that separates the classes of responders and non-responders, and the weights associated with this hyperplane can be examined to understand the contribution of each feature to the classification.\n\nThe feature weights generated by the SVM provide insights into which voxels (or brain regions) are most influential in predicting treatment response. These weights were separated into positive and negative contributions, indicating which regions are associated with responders and non-responders, respectively. The percent contribution of each voxel toward either positive or negative predictions was computed, allowing for a detailed analysis of the brain regions involved.\n\nAdditionally, the stability of these feature weights was assessed by plotting the standard deviation of feature weights across folds, ensuring that the model's predictions are consistent and reliable. Regions of interest (ROIs) were defined using the Harvard-Oxford atlas and ranked based on their average voxel percent contribution, highlighting specific brain regions that predict working memory improvements.\n\nThe top ten ranked ROIs, largely located in the frontal region, include the Right Superior Frontal Gyrus, Left Superior Frontal Gyrus, Right Middle Frontal Gyrus, and others. These regions were identified as critical for predicting treatment response, demonstrating the model's ability to provide transparent and interpretable results.",
  "model/output": "The model employed in this study is a classification model. Specifically, a Support Vector Machine (SVM) was used to distinguish between two discrete classes: responders and non-responders to transcranial direct current stimulation (tDCS). The SVM algorithm searches for the optimal hyperplane that separates these two classes with the maximal margin. This approach was chosen due to its effectiveness in handling high-dimensional data and its ability to perform well with a relatively small number of samples, which is crucial given the limited dataset of fourteen participants.\n\nThe model's performance was evaluated using a two-level nested stratified cross-validation approach, which helps to ensure that the results are robust and generalizable. The area under the receiver operating characteristic curve (AUC) was used as a primary metric to assess the model's ability to classify responders and non-responders. The AUC values for the different models (current intensity, current direction, and their interaction) were all above 50%, indicating that the models performed better than chance. The current intensity model had the highest AUC at 80.6%, suggesting it was the most effective in ranking responders higher than non-responders.\n\nThe overall accuracy of the SVM classification models was 86.43%, with a 95% confidence interval ranging from 85.03% to 87.83%. This high accuracy demonstrates the model's effectiveness in differentiating between responders and non-responders based on the distributed patterns of electric current. The use of feature selection techniques, such as voxel-wise t-tests, helped to reduce the dimensionality of the data and focus on the most relevant features for classification.\n\nIn summary, the model is a binary classifier that successfully predicts tDCS response classification with high accuracy, providing valuable insights into the critical components of stimulation response.",
  "model/duration": "The computational models of current density were constructed using the Realistic Volumetric-Approach to Simulate Transcranial Electric Stimulation (ROAST) toolbox. This process involved parallel processing on a high-performance cluster equipped with 50 CPU cores and 175 GB of RAM. The resampled T1 images were individually processed in parallel using ROAST, which allowed for efficient and timely computation. The segmentation process was carried out in FreeSurfer to classify tissue types into gray and white matter, and these segmentations were visually inspected and manually corrected for errors before reprocessing. This meticulous approach ensured the accuracy of the computational models. However, the exact execution time for the model to run is not specified.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a rigorous cross-validation procedure to ensure the robustness and generalizability of the results. Specifically, a two-level nested stratified cross-validation was used. This approach involved splitting the data into K-folds, with K set to 7. In each iteration of the outer cross-validation loop, K-1 folds were used for training, while one fold was held out for testing. Within the training data, an inner cross-validation loop was performed to optimize the hyper-parameter C, which is crucial for the Support Vector Machine (SVM) model.\n\nThe performance of the model was assessed by comparing the predicted labels of all subjects against the ground truth labels. A receiver operating characteristic (ROC) curve was plotted to demonstrate the separability of classes within each model, and the area under the ROC curve (AUC) was calculated. This metric provides a comprehensive measure of the model's ability to distinguish between responders and non-responders.\n\nTo further validate the stability and reliability of the feature weights, the standard deviation of feature weights across folds was plotted. This analysis helps in understanding the consistency of the model's predictions across different subsets of the data. Additionally, a final model was trained on all fourteen current density maps to derive overall classification weights. These weights can be applied to independent data from new subjects to predict their tDCS response classification.\n\nThe study also addressed potential limitations, such as the small sample size, by using permutations and cross-validated analyses. The average accuracy and confidence intervals across these permutations were used to estimate model performance on novel datasets. The results showed a 95% confidence interval of 85.03\u201387.83% when the average accuracy was 86.42%, indicating robust performance and a small standard deviation of the classification performance.\n\nIn summary, the evaluation method involved a comprehensive cross-validation procedure, ROC curve analysis, and stability assessments of feature weights. These steps ensured that the model's performance was thoroughly validated and that the results were generalizable to new data.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our Support Vector Machine (SVM) model in classifying treatment responders and non-responders based on current density maps. The primary metrics reported include:\n\n* **Average Test Accuracy (ACC)**: This metric represents the proportion of correctly classified instances out of the total instances. We reported an average test accuracy of 86.46% with a 95% confidence interval ranging from 85.03% to 87.83%. This indicates that our model correctly classified approximately 86.46% of the subjects in the test set.\n\n* **Area Under the ROC Curve (AUC)**: The AUC provides a measure of the model's ability to distinguish between the two classes (responders and non-responders). We calculated the AUC for three different data types: intensity, direction, and the combination of direction and intensity. The AUC values were 0.806, 0.776, and 0.749, respectively. These values suggest that the model has a good ability to separate the classes, with the intensity data type performing slightly better.\n\n* **F1 Score (F1)**: The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. For all three data types, the F1 score was 0.86. This indicates a good balance between precision and recall in our model's predictions.\n\n* **Matthews Correlation Coefficient (MCC)**: The MCC is a balanced measure that takes into account true and false positives and negatives. The MCC was 0.73 for all three data types, indicating a strong correlation between the predicted and actual classifications.\n\nThese metrics are widely used in the literature for evaluating binary classification models, making our results comparable to other studies in the field. The reported metrics provide a comprehensive evaluation of our model's performance, covering aspects such as overall accuracy, class separability, and balanced performance. The use of nested cross-validation further ensures that our performance estimates are robust and generalizable to new, unseen data.",
  "evaluation/comparison": "A comparison to simpler baselines was performed to evaluate the necessity of the original model's complexity. Specifically, simplified Support Vector Machine (SVM) models were assessed to determine if they could achieve sufficient performance without the full complexity of the original model. One of these simplified models used only electrode displacement as a feature for classifying responders and non-responders. Another simplified model utilized current characteristics exclusively from gray matter regions. These simplified models did not outperform the original model, indicating that the additional complexity in the original model is beneficial for achieving the observed performance. This evaluation helps to validate the robustness and necessity of the features and methods used in the original model.",
  "evaluation/confidence": "The evaluation of our model's performance includes confidence intervals for the metrics reported. Specifically, the average test accuracy has a 95% confidence interval of [85.03\u201387.83%], indicating a robust performance with a small standard deviation. This confidence interval provides a range within which the true accuracy is likely to fall, giving a measure of the precision of our accuracy estimate.\n\nStatistical significance was assessed using various methods. One-way ANOVA was used to compare the mean differences in model performance between different data types (direction, intensity, and direction x intensity). The results showed no significant difference among these data types. Additionally, Pearson\u2019s correlation analyses and linear regression were employed to determine the relationship between behavioral responses and field characteristics within essential voxels. Effect sizes were computed using Hedge\u2019s g, which is corrected for small sample bias.\n\nTo ensure the normality of the data, the Kolmogorov-Smirnov test was applied. The null hypothesis of a normal distribution was not rejected for any of the variables analyzed, confirming that the data met the assumptions required for statistical testing.\n\nThe use of two-level nested cross-validation helped to increase the number of independent test samples and assess the retest reliability of the models. This approach, along with 10 permutations, provided a comprehensive evaluation of model performance on novel datasets. The results demonstrate that the model's performance is reliable and generalizable, although validation on a larger, independent cohort is recommended to confirm these findings.",
  "evaluation/availability": "Not enough information is available."
}