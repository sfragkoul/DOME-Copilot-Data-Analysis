{
  "publication/title": "Unsupervised Learning for Spherical Surface Registration.",
  "publication/authors": "Zhao F, Wu Z, Wang L, Lin W, Xia S, Shen D, Li G,",
  "publication/journal": "Machine learning in medical imaging. MLMI (Workshop)",
  "publication/year": "2020",
  "publication/pmid": "33569552",
  "publication/pmcid": "PMC7871893",
  "publication/doi": "10.1007/978-3-030-59861-7_38",
  "publication/tags": "- Deep Learning\n- Spherical Surface Registration\n- Cortical Surface\n- Neuroimaging\n- Unsupervised Learning\n- Medical Image Registration\n- Pediatric Subjects\n- Cortical Parcellation\n- Spherical Demons\n- MSM\n- Cortical Features\n- Atlas-Based Registration\n- Computational Pipeline\n- Surface Mapping\n- Geometric Features\n- Registration Performance\n- Mean Absolute Error\n- Correlation\n- Dice Coefficient\n- Run-time Efficiency",
  "dataset/provenance": "The dataset used in our study consists of cortical surfaces reconstructed from 102 pediatric subjects. These surfaces were processed using iBEAT V2.0 Cloud, an online computational pipeline specifically designed for infants. The surfaces were then mapped onto a sphere using FreeSurfer. Each vertex on these surfaces was coded with two geometric features: 'sulc' (average convexity) and 'curv' (mean curvature). Additionally, a parcellation map was obtained and manually corrected to remove any obvious errors.\n\nAn atlas was constructed from 83 other subjects of similar ages by co-registering them using Spherical Demons. This atlas served as a reference for the registration tasks performed in our experiments. The dataset was used to compare our method with two popular surface registration methods: Spherical Demons and MSM. The evaluation metrics included mean absolute error (MAE), correlation, and Dice of parcellation results, which were assessed by comparing the registered surfaces to ensure they were in the same spherical space.",
  "dataset/splits": "There are two data splits: a training set and a validation set. The training set consists of 70 surfaces, while the validation set contains 32 surfaces. The distribution of data points in each split is not explicitly detailed, but the registration results of both sets verified that there is no significant difference between them.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not applicable.",
  "optimization/algorithm": "The optimization algorithm employed in our work is based on deep learning, specifically utilizing a class of models known as Convolutional Neural Networks (CNNs). The core architecture used is the Spherical U-Net, which is an adaptation of the traditional U-Net designed to operate on spherical data. This architecture is not entirely new but has been adapted and extended for the specific task of spherical surface registration.\n\nThe reason this work was not published in a machine-learning journal is that the primary focus of our research is on its application in neuroimaging, particularly in the context of cortical surface registration. The innovation lies in how we have adapted and utilized existing deep learning techniques to solve a specific problem in medical imaging, rather than in the development of a entirely new machine-learning algorithm. Our contributions are more aligned with the advancements in neuroimaging techniques and the application of deep learning in this field, which is why it was published in a medical imaging journal.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The method employs three orthogonal Spherical U-Nets to learn the registration function. These subnetworks are trained independently on different orientations of the spherical surface. The training data for each subnetwork consists of the original and rotated versions of the spherical surfaces, ensuring that the data is independent across the different orientations. The final deformation field is obtained by fusing the outputs of these three subnetworks. This approach helps to address the issue of discontinuity at the poles, which is a common problem in spherical surface registration.",
  "optimization/encoding": "The data encoding process involved representing each cortical surface with two geometric features at each vertex: 'sulc' (average convexity) and 'curv' (mean curvature). These surfaces were initially reconstructed using an online computational pipeline and then mapped onto a sphere. Additionally, a parcellation map was obtained and manually corrected to ensure accuracy. The feature values were normalized to fall within the range of [-1, 1] before being input into the machine-learning algorithm. This normalization step is crucial for stabilizing and accelerating the training process. The surfaces were discretized using icosahedron subdivisions, which allowed for a detailed and structured representation of the cortical geometry. This encoding method ensured that the data was in a suitable format for the registration tasks performed by the algorithm.",
  "optimization/parameters": "In our method, the number of parameters used in the model is determined by the architecture of the Spherical U-Net, which is based on the standard U-Net architecture adapted for spherical surfaces. The input to each subnetwork consists of two channels, representing the geometric features of the surfaces. The output is a deformation field represented by tangent vectors, also with two channels.\n\nThe specific number of parameters in the model is not explicitly stated, as it depends on the details of the Spherical U-Net architecture, such as the number of layers and the number of filters in each layer. However, the model is implemented using PyTorch, and the parameters are learned using the Adam optimizer with a learning rate that decreases from 1e\u22123 to 1e\u22126 over 100 epochs.\n\nThe selection of the number of parameters was not explicitly detailed, but it is likely based on standard practices in deep learning for similar tasks. The use of a multi-level, coarse-to-fine registration approach, with four levels of icosahedron subdivisions, allows the model to capture both global and local deformations effectively. The regularization parameters, such as \u03bbcc and \u03bbcon, were set to specific values to balance the similarity and smoothness of the deformation field. The exact values of these parameters were chosen to optimize the performance of the model on the validation set.",
  "optimization/features": "The input features used in our method are geometric features of the cortical surfaces. Specifically, two features are used at each vertex: 'sulc' (average convexity) and 'curv' (mean curvature). These features are mapped onto the sphere using FreeSurfer.\n\nFeature selection was not explicitly performed in the traditional sense, as these geometric features are standard in cortical surface analysis. The choice of these features is based on established practices in the field and their relevance to the registration task.\n\nThe features were obtained and manually corrected to remove obvious errors before being used in the registration process. This correction process was done independently of the training set, ensuring that the validation and training sets were treated equally.",
  "optimization/fitting": "The fitting method employed in our study involves a multi-level registration approach using spherical U-Nets and transform layers. The number of parameters in our models is indeed larger than the number of training points, which is a common scenario in deep learning. To address potential overfitting, we implemented several strategies.\n\nFirstly, we used a consistent weighting function in the loss function to disregard the influences of vertices near the poles, which helps in focusing the learning on more relevant areas of the spherical surface. Additionally, we enforced consistency among the three orthogonal spherical U-Nets, ensuring that the deformation fields learned by different subnetworks align well in their overlapping regions. This consistency term in the loss function helps in regularizing the model and prevents it from learning spurious patterns.\n\nTo further mitigate overfitting, we employed data augmentation techniques and used a validation set to monitor the model's performance during training. The validation set, consisting of 32 surfaces, allowed us to tune hyperparameters and select the best model based on its generalization performance rather than just its performance on the training data.\n\nRegarding underfitting, we ensured that our models had sufficient capacity to learn the complex registration task. We used a coarse-to-fine multi-level approach, starting with lower-resolution surfaces and progressively refining the registration at higher resolutions. This hierarchical approach allows the model to capture both global and local deformations effectively. Moreover, we used an Adam optimizer with a learning rate schedule that decreases from 1e\u22123 to 1e\u22126 over 100 epochs, ensuring that the model has enough time to converge to a good solution.\n\nThe use of three orthogonal spherical U-Nets also helps in capturing the spherical nature of the data more effectively, reducing the risk of underfitting. The final warped surface is obtained by fusing the deformation fields from the three subnetworks, which provides a more robust and accurate registration result.",
  "optimization/regularization": "In our optimization process, we employed several regularization techniques to prevent overfitting and ensure the robustness of our model. One key aspect was the use of a multi-level coarse-to-fine registration approach, which helped in capturing both global and local deformations effectively. This hierarchical strategy allowed the model to learn more general features at coarser levels and refine them at finer levels, reducing the risk of overfitting to specific details.\n\nAdditionally, we incorporated smoothness constraints into our objective function. These constraints ensured that the deformation fields were smooth and realistic, preventing the model from learning arbitrary, high-frequency deformations that could lead to overfitting. The smoothness term in our loss function penalized abrupt changes in the deformation field, promoting more natural and biologically plausible registrations.\n\nWe also utilized data augmentation techniques by rotating the spherical surfaces to three orthogonal orientations. This approach helped the model to learn rotation-invariant features, making it more robust and less likely to overfit to specific orientations of the data. By training three subnetworks independently for these orientations, we further enhanced the model's ability to generalize across different views of the spherical surfaces.\n\nMoreover, we employed an Adam optimizer with a learning rate schedule that decreased from 1e\u22123 to 1e\u22126 over 100 epochs. This learning rate decay helped in stabilizing the training process and preventing the model from converging too quickly to suboptimal solutions, which could otherwise lead to overfitting.\n\nLastly, we used a validation set consisting of 32 surfaces to monitor the model's performance during training. This allowed us to tune the regularization parameters and other hyperparameters effectively, ensuring that the model generalized well to unseen data. The consistent performance on both training and validation sets indicated that our regularization methods were effective in preventing overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our experiments are detailed within the publication. Specifically, the learning rate, number of epochs, and other training parameters are explicitly mentioned. For instance, the Adam optimizer was used with a learning rate that decreased from 1e\u22123 to 1e\u22126 over 100 epochs. The regularization parameters \u03bbcc, \u03bbcon, and \u03bbs are also specified for different levels of the registration process.\n\nThe model files and optimization parameters are not directly provided in the publication. However, the implementation details, including the architecture of the Spherical U-Net and the training procedure, are described comprehensively. This information allows for reproducibility of the experiments. The code for the Spherical Demons and MSM methods, which were used for comparison, is available through their respective official releases. The Spherical Demons code can be accessed via a provided link, and the MSM algorithm was used from the fsl/6.0.0 package.\n\nRegarding the availability and licensing, the publication does not specify the licensing terms for the code or data used. However, it is standard practice in academic research to make code and data available under open-source licenses or upon request. For the specific details on licensing, one would need to refer to the repositories or contact the authors directly.",
  "model/interpretability": "The model presented in this work is not a blackbox. It leverages a deep learning-based approach using Spherical U-Nets, which inherently provides some level of interpretability due to the structured nature of the network architecture. The model's operations, including the spherical transform layer and loss functions, are designed to be differentiable, allowing for backpropagation and gradient-based optimization. This differentiability ensures that the model's decisions can be traced back through the network layers, providing insights into how inputs are transformed into outputs.\n\nOne key aspect of the model's transparency is the use of tangent vector fields to represent deformations. These tangent vectors are predicted by the Spherical U-Net and are used to map vertices on the spherical surface. The relationship between the tangent vectors and the spherical deformations is well-defined, making it possible to understand how the model adjusts the surface geometry.\n\nAdditionally, the model incorporates a weighting function in the loss function to handle vertices near the poles, which are challenging regions due to the gauge variance. This weighting function explicitly disregards the influences of vertices near the poles, providing a clear mechanism for managing these difficult areas.\n\nThe consistency term in the loss function further enhances interpretability by enforcing agreement among the deformations predicted by different subnetworks. This term ensures that overlapping vertices trained by different subnetworks maintain consistent deformations, which can be visually inspected and verified.\n\nOverall, the model's design, including the use of differentiable operations, tangent vector fields, and explicit weighting functions, contributes to its transparency and interpretability. These features allow researchers to understand and validate the model's behavior, making it a more interpretable approach compared to traditional blackbox models.",
  "model/output": "The model presented in this work is a regression model. It is designed for spherical cortical surface registration, which involves predicting a deformation field that maps vertices on a moving surface to an atlas surface. This deformation field is represented by tangent vectors, indicating the direction and magnitude of displacement for each vertex. The model uses three orthogonal Spherical U-Nets to predict separate deformation fields, which are then fused to obtain the final warped surface. The objective of the model is to minimize a loss function that measures surface feature similarity and imposes smoothness on the deformation field. Therefore, the output of the model is a continuous deformation field, making it a regression task rather than a classification task.",
  "model/duration": "The execution time of our model is significantly faster compared to traditional methods. It takes approximately 17 seconds to register a pair of surfaces. This is a substantial improvement over existing methods like Spherical Demons, which takes around 1.5 minutes, and MSM, which takes about 8 minutes. The efficiency of our model is due to its learning-based framework, which allows it to quickly compute the deformation field using pre-learned parameters. This makes our method highly suitable for large-scale neuroimaging studies, where registering hundreds or thousands of surfaces to an atlas is required.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The method was evaluated using a dataset consisting of 102 pediatric subjects. The cortical surfaces of these subjects were reconstructed and mapped onto a sphere. Each surface was encoded with two geometric features: 'sulc' (average convexity) and 'curv' (mean curvature). A parcellation map was also obtained and manually corrected.\n\nThe evaluation involved registering the 102 moving surfaces to a fixed atlas surface constructed from 83 other subjects of similar ages. The performance of the registration was assessed using three metrics: mean absolute error (MAE), correlation, and Dice coefficient of parcellation results. These metrics were evaluated by comparing any two registered surfaces, as all surfaces should align in the same spherical space after registration.\n\nThe method was compared with two popular surface registration techniques: Spherical Demons and MSM. The official codes of these methods were used, and they were run with specific parameters to ensure a fair comparison. The experiments were conducted on an Intel Core i7\u20138700 CPU.\n\nThe evaluation also included a visual inspection of the registration results, focusing on the alignment of specific brain regions and the overall folding patterns. Group-average maps and representative results were generated to visually compare the performance of the different methods. Additionally, the run-time of each method was recorded to assess their efficiency.",
  "evaluation/measure": "In our evaluation of cortical surface registration methods, we employed several key performance metrics to comprehensively assess the accuracy and effectiveness of the registration processes. These metrics include mean absolute error (MAE), correlation, and Dice coefficient of parcellation results. These metrics are widely recognized and used in the literature for evaluating surface registration methods.\n\nThe mean absolute error (MAE) measures the average magnitude of errors between the registered surfaces and the fixed atlas surface, providing a direct indication of registration accuracy. Lower MAE values signify better alignment between the surfaces.\n\nCorrelation metrics assess the similarity in geometric features between the registered surfaces and the atlas. We specifically evaluated the correlation of two geometric features: 'sulc' (average convexity) and 'curv' (mean curvature). Higher correlation values indicate better preservation of these geometric features during the registration process.\n\nThe Dice coefficient measures the overlap between parcellation maps of the registered surfaces and the atlas. A higher Dice coefficient indicates better alignment of anatomical regions, which is crucial for accurate neuroimaging analyses.\n\nThese metrics collectively provide a robust evaluation framework, ensuring that our method is compared fairly against established techniques. The use of these metrics aligns with standard practices in the field, making our results comparable to other studies in the literature.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our method with two widely used surface registration techniques: Spherical Demons and MSM. This comparison was performed on a dataset consisting of 102 pediatric subjects, with cortical surfaces reconstructed using iBEAT V2.0 Cloud and mapped onto the sphere using FreeSurfer. Each surface was encoded with two geometric features: 'sulc' (average convexity) and 'curv' (mean curvature), along with a parcellation map that was manually corrected.\n\nWe evaluated the performance of these methods using three key metrics: mean absolute error (MAE), correlation, and Dice of parcellation results. These metrics were assessed by comparing any two registered surfaces, ensuring that all surfaces were aligned in the same spherical space post-registration.\n\nFor Spherical Demons, we utilized the official code with default parameters, registering surfaces at four levels of icosahedron subdivisions. For MSM, we optimized the parameters specifically for our dataset, adjusting the regularization and smoothing terms to achieve better performance.\n\nOur method was implemented using PyTorch and followed a coarse-to-fine multi-level registration approach, similar to Spherical Demons and MSM. We trained four models at different levels to align the 'sulc' and 'curv' features, using an Adam optimizer with a decreasing learning rate. The feature values were normalized between [-1, 1], and we employed specific loss functions to enhance the registration accuracy.\n\nThe results of our comparison are presented in Table 1, which shows that our method achieves superior performance in terms of sulc alignment and overall run-time efficiency. While MSM performed better in curv alignment, our method demonstrated sharper folding patterns in group-average maps and better alignment with the atlas in specific brain regions, such as the postcentral gyrus boundary.\n\nIn summary, our evaluation involved a direct comparison with publicly available and widely used methods on a benchmark dataset, providing a comprehensive assessment of our method's effectiveness and efficiency.",
  "evaluation/confidence": "The evaluation of our method includes performance metrics with confidence intervals, providing a measure of variability and reliability. For instance, the correlation of sulc is reported as 0.7359\u00b10.0526, indicating the mean correlation value and its standard deviation. Similarly, other metrics such as the mean absolute error (MAE) of sulc and curv, as well as the Dice coefficient, are presented with their respective confidence intervals. These intervals help in understanding the precision of the estimates and the consistency of the results across different subjects.\n\nStatistical significance is crucial for claiming the superiority of our method over others. While the exact p-values are not explicitly stated, the reported confidence intervals and the comparison with baseline methods suggest that the differences in performance are meaningful. For example, our method achieves a higher correlation of sulc (0.7359\u00b10.0526) compared to Spherical Demons (0.6982\u00b10.0521) and MSM (0.7103\u00b10.0431), indicating a likely statistically significant improvement. Similarly, the lower MAE of sulc (2.3730\u00b10.3362) and higher Dice coefficient (0.7903\u00b10.0566) further support the claim of superior performance.\n\nThe visual inspections and group-average maps also corroborate these quantitative results, showing sharper folding patterns and better alignment in critical regions. The run-time advantage of our method, being significantly faster than the baseline methods, adds another layer of confidence in its practical applicability and efficiency. Overall, the combination of quantitative metrics with confidence intervals, visual evidence, and run-time efficiency provides a robust basis for claiming the superiority of our method.",
  "evaluation/availability": "Not enough information is available."
}