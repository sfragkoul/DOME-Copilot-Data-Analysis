{
  "publication/title": "Establishment of a predictive model for spontaneous preterm birth in primiparas with grade A1 gestational diabetes mellitus.",
  "publication/authors": "Sun T, Zhang Y, Xie C, Teng A, Lin S, Zhang H, Li Y",
  "publication/journal": "Frontiers in global women's health",
  "publication/year": "2025",
  "publication/pmid": "40115385",
  "publication/pmcid": "PMC11922705",
  "publication/doi": "10.3389/fgwh.2025.1496085",
  "publication/tags": "- Preterm Birth\n- Gestational Diabetes Mellitus\n- Random Forest Algorithm\n- Prediction Model\n- Primiparas\n- Spontaneous Preterm Birth\n- Risk Factors\n- Maternal Health\n- Obstetrics\n- Clinical Prediction\n- GDM Classification\n- Pregnancy Complications\n- Maternal Characteristics\n- Machine Learning in Medicine\n- Pregnancy Outcomes\n- Gestational Age\n- Family History of Preterm Birth\n- Assisted Reproductive Technology\n- Premature Rupture of Membranes\n- Cervical Length",
  "dataset/provenance": "The dataset used in this study was sourced from electronic medical records. A total of 1,282 participants were initially recruited. After excluding those with cervical insufficiency and indicated preterm birth, 1,229 primiparas were included in the final analysis. Of these, 142 had preterm birth (PB) and the remaining had full-term birth (FTB).\n\nThe data used in this study included various clinical variables such as gestational age, age, height, pre-pregnancy and prenatal weight, cervical length (CL) measured by transvaginal ultrasound, fasting blood glucose (FBG) levels in the first trimester, trimester in which gestational diabetes mellitus (GDM) was diagnosed, assisted reproductive technology (ART) use, ureaplasma urealyticum (UU) infection, Group B Streptococcus (GBS) infection, hypertensive disorders of pregnancy (HDP), subclinical hypothyroidism (SCH), intrahepatic cholestasis of pregnancy (ICP), premature rupture of membranes (PROM), smoking in primipara or spouse, and family history of PB and diabetes mellitus (DM).\n\nThis dataset has not been used in previous papers or by the community. The study is retrospective and had a small sample size. Future work will involve collecting multi-center and different population data to verify the results of the model, which can improve the reliability of the results.",
  "dataset/splits": "The dataset was split into two parts: a training set and a validation set. The training set consisted of 70% of the data, while the validation set comprised 30%. This split was done randomly to ensure that the model could be trained and validated effectively. The total number of participants in the study was 1,229, so approximately 860 data points were used for training, and around 369 data points were used for validation. This approach helped in assessing the model's performance and ensuring its robustness.",
  "dataset/redundancy": "The dataset used in this study consisted of 1,282 participants, which was reduced to 1,229 after excluding those with cervical insufficiency and indicated preterm birth. The final dataset was split into two groups: those who experienced spontaneous preterm birth (SPB) and those who had full-term birth (FTB). The SPB group consisted of 142 participants, while the FTB group had 1,087 participants.\n\nTo ensure the robustness and reliability of the prediction model, the dataset was divided into a training set and a validation set. The training set comprised 70% of the data, while the validation set included the remaining 30%. This split was performed randomly to maintain the independence of the training and test sets. Additionally, repeated fitting was employed, involving ten random cross-iterations during model construction. This process helped evaluate the stability and consistency of the prediction models by comparing their performance metrics, such as AUC, accuracy, sensitivity, and specificity.\n\nThe distribution of the dataset was designed to reflect real-world conditions, focusing on primiparas with grade A1 gestational diabetes mellitus (GDM). This specific population was chosen due to the higher incidence of spontaneous preterm birth in GDM women compared to the general population. The dataset's structure and splitting method aimed to provide a comprehensive and reliable foundation for developing a predictive model for spontaneous preterm birth in this high-risk group.",
  "dataset/availability": "The original contributions presented in the study are included in the article and supplementary material. Further inquiries can be directed to the corresponding author. The data is not publicly available in a forum. The data is available upon request to the corresponding author. The data is not released under a specific license. The data is not enforced to be released publicly.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is the random forest algorithm. This algorithm is not new; it has been widely used in various fields, including disease risk prediction, early warning, and prognosis, due to its accuracy.\n\nThe random forest algorithm was chosen for this study because it is effective in handling complex datasets and can provide robust predictions. It was not published in a machine-learning journal because the focus of this research is on its application in predicting spontaneous preterm birth (SPB) in primiparas with grade A1 gestational diabetes mellitus (GDM). The study aims to identify risk factors and develop a predictive model tailored to this specific medical context, rather than introducing a novel machine-learning algorithm. The random forest algorithm's ability to manage multiple variables and provide reliable predictions makes it suitable for this medical research.",
  "optimization/meta": "The model described in this publication does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The model is a random forest algorithm, which is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n\nThe random forest algorithm used in this study involves parameters such as ntree and mtry. The data of 70% of the participants were extracted by the bootstrap method to establish a random forest training model. Different random forest models with varying parameters were created and verified on a validation set. The model with the smallest prediction error was selected, which occurred when mtry was set to 17.\n\nThe study employed internal validation and repeated fitting to validate the random forest model. Internal validation involved randomly dividing the complete dataset into a training set (70%) and a validation set (30%). Repeated fitting involved performing ten random cross-iterations during model construction to evaluate the stability of the prediction model. This process ensured that the results of the ten prediction models were consistent in terms of fitting effect, using metrics such as AUC, accuracy, sensitivity, and specificity.\n\nThe training data used in this study is independent. The participants were recruited and then divided into training and validation sets randomly. This ensures that the model's performance is generalizable and not overfitted to a specific subset of data.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the data was suitable for analysis. Continuous variables were described using means and standard deviations, while categorical variables were described using adoption rates or component ratios. This descriptive statistics approach helped in understanding the distribution and characteristics of the data.\n\nFor continuous variables, differences between groups were assessed using either a t-test or the Wilcoxon rank sum test, depending on the distribution of the data. Categorical variables were compared using the chi-square test or Fisher's exact test. These statistical methods were employed to identify significant differences between groups, with a P-value of less than 0.05 indicating statistical significance.\n\nThe random forest algorithm was used to predict preterm birth, focusing on variables obtained from clinical data. Predictors that were highly consistent with the predicted outcome, such as the delivery week, were excluded initially. Postpartum-related predictors, like newborn weight and sex, were also excluded to avoid bias. A total of 19 variables were selected for predicting the occurrence of preterm birth.\n\nDuring the construction of the random forest model, various parameters were tuned to achieve the highest possible prediction accuracy. The model parameters, including the number of features and the number of decision trees, were adjusted to minimize error. This process ensured that the model was optimized for predicting preterm birth accurately.\n\nThe validation of the random forest model involved internal validation and repeated fitting. The data set was randomly divided into a training set (70%) and a validation set (30%). Repeated fitting involved performing ten random cross-iterations during model construction. This approach evaluated the stability and consistency of the prediction models by comparing their performance metrics, such as AUC, accuracy, sensitivity, and specificity. This method also facilitated the selection of the prediction model with the highest sensitivity while maintaining high accuracy.",
  "optimization/parameters": "In the optimization process of our model, we utilized a total of 19 variables as input parameters. These parameters were carefully selected to predict the occurrence of preterm birth in primiparas with grade A1 gestational diabetes mellitus (GDM). The selection process involved excluding predictors that were highly consistent with the predicted outcome, such as the delivery week, and postpartum-related predictors like newborn weight and newborn sex. The remaining variables were chosen based on their relevance and potential impact on preterm birth.\n\nTo determine the optimal number of parameters, we constructed random forest models with different values of the model parameters, specifically the number of features (mtry) and the number of decision trees (ntree). Through this iterative process, we identified the combination of parameters that yielded the least error, thereby maximizing the accuracy of our prediction model. The final model parameters were selected based on their performance in minimizing prediction errors and ensuring high model accuracy.",
  "optimization/features": "The study utilized a total of 19 variables as input features for predicting the occurrence of preterm birth. Feature selection was performed to exclude predictors that were highly consistent with the predicted outcome, such as the delivery week, and postpartum-related predictors, like newborn weight and newborn sex. The selection process ensured that only relevant and independent risk factors were included in the model. The feature selection was conducted using the entire dataset before splitting it into training and validation sets, ensuring that the training set was not biased by the selection process.",
  "optimization/fitting": "The fitting method employed in this study utilized the random forest algorithm to predict preterm birth in primiparas with grade A1 gestational diabetes mellitus (GDM). The dataset consisted of 1,229 participants, with 142 experiencing preterm birth and the remaining having full-term births. The number of predictors used was 19, which is relatively small compared to the number of training points, thus mitigating the risk of overfitting.\n\nTo ensure the robustness of the model, a combination of internal validation and repeated fitting was used. The dataset was randomly divided into a training set (70%) and a validation set (30%). This process was repeated ten times through random cross-iterations, allowing for the evaluation of the model's stability and consistency. The performance metrics, including the area under the curve (AUC), accuracy, sensitivity, and specificity, were compared across the ten models to ensure high predictive accuracy and sensitivity.\n\nThe random forest algorithm's parameters, specifically the number of features (mtry) and the number of decision trees (ntree), were optimized to minimize prediction error. It was found that when mtry was set to 17, the prediction model achieved the smallest error. This parameter tuning helped in balancing the model complexity, preventing both overfitting and underfitting. The decision trees within the random forest were designed to completely separate the predicted results, ensuring that the model captured the underlying patterns in the data without being overly simplistic or overly complex.\n\nThe validation results showed that the AUC values ranged from 0.853 to 0.932, indicating a high overall prediction performance. The sensitivity and specificity of the model were 0.634 and 0.976, respectively, demonstrating the model's ability to correctly identify both positive and negative cases. These metrics collectively suggest that the model is well-fitted to the data, neither overfitting nor underfitting, and is reliable for predicting preterm birth in the specified population.",
  "optimization/regularization": "In our study, we employed the random forest algorithm to predict preterm birth in primiparas with grade A1 gestational diabetes mellitus (GDM). To prevent overfitting, we utilized several techniques inherent to the random forest method.\n\nFirstly, the random forest algorithm itself is designed to reduce overfitting by creating multiple decision trees during training and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees. This ensemble approach helps to average out the errors made by individual trees, leading to a more robust and generalizable model.\n\nSecondly, we used the bootstrap method to create multiple subsets of the data, ensuring that each tree in the forest is trained on a different subset. This technique, known as bagging (bootstrap aggregating), further helps to reduce overfitting by providing each tree with a diverse set of training examples.\n\nAdditionally, we optimized the model parameters, specifically the number of features (mtry) and the number of decision trees (ntree), to achieve the highest prediction accuracy. By tuning these parameters, we ensured that the model was neither too complex (leading to overfitting) nor too simple (leading to underfitting).\n\nFurthermore, we validated the model using internal validation and repeated fitting. The internal validation involved randomly dividing the complete dataset into training and validation sets, with the training set comprising 70% of the data and the validation set comprising 30%. Repeated fitting involved performing ten random cross-iterations during model construction to evaluate the stability and consistency of the prediction models.\n\nThese techniques collectively helped to mitigate the risk of overfitting and ensured that our prediction model was reliable and generalizable.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in the study are detailed within the publication. Specifically, the random forest model parameters, such as the number of features (mtry) and the number of decision trees (ntree), were tuned to minimize prediction error. The optimal value for mtry was found to be 17, which resulted in the smallest prediction error.\n\nThe model files and optimization parameters are not explicitly provided in the publication. However, the methods and results sections offer comprehensive details on the model construction and validation processes. This includes the use of internal validation and repeated fitting techniques to ensure the robustness and stability of the prediction model.\n\nRegarding availability and licensing, the original contributions presented in the study are included in the article and supplementary material. Further inquiries about the data can be directed to the corresponding author. The study was conducted in accordance with ethical guidelines and institutional requirements, ensuring the integrity and reproducibility of the research.\n\nFor those interested in replicating or building upon this work, the detailed methodology and results provided in the publication serve as a solid foundation. The emphasis on transparency and reproducibility is evident in the thorough description of the statistical analyses and model validation processes.",
  "model/interpretability": "The model employed in this study is a random forest algorithm, which is generally considered a \"black box\" model due to its complexity and the ensemble nature of decision trees. However, random forest models do offer some level of interpretability through feature importance rankings. This allows for the identification of key factors influencing the predictions.\n\nIn our study, the importance ranking of influencing factors for spontaneous preterm birth (SPB) in primiparas with grade A1 gestational diabetes mellitus (GDM) was determined. Factors such as cervical length (CL) in the second trimester, family history of preterm birth, pre-pregnancy weight, prenatal weight, assisted reproductive technology (ART), premature rupture of membranes (PROM), and fasting blood glucose (FBG) in the first trimester were identified as significant predictors. These rankings provide insights into which variables are most influential in the model's predictions, making it somewhat transparent in terms of understanding the key drivers of SPB.\n\nAdditionally, the random forest model's parameters, such as the number of features (mtry) and the number of decision trees (ntree), were optimized to minimize prediction error. This process ensures that the model is both accurate and interpretable to a certain extent. The use of cross-validation further validated the model's stability and reliability, confirming the consistency of the identified risk factors across different subsets of the data.",
  "model/output": "The model is a classification model. It is designed to predict the occurrence of spontaneous preterm birth (SPB) in primiparas with grade A1 gestational diabetes mellitus (GDM). The model uses various predictors to classify individuals into those who will experience SPB and those who will have full-term births (FTB). The performance of the model is evaluated using metrics such as the area under the curve (AUC), sensitivity, and specificity, which are typical for classification tasks. The random forest algorithm, which is used in this model, is also commonly employed for classification problems.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method for the prediction model involved a combination of internal validation and repeated fitting. The complete dataset was randomly divided into a training set, comprising 70% of the data, and a validation set, comprising the remaining 30%. This split allowed for the assessment of the model's performance on unseen data.\n\nTo ensure the stability and robustness of the model, ten random cross-iterations were performed during the model construction. This process involved constructing ten different prediction models and evaluating their performance using metrics such as the Area Under the Curve (AUC), accuracy, sensitivity, and specificity. The consistency of these metrics across the ten models provided insights into the model's reliability.\n\nThe AUC values for the models ranged from 0.853 to 0.932, indicating a high overall predictive performance. The sensitivity and specificity of the models were also assessed, with one example showing a sensitivity of 0.634 and a specificity of 0.976. These evaluations demonstrated the model's ability to correctly identify both positive and negative cases.\n\nAdditionally, the importance ranking of various predictors in the random forest model was determined using the Gini coefficient. This ranking helped identify key factors influencing the occurrence of preterm birth in primiparas with grade A1 gestational diabetes mellitus (GDM). The model's parameters, including the number of features (mtry) and the number of decision trees (ntree), were optimized to minimize prediction error and maximize accuracy.",
  "evaluation/measure": "The performance of the prediction model was evaluated using several key metrics. The Area Under the Curve (AUC) was reported, with values ranging from 0.853 to 0.932 across different models, indicating a high overall predictive accuracy. The sensitivity and specificity of the model were also provided, with values of 0.634 and 0.976, respectively. These metrics are commonly used in the literature to assess the effectiveness of predictive models, particularly in medical research. The high AUC values suggest that the model has a strong ability to distinguish between preterm birth and full-term birth cases. The sensitivity and specificity values indicate that the model is effective in correctly identifying both positive and negative cases, which is crucial for clinical applications. The use of these metrics aligns with standard practices in evaluating predictive models, ensuring that the results are both reliable and comparable to other studies in the field.",
  "evaluation/comparison": "Not applicable. The study focused on developing a prediction model using the random forest algorithm for spontaneous preterm birth in primiparas with grade A1 gestational diabetes mellitus. The evaluation of the model included internal validation and repeated fitting, but it did not involve a comparison to publicly available methods or simpler baselines on benchmark datasets. The study primarily aimed to identify key risk factors and construct a predictive model based on clinical variables, without benchmarking against existing models or baselines.",
  "evaluation/confidence": "The evaluation of the prediction model for spontaneous preterm birth (SPB) in primiparas with grade A1 gestational diabetes mellitus (GDM) was conducted using several performance metrics. The area under the receiver operating characteristic curve (AUC) was a key metric, with values ranging from 0.853 to 0.932 across different models. These AUC values indicate a high level of predictive accuracy.\n\nThe sensitivity and specificity of the model were also reported, with values of 0.634 and 0.976, respectively. These metrics provide a measure of the model's ability to correctly identify true positive and true negative cases.\n\nStatistical significance was assessed using P-values, with a threshold of <0.05 indicating significant differences between groups. This threshold was applied to various baseline characteristics and predictor variables, ensuring that the observed differences were not due to random chance.\n\nConfidence intervals were provided for the effect sizes of significant variables, offering a range within which the true effect size is likely to fall. This adds an additional layer of confidence in the reported results.\n\nThe robustness of the model was further validated through cross-validation, where the model was repeatedly fitted and tested on different subsets of the data. This process helped to ensure that the model's performance was consistent and not dependent on a specific subset of data.\n\nOverall, the evaluation metrics and statistical analyses provide a strong basis for claiming that the prediction model is effective and superior to other methods and baselines. The use of confidence intervals and statistical significance testing enhances the reliability of the results.",
  "evaluation/availability": "The original contributions presented in the study are included in the article and supplementary material. Further inquiries can be directed to the corresponding author. The data availability statement specifies that the original contributions are available within the article and supplementary material, indicating that the raw evaluation files are not publicly released. Instead, interested parties are directed to contact the corresponding author for more information. This approach ensures that the data can be accessed for verification or further research while maintaining control over its distribution."
}