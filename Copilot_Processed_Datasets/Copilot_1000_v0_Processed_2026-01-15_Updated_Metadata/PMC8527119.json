{
  "publication/title": "Machine learning approaches improve risk stratification for secondary cardiovascular disease prevention in multiethnic patients.",
  "publication/authors": "Sarraju A, Ward A, Chung S, Li J, Scheinker D, Rodr\u00edguez F",
  "publication/journal": "Open heart",
  "publication/year": "2021",
  "publication/pmid": "34667093",
  "publication/pmcid": "PMC8527119",
  "publication/doi": "10.1136/openhrt-2021-001802",
  "publication/tags": "- Machine Learning\n- Cardiovascular Disease\n- Risk Stratification\n- Secondary Prevention\n- Electronic Health Records\n- Multiethnic Patients\n- Gradient Boosting\n- Random Forests\n- Logistic Regression\n- Predictive Modeling",
  "dataset/provenance": "The dataset utilized in this study was sourced from electronic health records (EHRs) of a large, community-based Northern California health system. The study cohort included adults with established cardiovascular disease (CVD) who were over 18 years of age and received care between January 1, 2009, and December 31, 2018. Patients were required to have at least two outpatient visits that were at least one year apart and were excluded if they had less than five years of total follow-up and did not have an outcome event.\n\nThe index date was defined as the first outpatient visit that was one year after the first clinic visit. If there were no cholesterol lab results before a patient\u2019s index date, the index date was shifted to the date of the first cholesterol lab result. Established CVD was defined according to the 2013 American College of Cardiology/American Heart Association (ACC/AHA) Guidelines on the Assessment of Cardiovascular Risk, including conditions such as prior myocardial infarction (MI), stroke, coronary revascularization procedures, atrial fibrillation, congestive heart failure, or coronary artery disease.\n\nThe dataset included a variety of patient variables, such as age, diabetes status, hypertension, current smoking, peripheral artery disease, prior stroke, prior coronary artery bypass grafting (CABG), history of heart failure, renal dysfunction, and history of MI. Additionally, variables like total cholesterol, HDL cholesterol, systolic blood pressure, and smoking status were included, using the most recent values on or before the index date. The dataset also incorporated socioeconomic variables derived from patient addresses, including indicators of educational attainment and median household income. Healthcare utilization variables, such as the number of primary care, urgent care, specialty, and other service care visits in the previous year, were also included.\n\nThe outcome of interest was defined as the first acute MI, stroke, or fatal coronary artery disease per ACC/AHA guidelines after the index date. Acute MI and stroke events were defined using specific ICD-9-CM and ICD-10-CM codes. Fatal coronary artery disease was defined by the presence of certain ICD codes followed by death within a year. Follow-up was right censored after the occurrence of the outcome or the end of the study period.",
  "dataset/splits": "The dataset was split into two primary sets: a training/validation set and a held-out test set. The training/validation set comprised 80% of the data, while the held-out test set contained the remaining 20%. This split was stratified by outcome to ensure a balanced representation of events in each subset.\n\nWithin the training/validation set, a further split was made for cross-validation purposes. This set was divided into a prespecified training fold (64%) and a held-out cross-validation fold (16%). This process was repeated five times in a fivefold cross-validation scheme to tune hyperparameters and evaluate model performance robustly.\n\nThe held-out test set, which was not used during the training and cross-validation phases, was reserved for the final evaluation of the best-performing models. This approach ensured that the models were tested on unseen data, providing an unbiased estimate of their generalizability.",
  "dataset/redundancy": "The study cohort consisted of adults with established cardiovascular disease (CVD) who received care from a large, community-based Northern California health system between January 1, 2009, and December 31, 2018. Patients were required to have at least two outpatient visits that were at least one year apart and were excluded if they had less than five years of total follow-up and did not have an outcome event.\n\nThe cohort was randomly split into an 80% training/validation set and a 20% held-out test set, stratified by outcome. This split ensured that the training and test sets were independent, with the test set serving as an unbiased evaluation of the model's performance. The training set was further divided into prespecified training (64%) and held-out cross-validation (16%) folds for hyperparameter tuning.\n\nThe distribution of the cohort included 32,192 patients with a median age of 74 years, 46% female, 63% non-Hispanic white, 12% Asian, and 6% Hispanic patients. A total of 23,475 patients (72.9%) had atherosclerotic cardiovascular disease (ASCVD), and 15,724 (49%) were on statin therapy. During five years of follow-up, 4,010 patients (12.5%) experienced a CVD event.\n\nThe variables used in the models included a rich set of electronic health record (EHR) patient features, such as medication binary values, laboratory values, health utilization variables, comorbidity variables, family history, and socioeconomic status variables. This approach allowed for a comprehensive evaluation of risk factors and ensured that the models were applicable across the full cohort, addressing the limitations of traditional risk scores that may not be calculable due to missing variables.\n\nThe study's focus on a multiethnic population and the inclusion of nontraditional predictive variables, such as education level and healthcare utilization, highlight the importance of developing models that are broadly applicable and can reveal relevant risk factors for local populations. The use of EHR data and the inclusion of a diverse set of variables ensure that the models are robust and can be applied in real-world settings.",
  "dataset/availability": "The data analyzed during the current study are not publicly available. This decision is due to reasonable privacy and security concerns, as the underlying electronic health record (EHR) data are not easily redistributable to researchers outside of the Institutional Review Board-approved research collaborations within the current project.\n\nThe corresponding author can be contacted for access to the EHR data for an Institutional Review Board-approved collaboration. This ensures that any use of the data adheres to ethical guidelines and maintains patient confidentiality. The study was approved by the Stanford University Institutional Review Board, which oversees the ethical conduct of research involving human subjects. This approval process helps to enforce the responsible use of the data and protects the privacy of the individuals whose data were used in the study.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the class of supervised learning techniques. Specifically, several well-established algorithms were employed, including random forests (RF), gradient boosted machines (GBM), extreme gradient boosted models (XGBoost), and logistic regression with both L2 penalty (standard logistic regression) and L1 penalty (LASSO).\n\nThese algorithms are not new; they are widely recognized and accepted in the machine learning community. The choice to use these algorithms was driven by their proven effectiveness in various predictive modeling tasks, particularly in healthcare. The study aimed to compare the performance of these algorithms to identify the most effective approach for risk stratification in cardiovascular disease (CVD) prevention.\n\nThe decision to publish this work in a cardiovascular journal rather than a machine-learning journal was likely influenced by the study's primary focus on improving CVD risk stratification. The research highlights the application of machine learning techniques to a specific medical problem, demonstrating their practical utility in a real-world healthcare setting. This approach aligns with the journal's focus on cardiovascular health and the potential impact of the findings on clinical practice.",
  "optimization/meta": "The models developed in this study do not use data from other machine-learning algorithms as input. Instead, they directly utilize a rich set of electronic health record (EHR) patient features. Several supervised machine learning algorithms were trained and tested, including random forests (RF), gradient boosted machines (GBM), extreme gradient boosted models (XGBoost), and logistic regression with both L2 penalty (LR) and L1 lasso penalty (LASSO). These algorithms were chosen to predict the risk of a cardiovascular disease (CVD) event within the following five years.\n\nThe cohort was randomly split into an 80% training/validation set and a 20% held-out test set, stratified by outcome. This ensures that the training data is independent from the test data, maintaining the integrity of the model evaluation process. The training set was further divided into prespecified training (64%) and held-out cross-validation (16%) folds for hyperparameter tuning. This approach helps in selecting the best-performing models and ensures that the final models are evaluated on unseen data, providing a robust assessment of their performance.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithms involved several steps to ensure the data was suitable for training and testing. Initially, a rich set of electronic health record (EHR) patient features was extracted, consisting of 1181 variables. From these, 123 variables were selected for the final models, including medication binary values, laboratory values, health utilization variables, comorbidity variables, family history, and socioeconomic status variables.\n\nMissing variables were handled using Bayesian ridge estimators, which were applied iteratively in order of ascending percentage variable missingness. The mean posterior value was used as the imputed value for missing data. This approach ensured that the models could handle real-world data with inherent missingness.\n\nThe cohort was randomly split into an 80% training/validation set and a 20% held-out test set, stratified by outcome. The training set was further divided into prespecified training (64%) and held-out cross-validation (16%) folds for each step of the cross-validation pipeline. This stratification helped in maintaining the distribution of outcomes in each subset, which is crucial for the performance of machine-learning models.\n\nFor the machine-learning algorithms, variables were used as inputs to predict whether a patient would have a cardiovascular disease (CVD) event in the following 5 years. Fivefold cross-validation was employed on the training set to tune hyperparameters such as tree depth, learning rate, and the number of trees. A grid search approach was used for hyperparameter tuning, ensuring that the models were optimized for performance.\n\nThe final performance of the best-performing machine-learning models, including the area under the receiver operating characteristic curve (AUC) with 95% confidence intervals, sensitivity, specificity, precision, and F1-score, was reported and evaluated on the 20% held-out test set. This rigorous preprocessing and encoding ensured that the models were robust and generalizable to new data.",
  "optimization/parameters": "In our study, we utilized a comprehensive set of variables derived from electronic health records (EHR) to train and test our machine learning models. Initially, we considered 1181 variables, but through a rigorous selection process, we narrowed it down to 123 variables that were most relevant for predicting cardiovascular disease (CVD) events. These variables included a mix of medication binary values, laboratory values, health utilization metrics, comorbidity indicators, family history, and socioeconomic status factors.\n\nThe selection of these variables was driven by their potential to contribute to the predictive power of our models. We employed a feature importance metric, specifically the F-score, which measures the fraction of times a given feature was used to split the data across all trees in our models. This approach helped us identify the most influential variables and ensured that our models were trained on the most relevant data.\n\nAdditionally, we addressed missing data by using Bayesian ridge estimators applied iteratively. This method allowed us to impute missing values effectively, ensuring that our models could utilize the full dataset without significant data loss.\n\nIn summary, the number of parameters (p) used in our models was 123, carefully selected based on their importance and relevance to predicting CVD events. This selection process was crucial for optimizing the performance of our machine learning algorithms.",
  "optimization/features": "In our study, we utilized a total of 123 electronic health record (EHR) variables as input features for our machine learning models. These features were selected from an initial pool of 1181 variables, indicating that a feature selection process was indeed performed. The selected features encompassed a variety of categories, including 37 medication binary values, 21 binary laboratory values, 3 health utilization variables, 32 comorbidity variables, 1 family history variable, and 7 socioeconomic status variables.\n\nThe feature selection was conducted using the training set only, ensuring that the model's performance on the held-out test set remained unbiased. This approach helped in identifying the most relevant variables for predicting cardiovascular disease (CVD) events, thereby enhancing the model's predictive accuracy and generalizability. The selected features were then used to train and validate multiple machine learning algorithms, including random forests, gradient boosted machines, extreme gradient boosting, and logistic regression with L1 and L2 penalties.",
  "optimization/fitting": "The fitting method employed in this study involved several machine learning algorithms, including random forests (RF), gradient boosted machines (GBM), extreme gradient boosted models (XGBoost), and logistic regression with L2 (LR) and L1 (Lasso) penalties. To address the potential issues of overfitting and underfitting, several strategies were implemented.\n\nFirstly, the number of parameters in the models was managed carefully. For instance, in the case of tree-based models like RF and GBM, hyperparameters such as the maximum depth of the trees and the number of trees (estimators) were tuned using a grid search approach. This ensured that the models were neither too complex (leading to overfitting) nor too simple (leading to underfitting). The hyperparameters were selected based on their performance in a fivefold cross-validation process on the training set, which helped in generalizing the model's performance.\n\nTo rule out overfitting, the models were evaluated using cross-validation. Specifically, fivefold cross-validation was used, where the training set was split into five subsets. The model was trained on four subsets and validated on the remaining one, and this process was repeated five times. This method ensured that the model's performance was assessed on different portions of the data, reducing the risk of overfitting to any single subset.\n\nAdditionally, the final performance of the best-performing models was evaluated on a held-out test set, which consisted of 20% of the data that was not used during the training and cross-validation phases. This test set provided an unbiased evaluation of the model's performance, further ensuring that the models were not overfitting to the training data.\n\nTo address underfitting, the models were trained with a variety of hyperparameters, and the best-performing configurations were selected. For example, the number of estimators in the RF and GBM models ranged from 100 to 5000, and the maximum depth of the trees was varied to find the optimal balance between bias and variance. This thorough hyperparameter tuning helped in ensuring that the models were complex enough to capture the underlying patterns in the data without being too simplistic.\n\nMoreover, feature importance was reported using the F-score, which measures the fraction of times a given feature was used to split the data across all trees. This helped in identifying the most predictive variables and ensuring that the models were not underfitting by ignoring important features.\n\nIn summary, the fitting method involved careful management of model complexity, extensive hyperparameter tuning, cross-validation, and evaluation on a held-out test set. These strategies collectively helped in ruling out both overfitting and underfitting, ensuring robust and generalizable model performance.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. Regularization was a key aspect of our approach. For logistic regression, we used both L1 (Lasso) and L2 (Ridge) penalties. The L1 penalty encourages sparsity by driving some coefficients to zero, effectively performing feature selection. The L2 penalty, on the other hand, shrinks the coefficients but does not eliminate them, which helps in reducing the complexity of the model.\n\nAdditionally, we utilized tree-based methods such as Random Forests (RF), Gradient Boosting Machines (GBM), and Extreme Gradient Boosting (XGBoost). These methods inherently provide regularization through mechanisms like limiting the maximum depth of trees, setting a minimum number of samples required to split an internal node, and subsampling the data. For instance, in our Random Forests, we set a maximum depth for the trees and controlled the number of features considered for splitting at each node. In Gradient Boosting and XGBoost, we used techniques like subsampling the data and limiting the learning rate to prevent overfitting.\n\nFurthermore, we employed cross-validation to tune hyperparameters and select the best models. Specifically, we used fivefold cross-validation on the training set to optimize parameters such as tree depth, learning rate, and the number of trees. This process helped in ensuring that our models generalized well to unseen data.\n\nMissing data was handled using Bayesian ridge estimators, which iteratively imputed values based on the ascending percentage of variable missingness. This approach helped in maintaining the integrity of the data and preventing overfitting due to missing values.\n\nOverall, our use of regularization techniques, cross-validation, and careful handling of missing data contributed to the development of robust and generalizable machine learning models for predicting cardiovascular disease events.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available and have been reported in detail. Specifically, the hyper-parameters searched for various machine learning models, including logistic regression, Lasso, random forests (RF), gradient boosted machines (GBM), and extreme gradient boosting (XGB), are documented. For instance, the values for hyper-parameters such as `n_estimators`, `max_depth`, `learning_rate`, and `C` are provided, allowing for reproducibility of our experiments.\n\nThe optimization process involved a grid search approach for hyper-parameter tuning, which was conducted using five-fold cross-validation on the training set. This method ensured that the best-performing models were identified based on their mean area under the receiver operating characteristic curve (AUC).\n\nThe performance metrics of the models, including the XGBoost model, are also reported in supplementary tables. These tables provide detailed metrics such as sensitivity, specificity, precision, and F1-score for different percentages of patients, offering a comprehensive view of the models' performance.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly mentioned in the provided information. However, the study was conducted using Python V.3.7 with the scikit-learn and XGBoost packages, versions V.0.21.2 and V.0.90, respectively. This information should facilitate the replication of the optimization process and model training.\n\nThe supplementary materials, including tables and lists, are available but the licensing details for accessing these materials are not specified. Typically, such materials are made available under the terms of the journal's policies, which often allow for academic use and sharing with proper citation. For specific licensing details, one would need to refer to the journal's guidelines or contact the authors directly.",
  "model/interpretability": "The models developed in our study are not treated as black boxes. Instead, we prioritize interpretability to gain insights from the machine learning (ML) algorithms. This approach allows us to understand the underlying factors contributing to cardiovascular disease (CVD) risk rather than relying solely on the model's recommendations.\n\nOne of the key aspects of our models' transparency is the identification of important features. For instance, markers of education level and healthcare utilization were among the top predictive variables in our cohort, highlighting their relevance for CVD risk and associated disparities. Additionally, black race and Hispanic ethnicity were found to be predictive of CVD risk. Traditional variables such as age, blood pressure, cholesterol, and diabetes were also confirmed as significant predictors, reinforcing their importance in CVD risk assessment.\n\nBy using interpretable, locally trained models, we can reveal relevant nontraditional risk factors that are applicable to specific populations. This transparency is crucial for understanding the model's decisions and ensuring that the predictions are clinically meaningful and actionable. For example, the feature importance rankings provided by our models help identify which variables are most influential in predicting CVD risk, allowing healthcare providers to focus on these factors when developing prevention strategies.\n\nMoreover, the use of less complex approaches like penalized logistic regression (LR) can enhance interpretability while maintaining performance. These models can be retrained in local populations, preserving real-world performance and ensuring that the models remain relevant and accurate for diverse patient groups. This flexibility is essential for addressing the heterogeneity of CVD populations and improving risk stratification in secondary prevention.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict whether a patient with cardiovascular disease (CVD) will experience a CVD event within the next five years. The model uses various machine learning algorithms, including random forests, gradient boosted machines, extreme gradient boosting (XGBoost), and logistic regression, to classify patients based on their risk of future CVD events.\n\nThe performance of these models was evaluated using metrics such as the area under the receiver operating characteristic curve (AUC), sensitivity, specificity, precision, and F1 score. The AUC values for the models indicate their ability to distinguish between patients who will experience a CVD event and those who will not. For instance, the XGBoost model achieved an AUC of 0.70 in the full CVD cohort and 0.71 in the atherosclerotic cardiovascular disease (ASCVD) subcohort, demonstrating its effectiveness in risk stratification.\n\nAdditionally, the model's performance was compared to the TIMI risk score for secondary prevention (TRS 2\u00b0P), which performed poorly in comparison. The TRS 2\u00b0P model had an AUC of 0.512 in the full CVD cohort and 0.50 in the ASCVD subcohort, indicating that the machine learning models outperformed the traditional risk score in predicting CVD events.\n\nThe top predictive variables identified by the XGBoost model included various clinical, laboratory, and socioeconomic factors, highlighting the model's ability to integrate diverse data types for improved risk prediction. The model's outputs provide valuable insights into the risk of future CVD events, aiding in clinical decision-making and patient management.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the machine learning models developed in this study is not publicly released. However, the models were implemented using widely available software packages in Python, specifically scikit-learn and XGBoost. These packages are open-source and can be accessed through their respective repositories. The versions used in this study were scikit-learn V.0.21.2 and XGBoost V.0.90.\n\nWhile the specific code used for training and testing the models is not provided, the methods and algorithms employed are standard and well-documented in the literature. Researchers interested in replicating or building upon this work can refer to the documentation of these packages and the details provided in the study for guidance.\n\nAdditionally, the study does not provide a direct method to run the algorithm, such as an executable, web server, virtual machine, or container instance. The focus was on the development and evaluation of the models rather than the deployment of a ready-to-use tool. However, the use of standard libraries and clear documentation of the methods should facilitate the implementation of similar models by other researchers.",
  "evaluation/method": "The evaluation method employed in our study involved a rigorous process to ensure the robustness and generalizability of our machine learning models. We utilized a held-out test set approach, where the entire cohort of patients with cardiovascular disease (CVD) was randomly split into an 80% training/validation set and a 20% held-out test set, stratified by outcome. This stratification ensured that the distribution of outcomes was similar in both the training and test sets.\n\nFor the training and cross-validation pipeline, we used fivefold cross-validation on the training set to tune hyperparameters. This involved splitting the training set into prespecified training (64%) and held-out cross-validation (16%) folds. Hyperparameters such as tree depth, learning rate, and the number of trees were optimized using a grid search approach. The performance of the models was compared based on the mean area under the receiver operating characteristic curve (AUC), also known as the C-statistic, across the five folds. Additionally, 95% AUC confidence intervals were calculated to provide a measure of the uncertainty around the AUC estimates.\n\nFeature importance was reported using the F-score, which represents the fraction of times a given feature was used to split the data across all trees. Missing variables were imputed using Bayesian ridge estimators applied iteratively in order of ascending percentage variable missingness, with the mean posterior value used as the imputed value.\n\nThe final performance of the best-performing machine learning models, including AUC with 95% confidence intervals, sensitivity, specificity, precision, and F1-score, was reported and evaluated on the 20% held-out test set. This approach allowed us to assess the models' performance on unseen data, providing a more reliable estimate of their generalizability.\n\nIn addition to the held-out test set evaluation, we also calculated the TRS 2\u00b0P-based CVD risk in the test set. This involved assigning one point to the presence of each included clinical variable, as previously described. The analyses were performed using Python V.3.7 with the scikit-learn and XGBoost packages, versions 0.21.2 and 0.90, respectively. This comprehensive evaluation method ensured that our models were thoroughly tested and validated, providing confidence in their performance and potential for real-world application.",
  "evaluation/measure": "In the evaluation of our machine learning models for cardiovascular disease (CVD) risk stratification, we reported a comprehensive set of performance metrics to ensure a thorough assessment. The primary metric used was the area under the receiver operating characteristic curve (AUC), also known as the C-statistic, which provides an overall measure of the model's discriminative ability. This metric is widely accepted and used in the literature for evaluating predictive models in healthcare.\n\nIn addition to the AUC, we reported sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), Youden Index, F1 score, and the number needed to examine (NNE) at various percentage thresholds. Sensitivity, or recall, measures the proportion of actual positives correctly identified by the model, while specificity measures the proportion of actual negatives correctly identified. The PPV indicates the probability that a positive prediction is a true positive, and the NPV indicates the probability that a negative prediction is a true negative. The Youden Index is a single statistic that captures the performance of a diagnostic test and is defined as sensitivity + specificity - 1. The F1 score is the harmonic mean of precision and recall, providing a balance between the two. The NNE represents the number of patients that need to be examined to identify one true positive case.\n\nThese metrics were evaluated on a held-out test set, ensuring that the performance measures were assessed on data not used during the training process. This approach helps to provide an unbiased estimate of the model's generalizability to new, unseen data. The reported metrics are representative of those commonly used in the literature for evaluating predictive models in cardiovascular research, ensuring that our evaluation is comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we did not use benchmark datasets for comparison. Instead, we compared our machine learning (ML) models to the established TRS 2\u00b0P score, which is a traditional risk stratification method for cardiovascular disease (CVD). This comparison was performed on our own cohort of patients with prior CVD.\n\nWe trained and tested several supervised ML algorithms, including random forests (RF), gradient boosted machines (GBM), extreme gradient boosted models (XGBoost), and logistic regression with standard L2 penalty (LR) and with an L1 lasso penalty (LASSO). These models were chosen to represent a range of complexity and approaches to ML.\n\nTo ensure a fair comparison, we evaluated the performance of these ML models against the TRS 2\u00b0P score using the same held-out test set. The performance metrics included the area under the receiver operating characteristic curve (AUC), sensitivity, specificity, precision, and F1-score.\n\nThe results showed that our ML models, including XGBoost, GBM, RF, and LASSO, performed comparably to each other and significantly outperformed the TRS 2\u00b0P score. For instance, in the full CVD cohort, the AUC for XGBoost was 0.70, while the TRS 2\u00b0P score had an AUC of 0.512. Similarly, in the subcohort of patients with atherosclerotic cardiovascular disease (ASCVD), the AUC for XGBoost was 0.71, compared to 0.50 for the TRS 2\u00b0P score.\n\nThis comparison demonstrates the potential of ML models to improve risk stratification for CVD, especially in diverse and real-world patient populations. The use of simpler baselines, such as logistic regression, also showed promising results, suggesting that the richness of the electronic health record (EHR)-derived patient feature set may be more important than the specific choice of model.",
  "evaluation/confidence": "The evaluation of our models included a comprehensive assessment of performance metrics, which were accompanied by confidence intervals to provide a measure of uncertainty. For instance, the area under the receiver operating characteristic curve (AUC) values for our models, such as XGBoost, gradient boosting machines (GBM), random forests (RF), and Lasso regression, were reported with 95% confidence intervals (CIs). This approach ensures that the reported performance metrics are robust and not merely the result of random chance.\n\nIn the full cohort of patients with cardiovascular disease (CVD), the XGBoost model achieved an AUC of 0.70 (95% CI 0.68 to 0.71), while the GBM, RF, and Lasso regression models performed comparably with AUCs of 0.69 (95% CI 0.68 to 0.71), 0.69 (95% CI 0.67 to 0.71), and 0.69 (95% CI 0.67 to 0.71), respectively. These results indicate that our models have a consistent and reliable performance in predicting CVD events.\n\nSimilarly, in the subcohort of patients with atherosclerotic cardiovascular disease (ASCVD), the XGBoost model demonstrated an AUC of 0.71 (95% CI 0.69 to 0.73), with the GBM, RF, and Lasso regression models achieving AUCs of 0.70 (95% CI 0.68 to 0.72), 0.70 (95% CI 0.68 to 0.72), and 0.70 (95% CI 0.68 to 0.72), respectively. These findings further support the reliability and generalizability of our models across different patient populations.\n\nThe inclusion of confidence intervals in our performance metrics allows for a more nuanced interpretation of the results. It enables us to assess not only the point estimates of the AUC but also the range within which the true AUC is likely to fall. This is particularly important in the context of medical research, where the stakes are high, and decisions based on model performance can have significant implications for patient care.\n\nMoreover, the statistical significance of our results was evaluated using appropriate tests. For example, the comparison of our models with the TIMI risk score for secondary prevention (TRS 2\u00b0P) model, which performed poorly with an AUC of 0.512 (95% CI 0.498 to 0.526) in the full CVD cohort and 0.50 (95% CI 0.48 to 0.52) in the ASCVD subcohort, was conducted to demonstrate the superiority of our approach. The significant differences in AUC values between our models and the TRS 2\u00b0P model provide strong evidence that our methods are indeed superior.\n\nIn summary, the performance metrics of our models are accompanied by confidence intervals, and the results are statistically significant. This ensures that our claims of superiority over other methods and baselines are well-founded and reliable.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The underlying electronic health record (EHR) data used in this study are not easily redistributable due to privacy and security concerns. These data are only accessible to researchers engaged in Institutional Review Board-approved research collaborations within the current project. For access to the EHR data for an approved collaboration, the corresponding author can be contacted. The study adheres to the Creative Commons Attribution Non-Commercial (CC BY-NC 4.0) license, which permits non-commercial distribution, remixing, adaptation, and building upon the work, provided the original work is properly cited and any changes are indicated. However, this license does not apply to the raw evaluation files, which remain restricted."
}