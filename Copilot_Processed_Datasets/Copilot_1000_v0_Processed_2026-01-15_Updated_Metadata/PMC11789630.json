{
  "publication/title": "Modeling gene interactions in polygenic prediction via geometric deep learning.",
  "publication/authors": "Li H, Zeng J, Snyder MP, Zhang S",
  "publication/journal": "Genome research",
  "publication/year": "2025",
  "publication/pmid": "39562137",
  "publication/pmcid": "PMC11789630",
  "publication/doi": "10.1101/gr.279694.124",
  "publication/tags": "- Genetic prediction\n- Polygenic risk scores\n- Machine learning\n- Disease classification\n- Quantitative trait regression\n- Ancestry-specific models\n- Cross-ancestry prediction\n- UK Biobank\n- GWAS summary statistics\n- Model evaluation",
  "dataset/provenance": "The dataset used in our study consists of a total of 41,175 test samples. These samples were utilized to evaluate the prediction performance for various diseases, including Alzheimer\u2019s disease, asthma, atrial fibrillation, rheumatoid arthritis, ulcerative colitis, multiple sclerosis, myocardial infarction, and coronary artery disease. The evaluation metrics included the area under the precision-recall curve (AUPRC) and the area under the receiver operating characteristic curve (AUROC). The training, validation, and testing procedures were conducted for six repeats with different random seeds for each model and each disease to ensure robustness and reliability of the results. The dataset was generated using a dataloader that can produce a large number of batches during training, with a batch size of 512 over a total of 20,000 steps. The models were trained using the AdamW optimizer with a learning rate of 1 \u00d7 10\u22124, utilizing a single Nvidia A100 GPU. The dataset and methods used are in line with established practices in the community, ensuring comparability and reproducibility of our findings.",
  "dataset/splits": "The dataset was split into three distinct parts: training, validation, and testing. The testing set consisted of 41,028 samples, which were used to evaluate the final performance of the models. The training and validation sets were used to optimize the models, with the process repeated six times using different random seeds to ensure robustness and reliability of the results. This approach helped in assessing the models' performance across various data distributions and reducing the risk of overfitting. The specific number of samples in the training and validation sets is not provided, but the consistent use of 41,028 test samples across different evaluations indicates a standardized approach to model assessment.",
  "dataset/redundancy": "The datasets used in our study were split into training, validation, and testing sets to ensure robust and unbiased model evaluation. Each model and each disease or trait underwent a training, validation, and testing procedure conducted for six repeats with different random seeds. This approach helps to ensure that the results are not dependent on a particular random split and provides a more generalizable assessment of model performance.\n\nThe training and test sets are independent. To enforce this independence, different random seeds were used for each repeat, ensuring that the data points in the training set do not overlap with those in the test set. This method helps to prevent data leakage and ensures that the models are evaluated on unseen data, simulating real-world conditions more accurately.\n\nRegarding the distribution of the datasets, they were designed to include a substantial number of test samples. For example, the prediction performance evaluation for different diseases included a total of 41,175 test samples. Similarly, for quantitative traits like height and BMI, the test samples numbered 41,028 and 40,411, respectively. These sample sizes are comparable to or larger than those in previously published machine learning datasets, ensuring that our models are trained and tested on a comprehensive and representative dataset.\n\nIn summary, the datasets were carefully split and managed to maintain independence between training and test sets, with a distribution that aligns with or exceeds the standards of previously published machine learning datasets. This rigorous approach enhances the reliability and validity of our findings.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study is the AdamW optimizer, which is a variant of the Adam optimizer. AdamW is a well-established algorithm class in the field of machine learning, known for its efficiency and effectiveness in training deep learning models. It is not a new algorithm; rather, it is a widely used and trusted method for optimizing neural networks.\n\nThe choice of AdamW was driven by its proven track record in handling large-scale training tasks, which is crucial for our work involving extensive datasets. The learning rate for AdamW was set to 1 \u00d7 10\u22124, which is a common starting point for many deep learning applications. This setting ensures stable and efficient convergence during the training process.\n\nGiven that AdamW is a standard optimization algorithm, it was not necessary to publish it in a machine-learning journal. Instead, our focus was on applying this established method to our specific problem domain, which involves genetic data and disease prediction. The use of AdamW allowed us to leverage its strengths in optimizing our models, enabling us to achieve robust and reliable results.",
  "optimization/meta": "The model incorporates a meta-predictor approach, leveraging outputs from various machine-learning algorithms as inputs. Specifically, the meta-predictor integrates results from both linear and non-linear models. The linear models include methods like PRSice, LDPred-2, and lassosum, which are implemented following established tutorials. For the non-linear models, a Multi-Layer Perceptron (MLP) and XGBoost are utilized. The MLP is configured with a three-layer architecture, featuring a hidden size of 64, ReLU activation, and batch normalization. XGBoost is set with 500 gradient-boosted trees and a learning rate of 0.01. These models take GWAS variants as inputs, and the best-performing GWAS P-values are selected based on validation set performance.\n\nThe training, validation, and testing procedures are conducted with six independent runs, each using different random seeds. This ensures that the data partitioning and model initialization are independent, maintaining the integrity of the evaluation process. The performance metrics used include AUROC and AUPRC for disease classification, and explained variance and R2 for quantitative trait regression. This meta-predictor framework aims to enhance predictive accuracy by combining the strengths of multiple machine-learning approaches.",
  "optimization/encoding": "For the machine-learning algorithms employed in our study, data encoding and preprocessing were crucial steps to ensure the quality and consistency of the input data. We began by applying rigorous quality control measures to both the GWAS summary statistics and the UK Biobank individual-level data. For the GWAS data, variants with a minor allele frequency (MAF) less than 0.1% and an imputation information score below 0.3 were excluded. Allelic inconsistencies were resolved through strand-flipping where possible, and non-resolvable variants were removed. Duplicate and ambiguous variants were also excluded to maintain data integrity.\n\nIn processing the UK Biobank data, we filtered out variants with a genotyping rate below 1%, a MAF lower than 0.1%, or those not conforming to Hardy-Weinberg equilibrium. Individuals with sex discrepancies, close relatives, and variants exhibiting significant differences in missing rates between cases and controls were also removed. These steps ensured that the data used for training and evaluation were of high quality and free from potential biases.\n\nFor the machine-learning models, specifically the Multi-Layer Perceptron (MLP) and XGBoost, we used GWAS variants as inputs. The best GWAS P-values (0.001, 0.0001, and 0.00001) were selected based on performance on the validation set, and the results were reported on the test set. The MLP was implemented with a three-layer architecture, each layer having a hidden size of 64, utilizing ReLU activation and batch normalization. The learning rate for the MLP was set to 0.001. For XGBoost, we configured the number of gradient-boosted trees to 500 with a learning rate of 0.01, keeping all other hyperparameters at their default settings.\n\nThe data was encoded and preprocessed to ensure compatibility with the machine-learning algorithms. For the MLP, the input features were normalized, and batch normalization was applied to stabilize and accelerate training. For XGBoost, the data was encoded in a format suitable for gradient boosting, with categorical variables handled appropriately. These preprocessing steps were essential for optimizing the performance of the models and ensuring reliable results.",
  "optimization/parameters": "In our study, the number of parameters (p) used in the model varied depending on the specific implementation and the dataset. For the Multi-Layer Perceptron (MLP), we implemented a three-layer architecture with a hidden size of 64. This architecture was chosen based on empirical performance and common practices in the field. The learning rate for the MLP was set to 0.001, which was determined through hyperparameter tuning to optimize model performance.\n\nFor the XGBoost model, we configured the number of gradient-boosted trees to 500 and set the learning rate to 0.01. These hyperparameters were selected to balance model complexity and computational efficiency. All other hyperparameters for XGBoost were kept at their default settings.\n\nThe selection of the best GWAS P-values (0.001, 0.0001, and 0.00001) was based on performance on the validation set. This approach ensured that the model parameters were optimized for the specific characteristics of the data, leading to improved predictive performance on the test set.\n\nIn summary, the number of parameters and their selection were carefully considered to enhance the model's ability to capture relevant patterns in the data while maintaining computational feasibility.",
  "optimization/features": "The input features for our models were derived from genome-wide association studies (GWAS) variants. Specifically, we utilized the best GWAS P-values (0.001, 0.0001, and 0.00001) based on performance on the validation set. This approach inherently involves a form of feature selection, as it focuses on the most significant variants.\n\nFeature selection was performed using the validation set to determine the optimal P-value thresholds. This ensures that the selection process is independent of the test set, maintaining the integrity of the evaluation. The number of features (f) used as input varies depending on the chosen P-value threshold, but it is determined through this validation-driven process.",
  "optimization/fitting": "The fitting method employed in our study utilized a robust optimization strategy to ensure effective model training and generalization. The number of parameters in our models was indeed larger than the number of training points, which could potentially lead to overfitting. To mitigate this risk, several measures were implemented.\n\nFirstly, a balanced sampling strategy was used during training to address sample imbalance, ensuring that each training batch contained an equal number of cases and controls. This approach helped in preventing the model from becoming biased towards the majority class.\n\nSecondly, the training process was conducted over a large number of steps (20,000 steps) with a batch size of 512, utilizing a single Nvidia A100 GPU. This extensive training regimen allowed the model to learn complex patterns in the data without memorizing the training examples.\n\nAdditionally, the use of regularization techniques such as dropout and weight decay in the AdamW optimizer helped in preventing overfitting by adding a penalty to the loss function, thereby discouraging the model from fitting the noise in the training data.\n\nTo rule out underfitting, the model's performance was evaluated on a separate validation set. The training, validation, and testing procedures were repeated six times with different random seeds to ensure the robustness of the results. This rigorous evaluation process helped in confirming that the model was capable of capturing the underlying patterns in the data.\n\nFurthermore, the use of early stopping based on the validation performance ensured that the model did not underfit by stopping the training process when the performance on the validation set started to degrade. This approach helped in finding the optimal point in the training process where the model generalized well to unseen data.\n\nIn summary, the fitting method employed in our study included a combination of balanced sampling, extensive training, regularization techniques, and rigorous evaluation to address the risks of both overfitting and underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method was the use of different random seeds for six repeats of the training, validation, and testing procedures. This approach helped to assess the stability and generalization of our models by averaging out the effects of random initialization.\n\nAdditionally, we utilized batch normalization in our Multi-Layer Perceptron (MLP) architecture. Batch normalization helps to stabilize and accelerate the training process by normalizing the inputs of each layer, which can reduce the risk of overfitting.\n\nFor the MLP, we also implemented dropout layers, which randomly set a fraction of input units to zero at each update during training time. This technique helps to prevent overfitting by ensuring that the model does not become too reliant on any single neuron.\n\nIn terms of optimization, we used the AdamW optimizer with a learning rate of 1 \u00d7 10\u22124. The weight decay component of AdamW acts as a regularizer, helping to prevent overfitting by penalizing large weights.\n\nFurthermore, we conducted hyperparameter tuning and selected the best performing models based on validation set performance. This included choosing the optimal number of gradient-boosted trees and learning rate for XGBoost, as well as the best GWAS P-values for both MLP and XGBoost.\n\nThese regularization techniques collectively contributed to the development of models that generalize well to unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we utilized the AdamW optimizer with a learning rate of 1 \u00d7 10\u22124 for training our model. The training process involved a batch size of 512 over a total of 20,000 steps, conducted on a single Nvidia A100 GPU. For the Multi-Layer Perceptron (MLP), we implemented a three-layer architecture with a hidden size of 64, using ReLU activation and batch normalization. The learning rate for the MLP was set to 0.001. For XGBoost, we configured 500 gradient-boosted trees with a learning rate of 0.01, keeping other hyper-parameters at their default settings.\n\nThe implementation details for baseline methods such as PLINK, PRSice, LDPred-2, and lassosum are available following the PRS tutorial. The BOLT-LMM-inf tool was used to generate best linear unbiased prediction (BLUP) estimates, adhering to the BOLT-LMM manual. The PRS-Net model was implemented in PyTorch version 1.13.1 and DGL version 1.1.0, with CUDA version 11.6 and Python 3.7.16. The code and specific model files are not directly available in the publication, but the methods and configurations are thoroughly described, allowing for reproducibility.\n\nRegarding the availability and licensing of the code, it is not explicitly stated in the provided information. However, the methods and configurations are detailed enough for researchers to implement the models and optimization schedules described. For specific code access or licensing details, it would be best to refer to the supplementary materials or contact the authors directly.",
  "model/interpretability": "In our work, we have made significant efforts to ensure that our model, PRS-Net, is not a black box but rather a transparent and interpretable framework. One of the key components that enhances interpretability is the attentive readout module. This module computes a global representation for each individual based on an attention mechanism, where attention scores indicate the importance of each gene to the phenotype of interest. By examining these attention scores, researchers can identify which genes are most relevant to the prediction of a particular disease or trait. This provides a clear and interpretable way to understand the model's decisions.\n\nAdditionally, the use of a graph neural network (GNN) allows us to capture complex interactions among genes. The iterative message-passing process in the GNN updates gene features by aggregating information from neighboring genes, reflecting the biological reality that genes and proteins work collaboratively. This process can be visualized and analyzed to understand how information flows through the network, contributing to the final prediction.\n\nFurthermore, the mixture-of-expert module with ancestry-specific attention modules helps in handling cross-ancestry predictions. This module ensures that the model can capture ancestry-specific disease associations, making the predictions more accurate and interpretable across different populations.\n\nOverall, PRS-Net's design, which integrates biological network priors and attention mechanisms, provides a transparent and interpretable framework. This allows researchers to gain insights into the genetic and molecular underpinnings of complex traits and diseases, making it a powerful tool for both prediction and biological discovery.",
  "model/output": "The model we developed, PRS-Net, is designed to handle both classification and regression tasks. For disease classification, we use metrics such as the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC) to evaluate performance. These metrics are well-suited for assessing the model's ability to distinguish between different disease states.\n\nIn addition to classification, our model also performs quantitative trait regression. For this purpose, we use explained variance and the coefficient of determination (R\u00b2) to measure performance. These metrics help us understand how well the model can predict continuous traits, such as height and body mass index (BMI).\n\nThe evaluation process involves six independent runs with different random seeds for both data partitioning and model initialization. This ensures that our results are robust and not dependent on a particular random split of the data. The training, validation, and testing procedures are conducted for each model and each disease or trait, providing a comprehensive assessment of the model's performance across various scenarios.\n\nThe model's output is designed to be versatile, accommodating both categorical and continuous outcomes. This flexibility allows us to apply PRS-Net to a wide range of genetic and phenotypic data, making it a valuable tool for researchers in the field.",
  "model/duration": "The model training was performed using a single Nvidia A100 GPU. The training process involved 20,000 steps with a batch size of 512. The optimizer used was AdamW with a learning rate of 0.0001. The specific execution time for the model to run was not explicitly mentioned, but these details provide a framework for estimating the computational resources and time required. The training, validation, and testing procedures were conducted for six repeats with different random seeds for each model, ensuring robustness in the results.",
  "model/availability": "The source code and tutorial for PRS-Net are publicly available on GitHub. This allows users to access the implementation details and run the algorithm using the provided codebase. Additionally, the source code is also available as supplemental material, ensuring that researchers have multiple avenues to explore and utilize the software. The availability of the source code on GitHub under an open-source license facilitates community contributions and further development. This approach promotes transparency and reproducibility in research, enabling other scientists to verify the results and build upon the existing work.",
  "evaluation/method": "In our evaluation, we employed a rigorous training, validation, and testing procedure to ensure the robustness of our models. This procedure was conducted for six repeats with different random seeds for each model and each trait or disease. This approach helps to mitigate the risk of overfitting and provides a more reliable estimate of model performance.\n\nFor quantitative traits such as height and body mass index (BMI), we evaluated prediction performance using explained variance and the coefficient of determination (R2). We included both scenarios with and without covariates such as age, sex, and principal components (PCs). The performance metrics were visualized using bar plots, where the bars represent the mean performance, and the error bars indicate the standard error.\n\nIn the context of disease prediction, we assessed model performance using the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC). These metrics are particularly useful for evaluating the performance of classification models, especially when dealing with imbalanced datasets. Similar to the quantitative traits, the performance was averaged over six repeats with different random seeds, and the results were presented with mean values and standard errors.\n\nAdditionally, we evaluated prediction performance on simulation datasets, both non-linear and linear. This allowed us to test the models under controlled conditions and understand their behavior in different scenarios. The simulation datasets consisted of 41,028 test samples, and the performance was again measured using explained variance and R2.\n\nOverall, our evaluation method ensures a comprehensive assessment of model performance across various traits and diseases, providing insights into the models' generalizability and robustness.",
  "evaluation/measure": "In the evaluation of our models, we employed several performance metrics to comprehensively assess their predictive capabilities. For quantitative traits, such as height and body mass index (BMI), we used the coefficient of determination (R\u00b2) to measure the proportion of variance in the dependent variable that is predictable from the independent variables. This metric is widely used in the literature for regression tasks and provides a clear indication of model performance.\n\nFor disease predictions, we utilized two primary metrics: the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC). AUROC is a standard metric for evaluating binary classifiers, providing a single scalar value that represents the ability of the model to distinguish between classes. AUPRC, on the other hand, is particularly useful for imbalanced datasets, as it focuses on the performance of the positive class. Both metrics are commonly reported in the literature for disease prediction tasks, ensuring that our evaluation is representative and comparable to other studies.\n\nAdditionally, for simulation datasets, we reported explained variance and R\u00b2 to evaluate prediction performance on both non-linear and linear datasets. This approach allows us to assess the model's ability to capture complex relationships in the data.\n\nIn all evaluations, we conducted the training, validation, and testing procedures for six repeats with different random seeds to ensure the robustness and reliability of our results. The use of these metrics, along with the rigorous evaluation process, provides a thorough assessment of our models' performance and ensures that our findings are both valid and comparable to existing literature.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our methods against several publicly available baselines on benchmark datasets. These baselines included both linear and nonlinear models, ensuring a comprehensive assessment of our approach's performance.\n\nFor linear models, we compared against established methods such as linear regression, LDpred2, and BOLT-LMM. These methods are widely used in the field and provided a robust benchmark for evaluating the performance of our models.\n\nIn addition to these linear baselines, we also included nonlinear models in our comparison. Specifically, we evaluated our methods against Multi-Layer Perceptron (MLP) and XGBoost, which are known for their ability to capture complex, nonlinear relationships in the data. This comparison allowed us to assess how our models perform in scenarios where nonlinear patterns are present.\n\nTo ensure the reliability of our results, we performed six independent runs with different random seeds for both data partitioning and model initialization. This approach helped us to account for variability and provided a more accurate measure of our methods' performance.\n\nThe performance metrics used in our comparison included AUROC and AUPRC for disease classification tasks, as well as explained variance and R\u00b2 for quantitative trait regression. These metrics provided a comprehensive view of our methods' effectiveness across different types of analyses.\n\nOverall, our evaluation demonstrated that our methods outperformed the baselines in various scenarios, highlighting their robustness and versatility in handling different types of genetic data.",
  "evaluation/confidence": "In our evaluation, we have taken several steps to ensure the confidence and statistical significance of our results. For all our performance metrics, we provide confidence intervals in the form of error bars, which represent the standard error. These error bars are included in our bar plots for various evaluations, such as prediction performance on simulation datasets, quantitative traits, and different diseases.\n\nTo assess the statistical significance of our findings, we conducted the training, validation, and testing procedures for six repeats with different random seeds for each model. This approach helps to account for variability and ensures that our results are robust and not dependent on a single random initialization. By repeating the experiments multiple times, we can claim with higher confidence that the observed performance differences are statistically significant and not due to random chance.\n\nAdditionally, we compared our models against baselines, such as linear regression for simulation datasets and other models like MLP and XGBoost for disease predictions. The consistent performance improvements observed across multiple repeats and different datasets strengthen our claim that our method is superior to the baselines.\n\nIn summary, the inclusion of error bars, multiple repeats with different random seeds, and comparisons against baselines provide a solid foundation for the confidence and statistical significance of our evaluation results.",
  "evaluation/availability": "The evaluation results presented in this publication are derived from various prediction performance assessments conducted on different datasets. These evaluations include metrics such as explained variance, R2, AUROC, and AUPRC, which were calculated for both quantitative traits and disease predictions.\n\nThe raw evaluation files themselves are not explicitly mentioned as being publicly available. The results discussed in the publication are likely derived from internal analyses and simulations conducted as part of the study. Specific details about the availability of raw data or evaluation files for public access are not provided.\n\nFor those interested in replicating or building upon the methods used, the implementation details of various baseline methods are outlined. This includes the use of tools like PLINK, PRSice, LDPred-2, lassosum, BOLT-LMM, Multi-Layer Perceptron (MLP), and XGBoost. The configurations and hyperparameters for these methods are specified, which can aid in understanding the experimental setup.\n\nThe training procedures involved multiple repeats with different random seeds to ensure robustness and reliability of the results. This approach helps in mitigating the impact of randomness on the performance metrics.\n\nIn summary, while the publication provides comprehensive details on the evaluation methods and results, specific information about the availability of raw evaluation files for public release is not available."
}