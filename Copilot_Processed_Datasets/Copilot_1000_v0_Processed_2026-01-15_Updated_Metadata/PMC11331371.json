{
  "publication/title": "A study of duck detection using deep neural network based on RetinaNet model in smart farming.",
  "publication/authors": "Lee J, Kang H",
  "publication/journal": "Journal of animal science and technology",
  "publication/year": "2024",
  "publication/pmid": "39165750",
  "publication/pmcid": "PMC11331371",
  "publication/doi": "10.5187/jast.2023.e76",
  "publication/tags": "- Duck detection\n- Object detection\n- Smart farming\n- Data labeling\n- Image processing\n- Machine learning\n- Agricultural technology\n- Animal monitoring\n- RetinaNet\n- Data augmentation\n- Real-time detection\n- Duck states classification\n- Video data analysis\n- Bounding boxes\n- Class imbalance problem\n- Automatic labeling\n- Deep learning\n- Animal welfare\n- Computer vision\n- Agricultural AI",
  "dataset/provenance": "The dataset used in this study was collected directly from a duck farm over two years, 2021 and 2022. The primary data source consists of video footage captured in the duck cages. From these videos, frames were extracted and labeled for object detection. The labeling process involved identifying ducks in three states: normal, fallen, and dead. Due to the large number of ducks in the cages, specific criteria were established to ensure consistent labeling, focusing on ducks whose faces, bodies, tails, and feet were clearly visible.\n\nThe dataset comprises 2,852 images, with an average size of 1,748.30 pixels in width and 999.94 pixels in height. The total number of ducks identified across all images includes 10,461 normal ducks, 1,208 fallen ducks, and 381 dead ducks. The maximum number of ducks in a single image is 24 for normal ducks, 1 for fallen ducks, and 1 for dead ducks. The dataset includes images where ducks in various states may or may not be present, and multiple states can appear simultaneously. The ratios of duck objects to images are 0.056 for normal ducks, 0.053 for fallen ducks, and 0.082 for dead ducks. Notably, dead ducks always appear below the halfway point of the image.\n\nThere is no public data available for this specific task, making this dataset unique. The collection process involved ensuring that the video data had an appropriate height for real-world applications and that the duck states were sufficiently diverse. This dataset was used to train and validate object detection models, with a focus on addressing the class imbalance problem through techniques such as data augmentation and the use of focal loss.",
  "dataset/splits": "The dataset was divided into two main splits: training data and validation data. The ratio used for this split was 9:1, meaning 90% of the data was allocated for training and 10% for validation. This division was done based on classes to ensure a fair distribution of data points across different categories.\n\nThe total number of datasets created was 2,852. The average size of the images was 1,748.30 pixels in width and 999.94 pixels in height. The dataset included 10,461 normal ducks, 1,208 fallen ducks, and 381 dead ducks across all images. The maximum number of normal ducks in a single image was 24, while fallen and dead ducks had a maximum of 1 each.\n\nThe dataset was designed to handle various states of ducks, which could appear simultaneously or not at all in the images. The ratios of duck objects to images were 0.056 for normal ducks, 0.053 for fallen ducks, and 0.082 for dead ducks. Dead ducks were consistently found below the halfway point of the images.",
  "dataset/redundancy": "The dataset used in this study consists of 2,852 images, with an average size of 1,748.30 by 999.94 pixels. The images contain ducks in three states: normal, fallen, and dead. The total numbers of ducks in these states across all images are 10,461, 1,208, and 381, respectively. The maximum number of ducks in one image is 24 for normal ducks, and 1 for fallen and dead ducks.\n\nTo ensure the independence of training and test sets, the data were divided into training and validation sets at a ratio of 9:1. This division was done based on classes to ensure a fair distribution of duck states across both sets. This approach helps in maintaining the integrity of the evaluation process by preventing data leakage and ensuring that the model's performance is assessed on unseen data.\n\nThe distribution of duck states in the dataset differs from many previously published machine learning datasets, particularly due to the imbalance in the number of fallen and dead ducks compared to normal ducks. This imbalance is a characteristic of the collected dataset and was addressed using techniques such as data augmentation and the application of focal loss to improve the detection performance of the model. The dataset's unique characteristics necessitated these adjustments to ensure robust and reliable model training and evaluation.",
  "dataset/availability": "The datasets used in this study are not publicly available. However, they can be made available upon reasonable request to the corresponding author. This approach ensures that the data is shared responsibly and in accordance with the guidelines set by the supporting institutions. The datasets include video data of ducks in cages, bounding boxes specifying the locations of ducks by image frame, and state class labels. The data collection process involved extracting frames from videos taken at a duck farm over two years, 2021 and 2022. The frames were then labeled by humans, with specific criteria established to handle the density of ducks and the clarity of their states. To address the imbalance in the dataset, additional data from 2022 was incorporated, and data augmentation techniques were employed. The final dataset consists of 2,852 images, with an average size of 1,748.30 by 999.94 pixels, and includes labels for normal, fallen, and dead ducks. The data was divided into training and validation sets at a ratio of 9:1, ensuring a fair distribution across classes. This method allows for the responsible sharing of data while maintaining the integrity and security of the information.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is a one-stage object detection algorithm. Specifically, the RetinaNet model was employed, which is a well-established algorithm in the field of object detection. RetinaNet is known for its efficiency and effectiveness in real-time object detection tasks.\n\nThe RetinaNet model used in this study is not new; it has been previously developed and utilized in various applications. The decision to use RetinaNet was driven by its ability to handle class imbalance issues, which is crucial given the limited and imbalanced dataset collected from duck cages. RetinaNet's use of focal loss helps to address the problem of class imbalance by reducing the weight of easy examples and focusing on difficult ones.\n\nThe reason this study was not published in a machine-learning journal is that the primary focus is on the application of object detection in smart farming, specifically for duck detection in duck cages. The study aims to demonstrate the practical use of RetinaNet in a real-world scenario, rather than introducing a new machine-learning algorithm. The research highlights the challenges and solutions related to data collection, processing, and augmentation in the context of smart farming, which is more aligned with agricultural and technological journals.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps to ensure the data was suitable for training the object detection model. Initially, video data from duck cages were extracted as frames, and bounding boxes along with class labels were manually annotated. The classes included normal, fallen, and dead ducks. To manage the large volume of data, only one image per 5 to 10 frames was selected for labeling, resulting in 1,285 initially labeled images. Criteria were established to ensure consistency in labeling, focusing on ducks whose faces, bodies, tails, and feet were clearly visible.\n\nData augmentation techniques were employed to increase the dataset's diversity and size. These techniques included brightness conversion, contrast conversion, saturation conversion, rotation, random resize, and flipping. For brightness, contrast, and saturation, random values between 0.9 and 1.1 were applied. Rotation was applied within a range of -20 to 20 degrees, and flipping was applied with a 0.5 probability. Random resize adjusted the image dimensions based on the shortest side, with lengths selected from a predefined set and the longest side adjusted accordingly.\n\nThe final dataset consisted of 2,852 images, with an average size of 1,748.30 by 999.94 pixels. The dataset included 10,461 normal ducks, 1,208 fallen ducks, and 381 dead ducks. The maximum number of ducks in a single image was 24 for normal ducks, 1 for fallen ducks, and 1 for dead ducks. The dataset was divided into training and validation sets at a 9:1 ratio, ensuring a fair distribution of classes. The RetinaNet model, combined with ResNet50 and Feature Pyramid Network (FPN), was used for training. Focal loss was employed to address class imbalance, and Stochastic Gradient Descent (SGD) was used as the optimizer. The learning rate was managed using a warm-up scheduler and a step scheduler, with an initial learning rate of 1e-3.",
  "optimization/parameters": "In our study, we utilized the RetinaNet model, which is a combination of ResNet50 and Feature Pyramid Network (FPN). The ResNet50 backbone has approximately 23 million parameters. The FPN adds additional layers for feature extraction at multiple scales, contributing to the overall parameter count.\n\nThe selection of the ResNet50 backbone was influenced by its balance between computational efficiency and performance. It provides a good trade-off for real-time object detection tasks, which is crucial for our application in a smart farm environment. The FPN was chosen for its ability to handle objects at various scales, which is essential for detecting ducks in different sizes and positions within the cage environment.\n\nThe parameters were not explicitly tuned or selected beyond the standard configurations used in the original RetinaNet implementation. The focus was on fine-tuning the model using the COCO dataset and applying data augmentation techniques to improve detection accuracy. The use of focal loss and stochastic gradient descent (SGD) as the optimizer further aided in training the model effectively.",
  "optimization/features": "Not applicable",
  "optimization/fitting": "In our study, we employed fine-tuning techniques to adapt a pre-trained RetinaNet model for duck detection. The pre-trained models, specifically the 1x and 3x models, were initially trained on the COCO dataset, which contains a vast number of images and annotations. This pre-training phase involved a significant number of iterations and learning rate adjustments, ensuring that the models were well-generalized before fine-tuning.\n\nThe fine-tuning process involved training on a relatively smaller dataset of 2,852 images, which were divided into training and validation sets in a 9:1 ratio. Despite the smaller dataset, the pre-trained models had already learned robust feature representations from the COCO dataset, mitigating the risk of overfitting. Additionally, we used data augmentation techniques such as random resize, random flip, brightness conversion, contrast conversion, saturation conversion, and rotation to artificially increase the diversity of our training data. This helped in preventing overfitting by ensuring that the model did not memorize the training data but rather learned generalizable features.\n\nTo further address overfitting, we employed techniques like focal loss, which focuses the model's attention on hard-to-classify examples, and Stochastic Gradient Descent (SGD) with a warm-up scheduler and step scheduler. These schedulers adjusted the learning rate dynamically, allowing the model to converge more effectively without overfitting to the training data.\n\nUnderfitting was addressed by ensuring that the models were sufficiently complex and that the training process was thorough. The use of a combination of ResNet50 and Feature Pyramid Network (FPN) provided a strong backbone for feature extraction. The models were trained for a sufficient number of iterations (7,000), and the learning rate was carefully managed to ensure that the models had enough capacity to learn from the data.\n\nIn summary, the combination of a pre-trained model, data augmentation, and careful scheduling of the learning rate helped in balancing the model's capacity to learn from the data without overfitting or underfitting. The validation performance and generalization tests further confirmed the effectiveness of our approach.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One of the key methods used was data augmentation. We applied various augmentation techniques such as brightness conversion, contrast conversion, saturation conversion, rotation, random resize, and flipping. These techniques helped to increase the diversity of our training data, making the model more generalizable and less likely to overfit to the specific patterns in the original dataset.\n\nAdditionally, we utilized a combination of different learning schedules and models. We fine-tuned both the 1x and 3x pretrained models on the COCO dataset, which provided the model with a broader understanding of object detection beyond our specific dataset. This approach helped in reducing overfitting by exposing the model to a wider variety of scenarios during training.\n\nWe also implemented focal loss, which is designed to address class imbalance issues. By reducing the weight of easy examples and focusing on difficult ones, focal loss helps the model to learn more effectively from the minority classes, thereby improving overall performance and reducing overfitting.\n\nFurthermore, we divided our data into training and validation sets in a 9:1 ratio, ensuring that the model was evaluated on unseen data during the training process. This practice helped in monitoring the model's performance on new data and adjusting the training process accordingly to prevent overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we utilized two pretrained models, the 1x and 3x models, which were fine-tuned using the COCO dataset. The 1x model involves 90,000 iterations with learning rate reductions at 60,000 and 80,000 iterations. The 3x model, on the other hand, involves 270,000 iterations with learning rate reductions at 210,000 and 250,000 iterations. These details are provided to ensure reproducibility of our results.\n\nFor the optimization process, we employed focal loss as the loss function and Stochastic Gradient Descent (SGD) as the optimizer. The basic learning rate was set to 1e-3, with a warm-up scheduler for the first 1,000 iterations and a step scheduler that reduces the learning rate by 1e-1 at the 5,000th and 6,000th iterations. The batch size was set to 16, and the total number of iterations was 7,000. These configurations were chosen to balance between computational efficiency and model performance.\n\nRegarding the availability of model files and optimization parameters, they are not explicitly provided in the publication. However, the detailed descriptions of the configurations and schedules should allow other researchers to replicate the models and optimization processes. For access to the specific model files and parameters, interested parties may need to contact the authors directly, as the publication does not specify an open repository or license for these resources.",
  "model/interpretability": "The model used in this study is primarily a black-box model, as it leverages deep learning techniques, specifically the RetinaNet architecture combined with ResNet50 and Feature Pyramid Network (FPN). These models are known for their high performance in object detection tasks but are often considered black-box due to their complex, multi-layered neural network structures, which make it difficult to interpret how they arrive at specific predictions.\n\nHowever, there are aspects of the model that provide some level of transparency. For instance, the use of average precision (AP) metrics, such as AP50 and AP75, offers insights into the model's performance at different levels of intersection over union (IoU). These metrics help in understanding how well the model's predicted bounding boxes align with the ground truth boxes, providing a quantitative measure of the model's accuracy.\n\nAdditionally, the visual results presented in figures, such as the detection results for normal and slapped ducks, allow for qualitative assessment. These figures show the input images, ground truth annotations, and the model's detection results side by side, making it easier to visually inspect the model's performance and identify any patterns or errors.\n\nThe model's inference time, which is consistently within 0.003 seconds per image, also indicates its efficiency and reliability for real-time detection tasks. This short inference time is crucial for practical applications in smart farms, where timely detection is essential.\n\nIn summary, while the underlying neural network architecture of the model is a black-box, the use of performance metrics and visual results provides some level of interpretability. These elements help in understanding the model's strengths and areas for improvement, ensuring its effectiveness in real-world applications.",
  "model/output": "The model developed in this study is designed for object detection, which is a type of classification task. Specifically, it focuses on detecting and classifying the states of ducks in a cage environment. The model identifies and categorizes ducks into different states such as \"duck,\" \"slap,\" and \"dead.\" The performance of the model is evaluated using metrics like Average Precision (AP), AP50, and AP75, which measure the accuracy of the detected bounding boxes and their overlap with the ground truth.\n\nThe model utilizes RetinaNet, a one-stage object detection network, combined with a ResNet50 backbone and a Feature Pyramid Network (FPN). This combination allows the model to effectively detect objects at various scales and resolutions. The model was fine-tuned using pretrained weights from the COCO dataset and trained on a custom dataset of duck images.\n\nDuring the training process, different augmentation techniques were applied to improve the model's generalization performance. These techniques included random resizing, flipping, and full augmentation. The results indicated that the model with full augmentation performed better in terms of generalization, although partial augmentation sometimes yielded better validation performance due to the limited and similar nature of the validation data.\n\nThe inference time of the model is remarkably short, within 0.003 seconds per image, making it suitable for real-time detection. This efficiency is crucial for practical applications in smart farms, where real-time monitoring is essential.\n\nIn summary, the model is a classification-based object detection system tailored for detecting and classifying the states of ducks in a cage environment. It leverages advanced neural network architectures and augmentation techniques to achieve high accuracy and real-time performance.",
  "model/duration": "The model's inference time per image is remarkably short, averaging within 0.003 seconds. This efficiency makes the model suitable for real-time detection, which is crucial for applications in smart farming where timely detection of anomalies is essential. The model was trained using a single RTX 3090 GPU, and the entire learning process took approximately 2 hours. This quick execution time is a significant advantage, enabling the model to process images rapidly and provide immediate feedback, which is vital for monitoring duck cages and ensuring the well-being of the ducks.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method for our study involved a comprehensive approach to assess the performance of our object detection model for duck states in a cage. We divided the dataset into training and validation sets at a ratio of 9:1, ensuring a fair distribution of classes. This split allowed us to train the model on a substantial portion of the data while reserving a smaller, independent set for validation.\n\nTo measure performance, we primarily used Average Precision (AP), a common metric in object detection. AP evaluates the accuracy of predicted bounding boxes by considering the intersection over union (IoU) with the ground truth boxes. We reported AP at different IoU thresholds, such as AP50 (IoU > 0.5) and AP75 (IoU > 0.75), as well as the mean AP across a range of IoU values from 0.5 to 0.95.\n\nWe conducted experiments with different configurations of the RetinaNet model, including variations in the learning schedule (1x and 3x models) and data augmentation techniques. The results were tabulated to compare the performance of these configurations. Additionally, we visually inspected the detection results using images that were not part of the training set to ensure the model's generalizability.\n\nThe evaluation also included an analysis of the model's performance across different classes of duck states (e.g., normal, slapped, dead). This class-specific evaluation provided insights into how well the model generalized to various scenarios within the duck cage environment.\n\nOverall, the evaluation method combined quantitative metrics and qualitative assessments to thoroughly validate the effectiveness and robustness of our object detection model.",
  "evaluation/measure": "The primary performance metric reported in this study is the Average Precision (AP), which is a standard measure in object detection tasks. AP represents the percentage of correct predictions within the detected bounding boxes. It is further divided into AP50 and AP75, which evaluate the performance at different Intersection over Union (IoU) thresholds. AP50 measures the accuracy when the IoU is greater than 0.5, while AP75 measures it when the IoU is greater than 0.75. Additionally, the overall AP is calculated as the average accuracy across all IoU thresholds ranging from 0.5 to 0.95 in increments of 0.05.\n\nThese metrics are widely used in the literature and are representative of the current standards in evaluating object detection models. The use of AP, AP50, and AP75 provides a comprehensive evaluation of the model's performance at different levels of precision, ensuring that the results are both detailed and comparable to other studies in the field. This set of metrics allows for a thorough assessment of the model's ability to accurately detect objects under various conditions, making it a reliable indicator of the model's effectiveness.",
  "evaluation/comparison": "In our study, we primarily focused on evaluating the performance of the RetinaNet model, which is a well-established one-stage object detection network. We did not perform a direct comparison with other publicly available methods on benchmark datasets. Instead, our approach involved fine-tuning the RetinaNet model using our specific datasets, which were collected and defined for anomalous duck detection in a smart farm environment.\n\nWe utilized two variants of the RetinaNet model: the 1x model and the 3x model. The 1x model undergoes 90,000 iterations with learning rate reductions at specific intervals, while the 3x model undergoes 270,000 iterations with similar learning rate adjustments. This allowed us to compare the performance of models trained with different schedules.\n\nAdditionally, we experimented with various data augmentation techniques to enhance the robustness of our model. We compared models with no augmentation, partial augmentation (random resize and flipping), and full augmentation (including brightness, contrast, saturation conversion, rotation, and flipping). The results indicated that excessive augmentation did not necessarily improve validation performance, especially with a limited number of validation data.\n\nWhile we did not compare our method to simpler baselines in the traditional sense, our evaluation included assessing the impact of different training schedules and data augmentation strategies. This comparative analysis helped us understand the trade-offs between model complexity, training time, and detection accuracy.\n\nIn summary, our evaluation focused on optimizing the RetinaNet model for our specific use case, rather than comparing it to a wide range of publicly available methods or simpler baselines. The insights gained from this study will guide future research into developing more tailored and efficient object detection models for smart farm applications.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. However, upon reasonable request, the datasets can be made available from the corresponding author. This approach ensures that the data can be accessed for verification or further research while maintaining control over its distribution. The datasets were collected and labeled through a meticulous process involving both manual and automated labeling techniques, ensuring high accuracy and reliability. The data augmentation techniques applied, such as brightness conversion, contrast conversion, saturation conversion, rotation, random resize, and flip, helped in increasing the diversity of the dataset, which is crucial for training robust object detection models. The evaluation was conducted using metrics like average precision (AP), AP50, and AP75, which provide a comprehensive assessment of the model's performance. The results were visually verified using images different from those used for training, ensuring the model's generalizability."
}