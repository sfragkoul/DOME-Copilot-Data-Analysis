{
  "publication/title": "Evaluating Convolutional Neural Networks as a Method of EEG-EMG Fusion.",
  "publication/authors": "Tryon J, Trejos AL",
  "publication/journal": "Frontiers in neurorobotics",
  "publication/year": "2021",
  "publication/pmid": "34887739",
  "publication/pmcid": "PMC8649783",
  "publication/doi": "10.3389/fnbot.2021.692183",
  "publication/tags": "- EEG\n- EMG\n- CNN\n- Fusion\n- Classification\n- Motor Imagery\n- Hand Gestures\n- Rehabilitation\n- Assistive Robots\n- Task Weight",
  "dataset/provenance": "The dataset used in this study was collected from human participants who performed dynamic elbow flexion-extension motions at different speeds while holding varying weights. The EEG and EMG signals were recorded using an Intronix 2024F Physiological Amplifier System, sampled at 4,000 Hz. EEG signals were captured from the C3, C4, and Cz locations on the scalp, following the 10\u201320 International System, and EMG signals were recorded from the biceps and triceps of the dominant arm, adhering to the SENIAM Project guidelines.\n\nThe specific number of data points is not explicitly mentioned, but the dataset includes signals recorded during multiple repetitions of elbow movements at slow (10\u00b0/s) and fast (150\u00b0/s) speeds, with durations of 30 seconds and 2 seconds, respectively. The signals were segmented to remove portions where the subject was not moving, using markers placed manually by the experimenter during data recording. Synchronized video recordings were also taken for verification purposes.\n\nThe dataset is not readily available for public access because the Human Research Ethics Board at Western University has not given approval to share the data collected for this study. Requests to access the datasets should be directed to the corresponding author. This dataset has not been used in previous papers or by the community, as it is specific to this study. The signals were processed and converted into images suitable for input into a CNN classifier, with both spectrogram and signal images generated for each 250 ms window with 50% overlap.",
  "dataset/splits": "The dataset was split into three parts: training, validation, and testing. The first two repetitions of each speed-weight combination were dedicated as training data. Images generated from the third repetition were separated into two equally sized groups: validation and testing data. The order of the windows within the third motion repetition was randomized, and a stratified split was used to ensure a 50/50 division. This approach kept the number of observations of each class balanced within the validation and testing sets. The validation dataset was used during model optimization, while the testing set was kept separate until the final model evaluation to reduce potential for model bias and overfitting.",
  "dataset/redundancy": "The datasets were split into three parts: training, validation, and testing. The first two repetitions of each speed-weight combination were used for training, while the third repetition was divided equally into validation and testing sets. This split was designed to ensure independence between the training and test sets, which is crucial for reducing bias and overfitting.\n\nTo enforce this independence, the order of the windows within the third motion repetition was randomized. Additionally, a stratified split was employed to maintain a balanced number of observations for each class within the validation and testing sets. This approach ensures that the validation dataset was used during model optimization, while the testing set remained separate until the final model evaluation.\n\nThe distribution of the datasets in this study is comparable to previously published machine learning datasets in terms of ensuring independence between training and test sets. However, the specific details of the distribution may vary based on the unique characteristics of the EEG and EMG signals collected. The focus was on maintaining a balanced and unbiased split to facilitate robust model training and evaluation.",
  "dataset/availability": "The datasets used in this study are not publicly available. The Human Research Ethics Board at Western University has not granted approval to share the data collected for this study. Therefore, the datasets are not released in any public forum. Requests to access the datasets should be directed to the corresponding author. This restriction is in place to comply with ethical guidelines and to protect the privacy of the participants involved in the study.",
  "optimization/algorithm": "The machine-learning algorithm class used is Convolutional Neural Networks (CNNs). The specific models employed are spectrogram CNN models and signal CNN models. These models are not new; they are well-established in the field of machine learning and have been widely used for various tasks, including image and signal processing.\n\nThe choice of using CNNs for this study is due to their proven effectiveness in handling spatial and temporal data, which is crucial for EEG and EMG signal analysis. The models were optimized using hyperparameter tuning with the Random Search method, implemented through Keras-Tuner. This approach allowed for the exploration of different hyperparameter combinations to find the optimal settings for the models.\n\nThe decision to use established algorithms rather than developing new ones is driven by the robustness and reliability of CNNs in similar applications. Publishing in a neurorobotics journal aligns with the focus of the study, which is on evaluating CNNs for EEG\u2013EMG fusion in the context of neurorobotics. The emphasis is on applying and optimizing existing techniques to achieve specific goals in this domain, rather than introducing novel machine-learning algorithms.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding process involved converting EEG and EMG signals into images suitable for input into a convolutional neural network (CNN) classifier. This was necessary because CNNs were originally designed for image recognition tasks. Two primary methods were used to represent the signals as images: spectrogram images and signal images.\n\nSpectrogram images were generated by calculating the time-frequency domain representation of the signals. This method is useful for capturing the oscillatory behavior of the signals. Signal images, on the other hand, were created by organizing the processed signals in the time domain, which is better for representing time-varying behavior, such as changes in amplitude.\n\nTo ensure that the images were comparable and to account for differences in signal magnitudes, normalization was applied. For each subject and signal type, the max/min amplitude values of EEG and EMG were recorded and used to scale all signal values between 0 and 1. This normalization was done separately for EEG and EMG portions of the image to prevent larger EMG values from dominating the image and diminishing the contribution of smaller magnitude EEG signals.\n\nThe signals were segmented to remove portions where the subject was not moving, using markers placed manually during data recording. This segmentation was verified using synchronized video recordings. All signal processing and image generation were performed offline using MATLAB 2019b with the Signal Processing Toolbox.\n\nTo increase the number of images for classifier training, the signals were windowed using a 250 ms window with 50% overlap. This windowing was applied to both spectrogram and signal images, ensuring that both types of images were generated for each window. This approach helped in capturing the dynamic changes in the signals over time.\n\nIn summary, the data encoding process involved converting EEG and EMG signals into spectrogram and signal images, normalizing these images, segmenting the signals, and windowing the data to create a comprehensive dataset for training the CNN models.",
  "optimization/parameters": "In our study, we focused on tuning several key hyperparameters to optimize our convolutional neural network (CNN) models for EEG-EMG fusion. The specific hyperparameters that were adjusted during the optimization process included kernel size, kernel width, number of filters, dropout percentage, units in fully connected (FC) layers, and the learning rate for the Adam optimizer.\n\nThe kernel size for spectrogram models was chosen from options including 3x3, 5x5, and 7x7, with the 7x7 kernel size being used only in the third layer. For signal models, the kernel width ranged from 3 to 55, with a step size of 2. The number of filters in the convolutional layers was selected from a set of values: 8, 16, 32, 64, 128, 256, 512, and 1,024. However, due to memory constraints, the split convolution models were limited to a maximum of 512 filters.\n\nDropout percentages were varied from 0.0 to 0.5, with increments of 0.05, to help prevent overfitting. The number of units in the fully connected layers ranged from 20 to 500, with a step size of 20. The learning rate for the Adam optimizer was sampled logarithmically between 10^-5 and 10^-2.\n\nThese hyperparameters were tuned using the Random Search optimization method, which explored 50 random combinations of these parameters. Each combination was trained twice to account for variance in model training. The combination that resulted in the lowest validation loss was selected as the final set of hyperparameters.\n\nThe choice of these hyperparameters and their ranges was based on empirical testing and common practices in the field of deep learning. The goal was to find a balance between model complexity and performance, ensuring that the models could generalize well to new data.",
  "optimization/features": "In our study, the input features for the convolutional neural networks (CNNs) were derived from electroencephalogram (EEG) and electromyogram (EMG) signals. These signals were processed into two types of images: spectrogram images and signal images.\n\nFor the spectrogram images, three different fusion methods were employed to combine EEG and EMG data: grouped, mixed, and stacked. The grouped method arranges signal channels of the same type together within the image. The mixed method alternates EEG and EMG channels to mix signal types. The stacked method generates a multi-channel spectrogram by combining different EEG/EMG spectrograms in a depth-wise manner.\n\nThe signal images were constructed with a height containing rows for each signal channel and a width dictated by the number of samples in each 250 ms window. These images were normalized by subject and by signal type to scale all signal values between 0 and 1, accounting for magnitude differences between EEG and EMG signals.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the raw signal data was transformed into image representations, which were then used as input features for the CNNs. This approach leverages the spatial and temporal information contained within the signals, allowing the CNNs to learn relevant features directly from the data.\n\nThe normalization process ensured that the input features were standardized, which is crucial for the effective training of neural networks. This process was applied using the training set only, ensuring that the validation and testing sets remained independent and unbiased.",
  "optimization/fitting": "The fitting method employed in our study involved a careful balance to avoid both overfitting and underfitting. To address the potential issue of overfitting, given the complexity of our models and the relatively large number of parameters, several strategies were implemented.\n\nEarly stopping was used during model training to prevent overfitting. This technique monitors the validation loss and stops training when improvements are no longer observed, effectively halting the process before the model begins to memorize the training data. A patience value of five and an epoch limit of 50 were set to ensure that the model had sufficient opportunities to learn without overfitting.\n\nDropout layers were incorporated into both the convolutional and fully connected layers. Dropout randomly sets a fraction of input units to zero during training, which helps to prevent the model from becoming too reliant on any single feature and thus reduces overfitting.\n\nBatch normalization was initially tested but was found to reduce accuracy, so it was not used. Instead, dropout layers were relied upon to mitigate overfitting.\n\nTo ensure that the models were not underfitting, hyperparameter tuning was performed using the Random Search method. This involved evaluating 50 random combinations of hyperparameters, each trained twice to account for variance in model training. The hyperparameters that resulted in the lowest validation loss were selected as the final set for model training. This process helped to identify the optimal configuration for each model, ensuring that they were neither too simple nor too complex.\n\nAdditionally, the use of a validation dataset allowed for continuous evaluation of model performance during training. This ensured that the models were generalizing well to unseen data, further mitigating the risk of underfitting.\n\nIn summary, the fitting method included early stopping, dropout layers, and thorough hyperparameter tuning to balance the model complexity and prevent both overfitting and underfitting.",
  "optimization/regularization": "In our study, several regularization methods were employed to prevent overfitting and improve the generalization of our models. One key technique used was dropout, which was applied after each convolutional and fully connected layer. Dropout randomly sets a fraction of the input units to zero during training, helping to prevent the model from becoming too reliant on any single feature. The dropout rate was treated as a hyperparameter and tuned within a range of 0.0 to 0.5.\n\nAdditionally, early stopping was implemented during model training. This technique monitors the validation loss and stops the training process when the validation loss no longer improves for a specified number of epochs, known as the patience value. In our case, a patience value of five was used, along with an epoch limit of 50. This approach ensures that the model does not continue training beyond the point where it starts to overfit the training data.\n\nBatch normalization was initially tested as an alternative to dropout for the convolutional layers. However, it was found to reduce the model's accuracy, so it was not included in the final configurations. Instead, dropout remained the primary regularization method for both convolutional and fully connected layers.\n\nThese regularization techniques, combined with hyperparameter tuning and a careful split of the data into training, validation, and testing sets, helped to mitigate overfitting and enhance the robustness of our CNN models for EEG\u2013EMG fusion.",
  "optimization/config": "The hyperparameter configurations and optimization schedules used in our study are detailed within the publication. Specifically, the hyperparameters that were tuned, along with their respective ranges, are outlined in Table 2. This table includes details such as kernel sizes, filter counts, dropout percentages, units in fully connected layers, and learning rates for the ADAM optimizer. The optimization process involved using the Random Search method with Keras-Tuner 1.0.1, and the search space checked 50 random combinations of hyperparameters, each trained twice to account for variance.\n\nThe model configurations, including the base designs for both spectrogram and signal CNN models, are visually represented in Figures 5 and 6. These figures provide a clear depiction of the architectural layers and their arrangements. The spectrogram models consisted of three convolutional layers followed by two fully connected layers, while the signal models had two base configurations tested.\n\nRegarding the availability of model files and optimization parameters, the specifics of these are not directly provided in the text. However, the methods and results described are intended to be reproducible given the detailed information on hyperparameter tuning and model architecture. The publication does not explicitly mention the availability of model files or optimization parameters for download, nor does it specify any licensing details for such resources. For further details or access to specific model files, readers are encouraged to contact the authors directly.",
  "model/interpretability": "The models used in this study are primarily convolutional neural networks (CNNs), which are known for their black-box nature. This means that while they can achieve high accuracy in classification tasks, the internal workings and decision-making processes are not easily interpretable. The CNNs process input data through multiple layers of convolutions, pooling, and fully connected layers, transforming the data in ways that are not straightforward to understand or visualize.\n\nHowever, some aspects of the models can be considered transparent. For instance, the use of spectrograms and signal images as inputs provides a visual representation of the EEG and EMG data. These images show qualitative variations in the signals as task weights change, which can be observed and interpreted by humans. The distribution of frequency magnitudes across time/channels in spectrograms and the shape of the time-domain signals vary with different task weights, providing a qualitative demonstration of changing patterns within the images.\n\nAdditionally, the confusion matrices generated from the class predictions offer some level of interpretability. These matrices show how often each class is correctly or incorrectly predicted, allowing for an assessment of the model's performance on a per-class basis. Precision and recall scores calculated from the confusion matrices further help in understanding the balance between the metrics for each class.\n\nThe hyperparameter tuning process also adds a layer of transparency. By tuning parameters such as kernel size, filters, dropout percentage, and learning rate, the models are optimized for better performance. The ranges and values of these hyperparameters are documented, providing insight into the model's configuration and the decisions made during the training process.\n\nIn summary, while the CNNs used in this study are largely black-box models, certain aspects such as the visual representation of input data, confusion matrices, and hyperparameter tuning provide some level of interpretability. These elements help in understanding the model's behavior and performance, even if the internal decision-making processes remain opaque.",
  "model/output": "The model is designed for classification tasks. Specifically, it is trained to classify task weights based on EEG and EMG data. The final output layer of the model uses a softmax activation function, which is commonly used for multi-class classification problems. The model is trained to output a three-class prediction, corresponding to the three task weights used during data collection: 0 lbs, 3 lbs, and 5 lbs. The predictions are then compared to the actual class labels to evaluate the model's accuracy. This classification approach allows the model to determine the task weight being held during each test image, providing insights into the performance of CNN-based EEG-EMG fusion.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method for our study involved a comprehensive assessment of the performance of CNN-based EEG\u2013EMG fusion models. We trained optimized models for each subject and evaluated them using withheld test data. This data was used to obtain predictions about the task weight being held during each test image. Since three task weights were used during data collection (0 lbs, 3 lbs, and 5 lbs), each classifier was trained to output a three-class prediction, corresponding to one of the three task weights.\n\nThe accuracy of each model was calculated by comparing the predicted class labels with the actual class labels. This accuracy was then averaged across all subjects to obtain an overall accuracy score for each fusion method. Statistical analysis was performed using IBM SPSS 27 to compare the performance of different fusion methods.\n\nWe conducted a one-way Within-Subjects Analysis of Variance (ANOVA) followed by pairwise comparisons with the Bonferroni post-hoc test on the accuracy scores for the models of each type. This was done to determine if the increase in accuracy obtained via EEG\u2013EMG fusion was statistically significant. Separate ANOVAs were used for each model type to account for the different number of models present, depending on the type.\n\nAdditionally, we evaluated the robustness of each model by assessing the effect of movement speed on accuracy. The classifier output predictions were separated based on the speed at which the movement was being performed, and accuracy was calculated separately for fast and slow movement speed groups. A two-way Within-Subjects ANOVA was performed on the speed-separated accuracies for each model type to determine if the effect of speed was statistically significant.\n\nAs a final analysis of model performance, we combined the class predictions from every subject and plotted a confusion matrix for each CNN model. This was done to observe how the models performed for each task weight and to further verify that the classifiers were adequately trained. The confusion matrices were used to calculate class-wise precision and recall scores to check the balance between these metrics.",
  "evaluation/measure": "In our study, we employed several performance metrics to comprehensively evaluate the CNN-based EEG\u2013EMG fusion models. The primary metric reported is accuracy, which measures the proportion of correct predictions made by the models. This metric was calculated for each model and averaged across all subjects to obtain an overall accuracy score. We also performed statistical analyses, including one-way and two-way ANOVAs, to compare the accuracy of different fusion methods and to assess the significance of speed on model performance.\n\nIn addition to accuracy, we utilized confusion matrices to provide a detailed breakdown of the models' performance. These matrices allowed us to observe how well the models performed for each task weight and to verify that the classifiers were adequately trained. From the confusion matrices, we derived class-wise precision and recall scores. Precision indicates the likelihood that a class prediction is correct, while recall measures the likelihood that all observations of a specific class are correctly classified. These metrics helped us evaluate the balance between precision and recall, ensuring that the models were not overly biased towards any particular class.\n\nThe use of these performance metrics is representative of standard practices in the literature. Accuracy is a widely reported metric in classification tasks, providing a straightforward measure of model performance. Confusion matrices offer a more granular view of model performance, highlighting strengths and weaknesses in class predictions. Precision and recall are crucial for understanding the trade-offs between false positives and false negatives, which is particularly important in applications where the cost of misclassification varies between classes. By reporting these metrics, we aim to provide a thorough evaluation of our models' performance, aligning with established practices in the field.",
  "evaluation/comparison": "In our study, we evaluated the performance of various convolutional neural network (CNN) models for EEG\u2013EMG fusion, focusing on classifying task weight during dynamic elbow flexion\u2013extension motion. We compared different fusion methods, including grouped, mixed, stacked, and 1D convolution, as well as models that used EEG or EMG signals alone.\n\nWe did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our approach was to investigate the initial feasibility of different CNN-based EEG\u2013EMG fusion methods. We assessed the performance of these models by calculating their accuracy, precision, and recall scores. Statistical analyses, such as one-way and two-way within-subjects ANOVAs, were conducted to determine the significance of the differences in accuracy between the models.\n\nIn terms of simpler baselines, we compared the fusion methods to models that used either EEG or EMG signals alone. This allowed us to evaluate whether the fusion of EEG and EMG signals provided a significant advantage in classification accuracy. The results indicated that, in general, the fusion methods outperformed the models that used EEG or EMG alone, particularly for the spectrogram-based models. However, the performance of the 1D convolution method was not statistically significantly different from the other fusion methods, suggesting that it could be a viable option for real-time applications due to its computational efficiency.\n\nNot applicable",
  "evaluation/confidence": "The evaluation of the models included statistical analysis to determine the significance of the results. A significance threshold of p < 0.05 was used for all statistical tests. This threshold was applied to assess the overall model accuracy and the speed-specific accuracy.\n\nFor the spectrogram-based CNN models, the grouped fusion method achieved the highest accuracy, significantly outperforming the EEG and mixed fusion models. The stacked fusion method also showed a statistically significant increase in accuracy over the multi-channel EEG model. However, the differences between the grouped fusion method and the EMG model, as well as between the stacked fusion method and the multi-channel EMG model, were not statistically significant.\n\nIn the signal-based CNN models, the 1D convolution EEG\u2013EMG fusion model demonstrated a statistically significant increase in accuracy over using EEG alone. The trend suggested an increase in accuracy when using EMG alone, but this was not statistically significant.\n\nThe effect of speed on model performance was also statistically significant for all model types, with performance being worse during fast speeds. Despite this, all models remained above the chance level during fast motion.\n\nConfusion matrices were used to evaluate the class-wise precision and recall scores, providing a balanced view of the models' performance. The spectrogram-based models generally showed higher and more balanced precision and recall scores compared to the signal-based models.\n\nOverall, the statistical significance of the results supports the claim that the EEG\u2013EMG fusion methods, particularly the spectrogram-based models, are superior to using EEG or EMG alone. The confidence intervals, represented by error bars in the figures, indicate the variability in the accuracy measurements, further supporting the reliability of the findings.",
  "evaluation/availability": "Not enough information is available."
}