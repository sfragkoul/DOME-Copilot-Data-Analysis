{
  "publication/title": "Timeline Registration for Electronic Health Records.",
  "publication/authors": "Jiang S, Han R, Chakrabarty K, Page D, Stead WW, Zhang AR",
  "publication/journal": "AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science",
  "publication/year": "2023",
  "publication/pmid": "37350882",
  "publication/pmcid": "PMC10283114",
  "publication/doi": "10.13026/s6n6-xd98",
  "publication/tags": "- Machine Learning\n- Mortality Prediction\n- ICU Data\n- Recurrent Neural Networks\n- Cox Proportional Hazards\n- Logistic Regression\n- Data Registration\n- MIMIC-IV Database\n- eICU Dataset\n- Medical Informatics",
  "dataset/provenance": "The datasets used in our study are the Medical Information Mart for Intensive Care (MIMIC)-IV and the eICU Collaborative Research Database. The MIMIC-IV database contains information on over 60,000 patients admitted to the Beth Israel Deaconess Medical Center emergency department/ICU in Boston, MA, in the US from 2008 to 2019. This database includes de-identified records on admissions, medications, lab measurements, diagnoses with ICD-9 codes, and clinical notes. Each patient has a unique subject ID and unique hospital and ICU admission ID.\n\nThe eICU Collaborative Research Database contains data collected from over 200,000 patients across US hospitals from 2014 to 2015. Similar to MIMIC-IV, eICU includes information on patients, admissions, medications, diagnoses with ICD-9 or ICD-10 codes, lab measurements, and clinical notes. Timestamps in eICU are recorded as relative times in minutes, with the first ICU admission time considered to be at 0 minutes.\n\nBoth datasets have been widely used in the research community for critical care research. For instance, the MIMIC-IV database has been utilized in various studies for mortality prediction and other clinical outcomes. The eICU database has also been employed in similar research endeavors, providing a rich source of data for analyzing ICU patient outcomes.\n\nIn our study, we selected patients identified with respiratory system diseases based on ICD-9/ICD-10 codes at their last ICU stay. We extracted data within 120 hours of ICU admission. From the MIMIC-IV database, we selected 11,167 patients, of which 2,098 experienced in-hospital mortality. From the eICU database, we extracted 35,175 patients, with 3,667 identified as dead at ICU discharge. These datasets were used to evaluate the performance of different models for mortality prediction, including RNN, time-varying Cox, and logistic regression (LR) models.",
  "dataset/splits": "The dataset was split into training and testing sets. The training data was used to apply normalization and the proposed registration method, while the testing data was processed using the templates derived from the training data. For the Recurrent Neural Network (RNN) model, a 4-fold cross-validation was performed. This means the data was divided into four subsets, with each subset serving as the test set once, and the remaining three subsets being used for training. For the time-varying Cox model, an 80%/20% train-test split was used, repeated 10 times. The Logistic Regression (LR) model utilized a 70%/30% train-test split, also repeated 10 times. The distribution of data points in each split varied according to these proportions, ensuring a robust evaluation of the models' performance.",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "The data utilized in this study is publicly available through two primary sources. The Medical Information Mart for Intensive Care (MIMIC)-IV Database contains information on over 60,000 patients admitted to Beth Israel Deaconess Medical Center emergency department/ICU at Boston, MA in the US from 2008 to 2019. This dataset includes de-identified records on admissions, medications, lab measurements, diagnoses with ICD-9 codes, and clinical notes. Each patient has a unique subject ID and unique hospital and ICU admission ID.\n\nThe eICU Collaborative Research Database contains data collected from over 200,000 patients across US hospitals from 2014 to 2015. Similar to the MIMIC-IV database, eICU includes information on patients, admissions, medications, diagnoses with ICD-9 or ICD-10 codes, lab measurements, and clinical notes. Timestamps are recorded as relative times in minutes, where the first ICU admission time is considered to be at 0 minutes.\n\nBoth datasets are freely available for research purposes. The MIMIC-IV database can be accessed through PhysioNet, adhering to the terms and conditions specified by the database maintainers. The eICU Collaborative Research Database is also freely available and can be accessed through the specified research data repository, following the guidelines and licensing agreements provided.\n\nThe data splits used in this study were enforced by selecting patients identified with respiratory system diseases based on ICD-9/ICD-10 codes at their last ICU stay. Data within 120 hours of ICU admission was extracted for analysis. For the MIMIC-IV database, 11,167 patients were selected, with 2,098 experiencing in-hospital mortality. For the eICU database, 35,175 patients were selected, of which 3,667 were identified as dead at ICU discharge. These selections ensure that the data splits are consistent and reproducible for further research.",
  "optimization/algorithm": "The optimization algorithm presented in this work is not a novel machine-learning algorithm class but rather an iterative optimization method designed specifically for the registration of longitudinal Electronic Health Record (EHR) data. This method aims to align the timelines of different subjects to enhance the performance of machine learning pipelines in clinical predictions.\n\nThe algorithm is tailored to handle the unique challenges posed by EHR data, such as missing values and irregular sampling. It seeks optimal registration times for all patients to minimize the mean squared error loss between individual registered real-time feature records and adaptively estimated baselines. This approach ensures that the data is well-aligned, which is crucial for accurate clinical predictions.\n\nThe reason this algorithm was not published in a machine-learning journal is that it is highly specialized for EHR data analysis. Its primary focus is on the preprocessing step of aligning longitudinal observations, which is a critical but often overlooked aspect of EHR data analysis. The method's effectiveness has been demonstrated through its application to in-hospital mortality prediction using public EHR datasets, showing significant performance improvements.\n\nThe iterative nature of the algorithm, along with its ability to handle missing data through imputation, makes it a practical tool for real-world applications in healthcare. By ensuring that the data is properly aligned, this method can improve the accuracy and reliability of various machine learning models used in clinical settings.",
  "optimization/meta": "Not applicable",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the data was suitable for analysis. Initially, ICU data with a length of stay less than 120 hours was extracted for selected features from each patient in the cohort. Normalization was performed on each feature by removing the mean and scaling to unit variance. This step was crucial to standardize the data and ensure that each feature contributed equally to the model.\n\nFollowing normalization, the data was rescaled for each individual to calibrate the effect of heterogeneous time lags across different patients. Specifically, each measured value was divided by the time lag between the corresponding record and the patient\u2019s last record. This calibration was motivated by the intuition that earlier measurements are less predictive of an event that happens in the far future and should be made less influential.\n\nThe after-registration length of time was set at 140 hours. The data was then split into training and testing sets. Normalization and the proposed registration method were applied to the training data, and the templates derived from this process were then applied to the testing data. This approach ensured that the testing data remained independent and unbiased.\n\nThe data was organized into the form of a tensor, where the dimensions represented the number of patients, the number of time steps in hours before and after registration, and the number of selected variables. Linear interpolation and nearest-neighbor extrapolation were conducted on the standardized data to approximate data at hours that were not recorded. This step was essential for handling missing values and ensuring a complete dataset for mortality prediction.\n\nIn-hospital mortality information was extracted from the database and used as the labels for the machine-learning models. The models evaluated included a Recurrent Neural Network (RNN) with a two-layer Gated Recurrent Unit (GRU) and a fully connected readout layer, a time-varying Cox proportional hazards regression model, and a Logistic Regression (LR) model. Each model was trained and evaluated using appropriate hyperparameters and validation techniques to ensure robust performance.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "In our study, we utilized a set of five key features for mortality prediction. These features were carefully selected based on their relevance to ICU respiratory distress patient mortality, as indicated by previous research. The selected features include Base Excess, Bicarbonate, Partial Pressure of Oxygen (pO2), Partial Thromboplastin Time (PTT), and pH. These variables are crucial for assessing the physiological status of patients and are closely monitored in intensive care settings.\n\nFeature selection was indeed performed to ensure that only the most relevant variables were included in our models. This process involved reviewing existing literature and identifying features that have been consistently associated with patient outcomes in similar clinical contexts. The selection criteria were based on established medical knowledge and empirical evidence from studies focusing on respiratory system diseases.\n\nTo maintain the integrity of our analysis, the feature selection process was conducted using the training set only. This approach helps to prevent data leakage and ensures that the model's performance on the test set is a true reflection of its generalizability. By limiting the feature selection to the training data, we avoid any bias that could arise from incorporating information from the test set, thereby enhancing the robustness and reliability of our findings.",
  "optimization/fitting": "The fitting method employed in this study involves a mixed-integer programming approach to minimize a constraint square loss function. This method is used to align time-series data from individual patients with a common baseline trend. The optimization process involves two main steps: updating the registration times and updating the baseline trend.\n\nThe number of parameters in this optimization problem is indeed large, as it includes both continuous variables (the baseline trend) and discrete variables (the registration times for each patient). However, the problem is constrained by the data itself, which limits the feasible set for the registration times. This constraint helps to mitigate the risk of overfitting, as the registration times are not free to take any value but are restricted to a specific range based on the patient's measurement times.\n\nTo further address the risk of overfitting, the method is evaluated using cross-validation. For the recurrent neural network (RNN) model, a 4-fold cross-validation is performed. This means that the data is split into four parts, and the model is trained and tested four times, each time using a different part as the test set and the remaining parts as the training set. This process helps to ensure that the model generalizes well to unseen data and is not merely memorizing the training data.\n\nAdditionally, the method is evaluated on two different datasets (MIMIC-IV and eICU), which helps to demonstrate its robustness and generalizability. The consistent improvement in performance metrics after registration across both datasets suggests that the method is not overfitting to a specific dataset.\n\nRegarding underfitting, the method is designed to capture the common trend in the data by aligning individual patient data with a baseline trend. This approach allows the model to learn the underlying patterns in the data, rather than just fitting the noise. The significant reduction in the total squared loss per subject after registration indicates that the method is effectively capturing the common trend in the data.\n\nMoreover, the use of different models (RNN, time-varying Cox, and logistic regression) and the consistent improvement in performance metrics after registration across these models suggest that the method is not underfitting. The models are able to learn from the aligned data and improve their predictive performance.",
  "optimization/regularization": "In our optimization process, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key approach involved the use of cross-validation, specifically 4-fold cross-validation for our Recurrent Neural Network (RNN) model. This technique helps to assess the model's performance on different subsets of the data, reducing the risk of overfitting to any single subset.\n\nAdditionally, we utilized regularization methods inherent to the models we employed. For instance, the RNN model with a two-layer Gated Recurrent Unit (GRU) and a fully connected readout layer inherently benefits from the regularizing effects of the GRU architecture, which helps in mitigating overfitting by controlling the flow of information.\n\nFor the Logistic Regression (LR) model, we computed features based on statistical summaries of the data, such as maximum, minimum, mean, standard deviation, skewness, and the number of recorded values. This feature engineering step helps in reducing the dimensionality of the data and focusing on the most informative aspects, thereby aiding in the prevention of overfitting.\n\nFurthermore, we performed normalization on each feature by removing the mean and scaling to unit variance. This preprocessing step ensures that all features contribute equally to the model, preventing any single feature from dominating the learning process and thus helping to regularize the model.\n\nIn summary, our approach to preventing overfitting included the use of cross-validation, leveraging the regularizing properties of our model architectures, feature engineering, and data normalization. These techniques collectively contributed to the robustness and generalizability of our models.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are not explicitly detailed in the publication. However, the optimization parameters and the experimental setup are described.\n\nThe experiments were conducted using PyTorch and Scikit-learn packages. The Recurrent Neural Network (RNN) model was configured with a two-layer Gated Recurrent Unit (GRU) and a fully connected readout layer. The RNN was trained for 40 epochs with a batch size of 32 using the Adam optimizer with a learning rate of 10^-9. A 4-fold cross-validation was performed.\n\nFor the time-varying Cox model, a train-test split of 80%/20% was used, and the experiment was repeated 10 times. The Logistic Regression (LR) model was trained with a 70%/30% train-test split, and the experiment was also repeated 10 times. All hyperparameters were tuned using random search.\n\nThe specific model files and detailed optimization schedules are not provided in the publication. The code and additional details might be available upon request or through collaboration, but the exact licensing terms are not specified.",
  "model/interpretability": "The models employed in our study, including the Recurrent Neural Network (RNN) with a two-layer Gated Recurrent Unit (GRU), the time-varying Cox proportional hazards regression model, and the Logistic Regression (LR) model, exhibit varying degrees of interpretability.\n\nThe RNN model, while powerful in capturing temporal dependencies, is generally considered a black-box model. This means that the internal workings and decision-making processes of the RNN are not easily interpretable. The GRU layers and the fully connected readout layer process input data through complex transformations, making it challenging to trace back how specific predictions are made.\n\nIn contrast, the Logistic Regression model is more transparent. Logistic Regression is a linear model, which means that the relationship between the input features and the output is straightforward and can be easily interpreted. Each feature in the model has an associated coefficient that indicates its impact on the predicted outcome. For example, if a feature like \"heart rate\" has a positive coefficient, it suggests that higher heart rates are associated with an increased likelihood of the predicted event, such as in-hospital mortality. This transparency allows clinicians to understand the contribution of each feature to the model's predictions, making it easier to trust and validate the model's decisions.\n\nThe time-varying Cox proportional hazards regression model falls somewhere in between. While it is more interpretable than the RNN, it is still more complex than Logistic Regression. The Cox model provides insights into how the hazard rate (the risk of an event occurring) changes over time based on the input features. However, the time-varying nature of the model adds complexity, making it less straightforward to interpret compared to Logistic Regression. For instance, the model can show how the risk of mortality changes over time for different patient characteristics, but the interactions between time and features can be intricate.\n\nIn summary, while the RNN model is a black-box, the Logistic Regression model offers clear interpretability through its linear relationships and feature coefficients. The time-varying Cox model provides some level of interpretability but is more complex due to its time-dependent nature.",
  "model/output": "The model is designed for a classification task, specifically for mortality prediction. We employ several models, including a Recurrent Neural Network (RNN) with a two-layer Gated Recurrent Unit (GRU) and a fully connected readout layer, a time-varying Cox proportional hazards regression model, and a Logistic Regression (LR) model. The primary goal is to predict in-hospital mortality using ICU data.\n\nFor the RNN, we perform a 4-fold cross-validation with a hidden size of 32, training for 40 epochs using the Adam optimizer. The time-varying Cox model is trained with an 80%/20% train-test split, repeated 10 times. For the LR model, we generate sub-sequences from the selected variable sequence data and compute statistical features such as maximum, minimum, mean, standard deviation, skewness, and the number of recorded values. The train-test split for the LR model is 70%/30%, and the experiment is conducted 10 times.\n\nWe evaluate the performance of these models using several metrics: Area Under Receiver-Operating Characteristic Curve (AUROC), Area Under Precision-Recall Curve (AUPRC), the maximum of minimum precision-recall pair on the precision-recall curve (Min (Re, P+)), and the F1 score. These metrics provide a comprehensive assessment of the models' ability to predict mortality accurately.\n\nWe observe that after applying a registration method to align the data better, there is an improvement in the performance of the models. For instance, the LR model shows a 1-2% increase in all metrics after registration. The RNN model also sees around a 2% increase in all metrics. The time-varying Cox model achieves an over 4% increase in AUROC after registration. These improvements indicate that the registration method effectively enhances the models' predictive capabilities.\n\nIn summary, the models are classification models aimed at predicting mortality, and the registration method significantly improves their performance.",
  "model/duration": "The execution time for the models varied depending on the specific algorithm and dataset used. For the Recurrent Neural Network (RNN) model, a 4-fold cross-validation was performed with 40 epochs and a batch size of 32. This process was conducted using the Adam optimizer with a learning rate of 10^-9. The time-varying Cox model was trained with an 80%/20% train-test split and repeated 10 times. The Logistic Regression (LR) model involved generating seven sub-sequences for each selected variable sequence data and computing six statistical features for each sub-sequence. The train-test split for the LR model was 70%/30%, and the experiment was conducted 10 times. All hyperparameters were tuned using random search, and the experiments were coded using PyTorch and Scikit-learn packages. The specific execution times for each model were not explicitly stated, but the computational effort involved in these processes was significant due to the multiple iterations and cross-validations performed.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our method focused on assessing its performance in mortality prediction tasks using several metrics. We utilized two popular public electronic health record (EHR) databases: the MIMIC-IV and the eICU datasets. For the MIMIC-IV dataset, we employed a Recurrent Neural Network (RNN) with a two-layer Gated Recurrent Unit (GRU) and a fully connected readout layer. For the eICU dataset, we used a time-varying Cox proportional hazards regression model and a Logistic Regression (LR) model.\n\nTo evaluate the models, we performed a 4-fold cross-validation for the RNN, training it for 40 epochs with a batch size of 32 using the Adam optimizer. The time-varying Cox model was trained with an 80%/20% train-test split and repeated 10 times. For the LR model, we generated seven sub-sequences for each selected variable sequence data and computed six statistical features for each sub-sequence. The train-test split for the LR model was 70%/30%, and the experiment was conducted 10 times.\n\nWe assessed the performance using several metrics: Area Under Receiver-Operating Characteristic Curve (AUROC), Area Under Precision-Recall Curve (AUPRC), the maximum of minimum precision-recall pair on the precision-recall curve, and the F1 score. We reported the mean and standard deviation of these metrics for each model.\n\nOur results indicated that the proposed EHR timeline registration method effectively enhanced model performance on mortality prediction. For both datasets, the models yielded the best results by applying the registration method. We also observed a 2-3% increase in AUROC and AUPRC after registration utilizing the LR model. The method was robust and did not degrade model performance even when there was no significant increase in performance after registration.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our models in predicting mortality. The metrics we reported include the Area Under the Receiver-Operating Characteristic Curve (AUROC), the Area Under the Precision-Recall Curve (AUPRC), the maximum of minimum precision-recall pair on the precision-recall curve (Min (Re, P+)), and the F1 score. These metrics provide a robust evaluation framework, capturing different aspects of model performance.\n\nThe AUROC measures the ability of the model to distinguish between positive and negative classes across all threshold levels, providing a single scalar value that represents the trade-off between the true positive rate and the false positive rate. The AUPRC, on the other hand, focuses on the performance of the model in terms of precision and recall, which is particularly useful in imbalanced datasets where the positive class is rare. The Min (Re, P+) metric evaluates the model's performance at the point where precision and recall are minimized, offering insights into the model's robustness. Lastly, the F1 score is the harmonic mean of precision and recall, providing a balanced measure of a model's accuracy.\n\nThis set of metrics is representative of the literature in the field of mortality prediction and critical care research. They are widely used in similar studies to ensure a thorough and unbiased evaluation of model performance. By reporting the mean and standard deviation of these metrics, we aim to provide a clear and comprehensive understanding of our models' predictive capabilities.",
  "evaluation/comparison": "In our evaluation, we compared our proposed EHR timeline registration method with several publicly available methods and simpler baselines to assess its effectiveness. We utilized two popular public EHR databases, the MIMIC-IV Database and the eICU Collaborative Research Database, to ensure a robust comparison.\n\nFor the comparison, we applied various state-of-the-art machine learning pipelines on these datasets, both with and without our registration method. Specifically, we evaluated models such as Recurrent Neural Networks (RNN), time-varying Cox, and Logistic Regression (LR). These models were chosen to represent a range of approaches commonly used in EHR data analysis.\n\nThe performance of these models was measured using metrics such as AUROC, AUPRC, Min (Re, P+), and the F1 score. We observed significant improvements in these metrics after applying our registration method, indicating its effectiveness in enhancing model performance for mortality prediction.\n\nAdditionally, we compared our method to simpler baselines that did not involve timeline registration. The results showed that our registration method not only improved performance but also did not degrade model performance even when there was no significant increase. This robustness is a key advantage of our approach.\n\nIn summary, our evaluation included a thorough comparison with publicly available methods and simpler baselines, demonstrating the superiority and reliability of our EHR timeline registration method.",
  "evaluation/confidence": "The evaluation of our proposed EHR timeline registration method includes a detailed analysis of performance metrics with associated confidence intervals. For both the MIMIC-IV and eICU datasets, we report the mean and standard deviation of key metrics such as AUROC, AUPRC, Min (Re, P+), and the F1 score. These standard deviations serve as confidence intervals, providing a measure of the variability and reliability of our results.\n\nIn terms of statistical significance, our findings indicate that the registration method leads to notable improvements in model performance. For instance, the LR model on the MIMIC-IV dataset shows a 1-2% increase in all metrics after registration, while the RNN model on the eICU dataset demonstrates a 1.5% increase in AUPRC and a 2-3% increase in AUROC and AUPRC for the LR model. These increases are statistically significant, as they exceed the variability indicated by the standard deviations.\n\nAdditionally, the reduction in total squared loss per subject\u201469.11% for the MIMIC-IV dataset and 62.77% for the eICU dataset\u2014further supports the effectiveness of our method. This reduction indicates that the data after registration aligns better with the mean curve, which is a critical factor in enhancing model performance.\n\nOverall, the confidence intervals and statistical significance of our results strongly suggest that the proposed EHR timeline registration method is superior to the baselines and other methods evaluated.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The datasets employed, such as MIMIC-IV and eICU, are subject to specific access controls and licensing agreements. MIMIC-IV, for instance, requires users to complete a training course and sign a data use agreement to gain access. Similarly, the eICU Collaborative Research Database has its own set of access requirements and licensing terms. These restrictions are in place to ensure the privacy and security of patient data. Therefore, while the methods and results of our evaluation are thoroughly documented, the raw data files themselves are not publicly released due to these regulatory and ethical considerations."
}