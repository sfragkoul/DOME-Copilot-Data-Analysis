{
  "publication/title": "Ranking factors involved in diabetes remission after bariatric surgery using machine-learning integrating clinical and genomic biomarkers.",
  "publication/authors": "Pedersen HK, Gudmundsdottir V, Pedersen MK, Brorsson C, Brunak S, Gupta R",
  "publication/journal": "NPJ genomic medicine",
  "publication/year": "2016",
  "publication/pmid": "29263820",
  "publication/pmcid": "PMC5685313",
  "publication/doi": "10.1038/npjgenmed.2016.35",
  "publication/tags": "- Diabetes remission\n- Bariatric surgery\n- Single-nucleotide polymorphisms (SNPs)\n- Artificial neural networks\n- Machine learning\n- Genomic medicine\n- Predictive modeling\n- Clinical traits\n- Feature selection\n- Cross-validation\n- Genetic association studies\n- Obesity\n- Metabolic diseases\n- Patient stratification\n- Data processing",
  "dataset/provenance": "The dataset used in this study was obtained from dbGaP, specifically through Project ID 4676 and dbGaP accession number phs000380.v1.p1. Initially, the cohort consisted of 467 patients. However, to reduce redundancy and improve the quality of the dataset, a modified version of the Hobohm algorithm was applied. This process involved removing phenotypically similar patients, resulting in a final dataset of 268 individuals. This reduced dataset was used for further analysis and model training.\n\nThe dataset includes both clinical and genetic data. The clinical data encompasses various traits relevant to the study, while the genetic data consists of single nucleotide polymorphisms (SNPs). Initially, the genetic data included 86,444 SNPs, which were enriched for biological importance and reduced to a subset of 960 SNPs using prior biological knowledge. Further feature selection processes, including forward feature selection using artificial neural networks (ANNs), narrowed down the SNPs to a final set of eight.\n\nThe dataset has been utilized in previous analyses and by the community, as indicated by the use of data from the MAGIC investigators, which is available at www.magicinvestigators.org. The technical details and methodologies applied in this study build upon established practices and previous research in the field.",
  "dataset/splits": "In our study, we employed a nested cross-validation approach to ensure robust model training and validation. This method involved creating multiple data splits to evaluate the performance of our artificial neural network models.\n\nThe outer cross-validation consisted of five folds, meaning the data was divided into five parts. In each iteration, one part was used as the test set, and the remaining four parts were used as the training set. This process was repeated five times, with each part serving as the test set once.\n\nWithin each outer fold, an inner cross-validation was performed with five folds and repeated five times, resulting in a total of 125 sets of features selected. This nested structure helped to reduce the risk of overfitting and provided a more reliable estimate of model performance.\n\nInitially, the dataset consisted of 467 patients. However, to reduce redundancy and improve class balance, we applied a modified Hobohm 2 algorithm with a Gower similarity threshold of 0.925. This step reduced the number of patients to 268, ensuring that the final dataset included both patients similar to their peers and those who were very different from the majority.\n\nThe distribution of data points in each split was designed to ensure that subjects without diabetes remission were equally distributed across the different cross-validation splits. This balanced distribution helped in maintaining the integrity of the model evaluation process.\n\nIn summary, the dataset was split into five outer folds, each containing approximately 214 training samples and 54 test samples. Within each outer fold, the inner cross-validation further divided the training data into five parts, each containing around 171 samples for training and 43 samples for validation. This comprehensive splitting strategy allowed for thorough feature selection and model validation.",
  "dataset/redundancy": "The dataset was initially composed of 467 patients. To address the issue of patient similarity, a modified version of the Hobohm 2 algorithm was employed. This algorithm favors the removal of similar patients who have many missing observations, thereby creating a more complete final dataset. Patients were defined as similar if their Gower similarity coefficient of phenotype vectors was above 0.925. This threshold was chosen because the dataset includes both metric and dichotomous variables. The application of this algorithm resulted in a final dataset of 268 individuals, which included 154 remitters and 114 nonremitters.\n\nThe reduction in patient similarity was crucial for ensuring that the training and test sets were independent. High similarity between patients in the training and test sets can lead algorithms to merely reproduce their input rather than effectively interpolate and extrapolate. By removing phenotypically similar patients, the risk of overfitting was mitigated, and the algorithm's ability to generalize was enhanced.\n\nThe distribution of the final dataset, with 268 individuals, shows a more balanced class distribution compared to the initial dataset. This balance is important for training robust machine learning models, as it helps in avoiding biases that can arise from imbalanced datasets. The approach used for reducing redundancy and ensuring independence between training and test sets is rigorous and aligns with best practices in machine learning, particularly in the context of biological and clinical data.",
  "dataset/availability": "The data used in this study were obtained from dbGaP, specifically through dbGaP accession number phs000380.v1.p1. The data sets are available from the Database of Genotypes and Phenotypes (dbGaP) at http://www.ncbi.nlm.nih.gov/gap under Project ID 4676. This platform provides access to genomic data and requires users to apply for access, ensuring that the data is used responsibly and in accordance with ethical guidelines.\n\nThe data splits used for feature selection and model validation were part of a nested cross-validation process. This process involved five outer folds and five inner folds, with the inner split repeated five times, resulting in a total of 125 sets of features selected. The specifics of these splits are detailed in the supplementary figures and tables, particularly in Supplementary Figure S4, which outlines the approach for forward feature selection and subsequent internal validation.\n\nThe data availability and usage are governed by the terms and conditions set by dbGaP, which include requirements for data access agreements and compliance with ethical standards. This ensures that the data is used appropriately and that the privacy and rights of the individuals whose data is included are protected.\n\nThe data, including the splits used, are not publicly released in a forum that allows unrestricted access. Instead, they are made available through a controlled access process that ensures responsible use. This approach helps to maintain the integrity of the data and the trust of the participants whose information is included in the study.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is artificial neural networks, specifically a standard feed-forward-back-propagation network with one hidden layer containing three units. This approach is well-established and has been widely used in various contexts, including biological data analysis.\n\nThe algorithm employed is not new; it is a conventional artificial neural network implementation. The choice to use this established method was driven by its proven effectiveness and reliability in handling complex datasets, particularly in biological research. The implementation was facilitated using the nnet and caret R-packages, which are standard tools in the field.\n\nGiven that the focus of our study is on biological data and the application of machine learning to improve patient stratification and feature selection, the publication venue was chosen to align with the biological and medical significance of the work. The algorithm itself is a well-known tool in the machine-learning community, and its application in our study is novel in the context of diabetes remission prediction. Therefore, the emphasis was on the biological insights and improvements in patient outcomes rather than the novelty of the machine-learning algorithm.",
  "optimization/meta": "The model employed in this study does not function as a meta-predictor. Instead, it relies on a single machine-learning algorithm, specifically an artificial neural network (ANN). This ANN is used to predict diabetes remission in bariatric surgery patients. The ANN is trained and validated using a nested cross-validation approach, which involves five outer folds and five inner folds, with the inner split repeated five times. This results in a total of 125 sets of features selected.\n\nThe ANN is trained using a combination of clinical traits and single nucleotide polymorphisms (SNPs). The clinical traits and SNPs are selected through a forward feature selection process within the nested cross-validation framework. The performance of the ANN is evaluated using the area under the curve (AUC) for test performance as the primary performance measure.\n\nThe training data for the ANN is standardized within the cross-validation, using the mean and standard deviation for the given train data split to standardize both the train and test datasets. This ensures that the data is appropriately scaled and that the model generalizes well to unseen data.\n\nThe ANN implementation ignores individuals with missing information, so only the subset of individuals with complete information for the included features is used. Regularization with a weight decay parameter of 1 is included to minimize the risk of overtraining the relatively small dataset. The training of the weights in the neural network is performed with a maximum of 1,000 iterations and otherwise default parameters using the training data.\n\nIn summary, the model is a standard artificial neural network approach that uses good practices and builds on experience in the use of artificial neural networks in biological contexts. The use of nested cross-validation ensures that the training data is independent and that the model's performance is robust and generalizable.",
  "optimization/encoding": "For the machine-learning algorithm, data encoding and preprocessing were crucial steps to ensure optimal performance. Dichotomous variables were encoded as 0.05 and 0.95 to avoid extreme values that could disproportionately influence the model. Continuous variables underwent a log-transformation to normalize their distribution, making them more suitable for the neural network. Single nucleotide polymorphism (SNP) data were additively encoded as one-column vectors with counts of minor alleles, represented as 0, 1, or 2. This encoding method captures the additive genetic effect, which is often relevant in genetic studies.\n\nStandardization was applied to continuous variables within the cross-validation framework. For each training data split, the mean and standard deviation were calculated and used to standardize both the training and test datasets. This approach ensures that the model generalizes well to unseen data by preventing the test data from influencing the standardization process.\n\nRegularization with a weight decay parameter of 1 was included to minimize the risk of overtraining, especially given the relatively small dataset. The neural network was trained with a maximum of 1,000 iterations, using default parameters otherwise. This careful preprocessing and encoding strategy aimed to enhance the model's ability to learn meaningful patterns from the data, ultimately improving its predictive performance.",
  "optimization/parameters": "In our study, the number of parameters used in the model was determined through a rigorous feature selection process. We employed a sequential forward feature selection approach within a nested cross-validation framework. This involved five outer folds and five inner folds, with the inner split repeated five times, resulting in a total of 125 sets of features selected. The feature selection was initially applied to clinical features, and subsequently, the top-ranked clinical features were used as a basis for evaluating individual SNP importance.\n\nThe final set of features included four clinical traits and eight SNPs. These features were selected based on their ability to improve the area under the curve (AUC) by at least 0.01. The selection criteria ensured that features were chosen at least once in each of the five outer cross-validation folds and at least a specified number of times over all 125 feature selections. This approach helped to reduce the risk of circularity in feature selection and ensured robust internal validation.\n\nThe model used a standard feed-forward-back-propagation neural network with one hidden layer containing three units. The network was trained using a maximum of 1,000 iterations and included regularization with a weight decay parameter of 1 to minimize the risk of overtraining. Dichotomous variables were encoded as 0.05 and 0.95, continuous variables were log-transformed and standardized, and SNP data were additively encoded as one-column vectors with counts of minor alleles. This methodology ensured that the model parameters were selected and optimized in a systematic and statistically sound manner.",
  "optimization/features": "In our study, we utilized a combination of clinical features and single nucleotide polymorphisms (SNPs) as input features for our predictive model. Initially, we had a large set of potential features, but we employed a rigorous feature selection process to identify the most relevant ones.\n\nFeature selection was indeed performed using a sequential forward feature selection approach within a nested cross-validation framework. This process involved five outer folds and five inner folds, with the inner split repeated five times, resulting in a total of 125 sets of features selected. This extensive feature selection scheme tested over half a million models to ensure robustness.\n\nThe feature selection was conducted in two main steps. First, we focused on clinical features, identifying the top-ranked ones. Then, using these selected clinical features as a basis, we evaluated the importance of individual SNPs. This two-step approach proved more effective than considering all features simultaneously.\n\nThe final set of features consisted of those that were selected at least once in each of the five outer cross-validation folds and at least a specified number of times (45 for clinical features and 5 for SNPs) across all 125 feature selections. This method helped to reduce the risk of circularity in feature selection and ensured internal validation.\n\nIn summary, the final model used a combination of four clinical traits and eight SNPs as input features. This selection process was designed to enhance the model's predictive performance and reliability.",
  "optimization/fitting": "The fitting method employed a standard feed-forward-back-propagation neural network with one hidden layer containing three units. This approach was chosen to balance model complexity and the risk of overfitting, given the relatively small dataset of 268 individuals with complete information.\n\nTo mitigate overfitting, several strategies were implemented. Regularization with a weight decay parameter of 1 was included to penalize large weights and prevent the model from becoming too complex. Additionally, the training process was limited to a maximum of 1,000 iterations, ensuring that the model did not overly fit the training data. A nested cross-validation setup with five outer folds and five inner folds, repeated five times, was used to select features and validate the model. This rigorous approach helped to ensure that the model's performance was generalizable and not merely a result of overfitting to the training data.\n\nUnderfitting was addressed by carefully selecting features through a sequential forward feature selection approach within the nested cross-validation framework. Features were added as long as the area under the curve (AUC) improved by at least 0.01, ensuring that the model captured relevant patterns in the data without becoming too simplistic. Furthermore, dichotomous variables were encoded as 0.05 and 0.95, continuous variables were log-transformed and standardized, and SNP data were additively encoded. These preprocessing steps helped to ensure that the model could effectively learn from the data.\n\nThe final set of features was determined by those selected at least once in each of the five outer cross-validation folds and at least a specified number of times over all 125 feature selections. This procedure aimed to reduce the risk of circularity in feature selection and internal validation, ensuring that the model was robust and not underfitted.",
  "optimization/regularization": "In our study, we implemented a regularization method to prevent overfitting, which is a common issue when training machine learning models, especially with small datasets. We employed a weight decay parameter set to 1. This technique helps to minimize the risk of the model becoming too complex and fitting the noise in the training data rather than the underlying pattern. By adding this penalty to the loss function, we encouraged the model to keep the weights small, leading to a simpler model that generalizes better to unseen data. This approach was crucial given the relatively small size of our dataset, ensuring that our model's performance was robust and not merely a result of memorizing the training data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we employed a standard feed-forward-back-propagation neural network with one hidden layer containing three units. The network was trained using the nnet and caret R-packages, with a weight decay parameter of 1 to minimize the risk of overtraining. Training was performed with a maximum of 1,000 iterations, and default parameters were used otherwise.\n\nThe feature selection process involved a sequential forward feature selection approach within a nested cross-validation setup. This setup included five outer folds and five inner folds, with the inner split repeated five times, resulting in a total of 125 sets of features selected. The feature selection scheme tested over half a million models. Subjects without diabetes remission were equally distributed across the different cross-validation splits.\n\nFor the neural network, dichotomous variables were encoded as 0.05 and 0.95, continuous variables were log-transformed, and SNP data were additively encoded as one-column vectors with counts of minor alleles. Continuous variables were standardized within the cross-validation using the mean and standard deviation for the given train data split.\n\nThe relative importance of input variables was determined using a specific code described in a referenced blog post. Performance improvements were calculated using the PredictABEL R-package.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly provided in the publication. However, the methods and configurations described ensure reproducibility of the results within the constraints of the study design. For further details on the implementation and access to specific code or data, interested parties may refer to the supplementary materials or contact the authors directly.",
  "model/interpretability": "The model employed in our study is an artificial neural network (ANN), which is often considered a black-box model due to its complex, non-linear structure. This means that the internal workings of the model are not easily interpretable, and it can be challenging to understand how specific inputs influence the output.\n\nHowever, we implemented several strategies to enhance the interpretability of our model. Firstly, we used a sequential forward feature selection approach within a nested cross-validation framework. This method allowed us to identify the most important features, including clinical traits and single nucleotide polymorphisms (SNPs), that contributed to the model's predictive performance. By ranking these features, we gained insights into which variables were most influential in predicting diabetes remission.\n\nAdditionally, we calculated the relative importance of input variables using a specific code, which helped us understand the contribution of each feature to the model's predictions. This approach provided a more transparent view of the model's decision-making process.\n\nFurthermore, we visualized the distributions of variables for different patient subgroups using violin plots. These plots illustrated the frequency distributions of the top-ranked features, with interquartile ranges and median values clearly indicated. This visualization helped to interpret how the model differentiated between patients who achieved remission and those who did not.\n\nIn summary, while the ANN itself is a black-box model, our use of feature selection, relative importance calculations, and visualizations of variable distributions enhanced the interpretability of our model. These strategies allowed us to gain insights into the key factors driving the model's predictions and to better understand the underlying patterns in the data.",
  "model/output": "The model employed in this study is a classification model. It is designed to predict diabetes remission in patients undergoing bariatric surgery. The model uses an artificial neural network (ANN) with a single hidden layer containing three units. The output of this ANN is a score used to classify patients into two categories: remitters and nonremitters. A score of 0.5 is the threshold used to differentiate between these two classes. The model's performance is evaluated using metrics such as sensitivity, specificity, and the area under the receiver operating characteristic curve (AUC). Additionally, performance improvements are reported using categorical and continuous net reclassification improvement (NRI) and integrated discrimination improvement (IDI). The model incorporates both clinical traits and single nucleotide polymorphisms (SNPs) to enhance predictive accuracy.",
  "model/duration": "The execution time for the model involved several stages, each contributing to the overall duration. The process began with the generation of a non-redundant dataset, which reduced the number of patients from 457 to 268. This was followed by the creation of an enriched subset of candidate SNPs, narrowing down from 86,444 to 960 using prior biological knowledge.\n\nThe forward feature selection of clinical traits using an artificial neural network (ANN) resulted in the identification of four key clinical traits. Subsequently, forward feature selection of SNPs on top of these clinical traits led to the selection of eight SNPs. This feature selection was conducted using a nested cross-validation approach, which involved 125 sets of features selected across 1,000 different cross-validation splits.\n\nThe training of the neural network was performed with a maximum of 1,000 iterations for each split, ensuring thorough optimization of the model weights. Regularization with a weight decay parameter of 1 was included to minimize the risk of overtraining, given the relatively small dataset.\n\nThe overall execution time was influenced by the complexity of the feature selection process and the number of iterations required for training the neural network. The use of nested cross-validation and the repetition of feature selection across multiple splits ensured robust model performance but also contributed to the computational time required.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed a nested cross-validation approach to ensure robust and unbiased feature selection and model performance assessment. This involved five outer cross-validation folds, where the original data was partitioned into training and validation sets. For each outer fold, the training data was further divided into five inner folds, and this process was repeated five times, resulting in a total of 125 feature selection iterations.\n\nWithin each inner fold, the training data was standardized, and a forward feature selection method was applied using a neural network. Features were added as long as the Area Under the Curve (AUC) on the test data increased by at least 0.01. This procedure was repeated for both clinical features and single nucleotide polymorphisms (SNPs), with the final set of features determined by those selected at least once in each outer fold and a specified number of times across all iterations.\n\nAfter feature selection, the model was trained using the selected features and the combined training and test data from the outer folds. The validation data, standardized using the mean and standard deviation of the training data, was used to calculate the prediction performance. The internal performance of the model was evaluated using the AUC, and performance improvements were reported using categorical and continuous net reclassification improvement (NRI) and integrated discrimination improvement (IDI).\n\nAdditionally, benchmarking was performed by comparing the model's performance with permutations of labels and random SNPs, ensuring that the selected features contributed significantly to the model's predictive power. The evaluation process also included regularization techniques to minimize the risk of overtraining, given the relatively small dataset.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the improvement of our models. Specifically, we reported categorical and continuous net reclassification improvement (NRI) and integrated discrimination improvement (IDI). These metrics are widely used in the literature and provide a comprehensive assessment of model performance.\n\nCategorical NRI measures the correct reclassification of individuals into different risk categories, while continuous NRI assesses the improvement in the predicted probabilities. IDI, on the other hand, evaluates the difference in the average predicted probabilities between events and non-events, providing a measure of the model's discriminative ability.\n\nWe calculated these performance improvements using the PredictABEL R-package, ensuring robustness and reliability in our evaluations. The reported values indicate the enhancement in model performance when adding specific single nucleotide polymorphisms (SNPs) to a simpler model that only includes clinical traits. This approach allows us to quantify the additional value that genetic information brings to predictive modeling in our context.\n\nAdditionally, we benchmarked our models using the area under the curve (AUC) from 1,000 repetitions of either permutation of labels or sampling of eight random SNPs. This benchmarking process helps to validate the significance of the selected SNPs and the overall model performance. The mean AUC values, along with their standard deviations, provide insights into the stability and generalizability of our models.\n\nIn summary, the performance metrics we reported are representative of current standards in the field, offering a thorough evaluation of model improvements and benchmarking against random expectations.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our methods with simpler baselines to assess their performance. Specifically, we benchmarked our models using mean AUC from 1,000 repetitions of either permutation of labels or sampling of eight random SNPs. This approach allowed us to evaluate the robustness and generalizability of our models.\n\nFor the simpler baselines, we compared models that included only clinical traits against those that included both clinical traits and the selected SNPs. The results showed that the models incorporating both clinical traits and SNPs outperformed the models based solely on clinical traits, demonstrating the added value of genetic information in predicting outcomes.\n\nAdditionally, we performed permutation tests to ensure that the observed improvements were not due to random chance. These tests involved permuting the labels and evaluating the model performance, providing a rigorous benchmark for our methods.\n\nOverall, our comparisons with simpler baselines and publicly available methods highlighted the superior performance of our approach, which integrates both clinical and genetic data. This comprehensive evaluation underscores the effectiveness of our methods in improving predictive accuracy and reliability.",
  "evaluation/confidence": "The evaluation of our method includes several performance metrics that are accompanied by confidence intervals, providing a clear understanding of the variability and reliability of our results. For instance, the performance improvement when adding eight SNPs to a simpler model containing four clinical traits is reported with mean values and standard deviations for both categorical and continuous net reclassification improvement (NRI) and integrated discrimination improvement (IDI). This allows for a comprehensive assessment of the model's performance.\n\nStatistical significance is a crucial aspect of our evaluation. The p-values associated with the performance improvements indicate that the observed improvements are highly statistically significant. For example, the p-value for the categorical NRI is 1.45\u00d710\u207b\u2074, and for the continuous NRI, it is 2.81\u00d710\u207b\u00b2\u2079, both of which are well below conventional thresholds for statistical significance. This strong statistical evidence supports the claim that our method, which includes the eight selected SNPs, is superior to the baseline model that relies solely on clinical traits.\n\nAdditionally, we employed a rigorous cross-validation strategy to ensure the robustness of our findings. The internal validation performance improvements were assessed using 1,000 different cross-validation splits, providing a thorough evaluation of the model's generalizability. The mean AUC values from these repetitions further reinforce the reliability of our results, demonstrating consistent performance across multiple validation sets.\n\nIn summary, the performance metrics in our evaluation are robust, with confidence intervals and statistically significant results that strongly support the superiority of our method over baseline models. The use of extensive cross-validation ensures that our findings are reliable and generalizable.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The data used for the analyses described in this manuscript were obtained from dbGaP at http://www.ncbi.nlm.nih.gov/gap (Project ID 4676) through dbGaP accession number phs000380.v1.p1. Access to these data is subject to the dbGaP data access policies and requires approval from the dbGaP Data Access Committee. The data sets include information on glycaemic traits contributed by MAGIC investigators, which can be downloaded from www.magicinvestigators.org, subject to their terms of use. The specific data sets used in this study are not directly released by the authors due to the sensitive nature of the information and the need to comply with ethical and legal standards for protecting participant privacy. Researchers interested in accessing the data should follow the appropriate procedures outlined by dbGaP and the MAGIC investigators."
}