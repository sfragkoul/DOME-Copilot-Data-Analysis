{
  "publication/title": "Deep Learning for Acute Myeloid Leukemia Diagnosis.",
  "publication/authors": "Nazari E, Farzin AH, Aghemiri M, Avan A, Tara M, Tabesh H",
  "publication/journal": "Journal of medicine and life",
  "publication/year": "2020",
  "publication/pmid": "33072212",
  "publication/pmcid": "PMC7550141",
  "publication/doi": "10.25122/jml-2019-0090",
  "publication/tags": "- AML\n- Machine Learning\n- Deep Learning\n- Neural Network\n- Microarray\n- Cancer Diagnosis\n- Gene Expression\n- Data Analysis\n- Bioinformatics\n- Medical Informatics",
  "dataset/provenance": "The dataset utilized in this study was sourced from the Gene Expression Omnibus (GEO) database. This public and international database provides free genomic data obtained from various studies, including microarray studies and Next Generation Sequencing (NGS). The specific dataset used was GSE9476, which contains data from 36 cases. These cases include gene expression data from 22,283 genes derived from AML microarray experiments. The dataset comprises 10 healthy cases and 26 AML cases. It was initially registered in 2007 and was updated in 2017. This dataset has been previously used in the community for similar research purposes, making it a reliable source for our study.",
  "dataset/splits": "In our study, we utilized data from the Gene Expression Omnibus (GEO) database, specifically focusing on acute myeloid leukemia (AML)-related data. The dataset we retrieved was GSE9476, which included a total of 36 cases. These cases were divided into two main categories: 10 healthy individuals and 26 AML patients. This dataset was initially registered in 2007 and was updated in 2017.\n\nFor the purpose of our analysis, we split the data into training and testing sets. Specifically, 70% of the data was allocated for training the model, while the remaining 30% was used for testing. This split ensured that the model was trained on a substantial amount of data while also having a sufficient portion for evaluating its performance.\n\nThe dataset itself contained gene expression data for 22,283 genes, providing a comprehensive view of the genetic activity in both healthy and AML cases. This high-dimensional data was essential for our deep learning approach, which is capable of handling and extracting meaningful patterns from such complex datasets.",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "The data used in this study is publicly available through the Gene Expression Omnibus (GEO) database. Specifically, the dataset GSE9476 was utilized, which includes gene expression data from microarray experiments. This dataset contains information from 36 cases, comprising 22,283 gene expressions, with 10 healthy and 26 acute myeloid leukemia (AML) cases. The dataset was initially registered in 2007 and was updated in 2017.\n\nThe GEO database is an international platform that publishes free genomic data obtained from various sources, including microarray studies and Next Generation Sequencing (NGS). It handles high-throughput data submissions, ensuring that the data is accessible to researchers worldwide. The data is available under the terms and conditions set by the GEO database, which typically include proper citation and acknowledgment of the original source.\n\nTo ensure the integrity and reproducibility of the study, the data was normalized using Python 3.7, and its comparability and normality were confirmed. This process is crucial for maintaining the quality and reliability of the data used in the analysis. The normalization step is an essential part of microarray data analysis, as it balances the hybridization intensity of each point in the data matrix, making the samples comparable.\n\nThe dataset's availability and the normalization process were enforced by following standard protocols and using established tools and libraries in Python, such as TensorFlow, Pandas, Numpy, and Sklearn. These tools are widely recognized in the scientific community for their robustness and reliability in handling and analyzing large datasets. The use of these tools ensures that the data processing steps are transparent and reproducible, allowing other researchers to verify and build upon the findings presented in this study.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is deep learning, specifically deep neural networks (DNNs). Deep learning is a subset of machine learning that utilizes neural networks with many layers to model complex patterns in data. This approach is not entirely new but has gained significant attention in recent years due to its effectiveness in handling large and complex datasets.\n\nThe choice to use deep learning in this context is driven by its ability to automatically extract features from high-dimensional data, such as gene expression profiles, without the need for manual feature engineering. This capability is particularly valuable in bioinformatics, where datasets often have thousands of features and require sophisticated methods to identify meaningful patterns.\n\nThe decision to publish this work in a medical journal rather than a machine-learning journal is likely due to the focus of the study. The primary objective is to apply deep learning techniques to the classification of acute myeloid leukemia (AML) using gene expression data. The results and implications of this work are most relevant to the medical and bioinformatics communities, which are the target audiences for this journal. Additionally, the study contributes to the broader goal of improving diagnostic tools and understanding the genetic basis of diseases, which aligns well with the scope of medical research publications.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for ensuring the effectiveness of our machine-learning algorithms. We utilized the Gene Expression Omnibus (GEO) database to obtain AML-related data, which included both healthy and cancerous cases. The dataset, specifically GSE9476, contained 36 cases with 22,283 gene expressions from AML microarray experiments, comprising 10 healthy and 26 AML cases.\n\nThe initial step in data preprocessing involved normalization, which is essential for balancing the hybridization intensity of each point in the data matrix. This process ensures the comparability of samples. We confirmed data normality using Python 3.7. Following normalization, we employed Principal Component Analysis (PCA) for dependency testing and data quality assessment. PCA is a powerful technique for simplifying high-dimensional complexity, making it easier to identify patterns and trends in the data.\n\nFor the machine-learning models, we used TensorFlow, Pandas, Numpy, and Sklearn packages. TensorFlow facilitated large-scale numerical computing and machine learning techniques, making it easier and faster to implement deep learning models. Pandas provided fast, flexible, and illustrative data structures, enabling efficient data entry, manipulation, and analysis. Numpy supported scientific calculations, matrices, and multidimensional arrays, while Sklearn offered powerful data analysis and data mining tools.\n\nThe data was split into training and testing sets, with 70% of the data used for training. We utilized three hidden layers in our Deep Neural Networks (DNNs) to analyze the data. To mitigate overfitting, we implemented dropout techniques. The Adamax function was used as the optimizer, and sparse categorical cross-entropy was employed to calculate the loss. The accuracy of the network was considered the improvement criterion at each epoch, with the number of epochs set to 1000.\n\nIn summary, our data encoding and preprocessing involved normalization, PCA for quality assessment, and the use of robust machine-learning packages. These steps ensured that our models could effectively analyze the high-dimensional gene expression data, leading to reliable and accurate results in distinguishing between healthy and cancerous cells.",
  "optimization/parameters": "In our study, we employed deep neural networks (DNNs) with three hidden layers to analyze the data. The architecture of these networks inherently involves a large number of parameters due to the multiple layers and the connections between them. The exact number of parameters can vary based on the number of neurons in each layer and the input dimensions.\n\nThe selection of the number of parameters was guided by the need to capture the complex patterns in the high-dimensional microarray data. We aimed to balance model complexity with the risk of overfitting. To mitigate overfitting, we utilized dropout techniques, which randomly set a fraction of input units to zero at each update during training time. This helps in preventing the network from becoming too reliant on specific neurons and improves generalization to new data.\n\nThe learning process was optimized using the Adamax function, which is an extension of the Adam optimization algorithm. This choice was made to efficiently update the network weights based on the training data, ensuring that the model could learn effectively from the input parameters.\n\nIn summary, while the exact number of parameters is not explicitly stated, the model's architecture and the use of dropout and Adamax optimization reflect a careful consideration of the input parameters to achieve robust and accurate results.",
  "optimization/features": "In our study, we utilized a dataset containing 22,283 gene expression features from AML microarray experiments. This high-dimensional data was retrieved from the GSE9476 dataset, which includes samples from both healthy individuals and AML patients.\n\nFeature selection was not explicitly mentioned as a separate step in our methodology. However, we employed Principal Component Analysis (PCA) for dependency testing and data quality assessment. PCA is a dimensionality reduction technique that can help identify the most important features by capturing the variations in the data. This process indirectly aids in focusing on the most relevant gene expressions for distinguishing between cancerous and healthy samples.\n\nThe use of PCA ensures that the features used in our model are those that contribute most significantly to the separation of the data, thereby enhancing the model's performance and reliability. By leveraging PCA, we aimed to mitigate the challenges posed by the high dimensionality of the microarray data, making the subsequent deep learning analysis more efficient and accurate.",
  "optimization/fitting": "In our study, we employed deep neural networks (DNNs) with three hidden layers to analyze the data, which indeed resulted in a model with a large number of parameters compared to the number of training points. To address the potential issue of overfitting, we implemented dropout regularization. Dropout is a technique where during training, a random subset of neurons is temporarily removed from the network, which helps to prevent the model from becoming too reliant on any single neuron and thus reduces overfitting.\n\nAdditionally, we used a relatively large dataset containing 36 cases with 22283 gene expressions, which provided a substantial amount of data for training. We also ensured that 70% of the data was used for training, while the remaining 30% was reserved for validation and testing. This split helped in evaluating the model's performance on unseen data and ensured that the model generalized well.\n\nTo further mitigate overfitting, we utilized the Adamax optimization algorithm, which is an extension of Adam and is known for its adaptive learning rate properties. This optimizer helps in finding the optimal set of parameters efficiently and reduces the risk of overfitting.\n\nRegarding underfitting, we monitored the model's performance using accuracy as the improvement criterion at each epoch. The model was trained for 1000 epochs, which allowed it to learn the underlying patterns in the data effectively. The use of ReLU activation functions in the hidden layers also helped in capturing complex relationships in the data, thereby reducing the risk of underfitting.\n\nThe results showed that our deep learning-based network achieved a high accuracy of 0.9667, compared to a simple neural network with a middle layer, which had an accuracy of 0.6333. This significant improvement in accuracy indicates that our model was able to learn the intricate patterns in the data without underfitting.\n\nIn summary, by using dropout regularization, a substantial dataset, an appropriate training-validation split, the Adamax optimizer, and monitoring accuracy over multiple epochs, we effectively managed to avoid both overfitting and underfitting in our model.",
  "optimization/regularization": "In our study, we implemented a regularization method to prevent overfitting. Specifically, we utilized dropout in our deep neural network model. Dropout is a technique where, during training, a random selection of neurons is ignored or \"dropped out.\" This process helps to prevent the network from becoming too reliant on any single neuron, thereby reducing overfitting. By applying dropout, we were able to enhance the generalization capability of our model, ensuring that it performed well on unseen data. This approach was crucial in achieving a high accuracy of 0.9667 in distinguishing between cancerous and healthy tissues.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in this study are available within the publication. The configurations include the use of three hidden layers in the deep neural networks, with dropout to reduce overfitting. The Adamax function was employed as the optimizer, and sparse categorical cross-entropy was used to calculate the loss. The accuracy was the criterion for network improvement at each epoch, with 70% of the data used for training and 1000 epochs considered.\n\nThe model was implemented using several open-source packages, including TensorFlow, Pandas, Numpy, and Sklearn. These packages are widely available and can be accessed under their respective open-source licenses. TensorFlow, for instance, is an open-source library for large-scale numerical computing and machine learning, making it easier and faster to apply machine learning and deep learning techniques. Pandas provides fast, flexible, and illustrative data structures in Python, enabling data entry, manipulation, and analysis. Numpy supports scientific calculations, matrices, and multidimensional arrays, while Sklearn offers powerful data analysis and data mining tools.\n\nThe specific model files and optimization parameters are not explicitly detailed in the publication, but the methods and tools used are well-documented and publicly accessible. Researchers interested in replicating the study can refer to the mentioned packages and follow the described configurations to achieve similar results. The use of open-source tools ensures that the methods are reproducible and can be built upon by other researchers in the field.",
  "model/interpretability": "The model employed in this study is primarily based on deep learning techniques, which are known for their high performance in handling complex and high-dimensional data. However, deep learning models are often considered black-box models due to their intricate architecture and the non-linear transformations they perform. This means that the internal workings of the model, particularly how it arrives at its predictions, can be difficult to interpret directly.\n\nThe use of neural networks, including deep neural networks (DNNs), involves multiple layers of neurons that process input data through a series of weighted connections. These weights are adjusted during the training process to minimize the error in predictions. While this allows the model to capture complex patterns in the data, it does not provide a straightforward way to understand the decision-making process.\n\nOne aspect that contributes to the transparency of the model is the use of principal component analysis (PCA). PCA is a dimensionality reduction technique that helps in visualizing the data by reducing it to a lower-dimensional space while retaining most of the variance. In our study, PCA was used to separate cancerous and healthy data, which indicates that the experiment was performed correctly and that the results are reliable. This visualization step adds a layer of interpretability by showing how the data points are distributed and separated in the reduced dimensional space.\n\nAdditionally, the model's architecture, including the number of layers and neurons, as well as the activation functions used, can provide some insights into how the model processes information. For instance, the use of ReLU (Rectified Linear Unit) activation functions helps in making the network lighter and more efficient, which can indirectly contribute to better interpretability by reducing the complexity of the model.\n\nIn summary, while the deep learning model used in this study is largely a black-box model, techniques like PCA and the careful design of the neural network architecture provide some level of interpretability. These methods help in understanding the data's structure and the model's performance, even if the exact decision-making process remains opaque.",
  "model/output": "The model employed in this study is designed for classification purposes. Specifically, it is used to separate healthy and cancerous cells in leukemia-related genes. The neural networks and deep learning techniques utilized are trained to distinguish between these two categories, making it a classification model. The output of the model is a classification result indicating whether the input data corresponds to healthy or cancerous cells. This classification is crucial for diagnosing acute myeloid leukemia (AML), a type of cancer that starts in the bone marrow and can quickly spread to the blood. The model's architecture, including the number of neurons, layers, and types of communication between layers, is configured to optimize its performance in this classification task. Various activation functions, such as ReLU, are used to enhance the model's ability to learn and make accurate predictions. The loss functions employed measure the discrepancy between the predicted and actual values, guiding the training process to improve classification accuracy. Overall, the model's output provides a binary classification, enabling the identification of cancerous cells with high precision.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The method was evaluated using a combination of techniques to ensure its robustness and accuracy. Initially, the data underwent normalization and principal component analysis (PCA) to confirm data quality and the separation of cancerous and healthy cases. This step was crucial for verifying that the experiment was performed correctly and that the results were reliable.\n\nFor the modeling phase, the dataset was split into training and testing sets, with 70% of the data used for training. The deep neural network (DNN) was trained over 1000 epochs, using the Adamax optimizer and sparse categorical cross-entropy as the loss function. The accuracy of the network was monitored at each epoch to track improvements.\n\nThe performance of the DNN was compared against a simpler neural network with a single hidden layer. The simpler network achieved an accuracy of 63.33%, while the DNN with three hidden layers achieved a significantly higher accuracy of 96.67%. This comparison demonstrated the superior performance of deep learning techniques in handling high-dimensional data and complex patterns.\n\nAdditionally, the PCA results visually confirmed the separation of cancerous and healthy data, further validating the method's effectiveness. The use of dropout in the DNN helped reduce overfitting, ensuring that the model generalized well to unseen data. Overall, the evaluation process involved rigorous testing and comparison to establish the reliability and accuracy of the deep learning-based approach for diagnosing acute myeloid leukemia.",
  "evaluation/measure": "In our study, we primarily focused on accuracy as the key performance metric to evaluate the effectiveness of our models. Accuracy is defined as the ratio of correctly predicted instances to the total instances. For our deep learning model, which utilized three hidden layers and dropout to reduce overfitting, we achieved an accuracy of 0.9667. In comparison, a simpler neural network with a single hidden layer had an accuracy of 0.6333. This significant difference highlights the superior performance of deep learning techniques in handling complex, high-dimensional data such as that found in microarray experiments.\n\nWhile accuracy is a straightforward and commonly used metric, it is important to note that it may not always provide a complete picture, especially in imbalanced datasets. However, given the nature of our dataset and the clear separation observed in the Principal Component Analysis (PCA) results, accuracy serves as a reliable indicator of model performance. Future work could explore additional metrics such as precision, recall, and the F1 score to provide a more comprehensive evaluation, particularly if the dataset becomes more imbalanced or if the models are applied to different types of data.",
  "evaluation/comparison": "In our study, we conducted a comparison between different neural network architectures to evaluate their performance in analyzing acute myeloid leukemia (AML) data. We specifically compared a simple neural network with a single hidden layer to a deep learning-based network consisting of three hidden layers.\n\nThe simple neural network, which had a middle layer, achieved an accuracy of 0.6333. In contrast, the deep learning network, utilizing three hidden layers and incorporating techniques such as dropout to reduce overfitting, demonstrated a significantly higher accuracy of 0.9667. This comparison highlighted the superior performance of deep learning techniques in handling the complexity and high dimensionality of the AML dataset, which contained 22283 features.\n\nAdditionally, we employed principal component analysis (PCA) to assess the quality and separability of the data. The PCA results showed a clear distinction between cancerous and healthy data, indicating that the experiment was conducted correctly and that the methods used were reliable.\n\nOverall, the deep learning-based approach outperformed the simpler neural network, underscoring the effectiveness of deep learning in analyzing high-dimensional genomic data.",
  "evaluation/confidence": "Evaluation Confidence\n\nIn our study, we employed a deep learning-based network with three hidden layers to analyze AML data, achieving an accuracy of 0.9667. This was compared to a simple neural network with a middle layer, which had an accuracy of 0.6333. The significant difference in accuracy between the two models suggests that the deep learning approach is superior for this task.\n\nWe used the PCA method for dependency testing and data quality assessment, which showed a clear separation between cancerous and healthy data. This indicates that our experiment was performed correctly and that the results are reliable.\n\nHowever, specific confidence intervals for the performance metrics were not provided in this study. While the results are promising, further statistical analysis would be necessary to claim with certainty that the deep learning method is superior to other approaches and baselines. Future work could include more rigorous statistical testing to validate these findings and provide confidence intervals for the reported accuracies.",
  "evaluation/availability": "Not enough information is available."
}