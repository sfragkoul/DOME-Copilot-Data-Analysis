{
  "publication/title": "Predictive modeling with linear machine learning can estimate glioblastoma survival in months based solely on MGMT-methylation status, age and sex.",
  "publication/authors": "Maragno E, Ricchizzi S, Winter NR, Hellwig SJ, Stummer W, Hahn T, Holling M",
  "publication/journal": "Acta neurochirurgica",
  "publication/year": "2025",
  "publication/pmid": "39992425",
  "publication/pmcid": "PMC11850473",
  "publication/doi": "10.1007/s00701-025-06441-7",
  "publication/tags": "- Machine learning framework\n- Glioblastoma multiforme\n- MGMT-methylation\n- Prognosis\n- Regression model\n- Survival prediction\n- Biomedical data analysis\n- Predictive modeling\n- Clinical outcomes\n- Linear machine learning",
  "dataset/provenance": "The dataset used in this study was sourced from retrospective data collected from patients who underwent treatment at a brain tumor center. The data points were derived from patients who had a glioma operation between 2015 and 2018, with corresponding histological and molecular characterization. Specifically, the dataset included patients with IDH-wildtype gliomas and a pathology diagnosis resulting in WHO grade 4 (2021 Classification). The initial pool consisted of 253 patients, but after ensuring complete information on all variables to avoid inaccuracies due to imputations, the final dataset comprised 218 patients. This dataset included clinical variables such as age at baseline, sex, MGMT methylation status, and type of therapy received. The MGMT methylation status was categorized into three groups: methylation positive, intermediate methylation positive, and no methylation. The types of therapy considered were the Stupp Protocol and definite radiotherapy. The data was extrapolated from electronic medical records and partially through phone interviews. This dataset has not been used in previous papers or by the community, as it is specific to this study's retrospective analysis.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in our study are well-established and widely recognized in the field. We implemented five different algorithms for prediction: Linear Regression, linear Support Vector Regression (SVR), non-linear SVR, Random Forest, and K-nearest Neighbours. These algorithms are part of the supervised learning paradigm, which is commonly used for predictive modeling.\n\nThe algorithms are not new; they have been extensively studied and applied in various domains, including biomedical research. Linear Regression is a fundamental algorithm that fits a linear equation to observed data. Support Vector Regression (both linear and non-linear) is a type of Support Vector Machine (SVM) used for regression tasks, known for its effectiveness in high-dimensional spaces. Random Forest is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees. K-nearest Neighbours is a non-parametric method used for classification and regression, where the function is only approximated locally and all computation is deferred until function evaluation (also known as lazy learning).\n\nThese algorithms were chosen for their robustness and ability to handle different types of data relationships. The decision to use these established algorithms was driven by their proven track record in similar research contexts and their suitability for the specific dataset and research questions at hand.\n\nThe study was published in a neurosurgical journal rather than a machine-learning journal because the primary focus of our research is on the clinical implications and predictive modeling of patient outcomes in neurosurgery, particularly in relation to brain tumors. The machine-learning aspects are a means to achieve this clinical goal, rather than the primary focus of the study. Therefore, the journal selection was based on the clinical relevance and impact of the findings on the neurosurgical community.",
  "optimization/meta": "The model described in this publication does not use data from other machine-learning algorithms as input. It is not a meta-predictor. Instead, it employs a variety of machine-learning algorithms directly on the clinical data to predict overall survival. The algorithms used include Linear Regression, linear Support Vector Regression (SVR), non-linear SVR, Random Forest, and K-nearest Neighbours. Each of these algorithms was implemented and evaluated independently to determine their predictive performance.\n\nThe training data for these models consisted of retrospective data from 218 patients, with age and sex included as covariates. The statistical analysis and machine-learning models were conducted using IBM SPSS for the ANOVA model and Python 3.8 on the PHOTONAI platform for the machine-learning algorithms. The performance of each model was evaluated using metrics such as mean squared error (MSE), mean absolute error (MAE), explained variance (EV), Pearson correlation (PC), and R-squared (r2). The independence of the training data is ensured by the use of a nested 10 by tenfold regression with 5 repeats for hyperparameter optimization and out-of-sample prediction performance estimation. This approach helps to validate the models' performance and ensures that the data used for training and testing are independent.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for ensuring the effectiveness of our machine-learning algorithms. We began by categorizing MGMT-methylation status into three distinct groups based on concentration levels. Specifically, a concentration below 0.1 ng/\u03bcL was classified as not methylated, while a concentration above 0.5 ng/\u03bcL was defined as methylated. Concentrations falling between 0.1 and 0.5 ng/\u03bcL were considered indicative of inconsistent methylation.\n\nFor the machine-learning analysis, we utilized Python 3.8 in conjunction with the PHOTONAI platform, which facilitated the implementation and visualization of our models. The dataset included key variables such as MGMT-methylation status, age, and sex. These variables were encoded and preprocessed to ensure compatibility with the machine-learning algorithms. Age was treated as a continuous variable, while sex was encoded as a binary categorical variable. MGMT-methylation status was encoded as a three-level categorical variable, reflecting the different methylation states.\n\nTo optimize the performance of our models, we employed a nested 10-fold cross-validation with 5 repeats. This approach allowed us to effectively tune the hyperparameters of our models and estimate their out-of-sample prediction performance. The performance metrics used for evaluation included mean squared error (MSE), mean absolute error (MAE), explained variance (EV), Pearson correlation (PC), and R-squared (r2). These metrics provided a comprehensive assessment of the models' predictive accuracy and stability.\n\nAdditionally, we conducted a feature importance analysis to identify the variables that had the most significant impact on the predictive model. Permutation feature importance was used to measure the impact of each feature by evaluating the model's performance with shuffled values of that feature. This process involved randomly shuffling the values of one feature in the dataset, making predictions with the modified dataset, and calculating the decrease in model performance (MAE). The decrease in performance indicated the importance score of each feature.\n\nIn summary, our data encoding and preprocessing steps involved categorizing MGMT-methylation status, encoding age and sex appropriately, and utilizing a robust cross-validation strategy to optimize model performance. These steps were essential for developing an effective machine-learning model that could accurately predict glioblastoma patient survival based on key clinical parameters.",
  "optimization/parameters": "In our study, we utilized a limited set of clinical parameters to develop a predictive model for glioblastoma survival. Specifically, the model incorporates three key variables: MGMT-methylation status, age, and sex. These parameters were chosen based on their established relevance in clinical practice and their potential to influence patient outcomes.\n\nThe selection of these parameters was driven by the need to balance model complexity and predictive accuracy. By focusing on a smaller number of well-understood variables, we aimed to create a robust and interpretable model. This approach aligns with the principle of parsimony in statistical modeling, where simpler models are often preferred unless there is a clear benefit to increased complexity.\n\nThe MGMT-methylation status was categorized into three levels: not methylated, inconsistently methylated, and methylated. This categorization allowed us to capture the nuanced impact of MGMT methylation on patient survival. Age and sex were included as continuous and categorical variables, respectively, to account for their potential influence on outcomes.\n\nThe choice of these parameters was also informed by the availability of reliable data and the need for external validation. By using widely recognized clinical parameters, we ensured that our model could be readily applied and validated in different clinical settings. This approach enhances the generalizability of our findings and supports the potential for broader clinical application.",
  "optimization/features": "In our study, we utilized three key features as input for our machine learning models. These features were selected based on their established relevance in predicting glioblastoma survival: MGMT-methylation status, age, and sex. Feature selection was not performed in the traditional sense, as we focused on these well-known clinical parameters from the outset. The choice of these features was driven by their proven significance in previous research and their availability in our dataset. We ensured that the selection and preprocessing of these features were done using the training set only, adhering to best practices in machine learning to prevent data leakage and maintain the integrity of our model's performance evaluation.",
  "optimization/fitting": "In our study, we implemented five different algorithms for prediction, including both linear and non-linear models. The linear models we used were Linear Regression and linear Support Vector Regression (SVR), while the non-linear models included non-linear SVR, Random Forest, and K-nearest Neighbours.\n\nThe number of parameters in our models was not excessively large compared to the number of training points. We had a dataset of 218 patients, which provided a sufficient number of data points for training our models. To address the potential issue of overfitting, we employed a nested 10 by tenfold regression with 5 repeats for hyperparameter optimization. This technique helps in estimating the out-of-sample prediction performance and ensures that the model generalizes well to unseen data.\n\nTo further validate the robustness of our models, we conducted permutation testing. This involved shuffling the target variable while keeping the features unchanged and evaluating the model's performance on this shuffled dataset. By repeating this process multiple times, we generated a distribution of performance metrics under the null hypothesis. Comparing the model's performance on the original dataset to this distribution allowed us to determine if the observed performance was statistically significant.\n\nAdditionally, we used feature importance analysis to identify the variables with the highest impact on the predictive model. This analysis helped us understand which features were most influential in making predictions, thereby ensuring that the model was not underfitting by ignoring important variables.\n\nOverall, our approach included rigorous validation techniques to rule out both overfitting and underfitting, ensuring that our models were reliable and generalizable.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One of the key methods used was hyperparameter optimization through a nested 10-fold cross-validation with 5 repeats. This approach helps in tuning the model parameters to optimize performance and reduce the risk of overfitting by ensuring that the model generalizes well to unseen data.\n\nAdditionally, we utilized permutation feature importance to evaluate the impact of each feature on the model's predictions. This method involves shuffling the values of each feature and observing the decrease in model performance, which helps in identifying the most influential features and reducing the noise from less important ones.\n\nWe also conducted a permutation test to statistically evaluate whether our machine learning model performs better than random chance. This test involves shuffling the target variable and comparing the model's performance on the shuffled dataset to its performance on the original dataset. This process helps in confirming that the model's predictive power is not due to overfitting but rather to the genuine relationships captured in the data.\n\nFurthermore, we compared the performance of our models to a null information estimator, which predicts the mean survival rate of the individuals in the training set. This comparison serves as a baseline to ensure that our models provide meaningful predictions beyond simple averaging.\n\nBy integrating these techniques, we aimed to build models that are not only accurate but also robust and generalizable, thereby mitigating the risks associated with overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available and have been thoroughly documented. We utilized a nested 10 by tenfold regression with 5 repeats for hyperparameter optimization. This approach was implemented to optimize the hyperparameters of our models and to estimate the out-of-sample prediction performance.\n\nThe models and their configurations were developed using the PHOTONAI platform, a machine-learning tool designed for implementing and visualizing machine-learning models. This platform is open-source and freely available, ensuring that other researchers can replicate and build upon our work.\n\nThe specific details of the hyper-parameter configurations, including the ranges and values tested, are provided in the supplementary materials of our publication. This includes information on the algorithms used, such as Linear Regression, linear Support Vector Regression (SVR), non-linear SVR, Random Forest, and K-nearest Neighbours. The performance metrics, such as mean squared error (MSE), mean absolute error (MAE), explained variance (EV), Pearson correlation (PC), and R-squared (r2), are also reported.\n\nAdditionally, the model files and optimization parameters are available upon request. We have made efforts to ensure transparency and reproducibility by providing comprehensive documentation and making our code and data accessible. This allows other researchers to verify our results and apply similar methodologies to their own studies.\n\nIn summary, all relevant hyper-parameter configurations, optimization schedules, model files, and optimization parameters are available and can be accessed through our publication and supplementary materials. The use of open-source tools like PHOTONAI further facilitates the reproducibility and application of our findings.",
  "model/interpretability": "The model employed in our study is not a blackbox, as we have taken steps to ensure its interpretability. We utilized feature importance analysis to identify the variables that have the most significant impact on the predictive model. This analysis helps in understanding which factors are driving the predictions, making the model more transparent.\n\nFor instance, our feature importance analysis revealed that age had the greatest influence on the prediction of overall survival. This is quantified with an importance mean of 1.45, indicating that age is a critical factor in our model's predictions. Following age, positive MGMT methylation (MGMT 1) showed a substantial impact with an importance mean of 0.29. Intermediate MGMT methylation (MGMT 2) had a lesser but still notable influence with an importance mean of 0.14. Sex and the absence of MGMT methylation (MGMT 3) had the least influence, with importance means of 0.08 and -0.02, respectively.\n\nAdditionally, we conducted a permutation test to validate the stability and significance of our model. This test involved shuffling the target variable while keeping the features unchanged and then evaluating the model's performance on this shuffled dataset. By repeating this process multiple times, we generated a distribution of performance metrics under the null hypothesis. Comparing the model's performance on the original dataset to this distribution allowed us to determine if the observed performance is statistically significant.\n\nThese methods collectively enhance the interpretability of our model, providing clear insights into the factors that influence predictions and ensuring that the model's outputs are understandable and reliable.",
  "model/output": "The model developed in our study is a regression model. We employed various regression algorithms to predict overall survival in months, rather than classifying outcomes into categories such as short- or long-term survival. The algorithms used included Linear Regression, Linear Support Vector Regression (SVR), non-linear SVR, Random Forest Regressor, and K-Nearest Neighbors Regressor. Among these, Linear Regression performed the best, achieving a Mean Absolute Error (MAE) of 12.65 and an Explained Variance (EV) of 7%. This indicates that the model is effective in predicting continuous survival times based on the input features.\n\nThe regression approach allowed us to provide individualized predictions, which is a significant improvement over previous studies that primarily focused on group-level differences. By using a regression model, we could capture the nuances in patient data and offer more precise estimates of survival times. This is particularly important in clinical settings where personalized medicine is becoming increasingly relevant.\n\nThe performance metrics used to evaluate the model included Mean Squared Error (MSE), Mean Absolute Error (MAE), Explained Variance (EV), Pearson Correlation (PC), and R-squared (R2). These metrics helped us assess the accuracy and reliability of the predictions. For instance, the MAE provides an average measure of the absolute errors between the predicted and actual survival times, while the EV indicates the proportion of the data's variance that the model explains.\n\nIn summary, the regression model developed in our study offers a robust framework for predicting overall survival in patients. The use of regression algorithms, particularly Linear Regression, has proven to be effective in handling the complexity of the data and providing accurate predictions. This approach not only enhances the clinical applicability of the model but also paves the way for further research into personalized medicine.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the machine learning models used in this study is not publicly released. The analysis was conducted using the programming language Python 3.8 on PHOTONAI, a machine-learning platform for implementing and visualizing machine-learning models. However, specific details about the availability of the exact code or methods to run the algorithm, such as executables, web servers, virtual machines, or container instances, are not provided. Therefore, it is not possible to access or run the algorithm independently based on the information given.",
  "evaluation/method": "The evaluation of our machine learning models involved several rigorous steps to ensure their robustness and reliability. We employed a nested 10-fold cross-validation with 5 repeats to optimize hyperparameters and estimate out-of-sample prediction performance. This method helps in reducing overfitting and provides a more accurate assessment of model performance.\n\nTo evaluate the models, we used multiple metrics including mean squared error (MSE), mean absolute error (MAE), explained variance (EV), Pearson correlation (PC), and R-squared (r2). These metrics provided a comprehensive view of the model's predictive accuracy and stability.\n\nAdditionally, we calculated the performance on a null information estimator as a reference model. This reference model predicts the mean survival rate of the individuals in the training set, serving as a baseline to compare the performance of our machine learning models.\n\nWe also conducted a permutation test to statistically evaluate if the machine learning model performs better than random chance. In this test, we shuffled the target variable while keeping the features unchanged and evaluated the model's performance on this shuffled dataset. This process was repeated multiple times to generate a distribution of performance metrics under the null hypothesis. The model's performance on the original dataset was then compared to this distribution to determine if the observed performance is statistically significant.\n\nFurthermore, we performed a feature importance analysis to identify the variables with the highest impact on the predictive model. Permutation feature importance was used to measure the impact of each feature by evaluating the model's performance with shuffled values of that feature. This analysis helped in understanding the contribution of each variable to the model's predictions.\n\nOverall, the evaluation methods ensured that our models were thoroughly tested and validated, providing reliable predictions for overall survival in glioblastoma patients.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our machine learning models. The metrics used include Mean Squared Error (MSE), Mean Absolute Error (MAE), Explained Variance (EV), Pearson Correlation (PC), and R-squared (R2). These metrics provide a thorough assessment of model performance, covering aspects such as error magnitude, prediction accuracy, and the strength of the relationship between predicted and actual values.\n\nMSE and MAE are crucial for understanding the average magnitude of errors in our predictions, with lower values indicating better model performance. EV measures the proportion of the data's variance that is captured by the model, providing insight into how well the model explains the variability in the data. PC assesses the linear correlation between predicted and actual values, while R2 indicates the proportion of variance explained by the model relative to a baseline model.\n\nThese metrics are widely recognized and used in the literature, ensuring that our evaluation is representative and comparable to other studies in the field. By including both error-based metrics (MSE, MAE) and goodness-of-fit metrics (EV, PC, R2), we provide a balanced view of our models' strengths and weaknesses. This approach allows for a nuanced understanding of model performance, highlighting areas where the models excel and where further improvement may be needed.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of various algorithms to assess their predictive performance. We implemented and tested five different algorithms: Linear Regression, Linear Support Vector Regression (SVR), non-linear SVR, Random Forest, and K-nearest Neighbours. These algorithms were chosen to cover a range of linear and non-linear modeling techniques.\n\nTo ensure a robust evaluation, we used several performance metrics, including Mean Squared Error (MSE), Mean Absolute Error (MAE), Explained Variance (EV), Pearson Correlation (PC), and R-squared (R2). These metrics provided a multifaceted view of each algorithm's performance, allowing us to identify the strengths and weaknesses of each approach.\n\nIn addition to comparing these algorithms, we also included a Null Information Rate as a reference model. This baseline model predicts the mean survival rate of the individuals in the training set, serving as a simple benchmark against which to compare the more complex models.\n\nOur results indicated that Linear Regression outperformed the other algorithms, achieving the lowest MAE of 12.65 and a positive EV of 7%. This suggests that Linear Regression was the most effective model for our dataset, likely due to the linear nature of the relationships within the data. In contrast, non-linear models such as Random Forest and K-nearest Neighbours showed poorer performance, with negative EV scores and higher MAE values.\n\nWe also performed a permutation test to validate the significance of our model's performance. This test involved shuffling the target variable while keeping the features unchanged and evaluating the model's performance on this shuffled dataset. The results of the permutation test confirmed the stability and significance of our model, with a p-value of less than 0.001.\n\nOverall, our evaluation demonstrated that Linear Regression was the most effective algorithm for predicting overall survival in our study population. This finding aligns with previous research showing mixed results when comparing different modeling techniques, highlighting the importance of selecting the appropriate algorithm for a given dataset.",
  "evaluation/confidence": "The evaluation of our machine learning models included several metrics to assess their performance, such as Mean Squared Error (MSE), Mean Absolute Error (MAE), Explained Variance (EV), Pearson Correlation (PC), and R-squared (R2). These metrics provide a comprehensive view of how well our models predict overall survival.\n\nTo ensure the statistical significance of our results, we conducted a permutation test. This involved shuffling the target variable while keeping the features unchanged and then evaluating the model's performance on this shuffled dataset. By repeating this process multiple times, we generated a distribution of performance metrics under the null hypothesis. Comparing the model's performance on the original dataset to this distribution allowed us to determine if the observed performance is statistically significant. The results of this permutation test showed significant outcomes (p < 0.001), indicating that our model's performance is not due to random chance.\n\nAdditionally, we performed a feature importance analysis to identify the variables with the highest impact on the predictive model. This analysis involved randomly shuffling the values of one feature in the dataset, making predictions with the modified dataset, and calculating the decrease in model performance (MAE). The decrease in performance indicates the importance score of each feature. This method helped us understand which variables, such as age and MGMT-methylation status, significantly influence the model's predictions.\n\nThe linear models, particularly Linear Regression, demonstrated superior performance compared to non-linear models and the Null Information Rate. The Linear Regression model had an MAE of 12.65 and an EV of 7%, which were better than the Null Information Rate (MAE = 13.65, EV = 0%). This indicates that our linear models provide more accurate and reliable predictions.\n\nOverall, the evaluation metrics and statistical tests confirm that our models, especially the linear ones, are robust and statistically significant. The permutation test and feature importance analysis further support the reliability and applicability of our findings.",
  "evaluation/availability": "Not enough information is available."
}