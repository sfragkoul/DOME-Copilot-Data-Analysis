{
  "publication/title": "Tissue outcome prediction in hyperacute ischemic stroke: Comparison of machine learning models.",
  "publication/authors": "Benzakoun J, Charron S, Turc G, Hassen WB, Legrand L, Boulouis G, Naggara O, Baron JC, Thirion B, Oppenheim C",
  "publication/journal": "Journal of cerebral blood flow and metabolism : official journal of the International Society of Cerebral Blood Flow and Metabolism",
  "publication/year": "2021",
  "publication/pmid": "34159824",
  "publication/pmcid": "PMC8756479",
  "publication/doi": "10.1177/0271678x211024371",
  "publication/tags": "- MRI\n- Biomarkers\n- Neuroradiology\n- Penumbra\n- Stroke\n- Machine Learning\n- Tissue Outcome Prediction\n- Acute Ischemic Stroke\n- Diffusion-Weighted Imaging\n- Perfusion-Weighted Imaging",
  "dataset/provenance": "The dataset used in this study was sourced from a single-center, comprising patients who received reperfusion therapy for acute ischemic stroke (AIS) and underwent both initial and 24-hour MRI scans. Out of 788 AIS patients who met these criteria, 394 had complete datasets, including diffusion-weighted imaging (DWI) at baseline, perfusion-weighted imaging (PWI), and DWI at 24 hours. This subset of patients formed the basis for our analysis.\n\nThe data was collected as part of routine care and was anonymized and exported from the Picture Archiving and Communication System. The study was conducted in accordance with the French national information science and liberties commission and the General Data Protection Regulation, ensuring the ethical handling of patient data.\n\nThe dataset included various patient and stroke characteristics at admission and at 24 hours, which were summarized in a table. The MRI data underwent preprocessing steps, including registration to a reference brain scan in MNI space and intensity normalization. Perfusion maps were computed using the oSVD deconvolution method.\n\nThis dataset has not been used in previous publications by the community, as it is specific to this single-center study. The focus was on using MRI data to predict tissue outcomes in acute stroke, with an emphasis on the performance of different machine learning models. The dataset's homogeneity, due to the standardized stroke MRI protocol, enhances model performance but may limit generalizability to more heterogeneous multi-center datasets.",
  "dataset/splits": "In our study, we employed a 10-fold cross-validation scheme to ensure robust and unbiased model training and evaluation. This approach involved partitioning the dataset into 10 equally sized folds. For each fold, the model was trained on 81% of the patients, which constituted the training set. Additionally, 9% of the patients were used as a validation set to adjust hyperparameters, and the remaining 10% served as the test set to evaluate the model's performance. This process was repeated 10 times, with each fold serving as the test set once, ensuring that every patient's data was used for both training and testing. The cross-validation splits were identical across all models to maintain comparability.",
  "dataset/redundancy": "The datasets were split using a cross-validation scheme, specifically 10-fold cross-validation. This method involves partitioning the patients into 10 equal folds. For each fold, the model is trained on 81% of the patients (training set), hyperparameters are adjusted on 9% of the patients (validation set), and metrics are evaluated on the remaining 10% of the patients (test set). This process ensures that each patient's data is used for both training and testing, enhancing the statistical power of the models. The cross-validation splits were identical across different models to maintain comparability.\n\nTo ensure the independence of the training and test sets, the cross-validation process was designed to prevent any overlap between the sets in each fold. This independence is crucial for evaluating the model's performance accurately and avoiding overfitting.\n\nThe distribution of the dataset compares favorably to previously published ML datasets in the field. The use of a large dataset of acute ischemic stroke (AIS) patients who received reperfusion therapy and had both baseline and follow-up MRI scans ensures a comprehensive and representative sample. This approach increases the robustness and generalizability of the findings, making the results more reliable for clinical applications.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the class of supervised learning models. Three different models were tested: Random Forests, Gradient Boosting, and U-Net. These models are well-established in the field of machine learning and have been widely used for various prediction tasks, including medical imaging.\n\nRandom Forests and Gradient Boosting are both ensemble learning methods that combine multiple decision trees to improve predictive accuracy and control over-fitting. These models are known for their ability to handle complex, nonlinear relationships in the data.\n\nU-Net, on the other hand, is a type of deep learning model specifically designed for image segmentation tasks. It is a convolutional neural network with a U-shaped architecture that includes contracting and expanding paths, allowing it to capture both local and global contextual information in the images.\n\nNone of these algorithms are new; they have been extensively studied and applied in various domains, including medical imaging. The choice to use these established models was driven by their proven effectiveness in similar tasks and their ability to handle the specific challenges of predicting tissue outcomes in acute ischemic stroke.\n\nThe focus of this study is on comparing the performance of these machine-learning models in the context of tissue outcome prediction in acute ischemic stroke, rather than introducing a new algorithm. Therefore, it is appropriate for this work to be published in a journal focused on cerebral blood flow and metabolism, as it contributes to the understanding of how these models can be applied to improve clinical outcomes in stroke patients.",
  "optimization/meta": "The models used in this study do not constitute a meta-predictor. Instead, three distinct machine learning models were individually tested and compared for their performance in predicting final infarct volumes in acute ischemic stroke patients. These models include Random Forests, Gradient Boosting, and U-Net. Each model was trained and evaluated separately using cross-validation techniques to ensure robustness and comparability.\n\nThe training process involved using features computed from MRI voxels, including normalized signal intensities from patches on input sequences and voxel coordinates in MNI space. These features were used to predict the final infarct on MRI 24 hours post-stroke. The models were trained on a large dataset of acute ischemic stroke patients treated with reperfusion therapies, and their performance was assessed using metrics such as the Dice score, AUC, and absolute volume estimation error.\n\nThe study did not combine the outputs of different machine learning algorithms to create a meta-predictor. Instead, it focused on evaluating the individual performance of each model. The training data for each model was partitioned into 10 folds for cross-validation, ensuring that the training and test sets were independent for each fold. This approach helped in reducing bias and improving the generalizability of the results.",
  "optimization/encoding": "All MRI data were anonymized and exported from the Picture Archiving and Communication System. The DWI 24h sequences were registered onto DWI 0, and DWI0 and PWI were registered onto a reference brain scan in MNI space using 12-parameters affine registration. This process utilized global optimization with trilinear interpolation and correlation ratio as the similarity metric. The registration results were overlaid onto the MNI mask and manually checked by an experienced neuroradiologist. If necessary, the registration procedure was repeated using mutual information as the metric. For the registration of DWI 24h, a concatenation of the two computed registration matrices was used to prevent doubling the interpolation errors.\n\nA brain mask was computed using Otsu-based thresholding on DWI 0 to select in-brain voxels. Image intensity normalization was then applied for DWI 0 and DWI24h on each volume by mean centering and standard deviation scaling. This was done using the mean signal and standard deviation of in-brain voxels contralateral to the ischemic lesion.\n\nPerfusion maps were computed using Olea Sphere VR based on the oSVD deconvolution method with an oscillation index threshold of 0.02375. The arterial input function was automatically selected using an algorithm based on a clustering method that classifies curves using their area under the curve, roughness, and first moment to distinguish arterial from tissue signal.\n\nThe model inputs included DWI, ADC, and Tmax maps obtained from MRI 0. The primary focus was on Tmax as the sole surrogate for perfusion imaging because it correlates with clinical and tissue outcomes and is widely used in clinical trials. To test the added value of embedding other PWI maps, Extended-Perfusion models were also trained using Mean Transit Time (MTT), Cerebral Blood Flow (CBF), and Cerebral Blood Volume (CBV) as additional inputs. The ADC and PWI maps were not thresholded for ML model training. On MRI 24h, the final infarct was defined as the hyperintense lesion on DWI 24h and was manually segmented and considered the ground truth for the study. Given the high reproducibility of stroke volume measurements on DWI, segmentations were done by the same experienced neuroradiologist, who was blinded to all clinical information.",
  "optimization/parameters": "The models utilized in this study included three different supervised machine learning approaches: Random Forests, Gradient Boosting, and U-Net. Each of these models was trained to predict the final infarct volume based on specific input parameters derived from MRI data.\n\nFor the Tmax-only models, the input parameters consisted of Diffusion-Weighted Imaging (DWI), Apparent Diffusion Coefficient (ADC), and Time-to-Maximum (Tmax) maps obtained from the initial MRI scans. These parameters were chosen because Tmax is widely recognized for its correlation with clinical and tissue outcomes, making it a reliable surrogate for perfusion imaging in clinical trials.\n\nIn addition to the Tmax-only models, Extended-Perfusion models were also trained. These models incorporated additional Perfusion-Weighted Imaging (PWI) maps, including Mean Transit Time (MTT), Cerebral Blood Flow (CBF), and Cerebral Blood Volume (CBV). The inclusion of these additional parameters aimed to test the added value of embedding other PWI maps in the prediction models.\n\nThe selection of these input parameters was guided by their established relevance in stroke imaging and their potential to enhance the predictive accuracy of the models. The models were trained and evaluated using a 10-fold cross-validation scheme to ensure robustness and generalizability of the results. The performance of the models was assessed using metrics such as Dice scores, Area Under the Curve (AUC), and Mean Absolute Error, providing a comprehensive evaluation of their predictive capabilities.",
  "optimization/features": "The study utilized a total of 453 features as input for the patch-based models, specifically Gradient Boosting and Random Forests. These features were computed for each voxel and each input volume, including normalized signal intensity from 5x5x3 patches on each input sequence, normalized signal intensity from contralateral voxel patches, and voxel coordinates in MNI space. Feature selection was not explicitly mentioned, suggesting that all computed features were used in the models. The subsampling factor of 5% in-brain voxels in the training dataset was applied, ensuring that the feature set was derived and validated using the training set only.",
  "optimization/fitting": "In our study, we employed three different supervised machine learning models: Random Forests, Gradient Boosting, and U-Net. Each model was trained to predict final infarct volumes based on various inputs, including reperfusion status. To ensure robust and generalizable results, we used a 10-fold cross-validation scheme. This approach involved partitioning the patient data into 10 folds, where the model was trained on 81% of the patients, hyperparameters were tuned on 9% of the patients, and metrics were evaluated on the remaining 10% of the patients. This process was repeated for each fold, ensuring that every patient was included in the training, validation, and test sets.\n\nThe U-Net model, a deep learning architecture, operated slice-by-slice with additional input from adjacent slices, providing a voxel-wise probability map of infarction. The training process involved learning model weights by comparing the output to segmented brain infarct maps on DWI 24h. To prevent overfitting, we implemented early stopping based on the validation dataset's performance. This technique halted the training process when the model's performance on the validation set ceased to improve, thereby avoiding the memorization of training data.\n\nTo assess overfitting, we compared the Dice scores between the training and test sets. The results indicated no major overfitting for any of the studied models, as the differences between the Dice scores in these sets were minimal. Additionally, we evaluated model performance using a Bland-Altman graph and analyzed predictions using predefined final infarct volume cut-points (50 mL and 100 mL). This analysis helped identify any systematic biases in the model predictions.\n\nFor underfitting, we ensured that our models were sufficiently complex to capture the underlying patterns in the data. The use of cross-validation and the comparison of different models (Random Forests, Gradient Boosting, and U-Net) allowed us to select the most appropriate model for the task. Furthermore, the inclusion of reperfusion status as an input significantly improved the performance of all models in the subgroup of patients treated with mechanical thrombectomy, indicating that the models were capable of learning from the data without being overly simplistic.\n\nIn summary, our approach to fitting the models involved rigorous cross-validation, early stopping to prevent overfitting, and the use of multiple models to ensure that the chosen model was neither overfitting nor underfitting the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the primary methods used was cross-validation. Specifically, we utilized a 10-fold cross-validation scheme. This involved partitioning the patient data into 10 folds, where the model was trained on 81% of the patients, hyperparameters were adjusted on 9% of the patients, and metrics were evaluated on the remaining 10% of the patients. This process was repeated for each fold, ensuring that every patient was included in the training, validation, and test sets across different iterations. This approach helped to reduce bias and provided a more reliable estimate of model performance.\n\nAdditionally, we compared the Dice scores obtained in the training and test sets to assess overfitting. The differences between these scores were minimal, indicating that our models did not exhibit major overfitting issues. This was further supported by the fact that the performance metrics were consistent across different folds, suggesting that the models generalized well to unseen data.\n\nFor the U-Net model, we implemented early stopping during the training process. This technique involved monitoring the loss on the validation dataset and stopping the training when the loss ceased to improve. This prevented the model from overfitting to the training data by avoiding unnecessary epochs that could lead to memorization of the training examples rather than learning generalizable patterns.\n\nFurthermore, we ensured that the cross-validation splits were identical between models, which allowed for a fair comparison of their performances. This consistency in the evaluation process helped to isolate the effects of different model architectures and hyperparameters on the final performance metrics.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules for the models used in this study are detailed in the online-only Data Supplement. The architecture of the U-Net model is also provided there and will be published as open-source software on GitHub. This open-source release will include the model files and optimization parameters, allowing for reproducibility and further development by the research community. The specific license under which these resources will be made available has not been specified, but it is typical for such academic contributions to be released under permissive open-source licenses, such as MIT or Apache 2.0, to encourage wide use and collaboration.",
  "model/interpretability": "The models used in this study, including Gradient Boosting, Random Forests, and U-Net, are generally considered to be black-box models to varying degrees. This means that while they can provide accurate predictions, the internal workings and decision-making processes of these models are not easily interpretable.\n\nGradient Boosting and Random Forests are ensemble learning methods that combine multiple decision trees. While decision trees themselves are relatively interpretable, the ensemble nature of these models makes it challenging to understand the exact contributions of individual features to the final prediction. However, feature importance scores can be extracted from these models, providing some insight into which variables are most influential in the predictions.\n\nU-Net, on the other hand, is a type of convolutional neural network (CNN) specifically designed for image segmentation tasks. CNNs are known for their high performance in image-related tasks but are often considered black-box models due to their complex architecture and the large number of parameters involved. The layers in a U-Net encode and decode spatial hierarchies in the data, making it difficult to trace back how a specific prediction was made.\n\nIn summary, while these models offer powerful predictive capabilities, their interpretability is limited. For Gradient Boosting and Random Forests, feature importance can offer some transparency, but the overall decision-making process remains opaque. U-Net, being a deep learning model, is even more of a black-box, with its predictions arising from complex interactions within the network.",
  "model/output": "The model is a classification model. It is designed to predict the final infarct volume in patients with acute ischemic stroke using MRI data. The output of the model is a binary prediction map, indicating whether each voxel in the MRI scan is part of the final infarct or not. This is achieved by applying an arbitrary fixed threshold of 0.5 to the machine learning probability maps. The performance of the model is evaluated using metrics such as the Dice score, which measures the overlap between the predicted and ground-truth volumes. The model aims to provide spatially defined and contrasted prediction maps, which are crucial for clinical decision-making.",
  "model/duration": "The execution time for the models varied significantly. Training time was notably longer for U-Net, with a median duration of 160 minutes per cross-validation fold. In contrast, Gradient Boosting required 89 minutes, and Random Forests had the shortest training time at 19 minutes. When it came to inference times for each patient, U-Net was the fastest, with a median duration of 0.3 seconds. Gradient Boosting took 21 seconds per patient, while Random Forests required 2 seconds. These differences highlight the trade-offs between training and inference times across the models.",
  "model/availability": "The source code for the U-Net model architecture is planned to be published as open-source software. It will be available on GitHub at the repository https://github.com/NeuroSainteAnne/StrokePrediction. The specific details about the model's implementation and how to use it will be provided in the repository. The availability of the source code will facilitate reproducibility and further development by the research community.",
  "evaluation/method": "The evaluation of the models involved a comprehensive approach to ensure robustness and reliability. A 10-fold cross-validation scheme was employed, which involved equally partitioning the patients into 10 folds. For each fold, the model was trained on 81% of the patients, hyperparameters were adjusted on 9% of the patients, and metrics were evaluated on the remaining 10% of the patients. This method was chosen to balance computation time and bias reduction.\n\nTo assess the performance of the models, several metrics were used, including Dice scores, Area Under the Curve (AUC), AUC0, and Mean Absolute Error. These metrics were compared across different models to determine their effectiveness in predicting final infarct volumes.\n\nStatistical analysis was conducted using a pairwise Wilcoxon test with p-values adjusted for multiple comparisons using the Holm method. This approach helped to compare patient-level metrics across models and account for potential biases, such as the impact of large infarct volumes on prediction performance.\n\nAdditionally, Bland-Altman graphs were used to plot predicted and true infarct volumes, and predictions were analyzed using predefined final infarct volume cut-points (50 mL and 100 mL). This provided a visual and quantitative assessment of the models' performance.\n\nModel overfitting was evaluated by comparing Dice scores in the test and training sets. The results indicated that there was no major overfitting for any of the studied models.\n\nIn the subgroup of patients who underwent mechanical thrombectomy (MT), the performance of models with and without considering reperfusion status was compared using a pairwise Wilcoxon test. This analysis showed that adding reperfusion status as an input significantly improved the performance of all models.\n\nFurthermore, an additional analysis using a single train/validation/test split with a larger test set (25%) was conducted to check the robustness of the findings. While the results were similar in terms of Dice scores, the statistical power was insufficient to conclude regarding differences between machine learning models.\n\nIn summary, the evaluation method involved a rigorous cross-validation scheme, comprehensive statistical analysis, and visual assessments to ensure the reliability and robustness of the models' performance in predicting final infarct volumes.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the models' ability to predict final infarct volumes in acute ischemic stroke patients. The primary metric used was the Dice score, which measures the overlap between predicted and actual infarct volumes. This metric is particularly relevant for assessing the performance of models in predicting clinically relevant regions, although it can be unstable for small infarcts.\n\nAdditionally, we reported the Area Under the Curve (AUC), which is widely used but can be less informative due to the imbalance between healthy and infarcted tissue. To address this, we also calculated the AUC0, which is weighted on hypoperfused areas and is better suited for small volumes. This metric is threshold-independent and provides a more nuanced evaluation of model performance.\n\nMean Absolute Error (MAE) was another key metric reported. MAE has the advantage of being clinically significant, as infarct volume correlates with clinical scores and outcomes. It is also invariant to spatial registration errors, making it a robust measure. However, it does not account for spatial location errors, which can be crucial in stroke prediction.\n\nThe reported metrics are representative of those commonly used in the literature for evaluating stroke outcome prediction models. Dice score, AUC, and MAE are standard metrics that provide a comprehensive assessment of model performance. The inclusion of AUC0 adds an additional layer of evaluation, particularly for small infarct volumes, which is an important consideration in stroke prediction.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, we focused on evaluating the performance of three popular machine learning models\u2014Gradient Boosting, Random Forests, and U-Net\u2014against a clinical thresholding method using our own dataset of acute ischemic stroke (AIS) patients.\n\nWe did, however, compare our results with those reported in the literature. For instance, we noted that a recent large study reported better performance of a U-Net model with attention-gating units, which might improve performance. We also mentioned that other deep learning models, such as SegNet, X-Net, and multi-path U-Net, have been proposed for acute AIS segmentation and could be tested for prediction.\n\nRegarding simpler baselines, we chose to omit the comparison with linear models in our study. This decision was based on the expectation that linear models, which combine linear transformations of voxel intensities and have limited expressive power, would not perform as well as the more complex models we tested. This expectation was confirmed by the literature, as all models outperformed linear models whenever compared.\n\nWe also performed post-hoc supplementary analyses to compare the performance of our U-Net model with a deep convolutional network proposed by Nielsen et al. on our dataset. The performances were comparable, with similar median Dice scores.\n\nIn summary, while we did not perform a direct comparison to publicly available methods on benchmark datasets or simpler baselines, we did compare our results with those reported in the literature and performed supplementary analyses to validate our findings.",
  "evaluation/confidence": "The evaluation of our models included statistical analysis to ensure the robustness and significance of our results. We used a pairwise Wilcoxon test with p-value adjusted for multiple comparisons using the Holm method. This approach allowed us to compare patient-level metrics across different models and assess the statistical significance of the differences observed.\n\nFor the subgroup of patients who underwent mechanical thrombectomy (MT), we specifically compared Dice scores from models with and without considering reperfusion status. The results showed significant improvements when reperfusion status was included, with p-values indicating strong statistical significance (e.g., p = 0.008 for Gradient Boosting, p < 0.001 for Random Forests, and p = 0.038 for U-Net).\n\nAdditionally, we evaluated model overfitting by comparing Dice scores in the training and test sets. The differences between these scores were minimal, suggesting that our models did not overfit the data. This was further supported by the use of 10-fold cross-validation, which helps in assessing the generalizability of the models.\n\nWe also analyzed the performance of our models using Bland-Altman plots to examine the agreement between predicted and true infarct volumes. This analysis revealed that while there was some underestimation of larger infarct volumes, the models performed consistently across different volume ranges.\n\nIn summary, our evaluation included rigorous statistical methods to ensure the confidence and significance of our results. The use of pairwise Wilcoxon tests, cross-validation, and Bland-Altman analysis provided a comprehensive assessment of model performance and reliability.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study focused on evaluating machine learning models for predicting tissue outcomes in acute stroke using a large dataset. The evaluation metrics, such as Dice scores, AUC, and mean absolute error, were computed and reported within the publication. However, the specific evaluation files, including the detailed predictions and ground-truth segmentations for each patient, were not released publicly. The models and their performance were assessed using 10-fold cross-validation, and the results were presented in tables and figures within the paper. The statistical analyses were conducted using R, with specific packages like ggplot2 and BlandAltmanLeh, but the raw data used for these analyses was not made available."
}