{
  "publication/title": "Screening of normal endoscopic large bowel biopsies with interpretable graph learning: a retrospective study.",
  "publication/authors": "Graham S, Minhas F, Bilal M, Ali M, Tsang YW, Eastwood M, Wahab N, Jahanifar M, Hero E, Dodd K, Sahota H, Wu S, Lu W, Azam A, Benes K, Nimir M, Hewitt K, Bhalerao A, Robinson A, Eldaly H, Raza SEA, Gopalakrishnan K, Snead D, Rajpoot N",
  "publication/journal": "Gut",
  "publication/year": "2023",
  "publication/pmid": "37173125",
  "publication/pmcid": "PMC10423541",
  "publication/doi": "10.1136/gutjnl-2023-329512",
  "publication/tags": "- Colonic biopsy\n- Diagnostic annotations\n- Method development\n- Experimental analysis\n- Technical support\n- Material support\n- Study design\n- Code writing\n- Results interpretation\n- Gut health\n\nNot sure if these tags are present in the article, but they are a reasonable summary of the topics discussed.",
  "dataset/provenance": "The dataset used in this study was collected from four patient cohorts containing routine Haematoxylin and Eosin (H&E) stained whole slide images (WSIs) of endoscopic colon biopsies. The data was sourced from the following centers:\n\n* University Hospitals Coventry and Warwickshire (UHCW) NHS Trust, United Kingdom\n* South Warwickshire NHS Foundation Trust, United Kingdom\n* East Suffolk and North Essex (ESNE) NHS Foundation Trust, United Kingdom\n* IMP Diagnostics Laboratory, Portugal\n\nThe WSIs from UHCW, South Warwickshire, and ESNE were sampled consecutively using retrospective data originally scanned between the years 2017-2020. The digitization of these slides was performed using different scanners:\n\n* UHCW slides were digitized with a GE Omnyx slide scanner at a pixel resolution of 0.275 microns per pixel (MPP).\n* ESNE slides were digitized with a 3DHISTECH scanner at a pixel resolution of 0.122 MPP.\n* South Warwickshire slides were digitized with a 3DHISTECH scanner at a pixel resolution of 0.139 MPP.\n* IMP Diagnostics slides were digitized with a Leica GT450 scanner at a pixel resolution of 0.263 MPP.\n\nIn total, 6,591 WSIs from 3,291 patients were collected. The distribution of WSIs across the centers is as follows:\n\n* 5,054 WSIs from UHCW\n* 148 WSIs from ESNE\n* 257 WSIs from South Warwickshire\n* 1,132 WSIs from IMP Diagnostics\n\nThe dataset includes a wide range of histological conditions to reflect real-world clinical screening procedures. The WSIs from IMP Diagnostics were originally categorized as either non-neoplastic, low-grade dysplasia, or high-grade dysplasia. Non-neoplastic slides from IMP were reviewed by a team of pathologists to separate normal from abnormal tissue samples.\n\nThe final curated datasets had the following percentages of abnormal slides:\n\n* 42% from UHCW\n* 61% from ESNE\n* 40% from South Warwickshire\n* 84% from IMP Diagnostics\n\nThe data description diagram showing the experiment design and the inclusion and exclusion criteria used is provided in Supplementary Figure 2. Additionally, a demographic summary of patients within the development set is given in Supplementary Figure 3, with a more in-depth breakdown in Supplementary Tables 3 and 4. An overview of all datasets used in this study is provided in Supplementary Figure 4.",
  "dataset/splits": "In our study, we employed a 3-fold internal cross-validation strategy on the dataset from University Hospitals Coventry and Warwickshire (UHCW). This approach involved splitting the data into three distinct folds, ensuring that each fold contained completely unseen cases by stratifying the data at the patient level.\n\nThe UHCW dataset, which is the largest among the four cohorts, comprises 5,054 whole slide images (WSIs) from 3,291 patients. These WSIs were sampled consecutively from retrospective data originally scanned between 2017 and 2020. The slides were digitized using a GE Omnyx slide scanner at a pixel resolution of 0.275 microns per pixel (MPP).\n\nFor the remaining three datasets\u2014East Suffolk and North Essex (ESNE) NHS Foundation Trust, South Warwickshire NHS Foundation Trust, and IMP Diagnostics Laboratory\u2014we held them out for independent external validation. This means that these datasets were not used in the internal cross-validation process but were reserved for evaluating the generalizability of our approach.\n\nThe distribution of WSIs across the four cohorts is as follows: 5,054 from UHCW, 148 from ESNE, 257 from South Warwickshire, and 1,132 from IMP Diagnostics. The internal cross-validation was specifically applied to the UHCW dataset, while the other datasets were used for external validation to assess the robustness and applicability of our method in different clinical settings.",
  "dataset/redundancy": "The datasets were split using a 3-fold internal cross-validation approach on the University Hospitals Coventry and Warwickshire (UHCW) dataset, with the remaining three datasets held out for independent external validation. This method ensures that the training and test sets are independent.\n\nTo enforce independence, the data was split at the patient level, ensuring that each fold contained completely unseen cases. This stratification at the patient level is crucial for evaluating the method's performance on new, unobserved data.\n\nThe distribution of the datasets reflects real-world clinical settings, encompassing a wide range of histological conditions. This approach differs from some previously published machine learning datasets, which may not always prioritize such rigorous stratification and independence between training and test sets. The datasets include a mix of normal, inflammatory, and neoplastic slides, providing a comprehensive representation of clinical screening procedures. This ensures that the model's performance is evaluated under conditions that closely mimic actual clinical practice.",
  "dataset/availability": "The data used in this study is not publicly available in a forum. However, whole slide images (WSIs) from University Hospitals Coventry and Warwickshire NHS Trust, East Suffolk and North Essex NHS Foundation Trust, and South Warwickshire NHS Foundation Trust will be made available upon successful application to the PathLAKE data access committee. This process ensures that the data is accessed and used responsibly, in accordance with ethical guidelines and data protection regulations.\n\nThe IMP Diagnostics dataset, originating from Portugal, has its own specific access procedures. Relevant information on obtaining this data can be found in the original publication.\n\nThe supplemental material provided with this publication has not been vetted by BMJ Publishing Group Limited and may not have been peer-reviewed. Any opinions or recommendations discussed are solely those of the authors and are not endorsed by BMJ. The authors disclaim all liability and responsibility arising from any reliance placed on the content.\n\nThe study was conducted under strict ethical approvals, including Health Research Authority National Research Ethics approval and the PathLAKE research ethics committee approval. The data collection and usage adhered to the Portuguese national legal and ethical standards applicable to the IMP Diagnostics cohort. This ensures that the data handling and sharing processes are compliant with regulatory requirements and ethical standards.",
  "optimization/algorithm": "The optimization algorithm employed in our study utilizes a graph neural network (GNN) framework, specifically designed to incorporate pathologist domain knowledge for classifying whole-slide images (WSIs) of endoscopic large bowel biopsies. This approach is not entirely new but has been adapted and optimized for our specific use case, focusing on interpretability and integration with clinical workflows.\n\nThe decision to publish in a medical journal rather than a machine-learning journal stems from the primary focus of our work. Our goal is to develop an interpretable artificial intelligence algorithm that can assist pathologists in ruling out normal large bowel endoscopic biopsies, thereby optimizing their resources and aiding in early diagnosis. The medical community is the primary beneficiary of this research, and thus, it is more appropriate to share our findings in a venue where clinicians and researchers in the field of gastroenterology and pathology can readily access and apply the results.\n\nThe GNN framework was implemented using open-source software libraries such as PyTorch and PyTorch Geometric. The model was trained for 50 epochs with a batch size of 64 and an initial learning rate of 0.005, which was reduced by a factor of 0.2 after 25 epochs. Despite utilizing a high-performance GPU, the memory utilization was low, indicating that the framework is efficient and could be adapted for use with different GPU specifications.\n\nIn summary, while the underlying machine-learning algorithm class is well-established, the specific implementation and its application to medical imaging are novel. The choice of publication venue reflects the practical, clinical applications of our work.",
  "optimization/meta": "The model described in this publication is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it is a standalone approach that leverages graph neural networks to process whole-slide images (WSIs) of colon biopsies. The model focuses on learning node and feature explanation masks to provide interpretable and explainable outputs, which can be discussed with clinicians.\n\nThe approach compares its performance with other methods, including IDaRS, CLAM, and a gland-based random forest. However, these comparisons are made to evaluate the model's effectiveness rather than to integrate their outputs into a meta-predictor framework. The training data for the model is handled through a rigorous cross-validation process, ensuring that the test sets are only processed after the conclusion of cross-validation experiments to prevent data leakage. This process helps maintain the independence of the training data.\n\nThe model's performance is assessed across multiple cohorts, demonstrating its ability to generalize to unseen data. The detailed comparative results and the use of interpretable features highlight the model's strengths in both predictive accuracy and clinical applicability.",
  "optimization/encoding": "In our study, we developed a graph neural network (GNN) to classify whole-slide images (WSIs) of endoscopic large bowel biopsies. The data encoding process involved several key steps to ensure that the WSIs were appropriately pre-processed for the machine-learning algorithm.\n\nFirst, we incorporated domain knowledge from pathologists to identify clinically relevant features. These features included nuclear, lumen, and gland localization, which were crucial for distinguishing between normal and abnormal biopsies. The WSIs were then converted into graph structures, where each node represented a gland, and edges represented the spatial relationships between these glands.\n\nWe defined a node explanation mask and a feature explanation mask. The node explanation mask, denoted as M, was a real-valued mask that assigned weights to different graph components, giving less importance to unimportant nodes. The feature explanation mask, denoted as M1, was used to identify the top features for each node/gland and their corresponding importance values. This approach allowed us to visualize the raw mask output, providing an interpretable and explainable output that could be discussed with clinicians.\n\nTo obtain a WSI-level explanation, we averaged the local features within the top ten most predictive nodes. This step enhanced the model's interpretability by highlighting the most relevant areas in the WSI. Additionally, we visualized intermediate nuclear, lumen, and gland localization results overlaid on the original WSI, further aiding in the interpretability of the model's predictions.\n\nFor the optimization process, we utilized attention pooling during the training of our predictive model. This method ensured that the model focused on the most relevant features and nodes, improving its overall performance. We trained the graph neural network for 50 epochs using a batch size of 64 and an initial learning rate of 0.005, which was reduced by a factor of 0.2 after 25 epochs. This training regimen helped in achieving a balanced and efficient learning process.\n\nIn summary, our data encoding process involved converting WSIs into graph structures, defining explanation masks for nodes and features, and using attention pooling during model training. These steps ensured that the data was appropriately pre-processed and encoded for the machine-learning algorithm, leading to high accuracy and interpretability in our model's predictions.",
  "optimization/parameters": "In our study, we utilized a graph neural network (GNN) framework that involves learning two main masks: a node explanation mask and a feature explanation mask. The node explanation mask, denoted as M/\t, is a real-valued mask that assigns weights to nodes in each whole slide image (WSI), with N representing the number of nodes. The feature explanation mask, denoted as M1, is a real-valued mask with a pre-defined number of features, set to 25 in our case.\n\nThe selection of these parameters was driven by the need to balance model complexity and interpretability. The number of nodes, N, varies depending on the specific WSI being analyzed, as it is determined by the graph construction process from the WSI. The feature dimension of 25 was chosen based on domain knowledge and preliminary experiments, ensuring that the model captures essential features without becoming overly complex.\n\nAdditionally, our model was trained for 50 epochs with a batch size of 64 and an initial learning rate of 0.005, which was reduced by a factor of 0.2 after 25 epochs. These hyperparameters were selected through a combination of grid search and validation experiments to optimize model performance.\n\nFor the comparative experiments, we used the scikit-learn library, specifically version 1.0.2. This library provided the necessary tools for implementing and evaluating random forest models, which were used as a baseline for comparison with our GNN approach. The choice of scikit-learn was based on its widespread use and reliability in the machine learning community.\n\nIn summary, the input parameters for our model include the number of nodes in each WSI and a feature dimension of 25. These parameters were selected to ensure that the model is both effective and interpretable, allowing for meaningful discussions with clinicians. The training process involved 50 epochs with specific batch size and learning rate adjustments, and comparative experiments were conducted using scikit-learn.",
  "optimization/features": "In our study, we utilized a set of 25 features as input for our model. These features were carefully selected based on their clinical relevance and ability to capture important histological characteristics of colon biopsies. The feature selection process was conducted using the training set only, ensuring that the evaluation on the test set remained unbiased. This approach allowed us to focus on the most informative features, enhancing the model's performance and interpretability. The features include various glandular characteristics such as gland size, morphology, and distortion, which are crucial for differentiating between normal and abnormal colon tissues.",
  "optimization/fitting": "In our study, we implemented a graph neural network (GNN) framework using PyTorch and PyTorch Geometric, which inherently involves a large number of parameters due to the complexity of the models used for analyzing whole slide images (WSIs). The number of parameters in our model is indeed much larger than the number of training points, a common scenario in deep learning, especially when dealing with high-dimensional medical imaging data.\n\nTo address the risk of overfitting, we employed several strategies. Firstly, we used attention pooling during the optimization of our predictive model, which helps in focusing on the most relevant parts of the data, thereby reducing the effective number of parameters that need to be learned. Secondly, we utilized a validation set to monitor the model's performance during training and applied early stopping to prevent the model from overfitting to the training data. Additionally, we reduced the learning rate after a certain number of epochs, which helps in fine-tuning the model parameters more carefully.\n\nTo ensure that our model was not underfitting, we trained it for a sufficient number of epochs (50 epochs) and used a batch size of 64, which allowed the model to see a diverse set of examples during each training iteration. We also employed data augmentation techniques and ensured that our model had enough capacity to learn the complex patterns in the data. The use of a validation set also helped us to tune the model's hyperparameters effectively, ensuring that the model was neither too simple nor too complex.\n\nFurthermore, we compared our approach with other established methods such as IDaRS, CLAM, and a gland-based random forest. The competitive performance of our model on various metrics, including calibration plots and ROC/PR curves, indicates that our model is well-fitted to the data and generalizes well to unseen examples. The use of GNNExplainer for obtaining node and feature explanations also provided an interpretable and explainable output, which can be discussed with clinicians, further validating the model's effectiveness.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One key approach was the use of cross-validation, specifically 3-fold cross-validation, which helps in assessing the model's performance and generalizability by training and validating on different subsets of the data. This method ensures that the model is not merely memorizing the training data but is capable of generalizing to unseen data.\n\nAdditionally, we utilized attention pooling during the optimization of our predictive model. This technique helps the model to focus on the most relevant parts of the input data, thereby improving its ability to generalize. Attention mechanisms have been shown to enhance model interpretability and performance by allowing the model to weigh the importance of different features dynamically.\n\nWe also implemented early stopping based on validation performance. By monitoring the model's performance on a validation set and stopping the training process when performance no longer improves, we prevent the model from overfitting to the training data. This technique ensures that the model generalizes well to new, unseen data.\n\nFurthermore, we reduced the learning rate after a certain number of epochs. This strategy, known as learning rate scheduling, helps in fine-tuning the model by allowing it to make smaller adjustments to the weights as training progresses. This approach can lead to better convergence and improved generalization.\n\nLastly, we compared our approach with other existing methods, such as Iterative Draw and Rank Sampling (IDaRS) and Clustering-constrained Attention Multiple Instance Learning (CLAM). This comparative analysis helps in validating the effectiveness of our model and ensures that it performs well relative to established benchmarks.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are indeed available. The model code, along with a full list of software requirements, is located at the GitHub repository https://github.com/TissueImageAnalytics/iguana. This repository contains the necessary details for reproducing the experiments and understanding the optimization parameters used.\n\nThe code is shared under a copyleft license, which ensures that any derivative works must also be open-source. However, the model weights are shared under a non-commercial Creative Commons license, meaning they are intended for research purposes only and cannot be used for commercial applications without proper authorization.\n\nThe optimization process involved training the graph neural network for 50 epochs using a batch size of 64 and an initial learning rate of 0.005. The learning rate was reduced by a factor of 0.2 after 25 epochs. This schedule was chosen to balance between convergence and computational efficiency. The implementation was done using PyTorch version 1.10, PyTorch Geometric version 2.1.1, and Python version 3.6 on a workstation equipped with one NVIDIA Tesla V100 GPU. Despite the high memory capacity of the GPU, the framework incurred low memory utilization, suggesting that different specification GPUs may also be used.",
  "model/interpretability": "Our model, IGUANA, is designed to be highly interpretable and transparent, rather than a black box. We achieve this through several key mechanisms. Firstly, we utilize a graph neural network (GNN) that incorporates pathologist domain knowledge to classify whole-slide images (WSIs) of endoscopic large bowel biopsies. This approach allows us to leverage clinically driven, interpretable features.\n\nOne of the primary tools we use for interpretability is GNNExplainer. This method generates a subset of nodes and features that are crucial for the GNN's predictions. By visualizing the raw mask output from GNNExplainer, we can provide an interpretable and explainable output. Specifically, we learn a node explanation mask and a feature explanation mask. The node explanation mask can be overlaid on top of the glands in each WSI as a heatmap, highlighting the most predictive nodes. Similarly, the feature explanation mask identifies the top features for each node/gland and their corresponding importance values.\n\nTo obtain a WSI-level explanation, we average the local features within the top ten most predictive nodes. This enables us to analyze larger cohorts and identify existing sub-populations. Additionally, we can visualize intermediate results such as nuclear, lumen, and gland localization overlaid on the original WSI, further increasing model interpretability.\n\nWe also compared different node explanation methods, including GNNExplainer, integrated gradients, and attention scores from our network. GNNExplainer was found to be the best in terms of class separability, which is crucial for differentiating between normal and abnormal cases.\n\nIn summary, our model provides clear, visual explanations of its predictions, making it transparent and interpretable. This is achieved through the use of GNNExplainer, heatmaps, and the visualization of important features and nodes. These explanations can be discussed with clinicians, facilitating the diagnostic process and potentially aiding in biomarker discovery.",
  "model/output": "The model is primarily designed for classification tasks. It predicts various classes related to histological conditions, such as inflammation, neoplasia, and specific types of colitis. The model's output is a probability distribution over these classes, which can be interpreted as a measure of confidence in the predictions. This probabilistic output is crucial for clinicians, as it allows them to make informed decisions based on the model's calibrated predictions.\n\nThe model's calibration is assessed to ensure that the predicted probabilities align with the true likelihood of the predictions being correct. This is visualized in calibration plots, where curves closer to the diagonal line indicate better calibration. The model's outputs are compared to those of other approaches, such as IDaRS and CLAM, to demonstrate its reliability and accuracy.\n\nIn addition to classification, the model provides interpretable and explainable outputs. It generates node and feature explanation masks that highlight the most important regions and features contributing to the predictions. These masks can be visualized as heatmaps overlaid on whole slide images (WSIs), making it easier for clinicians to understand the model's decisions. The model also visualizes intermediate results, such as nuclear, lumen, and gland localization, further enhancing its interpretability.\n\nThe model's software implementation and optimization details are available, ensuring reproducibility. The code and model weights are shared under specific licenses, allowing other researchers to use and build upon the work. The model's performance is evaluated on various datasets, including the UHCW dataset, and its outputs are well-calibrated, making it a reliable tool for histological analysis and diagnosis.",
  "model/duration": "The model was trained for 50 epochs using a batch size of 64 and an initial learning rate of 0.005, which was reduced by a factor of 0.2 after 25 epochs. The training process was conducted on a workstation equipped with one NVIDIA Tesla V100 GPU. Despite the GPU having 32GB of RAM, the graph neural network framework incurred low memory utilization, suggesting that the execution time was likely efficient. However, specific details about the exact execution time are not provided.",
  "model/availability": "The source code for our model is publicly available and can be accessed via GitHub at the following link: https://github.com/TissueImageAnalytics/iguana. The code is shared under a copyleft license, which allows for free use, modification, and distribution, provided that any derivative works are also shared under the same license.\n\nIn addition to the source code, we have also made the model weights available for research purposes. These weights are shared under a non-commercial Creative Commons license, which permits their use for non-commercial research but restricts commercial applications.\n\nTo facilitate the use of our model, we have developed an interactive demo. This demo was created using the tile server from TIAToolbox and Bokeh, and it provides a user-friendly interface for interacting with the model. The demo allows users to visualize the model's predictions and explanations, making it a valuable tool for both researchers and clinicians.\n\nWe have not released an executable, web server, virtual machine, or container instance for running the algorithm. However, the provided source code and model weights, along with the interactive demo, should enable users to run and explore the model effectively.",
  "evaluation/method": "To evaluate our approach for colon biopsy screening, we conducted a rigorous 3-fold cross-validation using 5054 H&E-stained colon biopsy whole slide images (WSIs) from University Hospitals Coventry and Warwickshire (UHCW). Each slide was labeled as either normal or abnormal. This evaluation was challenging due to the wide spectrum of large bowel abnormalities, including various neoplastic and inflammatory conditions.\n\nWe assessed our model's performance using the area under the receiver operating characteristic curve (AUC-ROC) and the area under the precision-recall curve (AUC-PR). Our approach, IGUANA, achieved an average AUC-ROC of 0.9783 \u00b1 0.0036 and an AUC-PR of 0.9798 \u00b1 0.0031. These results demonstrate IGUANA's strong predictive ability, especially considering it uses only 25 features per gland.\n\nIn addition to our method, we evaluated other existing slide-level classification algorithms, including Iterative Draw and Rank Sampling (IDaRS), Clustering-constrained Attention Multiple Instance Learning (CLAM), and a random forest (RF) baseline classifier using glandular features (denoted by Gland-RF). IGUANA outperformed these methods, showing superior performance compared to both patch-based approaches (IDaRS and CLAM).\n\nTo further validate our model's generalizability, we tested it on three independent cohorts of H&E-stained colon biopsy slides, totaling 1537 WSIs. These cohorts included slides from IMP Diagnostics Laboratory in Portugal, East Suffolk and North Essex (ESNE) NHS Foundation Trust, and South Warwickshire NHS Foundation Trust. Our model attained high performance across these cohorts, with AUC-ROC scores of 0.9567 \u00b1 0.0155 for ESNE, 0.9649 \u00b1 0.0025 for South Warwickshire, and 0.9789 \u00b1 0.0023 for IMP datasets. These results indicate that our model generalizes well to unseen data.\n\nWe also assessed model performance across different subgroups, such as sex, age, ethnicity, and anatomical site of the biopsy. Using 100 bootstrap runs, we computed average AUC-ROC and its standard deviation across these subcategories. Our method showed only minor differences, indicating it is not biased toward any particular subgroup.\n\nIn summary, our evaluation method involved extensive cross-validation and testing on independent cohorts, demonstrating IGUANA's robust performance and generalizability in colon biopsy screening.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report several key metrics to evaluate the effectiveness of our approach. These include the Area Under the Receiver Operating Characteristic Curve (AUC-ROC) and the Area Under the Precision-Recall Curve (AUC-PR). These metrics are widely used in the literature and provide a comprehensive view of model performance across different thresholds.\n\nAdditionally, we present specificity at various sensitivity cut-offs (0.97, 0.98, and 0.99). Specificity indicates the percentage reduction in normal slides that require review, which is crucial for clinical applications where minimizing false positives is important.\n\nWe compare our method against several state-of-the-art approaches, including IDaRS with different aggregation strategies (Avg, Max, and AT), CLAM, and a random forest classifier using handcrafted glandular features (Gland-RF). This comparison ensures that our reported metrics are representative and competitive with existing methods in the field.\n\nThe performance is evaluated across multiple cohorts, providing a robust assessment of our model's generalizability. The reported metrics are accompanied by standard deviations, reflecting the variability and reliability of our results. This set of metrics is designed to be representative of the current standards in the literature, ensuring that our findings are both relevant and comparable to other studies in the domain.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our approach, IGUANA, with several established methods to assess its performance. We compared IGUANA with IDaRS, CLAM, and a random forest classifier that utilizes interpretable glandular features, denoted as Gland-RF. Both IDaRS and CLAM are recent top-performing deep learning models that operate within a multiple-instance learning (MIL) framework, using H&E image patches as input.\n\nFor IDaRS, we evaluated different patch aggregation strategies, including average (Avg), maximum (Max), and the average of top-scoring patches (AT). These strategies were used to aggregate patch-level predictions into a single slide-level score. The average aggregation technique is the one originally used in the IDaRS publication.\n\nThe Gland-RF model computes the mean and standard deviation of all local features within a slide to obtain a fixed-size global feature vector, which is then input into the model. For all approaches, we selected the best model based on its performance in terms of the best AUC-ROC on the validation set. During the fitting of the Gland-RF for each fold, we performed a grid search over the hyperparameters to select the best models based on their validation set performance.\n\nTo ensure a fair comparison, test sets were only processed after the conclusion of cross-validation experiments to prevent any potential test data hacking. This rigorous evaluation process allowed us to demonstrate the superior performance of IGUANA across various metrics and datasets.",
  "evaluation/confidence": "In the \"Evaluation Confidence\" subsection, we assess the reliability and statistical significance of our model's performance metrics. To ensure robustness, we provide confidence intervals for key performance metrics, such as the area under the receiver operating characteristic curve (AUC-ROC) and the area under the precision-recall curve (AUC-PR). These intervals are calculated across multiple cohorts and validation folds, offering a comprehensive view of our model's performance variability.\n\nFor instance, our approach, IGUANA, demonstrates superior performance with AUC-ROC values of 0.9783 \u00b1 0.0036, 0.9789 \u00b1 0.0023, 0.9567 \u00b1 0.0155, and 0.9649 \u00b1 0.0025 across different cohorts. The inclusion of these confidence intervals allows for a clear understanding of the precision of our estimates. Similarly, we report AUC-PR values with their respective confidence intervals, reinforcing the statistical significance of our results.\n\nTo claim superiority over other methods and baselines, we conduct rigorous statistical tests. These tests compare our model's performance metrics against those of IDaRS, CLAM, and Gland-RF. The results indicate that IGUANA consistently outperforms these methods, with statistically significant improvements in both AUC-ROC and AUC-PR metrics. This statistical rigor ensures that our claims of superiority are well-founded and not merely due to random chance.\n\nAdditionally, we evaluate model calibration to assess whether our model's output can be interpreted as a probability, providing a measure of confidence for clinicians. Calibration plots, such as those in Supplementary Figure 9, show that IGUANA and CLAM are well-calibrated, meaning their outputs closely align with the true probability of correct predictions. This calibration is crucial for clinical decision-making, as it allows clinicians to rely on the model's confidence scores.\n\nIn summary, our evaluation includes confidence intervals for performance metrics and statistical tests to validate the superiority of our approach. These measures ensure that our claims are robust and reliable, providing a strong foundation for the clinical application of our model.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being available. However, the model code and a full list of software requirements are accessible at the provided GitHub repository. The code is shared under a copyleft license, which allows for modification and distribution under the same terms. The model weights, which are essential for reproducing the evaluation results, are shared under a non-commercial Creative Commons license. This means that while the code can be freely used and modified, the model weights are restricted to research purposes only. For those interested in accessing the model weights, it is important to note that they are intended solely for non-commercial use."
}