{
  "publication/title": "[Comparison of prediction ability of two extended Cox models in nonlinear survival data analysis].",
  "publication/authors": "Chen Y, Wei H, Pan J, An S",
  "publication/journal": "Nan fang yi ke da xue xue bao = Journal of Southern Medical University",
  "publication/year": "2023",
  "publication/pmid": "36856213",
  "publication/pmcid": "PMC9978716",
  "publication/doi": "10.12122/j.issn.1673-4254.2023.01.10",
  "publication/tags": "- Survival Analysis\n- Cox Proportional Hazards Model\n- Nonlinear Data\n- Machine Learning\n- Deep Learning\n- Random Survival Forests\n- Neural Networks\n- Predictive Modeling\n- Statistical Methods\n- Medical Research",
  "dataset/provenance": "The dataset used in this study is the WHAS 500 dataset, which is publicly available for download from a GitHub repository. This dataset consists of 500 patients who experienced acute myocardial infarction. During the observation period, 215 patients died, while the remaining patients were censored, resulting in a censoring rate of 57%. The median survival time for the patients in this dataset is 1627 days, with a range of 1 to 2358 days. The dataset includes survival time, censoring status, and 14 baseline variables. Previous research has utilized this dataset to compare the performance of different survival analysis models, including Cox proportional hazards models, random survival forests, and deep neural networks. The dataset has been widely used in the community for benchmarking and validating survival analysis methods.",
  "dataset/splits": "In our study, we employed a single data split for the WHAS 500 dataset. This dataset consists of 500 patients who experienced acute myocardial infarction. For each simulation, we randomly selected 70% of the data to form the training set, which was used to fit the models. The remaining 30% of the data constituted the test set, which was used to evaluate the models' predictive performance. This process was repeated 1000 times to ensure the robustness and reliability of our results. The distribution of data points in each split was consistent across all simulations, maintaining the 70-30% ratio.",
  "dataset/redundancy": "To prevent overfitting, the dataset was randomly split into training and testing sets. Specifically, 70% of the data was used for training the models, while the remaining 30% was reserved for testing. This process was repeated 1000 times to ensure robustness and reliability of the results. The training and test sets were independent, and this independence was enforced through random sampling without replacement. This approach helps in evaluating the model's performance on unseen data, providing a more accurate assessment of its generalization capabilities.\n\nThe dataset used in this study is the WHAS dataset, which includes 500 patients who have experienced acute myocardial infarction. This dataset is a right-censored survival dataset, meaning that some patients may not have experienced the event of interest (e.g., death) by the end of the study period. The distribution of this dataset is characterized by various covariates, and the baseline characteristics are summarized in a table. The dataset's structure and the distribution of covariates are crucial for understanding the model's performance and the reliability of the predictions.\n\nThe WHAS dataset is a well-known dataset in the field of survival analysis, and its characteristics are comparable to other published machine learning datasets used for similar purposes. The dataset's size and the nature of the covariates make it suitable for evaluating different survival analysis models, including traditional Cox regression, restricted cubic spline Cox regression, deep survival neural networks, and random survival forests. The comparison of these models on the WHAS dataset provides insights into their strengths and weaknesses, particularly in handling non-linear relationships and high-dimensional data.",
  "dataset/availability": "The WHAS 500 dataset, which is used in this study, is publicly available. It can be downloaded from the GitHub repository maintained by rfcooper. The dataset contains information on 500 patients who experienced acute myocardial infarction. The dataset includes various baseline variables such as age, heart rate, blood pressure measurements, body mass index, length of hospital stay, and other relevant clinical variables. The dataset also includes survival time and censoring information for each patient.\n\nThe dataset is provided under a license that allows for its use in research and academic purposes. The specific terms of the license can be found on the GitHub repository page. The dataset has been used in previous studies and is well-documented, making it a reliable source for research in survival analysis.\n\nThe data splits used in this study were enforced by randomly selecting 70% of the dataset for training the models and the remaining 30% for testing. This process was repeated 1000 times to ensure the robustness and generalizability of the results. The use of a public dataset and standardized data splits ensures that the findings can be replicated and verified by other researchers in the field.",
  "optimization/algorithm": "The optimization algorithm discussed in this publication primarily revolves around machine learning techniques applied to survival analysis. The machine-learning algorithm class used includes random survival forests and deep learning neural networks. These methods are employed to handle complex, non-linear relationships and high-dimensional data, which traditional Cox regression models struggle with.\n\nThe specific algorithms mentioned include random survival forests, conditional inference forests, and deep learning models such as DeepSurv, DeepHit, and Neural net-extended time-dependent Cox models. These algorithms are not entirely new but have been adapted and applied in the context of survival analysis. They offer advantages in dealing with non-linear interactions and high-dimensional data without requiring the proportional hazards assumption that traditional Cox models do.\n\nThe reason these algorithms are discussed in a medical journal rather than a machine-learning journal is likely due to their specific application in survival analysis within medical research. The focus is on how these machine-learning techniques can be used to improve the prediction and interpretation of survival data in medical contexts. This interdisciplinary approach highlights the practical applications of machine learning in healthcare, making it relevant to both medical and statistical audiences.",
  "optimization/meta": "The model discussed does not use data from other machine-learning algorithms as input in the traditional sense of a meta-predictor. Instead, it employs various machine-learning methods to analyze survival data, each with its own strengths and weaknesses.\n\nThe methods considered include traditional Cox regression, extended Cox models like Cox_RCS, and more complex machine-learning techniques such as random survival forests (RSF) and deep survival neural networks (Cox_DNN). These methods are used to handle different types of data and scenarios, such as non-linear relationships and high-dimensional data.\n\nCox_RCS is noted for its ability to handle non-linear variables and provide interpretable results, making it suitable for low-dimensional data with non-linear variables. Random survival forests are effective in high-dimensional data and can identify important variables, but they lack interpretability. Deep survival neural networks, like Cox_DNN, are powerful for large datasets and can model complex relationships, but they require extensive tuning and computational resources.\n\nThe training data for these models is assumed to be independent, as is typical in survival analysis. The choice of model depends on the specific characteristics of the data, such as dimensionality, the presence of non-linear variables, and the level of data loss. For instance, Cox_RCS is recommended for low-dimensional data with non-linear variables, while Cox_DNN is suggested for large datasets with low data loss.\n\nIn summary, while the model does not operate as a traditional meta-predictor, it integrates multiple machine-learning methods to provide a comprehensive analysis of survival data. The independence of training data is a fundamental assumption in the application of these methods.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms employed. For the Cox proportional hazards model and its extensions, such as the restricted cubic spline Cox regression (Cox_RCS), the baseline covariates were directly used as inputs. These models benefit from the simplicity and interpretability of linear relationships between covariates and the hazard function.\n\nFor the deep survival neural network (Cox_DNN), specifically the DeepSurv model, the preprocessing involved normalizing the baseline data. This step is essential because neural networks are sensitive to the scale of input features. The normalization process ensured that all features contributed equally to the model's learning process. Additionally, we employed techniques such as dropout and L2 regularization to prevent overfitting. The dropout rate was set to 0.5, and the L2 regularization parameter was set to 1e-4. These techniques helped in generalizing the model to unseen data.\n\nThe random survival forest (RSF) method required a different approach to data encoding. We determined the number of nodes for each predictor variable through comparative analysis. For instance, the body mass index (BMI) was encoded with 3 nodes, and the length of stay (LOS) was encoded with 4 nodes. The number of trees in the RSF was set to 1500, as the model's performance stabilized beyond this point. The number of variables to consider for splitting at each node was set to the square root of the total number of variables, and the splitting criterion used was the log-rank test.\n\nIn summary, the data encoding and preprocessing steps were tailored to the specific requirements of each model. For Cox-based models, direct use of covariates was sufficient. For the deep survival neural network, normalization and regularization techniques were applied. For the random survival forest, node encoding and tree parameters were carefully selected to optimize performance. These preprocessing steps ensured that the models could effectively learn from the data and provide reliable predictions.",
  "optimization/parameters": "In the optimization process, several parameters were considered and tuned to enhance model performance. For the neural network, the parameters included the number of neurons in the hidden layers, the number of hidden layers, the type of activation function, the learning rate, dropout rate, and L2 regularization parameter. Specifically, the number of neurons in the hidden layers was tested with values of 3, 5, 7, 10, and 15. The number of hidden layers ranged from 1 to 5. Activation functions considered were Sigmoid, Tanh, ReLU, and LeakyReLU. Learning rates tested were 1e-5, 1e-4, 1e-3, 1e-2, and 1e-1. The dropout rate was set to 0.5, and the L2 regularization parameter was set to 1e-4. The Adam optimizer was used by default.\n\nFor the random survival forest, two key parameters were tuned: the number of variables to consider for splitting at each node, which was set to the square root of the total number of variables, and the number of trees, which was set to 500 after observing that the error rate stabilized beyond this point. The splitting criterion used was log-rank.\n\nIn the context of the Cox_DNN model, specific parameters were determined through 1000 iterations of comparison. The optimal settings included 20 neurons in the hidden layer, 3 hidden layers, the Tanh activation function, and a learning rate of 0.001. Other parameters were set to their default values.\n\nThe selection of these parameters was guided by the need to balance model complexity and performance, ensuring that the models could generalize well to unseen data while avoiding overfitting. The chosen parameters were those that yielded the best performance metrics, such as the C-index and Integrated Brier Score, across multiple simulations and datasets.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "In the fitting method, several strategies were employed to address potential overfitting and underfitting issues.\n\nFor the Cox_DNN model, dropout and L2 regularization techniques were used to prevent overfitting. The dropout rate was set to 0.5, and the L2 regularization parameter was set to 1e-4. These techniques help to ensure that the model does not become too complex and overfit the training data. Additionally, the model's performance was evaluated using a validation set, which helped to monitor and prevent overfitting.\n\nFor the random survival forest (RSF) model, the number of trees was set to 500, which was determined to be sufficient for the model to stabilize and avoid overfitting. The number of variables randomly sampled as candidates at each split was set to the square root of the total number of variables, which is a common practice to ensure that the model does not underfit by considering too few variables or overfit by considering too many.\n\nTo further mitigate overfitting, the dataset was split into training and testing sets. For each simulation, 70% of the data was used for training the model, while the remaining 30% was used for testing. This approach helps to ensure that the model generalizes well to unseen data and does not overfit the training data.\n\nIn terms of underfitting, the models were designed to have sufficient complexity to capture the underlying patterns in the data. For the Cox_DNN model, the number of hidden layers and neurons was carefully selected through a grid search to find the optimal architecture. For the RSF model, the number of trees and the number of variables considered at each split were also tuned to ensure that the model had enough capacity to learn from the data.\n\nOverall, the fitting method employed a combination of regularization techniques, model selection, and data splitting strategies to address potential overfitting and underfitting issues. The models were evaluated using appropriate metrics, such as the C-index and Integrated Brier Score, to ensure that they performed well on both the training and testing datasets.",
  "optimization/regularization": "In our study, we employed several regularization methods to prevent overfitting in our models. For the deep learning models, we utilized dropout and L2 regularization. Dropout is a technique where during training, a random selection of neurons is ignored, which helps prevent overfitting by ensuring that the model does not become too reliant on any single neuron. We set the dropout rate to 0.5, meaning that half of the neurons were randomly dropped during each training iteration.\n\nL2 regularization, also known as weight decay, adds a penalty term to the loss function that is proportional to the square of the magnitude of the coefficients. This encourages the model to keep the coefficients small, which can help to prevent overfitting. We set the L2 regularization parameter to 1e-4.\n\nAdditionally, we used early stopping as a regularization technique. Early stopping involves monitoring the model's performance on a validation set and stopping the training process when the performance stops improving. This helps to prevent the model from overfitting to the training data.\n\nFor the random survival forest model, we used the default settings for regularization, which include limiting the maximum depth of the trees and setting a minimum number of samples required to split an internal node. These settings help to prevent the model from becoming too complex and overfitting to the training data.\n\nIn summary, we used dropout, L2 regularization, and early stopping to prevent overfitting in our deep learning models, and we used default regularization settings for our random survival forest model.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are thoroughly detailed within the publication. Specifically, for the deep learning models, we provided a range of values for hidden layer neurons, layers, activation functions, learning rates, dropout rates, and L2 regularization parameters. These configurations were systematically explored to find the optimal settings for our models.\n\nFor the random survival forest (RSF) models, we specified the number of trees and the number of variables considered for splitting at each node. The exact values and the rationale behind these choices are clearly outlined.\n\nThe optimization schedule, including the use of techniques like dropout, L2 regularization, and the Adam optimizer, is also described. We mentioned the number of Monte Carlo simulations performed and the data splitting strategy used for training and testing.\n\nRegarding model files and optimization parameters, while the specific files are not directly provided in the text, the methods and configurations are fully disclosed. This allows for reproducibility of the experiments. The publication does not explicitly mention the availability of model files or their licensing details, but the detailed descriptions provided should enable other researchers to implement and validate the models independently.\n\nIn summary, the hyper-parameter configurations, optimization schedule, and model parameters are all reported in sufficient detail to ensure transparency and reproducibility. However, specific model files and their licensing information are not addressed.",
  "model/interpretability": "The models discussed in this publication include Cox proportional hazards models, random survival forests, and deep neural networks for survival analysis. These models vary in their interpretability.\n\nCox proportional hazards models, including those with restricted cubic splines (Cox_RCS), are generally more interpretable than black-box models. They provide hazard ratios that can be easily understood and interpreted in a clinical context. For example, a hazard ratio greater than 1 indicates an increased risk, while a hazard ratio less than 1 indicates a decreased risk. This transparency allows clinicians to understand the impact of each covariate on the survival time.\n\nRandom survival forests, while powerful for capturing complex interactions, are less interpretable. They are considered black-box models because they do not provide a straightforward way to interpret the contribution of each variable to the prediction. However, variable importance measures can be derived from the forest, indicating which variables are most influential in the model.\n\nDeep neural networks, such as DeepSurv, are also black-box models. They consist of multiple layers of nonlinear transformations, making it difficult to interpret how the model arrives at its predictions. However, techniques like SHAP (SHapley Additive exPlanations) values can be used to provide some interpretability by showing the contribution of each feature to the prediction for individual cases.\n\nIn summary, while Cox models offer clear interpretability through hazard ratios, random survival forests and deep neural networks are more opaque. Techniques like variable importance and SHAP values can help in understanding these black-box models to some extent.",
  "model/output": "The model discussed in this publication is primarily focused on survival analysis, which is a type of regression problem. Specifically, it involves predicting the time to an event, such as death or disease recurrence, based on various covariates. The models used include Cox proportional hazards models, random survival forests (RSF), and deep survival neural networks (Cox_DNN or DeepSurv). These models are designed to handle time-to-event data and provide insights into how different variables influence the survival time of individuals.\n\nThe output of these models typically includes several key metrics and visualizations. The concordance index (C-index) is used to evaluate the predictive discrimination of the models, with higher values indicating better performance. The integrated Brier score (IBS) is another metric used to assess the predictive accuracy, where lower values indicate better calibration. Survival curves, such as those generated for the WHAS 500 dataset, provide a visual representation of the survival probabilities over time for different strata of the data.\n\nIn addition to these metrics, the models can output variable importance rankings, especially in the case of random survival forests. This helps in identifying which covariates have the most significant impact on survival outcomes. For the Cox models, the output includes regression coefficients that indicate the direction and strength of the relationship between each covariate and the hazard of the event occurring.\n\nThe models also provide insights into the stability and robustness of their predictions. For instance, the Cox_DNN model, while capable of handling large datasets and complex relationships, may exhibit instability with a high number of small outliers. In contrast, the Cox_RCS model generally shows better predictive performance and calibration, particularly in datasets with non-linear relationships.\n\nOverall, the outputs of these models are designed to provide a comprehensive understanding of the factors influencing survival times and to offer reliable predictions for clinical and research applications.",
  "model/duration": "The execution time of the models varied depending on the specific algorithm and the parameters used. For the random survival forest (RSF) model, the number of trees significantly influenced the runtime. It was observed that the model's performance and error rate stabilized when the number of trees exceeded 500. Therefore, in all simulations, the RSF model was set to use 500 trees. However, for more stable results, 1500 trees were used in some cases, which naturally increased the execution time.\n\nThe neural network model, specifically the Cox_DNN (DeepSurv), had its parameters fine-tuned through 1000 iterations of comparison. The optimal settings included 20 hidden neurons, 3 hidden layers, and the tanh activation function. The learning rate was set to 0.001. These settings were chosen to balance model performance and computational efficiency.\n\nIn the simulations, the total sample sizes were set to 200, 500, and 1000, with censoring rates of 10%, 20%, 40%, and 60%. For each simulation, 70% of the data was used for training, and 30% was reserved for testing. This split ensured that the models were evaluated on unseen data, providing a more reliable assessment of their predictive performance.\n\nThe overall execution time for the simulations was influenced by the complexity of the models and the size of the datasets. The use of dropout and L2 regularization in the neural network model helped to prevent overfitting, but these techniques also added to the computational load. The adaptive momentum (Adam) optimizer was used to efficiently update the model parameters during training.\n\nIn summary, the execution time for the models was managed by carefully selecting the number of trees in the RSF model and optimizing the parameters of the neural network model. The simulations were designed to provide a comprehensive evaluation of the models' predictive performance while considering computational efficiency.",
  "model/availability": "The source code for the algorithms used in this study is not publicly released. However, the implementations rely on established software packages and libraries.\n\nFor the Cox regression and restricted cubic spline Cox regression, the R software packages \"survival\" and \"rms\" were used, respectively. These packages are widely available and can be installed from the Comprehensive R Archive Network (CRAN).\n\nThe random survival forests were implemented using the \"randomForestSRC\" package in R, which is also available on CRAN.\n\nThe deep learning approach, DeepSurv, was implemented using the Pysurvival library in Python. This library can be installed via the Python Package Index (PyPI).\n\nWhile the specific code used in this study is not available, the methods and packages used are well-documented and can be accessed by researchers interested in replicating or building upon this work.",
  "evaluation/method": "In the evaluation of our models, we employed two primary metrics: the C-index and the Integrated Brier Score (IBS). The C-index, which ranges from 0.5 to 1, was used as the main evaluation metric to assess the predictive discriminatory power of the models. A higher C-index indicates better model performance, with a value of 0.5 suggesting random prediction. The IBS, on the other hand, was used as a secondary metric to evaluate the predictive calibration of the models.\n\nTo ensure robust evaluation, we conducted experiments using the WHAS dataset, which consists of 500 patients who experienced acute myocardial infarction. This dataset is characterized by right-censored survival data. We split the dataset into training and testing sets, with 70% of the data used for training and the remaining 30% for testing. This process was repeated 1000 times to obtain reliable estimates of the model performance metrics.\n\nFor the Cox_RCS model, we found that it generally exhibited high predictive discriminatory power, particularly when the sample size was small to moderate. However, its performance became less stable as the sample size increased. The Cox_DNN model showed promising results, especially when the sample size was large (greater than or equal to 500) and the censoring rate was low (less than 40%). Its C-index values were consistently high, indicating good discriminatory power. However, the Cox_DNN model's performance was sensitive to the censoring rate and sample size, with its predictive calibration improving as the sample size increased and the censoring rate decreased.\n\nThe Cox model and the Random Survival Forest (RSF) model demonstrated weaker predictive performance compared to the Cox_RCS and Cox_DNN models. The Cox model had the lowest predictive calibration, while the RSF model performed better than the Cox model but worse than the other two models in terms of both discriminatory power and calibration.\n\nIn summary, our evaluation methodology involved using the C-index and IBS to assess the predictive performance of different models on the WHAS dataset. The Cox_RCS and Cox_DNN models showed superior performance in various scenarios, with the Cox_DNN model being particularly effective for large sample sizes and low censoring rates. The Cox and RSF models, while useful, did not perform as well as the other two models in our evaluation.",
  "evaluation/measure": "In the evaluation of our models, we primarily focus on two key performance metrics: the C-index and the Integrated Brier Score (IBS). The C-index, also known as the concordance index, is our main evaluation metric. It assesses the model's predictive discrimination, which is crucial for distinguishing between different risk groups. A higher C-index indicates better predictive accuracy, with a value of 0.5 suggesting random prediction and values closer to 1 indicating higher accuracy. This metric is particularly useful for survival analysis models like Cox regression.\n\nThe IBS, on the other hand, serves as a secondary metric to evaluate the model's predictive calibration. It measures the average squared difference between the observed survival status and the predicted survival probabilities. While the C-index focuses on the model's ability to differentiate between high-risk and low-risk individuals, the IBS provides insight into how well the predicted probabilities align with the actual outcomes.\n\nThese metrics are widely used in the literature for evaluating survival models, making our choice representative of standard practices in the field. The C-index is especially valuable for its interpretability and direct relevance to clinical decision-making, where distinguishing between risk groups is essential. The IBS complements the C-index by ensuring that the model's predictions are not only discriminative but also well-calibrated, providing a more comprehensive assessment of model performance.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we conducted a thorough evaluation of the predictive capabilities of two extended Cox models: the restricted cubic spline Cox model (Cox_RCS) and the DeepSurv neural network Cox model (Cox_DNN). This comparison was performed using both Monte Carlo simulations and empirical studies, with the conventional Cox Proportional Hazards model and Random Survival Forests serving as reference models.\n\nThe evaluation included benchmark datasets, specifically the WHAS 500 dataset, which consists of 500 patients who experienced acute myocardial infarction. This dataset is a right-censored survival dataset, making it a suitable benchmark for comparing survival analysis methods.\n\nTo ensure a fair comparison, we used established evaluation metrics. The concordance index (C-index) was employed to assess the differentiation of the prediction results, where a higher C-index indicates better predictive performance. The integrated Brier Score (IBS) was used to evaluate the calibration of the predictions, with a lower IBS signifying better predictive accuracy.\n\nThe comparison to simpler baselines, such as the traditional Cox model and Random Survival Forests, provided a baseline for understanding the performance improvements offered by the more complex models. This approach allowed us to demonstrate that while machine learning and deep learning methods can offer advantages in certain scenarios, traditional methods like Cox_RCS are not inferior and can be highly effective under specific conditions.\n\nOverall, the methods comparison subsection highlights the strengths and weaknesses of different survival analysis techniques, providing insights into when and how to apply each method based on the characteristics of the data.",
  "evaluation/confidence": "In the evaluation of our models, we employed performance metrics such as the C-index and Integrated Brier Score (IBS) to assess predictive discrimination and calibration, respectively. To ensure the robustness of our findings, we conducted extensive simulations and repeated our experiments multiple times.\n\nFor the C-index, which measures the model's ability to discriminate between different risk groups, we observed that the Cox_RCS model generally outperformed other models, particularly when the sample size was smaller. However, as the sample size increased, the Cox_DNN model's performance improved significantly, often achieving the highest C-index values, especially when the sample size was 500 or more. This indicates that the Cox_DNN model's predictive discrimination is highly dependent on the sample size.\n\nTo assess the statistical significance of our results, we performed repeated sampling and model training. Specifically, we randomly selected 70% of the dataset for training and the remaining 30% for testing, repeating this process 1000 times. This approach allowed us to generate confidence intervals for our performance metrics and to evaluate the stability and reliability of our models.\n\nThe results showed that the Cox_DNN model's performance was sensitive to both the sample size and the censoring rate. When the sample size was large (500 or more) and the censoring rate was low (less than 40%), the Cox_DNN model demonstrated superior predictive calibration. In contrast, the Cox_RCS model exhibited high predictive calibration across various sample sizes and censoring rates, but its discrimination performance was less stable.\n\nOverall, our evaluation suggests that the Cox_DNN model can achieve high predictive performance, particularly with large sample sizes and low censoring rates. However, its sensitivity to these factors highlights the importance of careful consideration when applying this model to different datasets. The Cox_RCS model, while generally reliable in terms of calibration, may require additional validation to ensure consistent discrimination performance.",
  "evaluation/availability": "Not enough information is available."
}