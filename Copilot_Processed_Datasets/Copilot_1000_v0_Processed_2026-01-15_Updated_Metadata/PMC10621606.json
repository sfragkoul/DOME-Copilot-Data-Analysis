{
  "publication/title": "Identifying Cardiovascular Disease Risk Factors in Adults with Explainable Artificial Intelligence.",
  "publication/authors": "K\u0131rbo\u011fa KK, K\u00fc\u00e7\u00fcksille EU",
  "publication/journal": "Anatolian journal of cardiology",
  "publication/year": "2023",
  "publication/pmid": "37624075",
  "publication/pmcid": "PMC10621606",
  "publication/doi": "10.14744/anatoljcardiol.2023.3214",
  "publication/tags": "- Cardiovascular disease\n- Explainable artificial intelligence\n- Machine learning\n- Prediction\n- Risk factors\n- Cardiovascular Disease Risk Factors\n- Gradient Boosting\n- Shapley Additive Explanations\n- Permutation Feature Importance\n- Cardiovascular Disease Prediction",
  "dataset/provenance": "The dataset used in this study was obtained from Kaggle by searching \"Exploring Risk Factors for Cardiovascular Disease in Adults\". Additionally, the dataset is available at https://data.world/kudem. This dataset contains detailed information about risk factors for cardiovascular disease (CVD) from 70,000 individuals. The data includes various clinical variables such as age, gender, height, weight, blood pressure values, cholesterol and glucose levels, smoking habits, physical activity, and alcohol consumption. It also includes information on whether each individual has CVD, which is crucial for the study's objectives. This dataset has been used previously by the community and is considered a valuable resource for researchers integrating machine learning, deep learning, and artificial intelligence techniques to explore relationships between CVD risk factors and outcomes. The dataset's comprehensiveness and the availability of CVD status for each individual make it suitable for developing predictive models and understanding the interactions between risk factors and CVD.",
  "dataset/splits": "The dataset was divided into three splits: calibration, validation, and test sets. The calibration set was further divided into training and validation subsets. The data was randomly split, with 80% allocated to the calibration set and 20% to the test set. This means that the training and validation subsets together comprised 80% of the data, while the test set contained the remaining 20%. The exact number of data points in each split can be calculated based on the total dataset size of 70,000 patient records. Therefore, approximately 56,000 data points were used for calibration (training and validation), and around 14,000 data points were reserved for testing. The validation and training sets were utilized in the model-building process to optimize parameters and initially assess model performance. The test set was employed to evaluate the advanced prediction capabilities and generalization performance of the optimized models.",
  "dataset/redundancy": "The dataset used in this study was divided into training, validation, and test sets to ensure robust model development and evaluation. The datasets were randomly split, with 80% allocated for calibration, which includes both training and validation, and 20% reserved for testing. This split ensures that the training and test sets are independent, preventing data leakage and overfitting.\n\nThe calibration set was further divided into training and validation subsets. The training set was used to build the models and find the optimal combination of parameters. The validation set was employed to make an initial assessment of the model\u2019s performance and to tune hyperparameters. This process helps in selecting the best model configuration before evaluating it on the unseen test set.\n\nThe test set, comprising 20% of the data, was used to evaluate the advanced prediction capabilities and generalization performance of the optimized models. This independent test set provides an unbiased estimate of the model's performance, ensuring that the results are reliable and generalizable to new, unseen data.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets for cardiovascular disease risk factors. The dataset includes a comprehensive set of clinical information on 70,000 patients, covering various risk factors such as age, gender, height, weight, blood pressure values, cholesterol and glucose levels, smoking habits, physical activity, and alcohol consumption. This rich dataset allows for a thorough exploration of potential relationships between CVD risk factors and outcomes, leading to a better understanding of CVD risk factors and the design of more effective preventive measures.",
  "dataset/availability": "The dataset used in this study is publicly available and can be accessed through two platforms. It can be found by searching \"Exploring Risk Factors for Cardiovascular Disease in Adults\" on Kaggle. Additionally, the dataset is available at the URL https://data.world/kudem. This dataset includes detailed information about risk factors for cardiovascular disease (CVD), such as age, gender, height, weight, blood pressure values, cholesterol and glucose levels, smoking habits, physical activity, and alcohol consumption. It also contains information on whether individuals have CVD, which is crucial for the study's objectives. The dataset comprises 70,000 patient records, providing a comprehensive resource for researchers to explore potential relationships between CVD risk factors and the disease itself. This availability supports the reproducibility and verification of the study's findings by other researchers.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is ensemble learning, specifically gradient boosting. We employed the Extreme Gradient Boost (XGBoost) Classifier, which is a well-established and widely used algorithm in the machine learning community. XGBoost is known for its efficiency and effectiveness in handling structured/tabular data, making it a popular choice for various predictive modeling tasks, including those in healthcare.\n\nThe XGBoost algorithm is not new; it has been extensively studied and applied in numerous domains. Its development and initial publications were focused on its practical applications and improvements over existing gradient boosting methods. The algorithm's success in various competitions and real-world applications has led to its widespread adoption. Therefore, it was published in journals and conferences related to machine learning and data science, rather than being introduced in a standalone machine-learning journal.\n\nIn our work, we utilized XGBoost alongside other machine-learning algorithms to develop predictive models for cardiovascular disease risk factors. The choice of XGBoost was driven by its proven performance in similar tasks and its ability to handle complex interactions within the data. The algorithm's interpretability, when combined with explainable AI techniques like Shapley additive explanations (SHAP), allowed us to not only make accurate predictions but also to understand the underlying factors contributing to the outcomes. This interpretability is crucial in healthcare, where transparency and explainability are essential for clinical decision-making.",
  "optimization/meta": "The model developed in this study does not function as a meta-predictor. Instead, it employs a single machine learning algorithm, specifically the Extreme Gradient Boost (XGBoost) Classifier, which was identified as the best-performing model among several evaluated algorithms. The XGBoost model was selected based on its superior accuracy, area under the curve (AUC), and other performance metrics.\n\nThe study utilized seven different machine learning algorithms to develop predictive models for cardiovascular disease (CVD) risk factors. These algorithms included Random Forest Classifier, XGBoost Classifier, Decision Tree Classifier, KNeighbors Classifier, Support Vector Machine (SVM) Classifier, Gaussian Naive Bayes (GaussianNB), and Logistic Regression. Each of these models was evaluated using criteria such as precision, recall, F1-score, mean accuracy, AUC, and Brier scores.\n\nThe XGBoost model demonstrated the highest performance with an accuracy of 0.739, an AUC of 0.803, and a Brier score of 0.260. This model was chosen as the best predictor for CVD risk factors due to its superior performance metrics. The training data used for the XGBoost model was independently divided into 80% for calibration (training and validation) and 20% for testing, ensuring that the training data was independent and not reused for validation or testing purposes.\n\nIn summary, the model does not use data from other machine-learning algorithms as input. It relies solely on the XGBoost Classifier, which was selected after evaluating multiple algorithms. The training data for the XGBoost model was independently divided to ensure unbiased performance evaluation.",
  "optimization/encoding": "In our study, data preprocessing was a crucial step to ensure the machine-learning algorithms performed optimally. The dataset included various features such as age, gender, height, weight, blood pressure values, cholesterol and glucose levels, smoking habits, physical activity, and alcohol consumption. These features were encoded and pre-processed as follows:\n\nNumerical features like age, height, weight, blood pressure, cholesterol, and glucose levels were scaled to ensure they contributed equally to the model's performance. This scaling was essential to prevent features with larger values from dominating the model.\n\nCategorical features such as gender, smoking habits, physical activity, and alcohol consumption were encoded using binary values. For instance, gender was encoded as 0 or 1, smoking and physical activity were encoded as 0 (no) and 1 (yes), and alcohol consumption was encoded as 0 (no) and 1 (yes). This binary encoding allowed the machine-learning algorithms to interpret these categorical variables effectively.\n\nCholesterol and glucose levels were categorized into three groups: normal, above normal, and significantly above normal. These categories were encoded as 1, 2, and 3, respectively. This encoding helped the models understand the different levels of these risk factors.\n\nNo common data preprocessing was applied to the entire dataset. Instead, the dataset was divided into training, validation, and test sets. The training and validation sets were used to find and correct the best combination of parameters, while the test set was used to evaluate the model's generalization performance. This approach ensured that the models were trained and validated on the same distribution of data, reducing the risk of overfitting.\n\nBy carefully encoding and preprocessing the data, we were able to develop machine-learning models that accurately predicted cardiovascular disease risk factors and provided interpretable results.",
  "optimization/parameters": "In our study, we utilized a dataset containing detailed information about 11 risk factors for cardiovascular disease (CVD). These risk factors included age, gender, height, weight, blood pressure values (both systolic and diastolic), cholesterol and glucose levels, smoking habits, physical activity, and alcohol consumption. The dataset comprised information from 70,000 individuals, making it a comprehensive resource for our analysis.\n\nThe selection of these parameters was based on their established association with CVD in the literature. We aimed to include a broad range of factors that are known to influence cardiovascular health. The dataset was obtained from multiple databases and was curated to ensure that it included risk factors highly associated with CVD.\n\nTo develop our predictive models, we employed seven different machine learning algorithms: Random Forest Classifier, Extreme Gradient Boost (XGBoost) Classifier, Decision Tree Classifier, KNeighbors Classifier, Support Vector Machine (SVM) Classifier, Gaussian Naive Bayes (GaussianNB), and Logistic Regression. Each of these algorithms was evaluated using criteria such as precision, recall, F1-score, mean accuracy, area under the curve (AUC), and Brier scores. The datasets were randomly divided into 80% for calibration (training and validation) and 20% for testing. This division allowed us to find and correct the best combination of parameters and to make an initial assessment of the model\u2019s performance.\n\nAmong the models, the XGBoost classifier emerged as the best-performing model, with an accuracy of 0.739, an AUC of 0.803, and a Brier score of 0.260. The feature importance was determined using the permutation feature importance (PFI) method and Shapley additive explanations (SHAP) method. Systolic blood pressure, cholesterol, and age were identified as the most critical factors driving the model\u2019s performance. These findings align with existing literature on CVD risk factors, reinforcing the validity of our model.",
  "optimization/features": "In our study, we utilized a dataset containing detailed information about various risk factors for cardiovascular disease (CVD). The dataset includes features such as age, gender, height, weight, blood pressure values (both systolic and diastolic), cholesterol and glucose levels, smoking habits, physical activity, and alcohol consumption. These features were selected based on their known associations with CVD risk factors.\n\nFeature selection was performed to identify the most significant predictors of CVD. We employed two primary methods for this purpose: permutation feature importance (PFI) and Shapley additive explanations (SHAP). The PFI method helped us understand which features significantly impact the model's performance by measuring the decline in model accuracy when individual feature values are permuted. This method was applied to both the training and test datasets to ensure robustness.\n\nThe SHAP method, on the other hand, provided a more detailed interpretation of feature importance by calculating the contribution of each feature to the model's predictions. This method was particularly useful for understanding the interactions between different risk factors and their collective impact on CVD prediction.\n\nBoth methods highlighted that systolic blood pressure, cholesterol levels, and age were the most critical factors influencing CVD risk. These findings were consistent across different models and validation techniques, reinforcing the importance of these features in predicting cardiovascular outcomes.\n\nIn summary, our study used a comprehensive set of features related to CVD risk factors. Feature selection was conducted using both PFI and SHAP methods, ensuring that the most relevant features were identified and utilized in our predictive models. This approach helped us develop accurate and interpretable models for CVD prediction.",
  "optimization/fitting": "The study utilized a dataset comprising 70,000 patient records, which included clinical information on cardiovascular disease (CVD) risk factors and outcomes. This dataset was divided into 80% for calibration (training and validation) and 20% for testing. The calibration set was further split into training and validation subsets to find and correct the best combination of parameters and to make an initial assessment of the model\u2019s performance.\n\nTo address the potential issue of overfitting, several techniques were employed. The models were evaluated using multiple criteria, including precision, recall, F1-score, mean accuracy, area under the curve (AUC), and Brier scores. These metrics were calculated for training, validation, and test sets to ensure that the models generalized well to unseen data. Additionally, the discriminative power was evaluated using receiver operating characteristic (ROC) curve analysis, which helps in assessing the model's ability to distinguish between different classes.\n\nThe study used seven machine learning models, including Random Forest Classifier, Extreme Gradient Boost (XGBoost) Classifier, Decision Tree Classifier, KNeighbors Classifier, Support Vector Machine (SVM) Classifier, Gaussian Naive Bayes (GaussianNB), and Logistic Regression. The XGBoost model was identified as the best-performing model with an AUC of 0.803, indicating strong discriminative power.\n\nTo rule out underfitting, the models were trained on a sufficiently large and diverse dataset, and their performance was rigorously evaluated using multiple metrics. The use of validation and test sets ensured that the models were not too simplistic to capture the underlying patterns in the data. The XGBoost model, in particular, demonstrated high accuracy and prediction values, suggesting that it effectively captured the complexities of the data without being overly simplistic.\n\nIn summary, the study employed a comprehensive approach to model development and evaluation, using multiple metrics and datasets to ensure that the models were neither overfitted nor underfitted. The XGBoost model was identified as the best-performing model, with strong discriminative power and generalization capabilities.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the primary methods used was the division of our dataset into training, validation, and test sets. The dataset was randomly split into 80% for calibration (which included both training and validation) and 20% for testing. This approach allowed us to train our models on a subset of the data, validate their performance on a separate subset to tune hyperparameters, and finally evaluate their generalization capability on an unseen test set.\n\nAdditionally, we utilized cross-validation techniques during the model-building process. Cross-validation helps in assessing how the statistical analysis will generalize to an independent data set. It is a robust technique to prevent overfitting by ensuring that the model is not too complex and can generalize well to new, unseen data.\n\nWe also employed regularization techniques inherent to some of our machine learning algorithms. For instance, the Extreme Gradient Boosting (XGBoost) classifier, which was identified as the best-performing model, includes built-in regularization parameters such as L1 and L2 regularization. These parameters help in penalizing complex models and thus prevent overfitting by keeping the model simple and generalizable.\n\nFurthermore, we evaluated the models using multiple criteria, including precision, recall, F1-score, mean accuracy, area under the curve (AUC), and Brier scores. These metrics provided a comprehensive assessment of the models' performance, ensuring that they were not only accurate but also reliable and generalizable.\n\nIn summary, our approach to preventing overfitting involved a combination of dataset splitting, cross-validation, and the use of regularization techniques within our machine learning algorithms. These methods collectively ensured that our models were robust, generalizable, and not prone to overfitting.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model developed in this study is designed to be transparent and explainable, rather than a black box. To achieve this, we employed several techniques to ensure that the relationships between the input features and the model's predictions are clear and interpretable.\n\nOne of the key methods used is the Shapley Additive Explanations (SHAP) approach. SHAP values provide a way to attribute the contribution of each feature to the model's output, making it possible to understand how individual risk factors influence the prediction of cardiovascular disease. This method allows us to generate force plots and other visualizations that show the impact of each feature on the model's predictions, providing a clear and intuitive understanding of the model's behavior.\n\nAdditionally, we used the permutation feature importance method. This technique measures the decline in model performance when the values of a single feature are randomly shuffled, indicating how dependent the model is on that feature. By using this method, we can identify which features are most important for the model's predictions and understand their relative significance.\n\nThe Extreme Gradient Boosting model, which was found to be the best prediction model for cardiovascular disease risk factors, was also designed with transparency in mind. This model's structure allows for the interpretation of feature importance and interactions, further enhancing its explainability.\n\nOverall, the combination of SHAP values, permutation feature importance, and the inherent transparency of the Extreme Gradient Boosting model ensures that our predictions are not only accurate but also interpretable. This transparency is crucial for clinical applications, as it allows healthcare professionals to understand the underlying factors contributing to the predictions and make informed decisions based on the model's outputs.",
  "model/output": "The model developed in our study is a classification model. It is designed to predict whether patients have cardiovascular disease (CVD) or not, based on various risk factors. We utilized several machine learning algorithms, including Random Forest Classifier, Extreme Gradient Boost (XGBoost) Classifier, Decision Tree Classifier, KNeighbors Classifier, Support Vector Machine (SVM) Classifier, Gaussian Naive Bayes (GaussianNB), and Logistic Regression. Among these, the XGBoost Classifier was identified as the best-performing model, with an accuracy of 0.739 and an area under the curve (AUC) of 0.803. The model's performance was evaluated using metrics such as precision, recall, F1-score, mean accuracy, AUC, and Brier scores. The XGBoost model demonstrated superior predictive capabilities and generalization performance, making it the most effective for classifying patients with and without CVD.\n\nThe model's output provides insights into the importance of various risk factors in predicting CVD. Through techniques like Shapley Additive Explanations (SHAP) and permutation feature importance, we identified systolic blood pressure, cholesterol, and age as the most critical factors influencing the model's predictions. These findings are crucial for clinicians, as they help in understanding the key risk factors associated with CVD and can guide early diagnosis and treatment strategies. The model's explainable nature ensures transparency, allowing healthcare professionals to interpret the results and make informed decisions.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in our study was comprehensive and multifaceted, ensuring the robustness and reliability of our models. We utilized several criteria to assess the statistical significance of all classifier algorithms, including precision, recall, F1-score, mean accuracy, area under the curve (AUC), and Brier scores. These metrics were evaluated across training, validation, and test sets to provide a thorough assessment of model performance.\n\nThe datasets were randomly divided into 80% for calibration (training and validation) and 20% for testing. The calibration sets were crucial in the model-building process, allowing us to find and correct the best combination of parameters and make an initial assessment of the model\u2019s performance. The discriminative power of the models was evaluated using receiver operating characteristic (ROC) curve analysis. Model accuracy scores, which measure the discrepancy between the projected probability of the models and the actual observed outcomes, were used to determine overall model accuracy.\n\nThe advanced prediction capabilities and generalization performance of the optimized models were evaluated using the test sets. We developed predictive models using seven machine learning algorithms: Random Forest Classifier, Extreme Gradient Boost (XGBoost) Classifier, Decision Tree Classifier, KNeighbors Classifier, SVM Classifier, GaussianNB, and Logistic Regression. Each model was rigorously evaluated to identify the best-performing algorithm.\n\nThe XGBoost model emerged as the top performer, with an accuracy of 0.739 and a prediction value of 0.72. It was selected as the best model with an AUC value of 0.80. The evaluation process involved comparing the models based on AUC, Brier score, and mean accuracy. This comprehensive evaluation ensured that our chosen model was not only accurate but also reliable and generalizable to new data.",
  "evaluation/measure": "In our study, we evaluated the performance of our machine learning models using a comprehensive set of metrics to ensure a thorough assessment. The primary metrics reported include precision, recall, F1-score, mean accuracy, area under the curve (AUC), and Brier scores. These metrics were calculated for training, validation, and test sets to provide a robust evaluation of model performance.\n\nPrecision measures the accuracy of positive predictions, recall assesses the ability to identify all relevant instances, and the F1-score balances these two metrics. Mean accuracy provides an overall measure of correct predictions, while AUC evaluates the model's ability to distinguish between classes. Brier scores quantify the accuracy of probabilistic predictions, with lower scores indicating better performance.\n\nThese metrics are widely recognized and used in the literature, making our evaluation representative and comparable to other studies in the field. The use of multiple metrics ensures that we capture different aspects of model performance, providing a holistic view of our models' strengths and weaknesses. This approach aligns with best practices in machine learning evaluation, ensuring that our findings are reliable and meaningful.",
  "evaluation/comparison": "In our study, we employed multiple machine learning algorithms to develop predictive models for cardiovascular disease (CVD) risk factors. The algorithms included Random Forest Classifier, Extreme Gradient Boost (XGBoost) Classifier, Decision Tree Classifier, KNeighbors Classifier, Support Vector Machine (SVM) Classifier, Gaussian Naive Bayes (GaussianNB), and Logistic Regression. Each of these models was evaluated using several criteria, including precision, recall, F1-score, mean accuracy, area under the curve (AUC), and Brier scores.\n\nTo ensure the robustness of our models, we divided the dataset into 80% for calibration (training and validation) and 20% for testing. The calibration set was used to find and correct the best combination of parameters and to make an initial assessment of the model\u2019s performance. The test set was then used to evaluate the advanced prediction capabilities and generalization performance of the optimized models.\n\nAmong the seven models, the XGBoost model emerged as the best performer with an accuracy of 0.739, a prediction value of 0.72, and an AUC of 0.803. This model was chosen based on its superior performance metrics, including AUC, Brier score, and mean accuracy.\n\nIn addition to comparing different machine learning models, we also utilized explainable artificial intelligence (XAI) methods to interpret the results. Specifically, we used Shapley Additive Explanations (SHAP) to identify the most significant risk factors influencing the predictions. The SHAP method provided a clear interpretation of how each feature contributed to the model's predictions, enhancing the transparency and interpretability of our results.\n\nFurthermore, we compared the feature importance determined by the permutation feature importance (PFI) method with the results obtained from SHAP. Both methods highlighted systolic blood pressure, cholesterol, and age as the most critical factors in predicting CVD. This consistency across different methods strengthened our confidence in the identified risk factors.\n\nNot applicable.",
  "evaluation/confidence": "The evaluation of our models involved several performance metrics, including precision, recall, F1-score, mean accuracy, area under the curve (AUC), and Brier scores. These metrics were calculated for training, validation, and test sets to ensure robust evaluation. The statistical significance of all classifier algorithms was assessed using these criteria.\n\nThe models were evaluated on a test set that comprised 20% of the total dataset, which was randomly divided. This division helped in assessing the generalization performance of the models. The discriminative power was evaluated using receiver operating characteristic (ROC) curve analysis, which is a standard method for evaluating the performance of classification models.\n\nThe XGBoost model, which was identified as the best-performing model, had an AUC of 0.80, indicating strong discriminative power. The precision, recall, and F1-score for this model were also competitive, with values of 0.72, 0.78, and 0.75, respectively. These metrics, along with the mean accuracy of 0.739, provide a comprehensive view of the model's performance.\n\nTo ensure the reliability of our results, we compared the performance of the XGBoost model with other models, such as Random Forest, Decision Tree, KNeighbors, SVM, GaussianNB, and Logistic Regression. The XGBoost model consistently outperformed these models across multiple metrics, including AUC, Brier score, and mean accuracy.\n\nThe statistical significance of the results was further supported by the use of permutation feature importance (PFI) and Shapley additive explanations (SHAP) methods. These methods helped in identifying the most critical risk factors, such as systolic blood pressure, cholesterol, and age, which were found to be significant predictors of cardiovascular diseases. The SHAP values provided a clear interpretation of the model's predictions, enhancing the transparency and explainability of the results.\n\nIn summary, the performance metrics used in our study are robust and statistically significant. The XGBoost model demonstrated superior performance compared to other models, and the results were supported by rigorous evaluation methods. The use of confidence intervals and statistical tests ensured that the claims of superiority are well-founded.",
  "evaluation/availability": "Not enough information is available."
}