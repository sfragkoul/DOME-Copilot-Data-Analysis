{
  "publication/title": "A machine learning approach for classifying and quantifying acoustic diversity.",
  "publication/authors": "Keen SC, Odom KJ, Webster MS, Kohn GM, Wright TF, Araya-Salas M",
  "publication/journal": "Methods in ecology and evolution",
  "publication/year": "2021",
  "publication/pmid": "34888025",
  "publication/pmcid": "PMC8654111",
  "publication/doi": "10.1111/2041-210x.13599",
  "publication/tags": "- Acoustic diversity\n- Animal vocalizations\n- Machine learning\n- Unsupervised learning\n- Random forest\n- Acoustic classification\n- Bioacoustics\n- Birdsong analysis\n- Signal processing\n- Repertoire size estimation",
  "dataset/provenance": "The datasets used in our study were sourced from both field and laboratory recordings, as well as synthetic data. Field recordings of long-billed hermit songs were collected from 43 known individuals in wild populations at La Selva Biological Station, Costa Rica, between 2008 and 2017. These recordings were from 16 leks, with a mean of 3.1 songs per group. We compiled a sample of 50 unique song types, selecting the 10 recordings with the highest signal-to-noise ratio for each type, resulting in a dataset of 500 signals.\n\nLaboratory recordings of budgerigar contact calls were collected between July and November 2010 from a laboratory population. These calls were recorded from 38 different individuals, with each individual typically having repertoires of 2\u20135 acoustically distinct contact call types. The calls were classified using spectrograms and verified through discriminant function analysis, resulting in a dataset of 15 unique elements.\n\nIn addition to these natural recordings, we also used synthetic datasets modeled on the natural vocalizations of long-billed hermits and budgerigars. These synthetic datasets included a variety of conditions, such as different repertoire sizes and signal properties, allowing us to conduct repeated tests of algorithm performance under various conditions. The synthetic datasets consisted of 48 datasets each for synthetic long-billed hermit songs and synthetic budgerigar calls, with repertoire sizes ranging from 5 to 100 unique elements.\n\nThe use of both natural and synthetic datasets enabled us to assess the performance of our method using vocal signals collected from live birds, as well as signals with distinct spectrotemporal properties. The availability of large datasets of live recordings from numerous individuals of each species, previously labeled by human experts, provided ground truth for testing our proposed method. The synthetic datasets allowed for thorough assessment and demonstrated that our approach can accurately approximate human analysis on diverse datasets.",
  "dataset/splits": "We utilized four primary datasets to evaluate our method's performance. The first dataset consisted of field recordings of long-billed hermit songs, comprising 50 unique song types with 10 recordings per type, resulting in a total of 500 signals. The second dataset included laboratory recordings of budgerigar contact calls, with 15 unique call types and 35 recordings per type, totaling 525 signals.\n\nAdditionally, we created two collections of synthetic datasets modeled on the natural vocalizations of these species. Each synthetic dataset contained repertoire sizes of 5, 10, 15, 20, 50, or 100 unique elements, with 10 examples per element type. For each repertoire size, we varied three features: duration (short or long), harmonic content (low or high), and background noise (low or high). This resulted in 48 synthetic datasets for both long-billed hermit songs and budgerigar calls, each containing 800 signals (8 repertoire sizes \u00d7 10 examples per element).\n\nThe synthetic datasets allowed us to conduct repeated tests under different conditions and to assess whether repertoire size can be approximated using acoustic area. The use of both live and synthetic datasets enabled a thorough evaluation of our method's performance across diverse vocal signals.",
  "dataset/redundancy": "The datasets used in our study were split into four main categories: annotated field recordings of long-billed hermits, annotated lab recordings of budgerigars, and two collections of synthetic datasets modeled on natural vocalizations of these two species. The field recordings of long-billed hermits were collected from 43 known individuals in wild populations, with songs recorded from 16 leks. To ensure diversity, we used songs recorded from the same lek in different years to compile a sample of 50 unique song types. For each song type, we selected the 10 recordings with the highest signal-to-noise ratio, resulting in a dataset of 500 signals.\n\nThe laboratory recordings of budgerigar contact calls were collected from 38 different individuals. These calls were recorded in an acoustic chamber and classified by trained research assistants using spectrograms. The classification was verified using a discriminant function analysis. We then randomly selected a subset of these calls to create our test dataset.\n\nThe synthetic datasets were created to model the natural vocalizations of long-billed hermits and budgerigars. These datasets included a range of repertoire sizes, from 5 to 100 unique elements, allowing us to test the performance of our method under various conditions. The synthetic datasets were used to conduct repeated tests of algorithm performance and to assess whether repertoire size could be approximated using acoustic area.\n\nThe training and test sets were independent, with the synthetic datasets providing a controlled environment to evaluate performance without the variability present in field or lab recordings. The use of synthetic datasets also allowed for a thorough assessment of algorithm performance under different conditions, which would not be feasible with field or lab recordings due to the time-consuming process of collecting, annotating, and measuring vocal repertoires.\n\nThe distribution of our datasets compares favorably to previously published machine learning datasets in the field of bioacoustics. By including both natural and synthetic data, we ensured a comprehensive evaluation of our method's performance. The synthetic datasets, in particular, provided a unique advantage by allowing us to control for variables such as repertoire size and signal characteristics, which is not possible with natural recordings. This approach enabled us to demonstrate the robustness and accuracy of our method across diverse datasets.",
  "dataset/availability": "The data used in this study are not publicly released in a forum. The datasets consist of annotated field recordings of long-billed hermits, annotated lab recordings of budgerigars, and synthetic datasets modeled on natural vocalizations of these two species. These datasets were used to evaluate the performance of our proposed method. The field recordings of long-billed hermits were collected from known individuals in wild populations at La Selva Biological Station, Costa Rica. The laboratory recordings of budgerigar contact calls were collected from a laboratory population. Additionally, synthetic data were created by extracting dominant frequency contours from natural bird vocalizations and synthesizing frequency contours similar to those of the exemplar species. The synthetic datasets varied in features such as duration, harmonic content, and background noise, and were used to test the ability of our method to estimate repertoire size and acoustic diversity. The data were labeled by human experts for the field and lab recordings, and by software for the synthetic data. The use of these datasets allowed us to assess the performance of our method under different conditions and to demonstrate its accuracy in approximating human analysis.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the random forest. This is a well-established ensemble learning method known for its robustness and versatility in handling various types of data. The random forest algorithm is not new; it has been widely used and studied in the field of machine learning for many years.\n\nThe reason it was not published in a machine-learning journal is that our focus is on applying this algorithm to a specific problem in behavioral ecology, rather than on the development of new machine-learning techniques. Our work demonstrates the effectiveness of random forests in classifying and quantifying acoustic diversity in animal signals, which is a novel application in the context of ecological and behavioral studies. By leveraging the strengths of random forests, such as their ability to handle high-dimensional data and their robustness to overfitting, we provide a powerful tool for researchers in this field. The algorithm's proven track record in other domains makes it a reliable choice for our purposes, and our contribution lies in adapting and validating it for acoustic diversity analysis.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It primarily relies on random forest algorithms, both supervised and unsupervised, to analyze and classify acoustic data. The supervised random forest models are trained using labeled datasets, where the labels are assigned by human experts for field and lab recordings, and by software for synthetic data. These models are evaluated using out-of-bag error rates to assess their performance in classifying signals.\n\nThe unsupervised random forest models, on the other hand, are used to find underlying structures within unlabeled data. These models generate pairwise distances between samples, which are then used for further analysis, such as estimating repertoire size and calculating acoustic diversity. The unsupervised approach simulates the workflow that researchers might use for their data, ignoring any pre-existing labels to ensure independence during the training process.\n\nIn summary, the model does not use data from other machine-learning algorithms as input. It solely relies on random forest methods, with a clear distinction between supervised and unsupervised approaches to ensure the independence of training data.",
  "optimization/encoding": "For the machine-learning algorithm, we began by extracting a suite of common acoustic feature measurements from each audio clip. We applied a 500 Hz high pass filter to remove low frequency noise and then created spectrograms using a 300-point Fast Fourier Transform with a Hann window and 90% overlap. From these spectrograms, we derived 179 descriptive statistics of mel frequency cepstral coefficients and 28 acoustic parameters using established R packages. These features are widely used metrics in bioacoustics analyses.\n\nWe also calculated two pairwise distance matrices for each dataset: one using spectrogram cross-correlation and another using dynamic time warping. These matrices were then translated into a five-dimensional space using classical multi-dimensional scaling, with the axis coordinates serving as additional feature measurements. This process resulted in a vector of 217 feature measurements for each signal.\n\nTo prepare the data for the random forest models, we collated the feature vectors into a single matrix for each dataset. We removed any collinear measurements, applied a Box-Cox transformation to improve normality, and scaled and centered all feature values. The resulting matrix was used as input for both supervised and unsupervised random forest models. This preprocessing ensured that the data was standardized and suitable for the machine-learning algorithms, enhancing their ability to accurately classify and quantify acoustic diversity.",
  "optimization/parameters": "In our study, we utilized a comprehensive set of acoustic feature measurements to ensure a robust analysis of acoustic diversity. Specifically, we extracted 179 descriptive statistics of mel frequency cepstral coefficients (MFCCs) and 28 additional acoustic parameters from each audio clip. Furthermore, we calculated two pairwise distance matrices for every dataset using spectrogram cross-correlation and dynamic time warping, which were then translated into five-dimensional space using classical multi-dimensional scaling (MDS). The coordinates from these MDS analyses were also included as feature measurements. This process resulted in a total of 217 feature measurements for each signal.\n\nThe selection of these parameters was guided by their common use in bioacoustics analyses, ensuring that our approach is both thorough and aligned with established practices in the field. After collating the feature vectors for each audio clip into a single matrix for each dataset, we removed any collinear measurements to avoid redundancy. We then applied a Box-Cox transformation to improve the normality of the data, followed by scaling and centering all feature values. This preprocessing step ensured that the resulting matrix was suitable for input into both supervised and unsupervised random forest models.",
  "optimization/features": "The input features for our models consisted of a comprehensive set of acoustic measurements. Initially, we extracted 179 descriptive statistics of mel frequency cepstral coefficients (MFCCs) and 28 acoustic parameters using established R packages. Additionally, we calculated two pairwise distance matrices for each dataset using spectrogram cross-correlation and dynamic time warping, which were then translated into five-dimensional space using classical multi-dimensional scaling. This process resulted in five SPCC MDS coordinates and five DTW MDS coordinates per sample. Together, this yielded a total of 217 feature measurements for each signal.\n\nTo ensure the robustness of our models, we performed feature selection by removing any collinear measurements. This step was crucial to avoid redundancy and improve the model's performance. The feature selection process was conducted using the training set only, ensuring that the validation and testing phases remained unbiased. After feature selection, we applied a Box-Cox transformation to improve the normality of the data, followed by scaling and centering all feature values. The resulting matrix of features was then used as input for both supervised and unsupervised random forest models.",
  "optimization/fitting": "The fitting method employed in this study utilized unsupervised random forest models, which are designed to handle high-dimensional data and mitigate overfitting risks. The number of parameters, or features, used in the analysis was indeed large, encompassing a suite of commonly used acoustic parameters. However, the random forest approach inherently manages overfitting through several mechanisms. Firstly, it uses bootstrap aggregating (bagging), where multiple decision trees are constructed using different subsets of the data, reducing the risk of overfitting to any single subset. Secondly, random forest models are robust to collinearity and can handle non-monotonic relationships, which helps in identifying the most relevant features without overfitting to noise.\n\nTo further ensure that overfitting was not an issue, we evaluated the models using out-of-bag error estimates, which provide an unbiased measure of prediction performance. This metric helped in assessing the generalization capability of the models. Additionally, the use of silhouette width to determine the optimal number of clusters in the partitioning around medoids clustering method provided a quantitative measure to avoid overfitting during the clustering process.\n\nUnderfitting was addressed by ensuring that the models were complex enough to capture the underlying structure in the data. The use of 10,000 decision trees in the random forest models ensured that the models had sufficient capacity to learn from the data. Furthermore, the inclusion of a diverse set of acoustic features allowed the models to capture a wide range of variations in the signals, reducing the risk of underfitting. The high classification accuracy observed in the supervised models and the consistent performance across different datasets indicated that the models were adequately capturing the acoustic diversity.\n\nIn summary, the fitting method employed in this study effectively balanced the risk of overfitting and underfitting through the use of robust random forest models, thorough evaluation metrics, and a comprehensive set of acoustic features.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method used was the out-of-bag error estimate, which provided an unbiased assessment of prediction performance for our supervised random forest models. This technique helps to gauge how well the models generalize to unseen data by using a subset of the data that was not included in the construction of each tree within the forest.\n\nAdditionally, the random forest algorithm itself inherently includes mechanisms to mitigate overfitting. By constructing multiple decision trees using bootstrapped samples of the data and random subsets of features, the algorithm reduces the risk of overfitting to the training data. This ensemble approach averages the predictions of many trees, leading to more stable and generalizable models.\n\nFurthermore, we evaluated the performance of our models under various conditions, including different durations of audio clips, levels of harmonic content, and background noise. This thorough assessment helped us understand the robustness of our method across diverse acoustic structures and recording scenarios, ensuring that our models were not overly tailored to specific datasets.\n\nIn summary, our use of out-of-bag error estimates, the inherent properties of the random forest algorithm, and comprehensive performance evaluations under varying conditions collectively served as effective regularization methods to prevent overfitting.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model employed in our study is not a black box. The random forest approach used offers several advantages that contribute to its interpretability. One key feature is its ability to determine which acoustic feature measurements best divide the data into distinct categories. This means that the algorithm can handle a large number of features and automatically identify which are most useful for a given dataset. This capability is particularly valuable in acoustic analysis, where numerous features can be extracted from signals.\n\nAdditionally, random forest can identify variables that contribute most to finding structure within a dataset. This allows researchers to understand which specific acoustic features are driving the classification or clustering of signals. For instance, the model might reveal that certain harmonic content or duration features are critical in distinguishing between different types of bird calls.\n\nFurthermore, random forest produces proximity measures between observations, which can be used to represent trait spaces. This means that the relationships and similarities between different signals can be visualized and interpreted, providing insights into the underlying structure of the acoustic data.\n\nIn summary, the random forest model used in our study is transparent and provides clear examples of how it arrives at its classifications. By identifying the most influential features and producing interpretable proximity measures, the model offers a robust tool for understanding and quantifying acoustic diversity.",
  "model/output": "The model employed in our study is primarily a classification model, utilizing both supervised and unsupervised random forest approaches. The supervised random forest models were used to classify signals into predefined categories, leveraging labeled datasets where field and lab recordings were labeled by human experts, and synthetic data were labeled by software. These models assessed the ability to classify signals from the same category together using the out-of-bag error estimate, which provides an unbiased measure of prediction performance.\n\nIn contrast, the unsupervised random forest models were used to find underlying structures within unlabeled data. This approach produces dissimilarity measures between all samples, which can be used to identify groupings within the data. The unsupervised models were crucial for estimating repertoire size and acoustic diversity, as they allowed us to simulate the workflow that researchers might use for their unlabeled data.\n\nThe performance of these models was evaluated using several metrics, including out-of-bag error rates for supervised models and classification accuracy, adjusted Rand index, and acoustic area for unsupervised models. These evaluations helped us understand how well the models could assign samples into different classes and measure acoustic diversity.\n\nOverall, the random forest approach was integral to our workflow due to its ability to handle large, multi-dimensional datasets, robustness to collinearity and outliers, and efficiency in determining which feature measurements best divide data into distinct categories. This makes it a powerful tool for acoustic classification tasks across various taxa and communities.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for implementing the approach described in our study is publicly available. We have provided R code that allows researchers to apply our method to their own datasets. This code can be accessed and downloaded from Figshare at the following link: [Acoustic diversity dataset](https://figshare.com/articles/dataset/Acoustic_diversity_dataset/13661315). The dataset and code are archived under the DOI: 10.6084/m9.figshare.13661315. The code is released under a permissive license, allowing for broad use and modification by the research community. This ensures that other researchers can easily replicate our findings and adapt our methods to their specific needs.",
  "evaluation/method": "The evaluation of our method involved a comprehensive assessment using both supervised and unsupervised random forest models. For the supervised approach, we utilized labeled datasets, including field recordings of long-billed hermits and laboratory recordings of budgerigars, as well as synthetic datasets modeled on natural vocalizations of these species. The performance of these models was evaluated using out-of-bag error estimates, which provided an unbiased measure of prediction accuracy. This step confirmed that our models could accurately classify signals based on their acoustic features.\n\nIn addition to supervised models, we also employed unsupervised random forest models to simulate the workflow for unlabeled data. This approach allowed us to assess the method's ability to estimate repertoire size and acoustic diversity without prior labeling. We constructed unsupervised models using 10,000 decision trees for each dataset, generating pairwise distances between all samples. To evaluate the performance of these models, we used partitioning around medoids to determine the optimal number of clusters, which represented the estimated repertoire size. We then compared this estimate to the true repertoire size and calculated classification accuracy by assigning cluster labels based on the most frequently placed signal types.\n\nFurthermore, we evaluated how well the unsupervised random forest could measure acoustic diversity by calculating the area of the acoustic space occupied by all signals in a dataset. This involved assessing the distribution of error rates and examining how factors such as the duration of audio clips, harmonic content, background noise, and the number of discrete elements influenced the models' ability to assign signals to the correct class. Overall, our evaluation methods demonstrated the robustness and accuracy of our approach in both supervised and unsupervised contexts.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our method in classifying signals and estimating acoustic diversity. For supervised random forest models, we primarily used the out-of-bag error rate, which provides an unbiased estimate of the model's misclassification rate. This metric is widely used in the literature and is considered a robust measure of model performance.\n\nFor unsupervised random forest models, we used multiple metrics to assess how well the method could estimate repertoire size and acoustic diversity. One key metric was the classification accuracy, which we calculated by assigning each cluster a label corresponding to the most frequently occurring signal type within that cluster. This allowed us to determine the proportion of correctly classified samples.\n\nAdditionally, we used the adjusted Rand index to measure the similarity between the clusters identified by the unsupervised model and the true labels. This index ranges from 0 to 1, where 0 indicates completely random classification and 1 indicates perfect agreement with the true labels. The adjusted Rand index is a well-established metric in the field of clustering and provides a comprehensive evaluation of the model's performance.\n\nTo estimate repertoire size, we applied partitioning around medoids, a variation of k-means clustering, to the pairwise distance matrix generated by the unsupervised random forest. We then calculated the silhouette width to determine the optimal number of clusters, which served as our estimate of repertoire size. This approach is commonly used in the literature for evaluating clustering performance.\n\nFurthermore, we calculated the acoustic area occupied by all signals in a dataset using multidimensional scaling and the 95% minimum convex polygon. This metric allowed us to assess the acoustic diversity within each dataset and to test whether acoustic area increased with true repertoire size using Spearman\u2019s rank correlation.\n\nLastly, we visualized the clustering results using t-distributed stochastic neighbor embedding (t-SNE) to display all samples in two dimensions. This visualization technique is widely used in the literature for exploring high-dimensional data and provided an intuitive way to assess the quality of the clustering.\n\nOverall, the set of metrics we reported is representative of those commonly used in the literature for evaluating the performance of supervised and unsupervised machine learning models in acoustic analysis. These metrics provide a comprehensive evaluation of our method's ability to classify signals and estimate acoustic diversity.",
  "evaluation/comparison": "Not applicable. The publication focuses on evaluating the performance of a proposed method using various datasets, including field recordings, lab recordings, and synthetic datasets. The evaluation involves assessing the method's ability to classify signals and estimate repertoire size using both supervised and unsupervised random forest models. However, there is no mention of comparing the proposed method to publicly available methods or simpler baselines on benchmark datasets. The evaluation is primarily internal, focusing on the method's performance across different types of datasets and conditions.",
  "evaluation/confidence": "The evaluation of our method involved several performance metrics, and we did assess statistical significance to support our claims. For the supervised random forest models, we used out-of-bag error rates, which provided a misclassification rate for each dataset. These error rates were compared to what would be expected by chance, and we found that our models performed significantly better than random guessing. For instance, the out-of-bag error for field recordings of long-billed hermits was 0.04, and for lab recordings of budgerigars, it was 0.093, both of which are well below the expected error rates by chance.\n\nFor the unsupervised random forest models, we evaluated performance using classification accuracy and the adjusted Rand index. We observed that classification accuracy was often above 90% for datasets with a small number of unique elements and decreased as the number of unique elements increased. The adjusted Rand index also showed a similar trend, indicating that our method could accurately cluster signals into distinct categories, especially when the number of unique elements was relatively small.\n\nWe also calculated confidence intervals for some of our estimates. For example, we provided mean and standard error values for the out-of-bag error rates of synthetic datasets, which give an indication of the variability and confidence in our estimates. Additionally, we used statistical tests such as Spearman\u2019s rank correlation to assess the relationship between acoustic area and true repertoire size, further supporting the validity of our method.\n\nOverall, our results suggest that the method is robust and generalizable across different types of acoustic signals. The statistical significance of our findings, along with the performance metrics and confidence intervals, provides strong evidence that our approach is superior to other methods and baselines for quantifying acoustic diversity.",
  "evaluation/availability": "Not enough information is available."
}