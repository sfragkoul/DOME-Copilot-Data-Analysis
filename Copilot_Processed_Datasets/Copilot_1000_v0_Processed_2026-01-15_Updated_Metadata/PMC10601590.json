{
  "publication/title": "End-to-end deep learning classification of vocal pathology using stacked vowels.",
  "publication/authors": "Liu GS, Hodges JM, Yu J, Sung CK, Erickson-DiRenzo E, Doyle PC",
  "publication/journal": "Laryngoscope investigative otolaryngology",
  "publication/year": "2023",
  "publication/pmid": "37899847",
  "publication/pmcid": "PMC10601590",
  "publication/doi": "10.1002/lio2.1144",
  "publication/tags": "- Artificial Intelligence\n- Voice Disorders\n- Deep Learning\n- Vocal Pathology\n- Convolutional Neural Networks\n- Audio Classification\n- Machine Learning\n- Medical Diagnosis\n- Speech Analysis\n- End-to-End Learning",
  "dataset/provenance": "The dataset used in this work is the SVD dataset, which contains labeled healthy and pathological audio samples. These samples include both sustained vowels (/a/, /i/, and /u/) and sentences, recorded in three different pitches: low, neutral, and high. The database includes recordings from over 2000 German-speaking individuals with over 70 class labels. For our study, we utilized a subset of this dataset, focusing on recordings from healthy participants and those with two specific vocal fold pathologies: hyperfunctional dysphonia and laryngitis. These pathologies were selected based on their presence in related works. Only recordings tagged with either hyperfunctional dysphonia or laryngitis were included, excluding those with both tags. The final dataset consisted of 687 healthy participants, 207 hyperfunctional dysphonia patients, and 127 laryngitis patients, with up to 9 vowel recordings per participant, depending on the model. The raw audio data were extracted at a sampling rate of 44,100 Hz and trimmed to clips of 0.50 seconds duration. The dataset was split into training, validation, and testing sets in a 60:20:20 ratio, stratified by class labels. The specific number of samples varied depending on the model architecture used. For instance, the baseline model used 3063 neutral pitch vowel audio samples, while the stacked-vowel model used 1021 samples after stacking. The stacked-pitch model also used 3063 samples after stacking the three pitches for each vowel.",
  "dataset/splits": "The dataset was split into three distinct parts: training, validation, and testing. The splits were done in a 60:20:20 ratio, stratified by class labels. For the baseline model architecture, a total of 3063 neutral pitch vowel audio samples were used. This resulted in approximately 1838 samples for training, 613 for validation, and 612 for testing. The VocalPathNet stacked-vowel model architecture involved stacking neutral pitch vowel recordings, resulting in a final sample count of 1021. The VocalPathNet stacked-pitch model architecture involved stacking three pitch vowel recordings for each vowel, maintaining the same final sample count of 3063 as the baseline model. The distribution of data points in each split was designed to ensure that each class was proportionally represented across the training, validation, and testing datasets.",
  "dataset/redundancy": "The dataset used in this work was derived from the SVD dataset, which contains labeled healthy and pathological audio samples consisting of both sustained vowels (/a/, /i/, and /u/) and sentences. The dataset includes recordings from over 2000 German-speaking individuals with over 70 class labels. For our study, we focused on a subset of this dataset, including recordings from healthy participants and patients with two specific vocal fold pathologies: hyperfunctional dysphonia and laryngitis. Recordings with tags of both pathologies were excluded to ensure clarity in classification.\n\nThe raw audio data were extracted at a sampling rate of 44,100 Hz and trimmed to clips of 0.50 seconds duration. The dataset consisted of 687 healthy participants, 207 hyperfunctional dysphonia patients, and 127 laryngitis patients, with up to 9 vowel recordings per participant, depending on the model.\n\nThe dataset was split into training, validation, and testing subsets in a 60:20:20 ratio, stratified by class labels to ensure that each subset had a representative distribution of the different classes. This stratification was crucial to maintain the integrity of the class distribution across all subsets, especially given the class imbalance in the dataset.\n\nThe training dataset was used to train the models, the validation dataset was used to tune hyperparameters and prevent overfitting, and the test dataset was used to evaluate the final performance of the models. The test set was a hold-out set from the same dataset, ensuring that the training and test sets were independent. This independence was enforced by the stratified splitting process, which ensured that no data from the test set was used during the training or validation phases.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the field of vocal pathology classification. The inclusion of multiple sustained vowels and different pitches provides a richer set of information for analyzing the effects of vocal pathology. However, the relatively small size of the dataset is a limitation, and future work could benefit from expanding the dataset to include more recordings and additional vocal pathologies. This expansion would help improve the generalizability and robustness of the models.",
  "dataset/availability": "The data used in this study is not publicly available. The dataset utilized was the SVD dataset, which contains labeled healthy and pathological audio samples consisting of both sustained vowels (/a/, /i/, and /u/) and sentences. The dataset includes recordings from over 2000 German-speaking individuals with over 70 class labels. However, the specific subset used in this study, which includes recordings from healthy participants and patients with hyperfunctional dysphonia and laryngitis, is not released in a public forum. The dataset was split into training, validation, and testing datasets in a 60:20:20 ratio, stratified by class labels. The data splits were enforced by ensuring that the distribution of classes was maintained across the different subsets. The raw audio data were extracted at a sampling rate of 44,100 Hz and trimmed to clips of a duration of 0.50 seconds. The dataset consisted of 687 healthy participants, 207 hyperfunctional dysphonia patients, and 127 laryngitis patients, with up to 9 vowel recordings per participant. The age and sex demographic characteristics of participants are also available.",
  "optimization/algorithm": "The optimization algorithm used in our study is the Adam optimization algorithm. This is a well-established algorithm in the field of deep learning and is not new. It is widely used for updating the learning rate using exponentially weighted moving averages of the gradient and squared gradient of each learnable parameter. The choice of Adam was made due to its efficiency and effectiveness in handling sparse gradients on noisy problems, which is common in deep learning tasks.\n\nThe reason this algorithm was not published in a machine-learning journal is that it is a standard and widely accepted method in the field. Our focus was on applying this optimization algorithm to a specific problem\u2014classifying vocal pathologies using stacked vowel inputs\u2014instead of developing a new optimization algorithm. The innovation lies in the application of the Adam optimization algorithm within the context of our novel deep learning framework, VocalPathNet, rather than in the algorithm itself.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It is an end-to-end deep learning framework named VocalPathNet, designed to classify vocal pathologies using raw audio recordings of sustained vowels. The framework directly learns from voice data without relying on data from other machine-learning algorithms as input. Instead, it utilizes a convolutional neural network (CNN) architecture to analyze the audio data.\n\nThe VocalPathNet model is distinguished by its ability to input stacked vowel recordings, which provide a richer set of information compared to single vowel recordings. This approach allows the model to analyze multiple sustained vowels simultaneously, potentially improving the classification of vocal pathologies.\n\nThe model architectures compared in this study include a baseline model that takes individual vowel recordings as inputs and two variations of the VocalPathNet model: one that uses stacked vowel inputs and another that uses stacked pitch inputs. These models were trained and evaluated using a dataset with class imbalances, and performance metrics such as precision, recall, F1 score, and area under the receiver operating curve (AUROC) were used to assess their effectiveness.\n\nThe training data for these models was accessed from the Saarbruecken voice database (SVD), which contains recordings of healthy and dysphonic voices. The models were evaluated on a test hold-out set from this dataset, and future iterations could involve validation using additional open labeled datasets of voice recordings. The generalizability of the model could be further enhanced by including recordings of sustained vowels in different pitches, additional vocal pathologies, and recordings in other languages.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the audio data was suitable for input into the convolutional neural network (CNN) models. The raw audio data was extracted at a sampling rate of 44,100 Hz using the librosa package in Python. This high sampling rate was chosen to capture a large number of data points, even after trimming the recordings to a duration of 0.50 seconds. The presence of silence in the raw audio data was examined, and it was found that there were no substantial periods of silence.\n\nThe dataset consisted of sustained vowel recordings (/a/, /i/, and /u/) and sentences, recorded in three different pitches: low, neutral, and high. For the baseline model, individual neutral-pitch vowel recordings were used as inputs. For the VocalPathNet stacked-vowel model, neutral-pitch vowel recordings of /a/, /i/, and /u/ were stacked in that order. For the VocalPathNet stacked-pitch model, the three pitch vowel recordings of each vowel were stacked in the order of low, neutral, and high.\n\nThe final sample counts after stacking were 3063 for the baseline and stacked-pitch models, and 1021 for the stacked-vowel model. The audio samples were split into training, validation, and testing datasets in a 60:20:20 ratio, stratified by class labels. Additionally, the sex and age demographic data of each speaker were combined with the audio data and fed into fully connected layers utilizing the rectified linear unit activation function. This preprocessing ensured that the models could effectively analyze the one-dimensional, time-domain audio data and identify relevant features within the recordings.",
  "optimization/parameters": "The models utilized in this study varied in the number of parameters based on their architecture. The baseline model, which processed individual neutral-pitch vowel recordings, had the fewest parameters. The VocalPathNet stacked-vowel model, which handled stacked recordings of three vowels (/a/, /i/, and /u/) in the neutral pitch, had more parameters due to the increased complexity of the input data. The VocalPathNet stacked-pitch model, which processed stacked recordings of the three pitches (low, neutral, and high) for each vowel, had the highest number of parameters.\n\nThe selection of the number of parameters was influenced by the need to balance model complexity with the risk of overfitting, especially given the relatively small size of the training dataset. Hyperparameter tuning was performed using a combination of random search via the keras_tuner package and manual training and error analysis. This process optimized the validation loss by adjusting hyperparameters such as the number of filters, filter size, size of the pooling layers, batch size, and the initial learning rate. The goal was to find the optimal configuration that would generalize well to unseen data while minimizing overfitting.",
  "optimization/features": "The input features for our models consist of audio data from sustained vowel recordings. Specifically, the baseline model uses individual vowel recordings, while the VocalPathNet models utilize stacked vowel recordings (/a/, /i/, and /u/ in neutral pitch) or stacked pitch recordings. This approach allows for a richer set of information by analyzing multiple sustained vowels together, which can better capture the effects of vocal pathology on different expression patterns.\n\nFeature selection was not performed in the traditional sense, as we employed an end-to-end deep learning approach. This method directly learns from the raw voice data without relying on pre-selected expert-derived voice features. The use of stacked vowel inputs is particularly advantageous because it provides a more comprehensive analysis of the voice data, overcoming the limitations of a two-step approach that requires pre-selection of features.\n\nThe models were implemented using the Keras deep learning library, and hyperparameter tuning was conducted using a combination of random search and manual training and error analysis. This process optimized the validation loss and ensured that the models could effectively learn from the input features. The key hyperparameters tuned included the number of filters, filter size, size of the pooling layers, batch size, and the initial learning rate.",
  "optimization/fitting": "The fitting method employed in our study utilized a deep learning approach with convolutional neural networks (CNNs). The models were implemented using the Keras deep learning library. The baseline model used a 1-dimensional CNN architecture, while the VocalPathNet models used a quasi-one-dimensional input of stacked vowel and stacked pitch data.\n\nThe dataset consisted of 3063 samples after stacking, which is relatively small for training deep learning models. This small dataset size posed a risk of overfitting, especially for the VocalPathNet models, which had successively higher numbers of model parameters. To mitigate overfitting, several strategies were employed. Hyperparameter tuning was performed using a combination of random search via the keras_tuner package and manual training and error analysis to optimize the validation loss. The hyperparameters tuned included the number of filters, filter size, size of the pooling layers, batch size, and the initial learning rate. Additionally, the Adam optimization algorithm was used to update the learning rate using exponentially weighted moving averages of the gradient and squared gradient of each learnable parameter.\n\nTo address class imbalance, the area under the receiver operating curve (AUROC) was weighted by the number of samples representing each class label, and the F1 score was calculated using the micro-average weighting scheme. These metrics were chosen because they are effective for imbalanced datasets.\n\nDespite these efforts, the small dataset size may have limited the performance of the models. Future work could involve expanding the training dataset size by an order of magnitude to improve model classification results. Additionally, other methods to address class imbalance, such as data augmentation of voice recordings, could be explored in future work.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and improve the generalization of our models. One of the primary methods used was hyperparameter tuning. We utilized a combination of random search via the keras_tuner package and manual training and error analysis to optimize various hyperparameters, including the number of filters, filter size, size of the pooling layers, batch size, and the initial learning rate. This process helped in finding the optimal configuration that minimized validation loss and reduced the risk of overfitting.\n\nAdditionally, we addressed class imbalance in our dataset, which is a common issue that can lead to overfitting. We explored the use of class-weighted loss functions and over- and under-sampling of classes. Although these strategies did not improve our classification results due to the small dataset size, they are standard techniques for mitigating the effects of class imbalance.\n\nThe use of the Adam optimization algorithm also played a role in regularization. Adam adapts the learning rate for each parameter, which can help in converging to a better solution and reducing overfitting. The algorithm uses exponentially weighted moving averages of the gradient and squared gradient of each learnable parameter, providing a more stable and efficient update process.\n\nFurthermore, the architecture of our models, particularly the use of convolutional layers with weight sharing, helped in capturing relevant features from the audio data while keeping the number of parameters manageable. This architectural choice is beneficial for preventing overfitting, especially in smaller datasets.\n\nLastly, we evaluated the generalizability of our models on a test hold-out set from a single open dataset. Future iterations could involve validating the models using additional open labeled datasets of voice recordings, which would further enhance the robustness and generalizability of our framework.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available within the text of the publication. We utilized a combination of random search via the keras_tuner package and manual training and error analysis to optimize the validation loss. The hyperparameters that were tuned included the number of filters, filter size, size of the pooling layers, batch size, and the initial learning rate.\n\nThe models were implemented using the Keras deep learning library, and the specific architectures for the baseline and VocalPathNet CNN models are illustrated in Figure 1. The key difference between the baseline and VocalPathNet model architectures is that the latter had input layers that accepted a quasi-one-dimensional input of the three-channel stacked vowel and stacked pitch input data.\n\nThe categorical cross entropy loss function was used for three-class multiclass classification of healthy, hyperfunctional dysphonia, and laryngitis class labels. For model training, the Adam optimization algorithm was employed to update the initial learning rate using exponentially weighted moving averages of the gradient and squared gradient of each learnable parameter.\n\nThe best performing candidate models of each model architecture were compared using two performance metrics: area under the receiver operating curve (AUROC) and the F1 score. To address class imbalance, the AUROC was weighted by the number of samples representing each class label, and the F1 score was calculated as the harmonic mean of the precision and recall. The F1 score metric was chosen because it works well for imbalanced datasets.\n\nThe precision, recall, and F1 scores are reported in Table 3, and the class-specific classification performance metrics for each model are shown in Table 4. Confusion matrices showing the test performance of the baseline, stacked vowel, and stacked pitch models are provided in Figure 2.\n\nThe dataset used in this study is the Saarbruecken Voice Database, which contains labeled healthy and pathological audio samples consisting of both sustained vowels (/a/, /i/, and /u/) and sentences. The database includes recordings from over 2000 German-speaking individuals with over 70 class labels. We used a subset of the voice recordings, including recordings from healthy patients and patients with two vocal fold pathologies: hyperfunctional dysphonia and laryngitis.\n\nThe raw audio data were extracted at a sampling rate of 44,100 Hz using the librosa package in Python. All recordings were trimmed to clips of a duration of 0.50 seconds. The final sample counts after stacking for the different model architectures are reported in the text.\n\nThe publication is open access under the terms of the Creative Commons Attribution-NonCommercial-NoDerivs License, which permits use and distribution in any medium, provided the original work is properly cited, the use is non-commercial and no modifications or adaptations are made.",
  "model/interpretability": "The interpretability of deep learning models, particularly in the context of vocal pathology classification, remains a significant challenge. Our models, including the baseline and VocalPathNet architectures, are largely considered black-box models. This means that while they can effectively classify vocal pathologies, the internal decision-making processes are not immediately transparent.\n\nTo address this, visualization techniques such as saliency maps and occlusion maps, which are well-established in computer vision, could be adapted for use with spectrograms. These techniques can help identify specific regions of the spectrogram that are influential in the model's predictions. By applying these methods, we can gain insights into which features of the voice recordings are most important for distinguishing between healthy and dysphonic voices.\n\nAdditionally, there are ongoing developments in visualization methods specifically designed for audio deep neural networks. These methods aim to make the decision-making processes of audio-based models more interpretable. As these techniques become more available, they could be integrated into our framework to provide clearer explanations for the model's predictions.\n\nIn summary, while our current models are not fully transparent, there are promising avenues for improving interpretability through visualization techniques. These methods can help delineate the areas of spectrograms that are crucial for vocal pathology predictions, making the models more understandable and trustworthy.",
  "model/output": "The model is designed for multiclass classification. It specifically focuses on classifying voice recordings into three categories: healthy, hyperfunctional dysphonia, and laryngitis. The output of the model is a prediction of which class a given voice recording belongs to. This is achieved through a deep learning framework that utilizes convolutional neural networks (CNNs) to analyze the audio data. The model's architecture includes convolutional and pooling layers that process the audio input, followed by fully connected layers that integrate demographic data such as sex and age. The final prediction layer outputs probabilities for each of the three classes, allowing for the classification of the voice recordings. The performance of the model is evaluated using metrics such as the F1 score and the area under the receiver operating curve (AUROC), which provide insights into the model's accuracy and effectiveness in distinguishing between the different classes of vocal pathologies.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the models involved comparing the performance of three different architectures: a baseline model, a stacked vowel model, and a stacked pitch model. The models were trained using voice samples from the Saarbruecken Voice Database, which included sustained vowels from both healthy participants and dysphonic patients. The performance metrics used for evaluation were the area under the receiver operating curve (AUROC) and the F1 score. To address class imbalance, the AUROC was weighted by the number of samples representing each class label, and the F1 score was calculated using the micro-average weighting scheme. The precision and recall scores, which are used to calculate the F1 scores, were also evaluated. The models were compared based on their overall performance in multiclass classification tasks and their class-specific performance metrics. Additionally, confusion matrices and precision and recall values were examined to provide a comprehensive evaluation of the models' test performance. The best performing candidate models of each architecture were selected for comparison.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our models in classifying vocal pathologies. The primary metrics reported are the F1 score and the Area Under the Receiver Operating Curve (AUROC). The F1 score was chosen because it is particularly well-suited for imbalanced datasets, which is a characteristic of our dataset. It is calculated as the harmonic mean of precision and recall, providing a balanced measure of a model's accuracy. To further account for class imbalance, we used the micro-average weighting scheme for the F1 score. Additionally, we evaluated precision and recall scores, which are fundamental components of the F1 score and provide insights into the models' performance in terms of positive predictive value and sensitivity, respectively.\n\nThe AUROC was also weighted by the number of samples representing each class label to address the class imbalance. However, it is important to interpret these AUROC values with caution due to the imbalanced nature of our dataset. These metrics are representative of the literature, as they are commonly used in machine learning and deep learning studies, especially those dealing with medical data and imbalanced datasets. The use of these metrics allows for a comprehensive evaluation of our models' performance, ensuring that our findings are both robust and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we developed an end-to-end deep learning framework named VocalPathNet for detecting and classifying vocal fold pathology using recordings of multiple sustained vowels. To evaluate the effectiveness of our approach, we compared it with a baseline model that took individual vowel recordings as inputs. This comparison allowed us to assess the performance of our proposed framework in classifying healthy and pathological voice recordings and distinguishing between vocal pathologies such as hyperfunctional dysphonia and laryngitis.\n\nThe baseline model utilized a 1-dimensional convolutional neural network (1-D CNN) architecture, which is well-suited for analyzing one-dimensional, time-domain audio data. This model architecture employs convolutional filters and weight sharing to identify relevant features within the recordings. Following the convolutional and pooling layers, demographic data such as the sex and age of each speaker were combined with the audio data and fed into fully connected layers utilizing the rectified linear unit activation function. The prediction layer utilized three outputs, one for each class.\n\nIn contrast, the VocalPathNet framework is distinguished by its ability to input multiple sustained vowel recordings simultaneously. This approach leverages a quasi-one-dimensional input of stacked vowel data, allowing the model to potentially improve classification accuracy by utilizing more of the available voice recording data. The architectures of the baseline and VocalPathNet CNN models were implemented using the Keras deep learning library.\n\nFor model evaluation, we used two performance metrics: the area under the receiver operating curve (AUROC) and the F1 score. The AUROC was weighted by the number of samples representing each class label to address class imbalance, while the F1 score was calculated as the harmonic mean of precision and recall. The F1 score metric was chosen because it works well for imbalanced datasets, and overall F1 score metrics for models were calculated using the micro-average weighting scheme to further account for class imbalance.\n\nThe results showed that the baseline model and the VocalPathNet stacked-pitch model achieved similar F1 scores (0.77 vs. 0.78, respectively), while the VocalPathNet stacked-vowel model's F1 score was higher (0.80). The baseline model, stacked vowel, and stacked pitch models had similar AUROC scores (0.90, 0.90, and 0.89, respectively). These AUROC values should be interpreted with caution due to the imbalanced dataset. As a reference, human expert classification of related vocal pathologies achieved an accuracy around 0.6.\n\nIn summary, our comparison to a simpler baseline model demonstrated the potential advantages of using multiple sustained vowel recordings in an end-to-end deep learning framework for vocal pathology classification. This approach sets the groundwork for future improvements in voice pathology classification using more of the available voice recording data.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The data was sourced from the Saarbruecken Voice Database (SVD), which contains labeled healthy and pathological audio samples consisting of both sustained vowels and sentences. The database includes recordings from over 2000 German-speaking individuals with over 70 class labels. However, access to the full dataset is restricted and not openly available for public release. The specific subset used in our study included recordings from healthy participants and patients with hyperfunctional dysphonia and laryngitis. These recordings were used to train and evaluate our deep learning models, but the raw files themselves are not provided with our publication. For further details on accessing the SVD, interested parties would need to contact Saarland University, the host of the database."
}