{
  "publication/title": "Radiomics analysis to predict pulmonary nodule malignancy using machine learning approaches.",
  "publication/authors": "Warkentin MT, Al-Sawaihey H, Lam S, Liu G, Diergaarde B, Yuan JM, Wilson DO, Atkar-Khattra S, Grant B, Brhane Y, Khodayari-Moez E, Murison KR, Tammemagi MC, Campbell KR, Hung RJ",
  "publication/journal": "Thorax",
  "publication/year": "2024",
  "publication/pmid": "38195644",
  "publication/pmcid": "PMC10947877",
  "publication/doi": "10.1136/thorax-2023-220226",
  "publication/tags": "- Radiomics\n- Pulmonary Nodules\n- Machine Learning\n- Lung Cancer Screening\n- Model Performance\n- Calibration\n- Discrimination\n- Epidemiologic Covariates\n- Gradient Boosted Trees\n- Logistic Regression\n- Feature Extraction\n- Medical Imaging\n- Quantitative Image Analysis\n- Nodule Malignancy\n- Clinical Decision-Making",
  "dataset/provenance": "The dataset utilized in this study is derived from four distinct lung cancer screening cohorts. These cohorts are IELCAP-Toronto, NLST, PanCan, and PLuSS. The total number of participants across these cohorts is 6,865. Specifically, IELCAP-Toronto includes 502 participants, NLST includes 3,743 participants, PanCan includes 1,785 participants, and PLuSS includes 835 participants.\n\nThe dataset comprises a total of 16,797 baseline screen-detected nodules. The number of nodules per participant varies across the cohorts. For instance, PanCan has the highest average number of nodules per participant, while the other studies have fewer nodules on average. The nodules were subjected to rigorous filtering to exclude those with technical issues in feature extraction, those not first-appearing on baseline scans, and those with missing patient-level data for the harmonized set of epidemiologic covariates.\n\nThis dataset has been used to develop and validate machine learning models aimed at classifying benign and malignant pulmonary nodules. The models were evaluated using metrics such as the area under the receiver operating characteristic curve (AUC) and calibration metrics to assess their performance in discrimination and risk estimation. The dataset and the models developed from it contribute to the ongoing efforts in the medical community to improve lung cancer screening and diagnosis.",
  "dataset/splits": "The dataset was partitioned into two main splits: a training/validation set and a testing set. This partitioning was done using group-based random sampling to ensure that all nodules for a participant were in a single set, preventing data leakage.\n\nThe training/validation set comprised 80% of the data, while the testing set comprised the remaining 20%. The training set was further divided into five folds using subject-level random sampling to perform cross-validation. This cross-validation process was essential for identifying the optimal machine learning model and the best set of hyperparameters.\n\nThe distribution of data points in each split was designed to maintain the integrity of participant-specific data, ensuring that all nodules from a single participant were not split across different sets. This approach helped in maintaining the consistency and reliability of the model's performance metrics.",
  "dataset/redundancy": "The dataset used in this study was partitioned into training/validation and testing splits using group-based random sampling. This approach ensured that all nodules for a specific participant were included in a single set, preventing data leakage. The training set constituted 80% of the data, while the testing set made up the remaining 20%. This split was performed to maintain the independence of the training and test sets, ensuring that the model's performance could be accurately evaluated on unseen data.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the context of pulmonary nodule assessment. The study included a large and diverse cohort of 16,797 baseline screen-detected nodules among 6,865 participants from four distinct lung cancer screening studies. This diversity helps in generalizing the model's performance across different patient populations and imaging protocols. The median time-to-diagnosis for baseline-detected nodules was 134 days, providing a robust temporal context for the analysis.\n\nTo further ensure the robustness of the model, K-fold cross-validation was performed on the training set. This technique involved splitting the training data into five folds and iteratively training the model on four folds while validating it on the remaining fold. This process was repeated five times, with each fold serving as the validation set once. The optimal machine learning model and hyperparameters were identified through this cross-validation process, ensuring that the final model was well-generalized and not overfitted to the training data.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the class of supervised learning techniques, specifically designed for classification tasks. The algorithms employed include penalized logistic regression (LASSO), Random Forest, and Gradient Boosted Trees (XGBoost). These are well-established methods in the field of machine learning and are widely used for their effectiveness in handling complex datasets.\n\nThe algorithms used are not new; they are established techniques in the machine learning community. The choice of these algorithms was driven by their proven track record in similar predictive modeling tasks, particularly in medical imaging and radiomics. The focus of this study is on applying these algorithms to the specific problem of predicting pulmonary nodule malignancy, rather than developing new machine-learning algorithms.\n\nThe decision to use these established algorithms in a medical context, rather than a machine-learning journal, is due to the study's primary focus on the medical application and validation of these models. The research aims to demonstrate the practical utility of these machine-learning techniques in a clinical setting, specifically for improving the diagnosis of pulmonary nodules. The study's contributions lie in the application and evaluation of these models within the context of lung cancer screening and radiomics, rather than in the development of novel machine-learning algorithms.",
  "optimization/meta": "The model developed in this study does not use data from other machine-learning algorithms as input. Instead, it directly utilizes radiomic features and epidemiologic covariates. The machine learning models evaluated include penalized logistic regression (LASSO), Random Forest (RF), and Gradient Boosted Trees (XGBoost). These models were assessed using cross-validation to identify the top-performing model.\n\nThe data partitioning ensured that all nodules for a participant were in a single set to avoid data leakage, maintaining the independence of the training and testing data. This approach helps in preventing overfitting and ensures that the model's performance is evaluated on truly independent data. The final model, referred to as the INTEGRAL-Radiomics model, was fitted to the full training set and evaluated on a hold-out test set, demonstrating robust performance metrics.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the quality and relevance of the features used. Initially, we started with 2,060 radiomic features extracted using PyRadiomics. These features were categorized into different classes, including first-order statistics, shape-based features, gray level co-occurrence matrix, gray level run length matrix, gray level size zone matrix, neighboring gray tone difference matrix, and gray level dependence matrix.\n\nTo handle the heterogeneity in image acquisition settings, all images and masks were resampled and interpolated to achieve unit voxel spacing. This step ensured isotropy in the data. Grayscale intensities were discretized into bins using a bin width of 25 for histogram-based features, and voxel intensities were right-shifted by 1,000 units to avoid negative values during feature computations.\n\nWe performed an initial set of filtering steps to remove low-quality and redundant features. Features with zero variance were excluded, as were those with observed numerical instability. Univariate models were fitted for each feature in the training data, and only features with a false discovery rate (FDR)-adjusted p-value less than 0.05 were retained. Additionally, pairwise sets of predictors with a correlation greater than 0.9 were evaluated, and the predictor with the larger p-value was removed to reduce redundancy.\n\nAfter filtering, 642 radiomic features were retained for model development. These features, along with nine epidemiologic covariates, were used to train the machine-learning models. All predictors were normalized prior to model fitting to ensure consistency and improve model performance. The epidemiologic covariates included age, sex, family history of lung cancer, history of COPD or emphysema, smoking status, smoking duration, smoking intensity, years since quitting, and body mass index. These covariates were harmonized across four screening studies to establish a common set of patient-level features.\n\nThe final dataset consisted of 16,797 baseline screen-detected nodules among 6,865 participants, with the data split into training and testing sets using subject-level random sampling. This approach ensured that all nodules for a specific participant were in the same split, avoiding data leakage. The training set was further divided into five folds for cross-validation, allowing for robust model evaluation and hyperparameter tuning.",
  "optimization/parameters": "In our study, we started with a comprehensive set of 2,060 radiomics features for model development. However, we applied several filtering steps to refine this set. Initially, we removed 78 features due to zero variance and 11 features due to numerical instability. Next, we retained features with a false discovery rate (FDR)-adjusted p-value less than 0.05 from univariate models, which resulted in 248 features. We further reduced redundancy by removing features with pairwise correlations greater than 0.9, ultimately retaining 642 radiomic features. Additionally, we included 9 epidemiologic covariates, bringing the total number of predictors to 651.\n\nThe selection of these parameters was systematic and involved multiple steps. We began with a broad set of radiomic features and applied rigorous filtering criteria to ensure that only high-quality, non-redundant features were retained. This process involved statistical tests and correlation analyses to identify the most predictive and independent features. The final set of 642 radiomic features and 9 epidemiologic covariates were used in our machine learning models, which included penalized logistic regression (LASSO), Random Forest, and Gradient Boosted Trees (XGBoost). The optimal hyperparameters for these models were determined through grid search and cross-validation, ensuring that the models were well-calibrated and performed robustly on the hold-out test set.",
  "optimization/features": "In the optimization process, we initially started with 2,060 radiomic features for model development. To ensure the quality and relevance of these features, we performed several filtering steps. First, we removed 78 features due to zero-variance and 11 features due to numerical instability. Next, we fit univariate models for each feature in the training data and retained those with a false discovery rate (FDR)-adjusted p-value less than 0.05, resulting in 248 features. We then evaluated all pairwise sets of predictors with a correlation greater than 0.9 in the training set and removed the predictor with the larger p-value. This process retained 642 radiomic features for model development.\n\nIn addition to the radiomic features, we also included 9 epidemiologic covariates in our models. These covariates were harmonized across four lung cancer screening cohorts and included variables such as age, sex, family history of lung cancer, history of COPD or emphysema, smoking status, smoking duration, smoking intensity, years since quitting, and body mass index.\n\nAll feature selection steps were performed using the training set only to prevent data leakage and ensure the generalizability of our models. This rigorous feature selection process helped us to identify the most relevant and non-redundant features for predicting nodule malignancy.",
  "optimization/fitting": "In our study, we began with a large number of radiomic features (2,060) relative to the number of training points, which could potentially lead to overfitting. To mitigate this risk, we implemented several filtering steps to reduce the feature set. We removed features with zero variance, low quality, and those that were weakly predictive or highly redundant. This process retained 647 radiomic features and 9 epidemiologic covariates for model development.\n\nTo further ensure that our models did not overfit, we employed K-fold cross-validation. This technique involves partitioning the data into K subsets, training the model on K-1 subsets, and validating it on the remaining subset. This process is repeated K times, with each subset serving as the validation set once. By averaging the performance across all folds, we obtained a more reliable estimate of the model's generalization performance.\n\nAdditionally, we performed a grid search over a set of hyperparameters chosen using a Latin hypercube space-filling design. This method helps in exploring the hyperparameter space efficiently. For the top-performing model, we conducted a finer random grid search to identify the optimal hyperparameters. This two-step approach helped in fine-tuning the model and reducing the risk of overfitting.\n\nTo address underfitting, we evaluated multiple machine learning models, including penalized logistic regression (LASSO), Random Forest, and Gradient Boosted Trees (XGBoost). Each model has its strengths and can capture different aspects of the data. By comparing their performance, we selected the model that best balanced bias and variance. The LASSO model, which includes a regularization term to prevent overfitting, ultimately performed the best in terms of both discrimination and calibration.\n\nMoreover, we assessed model performance using the area under the receiver operating characteristic curve (AUC) and calibration metrics. The AUC measures the model's ability to discriminate between malignant and benign nodules, while calibration compares the predicted risks to observed risks. By evaluating these metrics, we ensured that our model was neither underfitting nor overfitting the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One of the key methods used was regularization, specifically through the use of the LASSO (Least Absolute Shrinkage and Selection Operator) model. LASSO applies L1 regularization, which adds a penalty equal to the absolute value of the magnitude of coefficients. This process helps to shrink some coefficients to zero, effectively performing feature selection and reducing the model's complexity. By doing so, LASSO helps to mitigate overfitting by simplifying the model and focusing on the most relevant features.\n\nAdditionally, we performed feature filtering to exclude low-quality and highly-redundant features. This step involved removing features with zero variance, low quality, weak predictive value, and high pairwise correlation. By retaining only the most informative and non-redundant features, we further reduced the risk of overfitting.\n\nWe also utilized K-fold cross-validation to identify the optimal machine learning model and hyperparameters. This technique involves partitioning the data into K subsets, training the model on K-1 subsets, and validating it on the remaining subset. This process is repeated K times, with each subset serving as the validation set once. Cross-validation helps to ensure that the model generalizes well to unseen data by providing a more reliable estimate of its performance.\n\nFurthermore, we performed grid search over a set of hyperparameters chosen using a Latin hypercube space-filling design. This method systematically explores the hyperparameter space to find the optimal settings for the model, enhancing its performance and generalization ability. For the top-performing model, we conducted a finer random grid search to further refine the hyperparameters.\n\nIn summary, our approach to preventing overfitting included regularization through LASSO, feature filtering, K-fold cross-validation, and systematic hyperparameter tuning. These techniques collectively contributed to the development of a robust and generalizable machine learning model for classifying benign and malignant pulmonary nodules.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are available. The optimal hyperparameters for the top-performing models, such as XGBoost, LASSO, and Random Forest, are detailed in the results. For instance, the XGBoost model's optimal parameters include the number of trees, tree depth, minimum node size, number of predictors, learning rate, and loss reduction.\n\nThe optimization process involved a grid search over a set of hyperparameters chosen using a Latin hypercube space-filling design, followed by a finer random grid search for the top-performing model. This process ensured that the models were thoroughly optimized.\n\nThe model reported in the study, along with example code, is publicly available on GitHub. This repository provides access to the model files and the optimization parameters used in the study. The data used in the present study may be made available upon reasonable request to the Integrative Analysis of Lung Cancer Etiology and Risk (INTEGRAL) program, subject to approval by the Data Access Committee. This ensures that the configurations and parameters are accessible for further research and validation.",
  "model/interpretability": "The model developed in this study is designed to be more transparent and interpretable compared to many deep learning approaches. While deep learning models often operate as black boxes, our approach uses machine learning models that provide clearer insights into their decision-making processes.\n\nThe top-performing model in our study is a penalized logistic regression (LASSO) model. LASSO is particularly advantageous for interpretability because it performs both variable selection and regularization, leading to a sparse model where many coefficients are exactly zero. This means that only a subset of the features are used in the final model, making it easier to understand which features are most important for predicting nodule malignancy.\n\nIn total, 142 predictors were retained in the final LASSO model with non-zero coefficients. This subset of features can be examined to understand their contributions to the model's predictions. Additionally, the stability of these features across different cross-validation folds was assessed, showing that the top features were selected consistently, further enhancing the model's interpretability.\n\nThe use of radiomic features, which quantify aspects of the 3-dimensional morphology and grayscale distribution of nodules, adds another layer of transparency. These features can be directly linked to specific characteristics of the nodules, providing clinicians with actionable information. For example, features related to nodule size, shape, and texture can be interpreted in the context of known clinical indicators of malignancy.\n\nMoreover, the model's performance was compared with an established model based primarily on semantic nodule features, demonstrating that the inclusion of radiomic features improves predictive accuracy without sacrificing interpretability. The calibration and discrimination metrics of our model were superior, indicating that it not only predicts malignancy risk more accurately but also provides reliable risk estimates.\n\nIn summary, the model developed in this study leverages machine learning techniques that offer greater transparency and interpretability. The use of LASSO for feature selection and the incorporation of radiomic features ensure that the model's predictions are based on clinically meaningful and understandable variables. This transparency is crucial for clinical implementation and widespread adoption, as it allows clinicians to trust and act upon the model's outputs.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the malignancy of pulmonary nodules, specifically distinguishing between benign and malignant nodules. The performance of the model was evaluated using the area under the receiver operating characteristic curve (AUC), which is a metric commonly used for classification tasks. The model's ability to discriminate between malignant and benign lesions was assessed, along with its calibration, which compares the predicted risks to the observed risks. Three machine learning models were evaluated: penalized logistic regression (LASSO), Random Forest (RF), and Gradient Boosted Trees (XGBoost). The LASSO model was ultimately chosen for its superior calibration and discrimination performance. The final model, referred to as the INTEGRAL-Radiomics model, was fitted to the full training set and evaluated on a hold-out test set. The model's performance was compared to an established model, the PanCan nodule malignancy model, and it demonstrated better discrimination with a test-set AUC of 0.93 compared to 0.87 for the PanCan model. The model performed well across different types of nodules, including solid and subsolid nodules.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the model reported in the study is publicly available on GitHub. This includes example code that demonstrates how to use the model. The repository can be accessed at https://github.com/mattwarkentin/INTEGRAL-Radiomics. The availability of the source code allows other researchers to replicate the study, build upon the work, and integrate the model into their own research or clinical tools. The specific licensing terms for the use of the code are not mentioned, but it is common for such repositories to include a license file that outlines the permissions and restrictions for using the code.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to ensure the robustness and generalizability of the machine learning models developed. Initially, data were partitioned into training/validation and testing splits using group-based random sampling. This method ensured that all nodules for a participant were in a single set, preventing data leakage.\n\nRadiomic features were extracted and subjected to filtering to exclude low-quality and highly-redundant features. This step was crucial for enhancing the quality and relevance of the features used in the models.\n\nK-fold cross-validation was performed to identify the optimal machine learning model and the optimal set of hyperparameters. This technique involved dividing the data into k subsets, training the model on k-1 subsets, and validating it on the remaining subset. This process was repeated k times, with each subset serving as the validation set once.\n\nThe final machine learning model was fitted to the entire training dataset and then tested for out-of-sample performance using a hold-out test dataset. This approach allowed for an unbiased evaluation of the model's performance on unseen data.\n\nDiscrimination and calibration performance metrics were reported to assess the model's ability to distinguish between malignant and benign nodules and to evaluate the agreement between predicted and observed risks. The area under the receiver operating characteristic curve (AUC) was used to measure discrimination, while calibration was assessed by comparing predicted and observed risks within bins of predicted risks.\n\nThe model's performance was also compared with an established benchmark, the PanCan nodule malignancy model, to provide context and validate the improvements achieved. This comparison included evaluating the model's performance across different types of nodules, such as solid and subsolid nodules.",
  "evaluation/measure": "In the evaluation of our model, we focused on two primary performance metrics: the area under the receiver operating characteristic curve (AUC) and calibration metrics. The AUC was used to assess the model's ability to discriminate between malignant and benign lesions, with higher values indicating better performance. We also evaluated calibration by comparing the model-estimated risks to the observed risks, ensuring that the predicted probabilities align with the actual outcomes. This involved examining the ratio of expected to observed number of cancers and the difference between expected and observed numbers within bins of predicted risk.\n\nAdditionally, we reported sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), accuracy, and positive prevalence at various probability thresholds. Sensitivity measures the proportion of malignant nodules correctly identified, while specificity measures the proportion of benign nodules correctly identified. PPV indicates the proportion of positive predictions that are actually malignant, and NPV indicates the proportion of negative predictions that are actually benign. Accuracy reflects the overall correctness of the model's predictions, and positive prevalence shows the proportion of positive predictions out of the total predictions.\n\nThese metrics provide a comprehensive view of the model's performance, covering both its discriminative ability and its calibration. The inclusion of sensitivity, specificity, PPV, NPV, and accuracy at different thresholds ensures that the model's performance is evaluated across a range of clinical decision points, making the results highly relevant and applicable in real-world scenarios. This set of metrics is representative of standard practices in the literature, ensuring that our evaluation is rigorous and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we compared our model's performance with an established benchmark, the PanCan nodule malignancy model. This comparison was crucial to evaluate the effectiveness of our approach against a widely recognized standard. The PanCan model is a logistic regression model that incorporates demographics, medical history, and nodule characteristics.\n\nWe assessed model performance using two primary metrics: the area under the receiver operating characteristic curve (AUC) for discrimination and calibration metrics to compare model-estimated risks with observed risks. Our model demonstrated superior discrimination with a test-set AUC of 0.93 (95% CI: 0.90\u20130.96) compared to 0.87 (95% CI: 0.85\u20130.89) for the PanCan model. This indicates that our model is better at assigning higher risks to malignant lesions than to benign lesions.\n\nAdditionally, our model showed excellent calibration, with calibration ratios and differences that were closer to the ideal values compared to the PanCan model. This means that the predicted risks from our model more accurately reflect the observed risks.\n\nWe also compared clinically relevant metrics such as sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and accuracy at various probability thresholds. At nearly every threshold, our model outperformed the PanCan model, identifying fewer lesions as positive while maintaining higher sensitivity and specificity.\n\nIn summary, our comparison with the PanCan model highlights the improved performance of our radiomics-based approach, demonstrating its potential to enhance the management of screen-detected pulmonary nodules.",
  "evaluation/confidence": "The evaluation of our model's performance included the calculation of confidence intervals for key metrics. Specifically, we reported the area under the receiver operating characteristic curve (AUC) and calibration metrics with percentile-based bootstrap confidence intervals. This approach provides a range within which the true value of these metrics is likely to fall, giving a measure of the uncertainty around our estimates.\n\nIn comparing our model with the established PanCan nodule malignancy model, we found statistically significant differences in performance. Our model demonstrated superior discrimination, with a test-set AUC of 0.93 (95% CI: 0.90\u20130.96) compared to 0.87 (95% CI: 0.85\u20130.89) for the PanCan Model. The p-value for this comparison was 0.0002, indicating strong evidence that our model's performance is significantly better.\n\nAdditionally, we assessed calibration by comparing predicted and observed risks within bins of predicted risks. Our model showed excellent calibration, with calibration ratios and differences that were closer to the ideal values compared to the PanCan Model. These results suggest that our model not only discriminates better between malignant and benign nodules but also provides more accurate risk estimates.\n\nThe use of confidence intervals and statistical significance testing ensures that our claims of superiority are robust and not due to random chance. This rigorous evaluation process enhances the reliability and validity of our findings, making a strong case for the effectiveness of our approach in assessing pulmonary nodule malignancy.",
  "evaluation/availability": "The raw evaluation files used in the present study may be made available upon reasonable request to the Integrative Analysis of Lung Cancer Etiology and Risk (INTEGRAL) program, subject to approval by the Data Access Committee. This ensures that the data is accessible for further research while maintaining necessary controls.\n\nThe model reported in the study and example code are publicly available on GitHub. This provides transparency and reproducibility for the methods used, allowing other researchers to build upon the work. The GitHub repository can be accessed at [https://github.com/mattwarkentin/INTEGRAL-Radiomics](https://github.com/mattwarkentin/INTEGRAL-Radiomics)."
}