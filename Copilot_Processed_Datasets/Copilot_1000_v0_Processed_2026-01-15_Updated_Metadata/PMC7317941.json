{
  "publication/title": "LEAP: Using machine learning to support variant classification in a clinical setting.",
  "publication/authors": "Lai C, Zimmer AD, O'Connor R, Kim S, Chan R, van den Akker J, Zhou AY, Topper S, Mishne G",
  "publication/journal": "Human mutation",
  "publication/year": "2020",
  "publication/pmid": "32176384",
  "publication/pmcid": "PMC7317941",
  "publication/doi": "10.1002/humu.24011",
  "publication/tags": "- Machine Learning\n- Genetic Variants\n- Cancer\n- Cardiovascular Disorders\n- Model Performance\n- Feature Selection\n- Logistic Regression\n- Random Forest\n- Variant Classification\n- Cross-Validation",
  "dataset/provenance": "The dataset utilized in this study is derived from a high-quality variant classification database curated by trained variant scientists and approved by board-certified medical geneticists. This database reflects deep clinical testing experience in a set of genes evaluated for both hereditary cancer and cardiovascular disorders.\n\nThe dataset includes a total of 14,226 data points for hereditary cancer and 5,398 data points for cardiovascular disorders when considering multiclass classification models, which include pathogenic/likely pathogenic (P/LP), benign/likely benign (B/LB), and variants of uncertain significance (VUS). For binary classification models, the dataset consists of 1,855 data points for hereditary cancer and 918 data points for cardiovascular disorders, focusing only on P/LP and B/LB variants.\n\nThe dataset is not comparable in volume to some public databases but offers a consistent and critical variant classification protocol. It has been shown to be highly concordant with consensus ClinVar classifications, demonstrating a >98.5% concordance with participating genome centers. This suggests that the training set represents current best practices in the field.\n\nThe dataset includes features such as population frequency, splicing impact, and other genetic evidence, which are crucial for model training and validation. The use of high-quality, consistently classified data ensures that the model can learn from reliable signals, leading to higher quality predictions. The dataset has been used to train and validate various models, including LEAP_MODEL_1, LEAP_MODEL_2, LEAP_MODEL_3, and LEAP_MODEL_4, which have shown strong performance in classifying missense variants with high precision, recall, and specificity.",
  "dataset/splits": "In our study, we employed a 10-fold cross-validation approach for model assessment and comparison. This method involved splitting the dataset into 10 subsets, where the model was trained on 9 of these subsets and tested on the remaining one. This process was repeated 10 times, with each subset serving as the test set once. This approach ensured that every data point was used for both training and testing, providing a robust evaluation of model performance.\n\nFor the binary classification models, the dataset consisted of 1855 data points for cancer-associated genes and 918 data points for cardiovascular disorders. In the case of multiclass classification models, which included variants of uncertain significance (VUS), the dataset expanded to 14226 data points for cancer-associated genes and 5398 data points for cardiovascular disorders.\n\nAdditionally, we conducted gene holdout validation to assess model robustness. In this validation, the model was trained on variants from 23 out of 24 genes and tested on the held-out gene. This process was systematically repeated for all genes, ensuring that the model's performance was evaluated on unseen genes.\n\nFurthermore, we performed external holdout validation using an external set of 324 rare BRCA1 and BRCA2 variants, which were newly classified by the CAGI5 ENIGMA blind prediction challenge. This external validation provided an additional layer of assessment, ensuring that the model's performance was evaluated on data that was entirely separate from the training set.",
  "dataset/redundancy": "The datasets were split using a 10-fold cross-validation approach to assess model performance. This method ensures that the training and test sets are independent by rotating the data used for training and testing across 10 different folds. Each fold serves as the test set once, while the remaining nine folds are used for training. This process helps to mitigate overfitting and provides a more robust evaluation of the model's performance.\n\nTo further enforce independence and assess model robustness, gene holdout validation was employed. In this approach, the model was trained on a subset of genes, excluding one gene at a time, and then tested on the variants within the excluded gene. This systematic process was repeated for all genes, ensuring that the model's performance was evaluated on entirely new genes, thereby demonstrating its generalizability.\n\nAdditionally, external holdout validation was conducted using a set of 324 rare BRCA1 and BRCA2 variants newly classified by the CAGI5 ENIGMA blind prediction challenge. This external set was entirely separate from the training data, providing an independent assessment of the model's performance.\n\nThe distribution of the datasets used in this study reflects a deep clinical testing experience in the evaluated genes and the application of a consistent and critical variant classification protocol. The training labels were relatively imbalanced, with pathogenic rates varying across different conditions and classifications. For instance, the pathogenic rate in cancer was 1.7% including VUS and 12.8% excluding VUS, while in cardiovascular disorders, it was 2.3% including VUS and 13.3% excluding VUS. This imbalance was addressed by using precision-recall curves (AUPRC) as a more representative performance metric, which penalizes false positives more than the area under the receiver operating characteristic curve (AUROC).\n\nThe datasets used in this study are not directly comparable in volume to some public databases but offer high-quality variant classifications determined by trained variant scientists and approved by board-certified medical geneticists. This ensures that the training set represents current best practices and provides reliable signals for model training. The high concordance of the Color database with consensus ClinVar classifications further supports the quality and reliability of the datasets used.",
  "dataset/availability": "The data used in this study is not publicly released. The dataset includes high-quality variant classifications determined by trained variant scientists and approved by board-certified medical geneticists. This dataset is proprietary and reflects deep clinical testing experience in a specific set of genes. The data is not available in a public forum due to the sensitive nature of the information and the need to maintain the integrity of the variant classifications. The dataset is used internally for training and validating the LEAP model, and efforts have been made to ensure the robustness and generalizability of the model through various validation techniques, such as gene-holdout validation and external validation on holdout sets. The use of this proprietary dataset is enforced through internal protocols and guidelines to maintain data security and privacy.",
  "optimization/algorithm": "The machine-learning algorithms used in our study include logistic regression, random forest, and multiclass logistic regression. These are well-established algorithms in the field of machine learning and are not new. Logistic regression is a statistical method for analyzing a dataset in which there are one or more independent variables that determine an outcome. The logistic regression model predicts the probability of occurrence of a binary event utilizing a logistic function. Random forest is an ensemble learning method used for classification, regression, and other tasks that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Multiclass logistic regression is an extension of logistic regression to multi-class problems.\n\nThese algorithms were chosen for their robustness and ability to handle different types of data and feature sets. Logistic regression was used with L2 regularization to manage multicollinearity and prevent overfitting. Random forest was used for its ability to automatically adjust for correlated features and prioritize the most informative ones. Multiclass logistic regression was used for models that included variants of uncertain significance (VUS).\n\nThe focus of our publication is on the application of these algorithms to the specific problem of variant classification in the context of hereditary cancer and cardiovascular disorders. The algorithms themselves are well-documented in the machine learning literature, and our contribution lies in the novel application of these algorithms to this specific domain, as well as the development of new feature sets and model validation techniques tailored to this problem. Therefore, it was appropriate to publish this work in a journal focused on genetics and genomics rather than a machine learning journal.",
  "optimization/meta": "The Learning from Evidence to Assess Pathogenicity (LEAP) model is designed to predict the classification of missense variants by leveraging a wide array of underlying data used in manual variant classification. This includes functional prediction, splice prediction, evolutionary conservation, population frequency, protein domain information, co-occurring pathogenic variants, and individual and family health history.\n\nLEAP is a meta-predictor that integrates multiple types of evidence to make its predictions. It builds upon existing meta-predictors like REVEL, MetaSVM, and MetaLR, which primarily use functional predictors. However, LEAP goes beyond these by incorporating additional categories of evidence, such as population frequency and individual-level information.\n\nThe model's performance was evaluated using various feature sets and model types. For instance, the baseline model (LEAP_FEATURE_1) achieved a high performance of 94.6% AUROC and 70.0% AUPRC. By incorporating additional evidence categories, such as splicing impact features, the model's performance improved significantly. The best-performing model (LEAP_FEATURE_4) achieved 97.9% AUROC and 89.9% AUPRC.\n\nLEAP's training data consists of missense variants in genes associated with hereditary cancer and cardiovascular disorders. These variants were previously observed and classified in routine clinical testing. The model was trained and assessed separately for cancer and cardiovascular variants but shared the same modeling and validation framework.\n\nTo ensure the robustness of the model, various validation techniques were employed, including 10-fold cross-validation and gene holdout validation. The gene holdout validation involved training the model on a subset of genes and making predictions for variants in the held-out genes. This approach demonstrated the model's ability to generalize to unseen genes.\n\nIn summary, LEAP is a comprehensive meta-predictor that integrates multiple types of evidence to predict the pathogenicity of missense variants. Its performance was rigorously evaluated using various feature sets and validation techniques, demonstrating its robustness and generalizability.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the machine-learning algorithm could effectively learn from the input features. Numeric features were standardized by centering them at the median and scaling them to the interquartile range. This process helped to normalize the data, making it easier for the algorithm to detect patterns.\n\nMissing values were handled differently depending on the type of feature. For numeric features, the most frequent value was used to fill in any gaps. For categorical features, a \"missing\" label was introduced to indicate the absence of data. This approach allowed the model to learn from the presence or absence of certain categories.\n\nCategorical features were binarized, converting them into a format suitable for machine-learning algorithms. One categorical level was dropped for each feature to serve as the reference level during model training. This process resulted in k-1 binarized columns for a categorical feature with k levels. If a \"missing\" level existed, it was dropped as the reference level; otherwise, the first alphabetically sorted level was dropped.\n\nAdditional features were engineered to represent various classification criteria. Population frequency numeric features from gnomAD were grouped to create additional categorical features, such as log-scale and linear-scale minor allele frequency (MAF) groups. These groupings helped to capture the distribution of allele frequencies in the population.\n\nTo represent a variant's position relative to a clinically relevant transcript, the exon position of the variant was divided by the exon position at which the given transcript ends. This normalization allowed the model to understand the relative position of variants within transcripts.\n\nFurthermore, the potential splicing impact of variants was assessed using four RNA splicing impact algorithms: Human Splicing Finder, MaxEnt, NNSplice, and SpliceSiteFinder. The proportion of these algorithms indicating a significant difference between the variant and wild-type scores was calculated, giving each algorithm equal weight. This approach helped to integrate splicing information into the model.\n\nIn summary, our data encoding and preprocessing steps involved standardization, handling of missing values, binarization of categorical features, and engineering of additional features to capture relevant biological information. These steps were essential for preparing the data for effective machine-learning model training.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the feature categories included. We systematically evaluated the impact of different feature sets on model performance. The baseline model, which included functional predictors, had a certain number of parameters. As additional feature categories were incorporated, such as splicing impact, variant location, population frequency, and aggregated individual-level information, the number of parameters increased accordingly.\n\nThe selection of parameters was guided by the goal of improving model performance. We found that including additional evidence categories as features improved model performance to varying degrees. For instance, adding splicing impact features to the functional predictors led to initial performance improvements. The most significant improvements were observed when functional prediction, splicing prediction, and variant location features were combined. However, the inclusion of population frequency contributed only slightly, and aggregated individual-level information actually decreased model performance slightly.\n\nTo determine the optimal set of features and, consequently, the number of parameters, we compared the performance of models with different feature combinations using metrics such as AUROC and AUPRC. This iterative process allowed us to identify the feature set that yielded the best performance. The best-performing model in our feature comparison achieved high AUROC and AUPRC, indicating that the selected parameters were effective in capturing the relevant information for variant classification.",
  "optimization/features": "In the optimization process of our models, we utilized a comprehensive set of features to enhance predictive performance. The specific number of features (f) varied depending on the model and the feature categories included. For instance, our baseline model, LEAP_FEATURE_1, incorporated functional predictors, while subsequent models integrated additional categories such as splicing impact, variant location, population frequency, and aggregated individual-level information.\n\nFeature selection was a critical step in our approach. We systematically evaluated the contribution of different feature categories to model performance. This involved training binary L2-regularized logistic regression models on features from single categories to assess their individual impact. For example, we examined the performance of models trained on functional predictors alone, splicing impact features, and other categories to determine their significance.\n\nTo ensure the robustness of our feature selection process, we performed this evaluation using the training set only. This approach helped us avoid overfitting and ensured that the selected features were generalizable to unseen data. The inclusion of additional feature categories, such as splicing impact and variant location, significantly improved model performance, particularly in terms of the area under the precision-recall curve (AUPRC). This systematic feature selection process was essential in developing models that could accurately classify missense variants with high precision, recall, and specificity.",
  "optimization/fitting": "The fitting method employed in our study involved binary L2-regularized logistic regression models, which are well-suited for handling datasets with a large number of features relative to the number of training points. This regularization technique helps to mitigate overfitting by penalizing large coefficients, thereby reducing the model's complexity and preventing it from becoming too tailored to the training data.\n\nTo ensure that overfitting was not an issue, several validation strategies were implemented. Firstly, 10-fold cross-validation was used to assess the model's performance. This technique involves dividing the dataset into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. This method provides a robust estimate of the model's performance and helps to identify overfitting.\n\nAdditionally, gene holdout cross-validation was performed to assess the model's robustness to different genes. In this approach, the model was trained on variants from all but one gene and then tested on variants from the held-out gene. This process was repeated for each gene, ensuring that the model's performance was evaluated on entirely new genes, further validating its generalizability and ruling out overfitting.\n\nExternal holdout validation was also conducted using an external set of variants from the CAGI5 ENIGMA blind prediction challenge. This validation step ensured that the model's performance was assessed on data that was completely separate from the training set, providing an additional layer of confidence that overfitting was not a concern.\n\nTo address underfitting, the model's performance was evaluated using multiple metrics, including the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC). These metrics provide a comprehensive assessment of the model's ability to discriminate between pathogenic and benign variants. The inclusion of a wide range of feature categories, such as functional impact predictors, splicing impact predictors, variant location and domain, and population frequency, ensured that the model had access to a rich set of information, reducing the risk of underfitting.\n\nFurthermore, the use of L2 regularization in the logistic regression model helped to balance the trade-off between bias and variance, ensuring that the model was neither too simple (underfitting) nor too complex (overfitting). The regularization parameter was tuned using cross-validation to find the optimal value that minimized both bias and variance.\n\nIn summary, the fitting method involved the use of L2-regularized logistic regression models, validated through 10-fold cross-validation, gene holdout cross-validation, and external holdout validation. These strategies ensured that the model was neither overfitting nor underfitting, providing reliable and generalizable predictions.",
  "optimization/regularization": "In our study, we employed L2-regularized logistic regression models to prevent overfitting. This regularization technique, also known as ridge regression, adds a penalty equal to the square of the magnitude of coefficients to the loss function. By doing so, it discourages large coefficients and helps to keep the model simple, thereby reducing the risk of overfitting, especially when dealing with a large number of features.\n\nWe utilized this regularization method across various analyses, including feature category contribution assessments, gene holdout predictions, and feature comparison models. The consistent use of L2 regularization ensured that our models generalized well to unseen data, providing robust and reliable predictions.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the main text and supplementary materials. Specifically, the model validation section outlines the use of 10-fold cross-validation for assessing performance metrics such as AUROC and AUPRC. The feature comparison models are described in Table 2, which is referenced in the supplementary figures and text. Additionally, the supplementary tables provide full lists of features used for cancer and cardiovascular models, which are essential for replicating the experiments.\n\nThe model files themselves are not explicitly mentioned as being available for download, but the detailed descriptions of the models and their configurations should enable other researchers to replicate the work. The use of publicly available information in some models, such as LEAP_FEATURE_4, indicates a commitment to transparency and reproducibility. However, specific model files or scripts are not discussed in the provided context.\n\nRegarding the license, there is no explicit mention of licensing terms for the data or models. Typically, scientific publications do not include detailed licensing information for the methods and results presented, but they are generally intended for academic use and further research. For precise licensing details, one would need to refer to the original publication or contact the authors directly.",
  "model/interpretability": "The model, LEAP, is designed with a strong emphasis on interpretability, aiming to avoid the \"black box\" nature often associated with machine learning models. This transparency is crucial for its application in clinical genetics, where understanding the underlying logic of predictions is essential.\n\nLEAP achieves this by making the contributions of each specific evidence type for each variant clear to the human eye. This means that users can see exactly which features or types of evidence are influencing the model's predictions. For example, the model can highlight how functional predictions, splicing impact, variant location, and other factors contribute to the classification of a variant. This level of detail allows expert scientists to deeply understand and evaluate the reasoning behind LEAP's predictions. By providing this transparency, LEAP supports the usability and adoption of computational tools in clinical genetics, ensuring that predictions are not only accurate but also interpretable and trustworthy.",
  "model/output": "The model developed is a classification model, specifically designed to classify missense variants. It utilizes various machine learning techniques, including binary and multiclass logistic regression, as well as random forest models, to achieve high precision, recall, and specificity. The model's performance was validated using 10-fold cross-validation, gene holdout cross-validation, and external holdout validation from the ENIGMA challenge. Key metrics such as the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC) were used to assess the model's effectiveness. For instance, in a 10-fold cross-validation of missense variants in cancer-associated genes, the best model achieved 98.3% AUROC and 91.7% AUPRC. The model integrates multiple forms of evidence, including functional impact predictors, splicing impact predictors, variant location and domain, and population frequency, to emulate the classifications made by expert variant scientists. This approach ensures that the model is robust and reliable for clinical applications.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our models involved several rigorous methods to ensure robustness and generalizability. We primarily used 10-fold cross-validation to assess model performance, which involved splitting the data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process was repeated 10 times, each time with a different subset held out for validation. We evaluated the models using two key metrics: the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC). AUPRC was particularly important for our imbalanced datasets, as it penalizes false positives more severely than AUROC, providing a more representative performance metric.\n\nTo further validate the models, we employed a gene holdout procedure. This involved training the model on variants from all but one gene and then testing it on variants from the held-out gene. This process was repeated systematically for all genes, ensuring that the model's performance was consistent across different genes, even those it had not been explicitly trained on. The results from this procedure showed that the model maintained high AUROC and AUPRC, demonstrating its robustness.\n\nAdditionally, we conducted external holdout validation using an independent dataset of rare BRCA1 and BRCA2 variants classified by the CAGI5 ENIGMA blind prediction challenge. This external validation further confirmed the model's performance, with different versions of the model achieving high AUROC and AUPRC on this holdout set.\n\nOverall, our evaluation methods included cross-validation, gene holdout procedures, and external holdout validation, ensuring that our models were thoroughly tested and validated for robustness and generalizability.",
  "evaluation/measure": "In our evaluation, we primarily reported two key performance metrics: the Area Under the Receiver Operating Characteristic Curve (AUROC) and the Area Under the Precision-Recall Curve (AUPRC). These metrics were chosen to provide a comprehensive assessment of our models' performance.\n\nAUROC is a widely used metric that evaluates the model's ability to distinguish between positive and negative classes across all threshold levels. It provides a single scalar value that represents the trade-off between the true positive rate and the false positive rate. This metric is particularly useful when the classes are balanced.\n\nHowever, in our case, the training labels were relatively imbalanced, with a low pathogenic rate in both cancer and cardiovascular datasets. Therefore, we also reported AUPRC, which is a more representative performance metric for imbalanced datasets. AUPRC assesses precision, which is the ratio of true positives to the sum of true positives and false positives. It penalizes false positives more than AUROC, making it a more suitable metric for imbalanced datasets.\n\nBoth AUROC and AUPRC assess sensitivity, also known as recall, which is the ratio of true positives to the sum of true positives and false negatives. This metric is crucial for evaluating the model's ability to identify positive instances.\n\nIn addition to these metrics, we also assessed model robustness through gene holdout predictions and external holdout validation. For gene holdout predictions, we systematically withheld each gene from model training and evaluated the model's performance on variants in the withheld gene. This procedure helped us assess the model's generalizability to unseen genes. For external holdout validation, we evaluated our models on an external set of variants that were not used in model training. This procedure provided an independent assessment of our models' performance.\n\nThe reported metrics are representative of the literature, as AUROC and AUPRC are commonly used performance metrics in machine learning and bioinformatics. Moreover, the use of gene holdout predictions and external holdout validation is a standard practice in evaluating the robustness and generalizability of predictive models.",
  "evaluation/comparison": "A comparison to publicly available methods was performed using benchmark datasets. Specifically, the REVEL model was included as a baseline for comparison. REVEL is a well-known meta-predictor trained on functional predictors similar to those used in our initial model. However, it's important to note that REVEL's training set and model choice differed from ours, making a direct head-to-head comparison inequitable. Instead, the inclusion of REVEL served to compare differences in performance at varying levels of feature inclusion rather than absolute model performance.\n\nAdditionally, simpler baselines were considered. The baseline model, which included only functional predictors, achieved a reasonably high performance of 94.6% AUROC and 70.0% AUPRC. This baseline model was used to assess the incremental improvements gained by adding additional feature categories, such as splicing impact, variant location, and population frequency.\n\nThe performance of different models was also compared using all feature categories. This comparison included binary logistic regression, binary random forest, multiclass logistic regression (one-vs.-rest and multinomial methods), and multiclass random forest. Nonlinear models like random forest were of particular interest due to their potential to capture complex classification behavior more effectively. The comparison showed that nonlinear models could better differentiate between pathogenic and benign variants by considering interactions between features, such as population frequency and functional prediction scores.",
  "evaluation/confidence": "The evaluation of our models involved rigorous validation techniques to ensure the robustness and generalizability of our results. We employed 10-fold cross-validation to assess the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC). These metrics were chosen for their ability to provide a comprehensive evaluation of model performance, especially in the context of imbalanced datasets.\n\nThe AUROC and AUPRC metrics were used to evaluate the models' performance across different feature categories and model selections. For instance, our best model, LEAP_MODEL_2, achieved an AUROC of 98.3% and an AUPRC of 91.7% in a 10-fold cross-validation of missense variants in cancer-associated genes. These high scores indicate strong model performance.\n\nTo further validate our models, we conducted gene holdout predictions. This involved training the model on a subset of genes and then testing it on variants in the withheld genes. The results showed that the models were robust to this procedure, with gene holdout predictions achieving an AUROC of 96.8% and an AUPRC of 84.9%. This demonstrates the models' ability to generalize to new, unseen genes.\n\nAdditionally, we performed external validation using a holdout set of 324 rare BRCA1 and BRCA2 variants from the CAGI5 ENIGMA blind prediction challenge. The models submitted to this challenge achieved high AUROC and AUPRC scores, further confirming their performance and reliability.\n\nWhile specific confidence intervals for the performance metrics were not explicitly stated, the use of cross-validation and external validation provides a strong indication of the models' robustness and generalizability. The statistical significance of the results is supported by the consistent high performance across different validation techniques and datasets. This comprehensive evaluation approach ensures that our claims of model superiority are well-founded and reliable.",
  "evaluation/availability": "The data that support the findings in this study are available on request from the corresponding author. The data are not publicly available as they contain information that could compromise research participant privacy or consent. All reported variants have been submitted to ClinVar."
}