{
  "publication/title": "Machine Learning Models for ASCVD Risk Prediction in an Asian Population - How to Validate the Model is Important.",
  "publication/authors": "Hsiao YC, Kuo CY, Lin FJ, Wu YW, Lin TH, Yeh HI, Chen JW, Wu CC",
  "publication/journal": "Acta Cardiologica Sinica",
  "publication/year": "2023",
  "publication/pmid": "38022427",
  "publication/pmcid": "PMC10646597",
  "publication/doi": "10.6515/acs.202311_39(6).20230528a",
  "publication/tags": "- Atherosclerotic cardiovascular disease\n- Machine learning\n- Risk prediction model\n- Transfer learning\n- Cardiovascular risk\n- ASCVD risk scoring\n- Predictive modeling\n- Health care improvement\n- Medical data analysis\n- Clinical outcomes prediction",
  "dataset/provenance": "The dataset used in this study is derived from two Taiwanese cohorts: the T-SPARCLE and T-PPARCLE cohorts. These cohorts were assembled from 16 medical centers across Taiwan. The participants included in the study were men and women aged over 18 years who either had evidence of atherosclerotic cardiovascular disease (ASCVD) or had at least one major risk factor for ASCVD. The ASCVD criteria included coronary artery disease, cerebral vascular disease, transient ischemic attack, and peripheral atherosclerosis. Risk factors considered were diabetes mellitus, dyslipidemia, hypertension, smoking, older age, family history of premature coronary artery disease, and obesity.\n\nThe study population was split into two groups based on the time of enrollment: the early group, consisting of 7,362 individuals enrolled by April 8, 2013, and the late group, consisting of 3,145 individuals enrolled after that date. The baseline characteristics of these groups were compared, revealing differences in various health metrics and demographics.\n\nThe dataset includes a range of variables such as age, sex, smoking status, systolic blood pressure, diabetes mellitus, coronary artery disease, cerebrovascular disease, peripheral artery occlusive disease, HDL-C, total cholesterol, and estimated glomerular filtration rate. Some variables, like C-reactive protein and abdominal aneurysm, were omitted due to a lack of data in the cohort.\n\nThe study utilized both traditional statistical analysis and machine learning techniques to develop risk prediction models. The performance of these models was evaluated using metrics such as the area under the receiver operating characteristic curve (AUC), average precision score, and concordance index. The models were validated through methods like ten-fold cross-validation and temporal validation to ensure their robustness and generalizability.",
  "dataset/splits": "In our study, we employed multiple data splitting strategies to ensure robust model validation. For binary classification methods, we initially performed ten-fold cross-validation using the entire dataset. This approach involved dividing the data into ten subsets, where nine subsets were used for training and one subset was used for testing. This process was repeated ten times, with each subset serving as the test set once.\n\nAdditionally, we implemented a temporal validation method. In this approach, 70% of the data were allocated for training (model development), and the remaining 30% were reserved for testing. The training set consisted of 7,362 data points, while the test set comprised 3,145 data points. The early group, which included patients enrolled by April 8, 2013, was used for training, and the late group, which included patients enrolled after this date, was used for testing. This temporal split helped to simulate real-world scenarios where models are developed on historical data and tested on more recent data.\n\nFor time-to-event survival analysis, the data were randomly split into 70% for training and 30% for testing. Within the training set, 70% of the data were used for model development, and the remaining 30% were used for model tuning and selection. This strategy ensured that the models were not only trained on a substantial amount of data but also fine-tuned to improve their performance.\n\nIn summary, our study utilized ten-fold cross-validation for initial model assessment, followed by a temporal validation approach with a 70%-30% train-test split. For time-to-event analysis, a nested split within the training set was employed to further enhance model performance.",
  "dataset/redundancy": "In our study, datasets were split using two primary methods to ensure robust validation of our models. For binary classification, we employed ten-fold cross-validation, where the data was divided into ten subsets. Nine of these subsets were used for training, and the remaining one was used for testing. This process was repeated ten times, with each subset serving as the test set once. Additionally, we used a temporal validation method, where 70% of the data, enrolled earlier, were used for training, and the remaining 30%, enrolled later, were used for testing. This approach helps to simulate real-world scenarios where models are applied to future data.\n\nTo ensure the independence of training and test sets, especially in the temporal validation method, we strictly enforced that the test set contained data from a time period after the training set. This temporal separation helps to mimic the practical application of models, where future data is predicted based on past observations.\n\nThe distribution of our datasets compares favorably with previously published machine learning datasets in cardiovascular research. For instance, our study included a cohort with a mix of patients with and without cardiovascular disease (CVD), similar to other large-scale studies. The sample size of 10,507 is substantial, allowing for robust model training and validation. Moreover, the variables used in our models were carefully selected to include clinically relevant factors, ensuring that our findings are applicable and generalizable to real-world settings. This approach aligns with studies from the UK and Korea, which also utilized machine learning models for similar predictions.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are primarily tree-based and ensemble methods. Specifically, we employed eXtreme Gradient Boosting (XGBoost), random forest, and multilayer perceptron (MLP) classifiers. These algorithms are well-established in the field of machine learning and are known for their effectiveness in handling complex datasets.\n\nXGBoost is an ensemble method that uses gradient boosting for optimization, making it highly efficient and capable of handling missing data. It is particularly noted for its superior accuracy in binary outcome classification tasks. Random forest models are also popular due to their speed in training and good accuracy. They are an ensemble of decision trees, which helps in reducing overfitting and improving the model's robustness.\n\nThe MLP classifier, a type of neural network, is another commonly used model that can exhibit excellent performance. However, it is known for the difficulty in tuning its hyperparameters, which can be a challenge during the model development process.\n\nThese algorithms are not new and have been extensively studied and applied in various domains, including healthcare. The choice to use these established algorithms in our study was driven by their proven effectiveness and reliability in similar predictive tasks. The focus of our work is on applying these algorithms to develop cardiovascular risk prediction models, rather than introducing new machine-learning algorithms. Therefore, publishing in a machine-learning journal was not the primary objective, as our contributions lie in the application and validation of these models in a specific medical context.",
  "optimization/meta": "In our study, we did not employ a meta-predictor approach. Instead, we focused on individual machine learning models to develop ASCVD risk scoring systems. The models we utilized included XGBoost, random forest, and MLP classifiers, among others. Each of these models was trained and validated independently using our dataset.\n\nXGBoost often demonstrated superior accuracy in binary outcome classification compared to other models. Random forest models were noted for their speed in training and good accuracy, making them a popular choice. The MLP classifier, while capable of excellent performance, presented challenges in hyperparameter tuning.\n\nOur validation methods included ten-fold cross-validation and temporal validation, ensuring that the models were robust and generalizable. The temporal validation approach involved splitting the data based on the time of enrollment, which is analogous to external validation and helps in making the prediction models more applicable to new cohorts.\n\nWe did not combine the outputs of different machine learning algorithms to create a meta-predictor. Instead, we evaluated each model's performance individually and compared them to traditional methods. This approach allowed us to assess the strengths and limitations of each model in predicting ASCVD risk.",
  "optimization/encoding": "In our study, we recorded 22 clinical variables, which were selected based on established guidelines and studies relevant to atherosclerotic cardiovascular disease (ASCVD) risk stratification and management. These variables included both categorical and continuous data, such as age, sex, history of ASCVD, revascularization procedures, peripheral arterial occlusive disease, stroke, smoking status, systolic blood pressure, congestive heart failure, diabetes mellitus, renal function, body mass index, lipid profiles, and medication use.\n\nFor the machine learning models, these variables were encoded and pre-processed as follows:\n\nCategorical variables, such as sex, smoking status, and medication use, were encoded using one-hot encoding. This method creates binary columns for each category, allowing the models to interpret these variables effectively.\n\nContinuous variables, like age, systolic blood pressure, and lipid profiles, were standardized to have a mean of zero and a standard deviation of one. This process, known as z-score normalization, helps to ensure that all features contribute equally to the model's learning process, preventing variables with larger scales from dominating the optimization.\n\nMissing data were handled by imputation with the mean values of the respective variables. This approach helps to maintain the integrity of the dataset and ensures that all records can be used for training the models.\n\nAdditionally, we performed ten-fold cross-validation to assess the performance of our models. This technique involves splitting the data into ten subsets, training the model on nine subsets, and validating it on the remaining subset. This process is repeated ten times, with each subset serving as the validation set once, providing a robust estimate of the model's performance.\n\nFor the temporal validation method, we split the data into a training set (early group) and a test set (late group) based on the time of enrollment. This approach helps to evaluate the model's performance on data that was not available during the training phase, simulating a real-world scenario where the model is applied to future data.\n\nIn summary, our data encoding and pre-processing steps involved one-hot encoding for categorical variables, z-score normalization for continuous variables, mean imputation for missing data, and the use of cross-validation techniques to ensure the robustness and generalizability of our machine learning models.",
  "optimization/parameters": "In our study, the number of input parameters (p) varied depending on the specific model and its requirements. For instance, the logistic regression model utilized five key features: PAOD, DM, TG, heart failure, and ASCVD. Other models, such as XGBoost, incorporated different sets of important features, including eGFR, age, ASCVD, BMI, and DM. The selection of these parameters was driven by their significance in predicting the outcomes, as determined through feature importance analyses using permutation methods repeated 1000 times.\n\nThe choice of parameters was also influenced by the nature of the data and the specific algorithms employed. For example, the MLP model focused on age, systolic blood pressure, revascularization, DM, and ARB/ACEi use, while the random forest model highlighted age, eGFR, ASCVD, BMI, and HDL. These selections were made to ensure that the models could capture the most relevant information from the dataset, thereby enhancing their predictive performance.\n\nIt is important to note that the performance of these models could be further improved by incorporating additional clinical variables and advanced biomarkers. Future work may involve expanding the set of input parameters to include genetic profiles, novel agents, and imaging data, which could potentially enhance the prediction accuracy of the models.",
  "optimization/features": "The study utilized a variety of clinical variables as input features for the models. The specific features varied depending on the model, but generally included demographic information, medical history, and laboratory results. For instance, the logistic regression model identified PAOD, DM, TG, heart failure, and ASCVD as the five most important features. The XGBoost model highlighted eGFR, age, ASCVD, BMI, and DM. The MLP model focused on age, systolic blood pressure, revascularization, DM, and ARB/ACEi use. The random forest model considered age, eGFR, ASCVD, BMI, and HDL.\n\nFeature selection was performed using a permutation method repeated 1000 times to determine the importance of each feature. This process was conducted to ensure that the most relevant features were included in the models. The feature selection was done using the training set only, adhering to best practices to prevent data leakage and maintain the integrity of the validation process. The number of features used as input varied by model, but the selection process ensured that the most impactful variables were prioritized.",
  "optimization/fitting": "In our study, we employed several machine learning models, including XGBoost, random forest, and multilayer perceptron (MLP), to predict outcomes related to atherosclerotic cardiovascular disease (ASCVD). The number of parameters in these models was indeed larger than the number of training points, which could potentially lead to overfitting. To mitigate this risk, we implemented several strategies.\n\nFirstly, we used ten-fold cross-validation to ensure that our models generalized well to unseen data. This technique involves splitting the data into ten subsets, training the model on nine subsets, and validating it on the remaining subset. This process is repeated ten times, with each subset serving as the validation set once. By averaging the performance across all folds, we obtained a more reliable estimate of the model's performance.\n\nAdditionally, we performed temporal validation, where the data was split based on the time of enrollment. This approach is analogous to external validation and helps to make the prediction models more generalizable. We used 70% of the data for training and 30% for testing, with the early group used for training and the late group for testing. This method helped to rule out overfitting by ensuring that the model performed well on data from a different time period.\n\nTo address the risk of underfitting, we carefully selected and tuned the hyperparameters of our models. For instance, in the XGBoost model, we optimized parameters such as the learning rate, maximum depth of the trees, and the number of boosting rounds. Similarly, for the random forest model, we tuned parameters like the number of trees and the maximum depth of the trees. For the MLP model, we adjusted the number of hidden layers and neurons, as well as the learning rate.\n\nFurthermore, we compared the performance of our models with a traditional logistic regression model and the SMART score, a Dutch scoring system. The fact that our models, particularly XGBoost and random forest, showed numerically higher ROC-AUC values and average precision scores suggests that they were not underfitting the data.\n\nIn summary, we employed cross-validation, temporal validation, and careful hyperparameter tuning to address the risks of overfitting and underfitting in our models. These strategies helped to ensure that our models were robust and generalizable to new data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the primary methods used was cross-validation, specifically ten-fold cross-validation. This technique involves dividing the dataset into ten subsets, training the model on nine of them, and validating it on the remaining one. This process is repeated ten times, with each subset serving as the validation set once. This approach helps to ensure that the model generalizes well to unseen data.\n\nAdditionally, we utilized temporal validation, which involves splitting the data based on the time of enrollment. This method is analogous to external validation and helps to make the prediction models more generalizable. In our study, 70% of the data were used for training, and 30% were used for testing. This temporal split helps to simulate real-world scenarios where the model is applied to data collected at a later time.\n\nWe also performed feature importance analysis using a permutation method, which involves randomly shuffling the values of each feature and observing the change in the model's performance. This helps to identify which features are most important and ensures that the model is not overly reliant on any single feature.\n\nFurthermore, we compared the performance of our machine learning models with traditional scoring systems like the SMART score. This comparison helps to validate the performance of our models and ensures that they are not overfitting to the specific dataset used in the study.\n\nIn summary, we implemented cross-validation, temporal validation, feature importance analysis, and comparisons with traditional scoring systems to prevent overfitting and ensure the robustness of our models.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are not explicitly detailed in the provided information. However, the models employed, such as XGBoost and random forest, underwent ten-fold cross-validation, and their performance scores were averaged. Additionally, a temporal validation method was used, where 70% of the data were utilized for training and 30% for testing based on the enrollment time. This approach involved model development in the training set and validation in the test set.\n\nRegarding model files and optimization parameters, specific details are not provided. The analyses were conducted using SAS 9.4 software and Python with various open-source libraries, including Matplotlib and XGBoost. The use of these tools suggests that the methods and potentially the configurations could be replicated, but the exact parameters and schedules are not outlined.\n\nFor access to the models and configurations, it is not clear if they are publicly available or under what license they might be shared. Typically, such information would be included in supplementary materials or a dedicated repository, but this is not specified in the given context. Therefore, while the general approach and tools are known, the specific hyper-parameter configurations and optimization details are not readily available.",
  "model/interpretability": "The models employed in our study encompass a range of machine learning algorithms, each with varying degrees of interpretability. Logistic regression, for instance, is inherently transparent, providing clear insights into the relationships between input features and the predicted outcome. The coefficients in logistic regression models indicate the direction and magnitude of the effect of each feature on the outcome, making it straightforward to interpret which variables are most influential.\n\nIn contrast, models like XGBoost, random forest, and multi-layer perceptron (MLP) are often considered black-box models due to their complexity and the non-linear relationships they can capture. However, techniques such as feature importance and permutation methods were used to enhance interpretability. For example, in the XGBoost model, the most important features identified were eGFR, age, ASCVD, BMI, and DM. Similarly, random forest highlighted age, eGFR, ASCVD, BMI, and HDL as key features. These methods allow us to understand which variables contribute most to the model's predictions, even if the underlying decision processes are not as transparent as in logistic regression.\n\nThe MLP model, while more opaque due to its neural network structure, also benefited from feature importance analysis. Age, systolic blood pressure, revascularization, DM, and ARB/ACEi use were identified as significant features. This approach helps in demystifying the model's decisions to some extent, although it does not provide the same level of clarity as logistic regression.\n\nOverall, while some of our models are more interpretable than others, we have employed various techniques to ensure that the key drivers of predictions can be understood and communicated effectively. This balance between model complexity and interpretability is crucial for practical application and stakeholder trust.",
  "model/output": "The model employed in this study is primarily focused on classification tasks, specifically binary classification and time-to-event analysis. For binary classification, we utilized four different algorithms: logistic regression, eXtreme Gradient Boosting (XGBoost), multilayer perceptron (MLP), and random forest. These models were constructed to classify patients into those who experienced a major adverse cardiovascular event (MACE) within one year and those who did not.\n\nIn the binary classification methods, when the entire group was used for ten-fold cross-validation, XGBoost and random forest demonstrated numerically higher ROC-AUC values and average precision compared to other models. Logistic regression, XGBoost, MLP, and random forest each identified different sets of important features, reflecting the varied ways these models interpret the data.\n\nFor time-to-event analyses, we conducted survival analyses using Cox proportional hazards regression and a deep learning model called DeepHit. The Cox regression model showed a concordance index of 0.69 in the training set and 0.70 in the test set, indicating its performance in predicting the time to MACE. The deep learning model had a concordance index of 0.65 in both the training and test sets, while the gradient boost survival model achieved a concordance index of 0.70 in both sets.\n\nThe performance of these models varied across different validation methods. For temporal validation, XGBoost and random forest showed better performance in the training set but experienced a drop in the test set. In contrast, logistic regression and MLP models performed slightly better in the test set than in the training set.\n\nOverall, the models demonstrated modest performance, with room for improvement through further tuning and the incorporation of additional clinical variables and larger sample sizes. The use of machine learning algorithms in this study highlights their potential in developing risk prediction models for atherosclerotic cardiovascular disease, although challenges such as model robustness and explainability remain areas for future development.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models developed in this study is not publicly released. However, the analyses were performed using widely available software. Specifically, SAS 9.4 and Python (version 3.3.4) were utilized, along with open-source libraries such as Matplotlib (version 3.6.9) and XGBoost (version 1.5.1). These tools are accessible to the public and can be used to replicate or build upon the methods described. Unfortunately, there is no executable, web server, virtual machine, or container instance provided for direct use of the algorithm.",
  "evaluation/method": "The evaluation of the models involved several rigorous methods to ensure their performance and generalizability. For binary classification, ten-fold cross-validation was performed, where the data was split into ten parts, with nine parts used for training and one part for testing, repeated ten times. This process was repeated 1000 times using a permutation method to assess feature importance. Additionally, a temporal validation approach was employed, where 70% of the data, enrolled earlier, was used for training, and the remaining 30%, enrolled later, was used for testing. This method is analogous to external validation and helps in making the prediction models more generalizable.\n\nFor time-to-event survival analysis, the data was randomly split into 70% for training and 30% for testing. Within the training set, 70% was used for model development, and the remaining 30% was used for model tuning and selection. The performance of the models was measured using the concordance index for time-to-event analysis and the area under the receiver operating characteristic curve (ROC-AUC) and average precision for binary classification. The models evaluated included XGBoost, random forest, multilayer perceptron (MLP), logistic regression, and the SMART score. The results showed that XGBoost and random forest had numerically higher ROC-AUC values and average precision in the ten-fold cross-validation. However, in the temporal validation, the performance of these models dropped in the test set, indicating the need for further model adjustment before applying them to new cohorts. The SMART score showed a more stable performance, suggesting the importance of external or temporal validation for confirming the robustness of machine learning models.",
  "evaluation/measure": "For the binary outcome analytic model, the performance was evaluated using the area under the receiver operating characteristic (ROC) curve, commonly referred to as the AUC. This metric provides a single scalar value that represents the ability of the model to distinguish between the positive and negative classes across all possible classification thresholds. Additionally, the average precision score was calculated. This metric is the area under the precision-recall curve and is particularly useful when the classes are imbalanced, as it focuses on the performance of the model in predicting the positive class.\n\nFor the time-to-event model, the performance was assessed using the concordance index. This index measures the model's ability to correctly order pairs of subjects with respect to their event times. A concordance index of 1 indicates perfect prediction, while a value of 0.5 suggests no discriminative ability beyond random chance.\n\nThe choice of these performance metrics is representative of common practices in the literature. The AUC and average precision are widely used for evaluating binary classification models, especially in medical and healthcare applications where class imbalance is prevalent. The concordance index is a standard metric for survival analysis, providing a clear and interpretable measure of model performance in time-to-event predictions. These metrics collectively offer a comprehensive evaluation of the models' predictive capabilities, ensuring that the results are robust and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we compared the performance of various machine learning models to assess their effectiveness in predicting cardiovascular outcomes. We employed several models, including XGBoost, random forest, logistic regression, and a multi-layer perceptron (MLP) neural network. Each of these models underwent ten-fold cross-validation to ensure robust performance evaluation.\n\nFor binary classification tasks, we found that XGBoost and random forest models exhibited numerically higher ROC-AUC values and average precision scores compared to other models. Specifically, XGBoost achieved an ROC-AUC of 0.72 and an average precision of 0.18, while the random forest model had an ROC-AUC of 0.73 and an average precision of 0.17. These results indicate that ensemble methods like XGBoost and random forest may offer superior performance in this context.\n\nIn addition to these advanced models, we also compared our results with a simpler baseline model, the SMART score, which is a Dutch scoring system. The SMART score, using variables such as age, sex, smoking status, and various clinical measurements, had an ROC-AUC of 0.70 and an average precision of 0.04. This comparison highlights that while simpler models can provide a baseline for performance, more complex machine learning models can offer improved predictive accuracy.\n\nFor time-to-event survival analysis, we utilized models such as Cox proportional hazards regression, DeepHit, and a gradient boosted survival model. The Cox regression model demonstrated a concordance index of 0.69 in the training set and 0.70 in the test set, indicating consistent performance. The gradient boosted survival model also showed a concordance index of 0.70 in both the training and test sets, suggesting its robustness in handling time-to-event data.\n\nOverall, our comparison of different models and baselines reveals that advanced machine learning techniques can significantly enhance the prediction of cardiovascular outcomes. While simpler models like the SMART score provide a useful benchmark, the superior performance of models like XGBoost and gradient boosted survival methods underscores their potential for improving patient care and risk assessment.",
  "evaluation/confidence": "The performance metrics in our study include confidence intervals to provide a range within which the true value is likely to fall. For instance, the ROC-AUC values for XGBoost and random forest models are reported with their respective confidence intervals, such as 0.72 (0.68-0.76) and 0.73 (0.69-0.77). Similarly, the average precision scores also come with confidence intervals, like 0.18 (0.13-0.23) for XGBoost and 0.17 (0.12-0.22) for random forest.\n\nThe concordance index for the time-to-event models also includes confidence intervals obtained through bootstrapping of the testing data. For example, the Cox regression model had a concordance index of 0.69 in the training set and 0.70 in the test set, with confidence intervals provided for these estimates.\n\nStatistical significance is considered in our analysis to ensure that the observed differences in performance are not due to random chance. The use of confidence intervals helps in assessing the statistical significance of the results. For example, if the confidence intervals of two models do not overlap, it suggests that the difference in their performance is statistically significant.\n\nIn our temporal validation approach, we observed that the performance of some models, like XGBoost, dropped in the test set compared to the training set. This indicates that while the models showed promising results in the training phase, their generalization to new data needs further improvement. The differences in baseline risk factors and event rates between the early and late groups also highlight the importance of careful validation and adjustment before applying the models to new cohorts.\n\nOverall, the inclusion of confidence intervals and the consideration of statistical significance in our analysis provide a robust evaluation of the models' performance and their potential superiority over baselines. However, further tuning and validation are necessary to enhance the models' generalization and stability.",
  "evaluation/availability": "Not enough information is available."
}