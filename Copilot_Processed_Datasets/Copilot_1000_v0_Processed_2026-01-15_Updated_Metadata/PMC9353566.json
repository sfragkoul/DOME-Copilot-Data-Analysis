{
  "publication/title": "Development of Various Diabetes Prediction Models Using Machine Learning Techniques.",
  "publication/authors": "Shin J, Kim J, Lee C, Yoon JY, Kim S, Song S, Kim HS",
  "publication/journal": "Diabetes & metabolism journal",
  "publication/year": "2022",
  "publication/pmid": "35272434",
  "publication/pmcid": "PMC9353566",
  "publication/doi": "10.4093/dmj.2021.0115",
  "publication/tags": "- Machine Learning\n- Diabetes Prediction\n- Cross-Validation\n- Hyperparameter Tuning\n- Medical Records\n- Gradient Boosting\n- Random Forest\n- Diabetes Prevention\n- Health Checkups\n- Fasting Blood Glucose",
  "dataset/provenance": "The dataset used in this study was sourced from the electronic medical records of the Health Promotion Center at Seoul St. Mary\u2019s Hospital, spanning from 2009 to 2018. The medical records contained information on approximately 134,691 individuals. Among these, 3,952 were diagnosed with diabetes. The study included subjects who underwent at least two full checkups. Diabetic patients were defined based on specific criteria: self-reported diabetes, use of glucose-lowering agents, or specific blood test results. Prediabetes was defined by fasting glucose levels between 100 and 125 mg/dL or HbA1c levels between 5.7% and 6.4%. Non-diabetic subjects were randomly selected from those with fasting glucose levels below 126 mg/dL and HbA1c below 6.5%, including both normoglycemic and prediabetic individuals.\n\nThe dataset leveraged various parameters from health checkups, aiming to develop valid and applicable diabetes prediction models. Two sets of variables were used: one comprising 62 easily accessible examination results from the health promotion center, and another comprising 27 variables included in national routine health checkups. This approach allowed for the creation of models that could be applied to both intensive private health checkups and simplified national health examinations. The study utilized machine learning techniques, specifically gradient boosting and random forest algorithms, to develop and validate these models. The internal validation was performed using the stratified 10-fold cross-validation method, ensuring robust and reliable model performance.",
  "dataset/splits": "The dataset was divided into multiple splits for model training and validation. Specifically, a stratified 10-fold cross-validation method was employed. This involved randomly dividing the total number of patients into 10 subsets. In each fold, nine of these subsets were used as training sets, and the remaining one subset served as the validation set. This process was repeated 10 times, ensuring that each subset was used as the validation set exactly once.\n\nFor the diabetes prediction models, the dataset included approximately 134,691 individuals, of which 3,952 were diagnosed with diabetes. The models were designed to predict diabetes development under different conditions. Model-1 and Model-2 were 2-year and 1-year prediction models, respectively, for non-diabetic subjects. Model-1 included 752 diabetic subjects with data from the previous 24 months and an equal number of non-diabetic subjects. Model-2 included 641 diabetic subjects with data from the previous 12 months and an equal number of non-diabetic subjects.\n\nModel-3 was a 1-year prediction model for prediabetic subjects, including 519 diabetic subjects who were prediabetic 12 months before diagnosis and an equal number of non-diabetic subjects. Model-4 was also a 1-year prediction model for prediabetic subjects but included an additional learning step to differentiate between data from one and two years before diagnosis. This model included 281 diabetic subjects with data from the previous 24 months and an equal number of prediabetic subjects.\n\nThe non-diabetic subjects were randomly selected from the remaining individuals without diabetes, ensuring that the number of non-diabetic subjects matched the number of diabetic subjects in each model. This approach helped in maintaining a balanced dataset for training and validation purposes.",
  "dataset/redundancy": "The datasets were split using the stratified 10-fold cross-validation method. This involved randomly dividing the total number of patients into 10 subsets. For each fold, nine of these subsets were used as training sets, and the remaining one was used as the validation set. This process was repeated 10 times, ensuring that each subset served as the validation set exactly once. This method helps to ensure that the training and test sets are independent and that the model's performance is evaluated on unseen data.\n\nTo enforce independence between training and test sets, the data was split such that each validation set contained data that was not used in the training process for that particular fold. This was achieved by randomly but evenly splitting 90% of the subjects into training datasets and using the remaining 10% as the validation dataset. The cross-validation process was repeated 10 times, and the mean and standard deviation of the results were calculated to provide a robust evaluation of the model's performance.\n\nThe distribution of the datasets used in this study is comparable to previously published machine learning datasets in the sense that they aim to include a diverse range of variables and a large number of subjects. The datasets included information such as age, sex, medication use, underlying diseases, family history, physical examinations, and laboratory results. This comprehensive approach ensures that the models are trained on a wide variety of data points, which is crucial for developing accurate and reliable prediction models.\n\nThe datasets were designed to include both diabetic and non-diabetic subjects, with the number of subjects in each group being adjusted to be the same in each model. This balancing act helps to mitigate any potential biases that could arise from an imbalanced dataset. Additionally, the datasets were extracted from electronic medical records of a health promotion center, ensuring that the data is representative of real-world health checkup findings. This approach aligns with the goal of creating valid and applicable diabetes prediction models that can be used in clinical practice.",
  "dataset/availability": "The data used in this study were not released in a public forum. The study utilized an electronic medical record database from the Health Promotion Center of Seoul St. Mary\u2019s Hospital, spanning from 2009 to 2018. Privacy protection measures were strictly enforced, including encryption of all data and the use of random temporary identifications for examinees. Only anonymous data were available to observers and analysts, ensuring that personal information was not collected or accessible. This approach was approved by the Institutional Review Board of the Catholic University of Korea, Seoul St. Mary\u2019s Hospital, which waived the need for informed consent due to the anonymous nature of the data.",
  "optimization/algorithm": "The optimization algorithm employed in our study utilized well-established machine-learning techniques. Specifically, gradient boosting algorithms were used for three of the models, while a random forest algorithm was applied to the fourth model. These algorithms are widely recognized and have been extensively validated in various predictive modeling tasks.\n\nThe choice of these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to capture intricate patterns within the data. Gradient boosting is known for its robustness in improving predictive accuracy by combining multiple weak learners, while random forests offer advantages in terms of interpretability and handling high-dimensional data.\n\nThese algorithms are not new; they have been extensively studied and applied in numerous fields, including healthcare. The decision to use these established methods was based on their reliability and the extensive body of research supporting their use. Publishing in a diabetes-focused journal allowed us to highlight the specific application of these algorithms to diabetes prediction, demonstrating their practical utility in a clinical context. This approach ensures that the models developed are not only theoretically sound but also practically applicable in real-world healthcare settings.",
  "optimization/meta": "The models developed in this study do not function as meta-predictors. They are standalone machine learning models designed to predict the development of diabetes under various conditions. Specifically, four distinct models were created: Model-1 and Model-2 predict diabetes development after 2 and 1 years, respectively, in subjects without diabetes. Model-3 predicts diabetes development after 1 year in prediabetic subjects. Model-4 also predicts diabetes development after 1 year in prediabetic subjects but incorporates learning from the differences observed between one and two years before diabetes diagnosis.\n\nThe machine-learning methods used in these models include gradient boosting algorithms for Models 1, 2, and 3, and random forest algorithms for Model 4. The hyperparameters for each model were selected through a random search process. The training data for these models were derived from electronic medical records, ensuring that the data used for training and validation were independent through the use of stratified 10-fold cross-validation. This method involved randomly splitting the dataset into training and validation sets multiple times to evaluate the models' performance on unseen data.",
  "optimization/encoding": "The data used for the machine-learning algorithms were derived from electronic medical records, specifically from the Health Promotion Center of Seoul St. Mary\u2019s Hospital between 2009 and 2018. The dataset included subjects who underwent at least two full checkups. The variables used to develop the diabetes prediction models were divided into two sets. The first set comprised 62 easily accessible variables from commonly performed examinations at the health promotion center. The second set included 27 of these 62 variables, which are recorded in national health checkups, to increase the clinical applicability of the models.\n\nThe data encoding process involved several steps to ensure the variables were suitable for the machine-learning algorithms. For categorical variables, such as sex and medication use, encoding techniques like one-hot encoding or label encoding were likely employed to convert them into a numerical format. Continuous variables, such as age, laboratory results, and physical examination measurements, were normalized or standardized to ensure they were on a similar scale, which is crucial for algorithms like gradient boosting and random forest.\n\nThe dataset was split into training and validation sets using a stratified 10-fold cross-validation method. This method involved randomly dividing the total number of patients into 10 subsets, with nine subsets used for training and one subset used for validation. This process was repeated 10 times to ensure the model's performance was evaluated across different subsets of the data.\n\nAdditionally, the data were encrypted, and personal information was not collected to protect privacy. Subjects were assigned random temporary identifications, and only anonymous data were available to observers and analysts. This ensured that the data used for the machine-learning algorithms were de-identified and compliant with ethical standards.",
  "optimization/parameters": "In our study, we utilized two sets of variables to develop diabetes prediction models. The first set comprised 62 easily accessible variables from commonly performed examinations at a health promotion center. The second set consisted of 27 variables, which were selected from the initial 62 variables and are recorded in national health checkups. These variables included a mix of demographic information, such as age and sex, medication use, underlying diseases, family history, physical examinations, and laboratory results.\n\nThe selection of these variables aimed to balance the inclusion of as many relevant factors as possible while ensuring practicality and clinical applicability. The 27-variable set was chosen to enhance the model's usability in broader clinical settings, where not all 62 variables might be readily available.\n\nThe performance of our models was evaluated using both sets of variables. Notably, all prediction models performed better with the 62 variables compared to the 27 variables, demonstrating an AUC difference of over 0.04 between them. This indicates that while the 27-variable model is more practical, the 62-variable model provides more accurate predictions.",
  "optimization/features": "In the optimization process of our diabetes prediction models, we utilized two distinct sets of input features. The first set comprised 62 easily accessible variables derived from commonly performed examinations at a health promotion center. The second set was a more streamlined version, consisting of 27 variables that are routinely recorded in national health checkups. This approach allowed us to assess the impact of feature selection on model performance.\n\nFeature selection was indeed performed to create the second set of 27 variables. This selection process was conducted using the entire dataset, ensuring that the chosen features were representative and relevant for predicting diabetes. The goal was to develop a simplified model that could be more readily applied in clinical settings while maintaining a reasonable level of accuracy. The performance of the models was evaluated using both sets of variables, demonstrating that while the 62-variable models generally performed better, the 27-variable models still showed fairly good performance. This indicates that feature selection was effective in identifying the most important predictors of diabetes.",
  "optimization/fitting": "The fitting method employed in our study involved a stratified 10-fold cross-validation approach to ensure robust model performance. This method helps to mitigate overfitting by providing a more reliable estimate of model performance on unseen data. By dividing the dataset into 10 subsets, nine were used for training while one was held out for validation. This process was repeated 10 times, with each subset serving as the validation set once. The mean and standard deviation of the results were then calculated to assess the model's performance consistently.\n\nTo address the potential issue of overfitting, particularly given the large number of variables (62 and 27) relative to the number of training points, we tuned the model hyperparameters using a random search. This approach helped in finding the optimal set of hyperparameters that maximized the area under the curve (AUC) in the receiver operating characteristic (ROC) analysis. Additionally, we used gradient boosting algorithms for Models 1, 2, and 3, and random forest algorithms for Model 4, both of which are known for their ability to handle high-dimensional data and reduce overfitting through techniques like regularization and ensemble learning.\n\nUnderfitting was addressed by ensuring that the models were complex enough to capture the underlying patterns in the data. The use of 62 variables in the initial models allowed for a comprehensive capture of the data's complexity. Furthermore, the performance of the models was evaluated using multiple metrics, including accuracy, recall, precision, and AUC, which provided a holistic view of the model's effectiveness. The models demonstrated strong performance, with Model 2 achieving the highest AUC of 0.928 with 62 variables, indicating that they were neither too simple nor too complex for the task at hand.",
  "optimization/regularization": "The study employed stratified 10-fold cross-validation as a regularization method to prevent overfitting. This technique involved randomly dividing the total number of patients into 10 subsets, using nine for training and one for validation. This process was repeated 10 times, ensuring that each subset served as the validation set once. By evaluating the model's performance on unseen data, this method helped to generalize the model's predictions and reduce the risk of overfitting. Additionally, hyperparameter tuning was performed using a random search to find the optimal set of hyperparameters that yielded the largest area under the curve in the receiver operating characteristic analysis. This further aided in preventing overfitting by ensuring that the model was not too complex and could generalize well to new data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we employed a stratified 10-fold cross-validation method to tune the model hyperparameters, aiming to achieve the largest area under the curve (AUC) in the receiver operating characteristic (ROC) analysis. This process involved creating nine training datasets and one validation dataset, repeating the cross-validation 10 times to calculate mean and standard deviation results.\n\nThe models utilized different algorithms: gradient boosting for Models 1, 2, and 3, and random forest for Model 4. The hyperparameters for each model were selected through a random search. The details of these configurations and the optimization process are provided in the supplementary materials, which include figures and tables that outline the steps and results of the hyperparameter tuning.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly provided in the publication. However, the methods and results are thoroughly documented, allowing for replication and further study. The data used in this study were extracted from electronic medical records and were handled with strict privacy protections, ensuring that all personal information was encrypted and anonymized. This approach aligns with ethical standards and regulatory requirements, as approved by the Institutional Review Board of the Catholic University of Korea, Seoul St. Mary\u2019s Hospital.\n\nFor those interested in accessing the detailed methods and results, the supplementary materials accompanying the publication offer comprehensive insights into the hyperparameter configurations and optimization processes. These materials are available under the terms specified by the journal, ensuring that researchers can replicate the study and build upon the findings.",
  "model/interpretability": "The models developed in this study are not entirely black-box systems. While machine learning models, particularly those based on gradient boosting and random forest algorithms, are often considered complex and less interpretable, efforts were made to enhance transparency.\n\nFor instance, the importance of each variable in the prediction models was assessed and ranked. This analysis revealed that fasting glucose level was the most significant predictor, followed by body mass index (BMI). The regression coefficients and odds ratios for these variables were also provided, offering insights into their impact on diabetes prediction. This approach allows clinicians and researchers to understand which factors are most influential in the models' predictions.\n\nAdditionally, the models were evaluated using a set of 27 variables commonly included in national health checkups, making them more interpretable and applicable in clinical settings. This simplification helps in understanding the key predictors and their contributions to the model's outcomes.\n\nThe use of stratified 10-fold cross-validation further ensures that the models' performance is consistent and reliable, providing a clearer picture of their predictive power. This method helps in validating the models' generalizability and robustness, making them more transparent and trustworthy for practical use.",
  "model/output": "The models developed in this study are classification models. They are designed to predict the development of diabetes mellitus (DM) in individuals. Specifically, the models categorize subjects into diabetic or non-diabetic groups based on various health examination variables. The primary goal is to classify whether a subject will develop diabetes within a specified time frame, such as one or two years. The performance of these models is evaluated using metrics such as accuracy, recall, precision, and the area under the receiver operating characteristic curve (ROC-AUC), which are typical for classification tasks. The models use algorithms like gradient boosting and random forest, which are commonly employed in classification problems to predict binary outcomes.",
  "model/duration": "The execution time for the models was limited to predictions within 1 or 2 years. The performance of the 1-year prediction model was well-maintained when extended to a 2-year prediction, indicating efficient execution time for both timeframes. The models utilized gradient boosting algorithms for Models 1, 2, and 3, and random forest algorithms for Model 4, which are known for their computational efficiency. Hyperparameter tuning was performed using a random search, and the models were validated using a stratified 10-fold cross-validation method, ensuring robust and time-efficient performance evaluation. The models were developed using 62 easily obtainable variables, and simplified versions with 27 variables were also created for practical application, further optimizing execution time.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed for our diabetes prediction models involved a robust stratified 10-fold cross-validation technique. This approach ensured that the models were thoroughly validated and their performance was reliable.\n\nIn this method, the total dataset was randomly divided into 10 subsets. For each iteration of the cross-validation process, nine of these subsets were used as training data to train the model, while the remaining subset served as the validation data to evaluate the model's performance. This process was repeated 10 times, with each subset serving as the validation set exactly once. By doing so, every data point in the dataset was used for both training and validation, providing a comprehensive assessment of the model's performance.\n\nThe performance of each model was evaluated based on the area under the curve (AUC) in the receiver operating characteristic (ROC) analysis. The hyperparameters of the models were tuned to achieve the highest possible AUC, ensuring that the models were optimized for accuracy.\n\nAdditionally, the mean and standard deviation of the results were calculated across the 10 iterations to provide a statistical measure of the models' performance consistency. This evaluation method ensured that our diabetes prediction models were validated rigorously and were capable of generalizing well to unseen data.",
  "evaluation/measure": "In our evaluation of the diabetes prediction models, we reported several key performance metrics to comprehensively assess the models' effectiveness. These metrics include accuracy, recall, precision, the area under the receiver operating characteristic curve (ROC-AUC), and the Kappa score.\n\nAccuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. It provides an overall sense of how well the model performs across all classes.\n\nRecall, also known as sensitivity, focuses on the proportion of actual positives that are correctly identified by the model. This metric is crucial for understanding how well the model can identify individuals who will develop diabetes.\n\nPrecision, or the positive predictive value, indicates the proportion of positive identifications that are actually correct. It is essential for evaluating the model's ability to avoid false positives.\n\nThe ROC-AUC is a widely used metric that evaluates the model's ability to distinguish between classes. It provides a single scalar value that summarizes the trade-off between the true positive rate and the false positive rate across all possible classification thresholds.\n\nThe Kappa score measures the agreement between the predicted and actual classifications, adjusting for the agreement that could be expected by chance. This metric is particularly useful for evaluating models in imbalanced datasets.\n\nThese performance metrics are representative of those commonly reported in the literature on diabetes prediction models. They provide a robust evaluation framework that allows for comparison with other studies and ensures that our models are assessed on multiple dimensions of performance. The inclusion of these metrics helps to demonstrate the reliability and effectiveness of our models in predicting diabetes risk.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, our focus was on developing and validating our own models using a large dataset from electronic medical records. We aimed to create models that could be practically applied in clinical settings, particularly using data from routine health checkups.\n\nHowever, we did consider simpler baselines in our approach. We developed two sets of prediction models: one using 62 variables from comprehensive health checkups and another using 27 variables that are typically included in national health checkups. The models with fewer variables were intended to simulate simpler baselines and to assess the feasibility of using more widely available data. These simpler models, while not as performant as our full models, still showed competitive results compared to previous reports, indicating their potential utility in broader healthcare settings.\n\nAdditionally, we explored the impact of different prediction times, comparing 1-year and 2-year prediction models. This allowed us to evaluate the robustness of our models over different time frames, providing insights into their generalizability and practical application. We also compared the performance of our models with a previous study that used a longer prediction time, highlighting the challenges and trade-offs associated with extended prediction horizons.",
  "evaluation/confidence": "The evaluation of our diabetes prediction models included a comprehensive assessment of performance metrics, which were accompanied by confidence intervals to provide a clear understanding of their reliability. For instance, metrics such as accuracy, recall, precision, and the area under the receiver operating characteristic curve (ROC-AUC) were reported with their respective standard deviations. This approach allowed us to quantify the variability and precision of our estimates, ensuring that the results were not merely point estimates but were supported by a range of plausible values.\n\nThe statistical significance of our findings was rigorously evaluated. We employed stratified 10-fold cross-validation, a robust method that involves randomly dividing the dataset into 10 subsets, training the model on nine subsets, and validating it on the remaining one. This process was repeated 10 times, and the mean and standard deviation of the results were calculated. This method helps in assessing the generalizability of the model and ensures that the performance metrics are not overly optimistic.\n\nMoreover, we compared the performance of our models with and without specific variables, such as fasting glucose and HbA1c, to demonstrate their clinical importance. The inclusion of these variables significantly improved the ROC-AUC, indicating that our models' superior performance is not merely due to chance but is attributable to the inclusion of these critical predictors.\n\nIn summary, the performance metrics of our diabetes prediction models are supported by confidence intervals and statistically significant results, providing a strong basis for claiming that our methods are superior to others and baselines.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study utilized internal validation methods, specifically the stratified 10-fold cross-validation technique, to assess the performance of the diabetes prediction models. This process involved dividing the dataset into training and validation sets multiple times to ensure robust evaluation. The results of these evaluations, including metrics like the area under the receiver operating characteristic curve (AUC), are reported in the publication. However, the specific datasets used for these evaluations are not released publicly. The study adheres to privacy protection measures, ensuring that all data obtained from electronic medical records were encrypted and anonymized. Therefore, while the methodology and outcomes are detailed in the paper, the raw evaluation files themselves are not accessible to the public."
}