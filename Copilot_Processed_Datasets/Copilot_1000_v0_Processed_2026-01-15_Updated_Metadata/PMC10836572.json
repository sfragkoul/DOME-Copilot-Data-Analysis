{
  "publication/title": "Machine Learning-Based Classification of Parkinson's Disease Patients Using Speech Biomarkers.",
  "publication/authors": "Hossain MA, Amenta F",
  "publication/journal": "Journal of Parkinson's disease",
  "publication/year": "2024",
  "publication/pmid": "38160364",
  "publication/pmcid": "PMC10836572",
  "publication/doi": "10.3233/jpd-230002",
  "publication/tags": "- Machine Learning\n- Parkinson's Disease\n- Classification\n- Supervised Learning\n- Lipidomics\n- Voice Analysis\n- Web-Based Detection\n- Motor Severity Prediction\n- Freezing of Gait\n- Reinforcement Learning\n- Data Preparation\n- Model Evaluation\n- Confusion Matrix\n- Performance Metrics\n- Speech Biomarkers\n- Data Balancing\n- Cross-Validation\n- Feature Selection\n- Neural Networks\n- Support Vector Machines",
  "dataset/provenance": "The dataset used in this study was collected by the Department of Neurology at Istanbul University\u2019s Cerrahpa\u015fa. It consists of 756 instances, each representing a record from 252 individuals, with 754 attributes and one binary decision variable. The dataset includes 188 patients with Parkinson\u2019s disease (PD) and 64 healthy individuals. The PD patients, aged between 33 and 87 years, had a mean age of 65.1 \u00b1 10.9, while the healthy individuals, aged between 41 and 82 years, had a mean age of 61.1 \u00b1 8.9. The dataset was shared with the machine learning repository at the University of California Irvine (UCI), making it accessible to students, educators, and researchers worldwide. This dataset has been utilized in various studies focusing on the classification of Parkinson\u2019s disease using different machine learning techniques.",
  "dataset/splits": "The dataset was split into two primary subsets: a training set and a test set. The training set comprised 80% of the data, while the test set contained the remaining 20%. Specifically, the training set included 151 Parkinson's disease (PD) patients and 51 healthy individuals, totaling 564 records from PD patients and 153 records from healthy individuals. The test set consisted of 37 PD patients and 13 healthy individuals, amounting to 111 records from PD patients and 39 records from healthy individuals.\n\nTo ensure that the same patient's records did not appear in both the training and test sets, the dataset was sorted by patient ID and then randomized before splitting. This approach helped in maintaining the integrity of the data splits and avoiding any overlap of patient records between the training and test sets.",
  "dataset/redundancy": "The dataset used in this study consists of 756 records from 252 individuals, with each subject having three observations. The dataset is split into training and testing sets to ensure that the machine learning classifiers are evaluated on unseen data, which helps to prevent overfitting.\n\nTo maintain independence between the training and test sets, the dataset was first sorted by patient ID, which is unique to each individual. This sorting ensures that records from the same patient do not appear in both the training and testing sets. The patient positions were then randomized to avoid any serial ordering. This randomization step is crucial because it ensures that the training set contains a random selection of patient records, rather than a sequential subset.\n\nThe dataset is divided such that 80% of the patients are allocated to the training set, and the remaining 20% are allocated to the test set. Specifically, 151 Parkinson\u2019s disease (PD) patients and 51 healthy individuals are included in the training set, while 37 PD patients and 13 healthy individuals are in the test set. This split ensures that the training and test sets are independent and that the model's performance can be generalized to new, unseen data.\n\nThe distribution of the dataset in terms of gender is balanced, with 130 males and 122 females. However, the class variable, which indicates whether an individual has PD or is healthy, is not balanced. To address this imbalance, the Synthetic Minority Over-sampling Technique (SMOTE) is applied to the target variable \"class.\" SMOTE generates synthetic examples by selecting random examples from the minority class and creating new samples along the lines drawn between these examples and their nearest neighbors in the feature space. This technique helps to balance the dataset and improve the performance of the machine learning classifiers.\n\nIn summary, the dataset is split into training and test sets in a way that ensures independence between the sets. The randomization of patient positions and the use of SMOTE for balancing the class variable are key steps in preparing the dataset for effective machine learning classification.",
  "dataset/availability": "The dataset used in this study was collected by the Department of Neurology at Istanbul University\u2019s Cerrahpa\u015fa. It was shared with the machine learning repository at the University of California Irvine (UCI). This repository is accessible to students, educators, and researchers worldwide.\n\nThe dataset consists of 756 instances and 755 attributes, with one binary decision variable. It includes data from 252 individuals, with each subject having three records. The dataset is balanced by gender, with 130 males and 122 females. Among these, 188 are patients with Parkinson\u2019s disease (PD), and 64 are healthy individuals.\n\nThe data was split into training and testing sets, with 80% of the data used for training and 20% for testing. To ensure that the same patient records did not appear in both the training and testing sets, the data was sorted by patient ID and randomized before splitting.\n\nThe dataset does not contain any missing or null values, so no data processing was performed to deal with missing or null values. The dataset is publicly available, and the specific details of the data splits used in this study can be inferred from the description provided. The dataset is likely released under a license that allows for academic and research use, as is typical for datasets shared on the UCI machine learning repository. However, the exact license terms are not specified in the provided information.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are supervised learning techniques. Specifically, the classifiers employed include Support Vector Machine (SVM), Logistic Regression (LR), K-Nearest Neighbors (KNN), Decision Tree (DT), Random Forest (RF), AdaBoost, Gradient Boosting, and Multi-Layer Perception (MLP). These are well-established algorithms in the field of machine learning and are not new.\n\nThe choice of these algorithms was driven by their effectiveness in handling high-dimensional data and their ability to provide robust performance metrics. The study aimed to evaluate and compare the performance of these classifiers to identify the most accurate models for the classification task at hand.\n\nThe algorithms were selected based on their proven track record in similar classification problems and their ability to handle the complexities of the dataset used in this study. The evaluation metrics, such as accuracy, precision, F1-score, and sensitivity, were used to assess the performance of each classifier. AdaBoost classifiers were found to be the most accurate, achieving an accuracy of 84.21%, while KNN classifiers were the least accurate at 62.28%.\n\nThe study also involved the development of pipelines using Linear Support Vector Classifier (LinearSVC) in combination with the supervised machine learning algorithms. These pipelines were designed to improve the accuracy and performance of the classifiers. The best pipeline for accuracy was found to be AdaBoost with LinearSVC, which achieved an accuracy of 85.09%.\n\nThe algorithms used in this study are not new and have been extensively studied and published in various machine learning journals. The focus of this study was on applying these algorithms to a specific dataset and evaluating their performance in the context of the research question. The results provide insights into the effectiveness of these algorithms for the classification task and highlight the importance of feature selection and pipeline development in improving model performance.",
  "optimization/meta": "In our study, we did not employ a meta-predictor. Instead, we developed and evaluated several individual machine learning classifiers and pipelines for the classification of Parkinson's Disease. These classifiers included Decision Trees, Random Forests, AdaBoost, Gradient Boosting, Multi-Layer Perception (MLP), Logistic Regression (LR), K-Nearest Neighbors (KNN), and Support Vector Machines (SVM).\n\nEach of these models was trained and validated independently using the same dataset. The pipelines were constructed by combining a Linear Support Vector Classifier (LinearSVC) for feature selection with one of the aforementioned machine learning algorithms. The LinearSVC was used to select relevant features, which were then passed on to the respective machine learning algorithm for classification.\n\nThe performance of these models was evaluated using various metrics, including accuracy, precision, sensitivity, and F1-score. The AdaBoost classifier and its pipeline with LinearSVC showed the highest performance among the evaluated models. The use of pipelines generally improved the accuracy and other performance metrics of the classifiers, except for Logistic Regression.\n\nThe dataset used for training and validation was openly accessible, and we relied solely on this data. Therefore, the training data for each model and pipeline was independent, as it was derived from the same publicly available source. This independence is crucial for ensuring that the models' performance is not biased by overlapping data.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms. We began with feature scaling, a preprocessing technique that normalizes features to a specific range without altering the data's essence. This step is vital for reducing the time consumption of machine learning classifiers and addressing sparsity in the dataset. We employed Sklearn's Standard Scaler, which removes the mean and scales to unit variance. The standard score of samples is calculated using the formula s = (a-u)/c, where u represents the mean of the training samples and c represents the standard deviation. Each feature is centered and scaled independently based on the training set samples, with the mean and standard deviation stored for later use in the transformation process.\n\nThe dataset used in this study is a secondary and processed set, containing no missing or null values. Therefore, no additional data processing was required to handle missing or null values. To classify Parkinson's Disease (PD) from this dataset, we split the data into training and testing sets, balanced the dependent variable, and selected features according to the model's requirements. The dataset consists of 756 records from 252 patients, with each patient having three observations. To avoid having the same patient records in both the training and testing sets, we sorted the data by patient ID and randomized the patient's position before splitting. This ensured a random distribution of patient records in the training set.\n\nWe used a standard procedure of allocating 70% of the data for training and 30% for testing. The dataset was balanced by gender, with 130 males and 122 females. However, the variable \"class\" was not balanced, as there were 564 records from PD patients and 192 records from healthy patients. To address this imbalance, we applied the Synthetic Minority Over-sampling Technique (SMOTE). SMOTE involves selecting examples close to the feature space, drawing a line between them, and creating a new sample along this line. This technique helps to balance the dataset by generating synthetic examples for the minority class.\n\nIn summary, our data encoding and preprocessing steps included feature scaling using Standard Scaler, handling the imbalance in the \"class\" variable with SMOTE, and ensuring a proper split of the dataset into training and testing sets while avoiding the same patient records in both sets. These steps were essential for preparing the data for effective machine-learning classification.",
  "optimization/parameters": "In our study, we utilized a variety of machine learning models, each with its own set of parameters. For the Decision Tree classifier, we set the minimum samples split to 2 and the minimum samples leaf to 1. The Random Forest classifier was built using 300 estimators. The AdaBoost and Gradient Boosting classifiers were both constructed with 100 estimators. The Multi-Layer Perception (MLP) classifier had four hidden layers with 256, 128, 64, and 32 neurons respectively, using the ReLU activation function and the Adam solver. The Logistic Regression classifier used L1 penalty with the liblinear solver. The Support Vector Machine (SVM) classifier employed a linear kernel with a regularization parameter C set to 0.01. The K-Nearest Neighbors (KNN) classifier used 5 neighbors with uniform weights.\n\nThe selection of these parameters was based on a combination of domain knowledge, empirical testing, and cross-validation techniques. For instance, the number of estimators in the Random Forest and AdaBoost models was chosen to balance between computational efficiency and model performance. The hidden layers and neurons in the MLP were selected through experimentation to achieve optimal performance. The regularization parameters and solvers were chosen based on their suitability for the dataset and the specific algorithms used. Cross-validation was employed to ensure that the models generalized well to unseen data.",
  "optimization/features": "In our study, the dataset initially contained 756 features. However, recognizing that not all features are relevant for classifying Parkinson's Disease (PD) patients, we applied a feature selection technique. This process was crucial for minimizing the number of input features and identifying the most relevant ones for the target value.\n\nThe feature selection was performed using the training set only, ensuring that the model's performance on unseen data would not be compromised. This approach helped in reducing the dimensionality of the data and improving the efficiency of the machine learning classifiers.\n\nBy focusing on the most relevant features, we were able to develop pipelines that significantly enhanced the performance of traditional machine learning models. The selected features were then used in combination with various classifiers, including Support Vector Machine (SVM), Logistic Regression, K-Nearest Neighbors (KNN), Decision Tree, Random Forest, AdaBoost, Gradient Boosting, and Multi-Layer Perception (MLP). This method allowed us to achieve better precision, sensitivity, and F1-score, ultimately leading to more accurate and reliable classification of PD patients.",
  "optimization/fitting": "In our study, we employed several machine learning algorithms to classify Parkinson's disease, each with its own set of parameters and techniques to address overfitting and underfitting.\n\nThe dataset consisted of 756 features, which is indeed a large number compared to the 252 patients (with three observations each), potentially leading to overfitting. To mitigate this, we applied feature selection techniques to reduce the number of input features and focus on the most relevant ones for classification. This step helped in simplifying the models and making them more generalizable.\n\nFor the decision tree classifier, we set the minimum samples split to 2 and the minimum samples leaf to 1, which helps in preventing the tree from becoming too complex and overfitting the training data. Additionally, we used the Gini impurity criterion to split nodes, which is effective in creating pure splits and reducing overfitting.\n\nIn the random forest classifier, we used 300 estimators, which is a sufficiently large number to ensure that the model captures the underlying patterns in the data without overfitting. The randomness of bootstrapping and sample selection was controlled to build a robust ensemble of trees.\n\nThe AdaBoost classifier was built using 100 estimators, with a focus on adjusting the weights of incorrectly classified instances. This iterative process helps in focusing on difficult cases and improving the model's performance without overfitting.\n\nFor the gradient boosting classifier, we set the number of estimators to 100, the learning rate to 1, and the maximum depth to 1. These settings help in controlling the complexity of the model and preventing overfitting. The randomness of the model was also controlled to ensure stability.\n\nThe Multi-Layer Perception (MLP) classifier was designed with multiple hidden layers (256, 128, 64, 32) and the ReLU activation function. The regularization parameter was set to 1e-1 to prevent overfitting. The model was validated using 10-fold cross-validation, which helps in assessing the model's performance on different subsets of the data and ensuring that it generalizes well.\n\nTo further address overfitting, we used 10-fold cross-validation for all models. This technique involves splitting the data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. The average performance across all folds provides a reliable estimate of the model's generalization ability.\n\nUnderfitting was addressed by ensuring that the models were complex enough to capture the underlying patterns in the data. For example, the decision tree and random forest classifiers were allowed to grow to a sufficient depth, and the MLP classifier was designed with multiple hidden layers. Additionally, we used techniques such as feature scaling and SMOTE to balance the dataset and improve the models' performance.\n\nIn summary, we employed a combination of feature selection, regularization, cross-validation, and careful parameter tuning to address overfitting and underfitting in our models. These techniques helped in ensuring that the models were robust, generalizable, and capable of accurately classifying Parkinson's disease.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was regularization, which helps to simplify the model and prevent it from becoming too complex and overfitting the training data.\n\nFor the Logistic Regression classifier, we used L1 regularization, also known as Lasso regularization. This type of regularization adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function. L1 regularization is particularly useful when dealing with datasets that have many features, as it can help in feature selection by driving some coefficients to zero.\n\nIn the case of the Support Vector Machine (SVM) with a linear kernel, we set the regularization parameter C to a small value (0.01). This parameter controls the trade-off between achieving a low training error and a low testing error. A smaller value of C encourages a larger margin, which can help in generalizing better to unseen data.\n\nAdditionally, we utilized 10-fold cross-validation for model validation. This technique involves dividing the dataset into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. Cross-validation helps to ensure that the model's performance is consistent and not just a result of overfitting to a particular subset of the data.\n\nFurthermore, we applied feature selection techniques to minimize the number of input features and focus on the most relevant ones. This not only simplifies the model but also reduces the risk of overfitting by avoiding the inclusion of irrelevant or noisy features.\n\nBy combining these regularization methods and validation techniques, we aimed to build models that generalize well to new, unseen data and avoid the pitfalls of overfitting.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in this study encompass a mix of both interpretable and black-box approaches. Among the models used, Decision Trees (DT) and Random Forests (RF) offer a degree of transparency. Decision Trees, in particular, are highly interpretable as they provide a clear, rule-based structure that can be easily visualized and understood. Each decision node represents a condition based on input features, leading to a specific outcome. This makes it straightforward to trace the decision-making process and understand how predictions are made.\n\nRandom Forests, while more complex than individual Decision Trees, still retain some level of interpretability. By aggregating the predictions of multiple Decision Trees, Random Forests can provide insights into the importance of different features. Feature importance scores indicate which variables contribute most to the model's predictions, offering a way to interpret the model's behavior.\n\nIn contrast, models like Support Vector Machines (SVM), Logistic Regression (LR), and Multi-Layer Perception (MLP) are considered black-box models. These models do not provide an explicit, human-readable explanation of their decision-making process. For instance, MLP, a type of artificial neural network, involves multiple layers of interconnected nodes (neurons) that process input data through complex, non-linear transformations. The relationships between inputs and outputs in such models are not easily discernible, making them less interpretable.\n\nAdaBoost and Gradient Boosting are ensemble methods that combine multiple weak learners to create a strong predictive model. While these methods can achieve high accuracy, they are generally less interpretable. The boosting process involves adjusting the weights of misclassified instances and fitting new models to the residual errors, which makes it challenging to trace the exact contributions of individual features to the final prediction.\n\nIn summary, while some models like Decision Trees and Random Forests offer a degree of interpretability, others like SVM, LR, MLP, AdaBoost, and Gradient Boosting are more opaque. The choice of model depends on the trade-off between accuracy and interpretability, with transparent models providing clearer insights into the decision-making process.",
  "model/output": "The model is designed for classification, specifically for the classification of Parkinson's Disease (PD) patients. Various supervised learning algorithms were employed, including Support Vector Machine (SVM), Logistic Regression (LR), K-Nearest Neighbors (KNN), Decision Tree (DT), Random Forest (RF), AdaBoost, Gradient Boosting, and Multi-Layer Perception (MLP). These algorithms were used to predict the presence or absence of Parkinson's Disease based on the input features.\n\nThe performance of these models was evaluated using several metrics, including confusion matrices, precision, sensitivity (recall), F1-score, and AUC-ROC curves. The confusion matrix provides a detailed breakdown of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) for each model. For instance, the Multi-Layer Perception (MLP) model achieved the highest true positive value of 152, while the K-Nearest Neighbors (KNN) model had the lowest with 92. The true negative values varied across models, with SVM, DT, and MLP all showing a TN value of 35.\n\nThe accuracy of the models was also assessed, with the AdaBoost model outperforming others with an 84.21% test accuracy score. The MLP and Gradient Boost models both achieved an accuracy of 82%, while the KNN model had the lowest test accuracy at 62.28%. Precision, sensitivity, and F1-score were used to generate a classification report, providing a comprehensive evaluation of each model's performance. The AdaBoost model had the highest precision score of 0.91 and the best F1-score of 0.89, indicating its robustness in classifying PD patients accurately.\n\nThe AUC-ROC curve was another critical evaluation metric, with the AdaBoost classifier achieving the highest AUC score of 0.87. The Random Forest and Gradient Boost classifiers both had AUC scores of 0.85, while the KNN and SVM models had the lowest AUC scores of 0.76. The Decision Tree model had the lowest AUC score of 0.73.\n\nIn summary, the model is a classification model designed to predict the presence of Parkinson's Disease using various supervised learning algorithms. The performance of these models was thoroughly evaluated using multiple metrics, with the AdaBoost model demonstrating the highest accuracy and robustness in classifying PD patients.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to assess the performance of various machine learning classifiers and pipelines designed for Parkinson's disease classification. To ensure robust and unbiased results, a 10-fold cross-validation method was utilized. This method involves dividing the dataset into 10 subsets, where the model is trained on 9 subsets and validated on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. The mean value of these 10 validation scores is then calculated to provide a reliable estimate of the model's performance.\n\nIn addition to cross-validation, the dataset was split into training and testing sets. Specifically, 70% of the data was used for training the models, while the remaining 30% was reserved for testing. This split was carefully designed to ensure that records from the same patient did not appear in both the training and testing sets, thereby avoiding data leakage and ensuring the generalizability of the results.\n\nThe performance of the classifiers and pipelines was evaluated using several key metrics derived from the confusion matrix, including true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). These metrics were used to calculate accuracy, precision, sensitivity (recall), and the F1-score, which provide a comprehensive view of the model's performance.\n\nFurthermore, the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve was used to evaluate the classifiers' ability to distinguish between Parkinson's disease patients and healthy individuals. The AUC scores for different pipelines were compared to identify the most effective models.\n\nThe evaluation also included a comparison of traditional machine learning models with pipelines that incorporated feature selection techniques. This comparison highlighted the improvements in accuracy, precision, sensitivity, and F1-score achieved by the pipelines, demonstrating the effectiveness of feature selection in enhancing model performance.\n\nOverall, the evaluation method ensured a rigorous and thorough assessment of the classifiers and pipelines, providing reliable insights into their performance and potential for real-world applications in Parkinson's disease classification.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our machine learning classifiers. These metrics were derived from the confusion matrix, which provides a detailed breakdown of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n\nOne of the primary metrics we reported is accuracy, which is the ratio of correctly predicted instances (both true positives and true negatives) to the total number of instances. This metric gives an overall sense of how well the model performs across all classes.\n\nPrecision, also known as the positive predictive value, measures the proportion of true positives among all positive predictions. It is crucial for understanding the reliability of positive predictions made by the model.\n\nSensitivity, or recall, indicates the proportion of actual positives that were correctly identified by the model. This metric is particularly important in medical diagnostics, where identifying all positive cases is critical.\n\nThe F1-score is the harmonic mean of precision and sensitivity, providing a single metric that balances both concerns. It is especially useful when dealing with imbalanced datasets.\n\nAdditionally, we utilized the Area Under the Receiver Operating Characteristic Curve (AUC-ROC) to evaluate the performance of our classifiers. The AUC-ROC curve plots the true positive rate against the false positive rate at various threshold settings, providing a comprehensive view of the model's ability to distinguish between classes.\n\nThese metrics are widely used in the literature and are representative of standard practices in evaluating machine learning models, particularly in classification tasks. They provide a comprehensive view of model performance, covering aspects such as overall correctness, reliability of positive predictions, ability to identify positive cases, and the trade-off between precision and sensitivity.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of various machine learning models and pipelines to evaluate their performance in classifying Parkinson's Disease (PD). We assessed several supervised machine learning classifiers, including Support Vector Machine (SVM), Logistic Regression (LR), K-Nearest Neighbors (KNN), Decision Tree (DT), Random Forest (RF), AdaBoost, Gradient Boost, and Multi-Layer Perception (MLP). Each of these models was evaluated using standard performance metrics such as accuracy, precision, sensitivity, F1-score, and AUC-ROC.\n\nTo enhance the performance of these models, we developed pipelines that incorporated feature selection using Linear Support Vector Classifier (LSVC). These pipelines were designed to improve the accuracy and other performance metrics of the traditional machine learning models. We constructed seven pipelines, each combining LSVC with one of the aforementioned classifiers. The performance of these pipelines was compared to the standalone classifiers to determine the effectiveness of the feature selection process.\n\nThe results indicated that most pipelines outperformed their corresponding standalone classifiers. For instance, the AdaBoost pipeline achieved the highest accuracy of 85.09%, significantly higher than the standalone AdaBoost classifier, which had an accuracy of 84.21%. Similarly, the KNN pipeline improved accuracy from 62.28% to 68.48%. However, there were exceptions where the pipelines did not improve performance, such as with LR and DT, where the accuracy slightly decreased.\n\nIn addition to accuracy, we also evaluated precision, sensitivity, and F1-score for both the standalone models and the pipelines. The pipelines generally showed better precision, sensitivity, and F1-scores compared to the traditional models. The AUC scores from the ROC-AUC curves also improved significantly with the pipelines, indicating better overall performance.\n\nOur study did not involve a direct comparison to publicly available methods on benchmark datasets, as we focused on evaluating the performance of our own models and pipelines. However, we reviewed and compared relevant studies on PD classification from reliable literature repositories to contextualize our findings. This approach allowed us to understand the challenges and methodologies used in previous research, providing a broader perspective on the effectiveness of our models and pipelines.\n\nIn summary, our evaluation involved a thorough comparison of various machine learning models and pipelines, demonstrating the benefits of feature selection and pipeline construction in improving classification performance for Parkinson's Disease.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study utilized a dataset collected by the Department of Neurology at Istanbul University\u2019s Cerrahpas\u00b8a, which was shared on the UCI Machine Learning Repository. This dataset is accessible to students, educators, and researchers worldwide. However, the specific evaluation files generated during the analysis, such as confusion matrices and classification reports, are not provided separately. The results presented in the study are derived from the analysis of this dataset using various machine learning classifiers and pipelines. The dataset itself is open for public use, but the detailed evaluation outputs are not separately released."
}