{
  "publication/title": "Machine-learning classification of texture features of portable chest X-ray accurately classifies COVID-19 lung infection.",
  "publication/authors": "Hussain L, Nguyen T, Li H, Abbasi AA, Lone KJ, Zhao Z, Zaib M, Chen A, Duong TQ",
  "publication/journal": "Biomedical engineering online",
  "publication/year": "2020",
  "publication/pmid": "33239006",
  "publication/pmcid": "PMC7686836",
  "publication/doi": "10.1186/s12938-020-00831-x",
  "publication/tags": "- Texture\n- Morphological\n- Machine learning\n- Feature extraction\n- Classification\n- COVID-19\n- Portable chest X-rays\n- Deep learning\n- Artificial intelligence\n- Diagnostic efficiency",
  "dataset/provenance": "The dataset used in this study was sourced from publicly available repositories. For COVID-19 chest X-ray (CXR) images, the data was obtained from a GitHub repository. The original download contained 250 scans of COVID-19 and SARS, captured in multiple directions. However, only CXR images of COVID-19 taken in the anterior-posterior (AP) direction were included, resulting in a final sample size of 130.\n\nAdditionally, the Kaggle chest X-ray image dataset was utilized for pneumonia cases. Although this dataset is extensive, a comparable sample size to the COVID-19 dataset was randomly selected. Specifically, 145 samples each were chosen for bacterial pneumonia and non-COVID-19 viral pneumonia, and 138 samples were selected for normal CXR images.\n\nThe dataset was then split into training and testing sets using a 70% and 30% ratio, respectively, through a stratified sampling method. This approach ensures that the distribution of classes is maintained in both the training and testing sets. The training data was used for feature selection, while the testing data was reserved for evaluating the performance of the classifiers.",
  "dataset/splits": "The dataset was split into training and testing data using a 70% and 30% ratio, respectively. This split was achieved using a stratified sampling method to ensure that the distribution of classes was maintained in both the training and testing sets.\n\nThe dataset consisted of chest X-ray images from four groups: COVID-19, bacterial pneumonia, non-COVID-19 viral pneumonia, and normal. For the COVID-19 group, there were 130 images. For bacterial pneumonia and non-COVID-19 viral pneumonia, there were 145 images each. For the normal group, there were 138 images.\n\nThe training set, which comprised 70% of the data, was further divided into five folds for cross-validation. This means that during each iteration of the cross-validation process, four of the five folds were used for training the model, and the remaining fold was used for validation. This process was repeated five times, with each fold serving as the validation set once.\n\nThe testing set, which comprised 30% of the data, was used to evaluate the final performance of the model after it had been trained and validated using the training set and cross-validation process.",
  "dataset/redundancy": "The datasets used in this study were split into training and testing sets using a 70% and 30% ratio, respectively. This split was performed using a stratified sampling method to ensure that the distribution of classes was maintained in both the training and testing sets. This approach helps to enforce independence between the training and test sets, as the data is randomly shuffled and divided into k folds, with one fold used for validation and the remaining folds used for training in each iteration of the cross-validation process.\n\nThe distribution of the datasets in this study is comparable to previously published machine learning datasets in terms of the number of samples for each class. For instance, the COVID-19 dataset was reduced to 130 samples after quality and relevance evaluation, while the Kaggle chest X-ray image dataset was randomly sampled to have comparable sizes for bacterial pneumonia, non-COVID-19 viral pneumonia, and normal CXR classes, with 145, 145, and 138 samples, respectively. This ensures that the model is trained and tested on a representative distribution of the data, which is crucial for the generalization of the results.",
  "dataset/availability": "The data utilized in this study are publicly available. They can be accessed via a GitHub repository. The dataset includes chest X-ray images relevant to COVID-19, bacterial pneumonia, viral pneumonia, and normal cases. The specific link to access these data is provided in the acknowledgements section of the publication.\n\nThe dataset was originally downloaded from a source containing scans of COVID-19 and SARS in various directions. After evaluation by radiologists, only the anterior-posterior (AP) direction chest X-rays from COVID-19 cases were included, resulting in a final sample size of 130. Additionally, samples from the Kaggle chest X-ray image dataset were selected to match the sample size of COVID-19 cases, ensuring a comparable dataset for bacterial pneumonia, non-COVID-19 viral pneumonia, and normal chest X-rays.\n\nThe data were split into training and testing sets using a 70% and 30% ratio, respectively, with stratified sampling to maintain the distribution of classes. This split was enforced during the preparation of the dataset for analysis.\n\nThe dataset is available under terms that allow for public access and use, facilitating reproducibility and further research. The link provided ensures that researchers can easily access the data for validation or extension of the study.",
  "optimization/algorithm": "In our study, we employed several machine-learning algorithms, primarily focusing on ensemble methods and decision trees. The main algorithm used was XGBoost, which is a gradient boosting framework. This system, proposed by Chen and Guestrin in 2016, has become a standard in the field of machine learning due to its superior performance in supervised learning tasks. XGBoost combines weak base learners with stronger learning models iteratively, making it a powerful tool for classification and regression tasks.\n\nWe also utilized other algorithms such as Classification and Regression Trees (CART), k-nearest neighbor (KNN), and Na\u00efve Bayes. CART is a decision tree algorithm that predicts target variable values based on other values in the dataset. It was first proposed by Breiman in 1984 and is widely used for its simplicity and interpretability. KNN is an instance-based learning algorithm that classifies data points based on the majority vote of their nearest neighbors. Na\u00efve Bayes, based on Bayesian theorem, is suitable for high-dimensional problems and is known for its efficiency and effectiveness in classification tasks.\n\nThe algorithms used in this study are well-established and have been extensively validated in the machine-learning community. The choice to use these algorithms in a biomedical engineering context, rather than a pure machine-learning journal, is driven by the specific application and the need to demonstrate their effectiveness in a real-world scenario. The focus of our publication is on the application of these algorithms to solve a specific problem in biomedical engineering, rather than the development of new machine-learning techniques. This approach allows us to showcase the practical utility of these algorithms in a domain where they can have a significant impact.",
  "optimization/meta": "In our study, we employed ensemble methods, specifically XGBoost, which can be considered a type of meta-predictor. This approach involves combining multiple models to improve overall performance. The XGBoost algorithm, proposed by Chen and Guestrin in 2016, is based on gradient boosting and integrates weak base learners into stronger models iteratively.\n\nThe ensemble methods used in this study include XGBoost linear and tree models. For the XGBoost linear model, parameters such as lambda, alpha, and eta were initialized to control regularization and learning rate. The XGBoost tree model was configured with parameters like maximum depth, learning rate, gamma, minimum child weight, and subsample to optimize performance.\n\nAdditionally, we utilized other machine-learning algorithms alongside XGBoost, including classification and regression tree (CART), k-nearest neighbor (KNN), and Na\u00efve Bayes (NB). Each of these algorithms was initialized with specific parameters to ensure optimal performance. For instance, CART was configured with parameters like minsplit, complexity parameter, and maximum depth, while Na\u00efve Bayes was set with parameters like search method, laplace, and adjust.\n\nThe training data for these models was divided into train and test sets, with a fivefold cross-validation technique applied to ensure robustness. This method involves shuffling the data and splitting it into five folds, using one fold for validation and the remaining four for training in each iteration. This approach helps to validate the model's performance and generalizability.\n\nRegarding the independence of training data, it is crucial to note that the data used for training each model was carefully selected and preprocessed to ensure that it was independent and representative of the underlying distribution. This independence is essential for the reliable performance of ensemble methods and meta-predictors.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for ensuring the effectiveness of the machine-learning algorithms employed. We utilized several techniques to prepare the data for classification tasks.\n\nInitially, the dataset was divided into training and testing sets. For the training phase, we implemented a fivefold cross-validation technique. This involved shuffling the data and splitting it into five equal parts, or folds. In each iteration of the cross-validation, one fold was used as the validation set, while the remaining four folds were used for training the classifier. This process was repeated five times, with each fold serving as the validation set once, ensuring that the model was trained and validated on different subsets of the data.\n\nFor the classification algorithms, we used typical default parameters. Specifically, for the XGBoost linear model, we set the regularization terms lambda and alpha to 0, and the learning rate eta to 0.3. For the XGBoost tree model, we initialized parameters such as the maximum depth of the tree to 30, the learning rate eta to 0.3, the maximum loss reduction gamma to 1, the minimum child weight to 1, and the subsample rate to 1. The k-nearest neighbor parameter k was set to 5. For the classification and regression tree (CART), we initialized parameters with minsplit set to 20, the complexity parameter cp to 0.01, and the maximum depth to 30. For the Na\u00efve Bayes classifier, we used a grid search method, set laplace to 0, and adjust to 1.\n\nAdditionally, for the k-nearest neighbor (KNN) algorithm, we calculated the Euclidean distance to measure the similarity between data points. The value of k, which determines the number of nearest neighbors to consider, was chosen based on the volume and nature of the data. For larger datasets, a higher value of k was used, while for smaller datasets, a lower value of k was sufficient.\n\nThese preprocessing and encoding steps were essential for ensuring that the machine-learning algorithms could effectively learn from the data and make accurate predictions. The use of cross-validation and appropriate parameter settings helped to enhance the robustness and generalizability of the models.",
  "optimization/parameters": "In our study, we employed several machine learning algorithms, each with its own set of parameters. For the XGBoost linear model, we initialized the parameters with lambda set to 0, alpha set to 0, and eta set to 0.3. For the XGBoost tree model, the parameters included a maximum depth of 30, a learning rate (eta) of 0.3, gamma set to 1, minimum child weight set to 1, and subsample set to 1. Additionally, the nearest neighbor parameter k was set to 5. For the Classification and Regression Tree (CART) model, the parameters were minsplit set to 20, complexity parameter (cp) set to 0.01, and maximum depth set to 30. For the Na\u00efve Bayes model, the parameters included a grid search method, laplace set to 0, and adjust set to 1.\n\nThe selection of these parameters was based on typical default settings and optimization techniques to ensure robust performance across different models. The choice of k in the nearest neighbor parameter was influenced by the volume and nature of the data, with larger datasets requiring a larger k value and smaller datasets requiring a smaller k value. The parameters for each model were fine-tuned to balance complexity and accuracy, ensuring optimal performance in our classification tasks.",
  "optimization/features": "In our study, we utilized a combination of texture and morphological features extracted from chest X-ray (CXR) images. The texture features were derived from the Grey-level Co-occurrence Matrix (GLCM), which includes second-order statistical features such as contrast, correlation, mean, entropy, energy, variance, inverse different moment, standard deviation, smoothness, root mean square, skewness, kurtosis, and homogeneity.\n\nMorphological features were extracted using a method that converts image morphology into quantitative values, focusing on aspects like area and perimeter to calculate other morphological features. These features are crucial for detecting abnormalities in lung tissues.\n\nFeature selection was indeed performed to ensure that only the most relevant and stable features were used for classification. This process involved ranking features based on their importance using empirical receiver-operating characteristic curve (EROC) and random classifier slope, which assesses class separability. The selected features were then used as input for our machine-learning classifiers.\n\nImportantly, feature selection was conducted using only the training data, ensuring that the testing data remained unseen and unbiased during the feature ranking process. This approach helped in maintaining the integrity and reliability of our classification results.",
  "optimization/fitting": "In our study, we employed several machine learning models, each with its own set of parameters optimized for performance. The number of parameters varied depending on the model used. For instance, the XGBoost tree model had parameters such as maximum depth, learning rate, and subsample, while the XGBoost linear model had regularization terms and learning rate. The CART model had parameters like minimum split and maximum depth, and the Na\u00efve Bayes model had parameters like search method and Laplace adjustment.\n\nTo address the potential issue of overfitting, especially when the number of parameters is large, we utilized regularization techniques. For the XGBoost linear model, we included regularization terms (lambda and alpha) to penalize complex models and prevent overfitting. Additionally, we employed cross-validation, specifically the Jack-knife fivefold cross-validation technique. This method helps in validating the model's performance by ensuring that the model generalizes well to unseen data. By dividing the data into five folds and training on four folds while testing on the remaining one, we repeated this process five times to ensure robust performance evaluation.\n\nUnderfitting was addressed by carefully tuning the model parameters. For example, in the XGBoost tree model, we set a maximum depth of 30 to allow the model to capture complex patterns in the data. Similarly, for the CART model, we set a maximum depth of 30 and a minimum split of 20 to ensure that the model was not too simplistic. The Na\u00efve Bayes model, known for its simplicity, was optimized using grid search to find the best parameters that balanced bias and variance.\n\nOverall, our approach involved a combination of regularization, cross-validation, and parameter tuning to ensure that our models neither overfitted nor underfitted the data. This careful optimization process helped us achieve reliable and generalizable results.",
  "optimization/regularization": "In our study, we employed regularization techniques to prevent overfitting, particularly when using the XGBoost linear model. Regularization is a method used to constrain or regularize the model parameters to avoid overfitting. For the XGBoost linear model, we initialized the parameters with lambda and alpha set to 0. Lambda and alpha are regularization terms on weights. Lambda controls the L2 regularization, which penalizes large weights, while alpha controls the L1 regularization, which can lead to sparse models by driving some weights to zero. By setting these parameters, we aimed to improve the model's generalization performance and reduce the risk of overfitting.\n\nAdditionally, we used a learning rate (eta) of 0.3 for both the XGBoost linear and tree models. The learning rate controls the contribution of each tree to the final prediction, effectively shrinking the weights and helping to prevent overfitting. A smaller learning rate requires more trees to be built, but it can lead to better generalization.\n\nFor the XGBoost tree model, we also set a maximum depth of 30, which limits the complexity of the individual trees and helps to prevent overfitting. Furthermore, we used a subsample rate of 1, which means that all training examples were used to build each tree. However, subsampling can also be used to introduce randomness and reduce overfitting.\n\nIn summary, we utilized regularization techniques such as L1 and L2 regularization, learning rate shrinkage, and tree depth limitation to prevent overfitting in our models. These techniques helped to improve the generalization performance of our classifiers.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, for the XGBoost linear model, we initialized parameters such as lambda, alpha, and eta. For the XGBoost tree model, parameters like maximum depth, learning rate, gamma, minimum child weight, and subsample were set. Additionally, we used a nearest neighbor parameter k = 5. For the CART model, parameters included minsplit, complexity parameter, and maximum depth. The Na\u00efve Bayes model was initialized with parameters like search method, laplace, and adjust.\n\nThese configurations were chosen to optimize the performance of each model in our study. The details of these parameters are provided in the methods section of the paper, ensuring transparency and reproducibility. However, the specific model files and optimization schedules are not explicitly detailed in the publication. The data used in this study is available via a public repository, ensuring that others can access and utilize the dataset for further research. The license under which the data is available is not specified in the provided context.",
  "model/interpretability": "The model employed in this study leverages interpretable AI techniques to enhance transparency. Specifically, texture and morphological features were extracted from chest X-ray (CXR) images to distinguish COVID-19 cases from other conditions. These features were then visualized to highlight the locations of abnormalities, making the model's decision-making process more understandable.\n\nThe use of interpretable AI allows for the visualization of the areas in the CXR images that contribute most to the classification of COVID-19. This approach not only aids in distinguishing COVID-19 from non-COVID-19 viral infections, bacterial infections, and normal cases but also provides insights into the specific regions of the lung that are affected. For instance, the model can pinpoint areas in the lung that exhibit texture patterns indicative of COVID-19, thereby offering a clearer understanding of the disease's impact on the lung tissue.\n\nMoreover, the study utilized machine-learning algorithms that are inherently more interpretable than deep learning models. For example, the Classification and Regression Tree (CART) algorithm used in this research creates decision trees where each fork represents a split in a predictor variable, and each node at the end contains a prediction for the target variable. This structure allows for a step-by-step explanation of how the model arrives at its classification, making it easier to trace the decision path and understand the contributing factors.\n\nAdditionally, the feature ranking algorithms employed in this study further enhance interpretability. By ranking features based on their importance in classifying COVID-19, the model provides a clear indication of which texture and morphological features are most relevant. This ranking helps in eliminating redundant features and focusing on the most pertinent information, thereby improving the model's transparency and reliability.",
  "model/output": "The model employed in our study is a classification model. We utilized five supervised machine-learning classification algorithms: XGBoost linear, XGBoost tree, classification and regression tree (CART), k-nearest neighbor (KNN), and Na\u00efve Bayes. These algorithms were applied to classify different types of pneumonia, including COVID-19, bacterial pneumonia, viral pneumonia, and normal cases.\n\nThe XGBoost linear classifier demonstrated high accuracy in distinguishing COVID-19 from normal patients, bacterial pneumonia, and viral pneumonia. For two-class classification, the model achieved sensitivity and positive predictive value (PPV) higher than 95% for COVID-19 vs. normal, COVID-19 vs. bacterial pneumonia, and COVID-19 vs. viral pneumonia. In multi-class classification, the model successfully classified COVID-19 among the four groups with a combined accuracy of 79.52% and an area under the curve (AUC) of 0.87.\n\nThe classification performance was evaluated using various metrics, including sensitivity, specificity, PPV, and negative predictive value (NPV). The results indicated that the model performed exceptionally well in distinguishing COVID-19 from other conditions, with high sensitivity and specificity across different classifications. The feature ranking algorithms used in the study helped in selecting the most relevant features, which further improved the classification accuracy.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code used in this study is publicly available. It can be accessed via a GitHub repository. The dataset utilized for the study is also available through the same repository, ensuring that others can replicate and build upon the work presented. The repository contains all necessary files and instructions to run the algorithms and reproduce the results. The data is deidentified and obtained from a publicly available source, adhering to ethical standards and ensuring privacy. The specific link to the repository is provided in the publication for easy access.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to ensure the robustness and accuracy of the classification algorithms used. We utilized a fivefold cross-validation technique, which is a well-established method for validating the performance of classifiers. This technique involves dividing the dataset into five equal parts, or folds. In each iteration of the cross-validation process, one fold is used as the test set, while the remaining four folds are used as the training set. This process is repeated five times, with each fold serving as the test set once. The performance metrics are then averaged across all five iterations to provide a more reliable estimate of the classifier's performance.\n\nIn addition to cross-validation, we evaluated the performance of our classification models using several key metrics. These include sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and accuracy. Sensitivity, also known as the true positive rate, measures the proportion of actual positives that are correctly identified by the model. Specificity, or the true negative rate, measures the proportion of actual negatives that are correctly identified. PPV indicates the probability that a positive test result is a true positive, while NPV indicates the probability that a negative test result is a true negative. Accuracy provides an overall measure of the model's performance by calculating the proportion of true results (both true positives and true negatives) among the total number of cases examined.\n\nFurthermore, we used the receiver-operating characteristic (ROC) curve analysis to evaluate the performance of our classifiers. The ROC curve plots the true positive rate against the false positive rate at various threshold settings, providing a visual representation of the trade-off between sensitivity and specificity. The area under the ROC curve (AUC) is a single scalar value that summarizes the performance of the classifier, with higher AUC values indicating better performance.\n\nStatistical analysis was conducted using unpaired two-tailed t-tests with unequal variance to examine differences in outcomes. This approach ensures that the results are statistically significant and not due to random chance. The statistical analysis and performance measures were carried out using MATLAB and RStudio, which are widely used tools in the field of machine learning and data analysis.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our classification models. These metrics include sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), accuracy, and the area under the receiver-operating characteristic curve (AUC) with a 95% confidence interval. Sensitivity, also known as the true positive rate (TPR), measures the proportion of actual positives correctly identified by the model. Specificity, or the true negative rate (TNR), assesses the proportion of actual negatives correctly identified. PPV indicates the probability that a positive test result is a true positive, while NPV reflects the probability that a negative test result is a true negative. Accuracy provides an overall measure of the model's correctness, and AUC evaluates the model's ability to distinguish between classes.\n\nThese metrics are widely recognized and used in the literature for evaluating classification models, particularly in medical imaging and diagnostic systems. They provide a thorough assessment of the model's performance across different aspects, ensuring that our results are both robust and comparable to other studies in the field. By reporting these metrics, we aim to offer a clear and comprehensive understanding of our model's capabilities and limitations.",
  "evaluation/comparison": "In our study, we compared the performance of our approach with several publicly available methods and simpler baselines. We utilized five supervised machine-learning classification algorithms: XG boosting linear (XGB-L), XG boosting tree (XGB-tree), classification and regression tree (CART), k-nearest neighbor (KNN), and Na\u00efve Bayes (NB). These algorithms were chosen to evaluate the effectiveness of our feature extraction and classification pipeline.\n\nThe XG boosting ensemble methods were particularly highlighted in our experiments. Ensemble techniques, which combine multiple models, are known for their robustness and often outperform individual models. We found that these methods provided better performance metrics, such as accuracy and AUC, compared to simpler baselines.\n\nOur comparison included two-class and multi-class classification tasks. For two-class classification, we evaluated the performance of distinguishing COVID-19 from normal, bacterial pneumonia, and viral pneumonia. The results showed high sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) for these tasks, indicating that our approach effectively differentiates between these classes.\n\nIn multi-class classification, we assessed the performance of classifying COVID-19 among four groups: COVID-19, normal, bacterial pneumonia, and viral pneumonia. The combined accuracy and AUC for this task were also promising, demonstrating the robustness of our method in handling more complex classification problems.\n\nOverall, our approach outperformed traditional methods by extracting texture and morphological features from chest X-ray (CXR) images. These features were then used as input to robust machine-learning classifiers, leading to improved classification performance. The comparison with simpler baselines and publicly available methods underscores the effectiveness of our approach in accurately distinguishing COVID-19 from other conditions.",
  "evaluation/confidence": "The evaluation of our study's performance metrics includes confidence intervals, specifically for the area under the receiver-operating curve (AUC). This provides a range within which the true AUC value is expected to lie, with a 95% confidence level. This statistical measure helps to understand the reliability and precision of our AUC estimates.\n\nStatistical significance was assessed using unpaired two-tailed t-tests with unequal variance. This method allows us to determine if the differences in outcomes between groups are statistically significant. The significance of our results is indicated by the P-value, which helps to claim that our method is superior to others and baselines. A low P-value (typically \u2264 0.05) suggests strong evidence against the null hypothesis, indicating that the observed differences are unlikely to have occurred by chance.\n\nReceiver-operating characteristic (ROC) curve analysis was performed with COVID-19, normal, bacterial, and non-COVID-19 viral pneumonia as ground truth. The performance was evaluated by standard ROC analysis, including sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), accuracy, and AUC with 95% confidence interval. These metrics provide a comprehensive evaluation of our model's diagnostic ability. The AUC, in particular, is a critical metric as it summarizes the model's performance across all classification thresholds. An AUC greater than 0.5 indicates that the model has some discriminative ability, with higher values representing better performance. The inclusion of confidence intervals for the AUC adds an additional layer of confidence in our results, showing the range within which the true AUC is likely to fall.",
  "evaluation/availability": "The raw evaluation files used in this study are publicly available. They can be accessed via a specific GitHub repository. The dataset includes chest X-ray images that were used for evaluating the classification models. The images were obtained from a deidentified dataset, ensuring privacy and ethical considerations. The dataset is open for public use, allowing other researchers to replicate and build upon the findings presented in this study. The availability of this dataset facilitates transparency and reproducibility in research."
}