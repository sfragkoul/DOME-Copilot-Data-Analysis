{
  "publication/title": "Practical prediction model of the clinical response to programmed death-ligand 1 inhibitors in advanced gastric cancer.",
  "publication/authors": "Noh MG, Yoon Y, Kim G, Kim H, Lee E, Kim Y, Park C, Lee KH, Park H",
  "publication/journal": "Experimental & molecular medicine",
  "publication/year": "2021",
  "publication/pmid": "33547412",
  "publication/pmcid": "PMC8080676",
  "publication/doi": "10.1038/s12276-021-00559-1",
  "publication/tags": "- RNA sequencing\n- PD-L1\n- CXCL11\n- Immunohistochemistry\n- Machine learning\n- Random forest\n- Decision tree\n- Gastric cancer\n- Immunotherapy\n- Predictive modeling\n- Alternative splicing\n- Genetic variation\n- Structural variation\n- Bioinformatics\n- Clinical biomarkers\n- Tumor microenvironment\n- Histopathological features\n- Survival analysis\n- Progression-free survival\n- Overall survival",
  "dataset/provenance": "The dataset used in this study was sourced from a cohort of Korean patients. The total number of data points was 102 cases. Of these, 100 cases were subjected to histopathological evaluation, and 45 cases had RNA sequencing (RNA-seq) data available. However, only 33 cases had both histopathological and RNA-seq data.\n\nIn addition to the primary data collected for this study, whole exome sequencing data from a previous study conducted by Professor Lee's research team was also utilized. This data overlapped with some of the cases in the current study. The whole exome sequencing data included well-known biomarkers such as tumor mutational burden and microsatellite instability (MSI), which were used for comparative analysis with the biomarkers identified in this study.\n\nThe dataset has been deposited into the European Nucleotide Archive and is available under the accession code PRJEB25780. This ensures that the data is accessible for further research and validation by the scientific community.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "All data have been deposited into the European Nucleotide Archive and are available under accession code PRJEB25780. This ensures that the data is publicly accessible for further research and validation. The deposition of data in a recognized public forum like the European Nucleotide Archive promotes transparency and reproducibility in scientific research. The data is made available under terms that allow for its use in further studies, adhering to standard practices in the scientific community. This approach ensures that the data can be accessed and utilized by other researchers, fostering collaboration and advancing the field.",
  "optimization/algorithm": "The machine-learning algorithms used in this study were the C5.0 decision tree model and the random forest model. These are well-established methods in the field of machine learning and are not new. The random forest model is particularly noted for its strong predictive power and was used to determine the best model for classification among variables. The C5.0 decision tree model, while showing inferior performance compared to the random forest, was also utilized due to its simplicity, which can be more helpful in clinical settings.\n\nThe choice of these algorithms was driven by their effectiveness in handling complex datasets and their ability to provide insights into the relationships between variables. The random forest model, in particular, demonstrated excellent predictive performance, making it a suitable choice for this study. The C5.0 decision tree model, despite its simpler structure, offered a more interpretable model for clinicians.\n\nThe study did not aim to compare the performance of different machine-learning methods but rather to identify the most effective model for predicting responses to immune checkpoint inhibitors. The random forest model was found to be the best performer, with the C5.0 decision tree model serving as a useful alternative for its interpretability.",
  "optimization/meta": "The model developed in this study can be considered a meta-predictor as it integrates various types of data and machine-learning methods to enhance predictive performance. The meta-predictor combines histopathologic, transcriptomic, and immunohistochemical data to predict responders versus nonresponders to immune checkpoint inhibitors.\n\nThe whole consists of two primary machine-learning methods: the C5.0 decision tree model and the random forest classification. The C5.0 decision tree model was used for its simplicity and interpretability, while the random forest model was employed for its robust predictive power. Both models were trained using the caret R package, with the C5.0 decision tree incorporating a winnowing parameter to prevent overfitting.\n\nTo ensure the independence of training data, a 10-fold cross-validation was repeated 500 times for both models. This approach helps to validate the model's performance and generalizability by ensuring that each fold of the data is used for both training and validation in a systematic manner. Additionally, the random forest model utilized an 'out-of-bag' area under the curve (AUC) estimation, which further supports the independence and robustness of the training data.\n\nThe integration of these methods allows for a comprehensive analysis, leveraging the strengths of both decision tree and random forest approaches. The final models demonstrate superior predictive performance compared to individual biomarkers, such as the PD-L1 IHC test, highlighting the effectiveness of the meta-predictor approach.",
  "optimization/encoding": "For the machine-learning algorithms employed in our study, data encoding and preprocessing were crucial steps to ensure the models' effectiveness. Initially, we categorized genes into binary classes using the ROC() function from the Epi package in R, considering P values less than 0.05 as significant. This binary encoding facilitated the classification tasks in our predictive models.\n\nIn the predictive model generation phase, we utilized the caret R package to distinguish between responders and nonresponders. Two primary models were employed: the C5.0 decision tree and the random forest classifier. To mitigate overfitting, we set the winnow parameter to \"TRUE,\" which involved measuring the utility of each input field and excluding non-useful fields before modeling.\n\nFor the random forest training, we generated an ensemble of 500 trees. To optimize model performance, we conducted repeated 10-fold cross-validation, which involved splitting the data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process was repeated 500 times to ensure robustness.\n\nTo determine the optimal number of variables, we created a grid with mtry parameters set to 1, 2, 3, 4, 8, and 16. The roc() function from the proc package was used to compare the performance of each model. This systematic approach allowed us to identify the best-performing model, which in this case, was a single-variable model based on the presence or absence of signet ring cells (SRC), achieving an AUC value of 0.78.\n\nAdditionally, we performed RNA sequencing analysis using the bcbio-nextgen bioinformatics framework, which included alternative splicing, genetic variation, and structural variation analyses. However, these results required further validation and were not included in the final model due to the complexity and potential inconsistencies.\n\nIn summary, our data encoding and preprocessing involved binary categorization of genes, rigorous cross-validation, and parameter tuning to ensure the models' predictive accuracy and generalizability.",
  "optimization/parameters": "In our study, we employed two different modeling approaches: the C5.0 decision tree and random forest. For the C5.0 decision tree, we utilized a parameter known as \"winnow\" to prevent overfitting. This parameter helps in measuring the usefulness of input fields and excluding those that are not useful. The best model identified through this process was a single-variable model, which focused on the presence or absence of signet ring cells (SRC).\n\nFor the random forest model, we generated an ensemble of 500 trees. To determine the optimal number of variables, we created a grid with mtry parameters set to 1, 2, 3, 4, 8, and 16. Through repeated 10-fold cross-validation, we found that the best-performing model used mtry = 1. This indicates that the model with the highest predictive performance utilized a single variable, specifically CXCL11 (RNA-seq). Both the C5.0 decision tree and random forest models were trained using the caret R package, ensuring robust and reliable predictive performance.",
  "optimization/features": "In the optimization process, we utilized seven histological features as input for training our model. These features included the presence of signet ring cells, fibrous stroma, myxoid stroma, tumor-infiltrating lymphocytes, necrosis, tertiary lymphoid follicles, and epithelial ulceration. Feature selection was indeed performed to enhance the model's predictive power. This selection process was conducted using the training set only, ensuring that the model's performance was not biased by information from the test set. The winnow parameter was set to \"TRUE\" during the model training, which helped in preventing overfitting by excluding non-useful input fields in advance. This approach ensured that only the most relevant features were used for classification, thereby improving the model's generalization to new, unseen data.",
  "optimization/fitting": "In our study, we employed two primary machine learning models for classification: the C5.0 decision tree and random forest. Both models were implemented using the caret R package, which provided a robust framework for model training, validation, and optimization.\n\nThe number of parameters in our models was indeed larger than the number of training points, particularly in the case of the random forest, which generated an ensemble of 500 trees. To address the risk of over-fitting, we implemented several strategies. Firstly, we used the winnow parameter set to \"TRUE\" in the C5.0 decision tree model. This parameter helps to prevent over-fitting by measuring the usefulness of each input field in advance and excluding it if it is not useful. Secondly, we employed a 10-fold cross-validation repeated 500 times to estimate the 'out-of-bag' area under the curve (AUC). This rigorous cross-validation process ensured that our models were generalizable and not merely memorizing the training data. Additionally, for the random forest model, we created a grid with mtry parameters of 1, 2, 3, 4, 8, and 16 to define the optimal number of variables, further helping to mitigate over-fitting.\n\nTo rule out under-fitting, we carefully selected relevant features and ensured that our models had sufficient complexity to capture the underlying patterns in the data. The use of ensemble methods, such as random forest, inherently reduces the risk of under-fitting by combining the predictions of multiple decision trees. Furthermore, the repeated 10-fold cross-validation process helped to identify the best-performing model, ensuring that it was neither too simple nor too complex.\n\nIn summary, our approach to model fitting involved a balance between complexity and generalization, with a strong emphasis on cross-validation and feature selection to prevent both over-fitting and under-fitting.",
  "optimization/regularization": "In our study, we employed a regularization method to prevent overfitting. Specifically, we used the winnow parameter set to \"TRUE\" in our predictive model. This technique involves measuring whether each input field is useful for the model in advance. If a field is found to be not useful, it is excluded from the model. This approach helps to simplify the model and reduce the risk of overfitting by ensuring that only relevant features are included in the final model. Additionally, we utilized 10-fold cross-validation repeated 500 times to estimate the out-of-bag area under the curve (AUC), which further aids in assessing the model's performance and generalizability.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available through the R packages employed. Specifically, the caret package was utilized for model training and cross-validation. The parameters for the C5.0 decision tree model and random forest, including the winnow parameter and the number of trees, are detailed within the text. The mtry parameter grid for the random forest model, which included values of 1, 2, 3, 4, 8, and 16, was also specified. These configurations can be replicated using the caret package, which is open-source and freely available under the GPL-2 license.\n\nThe ROC function from the proc package was used to compare model performance, and the details of this function are available within the package documentation. The best models identified through 10-fold cross-validation repeated 500 times are described, including the AUC values for both the C5.0 decision tree and random forest models.\n\nModel files and specific optimization parameters are not explicitly provided as downloadable assets, but the methods and configurations are thoroughly documented within the text. Researchers can replicate the models using the described parameters and the open-source R packages mentioned. The caret package, in particular, provides extensive documentation and examples for replicating the models and optimization processes used in this study.",
  "model/interpretability": "The model employed in this study is primarily a random forest, which is known for its strong predictive power but is often considered a black box due to its complexity and lack of intuitive interpretability. This characteristic makes it challenging for clinicians to directly understand and apply the model's decisions in a clinical setting.\n\nHowever, to address this issue, a simpler decision tree model using the C5.0 algorithm was also trained. This model, while demonstrating inferior performance compared to the random forest, offers greater transparency. It identified CXCL11 (RNA-seq) as the single most important variable for classification. This transparency makes the C5.0 decision tree more interpretable and potentially more useful in clinical practice, where clear and understandable decision-making processes are crucial.\n\nIn summary, while the random forest model provides superior predictive performance, the C5.0 decision tree model offers a more interpretable and clinically applicable approach, focusing on a single key variable, CXCL11.",
  "model/output": "The model developed in this study is a classification model. It was designed to predict whether patients would be responders or nonresponders to immune checkpoint inhibitors. Two main types of models were employed: the C5.0 decision tree and random forest classification. The C5.0 decision tree model was used to create a simple, interpretable model, while the random forest model was used to achieve higher predictive performance. Both models were trained using various data types, including histopathology, RNA-seq, and immunohistochemistry results. The performance of these models was evaluated using metrics such as the area under the curve (AUC), with the random forest model demonstrating superior predictive power compared to the C5.0 decision tree model. The final models were selected based on their ability to accurately classify patients into responders and nonresponders, with the random forest model showing an AUC of 0.944, indicating excellent predictive performance.",
  "model/duration": "The execution time for our models varied depending on the specific method used. For the C5.0 decision tree model, we employed a winnowing parameter to prevent overfitting, which involved measuring the usefulness of each input field in advance and excluding it if it was not useful. This process, along with the 10-fold cross-validation repeated 500 times, contributed to the overall execution time. The random forest model, which generated an ensemble of 500 trees, also required significant computational resources. The repeated 10-fold cross-validation for the random forest model was used to accurately compare model performance, further adding to the execution time. The grid search for the optimal number of variables (mtry parameters of 1, 2, 3, 4, 8, and 16) in the random forest model also contributed to the overall runtime. While specific execution times are not provided, it is clear that both models required substantial computational effort to achieve the best performance.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several steps to ensure its robustness and applicability. A decision model was trained to determine the best classification model among variables. The random forest method, known for its strong predictive power, was employed and demonstrated superior performance compared to the C5.0 decision tree model. The random forest model utilized the ROC curve to select the optimal model, with the final value set at mtry=1. CXCL11 (RNA-seq) emerged as the model with the best performance, even when decision trees with one variable were randomly extracted from the random forest.\n\nTo further validate the model, a 10-fold cross-validation was repeated 500 times, and the best model was extracted. The winnow parameter was set to \"TRUE\" to prevent overfitting by measuring the usefulness of input fields and excluding those that were not useful. For random forest training, parameters were set to generate an ensemble of 500 trees. A grid with mtry parameters of 1, 2, 3, 4, 8, and 16 was created to define the optimal number of variables. The roc() function of the R package proc was used to compare the performance of each model.\n\nAdditionally, the presence or absence of signet ring cells (SRC) was used to predict responses, utilizing the ROC() function of the Epi package in R. P-values less than 0.05 were considered significant. The caret R package was used to predict responders versus nonresponders, employing both the C5.0 decision tree model and random forest for classification. The model's performance was compared using repeated 10-fold cross-validation to ensure accuracy.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our predictive models. For the random forest model, we utilized the Receiver Operating Characteristic (ROC) curve to select the optimal model, focusing on the largest value to determine the best performance. The final model used the parameter mtry=1, indicating that CXCL11 (RNA-seq) was the variable with the best performance even when all decision trees with one variable were randomly extracted from the random forest.\n\nAdditionally, we compared the performance of different models using the ROC function from the R package proc. This allowed us to assess the area under the curve (AUC) for each model, providing a comprehensive measure of their predictive accuracy.\n\nWe also conducted a 10-fold cross-validation repeated 500 times to estimate the 'out-of-bag' AUC, ensuring that our models were robust and generalizable. This cross-validation process helped us to identify the best model by comparing the performance metrics across multiple iterations.\n\nFor the C5.0 decision tree model, we similarly evaluated its performance using the AUC. Although the C5.0 model demonstrated inferior performance compared to the random forest model, it offered a simpler decision-making framework that could be more practical for clinicians in a clinical setting.\n\nOverall, our performance metrics included the ROC curve, AUC, and cross-validation results, which are standard and representative in the literature for evaluating the predictive power of machine learning models in biomedical research.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. However, we did compare the performance of different machine learning models to evaluate their predictive power. Specifically, we used the C5.0 decision tree model and the random forest model for classification. The random forest model demonstrated superior predictive performance compared to the C5.0 decision tree model. This comparison was conducted using a 10-fold cross-validation repeated 500 times to ensure robustness.\n\nAdditionally, we considered simpler baselines to assess their practical utility in clinical settings. For instance, we found that a decision tree model with a single variable, CXCL11 (RNA-seq), performed well and could be more intuitive for clinicians. This simpler model, while not as powerful as the random forest, provided a straightforward decision-making tool that could be more helpful in actual clinical practice. The comparison between these models highlighted the trade-off between predictive accuracy and interpretability, which is crucial for clinical applications.",
  "evaluation/confidence": "The evaluation of our models included statistical significance testing to ensure the robustness of our findings. We used the ROC curve to select the optimal model, focusing on the largest value to determine the best performance. The final value used for the model was mtry= 1, which indicated that CXCL11 (RNA-seq) was the model with the best performance even when all decision trees with one variable were randomly extracted from the random forest.\n\nFor the random forest model, we employed a 10-fold cross-validation repeated 500 times to estimate the 'out-of-bag' area under the curve (AUC). This rigorous cross-validation process helped to ensure that our model's performance was not due to overfitting and provided a reliable estimate of its generalizability.\n\nIn our logistic regression analysis, we obtained odds ratios for significant histological features, such as the presence of signet ring cells (SRC) and fibrous stroma. The odds ratio for SRC was 0.06 with a 95% confidence interval of 0.01\u20130.71 and a P-value less than 0.05, indicating a statistically significant association with nonresponse to immune checkpoint inhibitors. Similarly, the odds ratio for fibrous stroma was 0.08 with a 95% confidence interval of 0.01\u20130.74 and a P-value less than 0.05, also indicating statistical significance.\n\nSurvival analysis further supported these findings, showing significantly longer progression-free survival (PFS) and overall survival (OS) in patients without SRC compared to those with SRC. The log-rank P-values for PFS and OS were 0.004 and 0.001, respectively, both of which are statistically significant.\n\nAdditionally, we used the chi-squared and Fisher\u2019s exact tests to correlate histopathologic features with responsiveness, ensuring that our findings were statistically robust. The use of these statistical methods and the reporting of confidence intervals and P-values provide a strong basis for claiming that our methods are superior to others and baselines.",
  "evaluation/availability": "The raw evaluation files for this study are not publicly available. The RNA sequencing data used in this study have been deposited in the European Nucleotide Archive under the accession number PRJEB25780. However, the specific evaluation files, such as those related to the predictive models or histopathological assessments, are not released publicly. The study was conducted with data provided by the research team of Professor Lee, and access to the raw evaluation files would require permission from the authors or the institutional review board. The data sharing policies and licenses for the deposited RNA sequencing data can be found on the European Nucleotide Archive website. For further details or access to the raw evaluation files, interested parties should contact the corresponding author."
}