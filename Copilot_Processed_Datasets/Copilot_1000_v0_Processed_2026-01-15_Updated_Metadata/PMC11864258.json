{
  "publication/title": "Alzheimer's disease classification by supervised and intelligent techniques.",
  "publication/authors": "Mohamed Amine J, Mourad M,",
  "publication/journal": "Journal of Alzheimer's disease reports",
  "publication/year": "2025",
  "publication/pmid": "40034511",
  "publication/pmcid": "PMC11864258",
  "publication/doi": "10.1177/25424823241311838",
  "publication/tags": "- Alzheimer's Disease\n- Mild Cognitive Impairment\n- Machine Learning\n- Convolutional Neural Networks\n- Multilayer Perceptron\n- Classification Accuracy\n- Medical Imaging\n- MRI\n- Fuzzy Logic\n- Neurodegenerative Disorders",
  "dataset/provenance": "The dataset used in this study was obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. The ADNI was initiated in 2003 as a public-private partnership, led by Principal Investigator Michael W. Weiner. Its primary goal was to determine whether serial MRI, PET, and other biological markers, combined with clinical and neuropsychological assessments, could effectively measure the progression of mild cognitive impairment (MCI) and early Alzheimer's disease (AD).\n\nThe ADNI database has been a valuable resource for the scientific community, providing data to validate biomarkers for clinical trials and improve the generalizability of findings by increasing diversity in the participant cohort. The dataset includes 2D MRI images of the hippocampus brain from 394 individuals. Of this dataset, 65% was allocated for the learning step, and 35% was reserved for the testing step. This division ensures a robust training and evaluation process for the models developed in our study.",
  "dataset/splits": "The dataset used in our study consists of 2D MRI images of the hippocampus brain from 394 individuals. The data was split into two main parts: 65% was allocated for the learning (training) step, and 35% was reserved for the testing step. This split ensures that the model is trained on a substantial amount of data while also having a significant portion for evaluating its performance.\n\nThe dataset includes three classes: NC (Normal Cognition), MCI (Mild Cognitive Impairment), and AD (Alzheimer's Disease). The demographic information of the patients in each class is as follows:\n\n- **NC (Normal Cognition)**: 80 individuals (39 males and 41 females) with an average age of 79 for males and 80 for females.\n- **MCI (Mild Cognitive Impairment)**: 164 individuals (89 males and 75 females) with an average age of 79 for males and 80 for females.\n- **AD (Alzheimer's Disease)**: 150 individuals (74 males and 76 females) with an average age of 78 for males and 85 for females.\n\nThis distribution allows for a comprehensive analysis and classification of the different stages of cognitive impairment.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data utilized in this study were obtained from publicly available datasets, ensuring accessibility and transparency. These datasets include contributions from various investigators within the Alzheimer's Disease Neuroimaging Initiative (ADNI). The ADNI investigators played a significant role in the design and implementation of ADNI and provided the data, although they did not participate in the analysis or writing of this report.\n\nThe datasets are available for non-commercial use, reproduction, and distribution under the terms of the Creative Commons Attribution-NonCommercial 4.0 License. This license permits the use of the data as long as the original work is properly attributed. The specific details and acknowledgments for the ADNI investigators can be found in the ADNI Acknowledgement List, which is accessible online.\n\nTo ensure compliance with the license terms, all users of the data are required to attribute the original work as specified on the SAGE and Open Access page. This enforcement mechanism helps maintain the integrity and proper use of the datasets, ensuring that the contributions of the ADNI investigators are recognized and respected.",
  "optimization/algorithm": "The machine-learning algorithms used in our study primarily include Multilayer Perceptron (MLP) and Convolutional Neural Networks (CNN). These are well-established classes of algorithms in the field of machine learning, particularly in the domain of neural networks.\n\nThe MLP network employed in our research consists of three layers: an input layer with three nodes, a hidden layer with ten nodes utilizing \"tansig\" nonlinear activation functions, and an output layer with three nodes using a \"SoftMax\" activation function. The features selected for this model were chosen based on their significant biological relevance to Alzheimer's Disease (AD) progression, including empty space volume, amyloid-\u03b2 volume, and tau protein volume. These variables were validated through a comprehensive review of medical research and further confirmed by a neurologist.\n\nThe CNN architecture used in our study is designed to predict AD from MRI scans. We utilized a hybrid input featuring dosages of beta-amyloid and Tau protein mass, which has shown better performance in classification tasks compared to single modality approaches. The CNN model's architecture includes convolution layers, pooling layers, and activation functions, which allow it to learn complex and varied characteristics from the input data.\n\nThe optimization of these models involved tuning various parameters, such as learning rates, convolution kernel sizes, and the number of layers. For the MLP, the convergence behavior was observed to be stable under various configurations, with the number of iterations required to reach the desired mean squared error threshold being consistent across multiple runs. The stability of the network was verified by testing it on noisy and perturbed data, where it showed resilience and reliable performance.\n\nFor the CNN, several experiments were conducted to determine the optimal number of epochs. It was found that the loss rate becomes unstable when the number of epochs exceeds 50, indicating overfitting. The best loss rate was achieved at 50 epochs, as shown in the loss curve and confusion matrix.\n\nThe choice of these algorithms and their configurations was based on extensive experimentation and validation against traditional methods. The results demonstrate the robustness and reliability of the proposed models, ensuring their effectiveness in diagnosing AD.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithms. We utilized 2D MRI images of the hippocampus brain from 394 individuals. The dataset was split, with 65% allocated for the training phase and 35% for testing.\n\nPreprocessing techniques played a significant role in enhancing model performance. We experimented with various methods, including normalization and noise reduction. Normalization alone improved accuracy to 90.1%, while noise reduction alone achieved 91.0%. The combination of both normalization and noise reduction yielded the highest accuracy at 99.1%.\n\nFor the fuzzy logic approach, we employed three linguistic input variables: beta-amyloid volume, tau protein volume, and empty space volume. These variables were selected based on their biological relevance to Alzheimer's disease progression and were validated by a neurologist. The fuzzy logic system used standard defuzzification logic and functions, accommodating up to 8 rules. This method leveraged 3D MRI, 3D PET Florbetaben, and 3D PET Flortaucipir images, resulting in a classification rate of 99.1%.\n\nIn the MLP (Multilayer Perceptron) technique, our network consisted of three layers: an input layer with three nodes, a hidden layer with ten nodes using \"tansig\" nonlinear activation functions, and an output layer with three nodes using a \"SoftMax\" activation function. The same dataset used for the fuzzy logic method was applied here, achieving a classification rate of 94%.\n\nFor the CNN (Convolutional Neural Network) technique, extensive tests were conducted by varying parameters such as learning rate, number of epochs, and convolution kernel dimensions. The optimal parameters selected were a learning rate of 0.01, 50 epochs, and kernel sizes of 3x3 and 5x5, resulting in a classification rate of 90.67%.\n\nOverall, the integration of multimodal imaging and advanced machine-learning techniques, coupled with rigorous preprocessing and parameter tuning, significantly enhanced the classification accuracy for Alzheimer's disease.",
  "optimization/parameters": "In our study, several key parameters were used and carefully selected to optimize the performance of our models. For the Multilayer Perceptron (MLP) network, the primary parameters included the learning rate, the number of iterations, and the number of neurons in the hidden layer. The learning rate was fine-tuned to ensure stable convergence, and the number of neurons was set based on experimentation to balance model complexity and performance. Additionally, the activation functions\u2014\"tansig\" for the hidden layer and \"SoftMax\" for the output layer\u2014were chosen to facilitate effective learning and classification.\n\nFor the Convolutional Neural Network (CNN), the parameters included the learning rate, the number of epochs (iterations), and the convolution kernel dimensions. Extensive testing was conducted to determine the optimal values for these parameters. For instance, a learning rate of 0.01 and 50 epochs were found to be optimal, as increasing the number of epochs beyond this point led to overfitting. The convolution kernel dimensions were also varied, with combinations of 3x3 and 5x5 kernels yielding the best results.\n\nThe selection of these parameters was driven by a combination of theoretical knowledge and empirical testing. We conducted numerous experiments, varying one parameter at a time while keeping others constant, to observe their impact on model performance. This iterative process allowed us to identify the most effective parameter settings. For example, reducing the learning rate decreased oscillations in the results, leading to more stable and reliable performance. Similarly, the number of epochs was chosen based on the point at which the loss curve converged and remained stable, indicating that the model had learned the underlying patterns in the data.\n\nIn summary, the parameters used in our models were selected through a rigorous process of experimentation and validation. This approach ensured that our models were optimized for accuracy and robustness, providing reliable diagnostic results for Alzheimer's Disease.",
  "optimization/features": "In our study, we utilized three key input features for our Multilayer Perceptron (MLP) classification method. These features were carefully selected based on their significant biological relevance to Alzheimer's Disease (AD) progression. The features included empty space volume, amyloid-\u03b2 volume, and tau protein volume. The selection process involved a comprehensive review of medical research and was further validated by a neurologist. This approach ensured that the chosen features were not only biologically relevant but also clinically significant, thereby enhancing the robustness and reliability of our diagnostic model.",
  "optimization/fitting": "In our study, we employed several strategies to ensure that our model neither overfitted nor underfitted the data. The number of parameters in our convolutional neural network (CNN) and multilayer perceptron (MLP) models was indeed larger than the number of training points, which is a common scenario in deep learning. To address potential overfitting, we conducted extensive experiments by varying the number of epochs. We observed that increasing the number of epochs beyond a certain point led to unstable loss rates and overfitting. Specifically, for our CNN model, the optimal number of epochs was determined to be 50, as further training did not improve the loss rate and instead introduced overfitting. This was evident from the loss curve, which converged to zero and remained constant up to epoch 50.\n\nTo further mitigate overfitting, we used techniques such as normalization and noise reduction in our preprocessing steps. These techniques significantly improved model performance, as shown in our results where the combination of normalization and noise reduction yielded the highest accuracy of 99.1%. Additionally, we employed a stability analysis by varying input features within a controlled range to ensure consistent diagnostic results.\n\nUnderfitting was addressed by carefully selecting the architecture and hyperparameters of our models. For the MLP, we used a network with three layers: an input layer with three nodes, a hidden layer with ten nodes using \"tansig\" activation functions, and an output layer with three nodes using a \"SoftMax\" activation function. This architecture was chosen based on its ability to capture complex patterns in the data while avoiding underfitting. The convergence behavior of the MLP network was observed to be stable under various configurations, and the number of iterations required to reach the desired mean squared error threshold was consistent across multiple runs.\n\nMoreover, we prioritized biologically relevant features for our MLP model, such as empty space volume, amyloid-\u03b2 volume, and tau protein volume. These features were selected based on a comprehensive review of medical research and validated by a neurologist, ensuring that our model focused on the most informative aspects of the data.\n\nIn summary, we carefully managed the balance between overfitting and underfitting through rigorous experimentation, preprocessing techniques, and thoughtful model architecture design. This approach ensured that our models generalized well to unseen data and provided reliable diagnostic results.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and improve the robustness of our model. One of the key methods used was the incorporation of dropout layers within our convolutional neural network (CNN) architecture. Dropout layers randomly deactivate a fraction of neurons during training, which helps to prevent the model from becoming too reliant on specific neurons and thus reduces overfitting.\n\nAdditionally, we conducted extensive experiments to determine the optimal number of epochs for training. We observed that increasing the number of epochs beyond a certain point led to a disturbance in the loss rate, indicating overfitting. Through these experiments, we identified that 50 epochs provided the best balance, achieving the lowest loss rate without overfitting.\n\nWe also explored the impact of different preprocessing techniques on model performance. Our findings showed that combining normalization and noise reduction significantly improved accuracy, reaching up to 99.1%. This preprocessing step helped in reducing the noise in the data, which in turn aided in preventing the model from learning irrelevant patterns and thus mitigated overfitting.\n\nFurthermore, we utilized cross-validation approaches to validate our classification results. This method involved dividing the dataset into multiple subsets and training the model on different combinations of these subsets. By doing so, we ensured that our model's performance was consistent across various data splits, further reducing the risk of overfitting.\n\nIn summary, our approach to preventing overfitting involved the use of dropout layers, careful selection of the number of epochs, effective preprocessing techniques, and rigorous cross-validation. These methods collectively contributed to the development of a robust and accurate model for our classification tasks.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our experiments are detailed throughout the publication. Specifically, the learning rates, number of iterations, and convolution kernel dimensions are discussed in the context of various experiments and their impacts on model performance. For instance, a learning rate of 0.01 with 50 iterations using 3x3 and 5x5 convolution kernels yielded an accuracy of 90.67%. These details are presented in tables and figures, such as Table 8, which outlines different experimental setups and their corresponding accuracies.\n\nThe optimization schedule is implicitly described through the discussion of epochs and their effects on loss rates. It is noted that the loss rate becomes unstable when the number of epochs surpasses 10,000, with the best loss rate observed at 10,000 epochs for certain configurations. Similarly, for CNN techniques, the optimal number of epochs is identified as 50, beyond which overfitting occurs.\n\nModel files and specific optimization parameters are not explicitly provided in the text, but the methodologies and configurations are thoroughly documented. The publication emphasizes the importance of parameter tuning, such as adjusting learning rates and convolution kernel sizes, to achieve optimal model performance. The experiments and their results are intended to guide further research and implementation.\n\nRegarding availability and licensing, the publication does not specify where model files can be accessed or under what license they might be distributed. However, the detailed descriptions of the experimental setups and results are intended to be reproducible by other researchers in the field. The focus is on providing a comprehensive understanding of the methods and outcomes, which can be adapted and built upon in future studies.",
  "model/interpretability": "The model we developed is not a black box; instead, it incorporates elements designed to enhance interpretability. One key aspect is the use of fuzzy logic, which allows for a more transparent decision-making process. The fuzzy logic approach defines linguistic variables such as amyloid-\u03b2 volume, tau volume, and empty space volume, each with clear membership functions. These variables are categorized into subsets like small, medium, and great, making it easier to understand how different input values contribute to the final diagnosis.\n\nFor instance, the amyloid-\u03b2 volume is defined over a specific interval and is used to determine the likelihood of certain conditions. Similarly, the tau volume and empty space volume are also divided into interpretable subsets, providing a clear framework for understanding the model's decisions. This approach ensures that the model's outputs can be traced back to specific input features, aligning closely with medical interpretations and expert feedback.\n\nAdditionally, the model's convergence properties and stability have been thoroughly validated using real patient data. This validation process includes repeated iterations and stability analyses, ensuring that the model produces consistent and reliable results. The use of expert feedback further reinforces the model's transparency, as it has been designed to mirror real-world medical diagnoses.\n\nIn summary, the model's use of fuzzy logic and clear membership functions, along with extensive validation and expert input, makes it a transparent and interpretable tool for medical diagnosis.",
  "model/output": "The model is primarily focused on classification tasks. It is designed to categorize different classes related to Alzheimer's Disease (AD) diagnosis. Specifically, the model aims to distinguish between three classes: Normal Control (NC), Mild Cognitive Impairment (MCI), and Alzheimer's Disease (AD). The classification performance is evaluated using metrics such as accuracy, confusion matrices, and ROC curves, which are standard for assessing the precision, recall, and F1-score of a classifier. The model's output provides a classification rate for each class, with the overall average classification rate being a key indicator of its performance. For instance, the Multilayer Perceptron (MLP) method achieves an average classification rate of 94%, while the fuzzy logic approach reaches 99.1%. The confusion matrix further details the classification results, showing high accuracy for NC and AD classes but a reduced recognition rate for the MCI class. This indicates that the model is effective in distinguishing between NC and AD but faces more challenges with the MCI class. The stability and convergence of the model are also analyzed to ensure reliable diagnostic results across various configurations and input features.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our method involved several rigorous steps to ensure its robustness and reliability. We employed cross-validation approaches to verify the classification results, which helped in assessing the model's performance across different subsets of data. Additionally, feedback from medical professionals who were not involved in the development of the system provided a biologically and clinically relevant context for the system outputs. This external validation was crucial in ensuring that our model's predictions were not only statistically sound but also clinically meaningful.\n\nWe utilized various evaluation metrics beyond simple accuracy, including confusion matrices and ROC curves. These metrics allowed us to better understand the precision, recall, and F1-score of our classifier, providing insights into its performance across various thresholds. The confusion matrices, for instance, helped us identify the classification performance for each class, including the recognition rate for the MCI class, which was noted to be reduced compared to other classes.\n\nThe stability of our model was also thoroughly tested. We performed a stability analysis by varying the input features within a controlled range to ensure consistent diagnostic results. Furthermore, the convergence behavior of the model was observed to be stable under various configurations, with the number of iterations required to reach the desired mean squared error threshold being consistent across multiple runs. The model's resilience was demonstrated by testing it on noisy and perturbed data, where it showed reliable performance.\n\nIn summary, our evaluation method combined cross-validation, external medical feedback, and comprehensive performance metrics to ensure that our model was both accurate and clinically relevant. The stability and convergence properties of the model were also rigorously tested to guarantee its reliability in real-world applications.",
  "evaluation/measure": "In our evaluation, we utilized several performance metrics to comprehensively assess the effectiveness of our models. Beyond simple accuracy, we employed confusion matrices and ROC curves to gain deeper insights into the precision, recall, and F1-score of our classifiers. These metrics are crucial for understanding the performance of our models across various thresholds and ensuring their robustness and reliability.\n\nThe confusion matrices provided detailed information about the true positive, true negative, false positive, and false negative rates for each class, which is essential for evaluating the performance of our classifiers, especially for imbalanced datasets. The ROC curves, along with the area under the curve (AUC), helped us assess the trade-off between the true positive rate and the false positive rate, providing a more nuanced view of our models' performance.\n\nAdditionally, we compared our approach with existing methods in the literature. For instance, our fuzzy logic-based approach achieved an accuracy of 99.1%, significantly outperforming methods by Liu et al. (53.8%) and Hosseini-Asl et al. (89.1%). This comparison highlights the effectiveness of our multimodal imaging and preprocessing techniques.\n\nWe also reported the average classification rate, which was 94% for our MLP model. This metric gives a general idea of the model's performance across all classes, including NC, MCI, and AD. The recognition rate for the MCI class was slightly lower compared to the other classes, as indicated in our confusion matrices.\n\nIn summary, the performance metrics we reported are representative of the state-of-the-art in the field. They provide a comprehensive evaluation of our models' accuracy, precision, recall, and overall robustness, ensuring that our findings are reliable and comparable to other studies in the literature.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our proposed methods with both publicly available approaches and simpler baselines to ensure the robustness and effectiveness of our models. We utilized data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) public database, which includes 3D MRI, 3D PET Florbetaben, and 3D PET Flortaucipir medical images from 173 individuals. This dataset allowed us to benchmark our methods against established techniques in the field.\n\nOur proposed multimodal approach, which combines MRI and PET imaging with advanced preprocessing techniques, demonstrated significant accuracy improvements. Specifically, our fuzzy logic-based method achieved an accuracy of 99.1%, outperforming the methods of Liu et al. (53.8%) and Hosseini-Asl et al. (89.1%). This superior performance can be attributed to the integration of multimodal data and sophisticated preprocessing methods, such as noise reduction and normalization, which enhance data quality and model performance.\n\nWe also compared our approach with simpler baselines, including different configurations of Multilayer Perceptron (MLP) and Convolutional Neural Network (CNN) architectures. For the MLP model, we found that a hidden layer with 10 nodes yielded the best results, achieving an accuracy of 94%. In the case of CNNs, the most reliable results were obtained with a combination of 3 \u00d7 3 and 5 \u00d7 5 kernels across three layers, reaching an accuracy of 90.67%. These comparisons highlight the effectiveness of our chosen model configurations and preprocessing techniques.\n\nAdditionally, we performed an ablation study to assess the impact of various factors on classification accuracy. This study involved evaluating the contributions of different imaging modalities, preprocessing techniques, and model configurations. The results confirmed that the combination of 3D MRI and 3D PET Florbetaben, along with normalization and noise reduction, significantly enhanced the model's performance. This comprehensive evaluation underscores the superiority of our proposed methods over simpler baselines and publicly available approaches.",
  "evaluation/confidence": "The evaluation of our proposed methods involved a thorough assessment of their performance metrics, ensuring that the results are both reliable and statistically significant. While specific confidence intervals for the performance metrics are not explicitly detailed, the robustness of our models was validated through cross-validation approaches. This method helps to ensure that the performance metrics are not due to random chance but reflect the true capabilities of the models.\n\nThe statistical significance of our results was further supported by comparing our methods against established baselines. For instance, our MLP architecture achieved an accuracy of 94%, outperforming previous works such as Liu et al. and Hosseini-Asl et al., which reported accuracies of 53.8% and 89.1%, respectively. Similarly, our fuzzy logic approach reached an impressive 99.1% accuracy, demonstrating a clear superiority over other methods.\n\nAdditionally, the stability and convergence properties of our models were rigorously tested. The MLP network, for example, showed consistent performance across multiple runs and under various configurations, indicating its reliability. The convergence behavior was influenced by key parameters such as the learning rate and the number of neurons in the hidden layer, which were carefully tuned to optimize performance.\n\nThe use of dropout layers in our architecture also played a crucial role in reducing overfitting, thereby improving the model's generalizability. This was particularly important in ensuring that our models could perform well on unseen data, further bolstering the confidence in our results.\n\nIn summary, while explicit confidence intervals are not provided, the comprehensive evaluation process, including cross-validation, comparison with baselines, and stability analysis, provides strong evidence of the statistical significance and superiority of our proposed methods.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The article is distributed under the Creative Commons Attribution-NonCommercial 4.0 License, which permits non-commercial use, reproduction, and distribution of the work without further permission, provided the original work is attributed. However, this license does not cover the sharing of raw evaluation files. For access to specific data or files, interested parties may need to contact the corresponding author directly."
}