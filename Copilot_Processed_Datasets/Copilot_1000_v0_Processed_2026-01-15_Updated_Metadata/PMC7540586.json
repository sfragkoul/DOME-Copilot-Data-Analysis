{
  "publication/title": "Application of Whole-Genome Sequences and Machine Learning in Source Attribution of Salmonella Typhimurium.",
  "publication/authors": "Munck N, Njage PMK, Leekitcharoenphon P, Litrup E, Hald T",
  "publication/journal": "Risk analysis : an official publication of the Society for Risk Analysis",
  "publication/year": "2020",
  "publication/pmid": "32515055",
  "publication/pmcid": "PMC7540586",
  "publication/doi": "10.1111/risa.13510",
  "publication/tags": "- Machine Learning\n- Salmonella Typhimurium\n- Source Attribution\n- Whole Genome Sequencing\n- Predictive Modeling\n- Feature Reduction\n- Class Imbalance\n- Logit Boost\n- Random Forest\n- Public Health",
  "dataset/provenance": "The dataset used in this study consists of Salmonella Typhimurium and its monophasic variants isolated from domestic and imported food and animals, as well as human cases. The data was collected through the Danish national Salmonella surveillance program in 2013 and 2014. In total, 65% of the 325 samples of Salmonella Typhimurium and its monophasic variants isolated from domestic and imported food and animals were sequenced. The human dataset consisted of 18% of the 764 samples of Salmonella Typhimurium and its monophasic variants.\n\nThe isolates were previously used for the Danish Salmonella source account published in the annual reports on zoonoses in Denmark in 2013 and 2014. All isolates were sequenced using an Illumina HiSeq, NextSeq, or MiSeq sequencing machine. The dataset was assessed as appropriate for the development of a new source attribution model based on sequencing data, as the phylogenetic profiles of the human isolates were intermixed with those from potential food and animal sources.\n\nThe dataset included a total of 210 food and animal isolates, which were used to develop a supervised machine learning model. The model was based on Salmonella core genes and associated allelic variations. In total, 520 (0.08%) allelic values from the food and animal isolates and 15,176 (3.6%) of the allelic values from the human isolates were imputed. Of the 3,002 Salmonella core genes, 17 were found important after feature reduction and were selected for further modeling. The importance of these 17 core genes across the sources is listed in a table, with the values representing the area under the ROC curve (AUC) derived from the prediction of the sources by each feature (loci).",
  "dataset/splits": "The dataset was split into training and testing sets. The training set consisted of 70% of the data, while the testing set consisted of 30%. This split was performed randomly and repeated ten times with sevenfold cross-validation. In each iteration, the training data was further divided into seven subsets, with one subset held out as a validation set and the remaining six subsets used for training. This process was repeated until all subsets had been held out and predicted. The final model was constructed using the entire dataset, without upsampling, to ensure it learned as much as possible about the data's variability. The dataset included 210 food and animal isolates collected through the Danish national Salmonella surveillance program in 2013 and 2014. The distribution of these isolates across different sources is detailed in Table I.",
  "dataset/redundancy": "The dataset used in this study included food, animal, and human isolates collected specifically for this study. The data was split into training and testing sets to evaluate the model's ability to predict the host group. A large proportion of the data consisted of Salmonella Typhimurium samples isolated from the pig reservoir, with fewer isolates from other sources, reflecting the low prevalence of Salmonella Typhimurium in these sources.\n\nThe dataset was unbalanced, which posed challenges during the modeling process. To address this, upsampling was performed to adjust the sample sizes in each source category to match the number of domestically produced pigs in the dataset. This upsampling procedure was applied after feature reduction but before model selection. However, it is important to note that identical isolates might have been included in both the training and testing datasets, potentially leading to an overestimation of accuracy for the machine learning algorithms tested during the model selection step.\n\nThe final predictive model was developed using the entire dataset, as the sample size was relatively small (n = 210). This approach was taken to ensure that all available data was utilized for model development. The results obtained from the machine learning model were consistent with those from a Bayesian model, suggesting that the machine learning model could serve as a new standard for source attribution when sequence data is available.\n\nThe dataset was limited to include only Salmonella Typhimurium and its monophasic variants. This focus allowed for a more targeted analysis but also means that the findings may not be generalizable to other Salmonella serotypes. The distribution of the dataset reflects the epidemiology of Salmonella in the sources included, assuming that the training data set represents this epidemiology accurately. This approach differs from previous studies that have used different inputs and methodologies, such as the Bayesian model, which incorporates prior information about the distribution of variables in the model.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the ensemble learning class. Specifically, two algorithms were evaluated: logit boost and random forest. These algorithms are well-established and have been successfully applied in various studies analyzing sequence data.\n\nNeither of these algorithms is new; both have been extensively used and validated in the machine learning community. The logit boost algorithm is a type of boosting algorithm that combines multiple weak classifiers to create a strong classifier. The random forest algorithm is an ensemble of decision trees, which helps to improve the accuracy and control over-fitting.\n\nThe choice of these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to provide robust predictions. The study focused on applying these algorithms to the specific problem of source attribution for Salmonella Typhimurium, rather than developing a new algorithm. Therefore, the results and methodologies were published in a journal focused on the application domain rather than a machine-learning journal. The emphasis was on demonstrating the practical utility of these algorithms in a real-world scenario, specifically in the context of public health and food safety.",
  "optimization/meta": "The model described in the publication does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on whole genome sequencing (WGS) data and employs specific machine-learning algorithms directly.\n\nTwo primary machine-learning algorithms were evaluated for the model: logit boost and random forest. The logit boost algorithm was ultimately selected for the final model due to its superior performance in terms of average accuracy. The model construction and evaluation processes involved training the selected algorithm on a dataset comprising food and animal isolates, with performance assessed using valid accuracy, kappa value, and a confusion matrix.\n\nThe training data used for model selection and construction was carefully managed to address class imbalance issues, primarily through upsampling techniques. This process ensured that the model could effectively handle the varying prevalence of Salmonella Typhimurium across different animal reservoirs. However, the final predictive model was developed using the original, non-upsampled dataset to avoid overfitting and to ensure that the model's predictions were generalizable to new, unseen data.\n\nThe independence of the training data is a critical consideration in machine-learning model development. In this case, the dataset was split into training and testing subsets to evaluate the model's performance accurately. Additionally, the use of cross-validation techniques further ensured that the model's performance was assessed on independent data, reducing the risk of overfitting and enhancing the reliability of the results.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, the Salmonella core genome, consisting of 3,002 loci, was analyzed. To reduce the number of features and decrease computing time and complexity, zero variance within each variable was detected and removed. Additionally, the Boruta function was used to iteratively reduce less relevant features by comparing them to randomly generated features, ensuring that only relevant features were retained.\n\nGiven the imbalance in sample sizes among different sources, upsampling was performed. This involved randomly sampling (with replacement) cases from the minority classes to match the size of the majority class. This upsampled dataset was then used for model selection and construction, while the final predictive model was developed from the original, non-upsampled data.\n\nThe machine learning model was developed using data from food and animal sources. Model selection and construction were based on training sets, with the final model developed from the entire dataset. Two machine learning algorithms, logit boost and random forest, were evaluated. These algorithms were chosen for their successful application in studies analyzing sequence data. The models were trained on a randomly generated training dataset (70%) and their performance was estimated using a testing dataset (30%) with cross-validation. Ten iterations were performed, each involving sevenfold cross-validation, where the training dataset was partitioned into seven subsets. The average accuracies from these iterations were reported, and the algorithm with the highest average accuracy was selected for model construction.\n\nThe logit boost algorithm was ultimately chosen due to its superior performance, with an average accuracy of 0.933 compared to 0.783 for the random forest algorithm. The final model's valid accuracy was 0.92, with a kappa value of 0.9033, indicating almost perfect agreement. Sensitivity and specificity were high for most sources, except for domestically produced and imported pigs, which had lower values. The balanced accuracies were greater than 0.8 for all sources except domestically produced cattle, for which the sensitivity and balanced accuracy were inestimable.",
  "optimization/parameters": "The model utilized a core genome consisting of 3,002 loci. However, to enhance computational efficiency and reduce complexity, feature reduction techniques were employed. This process involved detecting zero variance within each variable and iteratively removing less relevant features using statistical comparisons with randomly generated features. Consequently, only 17 core genes were deemed important and selected for further modeling. These 17 features were chosen based on their predictive accuracy for distinguishing between different sources, as reflected in the area under the ROC curve (AUC) values. This reduction in the number of features from 3,002 to 17 significantly streamlined the model, focusing on the most informative genetic markers.",
  "optimization/features": "In the optimization process, feature selection was indeed performed to enhance the model's efficiency and accuracy. Initially, the Salmonella core genome consisted of 3,002 loci. To reduce computational complexity and avoid correlated features, feature reduction techniques were applied. The NearZeroVariance function was used to detect and remove features with zero variance. Additionally, the Boruta function was employed to iteratively reduce less relevant features by comparing them to randomly generated features. This process ensured that only the most informative features were retained for modeling.\n\nAfter feature reduction, 17 core genes were identified as important and selected for further modeling. The importance of these features was determined based on their predictive accuracy for the sources. The values represent the area under the ROC curve (AUC) derived from the prediction of the sources by each feature. This rigorous feature selection process was conducted using the training set only, ensuring that the model's performance on the testing set remained unbiased and reflective of its true predictive power.",
  "optimization/fitting": "The fitting method employed in this study involved a careful balance to avoid both overfitting and underfitting. The dataset consisted of 3,002 loci, which initially presented a high number of features relative to the number of training points. To mitigate the risk of overfitting, several steps were taken.\n\nFirst, feature reduction was performed to decrease the number of loci. This was achieved using the NearZeroVariance function to remove features with zero variance and the Boruta function to iteratively reduce less relevant features. This process ensured that only the most informative features were retained, thereby reducing the complexity of the model and the risk of overfitting.\n\nSecond, class imbalance was addressed through upsampling. The dataset had unequal sample sizes among different sources, which could influence model fitting. Upsampling was performed to balance the classes, ensuring that the model was not biased towards the majority class. This step helped in creating a more robust model that could generalize well to unseen data.\n\nModel selection involved evaluating two machine learning algorithms: logit boost and random forest. These algorithms were chosen for their proven effectiveness in analyzing sequence data. The models were trained on a randomly generated training dataset (70%) and their performance was estimated using a testing dataset (30%) with cross-validation. Ten iterations were performed, each applying sevenfold cross-validation, to ensure that the model's performance was consistent and not due to chance.\n\nThe logit boost algorithm was selected as the best-performing model based on its higher average accuracy. The final model was constructed using the complete, non-upsampled dataset to ensure that it learned as much as possible about the variability in the data. This approach helped in creating a predictive model that was both accurate and generalizable.\n\nTo further validate the model, its results were compared with those obtained from a Bayesian source attribution model. The alignment of results between the two models suggested that the machine learning model was not overfitting and was capable of making reliable predictions. Additionally, the model's performance was evaluated using valid accuracy, kappa value, and confusion matrix, providing a comprehensive assessment of its predictive capabilities.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One of the key steps involved feature reduction. We started with a core genome of Salmonella consisting of 3,002 loci. To decrease computing time and complexity, and to avoid correlated features, we used the NearZeroVariance function to detect and remove features with zero variance. Additionally, we utilized the Boruta function to iteratively reduce less relevant features by statistically comparing them to randomly generated features. This process helped in excluding irrelevant features that did not provide useful information for distinguishing between different sources.\n\nAnother crucial step was addressing class imbalance. Our dataset had unequal sample sizes among the sources, which could influence model fitting. To mitigate this issue, we performed upsampling using the upsample function. This involved randomly sampling (with replacement) cases from the minority classes to match the size of the majority class. The upsampled dataset was then used for model selection and construction, while the final predictive model was developed from the original, non-upsampled dataset.\n\nDuring model selection, we evaluated two machine learning algorithms: logit boost and random forest. Both algorithms have been successfully applied in studies analyzing sequence data. We trained the models on a randomly generated training dataset (70%) and estimated performance using a testing dataset (30%) with cross-validation. We performed ten iterations, each applying sevenfold cross-validation. This process involved randomly partitioning the training dataset into seven subsets, using one held-out subset for testing and the remaining six for training. The procedure was repeated until all subsets had been held out and predicted, ensuring that the model's performance was robust and not overfitted to any particular subset of the data. The algorithm with the highest average accuracy was selected for the final model construction.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model employed in this study is primarily a black-box model, particularly when considering the logit boost algorithm that was ultimately selected for its superior performance. Black-box models are known for their complexity and lack of transparency, making it challenging to interpret the internal workings and decision-making processes.\n\nHowever, certain aspects of the model's performance and predictions can be interpreted through various evaluation metrics and visualizations. For instance, the confusion matrix provides a clear view of how well the model predicts different sources. It shows the percentage of correctly and incorrectly predicted cases for each source, offering insights into the model's strengths and weaknesses. For example, domestically produced broilers, imported cattle, imported ducks, and domestically produced layers were correctly predicted with high accuracy. Conversely, a notable portion of domestically produced pigs and imported pigs were misclassified as domestically produced broilers.\n\nAdditionally, the sensitivity, specificity, and balanced accuracy values for each source offer further interpretability. These metrics indicate the model's ability to correctly identify positive cases (sensitivity) and negative cases (specificity) for each source, as well as the overall balanced accuracy, which is the average accuracy obtained on either class. For most sources, the sensitivity and specificity were around 0.9, indicating strong performance. However, there were exceptions, such as domestically produced pigs, which had lower sensitivity and specificity.\n\nThe kappa value, which measures the agreement between predicted and observed sources, was also calculated. A kappa value of 0.9033 was achieved, indicating almost perfect agreement according to established criteria. This metric provides a quantitative measure of the model's reliability and interpretability.\n\nFurthermore, the probability that each human case originated from a specific source was predicted, and these probabilities were visualized. This visualization helps in understanding the distribution of predicted sources for human cases, although cases with unknown sources are not shown.\n\nIn summary, while the logit boost model itself is a black-box model, various evaluation metrics and visualizations provide interpretability. These include the confusion matrix, sensitivity, specificity, balanced accuracy, kappa value, and probability visualizations, all of which offer insights into the model's performance and predictions.",
  "model/output": "The model developed is a classification model. It is designed to predict the source of human salmonellosis cases. The model assigns probabilities to different sources for each human case, with the sum of probabilities equaling the number of cases attributed to each source. The model also includes an unknown source category for cases where the source could not be predicted. The final model was constructed using a logit boost algorithm, which demonstrated high valid accuracy and a kappa value indicating almost perfect agreement between predicted and observed sources. The model's performance was evaluated using metrics such as valid accuracy, kappa value, and confusion matrix, which cross-tabulates observed and predicted cases. The model was validated by comparing its results with those obtained from a Bayesian source attribution model, showing consistent outcomes. The model's output provides a probabilistic attribution of human salmonellosis cases to specific sources, aiding in public health interventions and understanding the epidemiology of Salmonella.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method for the machine learning model involved several key steps to ensure its performance and reliability. Initially, two algorithms, logit boost and random forest, were evaluated using a training dataset that constituted 70% of the available data, while the remaining 30% served as the testing dataset. This process was repeated over ten iterations, each employing sevenfold cross-validation. Cross-validation involved partitioning the training data into seven subsets, where one subset was held out for validation while the model was trained on the remaining six subsets. This procedure was repeated until all subsets had been held out and predicted, providing a robust estimate of the model's performance.\n\nThe performance of the models was assessed using valid accuracy, which measured the ability to predict the labels of animal and food isolates in the testing dataset. The results were also expressed in a confusion matrix, which cross-tabulated observed and predicted cases. Additionally, the kappa value was calculated from the confusion matrix to reflect the agreement between predicted and observed sources. This metric is crucial as it accounts for the possibility of the agreement occurring by chance.\n\nThe final model was constructed using the entire dataset, without upsampling, to ensure it learned as much as possible about the variability in the data. However, the accuracy and confusion matrix obtained from this final model were not reported, as they would be misleading and artificially precise.\n\nIn summary, the evaluation method relied on cross-validation and the use of a confusion matrix and kappa value to assess the model's performance. This approach ensured that the model was thoroughly tested and validated before being applied to predict human cases.",
  "evaluation/measure": "In the evaluation of our models, several performance metrics were reported to provide a comprehensive assessment of their effectiveness. The primary metrics included valid accuracy, the kappa value, and the confusion matrix.\n\nValid accuracy was determined by the models' ability to correctly predict the labels of the animal and food isolates in the testing dataset. This metric provides a straightforward measure of the model's predictive performance. The kappa value, calculated from the row and column sums in the confusion matrix, reflects the agreement between the predicted and observed sources. A kappa value of 0.9033 was achieved, indicating almost perfect agreement according to Landis and Koch's criteria and excellent agreement according to Fleiss et al.\n\nThe confusion matrix was used to cross-tabulate observed and predicted cases, offering a detailed view of the model's performance across different sources. This matrix helps identify specific areas where the model performs well or poorly, such as the correct prediction of domestically produced broilers, imported cattle, imported ducks, and domestically produced layers, while highlighting misclassifications like 38% of domestically produced pigs and 27% of imported pigs being wrongly predicted as domestically produced broilers.\n\nAdditionally, sensitivity and specificity were reported for most sources, with values generally around 0.9, except for domestically produced and imported pigs, which had lower values of 0.6 and 0.7, respectively. Balanced accuracies, defined as the average accuracy obtained on either class, were greater than 0.8 for all sources except domestically produced cattle, for which these metrics were inestimable.\n\nThese metrics are representative of standard practices in the literature, ensuring that our evaluation is both rigorous and comparable to other studies in the field. The use of valid accuracy, kappa value, and confusion matrix provides a robust framework for assessing model performance, while sensitivity and specificity offer insights into the model's ability to correctly identify positive and negative cases across different sources.",
  "evaluation/comparison": "In the evaluation of our machine learning models, a comparison was made between two algorithms: logit boost and random forest. Both algorithms have been successfully applied in studies analyzing sequence data. The performance of these algorithms was evaluated using a dataset that included Salmonella Typhimurium and its monophasic variants from various sources such as pigs, broilers, ducks, layers, and cattle, both domestically produced and imported. The dataset also included human isolates.\n\nThe models were trained on a randomly generated training dataset (70%) and their performance was estimated using a testing dataset (30%) with cross-validation. Ten iterations were performed, each applying sevenfold cross-validation. This process involved randomly partitioning the training dataset into seven subsets, with one subset held out for validation while the model was built on the remaining six subsets. This procedure was repeated until all subsets had been held out and predicted.\n\nThe average accuracy for all 10 iterations was 0.783 for the random forest algorithm and 0.933 for the logit boost algorithm. Based on these results, the logit boost algorithm was selected for constructing the final model due to its higher average accuracy.\n\nAdditionally, the final model's performance was compared with a Bayesian source attribution model, often referred to as the Hald model. This comparison involved applying both models to the same dataset and evaluating the results. The source attribution results obtained from the logit boost model were found to be in line with those from the Bayesian model, suggesting that the machine learning model is a reliable alternative for source attribution when sequence data is available.\n\nThe comparison with the Bayesian model was particularly important because it provided a benchmark against an established method. The Bayesian model uses a framework that includes Markov Chain Monte Carlo simulations to estimate the number of human salmonellosis cases attributed to each source. This comparison validated the machine learning approach, showing that it can produce comparable results to a well-established method.\n\nIn summary, the evaluation involved a thorough comparison of different machine learning algorithms and a benchmarking against a publicly available Bayesian method. This comprehensive approach ensured that the selected model was robust and reliable for source attribution of Salmonella Typhimurium and its monophasic variants.",
  "evaluation/confidence": "The evaluation of our models included the calculation of confidence intervals for the performance metrics. For instance, the average accuracy for the random forest model was reported as 0.783 with a 95% confidence interval (CI) of 0.77 to 0.80. Similarly, the logit boost algorithm achieved an average accuracy of 0.933 with a 95% CI of 0.92 to 0.94. These intervals provide a range within which the true accuracy is likely to fall, indicating the reliability of our estimates.\n\nThe valid accuracy for the logit boost model was 0.92 with a 95% CI of 0.87 to 0.96, while the random forest model had a valid accuracy of 0.74 with a 95% CI of 0.68 to 0.79. The kappa value, which measures the agreement between predicted and observed sources, was 0.9033 for the logit boost model, classified as almost perfect, and 0.6982 for the random forest model.\n\nThe statistical significance of our results is supported by these confidence intervals, which do not overlap between the two models, suggesting that the logit boost algorithm is superior to the random forest model. Additionally, the sensitivity and specificity values, along with balanced accuracies, were greater than 0.8 for most sources, further validating the robustness of our models. The comparison with the random forest model and the Bayesian source attribution model (Hald model) also reinforces the reliability and superiority of our approach.",
  "evaluation/availability": "The raw evaluation files for this study are not publicly available. The evaluation process involved constructing and evaluating models using specific datasets, but these datasets were not released publicly. The study focused on the performance metrics derived from these evaluations, such as valid accuracy, kappa value, and confusion matrices, rather than making the raw data accessible. The models were validated internally by comparing results with a Bayesian source attribution model, ensuring the robustness of the findings. However, the specific datasets used for training and testing the models were not shared, and there is no mention of a public repository or license for accessing these raw evaluation files."
}