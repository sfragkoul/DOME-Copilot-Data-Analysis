{
  "publication/title": "Pharm-AutoML: An open-source, end-to-end automated machine learning package for clinical outcome prediction.",
  "publication/authors": "Liu G, Lu D, Lu J",
  "publication/journal": "CPT: pharmacometrics & systems pharmacology",
  "publication/year": "2021",
  "publication/pmid": "33793093",
  "publication/pmcid": "PMC8129712",
  "publication/doi": "10.1002/psp4.12621",
  "publication/tags": "- Automated Machine Learning\n- End-to-End Machine Learning\n- Clinical Outcome Prediction\n- Feature Importance\n- Model Interpretation\n- Gradient Boosting\n- SHAP Analysis\n- Hyperparameter Optimization\n- Biomedical Data\n- Machine Learning Models\n- Logistic Regression\n- Receiver Operating Characteristic\n- Precision Recall Curve\n- Cervical Cancer\n- Biomedical Relevance",
  "dataset/provenance": "Not enough information is available.",
  "dataset/splits": "In the dataset splits, the data is initially divided into two main sets: training data and test data. The training data constitutes 80% of the total dataset, while the test data makes up the remaining 20%. This split is performed in a stratified manner to ensure that the proportions of target variables are preserved in both the training and test sets.\n\nThe training data is further divided using n-fold cross-validation. This process involves splitting the training data into n folds, where n is a user-defined parameter. Each fold is used as a validation set once, while the remaining folds are used for training. This cross-validation procedure is repeated n_repeats times with different randomizations, controlled by the parameter n_repeats. The purpose of these additional splits is to reduce model overfitting and to identify the most stable model among all classifiers. The test data remains independent and is not used until the final evaluation of the fine-tuned model.",
  "dataset/redundancy": "The datasets used in our study were split into training and test sets in a stratified manner, ensuring that the proportions of target variables were preserved in both sets. This approach helps maintain the independence of the test data, which is crucial for evaluating the model's performance accurately. The test data is not used until the final model evaluation stage, ensuring that it remains independent from the training process.\n\nTo further enhance the model selection process, the training data is split into fold-specific training and validation sets using n_folds cross-validation. This procedure is repeated n_repeats times with different randomizations, controlled by the parameter n_repeats. This repeated stratified splitting helps in reducing model overfitting and identifies the most stable model among all classifiers.\n\nThe distribution of our datasets is comparable to previously published machine learning datasets, particularly those in biomedical research. We selected five representative biomedical datasets from the University of California, Irvine (UCI) Machine Learning repository. These datasets include heart failure clinical records, breast cancer data, hepatitis data, chronic kidney disease data, and risk factors for cervical cancer. Each dataset was standardized to meet the input requirements of our Pharm-AutoML package, ensuring consistency and compatibility across different analyses.\n\nBy maintaining the independence of the test data and using stratified splitting, we ensure that our models are robust and generalizable. This approach aligns with best practices in machine learning, providing reliable and reproducible results.",
  "dataset/availability": "The datasets used in our study are publicly available from the University of California, Irvine (UCI) Machine Learning Repository. Specifically, we utilized five biomedical outcome prediction datasets: the heart failure clinical record dataset, the breast cancer dataset, the hepatitis dataset, the chronic kidney disease dataset, and the risk factors for cervical cancer dataset. These datasets were standardized to meet the input requirements of Pharm- AutoML and are accessible on the Pharm- AutoML GitHub page.\n\nThe data, including the splits used for training and testing, are provided in a format that ensures reproducibility. The datasets are shared under the terms of the Creative Commons Attribution-NonCommercial-NoDerivs License, which allows for use and distribution in any medium, provided the original work is properly cited, the use is non-commercial, and no modifications or adaptations are made.\n\nTo enforce the proper use of these datasets, users are required to adhere to the licensing terms specified. This includes acknowledging the original source and ensuring that any use of the data is non-commercial and unmodified. The datasets are made available in a structured format, such as comma-separated values (CSVs), which includes both the input features and prediction targets as columns, with instances given as rows. This format ensures that the data can be easily integrated into the Pharm- AutoML pipeline for analysis and model training.",
  "optimization/algorithm": "The optimization algorithm employed in our work leverages the hyperopt Python package to search for the best set of hyperparameters. This approach is designed to optimize the accuracy-based loss function in each classifier. The hyperopt package is well-established and widely used in the machine learning community for hyperparameter optimization, making it a reliable choice for our purposes.\n\nThe machine-learning algorithm class used is not new; it is a well-known technique in the field of automated machine learning (AutoML). The focus of our work is not on introducing a novel machine-learning algorithm but rather on developing an end-to-end AutoML solution tailored for clinical outcome prediction in pharmacological interventions. This solution automates the construction of machine learning models, including data preprocessing, model tuning, model selection, results analysis, and model interpretation.\n\nThe reason this work was published in a pharmacometrics journal rather than a machine-learning journal is that the primary application and innovation lie in the pharmaceutical and healthcare domains. The development of Pharm-AutoML addresses the specific needs of biopharmaceutical researchers, who often lack the expertise necessary to apply advanced machine-learning techniques. By providing an automated, end-to-end solution, Pharm-AutoML lowers the technical barrier to applying machine learning in drug development and clinical pharmacology, making it a valuable tool for researchers in these fields.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. Instead, it employs a variety of machine learning classifiers within the scikit-learn library, including logistic regression, random forest, extra trees, adaboost, support vector machine, k-nearest neighbors, gradient boosting, stochastic gradient descent, and XGBoost. These classifiers are used to evaluate prediction performance across validation datasets.\n\nThe process involves splitting the preprocessed data into training and test sets in a stratified manner, ensuring that the proportions of target variables are preserved in both sets. The training data is further split into fold-specific training data and validation data using n-fold cross-validation. This approach helps in reducing model overfitting and identifying the most stable model among all classifiers.\n\nThe independence of the test data is preserved by not using it until the final evaluation of the fine-tuned model. The model selection module standardizes features in the training data and transforms the validation data accordingly. For high-dimensional data, principal component analysis (PCA) can be applied to find the optimal number of transformed components.\n\nThe hyperparameter search space is customized using representative data examples to optimize performance on biomedical datasets. For instance, in the XGBoost classifier, the hyperparameter min_child_weight is set with an upper bound of 10 to accommodate the typically small sample sizes in healthcare applications. This ensures that the model performs well on validation data by preventing a large portion of samples from falling into a few particular leaves.\n\nThe results analysis module refits the fine-tuned models on all training data and evaluates their prediction performance on test data. Various performance metrics, such as accuracy, F-1 score, sensitivity, specificity, AUROC, and AUPRC, are implemented to reveal the performance of the selected model. Plots showing n-fold results of the ROC and precision-recall curves are generated and saved.\n\nFeature importance is assessed using both model-dependent and model-agnostic methods. The model-dependent method uses scikit-learn's feature importance functionality, while the model-agnostic methods include drop-column feature importance and SHAP analysis. These methods help in understanding how useful the features are in contributing toward predicting the target variable.",
  "optimization/encoding": "In our work, data encoding and preprocessing are integral steps in preparing the input data for the machine-learning algorithms. The process begins with handling missing values and selecting relevant features. For missing imputation, we first remove instances lacking prediction targets and features with identical values. Users can also manually exclude specific features. Features with a missing fraction higher than a specified threshold are dropped, as are redundant features identified by a Pearson correlation coefficient above a set threshold.\n\nCategorical features with a high ratio of unique subjects are removed, and imputation strategies are applied based on whether the feature is numerical or categorical. By default, categorical features are imputed with the most frequent value, while numerical features are imputed with the mean value. Categorical features are then one-hot encoded, and a Boolean parameter determines whether to add new features to label all missing values.\n\nThe data is then split into training and test sets in a stratified manner, preserving the proportions of target variables. The training data is further split into fold-specific training and validation data using n_folds cross-validation, repeated n_repeats times to ensure robustness. Features in the training data are standardized to have zero mean and unit variance, and the same transformation is applied to the validation data. For high-dimensional data, such as genomics data, principal component analysis (PCA) can be applied to find the optimal number of transformed components.\n\nThis preprocessing pipeline ensures that the data is clean, relevant, and appropriately formatted for the machine-learning algorithms, enhancing the model's performance and interpretability.",
  "optimization/parameters": "In our implementation, the number of parameters, p, used in the model can vary significantly depending on the specific machine learning algorithm and the dataset being used. Our framework supports a wide range of classifiers, including logistic regression, random forest, extra trees, adaboost, support vector machines, k-nearest neighbors, gradient boosting, stochastic gradient descent, and XGBoost. Each of these classifiers has its own set of hyperparameters that need to be optimized.\n\nTo select the optimal set of hyperparameters, we employ Bayesian optimization. This method is particularly effective for hyperparameter tuning as it efficiently explores the hyperparameter space to find the best combination that maximizes the model's performance. The hyperparameter search space is customized based on five representative biomedical datasets to ensure that the selected hyperparameters are appropriate for datasets of biomedical relevance.\n\nFor example, in the XGBoost classifier, one critical hyperparameter is `min_child_weight`, which refers to the minimum number of samples required to be at a leaf node. In healthcare applications, where sample sizes are often small, setting this parameter too high can lead to poor model performance. Therefore, we have set an upper bound for `min_child_weight` to ensure that the model can effectively learn from the available data.\n\nThe specific number of parameters, p, is not fixed and depends on the chosen classifier and the results of the hyperparameter optimization process. This flexibility allows our framework to adapt to different datasets and tasks, ensuring robust and accurate predictions.",
  "optimization/features": "In our study, the number of input features (f) can vary depending on the dataset and the preprocessing strategy applied. We support both numerical and categorical data, including qualitative data with or without missing values. Users have the flexibility to specify which features should be treated as categorical.\n\nFeature selection is indeed performed as part of our data preprocessing step. This process involves several stages. Initially, instances lacking prediction targets are removed. Features that take on identical values are also eliminated. Users can manually ignore specific features using the drop_features setting. Additionally, features with a missing fraction higher than the missing_threshold are dropped. Redundant features, identified by a Pearson correlation coefficient greater than the correlation_threshold, are also removed, with the second feature of each redundant pair being discarded.\n\nThis feature selection process is conducted using the training data only, ensuring that the test data remains independent and unbiased. The resulting selected features are then used for subsequent steps in the machine learning pipeline.",
  "optimization/fitting": "The fitting method employed in our work addresses the challenges of both overfitting and underfitting through several strategies.\n\nTo mitigate overfitting, especially when dealing with high-dimensional data, we utilize cross-validation techniques. Specifically, we perform n-fold cross-validation, where the data is split into n subsets. The model is trained on n-1 subsets and validated on the remaining subset, repeating this process n times. This approach ensures that each data point is used for both training and validation, providing a robust estimate of model performance.\n\nAdditionally, we implement regularization techniques within our models. For instance, in tree-based models like XGBoost and gradient boosting, we carefully tune hyperparameters such as min_child_weight to prevent the model from becoming too complex. By setting an upper bound for min_child_weight, we ensure that the model does not overfit to the training data, especially in scenarios with a limited number of samples.\n\nTo address underfitting, we employ Bayesian optimization to search for the best set of hyperparameters. This method helps in finding an optimal balance between model complexity and performance, ensuring that the model is neither too simple nor too complex. Furthermore, we use feature selection and imputation techniques to preprocess the data, removing irrelevant features and handling missing values appropriately. This preprocessing step ensures that the model is trained on high-quality data, reducing the risk of underfitting.\n\nIn summary, our fitting method combines cross-validation, regularization, and Bayesian optimization to effectively manage overfitting and underfitting, resulting in models that generalize well to unseen data.",
  "optimization/regularization": "In our work, several techniques were employed to prevent overfitting and ensure the robustness of our models. One key method involved the use of cross-validation, specifically n-fold cross-validation. This process splits the training data into multiple folds, allowing the model to be trained and validated on different subsets of the data. By repeating this process multiple times with different randomizations, as controlled by the parameter n_repeats, we ensure that the model's performance is stable and not overly fitted to any particular subset of the data.\n\nAdditionally, we utilized Bayesian optimization to search for the best set of hyperparameters. This approach helps in finding an optimal balance between model complexity and performance, further reducing the risk of overfitting.\n\nFor high-dimensional data, such as genomics data, we applied principal component analysis (PCA) to reduce the dimensionality of the features. This not only helps in managing computational efficiency but also in mitigating the risk of overfitting by focusing on the most informative components of the data.\n\nFurthermore, we implemented feature selection and imputation strategies during the data preprocessing stage. By removing redundant features and handling missing values appropriately, we ensure that the model is trained on a clean and relevant dataset, which contributes to better generalization and reduced overfitting.\n\nIn summary, our approach incorporates multiple layers of regularization and validation techniques to prevent overfitting, ensuring that our models are robust and generalizable to new, unseen data.",
  "optimization/config": "The hyperparameter configurations and optimization parameters used in our framework are available and can be accessed through the source code. Specifically, the upper bound for the hyperparameter `min_child_weight` in the XGBoost classifier is set to 10, which is crucial for handling the relatively small sample sizes typical in healthcare applications. This configuration is detailed in the source code file located at `src_autoML/hpsklearn/components.py`.\n\nThe optimization process leverages the Python package `hyperopt` to search for the best set of hyperparameters, aiming to optimize the accuracy-based loss function in each classifier. The prediction performance of each model with optimal hyperparameters is evaluated across sets of validation data.\n\nFor those interested in replicating or building upon our work, the source code and relevant configurations are accessible. However, specific details about the license under which this code is distributed are not provided here. Users are encouraged to refer to the repository or contact the authors for licensing information.\n\nThe results of the optimization, including the performance metrics and model interpretations, are saved in the specified `result_path`. This includes various performance metrics such as accuracy, F-1 score, sensitivity, specificity, AUROC, and AUPRC. Additionally, plots showing the n-fold results of the ROC and precision-recall curves are generated and saved in the same directory.\n\nIn summary, the hyperparameter configurations, optimization schedule, and model files are available through the source code, and the results of the optimization are saved in the specified result path. For detailed licensing information, users should refer to the repository or contact the authors.",
  "model/interpretability": "Our model, Pharm-AutoML, is designed with interpretability in mind, ensuring that it is not a black box. We employ two main approaches to interpret our models: model-dependent and model-agnostic interpreters.\n\nFor model-dependent interpretation, we use scikit-learn feature importance plots. This method is particularly useful for tree-based models like Gradient Boosting (GB) classifiers. By analyzing the algorithm's characteristics, we can determine the importance of each feature in making predictions. For instance, in our study on predicting biopsy examination outcomes for a cervical cancer dataset, we found that features like \"Schiller,\" \"Hinselmann,\" and \"Citology\" had the strongest impact on the model's outcome.\n\nAdditionally, we utilize SHAP (SHapley Additive exPlanations) analysis, a model-agnostic interpreter. SHAP values provide a way to explain the output of any machine learning model. They indicate the contribution of each feature to the prediction, regardless of the model's underlying algorithm. This allows us to explore and understand the behavior of various high-performing algorithms, including XGBoost. The SHAP summary plot visually represents the rank of features extracted from the model, providing a clear and intuitive understanding of feature importance.\n\nFurthermore, we compared the SHAP values from our GB classifier with the regression coefficients from a logistic regression model. The high correlation between these values (Pearson correlation coefficient of 0.9052) demonstrates the consistency between interpreting nonlinear models with SHAP values and linear models with regression coefficients.\n\nIn summary, Pharm-AutoML ensures transparency by incorporating both model-dependent and model-agnostic interpretation methods. This approach allows users to understand the underlying mechanisms of their models, making it a valuable tool for researchers in biopharmaceutical development and healthcare.",
  "model/output": "The model discussed in this publication is designed for classification tasks. Specifically, it focuses on predicting biopsy examination outcomes from patient features within a cervical cancer dataset. The model's performance is evaluated using metrics such as accuracy, F-1 score, sensitivity, specificity, area under the receiver operating characteristic curve (AUROC), and area under the precision-recall curve (AUPRC). The model's output includes various plots, such as ROC curves and precision-recall curves, which are generated and saved for analysis. Additionally, the model provides feature importance plots using methods like scikit-learn's feature importance and SHAP analysis, which help in interpreting the model's predictions. The model's performance is compared with other traditional machine learning models, and it demonstrates competitive or superior performance across different datasets, particularly for heart, breast, and cervical cancer datasets. The model's stability and ability to prevent overfitting are also highlighted, as shown by its performance on test data.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for our software, Pharm- AutoML, is publicly available as an open-source Python package. This package enables users to automate the construction of machine learning models and predict clinical outcomes, particularly in the context of pharmacological interventions. It streamlines various steps within the machine learning workflow, including data preprocessing, model tuning, model selection, results analysis, and model interpretation.\n\nPharm- AutoML is designed to be user-friendly, allowing both novice and expert users to leverage its capabilities without requiring complex coding. The package is built upon popular open-source libraries such as scikit-learn and hyperopt, ensuring ease of maintenance and utilization of standardized hyperparameters for machine learning models.\n\nTo facilitate its use, Pharm- AutoML is available on the Pharm- AutoML GitHub page, where users can access the source code, contribute to its development, and stay updated on the latest features and improvements. The package is released under the Creative Commons Attribution-NonCommercial-NoDerivs License, which permits use and distribution in any medium, provided the original work is properly cited, the use is non-commercial, and no modifications or adaptations are made.",
  "evaluation/method": "In our evaluation of Pharm-AutoML, we employed a rigorous methodology to ensure the robustness and generalizability of our results. We utilized n-fold cross-validation to assess the prediction performance of various machine learning models across different datasets. This approach involved splitting the data into n subsets, training the models on n-1 subsets, and validating on the remaining subset. This process was repeated n times, with each subset serving as the validation set once.\n\nTo compare the performance of Pharm-AutoML with alternative frameworks, we evaluated five different machine learning or AutoML implementations on each of five biomedical datasets. These models included logistic regression, XGBoost classifiers, and the H2O framework, all with default settings. We also tested Pharm-AutoML with and without imputation. The datasets used for comparison included heart failure clinical records, breast cancer data, hepatitis data, chronic kidney disease data, and risk factors for cervical cancer.\n\nFor each dataset, we randomly stratified the data into training and test sets 100 times. This stratification ensured that the proportions of target variables were preserved in both the training and test sets. The selected optimal models were then refitted on the training data and evaluated on the test data. The area under the receiver operating characteristic curve (ROC-AUC) was used as the primary metric to measure performance across different models.\n\nOur results demonstrated that Pharm-AutoML either outperformed or matched the performance of other frameworks, particularly for the heart, breast, and cervical cancer datasets. This evaluation underscores the effectiveness and reliability of Pharm-AutoML in handling complex biomedical data and generating interpretable results.",
  "evaluation/measure": "In the evaluation of our models, we employ a comprehensive set of performance metrics to ensure a thorough assessment of their predictive capabilities. These metrics include accuracy (ACC), F-1 score, sensitivity (SEN), specificity (SPE), area under the receiver operating characteristic curve (AUROC), and area under the precision-recall curve (AUPRC). These metrics are widely recognized in the literature and provide a robust evaluation framework.\n\nAccuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. The F-1 score is the harmonic mean of precision and recall, offering a balance between these two metrics, which is particularly useful in imbalanced datasets. Sensitivity, also known as recall or true positive rate, indicates the proportion of actual positives that are correctly identified by the model. Specificity, or the true negative rate, measures the proportion of actual negatives that are correctly identified.\n\nThe AUROC is a widely used metric that evaluates the model's ability to distinguish between classes by plotting the true positive rate against the false positive rate at various threshold settings. A higher AUROC indicates better model performance. Similarly, the AUPRC evaluates the precision-recall trade-off, which is crucial for imbalanced datasets where the precision-recall curve provides a more informative assessment than the ROC curve.\n\nIn addition to these metrics, we generate plots showing the n-fold results of the receiver operating characteristic (ROC) and precision-recall curves. These visualizations help in understanding the model's performance across different folds and provide insights into its stability and generalization capabilities. The plots are saved in the result path for further analysis and comparison.\n\nBy using these metrics and visualizations, we ensure that our models are evaluated comprehensively, providing a clear and representative assessment of their performance. This approach aligns with established practices in the literature, ensuring that our results are comparable and reliable.",
  "evaluation/comparison": "In the evaluation of our Pharm- AutoML package, we conducted a comprehensive comparison with several publicly available machine learning (ML) methods and frameworks. To ensure a fair and robust assessment, we utilized five representative biomedical datasets from the University of California, Irvine (UCI) ML repository. These datasets included outcomes related to heart failure, breast cancer, hepatitis, chronic kidney disease, and risk factors for cervical cancer.\n\nFor each dataset, we compared the performance of Pharm- AutoML against five different ML or AutoML implementations. These included two scikit-learn models\u2014logistic regression and XGBoost classifiers, both configured with default settings\u2014and the H2O framework, also with default settings. Additionally, we evaluated Pharm- AutoML with and without imputation to handle missing values, providing a thorough comparison across various scenarios.\n\nThe performance of these models was assessed using the area under the receiver operating characteristic curve (ROC-AUC), a widely accepted metric for evaluating classification models. To ensure the reliability of our results, we randomly stratified each dataset into training and test sets 100 times. This process involved refitting the selected optimal models on the training data and evaluating their performance on the test data. The results demonstrated that Pharm- AutoML either outperformed or matched the performance of the H2O framework with default settings across all five datasets. Notably, Pharm- AutoML models significantly outperformed other models, particularly for the heart, breast, and cervical cancer datasets.\n\nThis rigorous comparison highlights the competitive performance of Pharm- AutoML, showcasing its effectiveness in handling complex biomedical data and providing reliable predictions. The inclusion of simpler baselines, such as logistic regression and default settings for XGBoost and H2O, ensures that our evaluation is comprehensive and that Pharm- AutoML's advantages are clearly demonstrated.",
  "evaluation/confidence": "The evaluation of Pharm-AutoML's performance includes several metrics with associated confidence intervals. For instance, the model performance is ranked by the ROC AUC metric, and error bars indicating the standard error of the ROC values over the fivefolds are provided. This allows for an assessment of the variability and reliability of the model's performance.\n\nThe comparison of Pharm-AutoML with alternative frameworks involves evaluating five different ML or AutoML implementations on five biomedical datasets. The performance is measured using the area under the ROC curve (ROC-AUC), and the results demonstrate that Pharm-AutoML either outperforms or matches the performance of other frameworks, such as H2O, with default settings. Specifically, Pharm-AutoML models significantly outperform other models for heart, breast, and cervical cancer datasets, indicating statistical significance in these comparisons.\n\nAdditionally, the stability of the models across different folds of validation data is evaluated, showing that Pharm-AutoML can prevent overfitting. This is further supported by the evaluation of model performance using test data, which confirms the robustness and generalizability of the models generated by Pharm-AutoML. The use of 100 random stratifications of the dataset into training and test data ensures that the performance metrics are reliable and not due to chance.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being publicly available. The publication discusses the use of various datasets, including those from the University of California, Irvine (UCI) Machine Learning repository, but it does not specify whether the raw evaluation files from the experiments conducted are publicly released. The results and model interpretation plots are saved in a results directory, but there is no indication that these directories or their contents are made publicly accessible. Additionally, the publication does not provide details on any specific licenses under which the data or results might be shared."
}