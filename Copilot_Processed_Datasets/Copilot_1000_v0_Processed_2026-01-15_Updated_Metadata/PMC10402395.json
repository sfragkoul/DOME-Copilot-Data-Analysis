{
  "publication/title": "Leveraging machine learning to identify acute myeloid leukemia patients and their chemotherapy regimens in an administrative database.",
  "publication/authors": "Cao L, Huang YS, Wu C, Getz K, Miller TP, Ruiz J, Fisher BT, Seif AE, Aplenc R, Li Y",
  "publication/journal": "Pediatric blood & cancer",
  "publication/year": "2023",
  "publication/pmid": "36815580",
  "publication/pmcid": "PMC10402395",
  "publication/doi": "10.1002/pbc.30260",
  "publication/tags": "- Machine Learning\n- Pediatric Health Information System\n- Acute Myeloid Leukemia\n- Support Vector Machine\n- Random Forest\n- Logistic Regression\n- Cross-validation\n- Chemotherapy Regimens\n- Data Matching\n- Electronic Health Records",
  "dataset/provenance": "The dataset utilized in this study is sourced from the Pediatric Health Information System (PHIS), which aggregates clinical and resource utilization data from 50 pediatric tertiary hospitals across all geographical regions in the United States. This system serves as a robust data source for machine learning (ML) applications and the assembly of rare disease cohorts.\n\nThe study involved a total of 6790 patients from 46 hospitals, determined to be eligible for the ML model application. These patients were identified using PHIS discharge data spanning from January 2004 through March 2020. The dataset includes both AML ICD-9 and ICD-10 discharge diagnosis codes, ensuring comprehensive coverage of relevant patient records.\n\nThe PHIS data has been previously used in similar studies, demonstrating its reliability and utility in pediatric research. For instance, the Home or Away From Home (HAFH) study, which included patients aged 0\u201318 years with newly diagnosed AML treated at 17 US pediatric centers between 2011 and 2019, utilized standardized manual chart abstractions. This external validation cohort served as a gold standard (GS) for comparing and validating the ML models developed in our study.\n\nThe dataset's stability and the consistent nature of pediatric AML chemotherapy regimens over the past two decades make it a valuable resource for ongoing and future research. The methods developed using this dataset can be easily generalized and adapted to similar databases reporting claim/billing data for drug administration, ensuring broad applicability and relevance in the medical community.",
  "dataset/splits": "The dataset was split into two main parts: a training set and a hold-out test set. The training set consisted of 75% of the manual-review cohort, which amounted to 1918 patients. The hold-out test set, used for internal validation, comprised the remaining 25% of the cohort, totaling 640 patients. This split was done randomly from the 2558 hospitalized patients with ICD-9 AML diagnosis codes and relevant chemotherapy drug utilization identified between January 2004 and September 2014.\n\nAdditionally, during the machine learning process, a five-fold cross-validation was employed for algorithm optimization and selection of the best-performing method. This means the training data was further divided into five subsets, with the model being trained and validated five times, each time using a different subset as the validation set and the remaining four subsets as the training set. This approach ensures that the model's performance is robust and generalizable.",
  "dataset/redundancy": "The dataset utilized in this study was derived from the Pediatric Health Information System (PHIS), which includes clinical and resource utilization data from 50 pediatric tertiary hospitals across all geographical regions in the United States. The dataset comprised 2,558 hospitalized patients with ICD-9 AML diagnosis codes and relevant chemotherapy drug utilization identified between January 2004 and September 2014.\n\nThe dataset was split into training and test sets to ensure independence. Specifically, 75% of the manual-review cohort was randomly selected for the training set, while the remaining 25% was reserved for the hold-out test set, also known as internal validation. This split was designed to evaluate the performance of the machine learning models on unseen data, thereby providing a robust assessment of their generalizability.\n\nTo enforce the independence of the training and test sets, a rigorous data quality and reliability process was implemented through a joint effort between the Children's Hospital Association (CHA) and participating hospitals. This process involved submitting encounter data, including demographics, diagnoses, and procedures, as well as resource utilization data such as pharmaceutical, imaging, laboratory test, and room-and-board information to PHIS.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the field of pediatric oncology. The dataset includes a comprehensive range of features derived from administrative data, which can be easily generalized and adapted to similar databases reporting claim/billing data for drug administration. This approach ensures that the methods developed are not only applicable to the specific dataset used in this study but also to other similar datasets, thereby enhancing the broader utility of the findings.\n\nThe dataset's stability and the consistent nature of pediatric AML chemotherapy regimens over the past two decades further support the reliability of the training and test sets. This stability reduces the need for frequent adjustments to the machine learning models, making them more robust and applicable over extended periods. However, as new regimens are implemented, such as liposome-encapsulated daunorubicin-araC, or CPX, which is under study in the Children\u2019s Oncology Group AAML1831 trial, the models can be updated through similar training and validation procedures. This adaptability ensures that the models remain relevant and accurate as clinical practices evolve.",
  "dataset/availability": "The data supporting the findings of this study are available from the Children\u2019s Hospital Association (CHA). However, there are restrictions on the availability of these data, as they were used under license for this study. The Pediatric Health Information System (PHIS) data utilized in the study will be made available by the authors, adhering to CHA data access policies. This ensures that the data is shared in a controlled manner, respecting the licensing agreements and privacy considerations. The data is not publicly released in a forum but can be accessed through the specified channels, maintaining the integrity and security of the information.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the class of supervised learning algorithms. Specifically, three commonly used algorithms were explored: Logistic Regression (LR), Random Forest (RF), and Support Vector Machine (SVM). These algorithms are well-established in the field of machine learning and are widely used for classification tasks.\n\nThe algorithms employed are not new; they are standard methods in the machine learning community. The choice of these algorithms was driven by their proven effectiveness in handling classification problems, particularly in medical and healthcare data analysis. The study focused on applying these algorithms to identify patients with newly diagnosed Acute Myeloid Leukemia (AML) and to determine the start dates and chemotherapy regimens for up to five treatment courses per patient.\n\nThe decision to use these established algorithms was based on their robustness and reliability in similar contexts. The study aimed to leverage the strengths of these algorithms to achieve high performance in identifying AML cases and chemotherapy regimens. The algorithms were optimized using five-fold cross-validation to ensure that the best-performing method was selected. This approach helped in fine-tuning the models to achieve high sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV).\n\nThe algorithms were implemented using Python 3.7.6 with the scikit-learn library version 1.0.1. Data manipulation was performed using pandas version 1.2.4, and mathematical analysis was conducted using the numPy library version 1.20.1. The use of these tools and libraries facilitated the efficient implementation and evaluation of the machine-learning models.",
  "optimization/meta": "The model described in this publication does not use data from other machine-learning algorithms as input. Instead, it employs a two-step process for identifying patients with newly diagnosed acute myeloid leukemia (AML) and their chemotherapy regimens.\n\nIn the first step, three different machine-learning algorithms\u2014logistic regression (LR), random forest (RF), and support vector machine (SVM)\u2014are applied to a randomly selected sample of patients from a manual review cohort. The SVM approach is ultimately chosen for its robustness and resistance to overfitting.\n\nIn the second step, the same training sample is used to identify chemotherapy regimens and their corresponding start dates for each treatment course. Again, three machine-learning algorithms\u2014RF, LR, and SVM\u2014are employed, with the RF algorithm demonstrating the best performance.\n\nRegarding the independence of training data, the model uses a hold-out test sample for internal validation, which consists of 25% of the manual review cohort. This sample is distinct from the training data, ensuring that the validation process is independent. Additionally, external validation is conducted using a separate cohort from the Home or Away From Home study, further confirming the independence of the training data.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, patient data was extracted from the Pediatric Health Information System (PHIS) database, focusing on encounter data such as demographics, diagnoses, procedures, and resource utilization. This data included pharmaceutical, imaging, laboratory test, and room-and-board information.\n\nFor the machine-learning process, the data was structured using SAS version 9.3. The main features used in the machine-learning models were based on daily chemotherapy billing data from the index admission (the initial admission containing an AML diagnosis code) up to 365 days. The raw data consisted of binary indicators of daily utilization of individual drugs, specifically cytarabine, daunorubicin, etoposide, and gemtuzumab. These drug utilization patterns were assessed within five different time windows (20, 25, 30, 35, or 40 days) starting from the index admission. The proportions of drug utilization days in each window were explored as features in supervised machine-learning algorithms.\n\nIn the second step of the machine-learning process, a 15-day window was used to transfer each patient\u2019s day-to-day drug utilization profile into a sparse binary vector. A series of these sparse binary vectors from all sliding 15-day windows were compared with the expected patterns of six treatment regimens to generate similarity metrics. These metrics, including dot product, cosine similarity, and Euclidean distance, were used as features in the supervised learning models. The similarity metrics were calculated both in weighted and unweighted versions, resulting in a total of six similarity measures.\n\nData manipulation was performed using the pandas library version 1.2.4, and mathematical analysis was conducted using the numPy library version 1.20.1. The machine-learning modeling, evaluation, and application were performed with the scikit-learn library version 1.0.1. The final model chosen was the Support Vector Machine (SVM) approach due to its robustness and lower tendency to overfit.",
  "optimization/parameters": "In our study, the input parameters for the machine learning models were derived from the daily chemotherapy drug utilization data. Specifically, we focused on the utilization patterns of four key drugs: cytarabine, daunorubicin, etoposide, and gemtuzumab (GMTZ). These drugs are consistent with the most common induction regimens for newly diagnosed AML patients.\n\nThe raw data consisted of binary indicators of daily utilization of these individual drugs. We assessed their utilization within five different time windows: 20, 25, 30, 35, and 40 days, starting from the index admission. The proportions of drug utilization days within each window were explored as features in our supervised machine learning algorithms. Among these windows, the 25-day window was chosen for constructing the machine learning features due to its slightly better performance compared to the other windows.\n\nThe selection of these parameters was based on a thorough exploration of different time windows and their impact on model performance. The 25-day window was ultimately chosen because it provided the best balance of sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) across all cross-validations. This approach ensured that our model was robust and less prone to overfitting, which is crucial for reliable patient identification and treatment regimen classification.",
  "optimization/features": "The input features for the machine learning models were derived from daily chemotherapy billing data. Specifically, the utilization patterns of four drugs\u2014cytarabine, daunorubicin, etoposide, and gemtuzumab\u2014were used. These drugs are consistent with the three most common induction regimens for newly diagnosed AML patients. The raw data consisted of binary indicators of daily utilization of individual drugs, assessed within five different time windows (20, 25, 30, 35, or 40 days) starting from the index admission. The proportions of drug utilization days in each window were explored as features in supervised machine learning algorithms.\n\nThe 25-day window was ultimately chosen for constructing the machine learning features due to its slightly better performance compared to the other windows explored. This selection was made based on the results of five-fold cross-validation, which indicated that all three machine learning algorithms (Logistic Regression, Random Forest, and Support Vector Machine) performed similarly. The Support Vector Machine approach was selected as the final model because it is generally less prone to overfitting.\n\nFeature selection was inherently performed by focusing on the utilization patterns of the specified drugs within the chosen time window. This process ensured that the most relevant features were used for training the models. The feature selection was conducted using the training set only, adhering to best practices in machine learning to prevent data leakage and ensure the model's generalizability.",
  "optimization/fitting": "The fitting method employed in this study involved a stepwise machine learning approach, utilizing three different algorithms: Logistic Regression (LR), Random Forest (RF), and Support Vector Machine (SVM). The number of parameters in the models was not excessively large compared to the number of training points. The training set consisted of 1918 patients, which is a substantial sample size for the models used.\n\nTo address overfitting, the SVM approach was ultimately chosen for the final model due to its general robustness against overfitting. Additionally, five-fold cross-validation was employed during the training process. This technique helps to ensure that the model generalizes well to unseen data by training and validating the model on different subsets of the data. The cross-validation results showed consistent performance across all folds, indicating that overfitting was effectively mitigated.\n\nUnderfitting was addressed by selecting the SVM model, which demonstrated high sensitivity and specificity across all cross-validations. The SVM model achieved a sensitivity greater than or equal to 0.94 and a specificity greater than or equal to 0.94, indicating that it captured the underlying patterns in the data without being too simplistic. Furthermore, the positive predictive value (PPV) and negative predictive value (NPV) were also high, confirming the model's ability to correctly identify both positive and negative cases.\n\nIn summary, the chosen model and validation techniques ensured that both overfitting and underfitting were adequately addressed, resulting in a robust and reliable machine learning model for identifying AML cases and their treatment regimens.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One of the key strategies was the use of cross-validation. Specifically, we conducted five-fold cross-validation to evaluate the performance of our models. This technique helps to ensure that the model generalizes well to unseen data by training and validating on different subsets of the data.\n\nAdditionally, we compared the performance of three different machine learning algorithms: Logistic Regression, Random Forest, and Support Vector Machine. Among these, the Support Vector Machine (SVM) was selected for the final model due to its general robustness and lower tendency to overfit compared to the other algorithms. This decision was supported by the consistent performance of the SVM across all folds of the cross-validation.\n\nFurthermore, we utilized a hold-out test sample consisting of 25% of the manual review cohort for internal validation. This sample was not used during the training process, providing an unbiased evaluation of the model's performance. The results from this validation step confirmed the high sensitivity, specificity, and positive predictive value of our SVM model.\n\nIn summary, our approach to preventing overfitting included the use of cross-validation, algorithm comparison, and internal validation with a hold-out test sample. These methods collectively ensured that our final model was reliable and generalizable to new data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are not explicitly detailed in the provided information. However, the machine learning models employed, including Logistic Regression, Random Forest, and Support Vector Machine, were optimized using five-fold cross-validation. This process involved selecting the best-performing method based on the performance metrics across the folds.\n\nThe software and libraries used for data manipulation and analysis are specified. For instance, Python 3.7.6 with Jupyter notebooks was utilized, along with pandas version 1.2.4 for data manipulation and numPy library version 1.20.1 for mathematical analysis. Machine learning modeling, evaluation, and application were performed with scikit-learn version 1.0.1. Additionally, SAS 9.3 was used for external validation, data preparation, data matching, and statistical analysis.\n\nRegarding the availability of model files and optimization parameters, the provided information does not specify where these can be accessed or under what license. Therefore, it is not clear whether the hyper-parameter configurations, optimization schedule, model files, and optimization parameters are publicly available or how they can be accessed.",
  "model/interpretability": "The models employed in our study, specifically Logistic Regression (LR), Random Forest (RF), and Support Vector Machine (SVM), vary in their levels of interpretability. Logistic Regression is generally considered the most transparent among the three. It provides clear, interpretable coefficients that indicate the direction and magnitude of the relationship between each feature and the outcome. This transparency allows for straightforward insights into how changes in specific features influence the model's predictions.\n\nRandom Forest, on the other hand, is somewhat of a black-box model. While it offers feature importance scores, which indicate the relative significance of each feature in making predictions, the individual decision paths within the forest are complex and not easily interpretable. However, tools like feature importance and partial dependence plots can be used to gain some insights into the model's behavior.\n\nSupport Vector Machine, the model ultimately chosen for our final implementation, is also relatively opaque. SVM models, particularly with non-linear kernels, do not provide straightforward interpretations of how individual features contribute to the predictions. The decision boundaries created by SVM are based on complex transformations of the input features, making it difficult to trace back the reasoning behind specific predictions. However, techniques such as Support Vector Coefficients and Recursive Feature Elimination can help in understanding the importance of features to some extent.\n\nIn summary, while Logistic Regression offers clear interpretability, Random Forest and Support Vector Machine provide more abstract insights into their decision-making processes. For our specific application, the trade-off between performance and interpretability led us to choose SVM, despite its relative lack of transparency.",
  "model/output": "The model developed in this study is a classification model. It is designed to identify patients with a new Acute Myeloid Leukemia (AML) diagnosis and to determine the start dates and chemotherapy regimens for up to five treatment courses per patient. The model uses supervised machine learning algorithms, specifically Logistic Regression (LR), Random Forest (RF), and Support Vector Machine (SVM), to classify patients based on their chemotherapy drug utilization patterns.\n\nThe first step of the model focuses on identifying AML cases by analyzing the utilization of four key drugs: cytarabine, daunorubicin, etoposide, and gemtuzumab. The model evaluates the proportions of drug utilization days within different time windows (20, 25, 30, 35, or 40 days) starting from the index admission. The 25-day window was chosen for constructing machine learning features due to its slightly better performance compared to other windows.\n\nThe second step of the model aims to identify course-specific chemotherapy regimens and their corresponding start dates. The model considers six commonly used regimens and uses similarity metrics such as dot product, cosine similarity, and Euclidean distance to match these regimens.\n\nThe performance of the model was evaluated using metrics such as sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). The Support Vector Machine (SVM) approach was selected as the final model for patient identification due to its robustness and lower tendency to overfit. The model demonstrated high sensitivity and PPV across all cross-validations, indicating its effectiveness in accurately identifying AML cases and their treatment regimens.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our study, we employed a rigorous evaluation method to ensure the robustness and generalizability of our models. We utilized a five-fold cross-validation approach, which is a widely accepted technique in machine learning for assessing model performance. This method involves dividing the dataset into five subsets, or folds. The model is then trained on four of these folds and tested on the remaining fold. This process is repeated five times, with each fold serving as the test set once. The results are then averaged to provide a comprehensive evaluation of the model's performance.\n\nFor the patient identification step, we used 25-day drug utilization as features. The performance metrics we focused on included Positive Predictive Value (PPV), Negative Predictive Value (NPV), Sensitivity, and Specificity. These metrics were calculated for each fold and then averaged to provide a stable estimate of the model's performance.\n\nIn the regimen and start-date identification step, we evaluated the models across different chemotherapy regimens. Again, we used five-fold cross-validation and reported PPV and Sensitivity for each regimen. This allowed us to assess how well our models performed across various treatment protocols.\n\nAdditionally, we compared the performance of different machine learning algorithms, including Support Vector Machine, Logistic Regression, and Random Forest. This comparison helped us identify the most effective model for our specific task.\n\nOverall, our evaluation method ensured that our models were thoroughly tested and validated, providing reliable insights into their performance and applicability in real-world scenarios.",
  "evaluation/measure": "In the evaluation of our models, we focused on several key performance metrics to ensure a comprehensive assessment. For both patient identification and course regimen identification, we reported sensitivity (also known as recall), specificity, positive predictive value (PPV or precision), and negative predictive value (NPV). These metrics are widely used in the literature and provide a robust evaluation of model performance.\n\nSensitivity measures the ability of the model to correctly identify positive cases, while specificity assesses the model's ability to correctly identify negative cases. PPV indicates the proportion of positive identifications that are actually correct, and NPV shows the proportion of negative identifications that are accurate. These metrics collectively offer a detailed view of the model's effectiveness in distinguishing between different outcomes.\n\nIn addition to these standard metrics, we performed sensitivity analyses using exact date matches and \u00b11 day matches to further validate our models. This approach ensures that our results are reliable and consistent across different scenarios.\n\nThe use of these metrics is representative of common practices in the field. They allow for a clear comparison with other studies and provide a standardized way to evaluate model performance. By including both overall performance and by-course performance, we ensure that our evaluation is thorough and that our models are robust across different chemotherapy regimens.",
  "evaluation/comparison": "In our evaluation, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on validating our machine learning models internally and externally using specific cohorts related to our study on pediatric acute myeloid leukemia (AML).\n\nFor internal validation, we employed a hold-out test sample to evaluate the performance of our selected models. We used metrics such as sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) to assess both patient-level identification and course-level chemotherapy identification. This internal validation helped us understand how well our models generalized to unseen data within the same dataset.\n\nIn terms of simpler baselines, our approach involved comparing different machine learning algorithms\u2014Random Forest, Logistic Regression, and Support Vector Machine\u2014to determine which performed best for our specific tasks. Each of these algorithms was evaluated using cross-validation to ensure robustness. The results, as shown in our tables, indicate the performance of these models across various chemotherapy regimens.\n\nFor external validation, we used a separate cohort from the Home or Away From Home (HAFH) study. This cohort included patients from different pediatric centers, providing a real-world test of our models' generalizability. The external validation results further confirmed the effectiveness of our models, particularly the Support Vector Machine for patient identification and the Random Forest for course regimen identification.\n\nWhile we did not compare our methods directly to other publicly available tools or simpler baselines in the traditional sense, our comprehensive internal and external validation processes provided a thorough assessment of our models' performance and reliability.",
  "evaluation/confidence": "In our evaluation, we have provided confidence intervals for key performance metrics to indicate the reliability of our results. For instance, in the external validation of our models, we reported sensitivity and positive predictive value (PPV) with 95% confidence intervals. This approach allows for a clearer understanding of the precision of our estimates.\n\nStatistical significance is crucial in determining whether our methods outperform others and established baselines. While specific p-values are not explicitly stated, the use of cross-validation and external validation provides a robust framework for assessing model performance. The consistent high performance across multiple folds and datasets suggests that our models are not only accurate but also generalizable. However, without explicit p-values or additional statistical tests comparing our models to baselines, it is challenging to definitively claim superiority in a statistical sense. Nonetheless, the performance metrics themselves are compelling, showing strong sensitivity and PPV across different chemotherapy regimens and patient identification tasks.",
  "evaluation/availability": "The data that support the findings of this study are available from the Children\u2019s Hospital Association (CHA). However, restrictions apply to the availability of these data, which were used under license for this study. The Pediatric Health Information System (PHIS) data used in the study will be made available by the authors consistent with CHA data access policies. The PHIS database was queried to extract each patient\u2019s entire inpatient data. The data was structured using SAS version 9.3. All machine learning analysis was conducted in Python 3.7.6 using Jupyter notebooks. Data manipulation was performed using pandas version 1.2.4, and mathematical analysis was performed using the numPy library version 1.20.1. Machine learning modeling, evaluation, and application were performed with the scikit-learn version 1.0.1. External validation, data preparation, data matching, and statistical analysis were performed in SAS 9.3."
}