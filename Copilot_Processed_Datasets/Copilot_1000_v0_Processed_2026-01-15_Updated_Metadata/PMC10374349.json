{
  "publication/title": "Artificial intelligence for diagnosing neoplasia on digital cholangioscopy: development and multicenter validation of a convolutional neural network model.",
  "publication/authors": "Robles-Medranda C, Baquerizo-Burgos J, Alcivar-Vasquez J, Kahaleh M, Raijman I, Kunda R, Puga-Tejada M, Egas-Izquierdo M, Arevalo-Mora M, Mendez JC, Tyberg A, Sarkar A, Shahid H, Del Valle-Zavala R, Rodriguez J, Merfea RC, Barreto-Perez J, Salda\u00f1a-Pazmi\u00f1o G, Calle-Loffredo D, Alvarado H, Lukashok HP",
  "publication/journal": "Endoscopy",
  "publication/year": "2023",
  "publication/pmid": "36781156",
  "publication/pmcid": "PMC10374349",
  "publication/doi": "10.1055/a-2034-3803",
  "publication/tags": "- Artificial Intelligence\n- Neoplasia Diagnosis\n- Endoscopy\n- Convolutional Neural Networks\n- Digital Cholangioscopy\n- Biliary Lesions\n- Machine Learning in Medicine\n- Medical Imaging\n- Diagnostic Accuracy\n- YOLO (You Only Look Once)\n- Early Stopping Regularization\n- Model Validation\n- Neoplastic Lesion Criteria\n- Carlos Robles-Medranda Classification\n- Mendoza Classification",
  "dataset/provenance": "The dataset utilized in this study was sourced from prerecorded DSOC (digital single-operator cholangioscopy) videos of treatment-na\u00efve patients. These videos were collected from multiple centers between October 2020 and December 2021. The study involved four participating centers, each contributing a significant number of cases. The total number of patients included in the study was 170, with approximately 33-34 patients from each center. The videos were assessed by both expert endoscopists and nonexpert general practitioners, who classified them as neoplastic or non-neoplastic based on established criteria.\n\nThe dataset consisted of a substantial number of frames extracted from these videos. Specifically, 81,080 frames from 23 patients were used to develop the CNN1 model. These frames were distributed into training and testing datasets to ensure robust model development and validation. Additionally, during the clinical validation phase, an extra 25 consecutive treatment-na\u00efve patients were included, further enriching the dataset.\n\nThe videos assessed in this study were recorded using advanced endoscopic systems, including the second-generation SpyGlass DS digital system and the Eye-Max microendoscopic system. These systems were chosen for their availability and capability to capture high-quality images necessary for accurate diagnosis.\n\nThe dataset was designed to include a balanced representation of neoplastic and non-neoplastic cases, with a 1:1 case vs. controls ratio. This design aimed to recreate the same probability of encountering neoplastic or non-neoplastic cases during DSOC video assessments, similar to a Bernoulli trial. The sample size was calculated to ensure statistical power, with an estimated 66 neoplastic or non-neoplastic cases required for an 80% statistical power, considering a 5% significance level and defined alpha and beta errors.\n\nThe dataset has not been previously used in other publications by the community. The videos and frames were specifically collected and curated for this study to validate the CNN models in a clinical setting. The focus was on ensuring that the dataset was comprehensive and representative of real-world clinical scenarios, thereby enhancing the reliability and generalizability of the study's findings.",
  "dataset/splits": "The dataset was divided into multiple splits for the development and validation of the convolutional neural network (CNN) models. Initially, 81,080 frames from 23 patients were used to develop the first model, CNN1. These frames were distributed into training and testing datasets, with 90% of the frames used for training and the remaining 10% for validation. This split was crucial for assessing the model's performance and preventing overfitting.\n\nFor the clinical validation of CNN1, an additional 25 consecutive treatment-na\u00efve patients were included. This validation set comprised 20 fine-tuned recorded videos and five real-time cholangioscopy procedures of patients with indeterminate biliary lesions. This step ensured that the model's performance was evaluated on new, unseen data, providing a more robust assessment of its diagnostic accuracy.\n\nIn the second stage of the study, a second version of the model, CNN2, was developed using 116 additional DSOC videos with identified neoplastic lesions. Similar to CNN1, CNN2 was trained on 90% of the dataset and validated on the remaining 10%. This approach ensured that the model was trained on a diverse set of data, enhancing its ability to generalize to new cases.\n\nThe distribution of data points in each split was designed to balance the number of neoplastic and non-neoplastic cases, ensuring a representative sample for training and validation. This balanced approach is essential for developing accurate and reliable diagnostic models. The final number of patients included in the study was 170, with approximately 33\u201334 patients at each of the five participating centers. This distribution helped in maintaining consistency and reliability across different centers.",
  "dataset/redundancy": "The dataset used in our study was carefully curated to ensure robustness and independence between training and test sets. For the development of the CNN1 model, a total of 81,080 frames from 23 patients were utilized. These frames were distributed into distinct training and testing datasets to prevent any overlap, thereby ensuring the independence of the sets. This split was crucial for validating the model's performance on unseen data, which is a standard practice in machine learning to assess generalization capabilities.\n\nThe distribution of cases across different centers was also considered to maintain a balanced and representative dataset. Approximately 33-34 patients were included from each of the five participating centers, totaling 170 patients. This distribution helped in recreating a similar probability of neoplastic or non-neoplastic cases, aligning with the Bernoulli trial principles. The probabilistic sample design took into account a 10% delta and a 50% prevalence, ensuring a 1:1 case-to-control ratio.\n\nIn terms of comparison with previously published machine learning datasets, our approach aligns with established methodologies. The use of statistical tests such as the Wilcoxon rank sum test, Pearson's chi-squared test, and Fisher's exact test ensured rigorous comparison of baseline characteristics between neoplastic and non-neoplastic cases. The Kolmogorov-Smirnov test was employed to assess the statistical distribution of numerical variables, which were described as mean (SD) or median (IQR) accordingly. Categorical variables were described as frequencies (%) with 95% confidence intervals.\n\nThe independence of the training and test sets was enforced through careful data partitioning and the exclusion of any cases where experts or non-experts omitted assessments. This rigorous approach helped in maintaining the integrity of the dataset and ensuring that the model's performance was evaluated on truly independent data. The final dataset included a total of 170 patients, with a balanced distribution of neoplastic and non-neoplastic cases, providing a comprehensive and reliable foundation for our study.",
  "dataset/availability": "The data used in our study, including the data splits, are not publicly released. The study involved prerecorded DSOC videos of treatment-na\u00efve patients from different centers, which were uploaded to the AIworks Cloud by expert endoscopists. These videos were then assessed by both expert endoscopists and nonexpert general practitioners. The assessment process was conducted in a blinded manner, with participants classifying the videos as neoplastic or non-neoplastic based on specific criteria.\n\nThe videos and associated data were handled within a controlled environment to ensure the integrity and confidentiality of the patient information. The study design included strict protocols for data handling and analysis, with statistical analyses performed using R version 4.0.3. The diagnostic accuracy of the CNN models was calculated based on both patients and frames, with histological findings and 12-month follow-up results serving as the gold standard.\n\nGiven the sensitive nature of the medical data involved, the data were not made publicly available. Instead, the study results and methodologies were published in scientific forums, allowing other researchers to replicate the study's findings under similar conditions. The focus was on ensuring the ethical use of patient data while advancing the clinical validation of the CNN models.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages convolutional neural networks (CNNs), specifically utilizing YOLOv3 and YOLOv4 architectures. These are well-established, state-of-the-art object detection algorithms known for their efficiency and accuracy in real-time processing.\n\nThe choice of YOLOv3 and YOLOv4 was driven by their proven capabilities in detecting and classifying objects within images, which is crucial for analyzing frames from cholangioscopy videos. These models are not new but have been extensively validated and optimized for various applications, including medical imaging.\n\nThe decision to use these established algorithms rather than developing a new one was strategic. YOLOv3 and YOLOv4 have already demonstrated robust performance in similar tasks, reducing the need for extensive validation and optimization that would be required for a novel algorithm. This allows us to focus on the specific medical application and ensure that the model's performance is reliable and reproducible.\n\nMoreover, publishing in a specialized machine-learning journal was not the primary goal of this study. Our focus was on applying these advanced machine-learning techniques to improve the diagnosis and characterization of biliary lesions using digital cholangioscopy. The medical community benefits more from seeing how these technologies can be integrated into clinical practice, rather than the intricacies of the algorithm itself, which have already been thoroughly documented in the machine-learning literature.",
  "optimization/meta": "The meta-predictor leverages data from multiple machine-learning algorithms to enhance its predictive capabilities. Specifically, it integrates outputs from convolutional neural network models (CNN1 and CNN2) along with expert classifications based on established criteria, such as the Carlos Robles-Medranda and Mendoza classifications.\n\nThe meta-predictor is designed to combine the strengths of different machine-learning methods. CNN1 and CNN2 are developed using YOLOv3 and YOLOv4 architectures, respectively, and are trained on extensive datasets of digital single-operator cholangioscopy (DSOC) videos. These models are validated using separate datasets to ensure their performance and generalization capabilities. Additionally, expert classifications provide a benchmark for the model's accuracy and reliability.\n\nTo ensure the independence of training data, rigorous measures are taken. The datasets used for training and validation are carefully curated to prevent overlap. For instance, CNN1 was trained on 90% of the frames, with the remaining 10% used for validation. Similarly, CNN2 was developed using an additional set of 116 DSOC videos, ensuring that the training and validation datasets are distinct. This approach helps in avoiding overfitting and ensures that the model's performance is robust and generalizable to new, unseen data.\n\nThe meta-predictor's design and training process emphasize the importance of independent data to maintain the integrity and reliability of the model's predictions. By integrating multiple machine-learning methods and ensuring data independence, the meta-predictor aims to achieve high accuracy and reliability in diagnosing neoplastic lesions.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, a large dataset consisting of 81,080 frames from 23 patients was used. These frames were distributed into training and testing datasets to develop the CNN1 model. The model was trained on 90% of the frames using an Nvidia RTX2080ti GPU, while the remaining 10% were used for validation to assess the model's performance.\n\nTo prevent overfitting, early stopping regularization was applied. This process automatically identifies the best metrics during the training process and notifies the point at which the model starts overfitting, ensuring optimal performance.\n\nFor the CNN2 model, an additional 116 DSOC videos with identified neoplastic lesions were used. This improved version of CNN1 was developed using YOLOv4, with a similar 90% training and 10% validation dataset distribution.\n\nThe numerical variables were described using the mean (standard deviation) or median (interquartile range), depending on their statistical distribution as assessed by the Kolmogorov-Smirnov test. Categorical variables were described as frequencies with 95% confidence intervals.\n\nThe baseline characteristics of neoplastic and non-neoplastic cases were compared using statistical hypothesis testing methods such as the Wilcoxon test, chi-squared test, or Fisher's exact test. Any missing baseline or 12-month follow-up data were described if necessary, and cases where experts or non-experts omitted any assessment were excluded from the analysis.\n\nThe diagnostic accuracy and the area under the receiver operating characteristic (ROC) curve were calculated for experts, non-experts, and the patient-based CNN2 model. The diagnostic accuracy of expert and non-expert estimations was calculated separately for the Carlos Robles-Medranda (CRM) and Mendoza classifications. The comparison between CNN2 and diagnostic accuracies through the CRM or Mendoza classifications was defined using DeLong's test for two ROC curves from dependent samples. Histological findings and 12-month follow-ups were considered the gold standard for these comparisons.",
  "optimization/parameters": "In our study, the convolutional neural network models, CNN1 and CNN2, were developed using the YOLO (You Only Look Once) architecture, specifically versions YOLOv3 and YOLOv4, respectively. These models inherently come with a predefined set of parameters that are optimized during the training process.\n\nThe exact number of parameters in a YOLO model can vary based on the specific configuration and the number of layers used. However, YOLOv3 and YOLOv4 are known for their efficiency and typically have a large number of parameters due to their deep architecture. For instance, YOLOv3 usually has around 63 million parameters, while YOLOv4 can have slightly more, depending on the modifications and additional layers included.\n\nThe selection of these parameters was not arbitrary but was based on established practices in the field of deep learning and computer vision. The YOLO architecture is renowned for its balance between speed and accuracy, making it suitable for real-time object detection tasks, which was crucial for our application in digital cholangioscopy.\n\nDuring the training process, we employed early stopping regularization to prevent overfitting. This technique automatically identifies the best metrics among the training processes and notifies the training point with the best metric results, ensuring that the model does not overfit to the training data. This approach helped in selecting the optimal set of parameters that generalized well to unseen data.\n\nAdditionally, the models were trained on a significant number of frames\u201481,080 frames for CNN1 and an additional 116 videos for CNN2\u2014ensuring that the parameters were well-tuned to the specific characteristics of the biliary lesions we were studying. The use of a powerful Nvidia RTX2080ti GPU further facilitated the efficient training and optimization of these parameters.",
  "optimization/features": "The input features for the models were derived from specific criteria for identifying neoplastic lesions. These criteria include the presence or absence of various visual characteristics observed during digital single-operator cholangioscopy (DSOC) videos. The disaggregated neoplasia criteria used as input features are:\n\n1. Presence or absence of tortuous and dilated vessels\n2. Presence or absence of irregular mucosal surfaces\n3. Presence or absence of polypoid lesions\n4. Presence or absence of irregular nodulations\n5. Presence or absence of raised intraductal lesions\n6. Presence or absence of ulcerations\n7. Presence or absence of honeycomb pattern\n8. Presence or absence of friability\n\nThese eight features were used to train and validate the convolutional neural network (CNN) models. Feature selection was implicitly performed by focusing on these specific criteria, which were chosen based on established classifications (Carlos Robles-Medranda and Mendoza classifications). The selection of these features was done using the training set only, ensuring that the models were trained on relevant and discriminative characteristics for detecting neoplastic lesions.",
  "optimization/fitting": "The convolutional neural network models developed in this study, CNN1 and CNN2, were trained using a substantial number of frames extracted from videos. Specifically, CNN1 was trained on 81,080 frames from 23 patients, with 90% of these frames used for training and the remaining 10% for validation. This distribution ensures that the model has a large number of training points relative to the number of parameters, which helps in mitigating the risk of overfitting.\n\nTo further prevent overfitting, early stopping regularization was employed. This technique monitors the model's performance on a validation set during training and stops the training process when the performance on this validation set starts to deteriorate. This approach helps in identifying the optimal point in the training process where the model generalizes best to new, unseen data, thereby avoiding overfitting.\n\nAdditionally, the models were validated using separate datasets that were not part of the training process. For CNN1, clinical validation was performed on 25 cases that included 20 fine-tuned recorded videos and five real-time cholangioscopy procedures. This external validation helps in assessing the model's performance on data it has not seen during training, providing a robust check against overfitting.\n\nThe metrics used for validation, such as mean average precision (mAP), intersection over union (IoU), F1 score, and total loss, were carefully monitored. These metrics provide a comprehensive evaluation of the model's performance, ensuring that it is not only fitting the training data well but also generalizing to new data.\n\nUnderfitting was addressed by ensuring that the models were complex enough to capture the necessary patterns in the data. The use of advanced architectures like YOLOv3 for CNN1 and YOLOv4 for CNN2, along with powerful hardware like the Nvidia RTX2080ti GPU, ensured that the models had the capacity to learn from the data effectively. The training process was also designed to converge properly, with the early stopping mechanism ensuring that the models did not stop learning prematurely.\n\nIn summary, the combination of a large training dataset, early stopping regularization, external validation, and the use of advanced architectures and hardware ensured that the models were neither overfitting nor underfitting. This rigorous approach to model development and validation is crucial for achieving reliable and generalizable results in the diagnosis of neoplastic lesions using digital cholangioscopy.",
  "optimization/regularization": "To prevent overfitting of the models, early stopping regularization was applied. This process involves monitoring the model's performance during training and stopping the training process when the model's performance on a validation dataset starts to degrade. This helps in identifying the best metrics among the training processes and automatically notifies the training point with the best metric results. Additionally, it indicates the point at which the model starts overfitting, ensuring that the model generalizes well to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule for the convolutional neural network (CNN) models developed in this study are not explicitly detailed in the available information. However, it is mentioned that early stopping regularization was applied to prevent overfitting. This process automatically identifies the best metrics during training and notifies the point at which the model starts overfitting.\n\nThe models were developed using specific versions of YOLO (You Only Look Once). CNN1 utilized YOLOv3, while CNN2 was developed using YOLOv4. The training process for CNN1 involved 90% of the frames from the dataset, with the remaining 10% used for validation. Similarly, CNN2 followed the same 90% training and 10% validation split.\n\nThe models were trained using an Nvidia RTX2080ti GPU. The performance metrics obtained from the model validation processes include mean average precision (mAP), intersection over the union (IoU), F1 score, and total loss. These metrics provide insights into the models' ability to detect features, the difference between prediction values and expected results, the association between recall and precision, and the precision to identify objects within detection boxes, respectively.\n\nRegarding the availability of model files and optimization parameters, the information provided does not specify where these can be accessed or under what license they are distributed. Therefore, it is not clear whether the model files and optimization parameters are publicly available or how they can be obtained.",
  "model/interpretability": "The models developed in this study, CNN1 and CNN2, are based on convolutional neural networks (CNNs), which are generally considered black-box models. This means that the internal workings of the models are not easily interpretable by humans. The models use complex layers of neurons to process and learn from the input data, making it challenging to trace the decision-making process.\n\nHowever, the models do provide some level of interpretability through the use of bounding boxes. These bounding boxes highlight areas in the biliary lesion suggestive of neoplasia. The color of the bounding box represents the CNN model used, with CNN1 using a yellow bounding box and CNN2 using a green bounding box. This visual aid helps in understanding which parts of the image the model is focusing on to make its predictions.\n\nAdditionally, the models were trained and validated using specific criteria for neoplastic lesions, based on the Carlos Robles-Medranda and Mendoza classifications. These criteria include features such as tortuous and dilated vessels, irregular mucosal surfaces, polypoid lesions, and more. By adhering to these criteria, the models provide a structured approach to identifying neoplastic lesions, which can aid in interpretability.\n\nThe models also output several metrics that can be used to assess their performance and interpret their results. These metrics include mean average precision (mAP), total loss, F1 score, and intersection over the union (IoU). These metrics provide quantitative measures of the model's ability to detect and accurately identify neoplastic lesions.\n\nIn summary, while the CNN models used in this study are not fully transparent, they do provide some level of interpretability through the use of bounding boxes, adherence to specific criteria, and output of performance metrics. These features help in understanding the model's decision-making process and assessing its effectiveness in detecting neoplastic lesions.",
  "model/output": "The model developed is a convolutional neural network (CNN) designed for classification tasks. Specifically, it is used to recognize neoplasia in indeterminate biliary lesions from digital single-operator cholangioscopy (DSOC) videos. The model identifies and classifies frames within these videos to detect neoplastic lesions.\n\nTwo versions of the model were created: CNN1 and CNN2. CNN1 was developed using YOLOv3 and trained on 90% of the frames, with the remaining 10% used for validation. CNN2, an improved version, was developed using YOLOv4 and also followed a 90% training and 10% validation dataset distribution. Both models were evaluated using four key metrics: mean average precision (mAP), total loss, F1 score, and intersection over union (IoU). These metrics assess the model's ability to detect trained features, the difference between prediction values and expected results, the balance between recall and precision, and the precision in identifying objects within detection boxes, respectively.\n\nThe models were clinically validated with 25 cases, including 20 fine-tuned recorded videos and five real-time cholangioscopy procedures. The validation process involved comparing the model's performance with that of experts and nonexperts using the Carlos Robles-Medranda (CRM) and Mendoza classifications. The models demonstrated high diagnostic accuracy, particularly in detecting neoplastic lesions, and outperformed nonexperts in real-time endoscopic procedures. The observed agreement between the models and experts varied, with CNN2 showing strong performance metrics across different classifications.",
  "model/duration": "The execution time of the model varied depending on the stage and the specific tasks being performed. During the development and validation of the second model (CNN2), the reading rate was observed to be between 30 to 60 frames per second. For validation data, a 5-second reading period was utilized. This efficiency allowed for rapid processing of the video frames, which is crucial for real-time applications such as digital cholangioscopy. The model's ability to handle a high frame rate ensures that it can provide timely and accurate detections during procedural use.",
  "model/availability": "The source code for the models developed in this study is not publicly released. However, the models were developed using established frameworks, specifically YOLOv3 for the first model (CNN1) and YOLOv4 for the second model (CNN2). These frameworks are widely available and can be accessed by researchers interested in replicating or building upon the work.\n\nThe models were trained and validated using specific datasets, and the performance metrics were thoroughly evaluated. While the exact implementation details and source code are not available, the methodologies and frameworks used are well-documented in the literature, allowing for potential replication or adaptation by other researchers in the field.\n\nFor those interested in utilizing similar models, the YOLO frameworks can be obtained from their respective repositories, and the training and validation processes can be followed as described in the study. This approach ensures that the community has access to the necessary tools and methods to advance research in this area.",
  "evaluation/method": "The evaluation method for our study involved a two-stage process. In the first stage, we developed and validated a convolutional neural network model, referred to as CNN1. This model was trained and tested using frames from 23 patients, with a total of 81,080 frames distributed into training and testing datasets. The performance of CNN1 was assessed using metrics such as mean average precision (mAP), intersection over union (IoU), F1 score, and total loss.\n\nIn the second stage, we conducted a multicenter diagnostic trial to compare the performance of CNN2, an improved version of CNN1, against four digital single-operator cholangioscopy (DSOC) experts and four nonexperts. The study included 170 patients from different centers, with approximately 33-34 patients at each participating center. The experts and nonexperts, who were blinded to clinical records, classified the DSOC videos as neoplastic or non-neoplastic based on the Carlos Robles-Medranda (CRM) and Mendoza classifications. CNN2 was also applied to these videos and classified them similarly.\n\nThe diagnostic accuracy of CNN2, as well as the experts and nonexperts, was calculated based on both patients and frames. Histological findings and 12-month follow-up results were considered the gold standard for evaluation. Statistical analyses were performed using R version 4.0.3, and a P value of less than 0.05 was considered statistically significant. The comparison between CNN2 and the diagnostic accuracies of the experts and nonexperts through the CRM or Mendoza classifications was defined through DeLong's test for two receiver operating characteristic (ROC) curves from dependent samples.",
  "evaluation/measure": "In our study, we evaluated the performance of our convolutional neural network (CNN) models using several key metrics. These metrics provide a comprehensive assessment of the models' capabilities in detecting and classifying neoplastic lesions.\n\nOne of the primary metrics we reported is the positive predictive value (PPV) and negative predictive value (NPV). These values indicate the proportion of true positive and true negative results, respectively, among all positive and negative predictions made by the models. This is crucial for understanding the reliability of the models in clinical settings.\n\nWe also reported the area under the curve (AUC) for the receiver operating characteristic (ROC) curve. The AUC provides a single scalar value that summarizes the performance of the model across all classification thresholds. A higher AUC indicates better overall performance in distinguishing between neoplastic and non-neoplastic lesions.\n\nAdditionally, we included precision and recall values. Precision measures the accuracy of the positive predictions made by the model, while recall (or sensitivity) measures the model's ability to identify all relevant instances. These metrics are particularly important for evaluating the model's performance in detecting neoplastic lesions, where missing a true positive can have significant clinical implications.\n\nThe F1 score, which is the harmonic mean of precision and recall, was also reported. The F1 score provides a balanced measure of the model's performance, especially when dealing with imbalanced datasets.\n\nWe also considered the mean average precision (mAP), which represents the model's ability to detect trained features accurately. This metric is essential for evaluating the model's performance in object detection tasks.\n\nFurthermore, we reported the total loss, which denotes the difference between the prediction values and the expected results. Lower loss values indicate better model performance.\n\nThe intersection over the union (IoU) was another metric we used. IoU represents the model's precision in identifying an object within the detection box, providing insights into the accuracy of the model's localization capabilities.\n\nThese metrics collectively offer a robust evaluation of our CNN models' performance, ensuring that they are reliable and accurate for clinical use. The set of metrics reported is representative of standard practices in the literature, providing a comprehensive assessment of the models' capabilities in detecting and classifying neoplastic lesions.",
  "evaluation/comparison": "In the evaluation of our CNN2 model, we conducted a comprehensive comparison with both expert and nonexpert groups using established DSOC classification systems. This comparison was crucial to assess the model's diagnostic accuracy and reliability in detecting lesions.\n\nFor the expert group, we evaluated the overall diagnostic accuracies using both the CRM and Mendoza classification criteria. The results were compared with those of CNN2 and subsequent follow-ups. Notably, CNN2 achieved statistical significance with both classifications when compared to expert #4. This expert had a sensitivity of 92.7%, specificity of 48.5%, PPV of 64.3%, and NPV of 86.8% using the CRM criteria, and a sensitivity of 100%, specificity of 2.9%, PPV of 50.8%, and NPV of 100% using the Mendoza criteria. The observed agreements for these classifications were 70.6% and 51.5%, respectively. The AUC for expert #4 using the CRM and Mendoza classifications were 0.755 (P = 0.005) and 0.753 (P = 0.04), respectively, when compared with CNN2.\n\nIn the nonexpert group, nonexpert #2 showed a significant difference between the two classifications and CNN2. The observed agreement was 65.7% for the CRM classification and 55.9% for the Mendoza classification. The AUCs of the CRM and Mendoza classifications were 0.657 (P = 0.01) and 0.582 (P < 0.001), respectively, which were significantly lower than the corresponding value for CNN2 (AUC 0.794). Additionally, CNN2 performed significantly better than nonexpert #4 with the CRM classification criteria (P < 0.05), with a sensitivity of 80.9% and an NPV of 73.5%. The observed agreement was 66.9%. The AUCs of the nonexpert using the CRM and Mendoza classifications were 0.683 (P = 0.04) and 0.737 (P = 0.31), individually, compared with 0.791 for CNN2.\n\nA pooled analysis of experts and nonexperts, using the CRM and Mendoza criteria, was also conducted. The results indicated that CNN2 was superior to both groups, demonstrating its robustness and reliability in diagnostic accuracy.\n\nThis thorough comparison with both expert and nonexpert groups, along with the use of established classification systems, provides a strong validation of CNN2's performance and its potential to enhance diagnostic accuracy in clinical settings.",
  "evaluation/confidence": "The evaluation of our models and expert assessments includes confidence intervals for the performance metrics, providing a range within which the true value is likely to fall. These intervals are crucial for understanding the reliability and precision of our results.\n\nFor instance, the positive predictive value (PPV) and negative predictive value (NPV) are reported with their respective confidence intervals, such as 88.7% to 99.9% for PPV and 44.3% to 73.6% for NPV. This approach ensures that the performance metrics are not just point estimates but are accompanied by a measure of uncertainty.\n\nStatistical significance is also a key consideration in our evaluation. We use DeLong's test for comparing the area under the receiver operating characteristic (ROC) curves of different models and expert classifications. This test helps determine whether the observed differences in performance are statistically significant, thereby providing a robust basis for claiming superiority over other methods and baselines.\n\nAdditionally, p-values are reported for various comparisons, such as 0.50 and 0.005, indicating the level of statistical significance. A lower p-value suggests stronger evidence against the null hypothesis, supporting the claim that the observed differences are not due to random chance.\n\nIn summary, our evaluation includes confidence intervals for performance metrics and employs statistical tests to ensure that the results are statistically significant. This rigorous approach enhances the credibility and reliability of our findings, allowing us to confidently claim the superiority of our methods over others and established baselines.",
  "evaluation/availability": "The raw evaluation files for our study are not publicly available. The evaluation process involved the assessment of prerecorded DSOC videos by both expert endoscopists and nonexpert general practitioners, as well as the application of our CNN2 model. These videos and the corresponding assessments were uploaded to the AIworks Cloud for analysis. The study design and statistical methods used for evaluation are detailed in our publication, but the specific raw data files are not released to the public. Access to these files is restricted to ensure patient privacy and comply with ethical guidelines. For further information or specific inquiries regarding the evaluation data, interested parties may contact the corresponding author."
}