{
  "publication/title": "Artificial intelligence and visual inspection in cervical cancer screening.",
  "publication/authors": "Nakisige C, de Fouw M, Kabukye J, Sultanov M, Nazrui N, Rahman A, de Zeeuw J, Koot J, Rao AP, Prasad K, Shyamala G, Siddharta P, Stekelenburg J, Beltman JJ",
  "publication/journal": "International journal of gynecological cancer : official journal of the International Gynecological Cancer Society",
  "publication/year": "2023",
  "publication/pmid": "37666527",
  "publication/pmcid": "PMC10579490",
  "publication/doi": "10.1136/ijgc-2023-004397",
  "publication/tags": "- Cervical Cancer\n- Screening\n- Visual Inspection with Acetic Acid (VIA)\n- Artificial Intelligence\n- Diagnostic Performance\n- Healthcare Workers\n- Expert Consensus\n- Intra-Observer Agreement\n- Inter-Observer Agreement\n- PRESCRIP-TEC Project",
  "dataset/provenance": "The dataset used in this study comprises cervical images that were sourced from various databanks. Initially, the dataset consisted of 96 unique images and four duplicates. After an expert consensus meeting, three images were excluded, leaving 93 images. Subsequently, ten images were excluded due to the AI's inability to assess them, resulting in a final dataset of 83 unique images.\n\nThese images were used to train and evaluate an AI algorithm for visual inspection with acetic acid (VIA). The dataset included images that were both VIA negative and VIA positive, with nine suspected cancers added to the VIA positives to enable comparison with the binary outcome of the AI. The final assessment comprised 48 VIA negative images, 28 VIA positive images, and nine suspected cancers, which were tagged as positive.\n\nThe images were stored in a folder on a computer and analyzed by the AI, which reported them as either negative or positive. The expert consensus, based on the agreement of at least five out of nine experts, served as the gold standard for the VIA assessment due to the lack of pathology.\n\nThe dataset was part of a study involving 31 individuals, including healthcare workers and experts from Bangladesh, India, Uganda, and the Netherlands. The study aimed to evaluate the diagnostic performance of healthcare workers, individual experts, and the AI using sensitivity, specificity, and other diagnostic metrics. The images were viewed on computer monitors, and the AI assessed only the images taken after the application of acetic acid.",
  "dataset/splits": "The dataset consisted of 96 unique images and four duplicates. Initially, three images were excluded during an expert consensus meeting, leaving 93 images. Subsequently, ten images were excluded because an AI failed to assess them, resulting in 83 images for the final assessment.\n\nThe final dataset comprised 48 VIA negative images, 28 VIA positive images, and nine suspected cancers. The nine suspected cancers were added to the VIA positives, making a total of 35 VIA positive images and 48 VIA negative images. This distribution enabled a comparison with the binary outcome of the AI.\n\nOf the 83 images, 24 had histopathological diagnoses: six were normal, four were CIN I, two were CIN II, three were CIN III, two showed features of HPV infection, and seven were squamous cell carcinomas. The expert consensus resulted in 48 VIA negative images, 26 VIA positive images, and nine suspected cancers, all of which were tagged as positive for comparison with the AI's binary outcome.",
  "dataset/redundancy": "The dataset consisted of 96 unique images and four duplicates. Initially, three images were excluded during an expert consensus meeting, leaving 93 images. Subsequently, ten more images were excluded because the AI failed to assess them, resulting in a final set of 83 images for assessment.\n\nThe final dataset comprised 48 VIA negative images, 28 VIA positive images, and nine suspected cancers. The nine suspected cancers were added to the VIA positives, making a total of 35 VIA positive images and 48 VIA negatives. This adjustment was made to enable comparison with the binary outcome of the AI.\n\nThe images were stored in a folder on a computer, and the AI analyzed them, reporting results as either negative or positive. The gold standard or reference was based on expert consensus due to the lack of pathology. An expert consensus meeting was conducted after the images had been assessed individually by all healthcare workers, experts, and the algorithm. Consensus was reached when at least five out of the nine experts agreed on the VIA assessment.\n\nThe dataset was carefully curated to ensure independence between the training and test sets. The expert team of nine gynecologists did not participate in the selection of these images to avoid recall bias. This approach helped maintain the integrity and independence of the dataset, ensuring that the assessments were unbiased and reliable.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets in the field of cervical cancer screening. The inclusion of a diverse set of images, along with the rigorous consensus process, ensures that the dataset is representative and robust for training and validating AI models. The final dataset of 83 unique images, with a balanced distribution of VIA negative and positive cases, provides a solid foundation for developing accurate and reliable diagnostic tools.",
  "dataset/availability": "The dataset used in this study is not publicly available. The images and questionnaires were uploaded to an online tool developed by the Marconi AI laboratory at Makerere University, Uganda. This tool was accessed by experts and healthcare workers independently on computer monitors. The dataset comprised 96 unique images and four duplicates, which were assessed and filtered through several stages, including expert consensus meetings and AI assessments. The final dataset consisted of 83 unique images, which were used for the final assessment. The images were stored in a folder on a computer, and the AI analyzed them to report results as either negative or positive.\n\nThe dataset was not released in a public forum, and there is no information provided about the licensing terms. The access to the dataset was controlled through personal accounts created by the healthcare workers and experts, who were instructed to complete the assessments individually without consulting external sources. After completion, access to the forms was automatically locked to ensure the integrity of the data collection process.",
  "optimization/algorithm": "The optimization algorithm employed in this study leverages artificial intelligence to enhance the diagnostic performance of visual inspection with acetic acid (VIA) in cervical cancer screening. The specific machine-learning algorithm class used is not explicitly detailed, but it is clear that the AI was developed to address the subjectivity and lack of skilled human resources in VIA.\n\nThe AI algorithm used in this study is not entirely new, as it was developed by collaborators involved in the project. The algorithm was provided by the Manipal Academy for Higher Education in India. The decision to publish the findings in a gynecological cancer journal rather than a machine-learning journal is likely due to the focus of the study on the clinical application and diagnostic performance of the AI in cervical cancer screening. The primary goal was to demonstrate the AI's effectiveness in improving screening accuracy and its potential to support healthcare workers in low-resource settings.\n\nThe algorithm's performance was evaluated alongside healthcare workers and experts, showing promising results in terms of sensitivity, specificity, and area under the curve (AUC). This indicates that the AI can be a valuable tool in cervical cancer screening, particularly in settings where expert consensus can be used as an alternative to histopathology for training and validation. The study highlights the need for further training and validation of the AI to maximize its performance in the field and enable task shifting to less trained healthcare workers.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding process involved the use of cervical images, which were uploaded to an online annotation platform developed by the Marconi AI laboratory in Makerere University, Uganda. These images were paired, showing both the state before and after the application of acetic acid, and were presented in a single frame. Each image was accompanied by a brief questionnaire that assessed the quality of the image, the visual inspection with acetic acid (VIA) assessment, and the eligibility for ablative therapy if rated positive.\n\nThe images were viewed on computer monitors, which facilitated the VIA-M procedure. The artificial intelligence (AI) algorithm was run on a computer and assessed only the images taken after the application of acetic acid. The AI provided a binary result, classifying each image as either positive or negative.\n\nThe healthcare workers and experts individually accessed the platform and completed their assessments within a specified period. They received guiding instructions via a video and a letter before starting their evaluations. The project managers ensured that all participants filled out the forms individually, without consulting external sources. After completion, access to the forms was automatically locked to maintain the integrity of the data.\n\nThe expert consensus meeting was conducted after all individual assessments were completed. Consensus was reached when at least five out of the nine experts agreed on the VIA assessment. During this meeting, three images were excluded due to insufficient quality or inappropriate follow-up conditions. Additionally, the AI could not assess 10 out of the 93 images due to file format issues, resulting in a final dataset of 83 unique images for analysis. These images were then used to train and evaluate the machine-learning algorithm, ensuring a robust and reliable diagnostic performance.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "The study utilized a dataset comprising 96 unique images and four duplicates, which were assessed for visual inspection with acetic acid (VIA). After excluding three images at an expert consensus meeting and ten images due to AI assessment failures, 83 images were included for final analysis. These images were categorized into 48 VIA negative images, 28 VIA positive images, and nine suspected cancers, which were added to the VIA positives for comparison with the binary outcome of the AI.\n\nThe input features for the AI algorithm were derived from these images, specifically focusing on the visual characteristics after the application of acetic acid. The AI assessed the images and reported them as either negative or positive. The diagnostic performance of the AI was evaluated using sensitivity, specificity, and the area under the curve (AUC), among other metrics.\n\nFeature selection was not explicitly mentioned in the context provided. However, the AI was trained on images from various sources with expert consensus as the reference, indicating a rigorous process to ensure the relevance and quality of the input features. The images were divided into training, validation, and testing sets without overlap to mitigate the risk of overfitting, ensuring that the input features were robust and generalizable.",
  "optimization/fitting": "The study involved a dataset comprising 93 unique images, with 83 ultimately included for final analysis. The number of parameters in the AI algorithm was not explicitly stated, but it is implied that the algorithm was trained on a relatively small dataset. To mitigate the risk of overfitting, the images were divided into three sets: training, validation, and testing, with no overlap between them. This approach helps ensure that the algorithm generalizes well to new, unseen data.\n\nTo rule out underfitting, the algorithm's performance was evaluated using sensitivity, specificity, and the area under the curve (AUC). The AI demonstrated a sensitivity of 80.0% and a specificity of 83.3%, with an AUC of 0.84. These metrics indicate that the algorithm performed adequately, suggesting that it was not underfitted. Additionally, the AI identified all images with suspected or invasive cancer as positive, which is a strong indicator of its effectiveness in detecting critical cases.\n\nThe diagnostic performance of the AI was compared to that of healthcare workers and experts, providing a benchmark for its accuracy. The AI's performance was found to be better than that of healthcare workers but lower than that of experts. This comparison further supports the conclusion that the AI was not underfitted and that its performance is reliable within the context of the study.\n\nIn summary, the study employed a rigorous approach to dividing the dataset and evaluating the algorithm's performance, ensuring that both overfitting and underfitting were addressed. The AI's diagnostic accuracy was validated through comparison with human assessments, confirming its effectiveness in the given context.",
  "optimization/regularization": "To mitigate the risk of overfitting during the development phase of the algorithm, images were divided into three sets: training, validation, and testing, with no overlap. This approach ensures that the algorithm is trained on one subset of data, validated on another, and tested on a completely separate set, helping to prevent the model from becoming too tailored to the training data and thus improving its generalizability. However, the total number of images used was relatively small, which could potentially limit the effectiveness of this regularization method.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model employed in this study is not a blackbox but rather a transparent system designed to aid in the prevention and screening of cervical cancer. The algorithm's performance was evaluated alongside healthcare workers and experts, providing a clear comparison of its diagnostic capabilities.\n\nThe algorithm's transparency is evident in its diagnostic metrics, which include sensitivity, specificity, and the area under the curve (AUC). For instance, the algorithm demonstrated a sensitivity of 80.0% and a specificity of 83.3%, indicating its ability to correctly identify positive and negative cases. The AUC for the algorithm was 0.84, which falls within the \"very good\" range of diagnostic accuracy. This metric provides a clear measure of the algorithm's performance, making it understandable and interpretable.\n\nFurthermore, the algorithm's performance was compared to that of healthcare workers and experts, with the algorithm outperforming healthcare workers but not experts. This comparison adds another layer of transparency, as it allows for a direct assessment of the algorithm's effectiveness relative to human evaluators.\n\nThe study also included an analysis of intra- and inter-observer agreement, using Fleiss \u03ba values to measure consistency within and between teams. The algorithm achieved a \u03ba value of 0.63, indicating substantial agreement. This metric further enhances the transparency of the model, as it provides a quantifiable measure of the algorithm's reliability and consistency.\n\nIn summary, the model used in this study is transparent, with clear diagnostic metrics and comparisons to human evaluators. This transparency allows for a thorough understanding of the algorithm's performance and its potential role in cervical cancer screening.",
  "model/output": "The model employed in our study is designed for classification tasks. Specifically, it focuses on the binary classification of visual inspection with acetic acid (VIA) images, categorizing them as either VIA positive or VIA negative. The dataset comprised 96 unique images, with four duplicates initially included. After an expert consensus meeting, three images were excluded, leaving 93 images for further assessment. Subsequently, ten images were excluded because the artificial intelligence (AI) model failed to assess them, resulting in a final set of 83 images. These final images were divided into 48 VIA negative images and 35 VIA positive images, which included nine suspected cancers added to the VIA positive category to facilitate comparison with the binary outcome of the AI.\n\nThe model's performance was evaluated using metrics such as sensitivity and specificity, which are crucial for assessing its effectiveness in correctly identifying VIA positive and VIA negative cases. The Area Under the Curve (AUC) values for different healthcare workers, experts, and the AI algorithm were also calculated to compare their diagnostic accuracies. These metrics provide a comprehensive view of the model's ability to classify VIA images accurately, which is essential for the prevention and screening innovation project aimed at eliminating cervical cancer.",
  "model/duration": "The execution time of the model is not explicitly detailed in the provided information. However, it is mentioned that the AI analyzed the images and reported them as being negative or positive. This implies that the model's processing time for each image was relatively quick, as it was able to handle a dataset of 83 unique images for final analysis. Additionally, the AI ran on a computer, which suggests that the computational resources were adequate for timely processing. The specific duration for the AI to assess each image or the entire dataset is not specified.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method involved a comprehensive assessment of cervical images using both human and artificial intelligence (AI) evaluations. Participants, including healthcare workers and experts, were invited to individually assess cervical images within a week, following guiding instructions provided via video and letter. The project managers ensured that all assessments were done independently, without external consultation.\n\nThe images were stored and analyzed by an AI algorithm, which reported them as either negative or positive. The gold standard for comparison was established through expert consensus, as pathology results were not available. This consensus was reached when at least five out of nine experts agreed on the visual inspection with acetic acid (VIA) assessment. Throughout this process, experts were blinded to the original databank's gold standard to avoid bias.\n\nIn cases where the expert consensus differed from the original databank or where results were inconclusive, re-evaluations were conducted. Images of insufficient quality or those taken post-treatment were excluded from the analysis. The AI also failed to assess some images due to file format issues, further refining the dataset.\n\nThe final analysis included 83 unique images, comprising various histopathological diagnoses. The diagnostic performance was evaluated using sensitivity, specificity, area under the curve (AUC), and Fleiss kappa (\u03ba) values. True positives, true negatives, false positives, and false negatives were reported for individual country teams, and feedback was provided to healthcare workers, who were subsequently re-trained.\n\nIntra-observer and inter-observer agreements were determined using Fleiss \u03ba values, assessing the consistency within and between teams of healthcare workers, experts, and the algorithm. The study was approved by institutional review boards in Bangladesh, India, and Uganda, with informed consent obtained from all participants.",
  "evaluation/measure": "In our study, we evaluated the diagnostic performance using several key metrics to ensure a comprehensive assessment. The primary metrics reported include sensitivity, specificity, and the area under the curve (AUC). Sensitivity measures the ability of the diagnostic test to correctly identify those with the condition, while specificity measures the ability to correctly identify those without the condition. The AUC provides a summary statistic of the receiver operating characteristics (ROC) curve, indicating the overall diagnostic accuracy of the test.\n\nAdditionally, we reported true positives, true negatives, false positives, and false negatives for the individual country teams. These metrics are crucial for understanding the performance of healthcare workers, experts, and the algorithm in distinguishing between positive and negative cases.\n\nThe agreement among observers was assessed using Fleiss \u03ba values, which measure the level of agreement among multiple raters. This metric is particularly important in our study as it helps to evaluate the consistency of diagnostic assessments within and between teams of healthcare workers, experts, and the algorithm.\n\nOur set of metrics is representative of standard practices in the literature. Sensitivity and specificity are commonly used to evaluate the performance of diagnostic tests, and the AUC is a well-established measure of overall diagnostic accuracy. The inclusion of true positives, true negatives, false positives, and false negatives provides a detailed view of the test's performance, while Fleiss \u03ba values are essential for assessing inter-observer agreement. This comprehensive approach ensures that our evaluation is robust and comparable to other studies in the field.",
  "evaluation/comparison": "The evaluation of our study involved a comprehensive comparison of diagnostic performance among healthcare workers, individual experts, and an artificial intelligence (AI) algorithm. The comparison was conducted using a dataset of cervical images, which included both VIA positive and negative cases, as well as suspected cancers.\n\nThe diagnostic performance was assessed using several key metrics: sensitivity, specificity, area under the curve (AUC), false positives, and false negatives. These metrics provided a robust framework for evaluating the accuracy and reliability of each group's assessments.\n\nIn addition to these metrics, we also determined the intra-observer and inter-observer agreement within the teams of healthcare workers, experts, and the algorithm using Fleiss \u03ba values. This step was crucial for understanding the consistency and reliability of the assessments across different evaluators.\n\nThe study did not specifically compare our methods to publicly available benchmarks or simpler baselines. Instead, the focus was on evaluating the performance of healthcare workers, experts, and the AI algorithm within the context of the provided dataset. The expert consensus served as the gold standard for this evaluation, given the lack of pathology data.\n\nThe AI algorithm's performance was particularly noteworthy. It demonstrated a sensitivity of 80.0% and a specificity of 83.3%, which were comparable to the performance of healthcare workers but lower than that of the experts. The AI's ability to identify all images with suspected or invasive cancer as positive, despite not being trained on such images, highlighted its potential for further development and improvement.\n\nOverall, the comparison of diagnostic performance among the different groups provided valuable insights into the strengths and weaknesses of each approach. The results indicated that while the AI performed adequately, there is room for improvement, particularly in terms of sensitivity and specificity. Further training and validation of the AI are recommended to enhance its diagnostic accuracy and applicability in clinical settings.",
  "evaluation/confidence": "The evaluation of the diagnostic performance included several key metrics, such as sensitivity, specificity, and the area under the curve (AUC), each accompanied by confidence intervals. These intervals provide a measure of the reliability and precision of the estimates. For instance, the AUC for healthcare workers was reported as 0.80 with a 95% confidence interval (CI) of 0.70 to 0.90, indicating a range within which the true AUC is likely to fall. Similarly, the AUC for experts was 0.93 (95% CI 0.87 to 1.00), and for the AI, it was 0.84 (95% CI 0.75 to 0.93). These confidence intervals are crucial for understanding the statistical significance and the potential variability in the performance metrics.\n\nThe statistical significance of the results was assessed to determine if the observed differences in performance were likely due to actual differences rather than random chance. The use of Fleiss \u03ba values to measure intra- and inter-observer agreement further supported the reliability of the findings. The \u03ba values for healthcare workers ranged from 0.43 to 0.48, indicating moderate agreement, while the overall \u03ba value for experts was 0.68, suggesting substantial agreement. When outliers were excluded, the agreement among healthcare workers improved to a \u03ba value of 0.63, which is also substantial.\n\nThe diagnostic performance of healthcare workers was found to be adequate, with a sensitivity of 80.4% and a specificity of 80.5%. The AI performed slightly better than the healthcare workers but not as well as the experts, who had a sensitivity of 81.6% and a specificity of 93.5%. The AI's sensitivity was 80.0% and its specificity was 83.3%. These results, along with the confidence intervals, provide a robust basis for claiming that the method is effective and comparable to existing standards. The statistical analysis and the inclusion of confidence intervals ensure that the claims of superiority or equivalence are supported by rigorous statistical evidence.",
  "evaluation/availability": "Not enough information is available."
}