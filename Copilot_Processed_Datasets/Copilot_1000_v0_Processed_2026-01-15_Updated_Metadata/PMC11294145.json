{
  "publication/title": "Surrogate modelling of heartbeat events for improved J-peak detection in BCG using deep learning.",
  "publication/authors": "Schranz C, Halmich C, Mayr S, Heib DPJ",
  "publication/journal": "Frontiers in network physiology",
  "publication/year": "2024",
  "publication/pmid": "39099720",
  "publication/pmcid": "PMC11294145",
  "publication/doi": "10.3389/fnetp.2024.1425871",
  "publication/tags": "- Ballistocardiogram\n- Ballistocardiography\n- J-peak\n- Heartrate estimation\n- Event detection\n- Peak detection\n- Deep learning\n- ResNet\n- Sleep monitoring\n- Signal processing",
  "dataset/provenance": "The dataset used in this study was acquired from a limited number of participants, specifically eleven individuals, over a total of seventeen nights. This dataset was utilized to evaluate the performance of various methods for heartbeat detection in ballistocardiogram (BCG) signals. The dataset was collected with the intention of providing a foundation for enhanced health monitoring during sleep, including comprehensive heart rate variability analysis and sleep stage classification.\n\nThe dataset was subjected to a non-linear time synchronization process to address temporal issues in the BCG data acquisition. This synchronization was conducted between R-peaks from ECG and preliminarily extracted J-peaks from BCG. The varying RJ-intervals, which are typically considered constant in current literature, were compensated for during this preprocessing step. This step was crucial for improving the suitability of using R-peaks as ground truth events for training a supervised neural network for J-peak extraction.\n\nThe dataset's limitations include its small size and the need for non-linear time synchronization. These limitations highlight the necessity for a more comprehensive and openly available dataset to perform rigorous method comparisons and to reduce high inter-subject variability. Future work aims to utilize a larger and more diverse dataset to improve the robustness of the results and to establish a more rigorous method comparison. Additionally, data augmentation methods will be employed to address both intra- and inter-subject variability.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "The dataset used in this study was collected from eleven participants over seventeen nights. To address inter-subject variability, cross-validation was grouped by participants. This means that the data from each participant was used in both training and testing phases, but not simultaneously. This approach ensures that the training and test sets are independent, as data from one participant is not used in both sets at the same time.\n\nTo analyze intra-subject variability, classical cross-validation without grouping by subject was also performed. This involved splitting the data into training and test sets without considering the participant's identity, which allowed for an assessment of how well the model generalizes within the same subject over different time intervals.\n\nThe dataset was further processed by applying a bandpass filter and normalization using the interquartile range. A time-delay estimation was then applied to ensure proper signal quality and synchronicity between BCG and ECG. Windows with a time-delay of 0.1 seconds or more between R-peaks and preliminary J-peaks were omitted from the training dataset, although this discarded approximately 32% of the windows. This step did not introduce a systematic bias because the synchronization between ECG and BCG is independent of signal quality, and only the latter affects the quality of subsequent model training. All windows were used for the validation dataset.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the field, as it includes a diverse set of participants and multiple nights of data collection. However, it is acknowledged that the dataset is relatively small, with only eleven participants. Future work aims to utilize a more comprehensive and open dataset to achieve more robust results and establish a more rigorous method comparison. Additionally, data augmentation techniques will be employed to improve inter-subject generalization.",
  "dataset/availability": "The raw data supporting the conclusions of this article will be made available by the authors, without undue reservation. This means that the dataset used in this study is not publicly available at the moment, but it can be obtained by requesting it from the authors. The authors have committed to sharing the data to ensure the reproducibility of the results and to allow other researchers to build upon this work. However, specific details about the licensing terms under which the data will be shared are not provided. It is implied that the data will be shared in a manner that respects the privacy and consent of the participants, as written informed consent was obtained for the publication of any potentially identifiable images or data included in this article.",
  "optimization/algorithm": "The machine-learning algorithm class used in this work is deep learning, specifically convolutional neural networks (CNNs) and recurrent neural networks (RNNs) such as GRUs and LSTMs. These architectures are well-established in the field of machine learning and have been widely applied to sequence-to-sequence problems, including time series data.\n\nThe specific models employed, such as the Modified CNN-GRUx2 and ResNet, are not entirely new but have been adapted and evaluated for the task of J-peak detection in ballistocardiogram (BCG) signals. The novelty lies in the application of these models to the specific problem of heartbeat extraction from BCG data, rather than the algorithms themselves.\n\nThe choice to publish this work in a network physiology journal rather than a machine-learning journal is driven by the focus on the physiological application and the specific challenges of BCG signal processing. The primary contributions of this study are in the domain of physiological signal processing and the improvement of heartbeat detection methods, which are more aligned with the scope of a network physiology journal. Additionally, the work involves significant domain-specific knowledge and data, making it more relevant to an audience in the field of network physiology.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on convolutional neural networks (CNNs) and, in some evaluations, ResNet architectures to process surrogate signals generated from BCG data. The surrogate signals are created using different kernel types, such as triangular, quadratic, and rectangular, to encode heartbeat events. These surrogate signals are then used to train the deep learning models for J-peak detection in BCG signals.\n\nThe evaluation of the model includes comparisons with other methods, such as those proposed by Pr\u00f6ll et al. (2021) and Pino et al. (2017), but these comparisons are for benchmarking purposes and do not involve using the outputs of these methods as inputs to the current model. The training data for the proposed model is independent and consists of BCG signals collected from participants, with cross-validation grouped by subjects to ensure unbiased estimation for new subjects. The hyperparameters, including kernel width and post-processing parameters, were optimized using a grid search approach to enhance the model's performance.",
  "optimization/encoding": "The data encoding process involved transforming the BCG measurements into a one-dimensional time-series, from which discrete heartbeat events could be detected more precisely. The core of the approach was the special encoding of heartbeats by a surrogate signal. This surrogate signal served as the target function learned by the neural network, aiming to improve the accuracy of subsequent peak detection.\n\nThe signal was initially bandpass-filtered and then normalized using the interquartile range, which is less sensitive to outliers compared to standard z-score normalization. This preprocessing step ensured better signal quality and synchronicity between the BCG and ECG signals. Windows with a time-delay of 0.1 seconds or more between R-peaks and preliminary J-peaks were omitted from the training dataset, although this discarded approximately 32% of the windows, no systematic bias was introduced. All windows were used for the validation dataset.\n\nThree different kernel shapes\u2014quadratic, triangular, and rectangular\u2014were empirically evaluated to find the most suitable kernel for aiding the model to learn the surrogate signal. These kernels were symmetric around the reference heartbeat, with the rectangular encoding reflecting binary masking of the heartbeat with an additional area of interest. Additionally, distance-time encoding was evaluated, where the value at any timestamp in the surrogate signal represents the distance to the closest heartbeat.\n\nThe surrogate signal was then subjected to post-processing using a low-pass filter or moving average to enhance the clarity and centering of the characteristic peak. A classical peak detection algorithm was applied with optimized temporal and magnitude thresholds to extract heartbeats. The individual timestamps of the heartbeats, denoted by p, constituted the elements of the set of J-peaks.\n\nThe evaluation of the kernel shapes showed that the triangular kernel was the most successful for each metric, indicating more accurate heartbeat estimation and higher precision of heartbeat detection. The hyperparameter optimization suggested a kernel width of 0.8 seconds for the triangular kernel and 1.2 seconds for the quadratic kernel. The rectangular (binary) encoding of heartbeats yielded solid but inferior results compared to continuous surrogate signals with a single optimum at the heartbeat timestamp.",
  "optimization/parameters": "In our study, the optimization process involved several key parameters that were carefully selected and tuned to enhance the performance of our heartbeat detection model. The primary parameters included those related to the preprocessing, model architecture, kernel width, and post-processing stages.\n\nFor the preprocessing stage, we applied a second-order low-pass filter with a cut-off frequency of 7.5 Hz to smooth the model output. This step was crucial for reducing noise and preparing the signal for accurate peak detection.\n\nIn terms of the model architecture, we evaluated different configurations, including a convolutional neural network (CNN) and a more complex ResNet. The CNN was initially used with a kernel width of 5, and we also experimented with various kernel types such as rectangular, triangular, and quadratic. The triangular kernel, in particular, showed superior performance, indicating its effectiveness in generating a surrogate signal that facilitated precise peak detection.\n\nThe kernel width was another critical parameter that underwent optimization. Through empirical validation, we determined that a kernel width of 0.8 seconds for the triangular kernel and 1.2 seconds for the quadratic kernel yielded the best results. This optimization was essential for balancing the precision of peak detection and the robustness of the model against imprecise measurements or imperfect datasets.\n\nPost-processing involved peak detection using the scipy package, with specific parameters set for distance, height, and prominence. These parameters were fine-tuned to ensure accurate identification of J-peaks in the surrogate signal.\n\nThe selection of these parameters was guided by a grid search approach during the cross-validation procedure. This method allowed us to systematically explore different combinations of parameters and identify the optimal settings that minimized errors in heartbeat estimation and peak detection. By grouping individual windows by subject, we ensured an unbiased estimator for new subjects, further enhancing the generalizability of our findings.\n\nIn summary, the optimization process involved a comprehensive evaluation of multiple parameters across different stages of the model. The triangular kernel with a width of 0.8 seconds emerged as the most effective configuration, demonstrating significant improvements in heartbeat detection accuracy and precision.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The fitting method employed in our study utilized a convolutional neural network (CNN) and a ResNet architecture, both of which are known for their capacity to handle complex patterns in data. The number of parameters in these models is indeed larger than the number of training points, which could potentially lead to overfitting. To mitigate this risk, we implemented several strategies.\n\nFirstly, we employed cross-validation, specifically grouping individual windows by subject to obtain an unbiased estimator for new subjects. This approach helps in assessing the model's generalization capability across different subjects. Secondly, we optimized the hyperparameters of the pre-processing, model, kernel width, and post-processing using a grid search approach. This systematic tuning ensured that the model parameters were well-suited to the data, reducing the likelihood of overfitting.\n\nAdditionally, we used a second-order low-pass filter with a cut-off frequency of 7.5 Hz to smooth the model output, which helped in reducing noise and improving the robustness of the peak detection. The peak detection was performed using the scipy package with specific parameters (distance = 20, height = 0.01, and prominence = 0.1), which were chosen to balance sensitivity and specificity.\n\nTo rule out underfitting, we evaluated the model's performance on various metrics, including HR MAE, HR MAE 8s, NAd_sym (ms), and IBI MAE (ms). The triangular kernel, in particular, yielded excellent results across these metrics, indicating that the model was capable of capturing the underlying patterns in the data. Furthermore, the use of different kernel types (rectangular, triangular, quadratic) and the distance-time encoding allowed us to compare and select the most effective surrogate signal for heartbeat detection.\n\nThe evaluation of the more complex ResNet architecture also resulted in a higher precision for J-peak detection, with a mean precision of less than 50 ms. This suggests that the model was sufficiently complex to capture the necessary features without being overly simplistic.\n\nIn summary, the fitting method was designed to balance model complexity and generalization capability. The use of cross-validation, hyperparameter optimization, and rigorous evaluation metrics ensured that both overfitting and underfitting were effectively addressed.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method was the use of dropout layers, specifically a 40% dropout rate, which was applied between residual blocks in our Residual Network (ResNet) architecture. Dropout helps to prevent overfitting by randomly setting a fraction of input units to zero at each update during training time, which forces the network to learn more robust features.\n\nAdditionally, we utilized batch normalization between each residual block. Batch normalization helps to stabilize and accelerate the training process by normalizing the inputs of each layer, which can also act as a regularizer and reduce overfitting.\n\nWe also implemented early stopping during the training process. This technique involves monitoring the model's performance on a validation set and stopping the training process when the performance stops improving. This helps to prevent the model from overfitting to the training data by avoiding unnecessary epochs.\n\nFurthermore, we performed hyperparameter optimization using a grid search approach. This method systematically works through multiple combinations of hyperparameter values, selecting the best combination based on performance metrics. This ensures that the model is not only optimized for the training data but also generalizes well to unseen data.\n\nLastly, we used a comprehensive cross-validation procedure where individual windows were grouped by subject. This approach helps to obtain an unbiased estimator for new subjects, ensuring that the model's performance is not overly dependent on any specific subset of the data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. These details include the specific kernel widths that were optimized for different kernel types, such as the triangular and quadratic kernels. The evaluation metrics and the cross-validation procedure, which involved grouping individual windows by subject, are also described. However, the exact model files and the optimization schedule are not explicitly provided in the text. The raw data supporting the conclusions of this article will be made available by the authors, without undue reservation. This implies that upon request, the necessary configurations and parameters can be shared. Regarding the license, it is not specified in the provided text, but typically, such data sharing would be subject to standard academic sharing agreements or institutional policies.",
  "model/interpretability": "The model proposed in this work is not entirely a black box, as it incorporates interpretable components that contribute to its transparency. The use of surrogate signals generated by different kernels, such as triangular and quadratic, provides a clear and interpretable way to encode heartbeat events. These kernels create continuous and well-defined peaks in the surrogate signal, which are designed to be easily learnable by the sequence-to-sequence model. This approach allows for a more intuitive understanding of how the model processes and detects heartbeat events.\n\nThe post-processing steps, including smoothing with a low-pass filter and peak detection using specific parameters, further enhance the interpretability of the model. The choice of these parameters and the resulting peaks in the surrogate signal can be directly linked to the detected heartbeat events, making it easier to trace back the model's decisions.\n\nAdditionally, the evaluation metrics used, such as HR MAE, NAd_sym, and IBI MAE, provide clear and quantifiable measures of the model's performance. These metrics allow for a detailed assessment of the model's accuracy in estimating heart rates and detecting heartbeat events, contributing to its overall transparency.\n\nThe use of a convolutional neural network (CNN) and, in some cases, a ResNet architecture, introduces some level of complexity. However, the focus on kernel shapes and widths for generating surrogate signals adds a layer of interpretability. The choice of kernel shape and width is hypothesized to significantly impact the model's performance, and this relationship is explored and discussed in the results.\n\nIn summary, while the model does leverage deep learning techniques that can be seen as complex, the use of interpretable surrogate signals and clear evaluation metrics makes it more transparent than a typical black-box model. The design choices, such as kernel shapes and post-processing steps, provide insights into how the model operates and makes decisions.",
  "model/output": "The model is primarily designed for regression tasks, specifically for estimating surrogate signals that approximate heartbeats in time series data. It focuses on generating continuous surrogate signals with distinct peaks, which are then used for accurate peak detection. The model outputs an approximation of the surrogate signal, which is subsequently processed to detect J-peaks in ballistocardiogram (BCG) data. This approach allows for the estimation of heart rates and interbeat intervals, which are crucial for various physiological analyses. The model's output is not a classification but a continuous signal that is smoothed and then subjected to peak detection algorithms to identify heartbeats accurately. The evaluation metrics used, such as HR MAE and IBI MAE, further emphasize the regression nature of the model's output.",
  "model/duration": "The execution time for the models varied depending on the architecture and the dataset used. For the convolutional neural network (CNN) with three layers, the training was conducted for 40 epochs using the Adam optimizer with a learning rate of 0.0001 and a batch size of 32. The specific execution time for this model was not explicitly mentioned, but it is reasonable to assume that it was completed within a few hours on standard hardware, given the parameters and the size of the dataset.\n\nFor the Residual Network (ResNet), the training involved an initial convolutional layer with 8 channels and a kernel width of 5, followed by two convolutional and two deconvolutional residual blocks. Batch normalization and a 40% dropout were applied between each residual block. The training details for the ResNet, including the number of epochs and the optimizer settings, were not specified, but it is likely that the execution time was similar to that of the CNN, possibly taking a bit longer due to the added complexity of the residual blocks.\n\nThe Modi\ufb01ed CNN-GRUx2 model, used from a previous study, was trained on our dataset for 75 epochs. This model estimates the mean heartbeat within 8-second windows, and the training time for this model would have been longer than the CNN and ResNet due to the increased number of epochs.\n\nIn summary, while the exact execution times were not provided, the CNN and ResNet models were trained for a reasonable duration given their architectures and the dataset size. The Modi\ufb01ed CNN-GRUx2 model required more time due to the extended training period.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the proposed method involved a comprehensive assessment using several metrics to ensure robustness and accuracy. The method was evaluated using cross-validation, where individual windows were grouped by subject to obtain an unbiased estimator for new subjects. This approach helps in assessing the generalizability of the method to unseen data.\n\nHyperparameters for pre-processing, the model, kernel width, and post-processing were optimized using a grid search approach. This systematic optimization ensures that the best possible configuration of parameters is used for the evaluation.\n\nThe evaluation metrics included HR MAE (Heart Rate Mean Absolute Error) for both full 64-second windows and reduced 8-second windows. This allows for a comparison with existing methods that report heart rates within 8-second intervals. Additionally, the Nearest-Advocate criterion (NAd_sym) was used to measure the synchronicity between detected J-peaks and reference R-peaks, providing insights into the temporal accuracy of heart peak detection. The Interbeat Interval Mean Absolute Error (IBI MAE) was also calculated to assess the precision of the detected heartbeats.\n\nThe results were summarized in a table, comparing the proposed methods against several existing approaches. The table includes mean and standard deviation across subjects for each evaluation metric, highlighting the performance of different kernels and architectures. This detailed evaluation ensures that the proposed method's effectiveness and reliability are thoroughly assessed.",
  "evaluation/measure": "In our evaluation, we employed a comprehensive set of performance metrics to assess the accuracy and precision of heartbeat estimation and detection. The primary metrics reported include:\n\n* **HR MAE**: This metric measures the mean absolute error in heart rate estimation within full 64-second windows. It provides an overall indication of how accurately the model estimates heart rates over extended periods.\n\n* **HR MAE 8s**: This metric evaluates the mean absolute error in heart rate estimation within reduced 8-second windows. This measure ensures comparability with other methods that estimate heart rates over shorter intervals.\n\n* **NAd_sym (ms)**: The Nearest-Advocate criterion quantifies the synchronicity between detected J-peaks and reference R-peaks. It reflects the average temporal deviation between detected heart peaks and their nearest reference peaks, measured in milliseconds. This metric is crucial for understanding the temporal precision of heartbeat detection.\n\n* **IBI MAE (ms)**: This metric calculates the mean absolute error between the interbeat intervals (IBI) of detected J-peaks and the reference IBI. Given the high precision of detection, the results are reported in milliseconds, providing insights into the accuracy of interbeat interval measurements.\n\nThese metrics collectively offer a robust evaluation framework, covering both the accuracy of heart rate estimation and the precision of heartbeat detection. They are representative of the literature, as they align with commonly used metrics in similar studies, ensuring that our results can be compared and validated against existing research. The inclusion of both short and long window assessments, along with temporal precision measures, provides a comprehensive view of the model's performance.",
  "evaluation/comparison": "In our evaluation, we compared our proposed methods against several existing approaches to assess their performance in heartbeat estimation and J-peak detection. We included state-of-the-art deep learning methods, such as the Modified CNN-GRUx2 architecture from a previous study, which we trained on our dataset. Additionally, we used classical digital signal processing methods with default parameters to ensure comparability. These methods served as simpler baselines to evaluate the effectiveness of our proposed techniques.\n\nOur proposed methods were evaluated using various kernels, including rectangular, triangular, and quadratic, as well as distance-time encoding. The triangular kernel, in particular, yielded excellent results, demonstrating significant improvements in heart rate estimation and J-peak detection compared to existing solutions. We also evaluated the most suitable kernel with a ResNet architecture to further validate its performance.\n\nThe results, summarized in a table, show the high performance of our proposed methods across multiple metrics, including heart rate estimation and the precision of heart peak detection. The triangular kernel was the most successful, indicating a more accurate heartbeat estimation and higher precision in heart peak detection. The evaluation also highlighted the importance of the kernel type and width in generating surrogate signals that are easily learnable by sequence-to-sequence models and facilitate precise peak detection.\n\nIn summary, our methods were rigorously compared to both advanced and simpler baselines, demonstrating their superiority in heartbeat estimation and J-peak detection. The use of different kernels and architectures provided insights into the optimal configurations for achieving high performance in these tasks.",
  "evaluation/confidence": "The evaluation of our method includes several performance metrics, such as HR MAE, HR MAE 8s, NAd_sym, and IBI MAE. These metrics provide a comprehensive assessment of the heart rate estimation and peak detection accuracy. However, specific confidence intervals for these metrics are not explicitly detailed in the provided results. The evaluation focuses on mean values and standard deviations across subjects, which offer insights into the variability and consistency of the results.\n\nStatistical significance is implied through the comparison of our proposed methods with existing approaches. The results indicate that our methods, particularly those using triangular and quadratic kernels, outperform other techniques in terms of heart rate estimation and peak detection precision. The lower inter-subject standard deviation for our methods, except for one specific measure, suggests a more robust and reliable performance.\n\nThe cross-validation procedure, which groups individual windows by subject, ensures an unbiased estimator for new subjects. This approach helps in assessing the generalizability of our findings. Additionally, the hyperparameter optimization using a grid search further enhances the reliability of our results by fine-tuning the model parameters for optimal performance.\n\nWhile the results are promising, the absence of explicit confidence intervals and detailed statistical tests limits the ability to make definitive claims about the superiority of our method over others. Future work could include more rigorous statistical analyses to provide stronger evidence of the method's effectiveness.",
  "evaluation/availability": "The raw data supporting the conclusions of this article will be made available by the authors, without undue reservation. This ensures that other researchers can access and verify the findings presented in the study. The data availability statement emphasizes the commitment to transparency and reproducibility in scientific research. By providing access to the raw data, the authors facilitate further analysis and potential replication of the results, contributing to the broader scientific community's understanding of the methods and outcomes discussed in the article."
}