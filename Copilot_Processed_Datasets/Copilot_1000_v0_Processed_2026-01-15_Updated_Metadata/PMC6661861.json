{
  "publication/title": "A Mixed Quantum Chemistry/Machine Learning Approach for the Fast and Accurate Prediction of Biochemical Redox Potentials and Its Large-Scale Application to 315\u202f000 Redox Reactions.",
  "publication/authors": "Jinich A, Sanchez-Lengeling B, Ren H, Harman R, Aspuru-Guzik A",
  "publication/journal": "ACS central science",
  "publication/year": "2019",
  "publication/pmid": "31404220",
  "publication/pmcid": "PMC6661861",
  "publication/doi": "10.1021/acscentsci.9b00297",
  "publication/tags": "- Gaussian Processes\n- Machine Learning\n- Quantum Chemistry\n- Redox Potentials\n- Biochemical Reactions\n- Thermodynamics\n- Computational Chemistry\n- Molecular Modeling\n- Data Calibration\n- Chemical Informatics\n- Reaction Networks\n- Semiempirical Methods\n- Group Contribution Method\n- Metabolites\n- KEGG Database",
  "dataset/provenance": "The dataset utilized in our study is derived from a comprehensive collection of biochemical redox reactions. We focused on generating a network of over 315,000 non-natural biochemical reactions involving approximately 70,000 compounds. This dataset is unique in its scope and focus, as it specifically targets redox biochemistry, unlike other network expansion algorithms that consider a broader range of chemical transformations.\n\nOur approach differs from previous methods by starting with natural metabolites as seeds but expanding to a vast network of non-natural compounds. This allows for a more extensive exploration of potential biochemical reactions and their redox potentials. The dataset was curated to include a wide variety of redox transformations, although the application of Gaussian Process (GP) regression to other redox transformations is limited by the availability of experimental data.\n\nThe high-throughput nature of our methodology enabled the generation of this extensive dataset, which provides a robust foundation for predicting redox potentials. The dataset includes reactions involving carbonyl functional groups, such as the reduction to alcohols or amines, and highlights the importance of generating more experimental data under controlled conditions to enhance the predictive power and scope of calibrated quantum chemical approaches.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our work is Gaussian Processes (GPs). This class of algorithms is well-suited for tasks where the amount of data is limited, as they grow in complexity according to the data and are robust to overfitting. GPs penalize complex models and provide estimates of uncertainty on predictions, making them particularly useful for our application.\n\nThe implementation of GPs in our study is not entirely new, but its application to reaction energy predictions using reaction fingerprints is novel. To the best of our knowledge, this is the first time GPs have been used in this specific way within the field of computational chemistry. The focus of our publication is on the application of this method to biochemical redox potentials, rather than the development of a new machine-learning algorithm per se.\n\nThe decision to publish this work in a chemistry journal, rather than a machine-learning journal, is driven by the primary focus of the research. The innovation lies in the application of GPs to a specific chemical problem\u2014predicting redox potentials\u2014and the development of a high-throughput methodology for generating a network of biochemical reactions. The machine-learning aspect is a tool that enables this chemical application, but the main contributions are in the chemical domain. Therefore, it is appropriate to publish in a chemistry journal to reach the relevant audience and highlight the chemical significance of the work.",
  "optimization/meta": "The model employs Gaussian Processes (GP) as the primary machine-learning technique. This choice was made due to the amount of experimental data available, as GPs are well-suited for tasks where the data set is not extremely large. They grow in complexity according to the data and are robust to overfitting, penalizing complex models and providing estimates of uncertainty on predictions.\n\nThe GP calibration approach uses molecular fingerprints to compute the similarity between molecules of interest and compounds with experimental data. This implementation is the first application of GP to reaction energy predictions using reaction fingerprints.\n\nThe model does not directly use data from other machine-learning algorithms as input. Instead, it relies on molecular fingerprints and experimental data to improve the accuracy of prediction methods. The use of GP calibration is a versatile approach that enhances the accuracy of a wide variety of prediction methods, including those based on electronic structure theory.\n\nThe training data for the GP calibration is independent of the data used for other methods. The model focuses on the prediction of standard redox potentials of carbonyl functional groups, specifically the reduction to alcohols or amines. The application of GP regression to other redox transformations is limited by the availability of experimental data, highlighting the importance of generating more experimental data for biochemical redox potentials under controlled conditions.\n\nThe model's high-throughput nature was demonstrated by generating a network of over 315,000 non-natural biochemical reactions involving approximately 70,000 compounds. The algorithm differs from other network expansion algorithms by focusing solely on redox biochemistry, which converges to molecules that cannot be further reduced or oxidized. This contrasts with other algorithms that consider various chemical transformations and start from natural seed compounds.",
  "optimization/encoding": "In our study, data encoding played a crucial role in enabling the machine-learning algorithm to effectively predict biochemical redox potentials. We utilized reaction fingerprints to encode the molecular structures of both substrates and products involved in redox reactions. Specifically, we employed Morgan fingerprints, which capture the local topological information of molecules by mapping the vicinity of connected atoms, including their formal charges, bond types, and positions relative to cyclic structures. These fingerprints were constructed as 2048-length vectors with a radius of 6, providing a detailed representation of molecular environments.\n\nTo enhance computational efficiency, we reduced the dimensionality of these fingerprints to approximately 200 using a gradient boosted tree. This dimensionality reduction did not significantly impact prediction accuracy but made the data more manageable for the machine-learning model.\n\nThe reaction fingerprints were derived by taking the difference between the fingerprints of the product and substrate molecules. This approach allowed us to focus on the changes in molecular environments that occur during the redox reactions, which is essential for accurate predictions.\n\nAdditionally, we normalized these fingerprints to facilitate the calculation of similarity between reactions using the cosine distance. This metric measures the similarity between two vectors, which is adapted in Gaussian process regression to quantify the similarity between reactions.\n\nBy encoding the data in this manner, we ensured that the machine-learning algorithm could effectively learn from the relationships between relative distances in feature space, leading to improved prediction accuracy for biochemical redox potentials.",
  "optimization/parameters": "In our model, the number of parameters, p, is determined by the choice of the kernel function used in the Gaussian Process (GP) regression. The kernel function we employ is a mixture of a squared exponential kernel and a noise kernel. This kernel involves several parameters:\n\n1. **\u03c3rxn**: The variance of the reaction fingerprint kernel, which controls the overall scale of the kernel.\n2. **l**: The length scale parameter, which determines the smoothness of the kernel.\n3. **\u03c3noise**: The white noise variance parameter, which accounts for the noise in the data.\n\nThese parameters are optimized during the training process to best fit the data. The selection of these parameters is crucial for the performance of the GP regression model. We used leave-one-out cross-validation (LOOCV) to ensure that the model is not overfitting. This involves training the model on all data points except one and predicting its value, repeating this process for all data points. This method helps in selecting the optimal parameters that generalize well to unseen data.\n\nAdditionally, the inherent robustness of GPs to overfitting, due to their penalization of more complex models via the objective function, aids in the selection of appropriate parameters. The model chemistry, which includes the choice of geometry optimization and single-point electronic energy calculation methods, also plays a role in determining the overall performance and parameter selection. For instance, the PM7 semiempirical method was chosen for its balance of computational cost and prediction accuracy.",
  "optimization/features": "In our study, the input features used for the Gaussian Process (GP) regression model are reaction fingerprints. These fingerprints capture the molecular structure of both the substrate (oxidized) and product (reduced) in a redox reaction. The specific type of fingerprint used is the Morgan fingerprint with a radius of 6. This choice allows for a comprehensive representation of the chemical reactions involved.\n\nFeature selection was not explicitly performed in the traditional sense, as the reaction fingerprints themselves are designed to encapsulate relevant chemical information. However, the use of a specific radius for the Morgan fingerprints can be seen as a form of feature engineering, ensuring that the most relevant structural information is included.\n\nThe process of calibrating the GP regression model involved leave-one-out cross-validation (LOOCV). This method ensures that the model's performance is evaluated on untrained data, thereby mitigating the risk of overfitting. During LOOCV, the model is trained on all data points except one, and its prediction is made for the excluded point. This procedure is repeated for all data points, ensuring that the reported predictions and accuracies are robust and generalizable.\n\nThe kernel used in the GP regression is a mixture of a squared exponential kernel and a noise kernel. This combination enhances the model's robustness by incorporating both the smoothness of the squared exponential kernel and the flexibility to account for noise in the data. The kernel function is defined with parameters that include the variance of the reaction fingerprint kernel, the length scale parameter, and the white noise variance parameter.\n\nIn summary, the input features for our model are reaction fingerprints, specifically Morgan fingerprints with a radius of 6. Feature selection in the traditional sense was not performed, but the choice of fingerprint parameters serves as a form of feature engineering. The model calibration process used LOOCV to ensure robust and generalizable predictions.",
  "optimization/fitting": "In our study, we employed Gaussian Process (GP) regression for calibration, which inherently possesses mechanisms to mitigate overfitting. The kernel used in our GP model is a mixture of a squared exponential kernel and a noise kernel, enhancing the model's robustness. This kernel design helps in penalizing more complex models via an objective function, thereby reducing the risk of overfitting.\n\nTo further ensure that our model did not overfit, we utilized leave-one-out cross-validation (LOOCV). This technique involves training the model on all data points except one and then predicting the value of the excluded point. This process is repeated for all data points, ensuring that the reported predictions, accuracies, and scatter plots are derived from untrained data. This rigorous validation method helps in assessing the model's generalization capability and ruling out overfitting.\n\nRegarding the number of parameters, GP regression does not explicitly define a fixed number of parameters like traditional machine learning models. Instead, it relies on the kernel function and its hyperparameters, which are optimized during the training process. The kernel function captures the relationships between data points, and its complexity is controlled by the hyperparameters, such as the length scale and noise variance. This approach allows the model to adapt to the data without explicitly increasing the number of parameters, thus avoiding the issue of having a much larger number of parameters than training points.\n\nUnderfitting was addressed by selecting an appropriate kernel and ensuring that the model had sufficient capacity to capture the underlying patterns in the data. The use of a mixture kernel, which includes both a squared exponential component and a noise component, provides the model with the flexibility to fit the data accurately. Additionally, the hyperparameters of the kernel were optimized to balance the trade-off between bias and variance, ensuring that the model neither underfits nor overfits the data.\n\nIn summary, our approach to fitting the model involved using a robust kernel design, rigorous cross-validation techniques, and careful optimization of hyperparameters to prevent both overfitting and underfitting. This methodology ensures that our model provides accurate and reliable predictions of redox potentials.",
  "optimization/regularization": "To prevent overfitting during the calibration process with Gaussian Process (GP) regression, leave-one-out cross-validation (LOOCV) was employed. This technique involves training the model on all data points except one and then predicting the value of the excluded point. This procedure is repeated for each of the 81 data points in the dataset, ensuring that the reported predictions, accuracies, and scatter plots are derived from untrained data. Additionally, GPs inherently possess robustness against overfitting. This is because the training procedure penalizes more complex models (higher-rank kernels) through an objective function, thereby discouraging overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the publication. Specifically, the model chemistries tested include various combinations of geometry optimization procedures and methods for calculating single-point electronic energies. These details are provided in the tables and descriptions within the text, which outline the different approaches such as UFF, PM7, DFTB, HF-3c, and PBEh-3c for geometry optimization, and DFTB, PM7, HF-3c, PBEh-3c, DSD-PBEP86/SVP, and DLPNO\u2212CCD(T)/SVP for single-point energy calculations.\n\nThe optimization schedule and specific parameters for each model chemistry are also discussed, including the use of Gaussian Process (GP) regression for calibration. The kernel function employed in the GP regression is a mixture of a squared exponential kernel and a noise kernel, which enhances the robustness of the model. The variance of the reaction fingerprint kernel, the length scale parameter, and the white noise variance parameter are key components of this kernel function.\n\nRegarding the availability of model files and optimization parameters, the publication provides comprehensive details on the methods and procedures used. However, specific model files or scripts are not directly provided within the text. For access to detailed implementations or additional resources, readers are encouraged to refer to the methods section of the publication or contact the authors for further information. The publication is open-access, ensuring that the methodologies and findings are freely available to the scientific community.",
  "model/interpretability": "The model employed in this study leverages Gaussian Processes (GPs), which are inherently interpretable due to their probabilistic nature and the use of kernel functions. Unlike black-box models, GPs provide a transparent framework where the predictions are accompanied by uncertainty estimates, offering insights into the confidence of the model's outputs.\n\nOne of the key aspects of interpretability in our model is the use of a mixture kernel, which combines a squared exponential kernel and a noise kernel. This mixture allows the model to capture both smooth trends in the data and the inherent noise, making it robust and interpretable. The squared exponential kernel, in particular, is known for its smoothness and ability to model continuous functions, which is crucial for predicting redox potentials accurately.\n\nAdditionally, the model uses reaction fingerprints (rxnFP) as inputs, which are designed to capture the chemical reactivity and structural features of molecules involved in redox reactions. These fingerprints provide a clear and chemically meaningful representation of the data, enhancing the interpretability of the model. For example, the fingerprints can highlight specific functional groups or molecular fragments that significantly influence the redox potentials, making it easier to understand the underlying chemical principles driving the predictions.\n\nThe use of leave-one-out cross-validation (LOOCV) further contributes to the model's transparency. By training the model on all data points except one and predicting the value of the excluded point, LOOCV ensures that the reported predictions and accuracies are based on untrained data. This approach not only helps in assessing the model's generalization performance but also provides a clear understanding of how well the model can predict new, unseen data.\n\nIn summary, the model's transparency is achieved through the use of interpretable kernel functions, chemically meaningful reaction fingerprints, and a rigorous cross-validation strategy. These elements collectively ensure that the model's predictions are not only accurate but also understandable, providing valuable insights into the chemical processes being studied.",
  "model/output": "The model employed in our study is a regression model, specifically Gaussian Process (GP) regression. This approach is used to predict redox potentials based on reaction fingerprints. The GP regression model leverages a kernel function, which is a mixture of a squared exponential kernel and a noise kernel, to capture the relationships between data points. The kernel function used is defined as:\n\nK(x, x') = \u03c3_rxn^2 * exp(-|x - x'|^2 / (2l^2)) + \u03c3_noise^2 * I\n\nwhere K is the kernel function, \u03c3_rxn is the variance of the reaction fingerprint kernel, l is the length scale parameter, and \u03c3_noise is the white noise variance parameter.\n\nThe model's predictive power comes from learning relationships between relative distances in feature space, utilizing reaction fingerprints to quantify the similarity between reactions. Morgan fingerprints, in particular, were found to outperform AP3 fingerprints in the context of biochemical redox potential predictions. The predictions obtained from the GP regression can be interpreted as weighted averages of the training data, where the weights are probabilistic in nature.\n\nTo ensure robustness and prevent overfitting, leave-one-out cross-validation (LOOCV) was used. This involved training the model on all data points except one and predicting its value, a process repeated for all data points. The reported predictions, accuracies, and scatter plots are based on untrained data, ensuring that the model's performance is evaluated on unseen data.\n\nThe model's accuracy was quantified using three metrics: Pearson correlation coefficient (r), the coefficient of determination (R\u00b2), and mean absolute error (MAE) in millivolts. Various model chemistries were tested to find a balance between computational cost and prediction accuracy. The PM7 semiempirical method was selected for both geometry optimizations and single-point electronic energies due to its reasonable accuracy and low computational cost.\n\nThe output of the model provides predictions for redox potentials, which are crucial for understanding and predicting biochemical reactions. The use of GP regression with reaction fingerprints allows for accurate and efficient predictions, making it a valuable tool in the field of computational chemistry.",
  "model/duration": "The execution time of the model varied significantly depending on the specific model chemistry used. For instance, the PM7 semiempirical method, which was chosen for its balance of computational cost and prediction accuracy, had a compute time of approximately 0.36 seconds per molecule when using geometry optimization. This method was notably faster than more computationally intensive approaches like DLPNO\u2212CCSD(T), which had a compute time of around 6111 seconds per molecule. The Universal Force Field (UFF) and Density Functional Tight Binding (DFTB) methods also showed relatively quick compute times, around 0.01 seconds per molecule. Other methods, such as HF-3c and PBEh-3c, had varying compute times depending on the geometry optimization strategy employed, ranging from a few seconds to several minutes per molecule. The choice of model chemistry thus played a crucial role in determining the overall execution time, with faster methods like PM7 being preferred for high-throughput predictions due to their efficiency.",
  "model/availability": "The software developed for this work is not publicly released. The algorithms and methods described in the publication were implemented using the Python programming language. The implementation leverages the RDKit cheminformatics software for molecular transformations and network expansion. However, the specific code and executables used to generate the results are not available for public use or distribution. Therefore, there is no executable, web server, virtual machine, or container instance provided for running the algorithm.",
  "evaluation/method": "The evaluation of the method involved several key steps to ensure its accuracy and robustness. Initially, we quantified prediction accuracy using three different metrics: the Pearson correlation coefficient (r), the coefficient of determination (R2), and mean absolute error (MAE, in mV). These metrics provided a comprehensive assessment of the model's performance.\n\nTo avoid overfitting, we employed leave-one-out cross-validation (LOOCV). This technique involved training the model on all data points except one and then predicting the value of the excluded point. This process was repeated for all 81 data points, ensuring that the reported predictions and accuracies were derived from untrained data.\n\nWe also compared the accuracy of our Gaussian Process (GP) regression calibrated quantum chemistry approach against the Group Contribution Method (GCM). The GCM implementation used was adapted to the thermodynamics of biochemical reactions, providing a relevant benchmark for comparison.\n\nAdditionally, we tested various model chemistries, which included different combinations of geometry optimization procedures and methods for calculating single-point electronic energies. The goal was to select a model chemistry that offered a balance between computational cost and prediction accuracy. The PM7 semiempirical method was chosen for both geometry optimizations and single-point electronic energies due to its reasonable accuracy and lower computational cost.\n\nThe evaluation also considered the impact of using multiple conformers versus a single conformer per molecule. While we did not observe a significant difference in predicted accuracy between these approaches, the adequacy of using a single geometry should be verified for other chemical applications and datasets.\n\nOverall, the evaluation process was rigorous and multifaceted, ensuring that the method's performance was thoroughly assessed and validated.",
  "evaluation/measure": "To evaluate the performance of our models, we employed three key metrics: the Pearson correlation coefficient (r), the coefficient of determination (R\u00b2), and the mean absolute error (MAE) in millivolts (mV). These metrics provide a comprehensive view of the prediction accuracy and reliability of our models.\n\nThe Pearson correlation coefficient (r) measures the linear relationship between the predicted and experimental redox potentials, indicating how well the predictions align with the actual data. The coefficient of determination (R\u00b2) further quantifies the proportion of variance in the experimental data that is predictable from our models, offering insight into the models' explanatory power.\n\nThe mean absolute error (MAE) is a crucial metric that represents the average magnitude of errors in the predictions, without considering their direction. A lower MAE indicates more accurate predictions. We also report the standard deviation (\u03c3) of the absolute errors to provide additional context on the variability of the prediction errors.\n\nThese metrics are widely used in the literature for evaluating the performance of predictive models in computational chemistry, ensuring that our evaluation is representative and comparable to other studies in the field. By focusing on these metrics, we aim to provide a clear and concise assessment of our models' predictive capabilities.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our method with publicly available methods using benchmark datasets. We assessed the prediction accuracies of various model chemistries, including semiempirical methods like PM7, density functional theory (DFT) methods such as HF-3c and PBEh-3c, and more computationally intensive approaches like DSD-PBEP86. These comparisons were performed using metrics such as the Pearson correlation coefficient (r), the coefficient of determination (R\u00b2), and mean absolute error (MAE) in millivolts (mV).\n\nWe also evaluated simpler baselines, such as the group contribution method (GCM), which is commonly used for predicting redox potentials. Our results showed that the GP-calibrated PM7 approach outperformed GCM in terms of higher Pearson correlation coefficients, higher coefficients of determination, and lower mean absolute errors. This indicates that our method provides more accurate predictions at a lower computational cost compared to traditional approaches.\n\nAdditionally, we explored the use of different geometries and computational times for these methods. For instance, we compared the performance of optimized geometries versus those obtained using the Universal Force Field (UFF). The results highlighted the trade-offs between computational efficiency and prediction accuracy, with PM7 emerging as a cost-effective choice that balances both factors.\n\nOverall, our comparisons demonstrated that the GP-calibrated quantum chemistry method offers significant improvements in prediction accuracy and computational efficiency over existing methods and simpler baselines.",
  "evaluation/confidence": "To evaluate the confidence in our results, we employed several statistical methods. We quantified prediction accuracy using three metrics: Pearson correlation coefficient (r), the coefficient of determination (R\u00b2), and mean absolute error (MAE). These metrics provide a comprehensive view of our model's performance.\n\nWe performed a nonparametric bootstrap hypothesis test to assess the statistical significance of the differences in prediction accuracy among various model chemistries. This involved subsampling data points from the table of redox reactions with experimental data and computing the prediction accuracy summary statistics for each subsample. By comparing the fraction of subsampled datasets that resulted in summary statistics greater than or equal to those of the model chemistry we were evaluating, we could determine the statistical significance of our findings.\n\nAdditionally, we considered the use of Gaussian Process (GP) regression calibration, which provides estimates of uncertainty on predictions. This approach is robust to overfitting and penalizes complex models, ensuring that our results are reliable and not due to chance. The GP method grows in complexity according to the data, making it well-suited for tasks where the amount of experimental data is limited.\n\nWhile our method showed improvements over the Group Contribution Method (GCM), there is still significant variance in prediction accuracies across individual reactions. This variance limits the confidence with which one can predict the redox potential of a specific reaction. However, the main value of our method lies in its ability to achieve a lower average prediction error, which is crucial for applications requiring high-throughput predictions.\n\nIn summary, our evaluation includes confidence intervals through bootstrap hypothesis testing and uncertainty estimates from GP regression. These methods provide a robust framework for assessing the statistical significance and reliability of our results.",
  "evaluation/availability": "Not enough information is available."
}