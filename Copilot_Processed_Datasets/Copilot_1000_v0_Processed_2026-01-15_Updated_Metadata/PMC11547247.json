{
  "publication/title": "Improving platelet-RNA-based diagnostics: a comparative analysis of machine learning models for cancer detection and multiclass classification.",
  "publication/authors": "Jopek MA, Pastuszak K, Sieczczy\u0144ski M, Cygert S, \u017baczek AJ, Rondina MT, Supernat A",
  "publication/journal": "Molecular oncology",
  "publication/year": "2024",
  "publication/pmid": "38887841",
  "publication/pmcid": "PMC11547247",
  "publication/doi": "10.1002/1878-0261.13689",
  "publication/tags": "- Platelet RNA\n- Cancer diagnosis\n- Machine learning\n- Bioinformatics\n- Gene expression\n- NSCLC\n- Pan-cancer\n- Logistic regression\n- Random forest\n- XGBoost\n- Hemolysis\n- Batch effect\n- Multiclass classification\n- Data normalization\n- Feature importance\n- Pathway analysis\n- Hemoglobin expression\n- Technical processing errors\n- Early cancer screening\n- Transcript analysis",
  "dataset/provenance": "The dataset utilized in this study is derived from publicly available raw platelet RNA samples. These samples were collected and processed following established guidelines. The dataset encompasses samples gathered from January 2013 to June 2021 by 11 different institutes. It includes samples from presumed healthy, asymptomatic controls and from 17 distinct types of cancer. The cancer types covered in the dataset are breast cancer, cholangiocarcinoma, colorectal cancer, endometrial cancer, esophageal cancer, glioma, hepatocellular carcinoma, head and neck squamous cell carcinoma, lymphoma, melanoma, multiple myeloma, non-small cell lung cancer, ovarian cancer, pancreatic ductal adenocarcinomas, prostate cancer, renal cell carcinoma, and urothelial carcinoma.\n\nThe study involved a comprehensive overview of the dataset's composition, which is detailed in a provided table. The dataset underwent rigorous preprocessing and normalization to ensure data integrity and quality. Samples with fewer than 100,000 total reads were omitted, and only genes with confirmed Gencode status were included. This preprocessing step was crucial for maintaining the dataset's reliability.\n\nThe dataset was initially used in a previous study by In \u2019t Veld et al. and has been further analyzed in the current investigation. The dataset's composition and the methods used for its processing are thoroughly documented, ensuring transparency and reproducibility. The final dataset, after excluding samples from the Netherlands Cancer Institute due to suspected technical processing errors, consists of 1751 samples, each with 5349 transcript features. This dataset has been utilized to train and test various machine learning models for cancer diagnosis.",
  "dataset/splits": "The dataset was divided into three distinct splits: training, validation, and test sets. The training set comprised 338 samples, the validation set also included 338 samples, and the test set consisted of 1077 samples, totaling 1751 samples across all splits.\n\nThe data was collected from various cancer types and asymptomatic healthy controls. For instance, the training set included 104 asymptomatic healthy controls, 20 breast cancer samples, 19 colorectal cancer samples, and so on. The validation set mirrored the training set in terms of the number of samples per category. The test set, being the largest, included a broader range of samples, such as 146 asymptomatic healthy controls, 53 breast cancer samples, 46 colorectal cancer samples, and others.\n\nThis distribution ensured a comprehensive evaluation of the models across different cancer types and healthy controls, facilitating robust training, validation, and testing phases.",
  "dataset/redundancy": "The dataset used in this study was split into training, validation, and test sets. Specifically, 60% of the healthy donors were assigned to the training set, while the remaining 40% were used for testing. The proportions between NKI and non-NKI donors were preserved in both the training and test sets to ensure that the class distributions were representative.\n\nThe training and test sets are independent. This independence was enforced by ensuring that the samples used in the training phase were not reused in the testing phase. This approach helps to validate the model's performance on unseen data, providing a more reliable assessment of its generalizability.\n\nThe distribution of samples in this study compares favorably to previously published machine learning datasets in the context of cancer diagnosis. The dataset includes a diverse range of cancer types, with a total of 1751 samples and 5349 transcript features each. This comprehensive dataset allows for robust training and validation of machine learning models, ensuring that the models can generalize well to new, unseen data. The inclusion of multiple cancer types and a large number of samples helps to mitigate issues related to class imbalance and overfitting, which are common challenges in machine learning for cancer diagnosis.",
  "dataset/availability": "The data utilized in this study is publicly available. The raw platelet RNA samples were sourced from a previous study. The dataset encompasses samples collected from various institutes between January 2013 and June 2021, including presumed healthy, asymptomatic controls and samples from 17 different types of cancer.\n\nThe dataset underwent rigorous processing to ensure quality and integrity. Samples with fewer than 100,000 total reads were omitted, and only genes with confirmed Gencode status were included. The data was normalized using the DESeq2 package in R, with the human reference genome (hg19) serving as the annotative reference point.\n\nThe dataset was split into training, validation, and test sets. The training set consisted of 60% of the healthy donors, preserving the proportions between samples from the Netherlands Cancer Institute (NKI) and non-NKI locations. The validation and test sets were used to evaluate the performance of the classifiers.\n\nThe final dataset, after excluding NKI samples due to suspected contamination, comprised 1,751 samples with 5,349 transcript features each. This dataset was used to train and test various machine learning models for cancer diagnosis.\n\nThe code, package versions, and models used in this study are available on a public repository, ensuring transparency and reproducibility. The repository includes comprehensive details on the data splits, model configurations, and performance metrics. This availability allows other researchers to verify the findings and build upon the work presented.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study primarily includes traditional and well-established methods such as logistic regression, random forest, balanced random forest, and XGBoost. These algorithms are widely recognized and have been extensively used in various diagnostic and predictive modeling tasks.\n\nThe algorithms employed are not new; they are established techniques in the field of machine learning. Logistic regression, for instance, is a classical statistical method used for binary classification problems. Random forest and its balanced variant are ensemble learning methods that combine multiple decision trees to improve predictive accuracy and control over-fitting. XGBoost is a popular gradient boosting framework that has been successfully applied in numerous competitions and real-world applications.\n\nThe reason these algorithms were not published in a machine-learning journal is that our focus was on applying and optimizing these methods for a specific biomedical application\u2014cancer detection using platelet-RNA data. The novelty lies in the application and the optimization of these algorithms for this particular diagnostic task rather than the development of new machine-learning algorithms. Our work contributes to the field of molecular oncology by demonstrating the effectiveness of these algorithms in improving diagnostic accuracy for cancer detection, which is crucial for early and reliable diagnosis.",
  "optimization/meta": "Not applicable",
  "optimization/encoding": "The data used in our study consisted of publicly available raw platelet RNA samples, which were collected and processed according to established guidelines. The dataset included samples from various cancer types and healthy controls, collected over several years by multiple institutes.\n\nTo ensure the integrity and quality of our dataset, we applied several preprocessing steps. We omitted samples with fewer than 100,000 total reads and included only genes with confirmed Gencode status. All samples underwent uniform preprocessing and were normalized collectively using the DESeq2 package in R, employing the variance stabilizing transformation. The human reference genome (hg19) served as the annotative reference point in this process.\n\nSignificant differences were observed between samples from the Netherlands Cancer Institute (NKI) and the rest of the cohort. To address this, we prepared a heatmap to highlight the batch effect present in NKI samples. We also compared the expression of hemoglobin between NKI and non-NKI samples, finding significantly higher levels in NKI samples. This suggested potential technical processing errors during sample collection, leading to severe hemolysis and platelet activation. Consequently, all NKI samples were excluded from the final dataset, resulting in a data table of 1751 samples with 5349 transcript features each.\n\nFor the machine learning models, we utilized Python packages such as scikit-learn, xgboost, imblearn, numpy, and pandas. We fine-tuned the models' hyperparameters using GridSearchCV and provided a comprehensive list of parameters. The data was split into training, validation, and test sets, with a specificity threshold exceeding 99% to account for the larger proportion of healthy individuals within the population. This ensured effective cancer identification for a future early cancer screening tool.",
  "optimization/parameters": "In our study, we employed several machine learning models, each with its own set of hyperparameters optimized through grid search. For the Random Forest and Balanced Random Forest models, we tuned six parameters: 'n_estimators' (with values 100, 200, 300), 'max_depth' (with values None, 5, 10, 15), 'min_samples_split' (with values 2, 5, 10), 'min_samples_leaf' (with values 1, 2, 4), and 'max_features' (with values 'sqrt' and 'log2'). For the XGBoost model, we focused on five parameters: 'n_estimators' (with values 100, 200, 300), 'max_depth' (with values 3, 5, 7), 'learning_rate' (with values 0.01, 0.1, 0.2), and 'subsample' (with values 0.5, 0.7, 1). For Logistic Regression, we optimized four parameters: 'C' (with values 0.01, 0.1, 1, 10), 'penalty' (with value 'l2'), 'solver' (with values 'newton-cg', 'lbfgs', 'liblinear', 'sag'), and 'class_weight' (with values None and 'balanced').\n\nThe selection of these parameters was based on a comprehensive grid search strategy, which involved systematically working through multiple combinations of parameter tunes to determine the best-performing model. This method ensures that the models are well-optimized for the specific task of cancer detection, enhancing their diagnostic accuracy and reliability. The grid search process was crucial in identifying the optimal hyperparameters that maximize the models' performance, particularly in distinguishing between healthy and cancerous samples.",
  "optimization/features": "In our study, the number of input features varied depending on the specific model and the stage of the analysis. Initially, all available features were used to train the original PSO-SVM model, which served as a benchmark for comparison. However, we also explored the impact of feature reduction on model performance.\n\nFeature selection was indeed performed to identify the most influential features. This process involved retraining models on subsets of features that demonstrated the highest impact on classifier decisions. For certain models, such as random forest and balanced random forest, this approach improved accuracy by approximately 10%. Notably, logistic regression did not benefit from feature reduction in the same way.\n\nThe feature selection process was conducted using the training set only, ensuring that the validation and test sets remained unbiased. This method helped in enhancing the models' performance, particularly in binary classification tasks. However, in multiclass classification, the reduction in feature count did not significantly affect model efficacy, with accuracy showing negligible variability of less than 10% depending on the feature set.\n\nOverall, while feature selection was a crucial step in optimizing some models, its impact varied across different classification tasks and model types.",
  "optimization/fitting": "The fitting method employed in our study involved a careful balance between model complexity and performance to avoid both overfitting and underfitting. We utilized several machine learning models, including Random Forest, Balanced Random Forest, XGBoost, and Logistic Regression, each with its own set of hyperparameters optimized through grid search.\n\nTo address the potential issue of overfitting, especially when the number of parameters is large relative to the number of training points, we implemented several strategies. First, we used cross-validation to ensure that our models generalized well to unseen data. This involved splitting the data into training and validation sets multiple times and averaging the performance metrics. Additionally, we applied regularization techniques, such as setting a maximum depth for tree-based models and using L2 regularization for logistic regression, to prevent the models from becoming too complex.\n\nUnderfitting was mitigated by thoroughly tuning the hyperparameters of each model. For instance, we explored a range of values for parameters like the number of estimators, maximum depth, and learning rate in tree-based models. For logistic regression, we adjusted the regularization strength and solver type. These optimizations ensured that the models were sufficiently complex to capture the underlying patterns in the data without being overly simplistic.\n\nFurthermore, we performed feature importance analysis and feature reduction to enhance the models' performance. This step helped in identifying the most relevant features, thereby reducing the risk of overfitting and improving the models' ability to generalize. The logistic regression model, in particular, demonstrated superior diagnostic performance, achieving high sensitivity and specificity, which indicates that it was neither overfitted nor underfitted.\n\nIn summary, our approach to fitting involved a combination of cross-validation, regularization, hyperparameter tuning, and feature selection. These methods collectively ensured that our models were well-calibrated, avoiding both overfitting and underfitting, and achieving robust performance in cancer detection.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was regularization, particularly for the logistic regression model. We utilized L2 regularization, which helps to penalize large coefficients and thus reduces the model's complexity, preventing it from overfitting the training data. The regularization parameter, denoted as 'C', was tuned during the hyperparameter optimization process to find the optimal balance between bias and variance.\n\nAdditionally, we performed hyperparameter tuning using grid search for various models, including random forest, balanced random forest, and XGBoost. This process involved systematically working through multiple combinations of hyperparameter values to determine the best configuration that minimizes overfitting while maximizing model performance.\n\nFeature reduction was another crucial step in our methodology. By identifying and selecting the most important features, we not only improved the model's interpretability but also enhanced its generalization capability. This approach helped in reducing the dimensionality of the data, which is essential for preventing overfitting, especially in high-dimensional datasets like ours.\n\nFurthermore, we used cross-validation techniques to evaluate the performance of our models. This involved splitting the data into multiple folds and training the model on different subsets while validating it on the remaining data. This method ensures that the model's performance is consistent across different data splits, reducing the risk of overfitting to a specific subset of the data.\n\nIn summary, our study incorporated regularization, hyperparameter tuning, feature reduction, and cross-validation to effectively prevent overfitting and enhance the reliability of our diagnostic models.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are indeed available. We have detailed the grid search parameters for various models, including Random Forest, Balanced Random Forest, XGBoost, and Logistic Regression. These parameters are crucial for understanding how we optimized our models to achieve superior diagnostic performance.\n\nThe specific configurations for each model type are as follows:\n\nFor Random Forest and Balanced Random Forest, we explored different values for 'n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', and 'max_features'. This allowed us to fine-tune the models to better handle the complexity of our dataset.\n\nFor XGBoost, we adjusted 'n_estimators', 'max_depth', 'learning_rate', and 'subsample' to optimize the model's performance. These parameters are essential for balancing the model's complexity and its ability to generalize to new data.\n\nFor Logistic Regression, we focused on 'C', 'penalty', 'solver', and 'class_weight'. These parameters helped us achieve a high sensitivity and specificity, making Logistic Regression the most effective model in our study.\n\nRegarding the availability of model files and optimization schedules, we have open-sourced our code. This means that all the details about our optimization process, including the schedules and any specific model files used, are accessible to the public. By open-sourcing our code, we aim to facilitate further advancements in the field and contribute to the development of more accurate and robust cancer detection tools.\n\nThe license under which our code is available allows for its use, modification, and distribution, provided that appropriate credit is given to the original authors. This approach ensures that our work can be built upon by other researchers, fostering collaboration and innovation in the scientific community.",
  "model/interpretability": "In our study, we prioritized the use of models that offer interpretability, ensuring that the decision-making process is transparent and understandable. Among the algorithms we evaluated, logistic regression (LogReg) stood out as particularly interpretable. Unlike black-box models, LogReg allows for the examination of coefficient significance, which provides insights into the relative importance of various predictor variables, or transcripts, in determining the outcome.\n\nBy analyzing the coefficients of the LogReg model, we can identify which transcripts have the most significant influence on the model's predictions. This feature is crucial for understanding the biological relevance of the model's decisions. For instance, our analysis highlighted several key transcripts, such as FKBP5, TMSB4XP8, MTRNR2L12, HBB, and SPDYC, which were found to have the largest weights in the model's predictions. This not only aids in the interpretability of the model but also offers promising biomarkers for future diagnostic applications.\n\nThe ability to interpret the model's predictions is essential for building trust in the diagnostic tool and for facilitating further research. By understanding which transcripts are most influential, researchers can delve deeper into the biological mechanisms underlying cancer detection, potentially leading to new therapeutic targets or diagnostic markers. This transparency is a significant advantage of using logistic regression in our study, as it allows for a nuanced understanding of the model's predictive capacity and its potential applications in clinical settings.",
  "model/output": "The model discussed in this publication is primarily focused on classification tasks. Specifically, it involves binary classification for distinguishing between cancer and healthy samples, as well as multi-class classification for identifying specific types of cancer. The binary classification model was trained and tested to detect cancer with high sensitivity and specificity, achieving notable performance metrics such as a 68% detection rate at 99% specificity. For multi-class classification, the model was evaluated on five types of cancer: GLIO, HNSCC, NSCLC, OVCAR, and PDAC, with the logistic regression model demonstrating superior accuracy and robustness compared to other tested algorithms.\n\nThe performance of the models was assessed using various metrics, including sensitivity, specificity, accuracy, and standard deviation across different folds and random seeds. Visual representations, such as confusion matrices and ROC curves, were used to illustrate the model's performance. Additionally, the impact of feature reduction on model performance was explored, highlighting the importance of selecting relevant predictor variables for accurate cancer detection.\n\nThe logistic regression model, in particular, stood out for its interpretability, allowing for the evaluation of the relative importance of different transcripts in predicting cancer outcomes. This interpretability is crucial for understanding the biological significance of the model's predictions and for potential clinical applications.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for our research has been open-sourced to facilitate further advancements in the field of liquid biopsy-based cancer detection. This includes the code, package versions, and models used in our study. The materials are available on GitLab at the following URL: [https://gitlab.com/jopekmaksym/improving-platelet-rna-based-diagnostics](https://gitlab.com/jopekmaksym/improving-platelet-rna-based-diagnostics). By making our code publicly accessible, we aim to encourage collaboration and innovation, ultimately contributing to earlier and more reliable cancer diagnosis, which is crucial for improving patient outcomes.",
  "evaluation/method": "The evaluation of the method involved several steps to ensure robustness and accuracy. Initially, the dataset was split into training, validation, and test sets, with specific proportions allocated to each cancer type and healthy controls. For healthy donors, 60% were assigned to the training set, while the remaining 40% were used for testing. This split preserved the proportions between samples from the Netherlands Cancer Institute (NKI) and non-NKI locations.\n\nA logistic regression classifier was trained to distinguish between NKI and non-NKI samples. Despite the class imbalance, the classifier achieved perfect classification with 100% accuracy and 100% ROC AUC on the test set. Feature importance analysis was conducted, and the most significant features were selected for pathway analysis. However, no pathways were significantly enriched after applying the Benjamini\u2013Hochberg FDR correction.\n\nThe same evaluation process was repeated for non-small cell lung cancer (NSCLC) patients. After applying the Benjamini\u2013Hochberg FDR correction, 4626 out of 5346 considered transcripts were found to be differentially expressed between NKI NSCLC patients and those from other locations. The classifier achieved 88% accuracy, 86% balanced accuracy, and 91.6% ROC AUC. The confusion matrix for these results is presented in a supplementary figure.\n\nAdditionally, gene ontology analysis was performed on the most important features to gain further insights into the biological significance of the findings. The overall performance across all cancer types was also evaluated using binary classification (Healthy vs Cancer). Sensitivity and detection accuracy were assessed across various random seeds and cancer stages, with error bars indicating standard deviation. The performance of the original PSO-SVM model was used for comparison in these evaluations.",
  "evaluation/measure": "In our evaluation, we focused on several key performance metrics to comprehensively assess the diagnostic capabilities of our models. For binary classification tasks, such as distinguishing between healthy and cancerous samples, we reported sensitivity and specificity. Sensitivity, also known as the true positive rate, measures the proportion of actual positives correctly identified by the model. Specificity, or the true negative rate, indicates the proportion of actual negatives correctly identified. These metrics are crucial for understanding how well the models can detect cancer while minimizing false positives and false negatives.\n\nFor multi-class cancer prediction, we reported balanced accuracy, which is the average of recall obtained on each class. This metric is particularly useful when dealing with imbalanced datasets, ensuring that the performance is not skewed by the majority class. We also provided detection accuracy for the first and second predictions across various random seeds and folds, giving a robust estimate of model performance.\n\nAdditionally, we included error bars representing the standard deviation acquired from multiple random seeds and folds, providing a measure of the models' stability and reliability. This approach allows for a more nuanced understanding of the models' performance variability.\n\nOur choice of metrics is representative of the literature, as sensitivity, specificity, and balanced accuracy are commonly used in cancer detection studies. By reporting these metrics, we aim to provide a clear and comparable evaluation of our models' diagnostic performance. The inclusion of error bars further enhances the transparency and reproducibility of our results, aligning with best practices in the field.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of various machine learning models to assess their diagnostic performance in cancer detection. We included a comparison with publicly available methods, specifically the PSO-SVM (particle swarm optimized support vector machine) model from an original study. This allowed us to benchmark our approaches against established techniques.\n\nAdditionally, we performed comparisons with simpler baselines, such as logistic regression. Our findings revealed that under certain conditions, traditional algorithms like logistic regression could achieve superior diagnostic performance compared to more complex models. Logistic regression emerged as the most effective model, demonstrating a sensitivity of 68% at a 99% specificity threshold and a balanced accuracy of 77.65% in multiclass cancer prediction across several cancer types. This highlights the importance of model selection and hyperparameter optimization in enhancing diagnostic accuracy.\n\nThe sensitivity of models was evaluated based on cancer type, stages, and feature groups. We grouped early stages (stage I-II) and late stages (III-IV) together for a comprehensive analysis. Error bars in our figures represent the standard deviation acquired from various random seeds, ensuring the robustness of our results. The overall performance in detecting cancer was assessed through binary classification (Healthy vs Cancer), as well as across individual cancer types.\n\nOur approach to feature reduction generally boosted the cancer detection performance of all models, except for our most sensitive algorithm, LogReg. This suggests that while simpler models can be highly effective, other methods might provide better results in specific contexts. Our research provides a strong foundation for developing more accurate and robust liquid biopsy-based cancer detection tools. By open-sourcing our code, we aim to facilitate further advancements in this field, ultimately contributing to earlier and more reliable cancer diagnosis.",
  "evaluation/confidence": "The evaluation of our models includes a comprehensive assessment of performance metrics with associated confidence intervals. For instance, the pan-cancer classification metrics presented in Table S3 include 95% confidence intervals for sensitivity and AUC_ROC values, providing a clear indication of the variability and reliability of these estimates.\n\nTo ensure the statistical significance of our results, we have employed error bars representing the standard deviation acquired from various random seeds and folds. This approach allows us to compare the performance of our models across different conditions and stages of cancer, as illustrated in Figures S5, S6, S7, and S8. These figures depict the sensitivity and detection accuracy for both the first and second predictions on test samples, facilitating a robust comparison with the original PSO-SVM model.\n\nThe inclusion of confidence intervals and standard deviations in our evaluations underscores the rigor of our statistical analyses. This methodology enables us to confidently claim the superiority of our models over baseline methods, as the performance metrics are supported by a strong statistical foundation.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study utilized publicly available raw platelet RNA samples from a specific dataset, but the evaluation files generated during the analysis are not released. The dataset used in this investigation was collected and processed according to established guidelines, and it included samples from various types of cancer and healthy controls. The study focused on distinguishing samples from the Netherlands Cancer Institute (NKI) from those of other locations using a logistic regression classifier. The performance of the classifier was evaluated, and the results were reported in the publication. However, the specific evaluation files, such as the confusion matrix and gene ontology analysis, are not made available to the public. The study adheres to ethical guidelines and ensures that all data used was anonymized and collected with informed consent."
}