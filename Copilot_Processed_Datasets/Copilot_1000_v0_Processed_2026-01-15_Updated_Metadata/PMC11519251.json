{
  "publication/title": "A radiomics-based interpretable machine learning model to predict the HER2 status in bladder cancer: a multicenter study.",
  "publication/authors": "Wei Z, Bai X, Xv Y, Chen SH, Yin S, Li Y, Lv F, Xiao M, Xie Y",
  "publication/journal": "Insights into imaging",
  "publication/year": "2024",
  "publication/pmid": "39466475",
  "publication/pmcid": "PMC11519251",
  "publication/doi": "10.1186/s13244-024-01840-3",
  "publication/tags": "- Machine Learning\n- Radiomics\n- Bladder Cancer\n- HER2 Status Prediction\n- CT Imaging\n- Model Interpretation\n- SHAP Values\n- Feature Selection\n- LASSO Regression\n- Clinical Decision-Making\n- Predictive Modeling\n- Medical Imaging\n- Diagnostic Performance\n- Statistical Analysis\n- Cross-Validation",
  "dataset/provenance": "The dataset used in this study was sourced from multiple centers, ensuring a diverse and representative sample. A total of 207 bladder cancer (BCa) patients were included, with 109 patients having HER2-positive (HER2-p) status and 98 patients having HER2-negative (HER2-n) status. The data collection involved enhanced CT scans performed within two weeks before surgery, with images saved in the original Digital Imaging and Communications in Medicine (DICOM) format. The nephrographic phase (NP) CT images were specifically chosen for analysis due to their common use in tumor identification for bladder cancer.\n\nThe dataset has not been previously published or used by the community, as it is part of an ongoing study. The data was collected from Picture Archiving and Communication Systems (PACS) and involved manual segmentation of regions of interest (ROIs) by experienced radiologists. This process ensured high-quality data for radiomics feature extraction and subsequent model construction. The dataset's diversity and the rigorous data collection process contribute to its potential value for future research in the field of bladder cancer prediction and diagnosis.",
  "dataset/splits": "The dataset consists of two primary splits: a training set and a test set. The training set comprises 154 data points, while the test set includes 53 data points. This results in a total of 207 data points across both splits.\n\nThe distribution of data points in each split is as follows:\n\n* **Training Set (n = 154):**\n  * Age: 67.41 \u00b1 9.80\n  * Gender:\n    * Male: 138 (89.6%)\n    * Female: 16 (10.4%)\n  * Pathological T-stage:\n    * Ta: 10 (6.5%)\n    * T1: 52 (33.8%)\n    * T2: 50 (32.5%)\n    * T3: 27 (17.5%)\n    * T4: 15 (9.7%)\n  * Pathological N-stage:\n    * N0/Nx: 148 (96.1%)\n    * N1-2: 6 (3.9%)\n  * Pathological grade:\n    * Low: 18 (11.7%)\n    * High: 136 (88.3%)\n  * HER2 status:\n    * Positive: 83 (53.9%)\n    * Negative: 71 (46.1%)\n\n* **Test Set (n = 53):**\n  * Age: 67.85 \u00b1 11.80\n  * Gender:\n    * Male: 47 (88.7%)\n    * Female: 6 (11.3%)\n  * Pathological T-stage:\n    * Ta: 0\n    * T1: 6 (11.3%)\n    * T2: 29 (54.7%)\n    * T3: 12 (22.6%)\n    * T4: 6 (11.3%)\n  * Pathological N-stage:\n    * N0/Nx: 46 (86.8%)\n    * N1-2: 7 (13.2%)\n  * Pathological grade:\n    * Low: 2 (3.8%)\n    * High: 51 (96.2%)\n  * HER2 status:\n    * Positive: 26 (49.1%)\n    * Negative: 27 (49.1%)",
  "dataset/redundancy": "The dataset used in this study consisted of 207 bladder cancer (BCa) patients, with 109 having HER2-positive status and 98 having HER2-negative status. The dataset was split into a training set and a test set. The training set comprised 154 patients, while the test set included 53 patients.\n\nThe training and test sets were designed to be independent to ensure that the model's performance could be evaluated on unseen data. This independence was enforced by randomly assigning patients to either the training or test set, ensuring that there was no overlap between the two groups.\n\nThe distribution of clinical and pathological characteristics between the training and test sets was compared. While there were no significant differences in age, gender, or pathological grade, notable differences were observed in pathological T-stage (p = 0.003) and pathological N-stage (p = 0.024). These differences highlight the variability in the dataset and the need for robust model validation.\n\nCompared to previously published machine learning datasets in similar domains, this dataset is relatively small, which may limit the generalizability of the findings. However, the use of multiple centers for data collection helps to mitigate some of the selection bias that can arise from smaller, single-center studies. The differences in baseline characteristics between the training and test sets underscore the importance of further validation in larger and more diverse cohorts to ensure the model's reliability and applicability in clinical settings.",
  "dataset/availability": "The datasets analyzed during the current study are not publicly available. This is due to the need for follow-up research. However, the datasets are available from the corresponding author upon reasonable request. This approach ensures that the data can be used responsibly and ethically, while also allowing for potential future collaborations and verifications. The decision to keep the data private helps maintain the integrity of ongoing research and ensures that any use of the data aligns with the study's objectives and ethical guidelines.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and widely recognized in the field. Specifically, the algorithms employed include logistic regression (LR), support vector machine (SVM), k-nearest neighbors (KNN), eXtreme Gradient Boosting (XGBoost), and random forest (RF). These algorithms are part of the broader ensemble learning and traditional machine learning techniques.\n\nThe algorithms used are not new; they have been extensively studied and applied in various domains. The choice of these algorithms was driven by their proven effectiveness in handling classification tasks and their ability to provide robust performance across different datasets. The study focuses on applying these algorithms to a specific medical imaging problem, rather than introducing a novel algorithm.\n\nThe decision to use these established algorithms in a medical imaging context is justified by their ability to handle complex data and provide interpretable results. The study leverages these algorithms to develop a predictive model for HER2 status in bladder cancer using radiomics features extracted from CT scans. The performance of these models was evaluated using standard metrics such as the area under the receiver operating characteristic curve (AUC), accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). The random forest (RF) algorithm, in particular, demonstrated superior performance in both the training and test sets, making it the optimal classifier for this study.",
  "optimization/meta": "The model developed in this study does not function as a meta-predictor. Instead, it directly utilizes radiomics features extracted from CT images to predict HER2 status in bladder cancer. The study employed five different machine learning algorithms: logistic regression (LR), support vector machine (SVM), k-nearest neighbors (KNN), eXtreme Gradient Boosting (XGBoost), and random forest (RF). Each of these algorithms was trained and evaluated independently on the same set of radiomics features.\n\nThe random forest (RF) model was identified as the optimal classifier due to its superior performance in both the training and test sets. The RF model demonstrated the highest area under the receiver operating characteristic curve (AUC) and achieved the best balance of accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV).\n\nThe training data used for each model was derived from a cohort of 207 patients with pathologically confirmed bladder cancer, divided into training and test sets. The independence of the training data was maintained through proper splitting and validation techniques, ensuring that the models were evaluated on unseen data to assess their generalizability.",
  "optimization/encoding": "In our study, we extracted a total of 1218 radiomics features from nephrographic phase CT scans for each patient. To ensure the reliability of these features, we retained only those with good inter- and intra-observer agreement, specifically those with an intraclass correlation coefficient (ICC) greater than 0.75. This step resulted in 887 radiomics features being used for further analysis.\n\nTo reduce dimensionality and mitigate multicollinearity, we removed highly correlated features, retaining 223 radiomics features. Subsequently, we performed a t-test analysis to further refine the feature set, which reduced it to 33 features. Finally, we employed the LASSO logistic regression method with fivefold cross-validation to screen and select the most relevant features. This process identified eleven key features that were used to establish the radiomics model.\n\nThe selected features were then fed into five different machine learning models: logistic regression (LR), random forest (RF), support vector machine (SVM), extreme gradient boosting (XGBoost), and k-nearest neighbors (KNN). Each model had specific hyperparameters tailored to optimize performance. For instance, the logistic regression model used a regularization parameter (C) of 0.056 and the 'lbfgs' solver, while the random forest model utilized 130 estimators with a maximum depth of 4. The support vector machine employed a radial basis function (RBF) kernel with a regularization parameter (C) of 1.0. The XGBoost model was configured with a maximum depth of 2 and 10 estimators, and the k-nearest neighbors model used 10 neighbors with the 'minkowski' metric.\n\nThe data encoding and preprocessing steps were crucial in ensuring that the machine learning models could effectively learn from the radiomics features and make accurate predictions. By carefully selecting and refining the features, we aimed to enhance the models' performance and interpretability.",
  "optimization/parameters": "In our study, we utilized five different machine learning algorithms to construct predictive models: logistic regression (LR), random forest (RF), support vector machine (SVM), extreme gradient boosting (XGBoost), and k-nearest neighbors (KNN). Each of these models has its own set of hyperparameters that were carefully tuned to optimize performance.\n\nFor the logistic regression model, we set the regularization strength (C) to 0.056, used the 'l2' penalty, and employed the 'lbfgs' solver. The maximum number of iterations was set to 1000, and the tolerance for stopping criteria was 0.0001.\n\nThe support vector machine model used a radial basis function (RBF) kernel with a default gamma value of 'auto'. The regularization parameter (C) was set to 1.0, and the maximum number of iterations was set to -1, indicating no limit.\n\nThe k-nearest neighbors model was configured with 10 neighbors, using the 'auto' algorithm and the 'minkowski' metric with a power parameter (p) of 2. The leaf size was set to 30, and the weights were uniformly distributed.\n\nFor the random forest model, we specified a maximum depth of 4, with 130 estimators. The minimum number of samples required to split an internal node was set to 9, and the minimum number of samples required to be at a leaf node was 5. The criterion for splitting was set to 'entropy', and the maximum number of features considered for splitting at each leaf node was set to 'log2'.\n\nThe XGBoost model was configured with a maximum depth of 2 and 10 estimators. The learning rate was not explicitly set, allowing the model to use the default value. The booster type was set to 'gbtree', and the tree method was set to 'auto'.\n\nThe selection of these hyperparameters was based on a combination of domain knowledge and empirical tuning. We performed extensive experimentation and validation to ensure that the chosen parameters provided the best possible performance for our specific dataset and problem. This process involved cross-validation and grid search techniques to systematically explore different parameter settings and identify the optimal configuration for each model.",
  "optimization/features": "In our study, we utilized a total of eleven radiomics features as input for our machine learning models. These features were carefully selected through a rigorous feature selection process. Initially, a large set of radiomics features was extracted from nephrographic phase CT scans for each patient. To ensure reliability, only features with good inter- and intra-observer agreement were retained. Subsequently, highly correlated features were removed, and a t-test analysis was conducted to further refine the feature set. The final selection of eleven features was achieved using the LASSO logistic regression method with fivefold cross-validation, ensuring that the feature selection was performed exclusively on the training set. This approach helped in identifying the most relevant features for establishing a robust radiomics model.",
  "optimization/fitting": "In our study, we employed several machine learning algorithms, including logistic regression (LR), random forest (RF), support vector machine (SVM), extreme gradient boosting (XGBoost), and k-nearest neighbors (KNN), to construct predictive models. The number of radiomics features initially extracted was significantly larger than the number of training points, which could potentially lead to overfitting. To mitigate this risk, we implemented a rigorous feature selection process.\n\nWe began by extracting 1218 radiomics features from nephrographic phase CT scans for each patient. To ensure reliability, we retained only those features that showed good inter- and intra-observer agreement, with an intraclass correlation coefficient (ICC) greater than 0.75, resulting in 887 features. We then removed highly correlated features, reducing the set to 223 features. Further refinement was achieved through t-test analysis, which narrowed down the features to 33. Finally, we applied the LASSO logistic regression method with fivefold cross-validation to select the most relevant features, ultimately screening eleven features to establish the radiomics model.\n\nTo rule out overfitting, we used cross-validation techniques and evaluated the models on a separate test set. The performance metrics, including the area under the receiver operating characteristic curve (AUC), accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV), were assessed in both the training and test sets. The random forest (RF) model, for instance, achieved an AUC of 0.965 in the training set and 0.815 in the test set, demonstrating its robustness and generalizability.\n\nUnderfitting was addressed by ensuring that the models were complex enough to capture the underlying patterns in the data. The hyperparameters for each model were carefully tuned to balance model complexity and performance. For example, the RF model used a maximum depth of 4 and 130 estimators, while the XGBoost model had a maximum depth of 2 and 10 estimators. These settings allowed the models to learn from the data without becoming too simplistic.\n\nIn summary, our approach involved a systematic feature selection process and rigorous model evaluation to prevent overfitting and underfitting, ensuring that the final models were both accurate and generalizable.",
  "optimization/regularization": "In our study, we employed the LASSO (Least Absolute Shrinkage and Selection Operator) logistic regression method with fivefold cross-validation to prevent overfitting. This technique is particularly useful for feature selection and regularization, helping to identify the most relevant radiomics features while reducing the complexity of the model. By applying LASSO, we were able to screen and retain eleven significant features out of the initial 1218 radiomics features extracted from nephrographic phase CT scans. This process not only enhanced the model's interpretability but also improved its generalization performance by mitigating the risk of overfitting to the training data. Additionally, the use of cross-validation ensured that the model's performance was robust and reliable across different subsets of the data.",
  "optimization/config": "The hyper-parameter configurations for the machine learning models used in this study are available. These configurations are detailed in the supplementary material, specifically in Appendix E3. The models included in the study are logistic regression (LR), random forest (RF), support vector machine (SVM), extreme gradient boosting (XGBoost), and k-nearest neighbors (KNN). Each model's hyper-parameters are explicitly listed, providing transparency and reproducibility for the experiments conducted.\n\nThe optimization schedule and parameters are also implicitly described through the methods used. For instance, the LASSO logistic regression method with fivefold cross-validation was employed to select features, indicating a rigorous approach to model optimization. Additionally, the use of the SHAP method for model interpretation ensures that the feature contributions are well-documented and understood.\n\nRegarding model files and optimization parameters, while the specific files are not directly mentioned, the detailed reporting of hyper-parameters and methods ensures that the optimization process can be replicated. The study was conducted using Python 3.8.0 for model construction and performance evaluation, and the SHAP package (version 2.0.0) for model interpretation. Statistical analyses were performed using R package (version 4.1.2) and SPSS statistical software, further ensuring the robustness and reproducibility of the results.\n\nThe availability of these details in the supplementary material allows other researchers to replicate the study's findings and build upon the work, promoting transparency and collaboration in the scientific community.",
  "model/interpretability": "The model employed in this study is not a black box. To ensure transparency and interpretability, the SHAP (Shapley Additive Explanations) method was utilized. This approach, rooted in game theory, provides insights into how each feature influences the model's predictions. By calculating the contribution of each feature, SHAP offers both global and local interpretations.\n\nGlobally, SHAP bar charts were generated to display the importance of each feature based on mean SHAP values. This visualization highlights which features have the most significant impact on the model's decisions. For instance, features like log-sigma-1-0-mm-3D_glszm_SmallAreaEmphasis and log-sigma-2-0-mm-3D_glszm_SmallAreaEmphasis were identified as major contributors to the model's predictions.\n\nLocally, SHAP summary plots were created to show the impact of each feature on individual predictions. These plots use dots to represent patients, with different colors indicating varying levels of influence on the model's output. Additionally, SHAP force plots were developed for specific cases, illustrating how individual features drive the prediction process for each patient.\n\nTo further enhance understanding, individual trees within the Random Forest (RF) model were visualized. This allowed for an intuitive grasp of the decision paths and logic used by the model. By breaking down the complex interactions within the RF, clinicians can better comprehend the internal mechanisms and decision-making processes, fostering trust and acceptance in clinical settings.",
  "model/output": "The model is a classification model. It is designed to predict the HER2 status in bladder cancer patients, which is a categorical outcome (HER2-positive or HER2-negative). The model uses various machine learning algorithms, including logistic regression, support vector machine, k-nearest neighbors, eXtreme Gradient Boosting, and random forest, to classify patients based on radiomics features extracted from CT images. The performance of these models is evaluated using metrics such as the area under the receiver operating characteristic curve (AUC), accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). The random forest model was identified as the optimal classifier, demonstrating robust and accurate performance in predicting HER2 status.",
  "model/duration": "The execution time for the models was not explicitly detailed in the publication. However, the models were constructed and evaluated using Python 3.8.0, with the SHAP package (version 2.0.0) used for model interpretation. The statistical analyses were performed using R (version 4.1.2) and SPSS statistical software. The overall workflow of the study is summarized in a figure, but specific details about the execution time for each model are not provided.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the constructed machine learning models involved several key steps and metrics to ensure their robustness and generalizability. Five commonly used machine learning algorithms were employed: logistic regression (LR), random forest (RF), support vector machine (SVM), extreme gradient boosting (XGBoost), and k-nearest neighbors (KNN). The performance of these models was assessed using the receiver operating characteristic (ROC) curve and the area under the ROC curve (AUC) value, which are standard metrics for evaluating the discrimination performance of predictive models.\n\nTo compare the AUC values between different models, Delong\u2019s test was utilized. This statistical test helps determine if there are significant differences in the AUC values, providing a rigorous way to compare model performance.\n\nThe cutoff value for evaluating model performance was identified using the Youden index. This index helps in finding the optimal threshold that maximizes the sum of sensitivity and specificity. Using this cutoff, several performance metrics were calculated, including accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). These metrics offer a comprehensive view of the model's performance across different aspects.\n\nThe overall workflow of the study, including model construction and performance evaluation, was conducted using Python 3.8.0. This ensured a consistent and reproducible environment for all analyses.\n\nIn addition to the primary evaluation metrics, model interpretability was addressed using the SHAP (SHapley Additive exPlanations) method. SHAP provides insights into the influence of each feature on model predictions by calculating the contribution of each feature. This method offers both global and local interpretations, helping to understand the model's decisions at both the overall and individual sample levels. SHAP feature importance plots and summary plots were generated to visualize these contributions. Furthermore, SHAP force plots were created for representative cases to better understand the model\u2019s predictions.\n\nStatistical analyses were performed using the R package (version 4.1.2) and SPSS statistical software. Continuous variables were presented as means and standard deviations, while categorical variables were presented as frequencies and percentages. Confidence intervals for these variables were calculated using the bootstrapping method. Clinical characteristics between the training and test sets were compared using the chi-square test (or Fisher exact test) and t-tests (or Mann\u2013Whitney U test), with a significance level set at a two-sided p-value of less than 0.05.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our machine learning models. The primary metric used was the area under the receiver operating characteristic curve (AUC), which provides a comprehensive measure of a model's ability to discriminate between positive and negative cases. We also calculated the accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) using the cutoff value identified by the Youden index.\n\nThe AUC values were reported for both the training and test sets across five different machine learning algorithms: logistic regression (LR), random forest (RF), support vector machine (SVM), extreme gradient boosting (XGBoost), and k-nearest neighbors (KNN). This allowed us to compare the discriminative power of each model. For instance, in the training set, the AUC values ranged from 0.827 for KNN to 0.965 for RF, indicating that RF had the highest discriminative ability. Similarly, in the test set, RF outperformed the other models with an AUC of 0.815.\n\nIn addition to AUC, we reported accuracy, sensitivity, specificity, PPV, and NPV for the RF model in the test set. The RF model achieved an accuracy of 0.755, sensitivity of 0.667, specificity of 0.846, PPV of 0.710, and NPV of 0.818. These metrics provide a detailed view of the model's performance in terms of correctly identifying positive and negative cases, as well as the reliability of positive and negative predictions.\n\nThe set of metrics reported in this study is representative of standard practices in the literature for evaluating machine learning models, particularly in the context of medical diagnostics. AUC is widely used as a primary metric for assessing model performance, while accuracy, sensitivity, specificity, PPV, and NPV offer additional insights into the model's practical utility. By including these metrics, we ensure a thorough evaluation of our models' performance and their potential clinical applicability.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. However, we did compare the performance of our models using several commonly used machine learning algorithms. These included logistic regression (LR), random forest (RF), support vector machine (SVM), extreme gradient boosting (XGBoost), and k-nearest neighbors (KNN). The performance of these models was evaluated using the receiver operating characteristic (ROC) curve and the area under the ROC curve (AUC) value. Delong\u2019s test was used to compare the AUC between models, providing a statistical measure of the differences in performance.\n\nAdditionally, we compared the models based on their accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). These metrics were calculated using the cutoff value identified by the Youden index. The random forest model ultimately outperformed the other models in both the training and test sets, demonstrating higher accuracy, sensitivity, PPV, and NPV compared to XGBoost, despite no significant difference in AUC.\n\nWhile we did not use simpler baselines in the traditional sense, the inclusion of logistic regression and k-nearest neighbors served as a form of baseline comparison. These models are generally considered simpler compared to more complex algorithms like random forest and XGBoost. The results showed that the random forest model had superior performance, indicating its effectiveness in handling the complexity of the radiomics features extracted from the CT scans.",
  "evaluation/confidence": "The evaluation of our machine learning models included several key metrics to assess their performance, with confidence intervals provided for the area under the receiver operating characteristic curve (AUC). For instance, in the training set, the AUC values were reported with 95% confidence intervals: 0.829 (0.759 \u2013 0.893) for logistic regression, 0.917 (0.863\u2013 0.961) for support vector machine, 0.827 (0.761\u2013 0.883) for k-nearest neighbors, 0.961 (0.927\u2013 0.987) for XGBoost, and 0.965 (0.933\u2013 0.987) for random forest. These intervals give a range within which the true AUC value is likely to fall, providing a measure of the uncertainty around the point estimates.\n\nStatistical significance was assessed using Delong\u2019s test to compare the AUC values between models. In the training set, the random forest model showed significant differences in AUC compared to logistic regression (p < 0.001), support vector machine (p = 0.035), and k-nearest neighbors (p < 0.001). However, there was no significant difference between random forest and XGBoost (p = 0.643). This indicates that while random forest performed better than some models, its performance was comparable to XGBoost.\n\nIn the test set, the random forest model also demonstrated superior performance with an AUC of 0.815, outperforming logistic regression (AUC of 0.803), support vector machine (AUC of 0.709), k-nearest neighbors (AUC of 0.679), and XGBoost (AUC of 0.794). The random forest model achieved an accuracy of 0.755, sensitivity of 0.667, specificity of 0.846, positive predictive value of 0.710, and negative predictive value of 0.818 in the test set. These metrics, along with the confidence intervals and statistical tests, provide a robust evaluation of the model's performance and its superiority over other methods.",
  "evaluation/availability": "Not applicable"
}