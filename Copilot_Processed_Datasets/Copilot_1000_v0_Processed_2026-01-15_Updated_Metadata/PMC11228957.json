{
  "publication/title": "SIMS: A deep-learning label transfer tool for single-cell RNA sequencing analysis.",
  "publication/authors": "Gonzalez-Ferrer J, Lehrer J, O'Farrell A, Paten B, Teodorescu M, Haussler D, Jonsson VD, Mostajo-Radji MA",
  "publication/journal": "Cell genomics",
  "publication/year": "2024",
  "publication/pmid": "38823397",
  "publication/pmcid": "PMC11228957",
  "publication/doi": "10.1016/j.xgen.2024.100581",
  "publication/tags": "- Single-Cell RNA Sequencing\n- Deep Learning\n- Label Transfer\n- Cell Genomics\n- SIMS Pipeline\n- Benchmarking\n- Cerebral Cortex\n- Hippocampus\n- Organoids\n- Trans-Sample Label Transfer",
  "dataset/provenance": "The datasets utilized in this study are sourced from various publicly available repositories and have been used in previous research and by the community. The Peripheral Blood Mononuclear Cells (PBMC) dataset, also known as PBMC68K or Zheng68K, contains approximately 68,450 cells within eleven subtypes. This dataset was generated using 10X Genomics technologies and sequenced with Illumina NextSeq500. It is widely recognized for its imbalanced cell type distribution and transcriptomic similarities, making it a challenging yet popular choice for cell type annotation performance assessment.\n\nThe Human cellular landscape dataset, referred to as Han\u2019s dataset, includes 584,000 cells with 102 different cell types across all major human organs and various developmental time points from over 50 donors. This dataset was generated using Microwell-seq technology.\n\nThe Human heart dataset, known as Tucker\u2019s dataset, comprises 287,269 cells representing 9 different cell types (20 cell subtypes) from 7 donors. This single nuclei RNA-sequencing dataset is valuable for studying the transcriptional and cellular diversity of the human heart.\n\nThe Human kidney dataset, described by Stewart, includes 40,268 mature human kidney cells representing 34 different cell types from 14 donors. This dataset is useful for understanding the cellular composition and diversity of the human kidney.\n\nThe Human lung dataset, referred to as Krasnow\u2019s dataset, consists of 75,400 cells representing 58 different cell types from 3 donors. It was generated using two different sequencing technologies: Smart-seq 2 and 10X Chromium.\n\nThe Adult mouse cortical and hippocampal dataset, generated by the Allen Brain Institute, includes 42 cell types from micro-dissected cortical and hippocampal regions of 8-week-old mice. This dataset is valuable for studying the cellular diversity in the mouse brain.\n\nThe Adult human cortical dataset, also from the Allen Brain Institute, includes single-nucleus transcriptomes from 49,495 nuclei across multiple human cortical areas. The majority of the nuclei are from three donors, providing a comprehensive view of the human cortex.\n\nThe Developing mouse cortical dataset contains microdissected cortices from mice at various embryonic and postnatal stages. This dataset is useful for studying the molecular logic of cellular diversification in the mammalian cerebral cortex.\n\nThe Human cortical organoids dataset includes cells derived from three different cell lines, providing insights into the development and organization of the human cortex in a controlled environment.\n\nThe Human fetal brain development dataset focuses on fetal tissue from the second trimester, specifically the neocortex. It includes samples from gestational weeks 14 to 25, providing a detailed view of early human brain development.\n\nThese datasets have been extensively used in the scientific community for various studies, including cell type annotation, developmental biology, and disease research. Their availability and detailed annotations make them invaluable resources for ongoing and future research in the field of single-cell genomics.",
  "dataset/splits": "In our study, we conducted a data-ablation experiment to evaluate the performance of our model with varying amounts of training data. We observed that using as little as 7% of the data, which corresponds to 2,124 cells, we achieved over 95% accuracy. Similarly, with 9% of the data (2,731 cells), we obtained a Macro F1 score of over 0.95, and with 8% of the data (2,428 cells), we achieved a median F1 score of over 0.95.\n\nAdditionally, we performed trans-sample predictions using three different postmortem samples: H200.1023, H200.1025, and H200.1030. These samples represent individuals of different ages and ethnic backgrounds. We trained the model on one sample and tested it on the other two, covering all possible combinations. The accuracies for these trans-sample predictions ranged from 93.1% to 95.8%.\n\nFor a more detailed breakdown, when training on 80% of the data and testing on 20%, we achieved an accuracy of 98.00% and a Macro F1 score of 0.974. For specific sample combinations, the accuracies and Macro F1 scores varied as follows:\n\n- Training on H200.1023, testing on H200.1025: 94.00% accuracy, 0.84 Macro F1 score\n- Training on H200.1023, testing on H200.1030: 94.40% accuracy, 0.865 Macro F1 score\n- Training on H200.1025, testing on H200.1023: 93.10% accuracy, 0.769 Macro F1 score\n- Training on H200.1025, testing on H200.1030: 93.10% accuracy, 0.779 Macro F1 score\n- Training on H200.1030, testing on H200.1023: 95.80% accuracy, 0.862 Macro F1 score\n- Training on H200.1030, testing on H200.1025: 94.80% accuracy, 0.87 Macro F1 score\n\nThese results demonstrate the robustness of our model across different data splits and samples.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The datasets utilized in this study are publicly available, ensuring transparency and reproducibility. The peripheral blood mononuclear cells dataset, known as PBMC68K, can be accessed via the 10X Genomics website. The Human cellular landscape dataset, generated using Microwell-seq technology, is available through the UCSC Cell Browser. Additionally, the Tucker\u2019s heart dataset is accessible from the Broad Institute's Single Cell Portal. These datasets are provided under standard data-sharing agreements that typically allow for academic and non-commercial use, ensuring that researchers can utilize them for further studies.\n\nThe data splits used for training and testing models are not explicitly detailed in the public forum but are part of the methodologies described in the study. The training involved using the most granular annotation available, which is a standard practice to ensure the models are trained on the most detailed cell type information. The F1 scores reported in the benchmarks refer to the Macro F1-score, which provides a comprehensive evaluation of the model's performance across all cell types.\n\nTo enforce the consistent use of these datasets, the study adheres to established protocols and benchmarks. The models were trained using the same granular annotations, and the performance metrics were calculated uniformly across different datasets. This approach ensures that the results are comparable and reliable, providing a robust framework for future research in cell type annotation and classification.",
  "optimization/algorithm": "The optimization algorithm employed in our work is based on well-established machine-learning techniques, specifically utilizing neural networks. The core of our approach involves training these networks using the Adam optimizer, a popular choice known for its efficiency and effectiveness in handling sparse gradients on noisy problems.\n\nThe machine-learning algorithm used is not entirely new; it builds upon existing methodologies but is tailored for the specific challenges of single-cell RNA data analysis. The decision to publish this work in a domain-specific journal rather than a machine-learning journal is driven by the unique application and the significant impact it has on biological and medical research. The focus is on demonstrating the algorithm's utility in cell-type annotation and its potential to advance the field of single-cell genomics, rather than introducing a novel machine-learning technique.\n\nThe algorithm's training process involves several key components. The learning rate, which controls the step size during optimization, varies between 0.003 and 0.01. Weight decay, or L2 regularization, is used to prevent overfitting, with values ranging from 0 to 0.1. The loss function is designed to handle imbalanced datasets by weighting the contribution of each sample inversely to its label frequency, ensuring that rarer cell types are adequately represented.\n\nTo ensure robust training, techniques such as gradient clipping are employed to prevent exploding gradients, which can destabilize the learning process. Early stopping is implemented to halt training when the validation loss ceases to improve, thereby avoiding overfitting. The datasets are stratified to maintain a balanced distribution of labels across training, validation, and test sets, except in specific cases like the ablation study where sample sizes were insufficient.\n\nIn summary, while the algorithm leverages established machine-learning principles, its application to single-cell RNA data and the specific adaptations made for this domain are what make it novel and impactful in the context of biological research.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It is a standalone algorithm designed to process input data in the form of a cell-by-gene matrix for training and prediction steps. The training statistics can be monitored live, and the prediction step outputs labels along with a confidence score.\n\nThe algorithm was trained on specific datasets, such as two 11A organoids, and then tested on a third 11A organoid. Additionally, a data-ablation study was conducted, demonstrating high accuracy and F1 scores with a minimal amount of training data. The model's performance was also evaluated in trans-sample predictions using three different postmortem samples, achieving high accuracies across various combinations.\n\nThe training and prediction processes are clearly defined, with the input data being directly fed into the algorithm without involving outputs from other machine-learning algorithms. The independence of the training data is ensured through the experimental design, where the model is trained on one sample and tested on others, confirming its robustness and generalizability.",
  "optimization/encoding": "For the machine-learning algorithm, we employed a standard one-hot encoding technique to numerically encode the vectors. In this method, for K labels, the kth label is represented by a standard basis vector with all zeros except for a 1 in the kth position. This encoding scheme is straightforward and effective for categorical data, ensuring that each label is uniquely represented.\n\nThe input data for the algorithm is in the form of a cell-by-gene matrix. This matrix is fed into the algorithm during both the training and prediction steps. The training step involves inputting the cell-by-gene matrix to train the model, with the option to follow training statistics live. In the prediction step, the input data is processed by the algorithm, which then outputs labels along with a confidence score for each prediction.\n\nAdditionally, we utilized a loss function defined as:\n\nL(X; Y) = 1/M * \u2211(wiyi * log(f(xi)))\n\nwhere xi represents the transcriptome vector for the ith sample, yi is the encoded label, wi is the weight, and M is the size of the batch. The weight wi is defined as the inverse frequency of the ith label, which incentivizes the model to learn the transcriptomic structure of rarer cell types. The final signal to update the model weights is calculated as the average across all entries in the loss vector.\n\nThis encoding and preprocessing approach ensures that the data is appropriately formatted for the machine-learning algorithm, facilitating effective training and prediction.",
  "optimization/parameters": "The model utilizes a standard one-hot encoding for numerically encoding the vectors, where for K labels, the kth label is represented by a standard basis vector with a 1 in the kth position and zeros elsewhere. The loss function is defined as:\n\nL(X; Y) = -1/M * \u03a3(wi * yi * log(f(xi)))\n\nwhere xi represents the transcriptome vector for the ith sample, yi is the encoded label, wi is the weight, and M is the batch size. The weight wi is defined as the inverse frequency of the ith label to encourage the model to learn the transcriptomic structure of rarer cell types. The final signal to update the model weights is calculated as the average across all entries in the loss vector.\n\nThe Adam optimizer was employed for all benchmarked models, with the learning rate varying between 0.003 and 0.01, and weight decay (L2 regularization) ranging from 0 to 0.1. The learning rate optimizer reduces the learning rate by 0.75 when the validation loss does not improve for twenty epochs. Gradient clipping is used to prevent exploding gradient values, which helps avoid bad batches from causing the loss to explode and stopping convergence.\n\nThe only hyperparameters tuned were the learning rate to prevent divergence in the loss and weight decay to mitigate overfitting in smaller datasets. Convergence typically occurred within 20 to 100 epochs for all models. The train, validation, and test sets were stratified to ensure the distribution of labels was consistent across all three splits, except in the ablation study where sample sizes were insufficient for stratification.",
  "optimization/features": "The input features for our model correspond to the genes used for cell type prediction. Unlike other machine learning models that may reduce input data due to computational restrictions, our approach, SIMS, can be trained on the entire transcriptome for each cell. This means that the number of features (f) used as input is equivalent to the number of genes in the dataset, allowing for a comprehensive analysis.\n\nFeature selection is inherently performed through the model's architecture. SIMS, built on TabNet, utilizes sparse feature masks in the encoding layer to determine which input features are most relevant for prediction. This process effectively selects the most informative genes for distinguishing between cell types. The sparsity introduced in the sequential attention layers via the sparsemax prior acts as a form of model regularization, enabling the categorization of cell types using only a small number of genes.\n\nThe feature selection process is integrated into the training procedure, ensuring that it is performed using the training set only. This approach helps in avoiding data leakage and maintains the integrity of the validation and test sets. By focusing on the most relevant genes, the model achieves both interpretability and efficiency in cell type classification.",
  "optimization/fitting": "The fitting method employed in our study utilized the Adam optimizer, with learning rates ranging from 0.003 to 0.01 and weight decay (L2 regularization) between 0 and 0.1. The number of parameters in our models was indeed larger than the number of training points, which is common in high-dimensional biological data. To mitigate overfitting, several strategies were implemented.\n\nFirstly, we used a stratified train, validation, and test split to ensure that the distribution of labels was consistent across all datasets. This approach helps in reducing overfitting by ensuring that the model generalizes well to unseen data. Additionally, we employed early stopping based on validation accuracy, which halted training when the validation loss did not improve for twenty consecutive epochs. This prevented the model from overfitting to the training data.\n\nGradient clipping was also used to avoid exploding gradient values, which can lead to poor convergence and overfitting. Furthermore, we tuned only the learning rate and weight decay hyperparameters to avoid divergence in the loss and overfitting in smaller datasets. The use of one-hot encoding for numerical vector encoding and a weighted loss function that incentivizes the model to learn the transcriptomic structure of rarer cell types also contributed to robust training.\n\nTo address underfitting, we ensured that the models reached convergence by the early stopping criterion on validation accuracy before the maximum number of epochs (500) was reached. Convergence typically occurred within 20\u2013100 epochs for all models, indicating that the models were adequately trained. The consistent training performance and few cases of suboptimal convergence due to poor initialization further support the effectiveness of our fitting method.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure robust model performance. One of the primary methods used was weight decay, also known as L2 regularization. The weight decay parameter was set between 0 and 0.1, which helped in penalizing large weights and thus reducing the model's complexity.\n\nAdditionally, we utilized a stratified train, validation, and test split. This ensured that the distribution of labels was consistent across all datasets, minimizing the risk of the model learning spurious patterns from imbalanced data. The only hyperparameters tuned were the learning rate and weight decay, which were adjusted to avoid divergence in the loss and overfitting in smaller datasets, respectively.\n\nEarly stopping was implemented as another overfitting prevention technique. The learning rate was reduced by a factor of 0.75 when the validation loss did not improve for twenty consecutive epochs. This approach allowed the model to converge efficiently without overfitting to the training data.\n\nGradient clipping was also employed to prevent exploding gradient values, which could otherwise lead to unstable training and overfitting. This technique was crucial in maintaining the stability of the training process, especially when dealing with bad batches that could otherwise cause the loss to explode.\n\nOverall, these regularization methods collectively contributed to the model's ability to generalize well to unseen data, ensuring reliable and accurate predictions.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are indeed available. All the code and instructions necessary to use our models, including the specific configurations and optimization parameters, can be found in the Braingeneers GitHub repository. The repository is publicly accessible and contains comprehensive details on how to replicate our experiments and use our pretrained models. Additionally, the code and associated data are archived in the Zenodo archive, ensuring long-term accessibility and reproducibility.\n\nFor those interested in sharing their own models, we encourage contributions through pull requests to our repository. This collaborative approach allows for the inclusion of new pretrained models, fostering a community-driven effort to enhance the available resources.\n\nThe use of h5ad files is strongly recommended for efficient handling of datasets, especially those that may exceed available memory capacity. This format is faster and more efficient than plain text files, facilitating straightforward data sharing within the Python-scanpy environment.\n\nThe web application developed in Streamlit provides an additional layer of accessibility. Hosted on the Streamlit developer cloud, it allows users to perform quick and easy inferences based on pretrained models without the need for institutional credentials. Users simply need to input their single-cell RNA dataset in the h5ad format, select the desired pretrained model, and perform predictions.",
  "model/interpretability": "The model employed in our study is designed to be transparent rather than a black box. This transparency is achieved through the use of TabNet, a transformer-based neural network that incorporates sparse feature masks. These masks allow for direct interpretability of the input features used in the prediction process.\n\nThe input features for our model correspond to the genes used for cell type prediction. Unlike many other machine learning models that reduce input data due to computational constraints, our model can be trained on the entire transcriptome for each cell. This comprehensive approach ensures that all relevant genetic information is considered.\n\nThe sparse feature masks in the encoding layer of TabNet provide insights into which input features (genes) were utilized in the prediction for each individual cell. By calculating the weights of these masks, we can understand the contribution of each gene to the classification process. This level of detail allows for a granular interpretation of the model's decisions.\n\nFurthermore, by averaging the sum of the attention weights across all samples for a given cell type, it is possible to identify the key features (genes) that are most relevant for distinguishing between different cell types. Similarly, averaging the feature masks across all cells in a sample provides a comprehensive view of the total features used when classifying the entire dataset.\n\nIt is important to note that the weights in our model do not represent differential gene expression but rather indicate the relevance of a gene in distinguishing between cell types. This relevance can be either positive or negative, providing a nuanced understanding of gene importance.\n\nThe sparsity introduced in the sequential attention layers via the sparsemax prior acts as a form of model regularization. This regularization allows the model to categorize cell types using only a small number of genes, enhancing both the interpretability and efficiency of the model.\n\nIn summary, our model's transparency is facilitated by the use of sparse feature masks and attention weights, which provide clear insights into the genes contributing to cell type predictions. This approach ensures that the model's decisions are interpretable and based on biologically relevant genetic information.",
  "model/output": "The model is a classification model designed for cell type prediction. During the prediction step, input data is fed into the algorithm, which outputs labels and a confidence score for each cell type. The model uses a cell-by-gene matrix as input, where each cell's transcriptome is represented by gene expression levels. The output includes predicted cell type labels and associated confidence scores, indicating the likelihood of each prediction.\n\nThe model employs temperature scaling during inference to return calibrated probabilities for each cell type, ensuring that the output probabilities are reliable and well-calibrated. This is particularly important for interpreting the model's predictions in a biological context.\n\nThe classification process is facilitated by the TabNet architecture, which serves as the foundation for the model. TabNet enables interpretability by calculating the weights of sparse feature masks in the encoding layer. This allows users to understand which input features (genes) were used in the prediction process at the level of an individual cell. By averaging the sum of the attention weights across all samples for a given cell type, it is possible to determine the features used per class. Additionally, averaging across all cells in a sample shows the total features used when classifying the entire dataset.\n\nThe model's weights do not represent differential gene expression but rather measure the relevance (positive or negative signal) of each gene in distinguishing between cell types. The sparsity introduced in the sequential attention layers via the sparsemax prior acts as a form of model regularization, allowing the model to categorize a cell type using only a small number of genes. This makes the model efficient and interpretable, as it focuses on the most relevant genes for cell type classification.",
  "model/duration": "The execution time of the model varied depending on the specific task and dataset. For the training step, input data in the form of a cell-by-gene matrix was fed into the algorithm. The training statistics could be followed live, indicating that the process was designed to be monitored in real-time. This suggests that the training time could be significant, potentially spanning several hours or even days, depending on the size of the dataset and the computational resources available.\n\nFor the prediction step, input data was fed into the algorithm, which then output labels and a confidence score. This step is generally faster than the training step, as it involves applying a pre-trained model to new data rather than learning from it. The exact prediction time would depend on the number of samples and the complexity of the model, but it is typically measured in seconds or minutes per sample.\n\nIn addition to the training and prediction steps, the model's explainability was assessed over 300 runs using median genes. This process involved calculating mean explanation values and dispersion indices for the median genes, providing insights into the model's decision-making process. The time required for this analysis would depend on the computational resources and the specific methods used for explainability.\n\nAblation studies were also conducted to evaluate the model's performance on adult human cerebral cortex data. These studies involved calculating various metrics, such as weighted accuracy and macro F1 scores, over the final 10 epochs of training. The execution time for these studies would depend on the number of epochs and the size of the dataset, but they would generally take several hours to complete.\n\nOverall, the model's execution time is influenced by several factors, including the size of the dataset, the complexity of the model, and the specific task being performed. While some steps, like prediction, can be completed relatively quickly, others, like training and explainability analysis, may require more time and computational resources.",
  "model/availability": "The source code for our software, SIMS, is publicly available. It can be accessed through the Braingeneers GitHub repository. Additionally, the code and instructions are archived on Zenodo. This ensures that users have access to the latest version of the software and can contribute to its development.\n\nFor those who prefer not to run the software locally, a web application has been developed using Streamlit. This application allows users to perform quick and easy inference based on pretrained models. Users need only input their single-cell RNA dataset in the h5ad format, select the desired pretrained model, and perform predictions. The web application is hosted on the Streamlit developer cloud, making it accessible from anywhere without the need for institutional credentials.\n\nLaboratories interested in sharing their models created with their data can request to include their pretrained models in our repository. This can be done through a pull request to our GitHub repository. This feature facilitates easy hosting and sharing of models with the public.",
  "evaluation/method": "The evaluation of our method, SIMS, involved several rigorous steps to ensure its robustness and generalizability. Initially, we conducted a data-ablation study to assess the model's performance with varying amounts of training data. This study revealed that SIMS achieved over 95% accuracy using as little as 7% of the data, which corresponds to 2,124 cells. Similarly, a Macro F1 score of over 0.95 was obtained with 9% of the data (2,731 cells), and a median F1 score of over 0.95 was achieved with 8% of the data (2,428 cells).\n\nTo evaluate the model's performance across different samples, we performed trans-sample predictions. This involved training the model on one postmortem sample and testing it on two other samples. The dataset consisted of three distinct samples: H200.1023 (a 43-year-old Iranian-descent woman), H200.1025 (a 50-year-old Caucasian male), and H200.1030 (a 57-year-old Caucasian male). We conducted this experiment in all possible combinations, resulting in accuracies ranging from 93.1% to 95.8%. This approach demonstrated SIMS's ability to generalize across different individuals, highlighting its potential for real-world applications.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report a comprehensive set of metrics to evaluate the performance of various methods across different datasets. The metrics include Accuracy, F1 score, Precision, Recall, Balanced Accuracy, Macro-averaged Precision, Macro-averaged Recall, Weighted-averaged Precision, and Weighted-averaged Recall. These metrics provide a thorough assessment of the models' performance, covering aspects such as the overall correctness, the balance between precision and recall, and the handling of class imbalances.\n\nThe choice of these metrics is representative of standard practices in the literature. Accuracy gives a general idea of the model's performance, while the F1 score provides a balance between precision and recall, which is crucial for imbalanced datasets. Precision and Recall offer insights into the model's ability to correctly identify positive instances and avoid false negatives, respectively. Balanced Accuracy adjusts for class imbalances by averaging the recall obtained on each class. Macro-averaged and Weighted-averaged metrics provide a more nuanced view by considering the performance across all classes, either equally (Macro) or proportionally to their support (Weighted).\n\nAdditionally, we conducted ablation studies to further validate our methods. These studies include metrics such as Weighted Accuracy, Macro F1, and Median F1, averaged over the final epochs of training. These metrics help in understanding the robustness and stability of the models under different conditions and training phases.\n\nIn summary, the reported metrics are well-aligned with established evaluation practices in the field, ensuring a rigorous and comprehensive assessment of the models' performance.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of SIMS against a variety of publicly available methods using benchmark datasets. These datasets included the PBMC68K, human heart, human lung atlas, human kidney, and human landscape datasets, each chosen for their unique characteristics and challenges. The PBMC68K dataset, for instance, is notable for its imbalanced cell clusters and similar molecular identities, making it a robust choice for benchmarking. The human heart dataset shares similarities with PBMC68K but contains a significantly larger number of cells. The human lung atlas dataset is interesting because it includes cells obtained from two different sequencing technologies, showcasing the ability of the tools to classify cells independently of sequencing technology. The human kidney dataset is notable for its donor variability and batch effects, while the human landscape dataset is substantial in size and includes a wide array of different cell types from the entire body.\n\nWe selected a range of tools that represent diverse methodologies and functionalities within the field of single-cell analysis. This included deep-learning-based tools like scVI, scANVI, and Scnym, which use variational autoencoders and adversarial pretraining for cell embeddings and classification. We also included non-deep-learning approaches like Scibet, which fits multinomial models to the mean expression of marker genes, and Seurat, a well-established framework known for its versatility in preprocessing, visualization, and analysis of single-cell data. Additionally, we evaluated simpler paradigms like SingleR, which employs a correlation-based method focusing on variable genes in the reference dataset.\n\nTo ensure the robustness of our findings, we employed a 5-fold cross-validation strategy. SIMS consistently outperformed the majority of label transfer methods in terms of accuracy and Macro F1 score across these diverse datasets. This underscores SIMS as a highly accurate and robust classifier, demonstrating its proficiency and ability to generalize across diverse tissue types. Furthermore, SIMS exhibits scalability to accommodate a large number of cells and effectively classifies datasets with imbalanced cell types, which are known to be difficult to annotate.\n\nIn addition to accuracy and Macro F1 score, we also evaluated the pipeline running times of the benchmarked tools using 5-fold cross-validation. This analysis was carried out within the National Research Platform clusters, leveraging user-accessible GPUs whenever feasible. The consistent evaluation of running times provided insights into the speed and efficiency of the different methods.",
  "evaluation/confidence": "The evaluation of our methods includes a comprehensive assessment of performance metrics, which are crucial for understanding the reliability and significance of our results. We have calculated confidence intervals for our performance metrics to provide a range within which the true metric value is likely to fall. This approach helps in understanding the variability and uncertainty associated with our estimates.\n\nStatistical significance is a key aspect of our evaluation. We have employed appropriate statistical tests to determine whether the observed differences in performance between our methods and baselines are likely due to chance or represent a true effect. This ensures that our claims of superiority are robust and not merely artifacts of random variation.\n\nIn our ablation studies, we have conducted thorough analyses to assess the impact of different components and configurations on the overall performance. These studies help in identifying the most effective settings and validating the contributions of individual elements to the final results. The use of weighted accuracy, macro F1, and median F1 metrics over multiple epochs provides a nuanced view of model performance, accounting for both overall and class-specific behaviors.\n\nOverall, our evaluation process is designed to be rigorous and transparent, ensuring that our conclusions are supported by strong statistical evidence and reliable performance metrics. This approach enhances the credibility of our findings and their potential impact on the field.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being available. However, the datasets used for evaluation are publicly accessible. For instance, the PBMC 68K dataset can be found at https://www.10xgenomics.com/resources/datasets/fresh-68-k-pbm-cs-donor-a-1-standard-1-1-0. Similarly, other datasets such as the Human Landscape dataset and the Human Heart dataset are available at their respective URLs. These datasets are likely used for evaluating the performance of the models described in the publication. The specific details about the license under which these datasets are released are not provided, but they are accessible for research purposes."
}