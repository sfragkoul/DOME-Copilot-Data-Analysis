{
  "publication/title": "Smartwatches Can Detect Walker and Cane Use in Older Adults.",
  "publication/authors": "Antos SA, Danilovich MK, Eisenstein AR, Gordon KE, Kording KP",
  "publication/journal": "Innovation in aging",
  "publication/year": "2019",
  "publication/pmid": "31025002",
  "publication/pmcid": "PMC6476414",
  "publication/doi": "10.1093/geroni/igz008",
  "publication/tags": "- Assistive Technology\n- Wearables\n- Accelerometer\n- Falls\n- Function/Mobility\n- Machine Learning\n- Activity Recognition\n- Smartwatch\n- Smartphone\n- Older Adults",
  "dataset/provenance": "The dataset used in this study was collected from fourteen older adults who met the inclusion criteria. These participants were either walker users or cane users. The data collection involved participants wearing a smartphone and a smartwatch while performing three specific tests: the six-minute walk, the 10-meter walk, and the Timed Up and Go tests. These tests were conducted with and without the use of their assistive devices over five separate days. The smartphone used was an Android Nexus 5, recording tri-axial accelerometer and gyroscope data at approximately 50 Hz. The smartwatch, an Actigraph wGT3X-BT, also recorded tri-axial accelerometer data at 50 Hz. The data collection included sensor measurements throughout the entire session, including rest breaks. The data was then processed offline, with features extracted from non-overlapping 3-second clips of the sensor signals. The features calculated included minimum, maximum, mean, standard deviation, skew, kurtosis, and interquartile range for each axis (x, y, z) and the resultant signal per sensor. This resulted in 56 features for the smartphone classifiers and 28 features for the smartwatch classifiers. The dataset was used to train machine-learning classifiers to detect whether a participant was walking with or without their assistive device. The performance of these classifiers was evaluated using four types of cross-validation to mimic different real-world use cases.",
  "dataset/splits": "In our study, we employed four distinct types of cross-validation to evaluate the performance of our classifiers, each mimicking different real-world scenarios.\n\nFirst, we conducted user-specific, within-day cross-validation. For this, we trained classifiers using data from all five days for a single participant and tested them on the same five days. We utilized fivefold cross-validation, dividing the dataset into five equal parts, training on four, and testing on the fifth, averaging the results over the five possible divisions. This approach demonstrated the classifier's ability to detect assistive device use for the same participant on the same day.\n\nSecond, we performed user-specific, across-day cross-validation with an 80/20 train/test split. Here, classifiers were trained using data from four days for one participant and tested on the remaining day. This method showed the classifier's capability to detect assistive device use for the same participant on different days.\n\nThird, we executed user-specific, across-day cross-validation with a 20/80 train/test split. In this case, classifiers were trained using data from just one day for a participant and tested on the remaining four days. This test was crucial as it indicated that small amounts of user-specific training data could be sufficient.\n\nLastly, we implemented user-generic, across-user cross-validation. For this, classifiers were trained using data from all but one participant and then tested on the remaining participant. This process was repeated until a user-generic classifier was evaluated on every participant. This method demonstrated the classifier's ability to detect assistive device use for new individuals, mimicking an off-the-shelf system with no additional calibration or training required.\n\nIn summary, our dataset was split into four distinct cross-validation types, each with its own training and testing distributions, to thoroughly evaluate the classifiers' performance in various scenarios.",
  "dataset/redundancy": "The datasets were split using four types of cross-validation to evaluate classifier performance under different scenarios. These splits ensured that the training and test sets were independent, mimicking various real-world use cases.\n\nFor user-specific, within-day cross-validation, data from all five days for one participant was used for training, and the same five days were used for testing. This was done using fivefold cross-validation, where the dataset was divided into five equal parts, with the classifier trained on four parts and tested on the fifth, repeating this process for all possible divisions.\n\nIn user-specific, across-day cross-validation with an 80/20 train/test split, data from four days for one participant was used for training, and the remaining day was used for testing. Conversely, in the 20/80 split, data from one day was used for training, and the remaining four days were used for testing. These splits ensured that the classifier's ability to generalize across different days was evaluated.\n\nFor user-generic, across-user cross-validation, data from all but one participant was used for training, and the remaining participant's data was used for testing. This process was repeated for each participant, ensuring that the classifier's performance on new, unseen users was assessed.\n\nThe distribution of the datasets compared favorably to previously published machine learning datasets in activity recognition studies. Enough data was collected to have at least hundreds, and often thousands, of samples to train the classifiers. This approach is typical for activity recognition studies, ensuring robust and meaningful machine-learning analysis.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and commonly used in the field. These include random forests, support vector machines, na\u00efve Bayes, logistic regression, and linear discriminant analysis. These algorithms are not new and have been extensively studied and applied in various domains, including activity recognition and classification tasks.\n\nThe choice of these algorithms was driven by their proven effectiveness in handling similar problems. They were selected to evaluate their performance in detecting assistive device use based on sensor data collected from smartphones and smartwatches. The study aimed to compare the accuracy and reliability of these algorithms under different cross-validation scenarios to determine their suitability for real-world applications.\n\nThe decision to use these established algorithms rather than developing a new one was based on the need for robust and validated methods. The focus of the study was on the application of machine learning to a specific problem\u2014detecting assistive device use\u2014rather than on innovating new algorithms. Therefore, publishing in a machine-learning journal was not the primary goal. Instead, the findings were presented in a journal that aligns with the study's focus on aging and assistive technologies.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data collected from the smartphone and smartwatch sensors was pre-processed and encoded for the machine-learning algorithms as follows:\n\nParticipants performed several walking tests, including the six-minute walk, 10-meter walk, and Timed Up and Go tests, both with and without their assistive devices. The sensor data from these tests was annotated using recorded timestamps. This data was then linearly interpolated to a consistent sampling rate of 30 Hz. The interpolated signals were divided into non-overlapping 3-second clips. Clips that contained multiple labels were removed to ensure clear and distinct data segments.\n\nFor each 3-second clip, a set of features was calculated. These features included statistical measures such as minimum, maximum, mean, standard deviation, skew, kurtosis, and interquartile range. These features were computed for the x, y, z axes, and the resultant signal for each sensor. The smartphone provided accelerometer and gyroscope data, resulting in 56 features, while the smartwatch provided accelerometer data, resulting in 28 features.\n\nThe features were then scaled to have a mean of 0 and unit variance for most classifiers, except for the support vector machine, which used linear scaling to the range 0\u20131. This scaling was applied to the training set to ensure that the data was normalized and ready for input into the machine-learning algorithms.",
  "optimization/parameters": "In our study, we utilized a set of features derived from sensor data to train our machine-learning classifiers. Specifically, for each sensor, we calculated a variety of statistical features from the accelerometer data. These features included minimum, maximum, mean, standard deviation, skew, kurtosis, and interquartile range, each computed for the x, y, z axes, and the resultant signal. This resulted in 28 features per sensor.\n\nFor the smartphone, which included both accelerometer and gyroscope data, we had a total of 56 features. For the smartwatch, which only included accelerometer data, we had 28 features.\n\nThe selection of these features was based on common practices in activity recognition studies, aiming to capture the essential characteristics of the sensor data that could differentiate between walking with and without an assistive device. We did not perform an exhaustive search for the optimal set of features but rather relied on a predefined set that has been shown to be effective in similar applications. The choice of features was also influenced by the need to balance computational efficiency and model performance.",
  "optimization/features": "In our study, we utilized a set of features derived from the sensor data to train our machine-learning classifiers. Specifically, we calculated eight different statistical features for each axis (x, y, z) and the resultant signal of the sensors. These features included the minimum, maximum, mean, standard deviation, skew, kurtosis, and interquartile range. For the smartphone classifiers, which used both accelerometer and gyroscope data, this resulted in a total of 56 features. For the smartwatch classifiers, which used only accelerometer data, there were 28 features.\n\nFeature selection was not explicitly mentioned as a separate step in our methodology. However, the features were calculated and used as they were, without any indication of a feature selection process. The features were scaled appropriately for each classifier type, ensuring that they were either mean-centered with unit variance or linearly scaled to the range 0\u20131. This scaling was performed on the training set only, adhering to best practices in machine learning to prevent data leakage.",
  "optimization/fitting": "The fitting method employed in this study involved training machine-learning classifiers using data from both smartphones and smartwatches. The classifiers tested included random forests, support vector machines, na\u00efve Bayes, logistic regression, and linear discriminant analysis. Each classifier's hyper-parameters were chosen through grid searches with across-user cross-validation on a different activity tracking dataset. This approach helped in selecting the optimal parameters for each classifier, ensuring that the models were neither overfitting nor underfitting the data.\n\nTo address the potential issue of overfitting, especially given the large number of features extracted from the sensor data, several measures were taken. First, the use of cross-validation, particularly user-generic, across-user cross-validation, ensured that the classifiers were evaluated on data from participants not included in the training set. This helped in assessing the generalizability of the models. Additionally, the classifiers were trained and tested on a substantial amount of data, with at least hundreds and often thousands of samples, which is typical for activity recognition studies. This large dataset helped in mitigating overfitting by providing a robust training set.\n\nUnderfitting was addressed by using a variety of state-of-the-art classifiers, each with its own strengths. The random forest classifiers, for instance, were found to be the most accurate, but similar trends in performance were observed across all classifiers. This consistency indicated that the models were complex enough to capture the underlying patterns in the data without being too simplistic. Furthermore, the features used for classification were carefully selected and calculated for each sensor, ensuring that the models had access to relevant information.\n\nIn summary, the fitting method involved a rigorous process of hyper-parameter tuning, extensive cross-validation, and the use of diverse and robust classifiers. These steps collectively ensured that the models were neither overfitting nor underfitting the data, leading to reliable and accurate predictions of assistive device use.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our classifiers. One of the primary methods used was cross-validation, which involved splitting the data into training and test sets in various ways to evaluate the classifier's performance. We utilized four types of cross-validation: user-specific within-day, user-specific across-day (80/20 and 20/80 train/test splits), and user-generic across-user. These methods helped to assess how well our classifiers generalized to different scenarios and participants.\n\nAdditionally, we performed feature scaling as part of our data preprocessing steps. Features were scaled to have a mean of 0 and unit variance for most classifiers, or linearly scaled to the range 0\u20131 for support vector machines. This standardization helped to ensure that no single feature dominated the learning process, thereby reducing the risk of overfitting.\n\nWe also conducted hyperparameter tuning through grid searches with across-user cross-validation on a different activity tracking dataset. This process helped to find the optimal settings for our classifiers, further enhancing their generalization capabilities.\n\nMoreover, we compared the performance of multiple classifiers, including random forests, support vector machines, na\u00efve Bayes, logistic regression, and linear discriminant analysis. By evaluating different algorithms, we could select the most robust and accurate model for our specific task.\n\nLastly, we collected a substantial amount of data, ensuring that we had hundreds to thousands of samples for training our classifiers. This abundance of data helped to mitigate overfitting by providing a more comprehensive representation of the underlying patterns in the data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, we mentioned that hyper-parameters were chosen through grid searches with across-user cross-validation on a different activity tracking data set. This process ensures that the selected hyper-parameters are optimized for the task at hand.\n\nRegarding the availability of model files, these are not explicitly provided in the publication. However, the methods and procedures used to train and evaluate the classifiers are thoroughly described, allowing for reproducibility. The classifiers tested included random forests, support vector machines, na\u00efve Bayes, logistic regression, and linear discriminant analysis. The features used for classification were also detailed, including statistical measures like minimum, maximum, mean, standard deviation, skew, kurtosis, and interquartile range, calculated for each axis (x, y, z) and the resultant signal per sensor.\n\nThe data preprocessing steps, such as annotating sensor signals, interpolating to 30 Hz, separating into 3-second clips, and calculating features, are all clearly outlined. This information, along with the description of the classification algorithms and their evaluation through various types of cross-validation, provides a comprehensive guide for reproducing the results.\n\nThe code and specific model files are not directly available through the publication, but the detailed methodology and parameters ensure that interested researchers can implement similar models. The use of Python with the scikit-learn library for training and testing the classifiers is also specified, which further aids in reproducibility.",
  "model/interpretability": "The model employed in this study is primarily a black-box model, as it relies on complex machine-learning algorithms such as random forests, support vector machines, and logistic regression. These algorithms are known for their ability to capture intricate patterns in data but often lack interpretability, making it difficult to understand the exact decision-making process.\n\nHowever, there are aspects of the model that can be considered somewhat transparent. For instance, the features extracted from the accelerometer and gyroscope data are well-defined and can be interpreted in the context of human movement. Features such as mean, standard deviation, and frequency domain measures provide insights into the characteristics of the walking patterns with and without assistive devices.\n\nAdditionally, the use of cross-validation techniques provides a clear framework for evaluating the model's performance. The different types of cross-validation\u2014user-specific within-day, user-specific across-day, and user-generic across-user\u2014offer a structured way to understand how the model generalizes to different scenarios. This transparency in evaluation helps in interpreting the model's robustness and reliability across various use cases.\n\nMoreover, the comparison between smartphone and smartwatch classifiers highlights the importance of sensor placement and data quality. The superior performance of smartwatch classifiers can be attributed to the consistent placement of the device on the wrist, which reduces variability in data collection. This insight into the data collection process adds a layer of interpretability to the model's performance.\n\nIn summary, while the core machine-learning algorithms are black-box models, the study incorporates several transparent elements, such as well-defined features and structured evaluation methods, which enhance the interpretability of the results.",
  "model/output": "The model is a classification model. It was designed to detect whether participants were using an assistive device while walking. The classifiers were trained to distinguish between walking with an assistive device and walking without one. Additionally, the model was extended to include nonwalking activities, such as sitting, standing, and transitions between these states, which were categorized under a single class named \"Not Walking.\" The performance of the classifiers was evaluated using various types of cross-validation, and the results were presented in the form of confusion matrices, which are typical for classification tasks.\n\nThe classifiers tested included random forests, support vector machines, na\u00efve Bayes, logistic regression, and linear discriminant analysis. These are all common machine-learning algorithms used for classification tasks. The features used for classification were extracted from sensor data collected from a smartphone and a smartwatch, including accelerometer and gyroscope data. The features calculated for each sensor included minimum, maximum, mean, standard deviation, skew, kurtosis, and interquartile range, among others.\n\nThe evaluation of the classifiers showed that smartwatch classifiers generally outperformed smartphone classifiers across different types of cross-validation. The classifiers were also evaluated for their ability to detect assistive device use in the presence of nonwalking activities, and the results indicated that the smartwatch classifiers maintained high accuracy even when including these additional activities. However, there was a small, statistically significant decrease in classification accuracy for user-specific classifiers when nonwalking activities were included. The combined use of smartphone and smartwatch data further improved the performance of the classifiers in most cases.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our classifiers was conducted using four distinct types of cross-validation, each designed to mimic different real-world scenarios. These methods included user-specific, within-day cross-validation, user-specific, across-day cross-validation with both 80/20 and 20/80 train/test splits, and user-generic, across-user cross-validation.\n\nIn the user-specific, within-day cross-validation, classifiers were trained using data from all five days for a single participant and tested on the same participant's data from the same days. This approach involved fivefold cross-validation, where the dataset was divided into five equal parts, with the classifier trained on four parts and tested on the fifth, averaging results over the five possible divisions. This method aimed to demonstrate the classifier's ability to detect assistive device use for the same participant on the same day, resembling a scenario where the user calibrates the system daily.\n\nFor the user-specific, across-day cross-validation with an 80/20 train/test split, classifiers were trained using data from four days for a single participant and tested on the remaining day. This evaluated the classifier's performance in detecting assistive device use for the same participant across different days, similar to a scenario where a patient calibrates the system in a supervised setting before use.\n\nThe user-specific, across-day cross-validation with a 20/80 train/test split involved training classifiers using data from just one day for a participant and testing on the remaining four days. This method assessed whether small amounts of user-specific training data are sufficient, mimicking a scenario where a patient undergoes a single day of training in a clinic or research lab.\n\nLastly, the user-generic, across-user cross-validation trained classifiers using data from all but one participant and tested on the remaining participant. This process was repeated for each participant to evaluate the classifier's ability to detect assistive device use for new individuals, simulating an off-the-shelf system requiring no additional calibration or training.\n\nAdditionally, we extended our classifiers to include non-walking activities such as sitting, standing, sit-to-stand, and stand-to-sit, merged into a single class named \"Not Walking.\" Data for this class was collected from participant rest breaks between walking trials. We then reevaluated our classifiers using the same four types of cross-validation.\n\nWe also combined data from smartphones and smartwatches to test whether using both sensors improved classifier performance. Statistical analyses were performed to compare the classification accuracy of smartphones and smartwatches across all types of cross-validation, reporting median accuracies and using Wilcoxon signed-ranked tests for comparisons due to non-normally distributed data.",
  "evaluation/measure": "In our evaluation, we primarily focused on classification accuracy as our key performance metric. This metric is widely used in the literature for evaluating activity recognition systems, making it a representative choice for our study.\n\nWe reported median accuracies across different types of cross-validation to provide a robust measure of classifier performance. This approach is particularly useful given the non-normal distribution of our data, as indicated by the Shapiro\u2013Wilk test.\n\nTo compare the performance of different classifiers, we employed Wilcoxon signed-ranked tests. This statistical method is appropriate for our data, which did not follow a normal distribution. The significance level was set at \u03b1 = 0.05, ensuring that our findings are statistically reliable.\n\nIn addition to accuracy, we also presented our results in the form of confusion matrices. These matrices offer a detailed view of classifier performance, showing not just the overall accuracy but also the specific types of errors made by the classifiers. This is particularly important when dealing with class imbalances, as in our case when we included nonwalking activities.\n\nFor the comparison between smartphone and smartwatch classifiers, we used the Mann\u2013Whitney U test. This non-parametric test is suitable for comparing differences between two independent groups, such as the accuracy of smartphone versus smartwatch classifiers.\n\nOverall, our set of performance metrics is comprehensive and aligns with established practices in the field of activity recognition. The use of median accuracies, confusion matrices, and appropriate statistical tests ensures that our evaluation is both rigorous and representative of the current literature.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on evaluating the performance of our classifiers using different types of cross-validation to mimic various real-world scenarios. These scenarios included user-specific, within-day cross-validation, user-specific, across-day cross-validation with different train/test splits, and user-generic, across-user cross-validation.\n\nWe did, however, compare the performance of classifiers using data from different sensors. Specifically, we evaluated the classification accuracy of smartphone-based classifiers against smartwatch-based classifiers across all types of cross-validation. This comparison allowed us to determine which sensor provided more accurate predictions for detecting assistive device use.\n\nAdditionally, we extended our analysis to include nonwalking activities, such as sitting, standing, and transitions between these states. This extension helped us assess the robustness of our classifiers in more realistic, everyday scenarios. We found that smartwatch classifiers generally outperformed smartphone classifiers, especially when including nonwalking activities.\n\nFurthermore, we combined data from both smartphone and smartwatch sensors to see if using multiple sensors improved classifier performance. This approach often led to better results, particularly in scenarios where the amount of training data was limited.\n\nIn summary, while we did not compare our methods to publicly available benchmarks, we conducted thorough comparisons between different sensor types and combinations, as well as different cross-validation strategies, to ensure the robustness and generalizability of our classifiers.",
  "evaluation/confidence": "The evaluation of our classifiers involved a comprehensive statistical analysis to ensure the robustness and reliability of our results. We reported median accuracies for all comparisons, as the data were not normally distributed. To assess the statistical significance of our findings, we employed Wilcoxon signed-ranked tests, with a significance level set at \u03b1 = 0.05. This approach allowed us to determine whether the differences in classification performance between the smartphone and smartwatch were statistically significant.\n\nFor all types of cross-validation, the smartwatch classifiers demonstrated superior performance compared to the smartphone classifiers, with p-values less than 0.01 for all comparisons. This indicates that the smartwatch classifiers are significantly more accurate than the smartphone classifiers across various use-case scenarios.\n\nThe confidence in our results is further supported by the consistency of our findings across different types of cross-validation. Whether evaluating within-day, across-day, or across-user performance, the smartwatch classifiers maintained high accuracy, whereas the smartphone classifiers showed more variability and generally lower performance.\n\nAdditionally, we ensured that our training data were sufficient for meaningful machine-learning analysis. We collected enough data to have at least hundreds, and often thousands, of samples to train our classifiers, which is typical for activity recognition studies. This extensive dataset allowed us to make reliable comparisons and draw confident conclusions about the performance of our classifiers.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study involved collecting sensor data from participants performing specific tests with and without assistive devices. This data was used to train and evaluate machine-learning classifiers. The evaluation process included various types of cross-validation to assess the classifiers' performance under different scenarios. However, the specific raw evaluation files generated during this process have not been made publicly accessible. Therefore, researchers or individuals interested in replicating or building upon this work would need to conduct their own data collection following the methods described in the study."
}