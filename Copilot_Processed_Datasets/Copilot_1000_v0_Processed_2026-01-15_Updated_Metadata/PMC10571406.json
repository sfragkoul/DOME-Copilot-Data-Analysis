{
  "publication/title": "Deep learning for automated left ventricular outflow tract diameter measurements in 2D echocardiography.",
  "publication/authors": "Zha SZ, Rogstadkjernet M, Kl\u00e6boe LG, Skulstad H, Singstad BJ, Gilbert A, Edvardsen T, Samset E, Brekke PH",
  "publication/journal": "Cardiovascular ultrasound",
  "publication/year": "2023",
  "publication/pmid": "37833731",
  "publication/pmcid": "PMC10571406",
  "publication/doi": "10.1186/s12947-023-00317-5",
  "publication/tags": "- Cardiovascular Ultrasound\n- Deep Learning\n- Echocardiography\n- LVOTd Measurement\n- PLAX View\n- ZPLAX View\n- Data Quality Labels\n- Model Training\n- Statistical Analysis\n- Clinical Validation",
  "dataset/provenance": "The dataset used in this study was sourced from a clinical database of transthoracic echocardiograms. Specifically, the data was collected from patients with coronary artery disease who were admitted to a university hospital between January and December 2018. The dataset consists of 1304 LVOTd (Left Ventricular Outflow Tract Diameter) measurements acquired from 649 unique patient examinations. The echocardiographic examinations were performed using GE HealthCare Vivid E95 ultrasound devices, adhering to recommended guidelines. The dataset includes measurements from both the parasternal long axis (PLAX) and zoomed parasternal long axis (ZPLAX) views. The data was anonymized, with only age and gender details available for the patients. The age of the patients averaged 65.1 years, with a standard deviation of 12.5 years, and 70% of the patients were male. The dataset was subjected to quality control by an experienced cardiologist, ensuring high standards for both image quality and the accuracy of cursor placements for the measured LVOTd. This dataset has not been used in previous publications by the community.",
  "dataset/splits": "The dataset was divided into three splits: training, validation, and testing. The training set comprised 68% of the data, the validation set 17%, and the testing set 15%. Since many patients had more than one LVOTd measurement, the data partitioning was done on a patient-wise basis to prevent overlaps between the datasets. This approach ensures that the model is trained and evaluated on distinct patient data, enhancing the robustness and generalizability of the results. The specific distributions of data points in each split are detailed in the supplementary tables provided with the publication.",
  "dataset/redundancy": "The dataset used in this study was derived from transthoracic echocardiograms of 656 patients with coronary artery disease. After excluding 7 patients due to measurement issues, 649 patients remained, with a total of 1304 LVOTd measurements. The dataset was split into training, validation, and testing sets with a 68%, 17%, and 15% distribution, respectively. This split was done on a patient-wise basis to ensure that data from the same patient did not appear in more than one set, maintaining the independence of the training and test sets. This approach prevents data leakage and ensures that the model's performance is evaluated on truly unseen data.\n\nThe distribution of the dataset in terms of echocardiographic views was 569 PLAX and 735 ZPLAX LVOTd measurements. Each LVOTd measurement included an echocardiographic still frame and two LVOTd coordinates, which were rescaled to a resolution of 256 \u00d7 256 pixels for standardization. Spatial geometry data was also extracted to allow conversion of predicted LVOTd coordinates from pixel units to metric units.\n\nCompared to previously published machine learning datasets in echocardiography, our dataset is of moderate size but benefits from high-quality annotations and a standardized acquisition protocol. The use of a single type of ultrasound device (GE HealthCare Vivid E95) ensures consistency in image quality, although it may limit the generalizability of the findings to other devices. The dataset's homogeneity due to repeated measurements from the same examinations is a notable characteristic, which was addressed by including a diverse range of patients and ensuring that the model's performance was evaluated across different quality labels.",
  "dataset/availability": "The dataset underlying this article cannot be shared publicly due to data privacy policies from the country of origin. The data privacy policies of the country where the data was collected prevent public sharing. This means that the data, including the specific data splits used for training, validation, and testing, are not released in any public forum. The enforcement of this policy is handled through the legal and regulatory frameworks of the country, which mandate the protection of patient data and privacy. However, data from the analysis of the results and the underlying source code are available from the corresponding author upon reasonable request. This approach ensures that the research can be verified and replicated while adhering to the necessary privacy regulations.",
  "optimization/algorithm": "The optimization algorithm employed in our study is based on a deep learning (DL) approach, specifically utilizing a U-Net architecture with an EfficientNet-B2 encoder. This combination is not a novel algorithm but rather an established framework in the field of medical image analysis. The U-Net architecture is widely recognized for its effectiveness in segmentation tasks, while EfficientNet-B2 provides a robust feature extraction capability.\n\nThe choice of this architecture was driven by its proven performance in similar tasks, particularly in medical imaging where precise localization and segmentation are crucial. The use of an open-source PyTorch implementation of U-Net with EfficientNet-B2 ensures reproducibility and leverages the extensive research and development already conducted in this area.\n\nThe decision to use this well-established architecture in a cardiovascular context, rather than a machine-learning journal, is due to the specific application and the need to demonstrate its utility in clinical settings. The focus of our work is on the practical application of DL in echocardiography, showcasing how these models can be integrated into clinical workflows to improve the accuracy and efficiency of measurements such as the left ventricular outflow tract diameter (LVOTd).\n\nThe optimization process involved a grid search to determine the best model parameters, resulting in a learning rate of 0.003, a batch size of 32, and 30 training epochs. Adaptive Moment Estimation (Adam) was chosen as the optimizer due to its effectiveness in handling sparse gradients, which is common in image-based tasks. Various loss functions and data configurations were experimented with during development, with the final model incorporating \"Low\" quality data to enhance performance, as observed during 5-fold validation.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It is a standalone deep learning implementation based on a U-Net architecture with an EfficientNet-B2 encoder. The model is trained to predict the left ventricular outflow tract diameter (LVOTd) coordinates directly from echocardiographic images. It does not use data from other machine-learning algorithms as input.\n\nThe training process involves image augmentation and model pre-training using weights from models trained on the ImageNet dataset. This approach is common for enhancing the performance of deep learning models with limited datasets. The model's performance is evaluated using metrics such as the mean pointwise Euclidean distance (ED) between the predicted and ground truth LVOTd coordinates, as well as the LVOTd length.\n\nThe training data consists of echocardiographic images that have undergone manual quality assessment by an experienced cardiologist. Each data pair is rated for image quality and the accuracy of cursor placements for the measured LVOTd. This ensures that the training data is of high quality and independent, focusing on the visual quality of the echocardiographic image and the precision of the ground truth LVOTd coordinates.\n\nIn summary, the model is a direct deep learning predictor and not a meta-predictor. It relies on a single, well-defined architecture and training process, with a clear emphasis on data quality and independence.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure standardization and quality. Echocardiographic still frames and left ventricular outflow tract diameter (LVOTd) coordinates were extracted for each measurement. These coordinates served as the ground truth for the model. All extracted images were rescaled to a resolution of 256 \u00d7 256 pixels to maintain consistency across the dataset. Since LVOTd measurements were acquired at different zoom levels, spatial geometry data was also extracted to allow conversion of predicted coordinates from pixel units to metric units.\n\nThe dataset was split into training, validation, and testing sets in a patient-wise manner to prevent overlaps. This split ensured that each patient's data appeared in only one of the sets, maintaining the independence of the datasets. The distribution of the data in these sets is detailed in supplementary tables.\n\nPrior to training, the dataset underwent manual quality assessment by an experienced cardiologist. Each data pair was rated for image quality and the accuracy of cursor placements for the measured LVOTd, with ratings of \"High,\" \"Medium,\" or \"Low.\" Data quality label distributions for the training and test sets are provided in supplementary matrices. Measurements with a \"Low\" quality rating in either category were initially removed to establish a solid baseline for model development.\n\nImage augmentation techniques were employed to extend the dataset. These augmentations included rotations, shifting, aspect ratio modification, cropping, blurring, noise addition, exposure and brightness adjustments, and magnification and demagnification. Constraints were applied to ensure that the augmented images remained clinically plausible. For model pre-training, weights from models trained on the public ImageNet dataset were used for initialization. This approach leveraged pre-trained weights to improve the model's performance, especially given the limited size of the dataset.",
  "optimization/parameters": "The model utilized an open-source PyTorch implementation of a U-Net with an EfficientNet-B2 encoder. The selection of model parameters was determined through a grid search process. This process resulted in the following key parameters:\n\n* A learning rate of 0.003\n* A batch size of 32\n* 30 training epochs\n* The use of Adaptive Moment Estimation (Adam) as the optimizer\n\nThe grid search was employed to systematically identify the optimal combination of these parameters, ensuring robust performance during the model's development. Additionally, various loss functions and data configurations were experimented with during the development phase, with detailed results provided in the supplementary materials. The final model included \"Low\" quality data in its training, as this approach demonstrated better performance during 5-fold validation.",
  "optimization/features": "The input features for the deep learning model consist of echocardiographic still frames and corresponding LVOTd coordinates. Each LVOTd measurement includes an echocardiographic still frame and two LVOTd coordinates, which are the points where the measurement cursors are placed by the clinical operator. These coordinates serve as the ground truth for the model's predictions.\n\nThe dataset was standardized by rescaling all extracted echocardiographic still frames and LVOTd coordinates to a resolution of 256 \u00d7 256 pixels. This standardization ensures consistency in the input data, which is crucial for the model's training and evaluation.\n\nFeature selection was not explicitly performed in the traditional sense, as the input features are directly derived from the echocardiographic images and the LVOTd coordinates. However, data quality control was conducted to ensure that only high-quality data was used for training. This involved manual quality assessment by an experienced cardiologist, who rated each data pair in terms of image quality and the accuracy of cursor placements. Data pairs with \"Low\" quality ratings were initially removed to establish a solid baseline, but they were later included in the training of the final model to improve performance.\n\nThe dataset was split into training, validation, and testing sets on a patient-wise basis to prevent overlaps between the datasets. This splitting ensures that the model's performance is evaluated on independent data, which is essential for assessing its generalizability. The distribution of the dataset is detailed in the supplementary information, providing transparency on how the data was partitioned.\n\nNot applicable",
  "optimization/fitting": "The deep learning model employed in this study utilized a U-Net architecture with an EfficientNet-B2 encoder, which is known for its efficiency and effectiveness in handling medical imaging tasks. The model parameters were determined through a grid search, resulting in a learning rate of 0.003, a batch size of 32, and 30 training epochs. Adaptive Moment Estimation (Adam) was used as the optimizer, which is well-suited for large datasets and high-dimensional parameter spaces.\n\nTo address the potential issue of overfitting, given the relatively large number of parameters compared to the number of training points, several strategies were implemented. First, 5-fold cross-validation was employed during the development phase to ensure consistent evaluation and to mitigate the risk of overfitting. This technique helps in assessing the model's performance on different subsets of the data, providing a more robust estimate of its generalization capability.\n\nAdditionally, image augmentation techniques were applied to artificially increase the diversity of the training dataset. These augmentations included rotations, shifting, aspect ratio modification, cropping, blurring, noise addition, modifications of exposure and brightness, and magnification and de-magnification. These augmentations helped in making the model more robust to variations in the input data, thereby reducing the likelihood of overfitting.\n\nFurthermore, the inclusion of \"Low\" quality data in the training of the final model was found to improve performance during 5-fold validation. This suggests that the model was able to learn from a wider range of data, enhancing its ability to generalize to new, unseen data.\n\nTo rule out underfitting, the model's performance was evaluated on both the training and validation sets. The use of a coordinate regression loss function, which has shown effectiveness for point location tasks in echocardiography, ensured that the model was capable of learning the relevant features from the data. The inclusion of pre-trained ImageNet weights for model initialization also provided a good starting point, allowing the model to converge more quickly and effectively.\n\nOverall, the combination of cross-validation, image augmentation, and the use of pre-trained weights helped in balancing the model's capacity to learn from the data without overfitting or underfitting. The model's performance was thoroughly evaluated using various statistical measures, including Bland-Altman plots and Pearson correlation coefficients, to ensure its reliability and accuracy in predicting LVOTd coordinates.",
  "optimization/regularization": "In our study, several regularization methods were employed to prevent overfitting and enhance the generalization of our deep learning model. One key technique used was data augmentation, which involved applying random transformations to the echocardiographic images. These transformations included rotations, shifting, aspect ratio modification, cropping, blurring, noise addition, and adjustments to exposure, brightness, magnification, and de-magnification. By enforcing constraints on the range of these modifications, we ensured that the augmented images remained clinically plausible.\n\nAdditionally, we utilized model pre-training as a regularization technique. The weights from models trained on the public ImageNet dataset were used for initialization. This approach leveraged the features learned from a large and diverse dataset, providing a robust starting point for our model.\n\nFurthermore, we employed 5-fold cross-validation during the development phase. This method involved splitting the dataset into five subsets, training the model on four subsets, and validating it on the remaining subset. This process was repeated five times, with each subset serving as the validation set once. This technique helped in assessing the model's performance more reliably and in reducing the risk of overfitting.\n\nThe inclusion of \"Low\" quality data in the training of the final model was also a strategic decision. Although such data was initially excluded to establish a solid baseline, its inclusion during the final training phase improved the model's performance, as observed during 5-fold validation. This approach helped the model to generalize better to a wider range of data qualities.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we employed a learning rate of 0.003, a batch size of 32, and trained the model for 30 epochs using the Adaptive Moment Estimation (Adam) optimizer. These parameters were determined through a grid search to ensure optimal performance.\n\nThe model implementation was based on an open-source PyTorch version of a U-Net with an EfficientNet-B2 encoder. The loss function utilized was based on coordinate regression, which has proven effective for point location tasks in echocardiography. Additionally, image augmentation techniques were applied to extend the dataset, including rotations, shifting, aspect ratio modifications, cropping, blurring, noise addition, and adjustments to exposure, brightness, and magnification. These augmentations were performed using an in-house Sci-kit Image implementation, with constraints to maintain clinical plausibility.\n\nFor model pre-training, weights from models trained on the public ImageNet dataset were used for initialization. This approach leveraged pre-trained weights to enhance the model's performance, especially given the limited size of our dataset.\n\nThe dataset underlying this article cannot be shared publicly due to data privacy policies. However, data from the analysis of the results and the underlying source code are available from the corresponding author upon reasonable request. This ensures that the configurations and parameters used in our study can be replicated and verified by other researchers.\n\nThe supplementary information provides additional details on the experiments conducted, including the effect of data quantity and alternate network architectures. These details are available in the supplementary tables and figures, which can be accessed through the provided link. The supplementary material includes tables on the distribution of patients and echocardiographic views, quality label distributions, and the results of 5-fold validation for different data configurations and loss functions.",
  "model/interpretability": "The model implemented in this study is designed with interpretability in mind, addressing the need for transparency in clinical applications. Unlike a \"black-box\" implementation that directly predicts the left ventricular outflow tract diameter (LVOTd), this model predicts the LVOTd coordinates. This approach mimics the current clinical workflow, making it more interpretable for validation purposes.\n\nThe model's interpretability is further enhanced by the use of probability maps generated by the final layer of the deep learning (DL) model. These maps provide visual insights into how the DL predictions are performed. For predictions sampled from the 50th percentile, the areas with high probability centralize around a distribution that follows the LVOT walls. This visualization helps in understanding the model's decision-making process.\n\nHowever, there are differences between the DL model's reasoning and clinical reasoning. For instance, some probability distributions extend distally past the hinge points of the aortic valve. This could be due to the model's emphasis on the mass-center of the probability distributions or the lack of valvular leaflet movements, which makes it challenging to pinpoint the aortic hinge points accurately.\n\nTo improve the model's predictions, incorporating contextual information such as consecutive frames from an echocardiographic cine-loop or anatomical constraints could be beneficial. This would provide the model with more data to make more accurate and clinically relevant predictions.",
  "model/output": "The model is a regression model. It is designed to predict the coordinates of the left ventricular outflow tract diameter (LVOTd) rather than classifying images. The use of a coordinate regression loss function during training further supports this, as it is tailored for point location tasks. The model outputs probability maps that highlight areas of interest, aiding in the spatial visualization of the predicted LVOTd coordinates. These maps are crucial for understanding how the model makes its predictions, ensuring that the outputs are interpretable and clinically relevant. The final output is the LVOTd length, derived from the predicted coordinates, which is essential for clinical measurements and assessments.",
  "model/duration": "The execution time of the model was not explicitly detailed in the publication. However, it is worth noting that the model was trained using a batch size of 32 over 30 epochs. The use of image augmentation and model pre-training with weights from the ImageNet dataset were employed to enhance the training process. These factors, along with the specific hardware used, would influence the overall training time. Additionally, the model's ability to provide near-instantaneous predictions suggests that the inference time is minimal, which is beneficial for clinical applications where quick results are essential.",
  "model/availability": "The source code for the deep learning model is not publicly available due to data privacy policies. However, the underlying source code and data from the analysis of the results can be made available from the corresponding author upon reasonable request. This approach ensures that the methodology and findings can be verified and replicated by other researchers while adhering to the necessary privacy regulations.",
  "evaluation/method": "The evaluation of the deep learning (DL) model for measuring the left ventricular outflow tract diameter (LVOTd) involved several rigorous steps to ensure its accuracy and reliability. Initially, a 5-fold cross-validation was employed during the development phase to provide a consistent evaluation of the model's performance. This method helps in assessing how the model generalizes to an independent dataset.\n\nThe model parameters were determined using a grid search, which resulted in specific settings such as a learning rate of 0.003, a batch size of 32, and 30 training epochs. The optimizer used was Adaptive Moment Estimation (Adam). Various loss functions and data configurations were experimented with, and the details from the 5-fold validation are provided in supplementary tables.\n\nTo minimize the influence of poor-quality data, all LVOTd measurements with a \"Low\" quality rating in either image quality or LVOTd cursor placements were removed during the initial development phase. However, for the final DL model, \"Low\" quality data was included in the training process as it showed better performance during the 5-fold validation.\n\nThe evaluation also involved calculating the Euclidean distance (ED) between the predicted LVOTd coordinates and the ground truth coordinates. This was visualized in relation to the ground truth and DL predicted LVOTd coordinates. The mean pointwise ED was derived from the ED of the superior and inferior LVOTd coordinates.\n\nStatistical analysis was performed on the test set, which was presented in its entirety and according to specific subgroupings such as PLAX, ZPLAX, and exclusion of \"Low\" quality data. Means with 95% confidence intervals (CI) and medians with interquartile ranges (IQR) were calculated for absolute and relative LVOTd errors. All statistical values were calculated on LVOTd dimensions after conversion from pixels to millimeters for direct clinical interpretability.\n\nBland-Altman plots were used to visualize trends between the clinical and DL predicted LVOTd measurements. Correlation plots with the calculation of the Pearson coefficient were also performed. T-tests were used to evaluate differences in precision between the clinicians and the DL model. The statistical measures were computed using StataSE 16.\n\nThe examination scan-rescan variability was also evaluated, with repeated LVOTd measurements performed on still frames from unique echocardiographic cine-loops acquired by the same operator. This method shares similarities with studies evaluating scan-rescan variability but differs in that the repeated measurements are performed within the same echocardiographic examination. The two LVOTd measurements with the largest difference were included for patients exceeding two repeated measurements to adjust for possible difficulties during the examination.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the deep learning (DL) model's predictions of the left ventricular outflow tract diameter (LVOTd). The primary metric reported is the mean absolute LVOTd error, which quantifies the average difference between the DL-predicted and clinical reference LVOTd measurements. This metric was reported for the entire test set as well as for specific subgroups, such as PLAX and ZPLAX views, and after excluding low-quality data.\n\nAdditionally, we calculated the median absolute LVOTd error to provide a measure less influenced by outliers. The mean and median relative LVOTd errors were also reported, offering a percentage-based assessment of the prediction accuracy relative to the true LVOTd dimensions.\n\nTo visualize the agreement between the DL predictions and clinical references, Bland-Altman plots were used. These plots helped identify any systematic bias or trends in the errors, such as underestimation of smaller LVOTd dimensions and overestimation of larger ones. The limits of agreement provided a range within which most differences between the methods lie.\n\nCorrelation plots, along with the Pearson correlation coefficient, were employed to assess the linear relationship between the DL-predicted and clinical reference LVOTd measurements. A significant correlation was found, indicating that the DL model's predictions align well with clinical measurements.\n\nFor evaluating the precision and reproducibility of the DL model, we calculated the coefficient of variation for both the DL predictions and clinical measurements. This metric was reported for different data groupings, including all data, PLAX data, ZPLAX data, and after removing low-quality data. The results showed that the DL model had comparable precision to clinicians, with some variations depending on the view type and data quality.\n\nFurthermore, we assessed the examination scan-rescan variability, which measures the consistency of LVOTd measurements within the same echocardiographic examination. This metric is crucial for understanding the DL model's robustness against operator variations and is reported for different data subgroups.\n\nThese performance metrics collectively provide a comprehensive evaluation of the DL model's accuracy, precision, and reproducibility in predicting LVOTd measurements. The set of metrics is representative of those commonly used in the literature for evaluating DL models in medical imaging, ensuring that our results can be compared and contextualized within the broader field of research.",
  "evaluation/comparison": "Not applicable. The study focused on developing and evaluating a deep learning model for measuring the left ventricular outflow tract diameter (LVOTd) in echocardiographic images. The evaluation primarily compared the deep learning model's performance against clinical experts' measurements, rather than comparing it to publicly available methods or simpler baselines on benchmark datasets. The study did not explicitly mention the use of benchmark datasets or simpler baselines for comparison. Instead, it concentrated on assessing the model's precision, accuracy, and reproducibility in clinical settings, using metrics such as Euclidean distance, Bland-Altman plots, and Pearson correlation coefficients. The evaluation also included an analysis of the model's performance across different image qualities and echocardiographic views, providing a comprehensive assessment of its clinical utility.",
  "evaluation/confidence": "The evaluation of our deep learning (DL) model for measuring the left ventricular outflow tract diameter (LVOTd) includes several performance metrics with associated confidence intervals, ensuring a robust assessment of the model's performance.\n\nFor the mean absolute LVOTd error, we reported a value of 1.04 mm with a 95% confidence interval (CI) ranging from 0.90 to 1.19 mm. This interval provides a clear indication of the precision of our estimate. Similarly, when excluding low-quality data, the mean absolute LVOTd error was 0.87 mm with a 95% CI from 0.74 to 1.00 mm. These intervals help in understanding the reliability of our model's predictions.\n\nIn terms of statistical significance, we employed Bland-Altman plots to visualize the agreement between the DL predicted and clinical reference LVOTd measurements. The limits of agreement were determined to be from -2.87 to 2.83 mm, which provides a range within which most differences between the methods lie. Additionally, correlation plots showed a significant correlation between the DL predictions and clinical measurements, with a Pearson coefficient of 0.80 (p < 0.001). This strong correlation indicates that our model's predictions are statistically significant and closely align with clinical measurements.\n\nWe also conducted t-tests to evaluate differences in precision between clinicians and the DL model. The p-values from these tests help in determining whether the observed differences are statistically significant. For example, when comparing the precision of the DL model and clinicians for patients with exactly three repeated LVOTd measurements, the p-values were provided to assess the significance of any observed differences.\n\nFurthermore, we used 5-fold cross-validation during the development phase to ensure consistent evaluation and to determine the optimal model parameters. This method helps in assessing the model's performance across different subsets of the data, providing a more reliable estimate of its generalizability.\n\nIn summary, our evaluation includes confidence intervals for key performance metrics and employs statistical tests to ensure that the results are robust and significant. This comprehensive approach allows us to confidently claim that our DL model performs well and is comparable to clinical measurements.",
  "evaluation/availability": "The raw evaluation files are not publicly available due to data privacy policies from the country of origin. However, data from the analysis of the results and the underlying source code are accessible. These can be obtained from the corresponding author upon reasonable request. This approach ensures that sensitive patient information remains protected while still allowing for transparency and potential replication of the study's findings."
}