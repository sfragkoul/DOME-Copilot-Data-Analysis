{
  "publication/title": "Exploring convolutional neural networks and spatial video for on-the-ground mapping in informal settlements.",
  "publication/authors": "Ajayakumar J, Curtis AJ, Rouzier V, Pape JW, Bempah S, Alam MT, Alam MM, Rashid MH, Ali A, Morris JG",
  "publication/journal": "International journal of health geographics",
  "publication/year": "2021",
  "publication/pmid": "33494756",
  "publication/pmcid": "PMC7831241",
  "publication/doi": "10.1186/s12942-021-00259-z",
  "publication/tags": "- Convolutional Neural Networks\n- Spatial Video\n- Health Risk Mapping\n- Informal Settlements\n- Machine Learning\n- Environmental Risks\n- Disease Prevention\n- Data Collection\n- Public Health\n- Geographic Data\n- Model Training\n- Image Classification\n- Health Geographics\n- Developing World\n- Disease Related Environmental Risks",
  "dataset/provenance": "The dataset utilized in this study was sourced from street-level imagery collected using Street View (SV) in multiple informal settlements in Haiti over several years. The imagery was gathered using cameras mounted on vehicles, traversing specific neighborhoods at different times. The selected neighborhoods included A in 2017 (two different cameras on the same route), 2018, and 2019; B in 2019; C in 2017; D in 2018 and 2019, and E in 2018.\n\nThe total number of images and objects for each category is detailed in the study. For instance, the category \"Drain\" had 98 images and 98 objects. Similarly, \"Trash\" had 67 images and 84 objects, and \"Muddy Water\" had 74 images and 86 objects. The category \"Water Point\" initially had 59 images and 61 objects, but an additional set was added due to a lack of images containing pipes and taps, bringing the total to 94 images and 104 objects. Other categories like \"Water Bucket,\" \"Tire,\" and additional categories were also included in the dataset.\n\nThis dataset is unique in its focus on informal settlements, where the environment tends to be unplanned and haphazard, leading to a lack of image clarity typically found in more developed areas. The imagery reflects the variability and challenges inherent in these settings, including overlaps between categories and considerable variation within each category.\n\nPrevious work has utilized high-resolution neighborhood imagery from sources like Google Street View (GSV) for various analyses, such as localizing and classifying waste, evaluating built environment characteristics, and extracting physical disorder. However, the specific type of data available for informal settlements, as used in this study, is scarce and presents unique challenges due to the dynamic and often unpredictable nature of these environments. The dataset amassed through this project provides a valuable resource for exploring model training and improving image classification in such settings.",
  "dataset/splits": "The dataset was split into two main parts: training and validation. The training to validation ratio was set to 10:1. This decision was made due to the relatively low number of images available for the object detection task, which generally requires well above 1000 images for a single class.\n\nThe total number of images and objects for each category is detailed in a table. For instance, the category \"Drain\" has 98 images and 98 objects in the training set. Similarly, \"Trash\" has 67 images and 84 objects, \"Muddy Water\" has 74 images and 86 objects, \"Water Bucket\" has 49 images and 96 objects, \"Tire\" has 55 images and 88 objects, and \"Water Point\" has 59 images and 61 objects in the training set. Additionally, \"Water Point\" has an extra set of 94 images and 104 objects added due to a lack of images containing pipes and taps.\n\nThe split was done randomly to ensure a diverse representation of images within each category. This random split helps in evaluating the model's performance on unseen data, which is crucial for assessing its generalization capabilities. The variation within each category, both in terms of image type and size, including overlaps between categories, was also considered during the split. For example, images that could be labeled as both a drain and a stream were included to reflect the real-world complexity of informal settlements.",
  "dataset/redundancy": "The datasets were split using a 10:1 training to validation ratio. This decision was made due to the relatively low number of images available for the object detection task, which typically requires well over 1000 images for a single class. The split was performed randomly to ensure independence between the training and test sets. This random splitting helped to mitigate any potential biases that might arise from a non-random selection process.\n\nThe images used for training exhibited considerable variation within each category, both in terms of image type and size. This variation included overlaps between categories, making the classification task more challenging. For instance, images that could be labeled as a drain varied significantly, including different sizes and types of drains, as well as engineered drains with distinct concrete sides and more naturally occurring channels. Similarly, water points varied widely in type, necessitating a second round of image extraction to include more taps and pipes.\n\nThis \"fuzziness\" is characteristic of informal settlements, where the environment tends to be unplanned and haphazard, leading to a lack of image clarity commonly found in more developed settings. The training hyper-parameters and other details, such as image resolution, were carefully documented to ensure reproducibility and transparency in the methodology.\n\nThe performance of the model improved significantly after adding a new set of sample images with taps and pipes to the training set, particularly for the water point category. This highlights the importance of localized nuances in model training. The model's performance was evaluated using various quality parameters, including precision, recall, mean average precision (mAP), and F-score, which is the harmonic mean of precision and recall.\n\nThe datasets used in this study are unique in their focus on informal settlements, which are notoriously data-poor environments. This contrasts with many previously published machine learning datasets, which often rely on more structured and well-documented data sources. The use of street view (SV) imagery in this context provides a valuable resource for identifying environmental health risks, but it also presents challenges due to the variability and complexity of the data. The next steps involve determining the geographic variation in local training needs and merging image recognition with associated coordinates to automatically map health risks, thereby reducing human effort and enhancing the sustainability of the mapping process.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is convolutional neural networks (CNNs), specifically the YOLOv3 architecture. This algorithm is not entirely new, as it builds upon previous versions of YOLO (You Only Look Once) and other object detection algorithms like R-CNN and its variants. YOLOv3 is an improvement over its predecessors, offering better accuracy and efficiency due to its additional convolution layers and unified approach to target localization and object detection.\n\nThe reason this algorithm was not published in a machine-learning journal is that our primary focus is on applying and evaluating this method for a specific real-world problem, rather than developing a new algorithm. Our study concentrates on using YOLOv3 for automated risk feature extraction in informal settlements, leveraging street-level imagery to identify environmental health risks. The innovation lies in the application of this algorithm to this particular domain and the insights gained from examining its effectiveness in these challenging environments.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It is a standalone machine learning model designed for object detection and classification in environmental health risk assessment using street view (SV) imagery. The model is trained using a dataset of labeled images, with various parameters optimized for performance.\n\nThe training algorithm utilizes specific initialization parameters, including learning rate, image size, mini-batch size, total epochs, momentum, initial learning rate, final learning rate, and weight decay. These parameters are set to ensure effective training and convergence of the model.\n\nThe model is trained to identify and classify several environmental features, such as drains, trash, muddy water, water points, water buckets, tires, and animals. The performance of the model is evaluated using metrics like F-score and mean average precision (mAP), which provide insights into the classification accuracy and detection capabilities of the model.\n\nThe model's accuracy is tested across different rounds and image resolutions, with adjustments made to improve performance. For instance, adding a new set of sample images with taps and pipes to the training set significantly improved the accuracy for water points. Similarly, varying the image resolution from 224 to 1280 pixels enhanced the accuracy for models trained to classify drains, trash, muddy water, and water points.\n\nThe model's runtime statistics indicate that the prediction time for a single image frame remains consistent across different image resolutions, which is crucial for processing large volumes of SV data efficiently. This consistency allows for the stacking of multiple frames into a single packet for running predictions, facilitating the automatic mapping of environmental health risks.\n\nIn summary, the model is a robust object detection and classification system tailored for environmental health risk assessment using SV imagery. It does not rely on data from other machine-learning algorithms as input, and its training data is independent, ensuring reliable and accurate predictions.",
  "optimization/encoding": "For the machine-learning algorithm, the data encoding and preprocessing involved several key steps. Initially, a bespoke software called Frame Selector was developed to extract single frames from the street view (SV) image archive. These frames were selected based on user preferences and environmental category types. Each frame was associated with a specific time, allowing for the extraction of corresponding frames from the source video.\n\nThe extracted frames were then labeled using the Object Detection Client software. This labeling process involved annotating each frame with five values: the center coordinates (x, y), the width (w), and height (h) of the bounding box for the object (all normalized to values between 0 and 1), and the class to which the object belonged. These annotations were crucial for training the model to recognize and classify different environmental features.\n\nThe labeled data, along with the bounding box dimensions, were stored in a JavaScript Object Notation (JSON) file. This file facilitated further retrieval and processing of the image data. Additionally, a frame \"packet\" was generated, which included all the image frames, corresponding labels, separate text files indicating the images used for training and validation, and a configuration file. This packet was then sent to the training server for processing, and the resulting model file was saved in a common repository.\n\nThe training images used for labeling included various environmental features such as trash, muddy water, drains, and water points. These images were selected to ensure that the model could accurately identify and classify objects associated with health risks. The training process involved using sum-squared error in the output as the optimization procedure, with the loss function combining errors on bounding box prediction, object prediction, and class prediction.\n\nAnchor boxes, which are a priori bounding boxes, were generated using a k-means algorithm applied to the height and width of the training set of bounding boxes. This approach helped the network predict appropriately sized bounding boxes, thereby speeding up the training process and reducing detection errors.\n\nThe images used for training were resized to 608 pixels, and a mini-batch size of 1 was employed. The training algorithm utilized a learning rate of 0.002, with a momentum of 0.97 and a weight decay of 0.0004. The total number of epochs for training was set to 273. These parameters were carefully chosen to optimize the model's performance in identifying and classifying environmental health risks from the street view images.",
  "optimization/parameters": "In our study, we utilized several key parameters for training our object detection model. The primary parameters included the learning rate, image size, mini-batch size, total epochs, momentum for stochastic gradient descent (SGD), initial learning rate, final learning rate, and weight decay.\n\nThe learning rate was set to 0.002 initially and adjusted to -0.04 as the training progressed. The image size was standardized at 608 pixels, which was chosen to balance between computational efficiency and the detail required for accurate object detection. The mini-batch size was set to 1, and the total number of epochs was 273. Momentum for SGD was set to 0.97 to help accelerate gradients vectors in the right directions, thus leading to faster converging. Weight decay was set to 0.0004 to prevent overfitting by adding a penalty for large weights.\n\nThese parameters were selected based on empirical testing and common practices in the field of object detection. The learning rate and weight decay were fine-tuned to ensure the model converged efficiently without overfitting. The image size was chosen to match the resolution of the training images, which were extracted from videos captured in informal settlements. The number of epochs was determined to provide sufficient training iterations while avoiding overfitting. The momentum value was selected to enhance the stability and speed of convergence during training.",
  "optimization/features": "The input features for our model are derived from images captured in informal settlements, focusing on specific categories relevant to health risks. These categories include trash, muddy water, drains, and water points, among others. Each image is labeled and used to train our object detection model.\n\nThe images used for training vary in type and size, with considerable variation within each category. For instance, drains can range from engineered structures with concrete sides to more natural channels. Water points also exhibit significant diversity, including taps and pipes, which required an additional round of image extraction to ensure comprehensive representation.\n\nThe training set consists of a total of 343 images across various categories, with the number of images per category ranging from 49 to 98. The images were split into training and validation sets with a ratio of 10:1, reflecting the need for a larger training dataset due to the relatively low number of images available for each category.\n\nFeature selection was not explicitly performed in the traditional sense, as the features are inherently defined by the pixel data of the images and the bounding boxes drawn around the objects of interest. The labeling process involves drawing rectangular bounding boxes around risk features, such as drains, and labeling them from a set of dropdown options. This process ensures that the relevant features are highlighted and used for training the model.\n\nThe training algorithm parameters, such as learning rate, image size, mini-batch size, and total epochs, were carefully selected to optimize the model's performance. The initial learning rate was set to 0.002, with a final learning rate of -0.04, and a weight decay of 0.0004. These parameters were chosen to balance the trade-off between convergence speed and model stability.\n\nIn summary, the input features consist of labeled images categorized into specific risk features. The training set was split into training and validation sets with a 10:1 ratio, and the model was trained using carefully selected hyperparameters to ensure optimal performance. Feature selection was inherently managed through the labeling process, focusing on the relevant risk features in the images.",
  "optimization/fitting": "The fitting method employed in this study involved training an object detection model using the YOLOv3 architecture. The number of parameters in the model is indeed much larger than the number of training points, as is typical in deep learning models. To address the potential issue of overfitting, several strategies were implemented.\n\nFirstly, the training to validation ratio was set to 10:1, which helped in ensuring that the model generalizes well to unseen data. Additionally, techniques such as anchor boxes generated using a k-means algorithm were used to improve the model's ability to predict appropriately sized bounding boxes, thereby enhancing its performance on the validation set.\n\nThe model was trained using a sum-squared error loss function, which combines errors on bounding box prediction, object prediction, and class prediction. This approach helps in balancing the learning process and prevents the model from becoming too specialized to the training data.\n\nTo further mitigate overfitting, various quality parameters such as precision, recall, mean average precision (mAP), and F-score were captured during training. These metrics provided a comprehensive evaluation of the model's performance and helped in identifying any signs of overfitting.\n\nMoreover, the training process involved multiple rounds of testing with varying image resolutions and frame frequencies. This iterative approach allowed for fine-tuning the model and ensuring that it performs well across different conditions, thereby reducing the risk of overfitting.\n\nIn terms of underfitting, the model's performance was closely monitored using the aforementioned metrics. The F-score, which is the harmonic mean of precision and recall, provided a clear indication of the model's classification accuracy. The mAP, which considers the Intersection over Union (IoU) criteria, offered a deeper understanding of the model's detection capabilities. By analyzing these metrics, it was possible to ensure that the model was not underfitting the data.\n\nOverall, the fitting method involved a careful balance of techniques to prevent both overfitting and underfitting, resulting in a robust object detection model.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and improve the generalization of our object detection model. One key method involved the use of anchor boxes, which are predefined bounding boxes generated using a k-means algorithm applied to the height and width of the training set of bounding boxes. These anchor boxes help the network to predict appropriately sized bounding boxes, thereby enhancing the model's ability to generalize to new, unseen data.\n\nAdditionally, we utilized a sum-squared error in the output as the optimization procedure. The loss function is a combination of errors on the bounding box prediction, object prediction, and class prediction. This multi-faceted loss function ensures that the model learns to balance accuracy in object localization, object detection, and class classification, reducing the likelihood of overfitting to any single aspect.\n\nFurthermore, we implemented a frame frequency parameter and a frame window concept during testing. The frame frequency parameter allowed us to extract frames from the video at different intervals, reducing processing time and potentially mitigating overfitting by ensuring the model does not rely too heavily on specific frames. The frame window concept involved selecting all frames within an interval window for object detection, which helped in capturing the temporal context and improving the model's robustness.\n\nWe also conducted multiple rounds of testing with varying image resolutions, including 224px, 416px, 608px, 832px, 1024px, and 1280px. This approach helped us understand the impact of image resolution on prediction accuracy and ensured that the model could perform well across different image qualities, further enhancing its generalization capabilities.\n\nLastly, we maintained a training to validation ratio of 10:1, which, although lower than typical object detection tasks, was necessary due to the limited number of images available. This ratio, combined with random splitting, helped in evaluating the model's performance on unseen data and preventing overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are reported in detail. Specifically, the initialization parameters for the training algorithm, including learning rate, image size, mini-batch size, total epochs, momentum, initial learning rate, final learning rate, and weight decay, are provided in a table. This table serves as a comprehensive guide to the settings used during the training process.\n\nThe model files and optimization parameters are not explicitly mentioned as being available for download or access. However, the software tools developed for this project, such as the Frame Selector and Object Detection Client, are mentioned as being available for download. These tools facilitate the extraction and labeling of images, which are crucial steps in the training pipeline.\n\nRegarding the license, there is no specific information provided about the licensing terms for the software tools or the model files. It is assumed that the tools can be downloaded and used as per standard academic sharing practices, but specific licensing details are not available.\n\nIn summary, while the hyper-parameter configurations and optimization schedule are thoroughly documented, the availability of model files and optimization parameters is not explicitly stated. The software tools developed for the project are available for download, but licensing details are not provided.",
  "model/interpretability": "The model employed in our study is not a black box; it provides several layers of interpretability. One key aspect of its transparency is the use of bounding boxes for object detection. These bounding boxes visually indicate the predicted locations of various objects within an image, making it clear which parts of the image the model is focusing on. For instance, when identifying a drain, a rectangular bounding box is drawn around the drain, and it is labeled accordingly. This visual feedback helps in understanding the model's decision-making process.\n\nAdditionally, the model's performance metrics, such as F-score and mean average precision (mAP), offer insights into its classification accuracy and detection capabilities. The F-score provides a balance between precision and recall, indicating how well the model identifies true positives while minimizing false positives and false negatives. The mAP, which considers the Intersection over Union (IoU) criteria, ensures that the bounding boxes predicted by the model are not only correct in terms of labels but also accurate in terms of location.\n\nThe model's training and testing processes are also transparent. We have detailed the initialization parameters, including learning rate, image size, mini-batch size, and total epochs, which allow for reproducibility and understanding of the training algorithm. Furthermore, the accuracy testing results for different categories, such as drains, trash, muddy water, water points, water buckets, tires, and animals, are thoroughly documented. This includes the impact of adding new sample images and changing detection resolutions on the model's accuracy.\n\nIn summary, the model's use of bounding boxes, detailed performance metrics, and transparent training and testing processes make it interpretable. These features allow users to understand how the model makes predictions and to assess its reliability in identifying various environmental features.",
  "model/output": "The model is designed for classification tasks, specifically object detection within images. It identifies and classifies various health risk categories such as drains, trash, muddy water, water points, water buckets, and tires. The model's performance is evaluated using metrics like F-score, mean average precision (mAP), and accuracy, which are typical for classification problems. The F-score, for instance, is the harmonic mean of precision and recall, providing a single metric that balances both concerns. The model's output includes bounding boxes around detected objects, along with their corresponding labels, indicating the class of the detected object. This classification is crucial for assessing environmental health risks in informal settlements. The model's accuracy varies across different categories, with some achieving near-perfect scores after extensive training and optimization. For example, the drain classifier reached an accuracy of 97% in the second round of testing. The model's performance improvements, such as increased accuracy after adding more diverse training images, highlight its effectiveness in classifying objects within complex and varied environments.",
  "model/duration": "The execution time of the model varied depending on several factors, including image resolution and the number of frames processed. During the accuracy testing, it was observed that the prediction time for a single image frame remained almost constant across different image resolutions. This consistency is crucial because the total number of frames that can be stacked together to form a single packet for running predictions varies with image resolution. Lower resolution images allow for more frames to be processed in a single packet, which can help in reducing the overall processing time. However, the trade-off between image resolution and processing time is balanced by the need for accurate object detection. Higher resolutions generally improve detection accuracy but increase the processing time per frame. The runtime statistics indicated that while the prediction time per frame was stable, the total runtime could be influenced by the frame stacking size and the image resolution used. These factors were carefully considered to optimize the model's performance in real-world applications.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method for our study involved a multi-round testing process to assess the effectiveness of machine learning models on street-level imagery collected via street view (SV). Initially, videos were manually classified for environmental risks to verify the predictive model's accuracy. The evaluation dataset consisted of 12 SVs, chosen to represent diverse neighborhoods, time periods, and image qualities. These videos were captured using various cameras, including extreme sports and body cameras, resulting in a mix of image resolutions and angles.\n\nIn the first round of testing, images were set at a resolution of 608px, with a frame frequency parameter of 10 frames. This meant that every 10th frame was used for prediction. The results were analyzed in packets of 100 images to check for positive matches with the labeled risks.\n\nTo understand the impact of skipping image frames on object detection, a second round of testing introduced a frame window concept. This involved selecting all frames within a specified interval around a timestamp where an object was detected. For instance, with a frame window size of 60, all 30 frames before and after the timestamp were used for object detection.\n\nThe third round of testing focused on the impact of image resolution on prediction accuracy. Various resolutions, including 224px, 416px, 832px, 1024px, and 1280px, were tested. Images that remained unclassified in the first two rounds were used in this phase. Additionally, performance measures such as total program runtime, variation in performance with frame stack size, and changes in runtime and memory utilization with different image resolutions were noted.\n\nThe evaluation also included assessing the prediction accuracy for different environmental features like drains, trash, muddy water, water points, water buckets, tires, and animals. The results showed varying levels of accuracy across these features, highlighting the importance of localized nuance in model training. For example, adding sample images with taps and pipes to the training set significantly improved the prediction accuracy for water points.\n\nOverall, the evaluation method demonstrated that an SV-machine learning approach is viable for identifying environmental health risks in challenging environments. The findings suggest that future work, including reattaching labeled video frames to their GPS coordinates, could enable automatic mapping of dynamic environments.",
  "evaluation/measure": "In our evaluation, we employed several key performance metrics to assess the effectiveness of our models. The primary metrics reported include the F-score, mean average precision (mAP), and accuracy.\n\nThe F-score, which is the harmonic mean of precision and recall, provides a comprehensive measure of a model's accuracy by balancing both false positives and false negatives. This metric is particularly useful for understanding the classification performance across different categories.\n\nMean average precision (mAP) offers a more nuanced evaluation by considering the Intersection over Union (IoU) criteria. IoU determines whether the bounding box for detected objects is correctly predicted, adding an additional layer of accuracy to the assessment. A prediction is considered a \"match\" only if the label is correct and the IoU exceeds a specified threshold, typically 0.5.\n\nAccuracy is reported as the percentage of correctly identified objects out of the total objects in the test set. This metric gives a straightforward measure of how well the model performs in real-world scenarios.\n\nThese metrics are representative of standard practices in the literature for evaluating object detection models. They provide a robust framework for assessing both the classification and localization performance of our models, ensuring that our results are comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we focused on leveraging advanced object detection algorithms to identify environmental health risks in informal settlements. Specifically, we utilized YOLOv3, a state-of-the-art method known for its efficiency and accuracy in object detection tasks. YOLOv3 unifies the target localization and object detection into a single regression problem, making it faster compared to two-step processes like R-CNN and its variants.\n\nWe did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our approach was tailored to the unique challenges presented by informal settlements, where the environment is often unplanned and haphazard. This context requires a nuanced understanding of the local conditions, which is why we relied on a extensive library of granular environmental imagery collected from Street View (SV) over multiple environments and time periods.\n\nRegarding simpler baselines, our focus was on optimizing the performance of YOLOv3 rather than comparing it to simpler models. The complexity of the task, involving the identification of various health risks such as drains, trash, muddy water, water points, water buckets, and tires, necessitated the use of a robust and efficient algorithm. The YOLOv3 architecture, with its Darknet-53 backbone for feature extraction, proved to be well-suited for this purpose.\n\nOur evaluation involved rigorous testing and optimization of the model, including adjustments to image resolution, frame frequency, and the addition of new training images to improve accuracy. For instance, adding a set of 35 images with pipe-based water points significantly enhanced the accuracy of the water point classifier. Similarly, varying the detection resolution led to improvements in the accuracy of models trained to classify drains, trash, muddy water, and water points.\n\nIn summary, while we did not conduct a direct comparison with publicly available methods or simpler baselines, our approach was thoroughly evaluated and optimized to address the specific challenges of identifying environmental health risks in informal settlements. The use of YOLOv3, along with extensive testing and optimization, ensured that our model was both efficient and accurate in its predictions.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}