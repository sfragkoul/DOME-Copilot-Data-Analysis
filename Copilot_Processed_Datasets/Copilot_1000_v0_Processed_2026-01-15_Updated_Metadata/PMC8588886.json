{
  "publication/title": "Generative Adversarial Network Image Synthesis Method for Skin Lesion Generation and Classification.",
  "publication/authors": "Mutepfe F, Kalejahi BK, Meshgini S, Danishvar S",
  "publication/journal": "Journal of medical signals and sensors",
  "publication/year": "2021",
  "publication/pmid": "34820296",
  "publication/pmcid": "PMC8588886",
  "publication/doi": "10.4103/jmss.jmss_53_20",
  "publication/tags": "- DCGAN\n- Dermoscopy\n- Pretraining\n- Skin lesion\n- Generative Adversarial Network\n- Image Synthesis\n- Skin Cancer Classification\n- Deep Learning\n- Medical Imaging\n- Machine Learning",
  "dataset/provenance": "The dataset used in this study was obtained from Kaggle and is specifically referred to as the Kaggle Skin Cancer Dataset. This dataset is notable for having an equal number of samples for both malignant and benign classes, which is crucial for balanced training and evaluation of machine learning models. The dataset comprises a variety of skin lesion images, which were utilized to train and test the deep convolutional generative adversarial network (DCGAN) developed in this research. The balanced nature of the dataset helps in mitigating biases and ensures that the model can generalize well across different types of skin lesions. The images in the dataset underwent preprocessing techniques such as gamma correction and bilateral filtering to enhance their quality and remove noise, thereby improving the model's performance. This preprocessing step is essential for accurate feature detection and extraction during the training process. The dataset's structure and the preprocessing techniques applied to it played a significant role in achieving the high accuracy levels reported in the study.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is Generative Adversarial Networks (GANs), specifically a Deep Convolutional Generative Adversarial Network (DCGAN). This class of algorithms is well-established in the field of machine learning and has been widely used for various applications, including image generation and classification.\n\nThe DCGAN algorithm used in our work is not entirely new. It builds upon the existing DCGAN architecture, which has been previously explored in the literature. The novelty of our work lies in the application of DCGAN for skin lesion generation and classification, particularly in the context of dermatology and cancer detection. We incorporated effective image filtering and enhancement algorithms, such as bilateral filters, to improve feature detection and extraction during training. Additionally, we fine-tuned several hyperparameters, including the learning rate and momentum for the Adam optimization algorithm, to address instability issues related to GAN models.\n\nRegarding the publication venue, our focus was on the application of DCGAN in the medical field, specifically for skin lesion classification. The Journal of Medical Signals & Sensors is a suitable platform for presenting our findings, as it caters to an audience interested in medical signal processing and sensor technologies. While the core DCGAN algorithm is well-known in the machine-learning community, its application in dermatology and cancer detection is less explored. Therefore, publishing in a medical journal allows us to reach a relevant audience and highlight the practical implications of our work in the medical field.",
  "optimization/meta": "Not applicable. The model described in the publication is a Deep Convolutional Generative Adversarial Network (DCGAN) designed for skin lesion generation and classification. It does not function as a meta-predictor that uses data from other machine-learning algorithms as input. The DCGAN consists of a generator and a discriminator, both of which are neural networks trained simultaneously in an adversarial process. The generator creates synthetic images, while the discriminator evaluates their authenticity. The training data for this model is independent and consists of skin lesion images used to train the generator to produce realistic images and the discriminator to distinguish between real and fake images. The focus is on optimizing the performance of these two components within the GAN framework rather than integrating outputs from other machine-learning methods.",
  "optimization/encoding": "The data encoding and preprocessing for our machine-learning algorithm involved several key steps to ensure optimal performance. Initially, we collected a dataset consisting of 3,597 skin cancer images from Kaggle, which included two classes: benign and malignant. These images underwent preprocessing to enhance feature detection and extraction. We employed effective image filtering and enhancement algorithms, such as the bilateral filter, to improve the quality of the images. This step was crucial for ensuring that the model could accurately identify and classify skin lesions.\n\nThe images were resized to fit the input size requirements of our Deep Convolutional Generative Adversarial Network (DCGAN) framework. Specifically, we set the output size to 64 \u00d7 64 pixels, which was the ultimate outcome expected from our generator. The deconvolution step in the generator had a movement step of 2, and each output layer augmented fourfold compared to the input, resulting in output sizes of 32 \u00d7 32, 16 \u00d7 16, 8 \u00d7 8, and 4 \u00d7 4 accordingly.\n\nWe also incorporated data augmentation techniques to increase the diversity of our training dataset. This involved applying various transformations such as rotations, flips, and zooms to the images. These augmentations helped the model generalize better and reduce overfitting.\n\nThe noise vector used for generating images was taken from a standard normal distribution. This noise was fed into the generator to produce synthetic images that mimicked real skin lesion images. The generator and discriminator were trained simultaneously, with the generator aiming to produce images that could fool the discriminator, and the discriminator aiming to correctly classify real and fake images.\n\nDuring training, we used the sigmoid activation function and computed the loss value using the `tf.nn.sigmoid_cross_entropy_with_logits` function from the TensorFlow library. The Adam optimizer was employed with a learning rate of 0.0002 and a momentum of 0.5 to balance the loss between the generator and discriminator. This optimization algorithm was chosen for its ability to handle non-convex optimization characteristics, which are common in deep learning models.\n\nOverall, the data encoding and preprocessing steps were designed to enhance the quality and diversity of the input images, ensuring that the DCGAN could effectively learn to generate and classify skin lesions.",
  "optimization/parameters": "In our study, the model architecture was designed with specific input parameters to ensure optimal performance. The generator was constructed to meet the requirements of the DCGAN framework, with the OUTPUT_SIZE fixed at 64, aiming for a final output dimension of 64 \u00d7 64. The deconvolution step size was set to 2, allowing each output layer to quadruple in size compared to the input, resulting in intermediate layer sizes of 32 \u00d7 32, 16 \u00d7 16, 8 \u00d7 8, and 4 \u00d7 4. The BATCH_SIZE and the number of feature maps in the generator were set to 64, with feature maps decreasing progressively through the layers: 512, 256, 128, and 64.\n\nThe discriminator, a feed-forward neural network, consisted of five layers, including an input and output layer, and three dense layers, without spatial pooling layers. The convolution kernel's moving step was set to 2, reducing the output size to a quarter of the original after each convolution. The feature maps in the discriminator increased progressively through the layers: 64, 128, 256, and 512.\n\nThe selection of these parameters was driven by extensive experimentation and tuning. Various setups were tested, and different components such as hyperparameters, loss calculators, optimizers, learning rate, and activators were adjusted to enhance understanding and achieve desirable results. This iterative process involved observing the results at each stage, ensuring that the model's performance was optimized. The final configuration of the generator and discriminator was chosen based on the best-performing set of parameters identified during this tuning process.",
  "optimization/features": "Not applicable.",
  "optimization/fitting": "In our study, we employed a Deep Convolutional Generative Adversarial Network (DCGAN) for skin lesion generation and classification. The model indeed has a large number of parameters, which is typical for deep learning models. This can potentially lead to overfitting, especially when the dataset is not sufficiently large.\n\nTo mitigate overfitting, we implemented data augmentation techniques. These techniques involved transforming existing images through methods such as rotation, shear, and flip. This approach increased the effective size of our dataset and helped the model generalize better to unseen data. Additionally, we carefully tuned various hyperparameters, including the learning rate and batch size, to ensure optimal performance and to prevent the model from becoming too specialized to the training data.\n\nOn the other hand, underfitting was addressed by ensuring that the model had enough capacity to learn the underlying patterns in the data. We monitored the training and validation losses to ensure that the model was learning effectively. The use of the Adam optimizer, which generally converges faster and more efficiently than Stochastic Gradient Descent (SGD), also helped in achieving a better fit to the data. Furthermore, we conducted extensive experiments with different setups and hyperparameters to enhance our understanding of the algorithm and to achieve desirable results.\n\nThe balance between the generator and discriminator was crucial in preventing both overfitting and underfitting. We ensured that neither component became too accurate, as this could lead to undesirable results. The generator and discriminator were trained simultaneously, with the generator aiming to produce realistic images and the discriminator aiming to distinguish between real and fake images. This adversarial training process helped in achieving a model that could generalize well to new data.",
  "optimization/regularization": "In our work, we employed data augmentation as a regularization method to prevent overfitting. This technique is particularly useful when dealing with small datasets, as it helps to increase the effective size of the training data. By applying transformations such as rotation, shear, and flip to the existing images, we generated new, varied samples. This process not only enriched our dataset but also helped the model to generalize better by reducing the correlation among samples, which is a common issue in small datasets that can lead to overfitting.\n\nOverfitting occurs when a model becomes too specialized in the training data and fails to perform well on unseen data. To mitigate this, we focused on enhancing the entropic capacity of our model. Data augmentation played a crucial role in this regard, as it provided the model with a more diverse set of training examples, thereby improving its ability to generalize.\n\nAdditionally, the use of a generative adversarial network (GAN) framework inherently contributes to regularization. The generator and discriminator in the GAN work in tandem, with the generator producing new data samples and the discriminator evaluating their authenticity. This adversarial process encourages the generator to create more realistic and diverse samples, further aiding in the prevention of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. These include details about the learning rate, batch size, optimizers, and other relevant hyperparameters that were tuned during the training process. The specific values and their effects on the model's performance are discussed and summarized in various tables throughout the paper.\n\nHowever, the actual model files and the exact optimization schedule are not provided directly within the publication. The focus was on presenting the results and the methodology used to achieve them, rather than providing the raw model files or detailed optimization schedules. The experiments were conducted using the Keras Python framework, and the proposed method was applied to the Kaggle Skin Cancer Dataset.\n\nFor those interested in replicating the study, the publication provides sufficient information on the hyperparameters and the general approach taken. This should allow researchers to implement similar configurations in their own work. The dataset used is publicly available, and the Keras framework is widely accessible, making it feasible for others to follow the described methods.",
  "model/interpretability": "The model we developed is based on a Deep Convolutional Generative Adversarial Network (DCGAN), which inherently possesses some level of interpretability due to its architecture. Unlike purely black-box models, DCGANs provide insights into the learning process through their generative and discriminative components.\n\nThe generative model in our DCGAN learns to create synthetic skin lesion images that mimic real ones. By observing the progression of generated images over training epochs, we can visually assess the model's learning trajectory. For instance, early in the training process, the generated images are noisy and lack clear features. However, as training progresses, the model begins to produce more refined and realistic lesions, demonstrating its ability to capture essential features of skin lesions.\n\nThe discriminative model, on the other hand, evaluates whether an image is real or synthetic. By analyzing the discriminator's performance, we can understand which features it finds most indicative of real versus fake images. For example, if the discriminator struggles to distinguish between real and synthetic images, it suggests that the generator has successfully learned to produce convincing lesions.\n\nAdditionally, we used metrics such as the confusion matrix and receiver operating characteristic-area under the curve (ROC-AUC) to evaluate the model's performance. The confusion matrix provides a detailed breakdown of true positives, true negatives, false positives, and false negatives, offering a clear view of the model's classification accuracy. The ROC-AUC curve further illustrates the model's ability to discriminate between benign and malignant lesions across different threshold settings.\n\nMoreover, the use of image filtering and enhancement algorithms, such as the bilateral filter, aids in feature detection and extraction during training. This process makes the model's decision-making more transparent, as it highlights the specific features that contribute to accurate classification.\n\nIn summary, while the DCGAN model is not entirely transparent, it offers several avenues for interpretability. Through visual inspection of generated images, analysis of the discriminator's performance, and evaluation using metrics like the confusion matrix and ROC-AUC, we can gain insights into the model's learning process and decision-making criteria.",
  "model/output": "The model is a classification model. Specifically, it is designed for binary classification, predicting two classes present in the dataset: benign and malignant skin lesions. The model utilizes a Deep Convolutional Generative Adversarial Network (DCGAN) to generate synthetic images of skin lesions, which are then classified by the discriminator component of the GAN. The output of the model is the classification of these images into either benign or malignant categories.\n\nThe performance of the model is evaluated using several metrics, including the receiver operating characteristic-area under the curve (ROC-AUC) and the confusion matrix. These metrics help in assessing the accuracy and reliability of the model's classifications. The confusion matrix provides detailed information about the true positives, true negatives, false positives, and false negatives, which are crucial for understanding the model's performance in distinguishing between the two classes.\n\nThe model's output is visualized through the generated images, which show a smooth transition in resolution and improve in quality as the training progresses. The final test accuracy achieved by the model is 93.5%, indicating a high level of accuracy in classifying skin lesions. This accuracy is a result of fine-tuning various parameters of the network, including the learning rate and momentum for the Adam optimization algorithm, to balance the loss between the generative and discriminative networks.",
  "model/duration": "The execution time for our model was significantly influenced by the time-consuming training sessions. Each training session for the skin lesion dataset could take approximately 1 hour and 30 minutes, with some sessions extending up to 3 hours per session. These lengthy training periods consumed a substantial portion of the time allocated for the study, complicating the fine-tuning process of the Deep Convolutional Generative Adversarial Network (DCGAN) on the dataset. Additionally, the constraints in GPU power further limited our ability to generate high-quality samples efficiently. We utilized an NVIDIA GeForce RTX 2060 graphics card with 6 GB of dedicated GDDR6 VRAM for our experiments, which took around 1 hour, 35 minutes, and 16 seconds to complete. Despite these challenges, we managed to achieve an overall test accuracy of 93.5% after fine-tuning most parameters of our network. The extensive time required for training highlighted the need for more robust computational resources to enhance the model's performance and stability.",
  "model/availability": "The source code for our Deep Convolutional Generative Adversarial Network (DCGAN) model is not publicly released. However, our implementation was based on a publicly accessible repository that trained a DCGAN model on the CIFAR-10 dataset. We modified this code to suit our specific needs, including adjusting the input size and the number of filters in the convolutional layers.\n\nWe utilized Keras in the Python Anaconda development application software to realize our undertaking. The experiments were conducted on an IBM computer equipped with Windows 10 Home 64 Bit and a 10th Generation Intel Core i7-10750H 6-Core Processor. Additionally, we used an NVIDIA GeForce RTX 2060 graphics card with 6 GB of dedicated GDDR6 VRAM, 16 GB DDR4 2933MHz Dual-Channel Memory, and a 512GB NVMe SSD. The training process for the NVIDIA GeForce RTX 2060 took approximately 1 hour, 35 minutes, and 16 seconds.\n\nUnfortunately, we do not provide a direct method to run the algorithm, such as an executable, web server, virtual machine, or container instance.",
  "evaluation/method": "In our evaluation, we employed several metrics and methods to assess the performance of our generative adversarial network (GAN) for skin lesion generation and classification. We utilized classification accuracy to determine the ratio of correctly classified samples out of all samples. This was calculated using a confusion matrix, which provided insights into how well our model classified malignant or benign lesions. The confusion matrix helped us measure various performance metrics, including the area under the receiver operating characteristic curve (AUC-ROC), specificity, accuracy, recall, and precision.\n\nTo further evaluate our model's performance, we used receiver operating characteristic (ROC) curves. These curves summarize and illustrate the performance of our binary classification model, particularly focusing on the positive class. A good classifier is indicated by a ROC curve that is closer to the top-left corner, signifying better performance. Our ROC curve demonstrated a fair classification, as it deviated from the diagonal, indicating better than random performance.\n\nAdditionally, we compared the performance of different optimizers, specifically Adam and stochastic gradient descent (SGD). The Adam optimizer outperformed SGD in terms of accuracy, precision, recall, F1 score, and ROC-AUC. This comparison highlighted the importance of choosing the right optimizer for model training.\n\nWe also evaluated the impact of learning rates on our model's performance. By adjusting the learning rate, we observed variations in accuracy, precision, recall, F1 score, and ROC-AUC. A lower learning rate generally led to better convergence and performance.\n\nTo assess the quality and diversity of the generated images, we used the Fr\u00e9chet inception distance (FID). This metric measures the distance between the distribution of generated images and the true distribution of real images. A lower FID score indicates better performance, and our model achieved a competitive FID score compared to other models.\n\nIn summary, our evaluation methods included classification accuracy, confusion matrix analysis, ROC curves, optimizer comparisons, learning rate adjustments, and FID score calculations. These comprehensive evaluations provided a thorough assessment of our model's performance in generating and classifying skin lesions.",
  "evaluation/measure": "In our evaluation, we reported several key performance metrics to comprehensively assess the effectiveness of our skin lesion classification model. These metrics include accuracy, precision, recall, F1 score, and the receiver operating characteristic-area under the curve (ROC-AUC).\n\nAccuracy is the ratio of correctly classified samples out of all samples, providing a general measure of the model's performance. Precision indicates the proportion of true positive predictions among all positive predictions, reflecting the model's ability to correctly identify malignant lesions when it predicts them. Recall, also known as sensitivity, measures the proportion of true positive predictions among all actual positives, showing how well the model identifies all malignant lesions. The F1 score is the harmonic mean of precision and recall, offering a balanced measure that considers both false positives and false negatives.\n\nThe ROC-AUC is a crucial metric that evaluates the model's performance across all classification thresholds. It plots the true positive rate against the false positive rate, providing a comprehensive view of the model's ability to distinguish between malignant and benign lesions. A higher ROC-AUC indicates better model performance.\n\nThese metrics are widely used in the literature for evaluating classification models, particularly in medical imaging. They provide a robust set of measures that cover different aspects of model performance, ensuring a thorough evaluation. Our choice of metrics aligns with standard practices in the field, making our results comparable to other studies in skin lesion classification.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our method with both publicly available methods and simpler baselines to ensure a comprehensive assessment of its performance.\n\nWe compared our deep convolutional generative adversarial network (DCGAN) approach with other generative adversarial network models, such as GAN and LAPGAN, using the Fr\u00e9chet inception distance (FID) metric. The FID score measures the quality and diversity of generated images by comparing the distribution of generated images to that of real images. Our DCGAN model achieved a lower FID score compared to the other models, indicating better performance in generating realistic skin lesion images.\n\nAdditionally, we evaluated the impact of different optimizers on our model's performance. We compared the Adam optimizer with stochastic gradient descent (SGD) and found that Adam consistently outperformed SGD across various metrics, including accuracy, precision, recall, F1 score, and ROC-AUC. This comparison highlights the importance of choosing an appropriate optimizer for training deep learning models.\n\nWe also benchmarked our method against other related works in the field of skin lesion classification. Our model achieved an accuracy of 93.5% and a ROC-AUC of 0.861, which are competitive with or superior to the results reported in previous studies. This comparison demonstrates the effectiveness of our approach in distinguishing between malignant and benign skin lesions.\n\nFurthermore, we investigated the effect of different learning rates on our model's performance. By adjusting the learning rate, we were able to improve the convergence and overall performance of our DCGAN model. This comparison underscores the significance of hyperparameter tuning in optimizing deep learning models.\n\nIn summary, our evaluation included comparisons with publicly available methods, simpler baselines, and other related works in the field. These comparisons provide a comprehensive assessment of our method's performance and highlight its strengths in skin lesion generation and classification.",
  "evaluation/confidence": "The evaluation of our model's performance involved several key metrics, including accuracy, precision, recall, F1 score, and ROC-AUC. These metrics were calculated to assess the effectiveness of our generative adversarial network (GAN) in skin lesion generation and classification.\n\nAccuracy was defined as the ratio of correctly classified samples out of all samples. We used the classification accuracy method to determine the accuracy of our model, which was found to be 86.1%. This metric provides a general measure of how well the model performs across all classes.\n\nPrecision, recall, and F1 score were also calculated to provide a more detailed evaluation. Precision measures the accuracy of positive predictions, while recall measures the ability of the model to identify all relevant instances. The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. Our model achieved a precision of 90%, a recall of 83%, and an F1 score of 87.1%, indicating a good balance between precision and recall.\n\nThe ROC-AUC curve was used to assess the performance of our classifier over its complete operating range. The ROC-AUC value of 0.861 indicates that the model has a good ability to distinguish between malignant and benign lesions. The ROC curve plots the true positive rate against the false positive rate, providing a visual representation of the model's performance at various threshold settings.\n\nConfidence intervals and statistical significance were not explicitly mentioned in our evaluation. However, the performance metrics were derived from a robust training and testing process, which included multiple iterations and adjustments to hyperparameters such as learning rate and batch size. The use of a confusion matrix and ROC-AUC curve further supports the reliability of our results.\n\nIn summary, our model demonstrated strong performance in skin lesion classification, with high accuracy, precision, recall, and ROC-AUC values. While confidence intervals and statistical significance were not explicitly reported, the thorough evaluation process and the use of multiple performance metrics provide a comprehensive assessment of the model's effectiveness.",
  "evaluation/availability": "Not enough information is available."
}