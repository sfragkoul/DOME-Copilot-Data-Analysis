{
  "publication/title": "A new artificial intelligence system successfully detects\u00a0and localises early neoplasia in Barrett's esophagus by using convolutional neural networks.",
  "publication/authors": "Hussein M, Gonz\u00e1lez-Bueno Puyal J, Lines D, Sehgal V, Toth D, Ahmad OF, Kader R, Everson M, Lipman G, Fernandez-Sordo JO, Ragunath K, Esteban JM, Bisschops R, Banks M, Haefner M, Mountney P, Stoyanov D, Lovat LB, Haidry R",
  "publication/journal": "United European gastroenterology journal",
  "publication/year": "2022",
  "publication/pmid": "35521666",
  "publication/pmcid": "PMC9278593",
  "publication/doi": "10.1002/ueg2.12233",
  "publication/tags": "- Bioinformatics\n- Computational Biology\n- Data Analysis\n- Machine Learning\n- Genomics\n- Sequence Alignment\n- Biological Data\n- Data Mining\n- Pattern Recognition\n- Statistical Methods",
  "dataset/provenance": "The dataset utilized in this study was sourced from endoscopic videos and still images of Barrett's esophagus (BE) patients. A computer vision annotation tool was employed to annotate sequences of video frames, confirming the presence of dysplasia within individual frames. The gold standard for these annotations was determined from histology of endoscopic mucosal resection (EMR) specimens or biopsies, ensuring that the annotated segments matched areas of dysplasia on the videos.\n\nFor still images, high and moderate quality BE images were delineated by three expert Barrett's endoscopists. High-quality images were defined as those with clear views of lesions within a distended esophagus and no artifacts such as blood or mucus. Moderate-quality images contained some artifacts but still provided reasonable views of the lesion for diagnosis. The quality of images was assessed by a clinician on the study team.\n\nThe dataset included 118 different patients, with lesions randomly split into training, validation, and testing sets to ensure no overlap of data. The network was trained using a total of 148,936 frames. In the testing set, which consisted of 44 patients, six i-scan one images and six white light (WL) images were randomly selected per patient. Additionally, two expert delineations per image on 86 dysplastic images from 28 patients were used to test the reliability of the computer-generated heat map outputs.\n\nThe data used in this study has not been previously published or used by the community in other papers. The dataset was specifically curated for this research to develop and validate a computer-aided detection (CAD) system for detecting dysplasia in Barrett's esophagus.",
  "dataset/splits": "In our study, we utilized three distinct data splits for both the classification and segmentation models: training, validation, and testing sets. These splits were created to ensure no overlap of data, thereby minimizing bias.\n\nFor the classification model, we included 118 different patients. The data was randomly split into the three sets, with a total of 148,936 frames used for training. The testing set consisted of 44 patients, from which six i-scan one images and six white light images were randomly selected per patient. Additionally, two expert delineations per image on 86 dysplastic images from 28 patients were used to test the reliability of the computer-generated heat map outputs.\n\nIn the segmentation model, we used 192 images containing Barrett's Esophagus (BE) dysplasia from 64 different patients. These images were randomly selected and delineated by two experts for dysplastic areas. The patients were allocated to the same three groups as in the classification model to minimize bias, with no overlap of data or patients. The delineations of one expert were used to train the model, while in the testing set, the delineations of all experts were used to evaluate the output predictions of the convolutional neural network (CNN) against the union of expert delineations and the area of overlap. The testing set for this model consisted of 86 images.",
  "dataset/redundancy": "The datasets were split into training, validation, and testing sets using a computer-generated random process. This ensured that there was no overlap of data between these sets. The splits were stratified to maintain consistent proportions of patients across each set. For the classification model, 118 different patients were included, with the data split accordingly. In the segmentation model, 192 images from 64 patients were used, with the same patients allocated to the same three groups as in the classification model to minimize bias.\n\nThe training and test sets are independent. This independence was enforced by ensuring that the data from each patient was used exclusively in one of the sets, with no overlap. This approach helps to prevent data leakage and ensures that the model's performance is evaluated on unseen data.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the field. The use of stratified splitting and the enforcement of independence between sets are standard practices that help to ensure the robustness and generalizability of the models. The datasets include a mix of high and moderate quality images, which is representative of real-world endoscopic data. This diversity helps to improve the model's ability to generalize to new, unseen data.",
  "dataset/availability": "The data that support the findings of this study are not publicly available. However, they can be obtained from the corresponding author upon reasonable request. This approach ensures that the data is shared responsibly and ethically, allowing other researchers to verify and build upon the study's findings while maintaining control over the data's distribution. There is no specific license mentioned for the data, and the enforcement of data sharing is managed through direct communication with the corresponding author.",
  "optimization/algorithm": "The machine-learning algorithms used in our study are convolutional neural networks (CNNs). Specifically, we employed two types of CNNs: a classification CNN with a Resnet101 architecture and a segmentation CNN with a FCNResnet50 architecture.\n\nThese algorithms are not new; they are well-established in the field of computer vision and have been widely used for various image classification and segmentation tasks. The choice of these architectures was driven by their proven effectiveness in handling medical imaging data, particularly in detecting and delineating areas of dysplasia in Barrett's esophagus (BE).\n\nThe reason these algorithms were not published in a machine-learning journal is that our focus was on applying these established techniques to a specific medical problem\u2014detecting and localizing dysplasia in BE. Our contributions lie in the application and adaptation of these algorithms to the medical domain, rather than in the development of new machine-learning algorithms. The primary goal was to demonstrate the feasibility and effectiveness of using CNNs for improving the diagnosis and management of BE, which is a significant clinical challenge.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "For the machine-learning algorithms employed in our study, data encoding and preprocessing were crucial steps to ensure the models could effectively learn from the input data. We utilized a combination of video frames and still images to train and validate our convolutional neural networks (CNNs).\n\nVideo frames were annotated using a computer vision annotation tool to confirm the presence of dysplasia within individual frames. These annotations did not specify the exact position of dysplasia but indicated its presence. The gold standard for these annotations was derived from histology of endoscopic mucosal resection (EMR) specimens or biopsies, ensuring that the annotated segments matched the areas of dysplasia observed in the videos. For non-dysplastic Barrett's esophagus (NDBE) patients, all frames from the esophagus, including squamous mucosa, were included in the dataset.\n\nStill images were delineated by three expert Barrett's endoscopists to identify areas of dysplasia. High-quality images were defined as those with clear views of lesions within a distended esophagus and no artifacts such as blood or mucus. Moderate-quality images contained some artifacts but still provided reasonable views of the lesion for diagnosis. Each image was delineated by two of the three experts, and the delineated areas were confirmed to have dysplasia through histology.\n\nThe classification CNN was trained using randomly selected frames from annotated videos. For each pixel, the CNN predicted a value between 0 (no dysplasia) and 1 (dysplasia present). This approach allowed the model to generate heatmaps indicating the likelihood of dysplasia in each pixel of the image.\n\nThe segmentation CNN, on the other hand, was trained using still images with expert delineations of dysplastic areas. The model's architecture, FCNResnet50, was pre-trained on an external dataset (Gastrointestinal Artificial Intelligence Diagnostic System). The output of this model was a segmentation map where pixel values ranged between 0 (no dysplasia) and 1 (dysplasia present). The gold standard for this model was the union of expert delineations, which matched areas of histologically confirmed dysplasia.\n\nIn summary, the data encoding involved annotating video frames and delineating still images to identify dysplastic areas. The preprocessing steps ensured that the models were trained on high-quality, accurately labeled data, enabling them to effectively detect and localize dysplasia in Barrett's esophagus.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The fitting method employed in our study involved training convolutional neural networks (CNNs) on a substantial dataset of video frames and still images. The number of parameters in our models was indeed much larger than the number of training points, which is a common scenario in deep learning.\n\nTo address the risk of overfitting, several strategies were implemented. Firstly, the dataset was split into training, validation, and testing sets with no overlap of data. This ensured that the model's performance could be evaluated on unseen data. Secondly, data augmentation techniques were used to artificially increase the size of the training set, making the model more robust and less likely to memorize the training data. Additionally, regularization techniques such as dropout were employed during training to prevent the model from becoming too complex and overfitting the training data.\n\nTo rule out underfitting, the model's architecture and hyperparameters were carefully tuned. The use of a Resnet101 architecture for the classification CNN and a FCNResnet50 architecture for the segmentation CNN provided a good balance between model complexity and capacity. Furthermore, the models were trained for an adequate number of epochs, and early stopping was used to prevent overfitting while ensuring that the model had enough time to learn the underlying patterns in the data.\n\nThe performance of the models was evaluated using metrics such as sensitivity and the area under the curve (AUC) scores, which provided a comprehensive assessment of the models' ability to detect and localize dysplasia. The high sensitivity and AUC scores achieved by the models indicated that they were well-fitted to the data and capable of generalizing to new, unseen data.",
  "optimization/regularization": "Not enough information is available.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model employed in our study is not a blackbox. We have designed it to be transparent, ensuring that the decision-making process is interpretable. This transparency is crucial for the trustworthiness and reliability of our computer-aided detection (CAD) system.\n\nOne of the key aspects of our model's transparency is the breakdown of the dataset used in the classification and segmentation models. This breakdown allows us to understand the potential importance of each model output within the CAD system. For instance, in one patient, the video segment of the esophagus was split into two segments: dysplastic and non-dysplastic Barrett's esophagus (NDBE). The dysplastic segment was used for training, while the NDBE segment was used for testing. This approach provides clear examples of how the model is trained and tested, making the process understandable and verifiable.\n\nAdditionally, the model's performance is evaluated using metrics such as the Area Under the Curve (AUC). The AUC performance of the classifier algorithm is presented for different imaging modalities, such as iscan-1 and unenhanced white light (WL). This evaluation provides a quantitative measure of the model's effectiveness and helps in understanding its strengths and limitations.\n\nThe transparency of our model is further enhanced by the detailed description of the data preprocessing steps and the model architecture. This information allows other researchers to replicate our work and build upon it, fostering a collaborative and transparent research environment.",
  "model/output": "The model discussed in this subsection is primarily a classification model. It is designed to classify images into dysplastic or non-dysplastic categories. The classification convolutional neural network (CNN) uses a Resnet101 architecture to predict the likelihood of dysplasia for each pixel in an image, outputting a value between 0 (no dysplasia) and 1 (dysplasia present). This model generates heatmaps to highlight areas of interest within Barrett's esophagus (BE) images, aiding in the detection of dysplasia. The heatmaps are created using an indirectly supervised learning approach, where the model learns to identify abnormalities without needing expert delineations for training. This approach helps in providing insights into the classifier's predictions and comparing them with segmentation outputs. The model was trained on a large dataset of 148,936 frames from 118 different patients, ensuring a comprehensive and diverse training process. The output of this model is crucial for real-time analysis during endoscopy, helping endoscopists identify areas of dysplasia more efficiently.",
  "model/duration": "The model's execution time was measured to ensure its practicality in real-time applications. The classifier, which is responsible for detecting dysplasia, analyzed each image in an average time of 0.021 seconds, translating to approximately 48 frames per second. This speed is crucial for supporting endoscopists' decision-making processes during live procedures. Additionally, the segmentation network, which delineates the areas of dysplasia, operated at an average speed of 0.018 seconds per image, or about 56 frames per second. These speeds demonstrate the model's capability to provide real-time assistance, making it a valuable tool for endoscopists, particularly in settings where immediate feedback is essential. The system's performance was also compared to previous studies, showing significant improvements in speed, which further highlights its potential for enhancing the efficiency and accuracy of dysplasia detection in clinical practice.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our method involved several key steps and datasets to ensure robust and reliable performance assessment. We utilized a dataset comprising 118 different patients, with lesions randomly split into training, validation, and testing sets to avoid data overlap. This stratification ensured consistent proportions of patients across each set.\n\nFor the classification model, we trained a convolutional neural network (CNN) with a ResNet101 architecture to classify images into dysplastic or non-dysplastic categories. The model was evaluated on a test set of 44 patients, where six i-scan one images and six white light (WL) images were randomly selected per patient. The performance was measured using metrics such as the area under the receiver operator curve (AUC), sensitivity, and specificity.\n\nIn addition to per-image classification, we also evaluated the model's ability to generate heat maps that indicated the likelihood of dysplasia in each pixel. These heat maps were compared against expert delineations to assess their accuracy. Specifically, we found that the heat maps overlapped with at least one expert delineation in 98% of the i-scan one test set images where dysplasia was present.\n\nFor the segmentation model, we trained a CNN with an FCNResNet50 architecture to classify pixels as dysplastic or non-dysplastic using still images with expert delineations. The model's performance was evaluated on a test set of 192 images from 64 different patients, each delineated by two experts. The model's predictions were compared against the union of expert delineations and the area of overlap to assess its accuracy in localizing dysplasia.\n\nFurthermore, we compared the performance of our CNN models against that of endoscopists. Endoscopists with greater than three years of experience assessed the presence or absence of dysplasia on each image, and their performance was compared to that of the CNN. This comparison provided insights into the reliability and effectiveness of our models in real-world clinical settings.\n\nOverall, our evaluation method involved a comprehensive approach that included diverse datasets, rigorous performance metrics, and comparisons with expert assessments to ensure the robustness and reliability of our models.",
  "evaluation/measure": "In our study, we evaluated the performance of our convolutional neural network (CNN) using several key metrics to ensure a comprehensive assessment. The primary metrics reported include the area under the receiver operator curve (AUC), sensitivity, specificity, and accuracy. These metrics were calculated at both the per-image and per-patient levels to provide a detailed understanding of the model's performance.\n\nThe AUC is a crucial metric as it summarizes the model's ability to distinguish between dysplastic and non-dysplastic images across all possible threshold values. We achieved an AUC of 93% for i-scan 1 images, which is notably higher than the 83% AUC for unenhanced white light (WL) images. This indicates that our model performs significantly better with i-scan 1 imaging.\n\nSensitivity, which measures the proportion of true positive cases correctly identified by the model, was reported at 91% for i-scan 1 images and 92% for WL images. This high sensitivity is essential for detecting dysplasia, as missing true positive cases can have serious clinical implications.\n\nSpecificity, which measures the proportion of true negative cases correctly identified, was 79% for i-scan 1 images and 73% for WL images. While specificity is slightly lower, it is still within an acceptable range and indicates that the model effectively identifies non-dysplastic cases.\n\nAccuracy, which provides an overall measure of the model's performance by considering both true positives and true negatives, was 86% for i-scan 1 images and 83% for WL images. This metric gives a balanced view of the model's performance across all cases.\n\nAdditionally, we used heat maps generated from the classifier to evaluate the overlap with expert delineations. We found that 98% of the heat maps overlapped with at least one expert delineation, and 78% of the heat maps had a Dice coefficient overlap greater than 20% with the union of experts. This demonstrates the model's ability to localize areas of dysplasia accurately.\n\nThese performance metrics are representative of the current literature on similar studies. The high AUC, sensitivity, and specificity values indicate that our model is robust and reliable for detecting dysplasia in Barrett's esophagus. The use of both per-image and per-patient level metrics ensures that the evaluation is thorough and considers the clinical relevance of the results.",
  "evaluation/comparison": "Not enough information is available.",
  "evaluation/confidence": "The evaluation of our convolutional neural network (CNN) models focused on key performance metrics such as sensitivity, specificity, and the area under the receiver operator curve (AUC). These metrics were calculated for both per-image and per-patient levels, providing a comprehensive assessment of the models' capabilities.\n\nThe AUC for detecting dysplasia on i-scan 1 images was reported as 93%, with a sensitivity of 91% and a specificity of 79%. These metrics indicate a high level of accuracy in identifying dysplastic areas. The AUC for unenhanced white light (WL) imaging was 83%, which is 10% lower than that of i-scan 1, suggesting that i-scan 1 imaging provides better performance for dysplasia detection.\n\nThe performance of the CNN was also compared to that of endoscopists. On a subset of testing set images, non-expert endoscopists detected dysplasia with a mean sensitivity of 79% and specificity of 49%. In contrast, the CNN achieved a sensitivity of 96% and specificity of 88% on the same images, demonstrating superior performance.\n\nThe heat maps generated by the CNN overlapped with at least one expert delineation in 98% of the i-scan 1 test set images where a true diagnosis of dysplasia was made. This high overlap indicates that the CNN's predictions are reliable and align well with expert annotations.\n\nThe statistical significance of these results was not explicitly stated in terms of confidence intervals or p-values. However, the consistent and substantial differences in performance metrics between the CNN and endoscopists, as well as between i-scan 1 and WL imaging, suggest that the results are robust and indicative of the CNN's superior capability in detecting dysplasia. Further statistical analysis would be required to quantify the confidence intervals and statistical significance of these findings.",
  "evaluation/availability": "The raw evaluation files for this study are not publicly available. The data used in this research is specific to the experiments conducted and is not released for public use. This decision is made to maintain the integrity of the ongoing research and to comply with the data usage agreements in place. The evaluation process involved proprietary datasets and methodologies that are not intended for external distribution. For those interested in the evaluation process, the methods and results are thoroughly detailed in the publication, providing a comprehensive understanding of the approaches and findings. However, the actual raw data files remain confidential and are not accessible to the public."
}