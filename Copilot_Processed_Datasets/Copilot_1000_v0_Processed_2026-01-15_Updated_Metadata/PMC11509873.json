{
  "publication/title": "Cardiovascular Risk Factors as Independent Predictors of Diabetic Retinopathy in Type II Diabetes Mellitus: The Development of a Predictive Model.",
  "publication/authors": "Ro\u015fu CD, Bratu ML, Stoicescu ER, Iacob R, Ha\u021began OA, Ghenciu LA, Bolintineanu SL",
  "publication/journal": "Medicina (Kaunas, Lithuania)",
  "publication/year": "2024",
  "publication/pmid": "39459404",
  "publication/pmcid": "PMC11509873",
  "publication/doi": "10.3390/medicina60101617",
  "publication/tags": "- Cardiovascular risk factors\n- Diabetic retinopathy\n- Type 2 diabetes mellitus\n- Predictive modeling\n- Machine learning\n- XGBoost\n- Random Forest\n- SHAP analysis\n- Retinal imaging\n- Early detection\n- Blood pressure\n- Lipid profiles\n- BMI\n- Clinical intervention\n- Disease progression\n- Cross-sectional study\n- Longitudinal research\n- Synthetic Minority Over-sampling Technique (SMOTE)\n- Feature engineering\n- Model performance",
  "dataset/provenance": "The dataset used in this study was sourced from patients attending the outpatient ophthalmology clinic at the Timis County Emergency Clinical Hospital. A total of 377 patients with a confirmed diagnosis of type 2 diabetes mellitus (T2DM) were included. These patients were aged 18 years or older, had a diagnosis of diabetes for at least one year, and had undergone a comprehensive eye exam to assess the presence and stage of diabetic retinopathy (DR). The data collection included various clinical and demographic variables such as age, gender, body mass index (BMI), blood pressure (systolic and diastolic), lipid profile (LDL cholesterol, HDL cholesterol, triglycerides, total cholesterol), and smoking status. The DR status was classified based on fundus examination and retinal imaging, following standardized criteria similar to the Early Treatment Diabetic Retinopathy Study (ETDRS) guidelines.\n\nThe dataset was not explicitly mentioned to have been used in previous papers or by the community. However, the study design and data collection methods were conducted in accordance with the TRIPOD (Transparent Reporting of a Multivariable Prediction Model for Individual Prognosis Or Diagnosis) guidelines, ensuring transparent and comprehensive reporting of the predictive models. The sample size was determined based on the number of available patients meeting the inclusion criteria, with a minimum of 300 participants required for adequate model development and validation.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a testing set. The training set comprised 70% of the total data, while the testing set included the remaining 30%. This split was used to train and validate the machine learning models, ensuring that the models were evaluated on unseen data to assess their generalizability.\n\nThe dataset consisted of 377 patients with a confirmed diagnosis of type 2 diabetes mellitus (T2DM). The sample size was determined based on the number of available patients meeting the inclusion criteria, with a minimum of 300 participants required for adequate model development and validation.\n\nThe training set was used to develop and optimize the models, including hyperparameter tuning through techniques such as grid search and cross-validation. The testing set was then used to evaluate the final performance of the models, providing an unbiased assessment of their accuracy, precision, recall, and other relevant metrics.\n\nIn addition to the training and testing splits, a 5-fold cross-validation approach was employed during the model development phase. This involved dividing the training data into five subsets, or folds, and training the model on four of these folds while validating it on the remaining fold. This process was repeated five times, with each fold serving as the validation set once, ensuring that the model's performance was robust and not dependent on a particular subset of the data.\n\nThe distribution of data points in each split was designed to reflect the overall distribution of the dataset, ensuring that each subset was representative of the entire population. This approach helped to minimize bias and improve the reliability of the model's predictions.",
  "dataset/redundancy": "The datasets used in this study were split into training and testing sets to evaluate the performance of the machine learning models. Specifically, 70% of the dataset was allocated for training the models, while the remaining 30% was reserved for testing and validation purposes. This split ensures that the training and test sets are independent, which is crucial for obtaining unbiased performance metrics.\n\nTo enforce the independence of the training and test sets, standard practices were followed. The data was randomly shuffled before splitting to ensure that the distribution of cases across different stages of diabetic retinopathy (DR) was representative in both sets. This randomization helps in mitigating any potential bias that could arise from a non-random split.\n\nThe distribution of the dataset in this study compares favorably with previously published machine learning datasets in the field of diabetic retinopathy. The inclusion of a diverse range of DR stages, from no DR to proliferative DR, ensures that the models are trained on a comprehensive set of data. This comprehensive approach is essential for developing robust predictive models that can generalize well to real-world clinical scenarios.\n\nAdditionally, the use of multiple imputation for handling missing data ensures that all cases are retained in the analysis, maintaining the integrity of the dataset. This method helps in preserving the distribution of the data, making the training and test sets more representative of the overall population.\n\nIn summary, the datasets were carefully split and managed to ensure independence and representativeness, aligning with best practices in machine learning and providing a solid foundation for the development and evaluation of predictive models for diabetic retinopathy.",
  "dataset/availability": "The data used in this study is not publicly available. However, it is available upon request. This approach ensures that the data can be accessed by researchers who are interested in replicating or building upon the study, while also maintaining control over how the data is used. By providing the data upon request, we can enforce proper usage and citation, ensuring that the data is used ethically and responsibly. This method also allows for the protection of patient privacy, as the data can be shared under controlled conditions that comply with ethical guidelines and regulations.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is gradient boosting. Specifically, we employed the XGBoost algorithm, which is a popular and well-established implementation of gradient boosting. XGBoost is not a new algorithm; it has been widely used and validated in various fields, including healthcare, for its robust performance in predictive modeling.\n\nThe reason XGBoost was not published in a machine-learning journal is that it is an existing and well-documented algorithm. XGBoost was developed by Tianqi Chen and his team and has been extensively studied and optimized over the years. Its effectiveness and efficiency have been demonstrated in numerous applications, making it a go-to choice for many researchers and practitioners in the field of machine learning.\n\nIn our study, we utilized XGBoost because of its ability to handle large datasets efficiently and its superior performance in predicting diabetic retinopathy (DR) stages. The algorithm's scalability and efficiency were particularly advantageous given the size of our dataset, which contained hundreds of patient records. Additionally, XGBoost's flexibility in hyperparameter tuning allowed us to optimize the model's performance, achieving an ROC-AUC of 0.72, which was the best among the models we tested.\n\nWhile XGBoost is not new, its application in predicting DR based on cardiovascular risk factors is a novel contribution to the field of ophthalmology and diabetes management. Our work demonstrates the potential of advanced machine-learning techniques in improving early detection and intervention strategies for DR, ultimately aiming to delay or prevent the progression of this sight-threatening complication.",
  "optimization/meta": "The models developed in this study do not utilize data from other machine-learning algorithms as input. Instead, they rely on cardiovascular risk factors and other clinical data directly collected from patients. The study employed several machine-learning methods, including Random Forest, XGBoost, Support Vector Machines (SVMs), and Logistic Regression, to evaluate their performance in predicting diabetic retinopathy (DR). Each of these models was trained and evaluated independently using the same dataset of cardiovascular risk factors.\n\nThe dataset consisted of clinical and demographic data from 377 patients with Type 2 Diabetes Mellitus (T2DM). The data included variables such as age, gender, body mass index (BMI), blood pressure (systolic and diastolic), lipid profile (LDL cholesterol, HDL cholesterol, triglycerides, total cholesterol), and smoking status. The models were evaluated using 5-fold cross-validation to ensure that the training data was independent for each fold. This approach helps in assessing the generalizability of the models and reduces the risk of overfitting.\n\nThe study did not combine the outputs of different machine-learning algorithms to create a meta-predictor. Instead, each model was evaluated separately, and their performance metrics were compared. The XGBoost model demonstrated the best performance with an ROC-AUC of 0.72, indicating its superior ability to distinguish between patients with and without DR. The other models, such as Random Forest, SVMs, and Logistic Regression, also showed moderate predictive ability but did not outperform XGBoost.\n\nIn summary, the models developed in this study are standalone machine-learning algorithms that do not use data from other machine-learning algorithms as input. The training data for each model was independent, and the performance of each model was evaluated separately. The study focused on comparing the performance of different machine-learning methods to identify the most effective approach for predicting DR based on cardiovascular risk factors.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for ensuring the machine-learning algorithms performed optimally. Initially, all categorical variables were transformed using one-hot encoding to convert them into a format suitable for the models. This technique helped in representing categorical data as binary vectors, which is essential for algorithms like XGBoost, Random Forest, and Support Vector Machines.\n\nFor continuous variables, such as blood pressure and lipid profiles, we performed standardization to ensure that each feature contributed equally to the model's predictions. Standardization involved scaling the data to have a mean of zero and a standard deviation of one, which is particularly important for algorithms sensitive to the scale of input features.\n\nHandling missing data was another critical preprocessing step. We employed multiple imputation to address any missing values in the dataset. This method involved creating several imputed datasets by estimating the missing values based on the observed data. Each imputed dataset was then analyzed, and the results were combined to account for the variability introduced by the imputation process. This approach ensured that all cases were retained in the analysis, minimizing potential bias while maintaining the integrity of the dataset.\n\nAdditionally, we performed feature scaling for the machine-learning models. Feature scaling involved normalizing the range of features to a standard scale, which helped in improving the convergence and performance of the models. This step was particularly important for algorithms like Support Vector Machines, which are sensitive to the scale of the input features.\n\nIn summary, the data encoding and preprocessing steps included one-hot encoding for categorical variables, standardization and scaling for continuous variables, and multiple imputation for handling missing data. These steps were essential for ensuring that the machine-learning algorithms could effectively learn from the data and make accurate predictions.",
  "optimization/parameters": "In our study, we utilized several cardiovascular risk factors as input parameters for our models. Specifically, we focused on systolic blood pressure (SBP), diastolic blood pressure (DBP), high-density lipoprotein (HDL) cholesterol, and body mass index (BMI). These parameters were chosen based on their known associations with diabetic retinopathy (DR) and their availability in clinical settings.\n\nThe selection of these parameters was driven by their significance in predicting DR, as evidenced by feature importance analyses. For instance, SBP emerged as the strongest positive predictor, while HDL cholesterol showed a protective effect. BMI also played a crucial role in assessing DR risk. These factors were identified through global feature importance analysis using SHAP values, which highlighted their individual contributions to the model's predictions.\n\nAdditionally, we considered interactions between these parameters. For example, the combined effect of high blood pressure and obesity (BMI > 30) significantly increased the risk of DR. This interaction was also captured in our models, enhancing their predictive accuracy.\n\nThe number of parameters used in the model was determined by balancing model complexity and interpretability. While more parameters could potentially improve predictive performance, they might also increase the risk of overfitting and reduce the model's clinical utility. Therefore, we focused on a subset of key cardiovascular risk factors that are routinely measured in clinical practice, ensuring that our model remains practical and interpretable for healthcare providers.",
  "optimization/features": "In our study, we utilized several cardiovascular risk factors as input features to predict diabetic retinopathy (DR). The primary features included systolic blood pressure (SBP), diastolic blood pressure (DBP), high-density lipoprotein (HDL) cholesterol, and body mass index (BMI). These features were selected based on their known associations with DR and their availability in clinical datasets.\n\nFeature selection was performed to ensure that the most relevant predictors were included in the models. This process involved evaluating the importance of each feature in predicting DR using techniques such as SHAP (Shapley Additive exPlanations) values. The selection was conducted using the training set only, adhering to best practices to prevent data leakage and ensure the robustness of the model's performance.\n\nThe global feature importance analysis revealed that SBP had the highest average SHAP value, indicating it was the strongest positive predictor of DR. HDL cholesterol showed a protective effect, with higher levels reducing the predicted risk of DR. BMI also played a significant role, reinforcing its contribution to DR risk. These findings guided the inclusion of these features in the final models, enhancing their predictive accuracy and clinical relevance.",
  "optimization/fitting": "In our study, we employed several machine learning models, including XGBoost, Random Forest, Support Vector Machines (SVMs), and Logistic Regression, to predict diabetic retinopathy (DR) using cardiovascular risk factors. The number of parameters in these models varied, but we ensured that the models were appropriately fitted to the data without overfitting or underfitting.\n\nFor XGBoost, we utilized a grid search and cross-validation approach to optimize key hyperparameters such as the learning rate, maximum tree depth, and number of estimators. This process helped in selecting the best parameters that minimized overfitting while maximizing the model's performance. Additionally, we adjusted the regularization parameters to control the complexity of the model, further mitigating the risk of overfitting.\n\nRandom Forest models, while having a larger number of parameters due to the ensemble of decision trees, were managed by tuning the number of trees and the maximum depth of each tree. This ensured that the model did not become too complex, thereby avoiding overfitting. The use of cross-validation also helped in assessing the model's performance on unseen data, providing a robust estimate of its generalization capability.\n\nSVMs, which can have a large number of parameters depending on the kernel used, were tuned by adjusting the regularization parameter and the kernel function. This tuning process helped in finding the right balance between bias and variance, ensuring that the model did not underfit or overfit the data.\n\nLogistic Regression, being a simpler model, had fewer parameters. However, we still performed hyperparameter tuning to optimize the regularization parameter, which helped in preventing overfitting and ensuring that the model generalized well to new data.\n\nTo rule out underfitting, we evaluated the performance of each model using 5-fold cross-validation. This method provided a comprehensive assessment of the model's ability to capture the underlying patterns in the data. Models that showed poor performance on the validation sets were further tuned or discarded in favor of more complex models that could better capture the data's intricacies.\n\nIn summary, we employed rigorous hyperparameter tuning and cross-validation techniques to ensure that our models were neither overfitted nor underfitted. This approach allowed us to select the best-performing models that could accurately predict DR while generalizing well to new, unseen data.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and enhance the generalization of our models. One of the key methods used was hyperparameter tuning, which involved adjusting parameters such as the learning rate, maximum tree depth, and number of estimators for the XGBoost model. This process helped in optimizing the model's performance and reducing the risk of overfitting.\n\nAdditionally, we utilized regularization parameters within the XGBoost model, such as L1 and L2 regularization, to penalize complex models and encourage simpler ones. This approach helped in controlling the model's complexity and improving its ability to generalize to new data.\n\nFor the Support Vector Machines (SVMs), we tuned the regularization parameter (C) and the kernel function to balance the trade-off between achieving a low training error and a low testing error. This ensured that the model did not overfit to the training data.\n\nFurthermore, we applied dimensionality reduction techniques, such as feature selection and engineering, to focus on the most relevant predictors. This not only simplified the models but also reduced the risk of overfitting by eliminating noise and irrelevant features.\n\nOverall, these regularization methods played a crucial role in enhancing the robustness and reliability of our predictive models for diabetic retinopathy.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in detail within the publication. We employed a grid search and cross-validation approach to fine-tune key hyperparameters for the XGBoost, Random Forest, and SVM models. For XGBoost, parameters such as the learning rate, maximum tree depth, and number of estimators were optimized. In the case of Random Forest, we adjusted the number of trees and the maximum depth. For SVMs, we tuned the regularization parameter and the kernel function. These configurations were crucial in enhancing the models' performance and ensuring their generalizability.\n\nThe specific model files and optimization schedules are not explicitly provided in the publication. However, the methods and techniques used for hyperparameter tuning and model validation are thoroughly described, allowing for reproducibility. The statistical analyses were performed using Python and SPSS software (version 29), and the results were validated using 5-fold cross-validation. This approach ensures that the models can be replicated and validated by other researchers.\n\nRegarding the availability and licensing of the configurations and parameters, the publication itself serves as the primary source of information. The methods and results are presented in a manner that adheres to standard scientific reporting practices, ensuring transparency and reproducibility. While the exact model files may not be directly accessible, the detailed descriptions of the optimization processes and parameter settings provide a clear roadmap for implementing similar models.",
  "model/interpretability": "The model developed in this study, specifically the XGBoost model, is not entirely transparent and can be considered somewhat of a black box. However, efforts were made to enhance its interpretability, particularly for clinical applications. To achieve this, a feature importance analysis was conducted, revealing that systolic blood pressure (SBP), diastolic blood pressure (DBP), HDL cholesterol, and body mass index (BMI) were the most influential predictors. This analysis provided insights into which factors significantly impact the model's predictions.\n\nAdditionally, SHAP (Shapley Additive exPlanations) values were applied to further improve transparency. The global feature importance analysis using SHAP values indicated that SBP had the highest average SHAP value of 0.35, making it the strongest positive predictor of diabetic retinopathy (DR). Conversely, HDL cholesterol had an average SHAP value of \u22120.27, demonstrating its protective effect by reducing the predicted risk of DR as its levels increased. BMI also played a crucial role, with an average SHAP value of 0.19, reinforcing its contribution to DR risk.\n\nFocusing on individual features, it was observed that patients with SBP levels above 140 mmHg had SHAP values ranging from 0.30 to 0.50, indicating a significant increase in DR risk. For instance, a patient with an SBP of 150 mmHg had a SHAP value of 0.45, highlighting the strong influence of high blood pressure on DR predictions. In contrast, patients with HDL levels below 40 mg/dL had consistently negative SHAP values between \u22120.20 and \u22120.35, with a patient having an HDL level of 35 mg/dL showing a SHAP value of \u22120.32.\n\nThe SHAP analysis also revealed a significant interaction between SBP and BMI. For patients with both high blood pressure and obesity (BMI > 30), the combined SHAP values reached up to 0.55, indicating a compounded risk of DR. These enhancements not only improved the model\u2019s clinical utility but also ensured that clinicians could more easily understand the contribution of each risk factor. This understanding facilitates the development of more personalized prevention and intervention strategies for patients at risk of DR.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the presence and severity of diabetic retinopathy (DR) based on cardiovascular risk factors. The model categorizes patients into different stages of DR, including No DR, Mild, Moderate, Severe, and Proliferative DR. This classification allows for the stratification of patients according to their disease severity, enabling clinicians to tailor treatment plans to individual patient needs. The model's performance was evaluated using metrics such as accuracy, precision, recall, and ROC-AUC, which are standard for classification tasks. Additionally, the model's ability to handle imbalanced classes, such as Moderate and Severe DR, was a key focus, demonstrating its effectiveness in distinguishing between different stages of the disease. The use of techniques like SHAP values further enhanced the interpretability of the model's predictions, providing insights into the contribution of each risk factor to the classification outcomes.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to ensure the robustness and generalizability of the predictive models. We utilized 5-fold cross-validation for all models, which involved dividing the dataset into five subsets. Each subset was used once as a validation set while the remaining four subsets were used for training. This process was repeated five times, with each subset serving as the validation set once. Performance metrics, including accuracy, precision, recall, and ROC-AUC, were calculated across each fold to provide an average performance measure.\n\nIn addition to cross-validation, we also performed hyperparameter tuning using a grid search and cross-validation approach. This involved optimizing key hyperparameters for the XGBoost, Random Forest, and SVM models. For XGBoost, parameters such as the learning rate, maximum tree depth, and number of estimators were fine-tuned. For Random Forest, we adjusted the number of trees and the maximum depth, while for SVMs, we tuned the regularization parameter and the kernel function. These enhancements not only improved the model\u2019s clinical utility but also ensured that clinicians can more easily understand the contribution of each risk factor, making it possible to develop more personalized prevention and intervention strategies for patients at risk of DR.\n\nThe models were trained using 70% of the dataset, and 30% was used for testing and validation. The performance of the models was evaluated using accuracy, precision, recall, and the confusion matrix. The confusion matrix provided insights into the number of true positives, true negatives, false positives, and false negatives, helping to assess the models' ability to correctly classify patients with and without DR.\n\nFurthermore, we applied SHAP (Shapley Additive exPlanations) values to enhance the transparency of the models. SHAP values helped in understanding the contribution of each feature to the model's predictions, identifying key predictors such as systolic blood pressure (SBP), HDL cholesterol, and BMI. This analysis provided a deeper understanding of how these factors influence the risk of DR, aiding in the development of targeted interventions.\n\nThe calibration plots and Hosmer\u2013Lemeshow test results were also used to evaluate the models. The calibration plots showed that the XGBoost model was well calibrated, with predicted probabilities closely matching observed outcomes. The Hosmer\u2013Lemeshow test results indicated no significant lack of fit for either the XGBoost or Random Forest models, with p-values of 0.28 and 0.32, respectively. These evaluations ensured that the models were reliable and could be trusted for clinical use.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our models in predicting diabetic retinopathy (DR). The primary metrics reported include accuracy, precision, recall, and the area under the receiver operating characteristic curve (ROC-AUC). These metrics were chosen for their ability to provide a thorough assessment of model performance across different aspects.\n\nAccuracy measures the overall correctness of the model's predictions, indicating the proportion of true results (both true positives and true negatives) among the total number of cases examined. Precision focuses on the correctness of positive predictions, showing the proportion of true positives among all positive predictions made by the model. Recall, also known as sensitivity, evaluates the model's ability to identify all relevant instances, representing the proportion of true positives among all actual positives. The ROC-AUC provides a single scalar value that summarizes the model's performance across all classification thresholds, offering a comprehensive view of the trade-off between the true positive rate and the false positive rate.\n\nIn addition to these metrics, we utilized the confusion matrix to provide a detailed breakdown of the model's performance. The confusion matrix displays the counts of true positives, true negatives, false positives, and false negatives, allowing for a granular analysis of where the model succeeds and where it may falter. This is particularly useful for understanding the specific types of errors the model makes, which can inform further improvements.\n\nThe set of metrics reported in this study is representative of standard practices in the literature. Accuracy, precision, recall, and ROC-AUC are commonly used in machine learning evaluations, especially in medical diagnostics, where the consequences of false positives and false negatives can be significant. The inclusion of the confusion matrix further aligns with best practices, providing a detailed view of model performance that is both transparent and actionable.\n\nMoreover, we employed 5-fold cross-validation to ensure the robustness and generalizability of our results. This technique involves dividing the dataset into five subsets, training the model on four subsets, and validating it on the remaining subset. This process is repeated five times, with each subset serving as the validation set once. The performance metrics are then averaged across the five folds, providing a more reliable estimate of the model's performance.\n\nIn summary, the performance metrics reported in this study are comprehensive and aligned with established standards in the field. They offer a detailed and nuanced evaluation of the models' abilities to predict diabetic retinopathy, ensuring that the findings are both reliable and informative for clinical applications.",
  "evaluation/comparison": "In our study, we evaluated multiple machine learning models to predict diabetic retinopathy (DR) using cardiovascular risk factors. We compared the performance of several models, including XGBoost, Random Forest, Support Vector Machines (SVMs), and Logistic Regression. Each model was assessed using 5-fold cross-validation to ensure robustness and generalizability.\n\nXGBoost demonstrated the best performance with an ROC-AUC of 0.72 after hyperparameter tuning, indicating its superior ability to distinguish between patients with and without DR. This model also showed notable improvements in accuracy, precision, and recall compared to other models. Random Forest, while offering higher interpretability, achieved an ROC-AUC of 0.65, which was lower than XGBoost but better than SVMs and Logistic Regression. SVMs and Logistic Regression had ROC-AUCs of 0.67 and 0.60, respectively, suggesting that they may not capture the non-linear relationships in the data as effectively as XGBoost.\n\nWe also compared the models' ability to predict different stages of DR. The XGBoost model achieved an overall accuracy of 54.8% in predicting DR stages, outperforming the Random Forest model, which had an accuracy of 43.1%. XGBoost was particularly effective in identifying patients without DR, with a precision of 63% and a recall of 85% for the No DR class. This indicates that XGBoost was more adept at differentiating between patients with no signs of DR and those with more advanced stages.\n\nIn summary, while simpler models like Logistic Regression and Random Forest provide valuable insights and are easier to implement, XGBoost's superior performance in predicting DR and its stages makes it a more effective tool for clinical applications. The comparison to these baselines highlights the strengths and weaknesses of each model, providing a comprehensive evaluation of their predictive capabilities.",
  "evaluation/confidence": "The evaluation of our models included a thorough assessment of their performance metrics, which were accompanied by confidence intervals to provide a clear understanding of their variability. For instance, the XGBoost model demonstrated an accuracy of 64.3% \u00b1 2.7%, a precision of 63.5% \u00b1 3.1%, a recall of 61.9% \u00b1 2.9%, and an ROC-AUC of 0.68 \u00b1 0.04. These intervals indicate the range within which the true performance metrics are likely to fall, offering a more comprehensive view of the model's reliability.\n\nStatistical significance was also a key consideration in our evaluation. The Hosmer\u2013Lemeshow test results indicated no significant lack of fit for either the XGBoost or Random Forest models, with p-values of 0.28 and 0.32, respectively. This suggests that the models are well-calibrated and that their predicted probabilities closely match the observed outcomes. Additionally, the ROC-AUC comparisons between models, such as XGBoost's 0.68 versus Random Forest's 0.62, provide evidence of superior discriminatory ability. The improvements in ROC-AUC after tuning further support the statistical significance of our findings, with XGBoost achieving an ROC-AUC of 0.72 \u00b1 0.03 post-tuning.\n\nThe calibration plots also played a crucial role in evaluating the models' confidence. The XGBoost model showed well-calibrated predicted probabilities, while the Random Forest model exhibited slight overconfidence at higher probabilities. This analysis ensures that the models' predictions are reliable and that the confidence in their outputs is justified.\n\nOverall, the inclusion of confidence intervals and statistical tests provides a robust framework for evaluating the models' performance and claiming their superiority over other methods and baselines. The results are statistically significant, and the models demonstrate strong calibration and discriminatory ability.",
  "evaluation/availability": "Not enough information is available."
}