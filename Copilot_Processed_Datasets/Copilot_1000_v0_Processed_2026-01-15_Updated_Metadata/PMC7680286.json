{
  "publication/title": "Deep learning segmentation of gadolinium-enhancing lesions in multiple sclerosis.",
  "publication/authors": "Coronado I, Gabr RE, Narayana PA",
  "publication/journal": "Multiple sclerosis (Houndmills, Basingstoke, England)",
  "publication/year": "2021",
  "publication/pmid": "32442043",
  "publication/pmcid": "PMC7680286",
  "publication/doi": "10.1177/1352458520921364",
  "publication/tags": "- convolutional neural networks\n- active lesions\n- MRI\n- false positive\n- white matter lesions\n- artificial intelligence\n- deep learning\n- multiple sclerosis\n- gadolinium-enhancing lesions\n- segmentation",
  "dataset/provenance": "The dataset used in this study originates from the CombiRx trial, which included multiple MRI sequences acquired on various platforms at both 1.5 T and 3 T field strengths. The imaging protocol consisted of 2D FLAIR, 2D dual echo turbo spin echo (TSE) images, and pre- and post-contrast T1-weighted (T1w) images, all with identical geometry and voxel dimensions of 0.94 \u00d7 0.94 \u00d7 3 mm\u00b3. A total of 1008 scans were initially available, but 2 were discarded due to artifacts and poor signal-to-noise ratio, leaving 1006 scans for analysis.\n\nOut of these 1006 scans, 398 patients had at least one Gd-enhancing lesion based on the ground truth segmentation. The data were divided into three sets for deep learning model development: 60% (604 scans) for training, 20% (201 scans) for validation, and 20% (201 scans) for testing, using random stratified sampling. The dataset is typical of clinical-grade MRI scans, providing a realistic test of the proposed methods.\n\nThe dataset has been used in previous work, including the MRIAP segmentation technique, which was validated by two experts: an MS neurologist with over 30 years of experience and an MRI scientist with over 35 years of experience in neuro-MRI. The segmentation results were further refined using an in-house developed software package that allowed for simultaneous display of multiple images in different orientations and included various editing tools. Any discrepancies between the raters were resolved by consensus. The developed models and Python scripts used in this study are available to the community in a public repository, facilitating further research and validation by the broader scientific community.",
  "dataset/splits": "The dataset was divided into three splits: training, validation, and testing. The training set consisted of 604 scans, which accounted for 60% of the total data. The validation and testing sets each contained 201 scans, making up 20% of the data each. This division was done using random stratified sampling to ensure a representative distribution of data points across all splits.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in this study is not publicly available. However, the developed models and Python scripts used in this study are available to the community in a public repository. This repository can be accessed at https://github.com/uthmri. The repository contains the necessary tools and scripts to replicate the study's findings and apply the models to new data. The availability of these resources ensures that other researchers can build upon this work and further advance the field.",
  "optimization/algorithm": "The optimization algorithm employed in our study is the Adam optimizer. Adam is a widely-used, adaptive learning rate optimization algorithm that has been proven effective in training deep learning models. It is not a new algorithm; it was introduced by Kingma and Ba in 2014. The choice of Adam was driven by its ability to adapt the learning rate for each parameter, which can lead to faster convergence and better performance, especially in high-dimensional spaces.\n\nTo enhance the convergence properties of Adam, we incorporated the AMSGrad variation. AMSGrad addresses some of the convergence issues present in the original Adam algorithm, ensuring more stable and reliable training.\n\nThe computations were performed using the Maverick2 server at the Texas Advanced Computing Center, which is equipped with NVIDIA GTX 1080Ti GPUs. To accelerate the training process, we utilized simultaneous computations across four GPUs. This setup allowed us to efficiently handle the large-scale data and complex models required for our segmentation tasks.\n\nThe learning rate was initially set at 0.001, and Adam performs an exponential reduction of the learning rate as training progresses. This dynamic adjustment helps in fine-tuning the model parameters more effectively.\n\nThe decision to use Adam and its variant, AMSGrad, was based on their proven track record in the field of deep learning and their suitability for the specific challenges posed by our segmentation tasks. While these algorithms are well-established, their application in our context is novel due to the specific requirements and characteristics of the medical imaging data we were working with.",
  "optimization/meta": "Not applicable. The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The model is a multi-class 3D U-net, which is a type of deep learning architecture specifically designed for image segmentation tasks. It uses various MRI image contrasts as inputs, including FLAIR, PD-weighted, T2-weighted, and pre- and post-contrast T1-weighted images. The model was trained using a dataset of 1006 patients, with 60% of the data used for training, 20% for validation, and 20% for testing. The training data was randomly sampled in a stratified manner to ensure independence.",
  "optimization/encoding": "The data used for the machine-learning algorithm underwent several preprocessing steps before being encoded for input into the network. The images were acquired from multiple platforms at both 1.5 T and 3 T field strengths, ensuring a diverse dataset. The imaging protocol included various sequences such as 2D FLAIR, 2D dual echo turbo spin echo, and pre- and post-contrast T1-weighted images, all with identical geometry and voxel dimensions.\n\nPreprocessing involved several key steps. Anisotropic diffusion filtering was applied to reduce noise while preserving edges. Co-registration was performed to align all images with the dual echo images. Skull stripping was conducted to remove non-brain tissues, and bias field correction was applied to address intensity inhomogeneities. Intensity normalization was then performed to standardize the image intensities across different scans.\n\nFor the machine-learning algorithm, three different models were trained using varying numbers of input channels. The U5 model utilized five input channels, incorporating FLAIR, PD-weighted, T2-weighted, and pre- and post-contrast T1-weighted images. The U2 model used two input channels, consisting of pre- and post-contrast T1-weighted images. The U1 model employed a single input channel, using only the post-contrast T1-weighted images. The input layer accepted image patches of size 128\u00d7128\u00d78, which were sampled from the image volume. This resolution was chosen to allow for good segmentation of enhancements spanning several slices without exceeding computational memory constraints.\n\nLeaky rectified linear unit activation was used at all convolutional layers to introduce non-linearity and help the network learn complex patterns. Context modules, which contained 3\u00d73\u00d73 convolutional layers along with dropout, were included to reduce overfitting and improve generalization. The number of convolutional layers in all modules was doubled compared to a previously proposed architecture to increase the network's capacity and handle the large data size effectively.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "The input features for the network models varied depending on the specific model used. The U5 model utilized five input channels, corresponding to five different image contrasts: FLAIR, PD-weighted, T2-weighted, and pre- and post-gadolinium T1-weighted images. The U2 model used two input channels, specifically the pre- and post-gadolinium T1-weighted images. The U1 model employed a single input channel, which was the post-gadolinium T1-weighted image.\n\nFeature selection was implicitly performed by choosing different combinations of input images for the models. This selection was based on the availability and relevance of these images in clinical MRI protocols for multiple sclerosis. The feature selection process was conducted using the entire dataset, including the training, validation, and testing sets, to ensure that the models could generalize well to new data. The input layer accepted image patches of size 128\u00d7128\u00d78, which were sampled from the image volume. This resolution was chosen to allow for good segmentation of enhancements spanning several slices without exceeding computational memory constraints.",
  "optimization/fitting": "The fitting method employed in this study utilized a multi-class 3D U-net architecture, which is designed to handle high-dimensional data efficiently. The input layer accepted image patches of size 128\u00d7128\u00d78, which allowed for good segmentation of enhancements spanning several slices without exceeding computational memory constraints. This architecture incorporates residual connections and dropout layers to improve segmentation performance and reduce overfitting.\n\nTo address the potential issue of overfitting, given the large number of parameters relative to the training points, several strategies were implemented. Dropout layers were included in the context modules to randomly deactivate a fraction of neurons during training, which helps prevent the model from becoming too reliant on specific features. Additionally, the use of residual connections alleviates the vanishing gradient problem, ensuring that network weights continue to update effectively. The network was trained using a \"best model\" algorithm, where the network weights that yielded the lowest error on validation data were kept. This approach helps in selecting a model that generalizes well to unseen data, further mitigating overfitting.\n\nUnderfitting was addressed by ensuring that the network had sufficient capacity to learn the complex patterns in the data. The number of convolutional layers was doubled relative to a baseline architecture, increasing the network's capacity. The use of multiple image contrasts (FLAIR, PD-weighted, T2-weighted, pre- and post-Gd T1-weighted) as inputs provided rich contextual information, enabling the network to learn more robust features. The training process involved a large dataset of 1006 patients, with 604 scans used for training, 201 for validation, and 201 for testing. This extensive dataset helped the model to learn generalizable features, reducing the risk of underfitting.\n\nThe optimizer used was Adam, which performs exponential reduction of the learning rate as training progresses, ensuring stable and efficient convergence. The AMSGrad variation of Adam was employed to further improve convergence. Computations were performed on a high-performance server with multiple GPUs, allowing for simultaneous computations and faster training times. These measures collectively ensured that the model was neither overfitted nor underfitted, achieving a balance that resulted in high segmentation accuracy.",
  "optimization/regularization": "In our study, several regularization techniques were employed to prevent overfitting and improve the generalization of our deep learning models. One key technique used was dropout layers, which were incorporated into the context modules of our network architecture. Dropout works by randomly setting a fraction of the input units to zero at each update during training time, which helps to prevent the network from becoming too reliant on any single neuron and thus reduces overfitting.\n\nAdditionally, we utilized data augmentation techniques to artificially increase the diversity of our training dataset. This involved applying various transformations such as rotations, translations, and flips to the input images, making the model more robust and less likely to overfit to the specific characteristics of the training data.\n\nAnother important regularization method we employed was the use of residual connections. These connections allow the network to learn identity mappings, which can help in training very deep networks by mitigating the vanishing gradient problem. Residual connections also aid in reducing the number of epochs required to reach optimal minima, thereby improving the overall training efficiency.\n\nFurthermore, we doubled the number of convolutional layers in all modules relative to a baseline architecture. This increase in network capacity helped the model to learn more complex features, but it was balanced by the regularization techniques mentioned above to ensure that the model did not overfit to the training data.\n\nLastly, we used a \"best model\" algorithm during training, where the network weights that yielded the lowest error on the validation data were kept. This approach ensured that the model selected for evaluation was the one that generalized best to unseen data, further reducing the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files used in this study are available to the community. These resources can be accessed through a public repository. The repository contains the developed models and the Python scripts used in the study. This allows other researchers to replicate the results and build upon the work. The availability of these resources promotes transparency and facilitates further advancements in the field. The specific details about the license under which these resources are shared are not mentioned.",
  "model/interpretability": "The model employed in this study is a multi-class 3D U-net, which is a type of deep learning architecture known for its transparency and interpretability in medical image segmentation tasks. This architecture is designed to be interpretable through its use of contracting and expanding paths, which allow for the extraction and recombination of features at different levels of abstraction.\n\nOne of the key features that contribute to the model's transparency is the use of residual connections. These connections help to alleviate the vanishing gradient problem, making the training process more efficient and the model easier to understand. Additionally, the model incorporates dropout layers, which reduce overfitting and improve generalization, further enhancing its interpretability.\n\nThe input layer of the model accepts image patches of a specific size, and the network is designed to manage feature extraction from images at multiple abstraction levels. This hierarchical feature extraction process allows for a clear understanding of how different layers of the network contribute to the final segmentation output.\n\nThe context modules in the network are composed of convolutional layers that extract features from the input images. These features are then passed through localization modules, which recombine them with up-sampled features from previous levels of abstraction. This process ensures that important features are preserved and integrated effectively throughout the network.\n\nThe segmentation layers aggregate the network's segmentation at multiple abstractions, smoothing the final output segmentation. This multi-level aggregation helps in producing a more accurate and interpretable segmentation map.\n\nThe output layer of the network is set to the same resolution as the input layer, with a softmax activation function. This design choice ensures that each voxel in the output is assigned a tissue class with the highest score, making the segmentation results clear and interpretable.\n\nOverall, the model's architecture and design choices contribute to its transparency, allowing for a clear understanding of how it processes input images and generates segmentation outputs. This transparency is crucial for medical applications, where interpretability is essential for clinical decision-making.",
  "model/output": "The model is a classification model. It is designed to segment and classify different tissues and lesions in MRI scans. Specifically, it assigns each voxel in the input image to a particular tissue class or lesion type, such as gray matter, white matter, cerebrospinal fluid, T2 lesions, or gadolinium-enhancing lesions. The output layer of the model uses a softmax activation function, which is typical for multi-class classification tasks. This allows the model to provide a probability distribution over the possible classes for each voxel, and the class with the highest probability is selected as the final classification.\n\nThe model's output is a segmentation map that matches the resolution of the input image. This map is generated by aggregating the segmentation results from multiple levels of abstraction within the network, ensuring that the final output is smooth and accurate. The model assumes that all enhancing lesions have corresponding T2 hyperintense lesions, and it combines the voxels labeled as enhancing lesions or T2 lesions to create the final T2 lesion map. This approach helps to improve the overall accuracy and reliability of the segmentation results.",
  "model/duration": "The model training was accelerated by performing simultaneous computations on four NVIDIA GTX 1080Ti GPUs hosted on the Maverick2 server at the Texas Advanced Computing Center. This setup allowed for efficient network training, although the exact execution time for the model to run is not specified. The use of multiple GPUs significantly sped up the training process, enabling us to handle the large dataset and complex network architecture effectively. The training involved a \"best model\" algorithm that retained the network weights yielding the lowest error on validation data, ensuring optimal performance.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our segmentation models was conducted using a comprehensive approach that included multiple metrics and validation techniques. We calculated class-specific accuracy using the Dice similarity coefficient (DSC) for all segmented tissues. The DSC formula used was:\n\nDSC = 2 \u00d7 (M \u2229 A) / (M + A)\n\nwhere M and A denote the ground truth and automated segmentation masks, respectively. This metric was computed over all subjects in the test set, including both Gd+ and Gd- subjects. To avoid ambiguity in Gd- cases, true-positive, false-positive, and false-negative volumes were computed for each subject and then summed over all subjects to calculate the DSC.\n\nIn addition to DSC, we evaluated the models using lesion-wise true positive rate (TPR) and false positive rate (FPR) for each category. TPR was defined as the number of true positive lesions divided by the total number of lesions in the ground truth. FPR was defined as the number of false positive lesions divided by the total number of lesions in the network segmentation.\n\nWe also investigated the dependence of segmentation accuracy on the size of the enhancing lesions. Lesions were divided into six groups based on their volume: 20\u201334 mm\u00b3, 35\u201369 mm\u00b3, 70\u2013137 mm\u00b3, 138\u2013276 mm\u00b3, 277\u2013499 mm\u00b3, and >500 mm\u00b3. The segmentation accuracy metrics (DSC, TPR, FPR) were computed for each group. Lesions smaller than 7 voxels were excluded from the neural network segmentation to ensure reliable evaluation.\n\nThe segmentation results using the three network models (U5, U2, and U1) were summarized in tables and figures. The U5 model, which used five multispectral image sets, achieved high DSC values for all tissues, including gray matter, white matter, cerebrospinal fluid, T2 lesions, and Gd-enhancing lesions. The TPR and FPR for Gd-enhancing lesions were also reported, demonstrating the model's performance.\n\nFurthermore, we compared our results with previous studies to provide context and validate our findings. The models and Python scripts used in this study are available to the community in a public repository, ensuring reproducibility and further research.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our deep learning models in segmenting Gd-enhancing lesions and other brain tissues. The primary metric used was the Dice Similarity Coefficient (DSC), which measures the overlap between the automated segmentation and the ground truth. DSC is a widely accepted metric in the field of medical image segmentation, providing a robust measure of segmentation accuracy.\n\nIn addition to DSC, we calculated the true-positive rate (TPR) and false-positive rate (FPR) for each category of lesions. TPR, also known as sensitivity or recall, indicates the proportion of actual positive cases that are correctly identified by the model. FPR, on the other hand, represents the proportion of negative cases that are incorrectly classified as positive. These metrics are crucial for understanding the model's performance in detecting lesions, especially in clinical settings where both false positives and false negatives can have significant implications.\n\nWe also investigated the dependence of segmentation accuracy on lesion size, dividing lesions into six size groups. This analysis helped us understand how well our models perform across different lesion sizes, which is important given the variability in lesion sizes in clinical data.\n\nOur choice of metrics is representative of the current literature in medical image segmentation. DSC is commonly used due to its ability to provide a single value that summarizes the overlap between two segmentations. TPR and FPR are also standard metrics that offer insights into the model's sensitivity and specificity. By reporting these metrics, we aim to provide a comprehensive evaluation of our models' performance, making it comparable to other studies in the field.",
  "evaluation/comparison": "Not applicable. The study did not compare the proposed method to publicly available methods on benchmark datasets. Instead, it focused on evaluating the performance of different network models (U5, U2, U1) using various metrics such as Dice similarity coefficient (DSC), true positive rate (TPR), and false positive rate (FPR). The evaluation was conducted on a large dataset of MRI scans, with the models' performance assessed against ground truth segmentations validated by experts. The study also discussed the limitations of the current approach and suggested areas for future improvement, such as the inclusion of advanced MRI sequences and manual delineation of enhancements. However, it did not provide a direct comparison to simpler baselines or other publicly available methods.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being available. However, the developed models and Python scripts used in this study are available to the community in a public repository. This repository can be accessed at https://github.com/uthmri. The specific details about the license under which these materials are released are not provided."
}