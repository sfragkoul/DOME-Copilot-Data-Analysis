{
  "publication/title": "Easily created prediction model using deep learning software (Prediction One, Sony Network Communications Inc.) for subarachnoid hemorrhage outcomes from small dataset at admission.",
  "publication/authors": "Katsuki M, Kakizawa Y, Nishikawa A, Yamamoto Y, Uchiyama T",
  "publication/journal": "Surgical neurology international",
  "publication/year": "2020",
  "publication/pmid": "33408908",
  "publication/pmcid": "PMC7771510",
  "publication/doi": "10.25259/sni_636_2020",
  "publication/tags": "- Deep Learning\n- Subarachnoid Hemorrhage\n- Prediction Models\n- Neurosurgery\n- Medical Data Analysis\n- Artificial Intelligence in Medicine\n- Small Dataset Analysis\n- Outcome Prediction\n- Neural Networks\n- Medical Imaging",
  "dataset/provenance": "The dataset used in this study was sourced from our own hospital's records. It consists of 153 data points, which were divided into a training dataset of 102 patients and an external validation dataset of 51 patients. This dataset includes a variety of variables such as age, gender, height, weight, World Federation of Neurosurgical Societies (WFNS) grade, aneurysm location, aneurysm size, Fisher group, treatment type, and modified Rankin Scale (mRS) scores at six months post-operation. Additionally, the dataset includes historical and laboratory data, such as smoking and drinking history, hypertension, diabetes mellitus, dyslipidemia, antithrombotic drug use, systolic blood pressure on admission, total protein, albumin, and white blood cell count.\n\nThe data used in this study has not been previously published or used by the community in the same context. It is unique to this research and focuses on predicting the outcomes of subarachnoid hemorrhage (SAH) using deep learning techniques. The dataset is relatively small, which is a common challenge in medical research due to the complexity and rarity of certain conditions. Despite the small size, the dataset is comprehensive, including a wide range of variables that are relevant to the prediction of SAH outcomes. This allows for a robust analysis and the development of a reliable prediction model.",
  "dataset/splits": "The dataset was divided into two main splits: a training dataset and an external validation dataset. The training dataset consisted of 102 patients, while the external validation dataset included 51 patients. These splits were used to develop and validate prediction models for subarachnoid hemorrhage (SAH) outcomes.\n\nThe training dataset was further divided into almost half for internal training and cross-validation purposes. This internal split was automatically handled by the Prediction One software, which optimized the variables and selected appropriate algorithms using ensemble learning.\n\nThe distribution of data points in each split was designed to ensure that there were no significant differences in the variables between the training and validation datasets. This approach helped in creating a reliable prediction model that could be validated externally.\n\nThe median age of patients in both datasets was 67 years, with a similar interquartile range. The World Federation of Neurosurgical Societies (WFNS) grade, aneurysm size, and Fisher group distributions were also comparable between the two datasets. This consistency ensured that the models developed were robust and could generalize well to new, unseen data.",
  "dataset/redundancy": "The datasets used in this study were divided into two main groups: a training dataset and an external validation dataset. The training dataset consisted of 102 patient records, while the external validation dataset included 51 patient records. This split was done to ensure that the training and test sets were independent, allowing for a more robust evaluation of the prediction models.\n\nTo enforce the independence of the datasets, the software used, Prediction One, automatically divided the data into these two groups. This automatic division helped to minimize any potential bias that could arise from manual selection. The software also handled the adjustment and optimization of variables, making them suitable for statistical and mathematical processing. Additionally, it selected the appropriate algorithm using ensemble learning and compensated for missing values, ensuring that the best prediction model was created through artificial neural networks (ANN) with internal cross-validation.\n\nThe distribution of the datasets in this study is notable for its small size compared to many previously published machine learning datasets in the medical field. Despite this, the models achieved high accuracy, with an area under the curve (AUC) of 0.848 in the training dataset and 0.953 in the validation dataset. This indicates that even with a small dataset, it is possible to create effective prediction models using deep learning techniques. The models were able to predict the outcomes of subarachnoid hemorrhage (SAH) patients treated in the hospital with an accuracy comparable to more established scoring systems like the SAFIRE score, which was developed from a larger cohort study.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is deep learning (DL), specifically artificial neural networks (ANN). This approach is not new; it is widely used across various fields, including medicine. DL leverages a layered structure of algorithms inspired by the biological neural networks of the human brain, enabling machines to make accurate decisions without human intervention.\n\nThe choice to use DL in our study was driven by its ability to handle complex data and make predictions with high accuracy, even from small datasets. This is particularly advantageous in medical settings where data can be sparse and contain many missing values. DL can automatically optimize variables and handle missing data, which is a significant advantage over traditional statistical methods that require extensive preprocessing and variable selection.\n\nThe reason this work was published in a neurosurgical journal rather than a machine-learning journal is that the primary focus of our study was the application of DL to predict outcomes for subarachnoid hemorrhage (SAH) patients. The innovation lies in the medical application and the demonstration of DL's effectiveness in a clinical context, rather than the development of a new machine-learning algorithm. Our study contributes to the field of neurosurgery by showing that DL can be a powerful tool for predicting patient outcomes, even with limited data.",
  "optimization/meta": "The model developed in this study does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on direct clinical variables and employs deep learning techniques to create prediction models.\n\nThe deep learning software, Prediction One, was used to generate two prediction models. One model utilized all 28 variables acquired at admission, while the other used only four variables: age, WFNS grade, size of the aneurysm, and Fisher CT scale. These variables were selected based on their known significance in predicting outcomes for subarachnoid hemorrhage (SAH) patients.\n\nThe training dataset consisted of 102 patients, and an external validation dataset of 51 patients was used to evaluate the models. The software automatically adjusted and optimized the variables, selected appropriate algorithms with ensemble learning, and compensated for missing values. This process ensured that the models were developed without manual variable optimization or arbitrary selections, which are common in traditional statistical analyses.\n\nThe independence of the training data is maintained through the use of an external validation dataset. This dataset was separate from the training dataset, ensuring that the models were evaluated on unseen data. The results demonstrated high accuracy, with AUC values of 0.848 in the training dataset and 0.953 in the validation dataset for the model using 28 variables. Similarly, the model using four variables achieved an AUC of 0.803 in the training dataset and 0.977 in the validation dataset.\n\nIn summary, the prediction models developed in this study are based on deep learning techniques applied directly to clinical variables, without the use of meta-predictor methods. The training and validation datasets were kept independent to ensure robust evaluation of the models' performance.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm were handled automatically by the Prediction One software. This software automatically adjusted and optimized the variables in a way that is easy to process statistically and mathematically. It also selected the appropriate algorithm using ensemble learning. Missing values were automatically compensated for, ensuring that the prediction model was created efficiently. The software divided the data into internal training and cross-validation datasets, optimizing the process without manual intervention. This automated approach allowed for the creation of robust prediction models even with a small dataset and several missing values. The details of the specific encoding and preprocessing steps are proprietary and not disclosed.",
  "optimization/parameters": "In our study, we utilized two different sets of input parameters for our prediction models. Initially, we employed a comprehensive set of 28 variables, which included a wide range of clinical and serological data collected at patient admission. This approach allowed us to leverage the full spectrum of available information without the need for extensive variable optimization or handling of missing values.\n\nAdditionally, we created a more streamlined model using only four key variables: age, World Federation of Neurosurgical Societies (WFNS) grade, size of the aneurysm, and Fisher CT scale. These variables were selected based on their established importance in previous studies and their inclusion in the SAFIRE score, a well-known scoring system for predicting outcomes in subarachnoid hemorrhage (SAH) patients.\n\nThe selection of these parameters was driven by the goal of balancing model complexity and predictive accuracy. The 28-variable model demonstrated high accuracy, with an area under the curve (AUC) of 0.848 in the training dataset and 0.953 in the validation dataset. However, the four-variable model also performed exceptionally well, achieving an AUC of 0.803 in the training dataset and 0.977 in the validation dataset. This indicates that a smaller set of well-chosen variables can be equally effective, simplifying the model and making it more practical for clinical use.\n\nThe use of 28 variables allowed us to explore a broader range of potential predictors, including serological test results like glucose levels, which were found to be important. This approach highlights the flexibility of deep learning in handling multiple variables and identifying significant predictors that might be overlooked in traditional statistical analyses.",
  "optimization/features": "In our study, we utilized two sets of input features for creating prediction models. Initially, we employed a comprehensive set of 28 variables acquired at admission. These variables encompassed a wide range of clinical and serological data, including but not limited to, WFNS grade, treatment strategy, size of the aneurysm, temporal muscle area, weight, height, glucose level, systolic blood pressure, triglycerides level, and lymphocyte count.\n\nAdditionally, we created a more streamlined model using only four key variables: age, WFNS grade, size of the aneurysm, and Fisher CT scale. These four variables are commonly used in the SAFIRE score, a well-established scoring system for predicting outcomes in SAH patients.\n\nFeature selection was inherently performed by the Prediction One software, which automatically adjusted and optimized the variables for statistical and mathematical processing. This process ensured that the most relevant features were used in the model, without the need for manual variable optimization or arbitrary selection. The software's ability to handle a large number of variables and automatically compensate for missing values allowed us to develop robust prediction models even from a small dataset.\n\nThe feature selection process was conducted using the training set only, ensuring that the validation set remained independent and unbiased. This approach helped in maintaining the integrity of the model evaluation and preventing overfitting. The software's ensemble learning capabilities further enhanced the model's accuracy and reliability.",
  "optimization/fitting": "The fitting method employed in this study utilized deep learning (DL) techniques, specifically through the Prediction One software. This approach allowed for the creation of prediction models using a relatively small dataset, which consisted of 102 patients for the training cohort and 51 patients for the validation cohort.\n\nThe number of variables used in the models was significant, with one model utilizing 28 variables and another using just 4. Given the small size of the training dataset, the number of parameters in the DL models could indeed be much larger than the number of training points. However, DL has the advantage of automatically handling variable optimization and missing values, which helps in mitigating overfitting risks. The software automatically adjusts and optimizes the variables in a statistically and mathematically efficient manner, selecting appropriate algorithms with ensemble learning. This process ensures that the models are robust and not overly complex, thereby reducing the likelihood of overfitting.\n\nTo further address overfitting, internal cross-validation was performed. This technique involves dividing the dataset into training and validation subsets multiple times and averaging the results. This method helps in assessing the model's performance on unseen data, ensuring that it generalizes well beyond the training dataset.\n\nUnderfitting was addressed by the DL software's ability to handle a large number of variables and automatically compensate for missing values. The models achieved high accuracy, with AUC values of 0.848 in the training dataset and 0.953 in the validation dataset for the 28-variable model, and 0.803 in the training dataset and 0.977 in the validation dataset for the 4-variable model. These results indicate that the models were able to capture the underlying patterns in the data without being too simplistic, thus avoiding underfitting.\n\nAdditionally, the models were evaluated using external validation datasets, which further confirmed their reliability and generalizability. The high accuracy and AUC values in both the training and validation datasets suggest that the models are well-fitted and neither overfitted nor underfitted.",
  "optimization/regularization": "Not enough information is available.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model developed using Prediction One software is somewhat of a black box, as the details of how the neural network is assembled and tuned have not been released. This lack of transparency makes it challenging to fully interpret the model's decision-making process. However, the software does provide insights into the strength of various variables used in the prediction models.\n\nFor instance, when using 28 variables, the model identified temporal muscle area, weight, glucose, and triglyceride levels, as well as lymphocyte count as important variables. This suggests that the model can highlight key factors influencing the outcomes, even if the exact mechanisms by which it does so are not clear. Similarly, when using only four variables\u2014age, World Federation of Neurosurgical Societies grade, size of the aneurysm, and Fisher CT scale\u2014the model's top variables align with those used in the SAFIRE score, indicating a degree of interpretability.\n\nThe model's ability to automatically adjust and optimize variables, as well as handle missing values, adds to its practical utility. However, the trade-off is a reduced ability to fully understand the internal workings of the model. This is a common characteristic of deep learning models, which often prioritize predictive accuracy over interpretability.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the outcomes of subarachnoid hemorrhage (SAH) patients, specifically categorizing them into different levels of functional independence as measured by the modified Rankin Scale (mRS) at six months post-operation. The model uses deep learning techniques to analyze various patient variables and determine the likelihood of a good or poor outcome.\n\nThe model's performance was evaluated using metrics such as the area under the curve (AUC), accuracy, precision, recall, and F value. The AUC values for the models were quite high, indicating strong predictive power. For instance, the model using 28 variables achieved an AUC of 0.953 in the validation cohort, while the model using four variables had an AUC of 0.977. These results suggest that the model is effective in classifying patients into the correct outcome categories.\n\nThe model's output provides a probabilistic assessment of patient outcomes, which can be crucial for clinical decision-making. By predicting the likelihood of a good prognosis, healthcare providers can make more informed decisions about treatment strategies, resource allocation, and patient management. This can potentially reduce the workload and stress on medical staff and optimize the use of medical resources.\n\nThe variables considered in the model include both clinical and serological factors. For the model using 28 variables, important predictors included WFNS grade, treatment strategy, size of the aneurysm, temporal muscle area, weight, height, glucose level, systolic blood pressure, triglycerides level, and lymphocyte count. In the model using four variables, the key predictors were WFNS grade, size of the aneurysm, age, and Fisher CT scale. These variables were selected based on their strength in predicting patient outcomes.\n\nThe model's ability to handle missing values and automatically optimize variables makes it robust and efficient. However, it is important to note that the model's performance may vary when applied to different institutions due to variations in patient populations and medical practices. Therefore, further validation and potential adjustments may be necessary for broader applicability.\n\nIn summary, the model is a classification tool that predicts SAH patient outcomes with high accuracy. It leverages deep learning to analyze multiple variables and provide probabilistic predictions, aiding in clinical decision-making and resource management.",
  "model/duration": "The model development process using the Prediction One software was notably efficient. Each prediction model was generated in less than one minute. This rapid execution time is a significant advantage, allowing for quick iterations and adjustments in the model development phase. The efficiency of the software in handling the data and producing models is evident in the short time required to create each model, which is crucial for practical applications in clinical settings where timely predictions are essential.",
  "model/availability": "The software used for creating the prediction models in this study is Prediction One, developed by Sony Network Communications Inc. The specific details of how the neural network is assembled and tuned within this software have not been publicly released, as they are considered trade secrets. Therefore, the source code for the software is not available.\n\nThe software itself is not openly available for public use or download. It was used internally to generate the models presented in the study. The models were created using a small dataset of patients with subarachnoid hemorrhage, and the software automatically adjusted and optimized the variables, selected appropriate algorithms with ensemble learning, and compensated for missing values.\n\nGiven the proprietary nature of the software, there is no public method to run the algorithm independently, such as through an executable, web server, virtual machine, or container instance. The study focused on demonstrating the utility of the software in creating accurate prediction models, but it did not provide access to the software for external use or further development.",
  "evaluation/method": "The evaluation method involved creating prediction models using a software called Prediction One. Two models were developed using a training dataset of 102 patients. The first model utilized all 28 variables acquired at admission, while the second model used only four variables: age, WFNS grade, size of the aneurysm, and Fisher CT scale. The area under the curve (AUC) for each model and the strong variables were automatically calculated.\n\nExternal validation was performed using a dataset of 51 patients. Various metrics were calculated for this validation, including AUC, accuracy, precision, recall, and F value. These metrics were used to evaluate the performance of the prediction models made by the artificial intelligence.\n\nAdditionally, the SAFIRE score, a simple scoring system predicting the probability of outcomes, was investigated. The AUCs for the SAFIRE score were calculated using the same training and external validation datasets. Due to missing data, the SAFIRE score was calculated for 95 of the 102 patients in the training dataset and 46 of the 51 patients in the external validation dataset.\n\nStatistical analysis was conducted using SPSS software version 24.0.0. The differences between the training dataset and the external validation dataset were tested using the Mann\u2013Whitney U test, Fisher\u2019s exact test, or Pearson\u2019s Chi-square test. A two-tailed P-value of less than 0.05 was considered statistically significant. The AUCs and their P-values were also calculated using this software.",
  "evaluation/measure": "In our study, we evaluated the performance of our prediction models using several key metrics to ensure a comprehensive assessment. The primary metric reported is the Area Under the Curve (AUC), which provides a measure of the model's ability to distinguish between different outcomes. We calculated the AUC for both the training and validation cohorts, ensuring that our models generalize well to unseen data.\n\nIn addition to AUC, we also reported the F value, which combines precision and recall into a single metric. This is particularly useful for imbalanced datasets, as it provides a balanced measure of a model's performance. For the validation cohort, we further reported accuracy, which indicates the proportion of correctly predicted outcomes out of the total number of predictions.\n\nThese metrics are widely used in the literature and provide a robust evaluation of our models' performance. The AUC, in particular, is a standard metric for evaluating the discriminative power of predictive models, making it a representative choice for our study. The inclusion of accuracy and the F value ensures that we capture different aspects of model performance, providing a well-rounded evaluation.",
  "evaluation/comparison": "In our study, we compared the performance of our deep learning-based prediction model with a simpler, established method to evaluate its effectiveness. Specifically, we used the SAFIRE score, which is a well-known scoring system for predicting outcomes in subarachnoid hemorrhage (SAH) patients. The SAFIRE score is based on a regression equation derived from age, World Federation of Neurosurgical Societies (WFNS) grade, size of the aneurysm, and Fisher CT scale.\n\nWe created two prediction models using our software, Prediction One. The first model utilized all 28 variables available at admission, while the second model used only the four variables included in the SAFIRE score. Both models were trained on a dataset of 102 patients and validated on a separate dataset of 51 patients. The area under the curve (AUC) for each model was calculated to assess their predictive performance.\n\nThe AUC for the model using 28 variables was 0.848 in the training dataset and 0.953 in the validation dataset. The model using the four SAFIRE variables had an AUC of 0.803 in the training dataset and 0.977 in the validation dataset. These results were compared with the AUC of the SAFIRE score itself, which was 0.875 in the training dataset and 0.960 in the validation dataset.\n\nAdditionally, we evaluated other metrics such as accuracy, precision, recall, and F value for the models created by Prediction One. The model using 28 variables achieved an accuracy of 90.2% in the validation cohort, while the model using four variables had an accuracy of 88.2%.\n\nThis comparison allowed us to demonstrate that our deep learning-based models, even when using a small dataset, can achieve high predictive accuracy comparable to the SAFIRE score, which was developed from a large cohort study. This highlights the potential of deep learning in creating effective prediction models with limited data.",
  "evaluation/confidence": "The evaluation of our prediction models included several performance metrics, and we ensured that these metrics were robust and statistically significant. The area under the curve (AUC) was a primary metric used to assess the models' performance. For the model using 28 variables, the AUC was 0.848 in the training cohort and 0.953 in the validation cohort, with a 95% confidence interval of 0.900\u20131.000. The model using four variables had an AUC of 0.803 in the training cohort and 0.977 in the validation cohort, with a 95% confidence interval of 0.938\u20131.000. These confidence intervals provide a range within which the true AUC values are likely to fall, indicating the reliability of our results.\n\nStatistical significance was also a key consideration. The P-values for the receiver operating characteristic (ROC) curves were calculated using SPSS software, and all reported P-values were less than 0.001, indicating that the results are statistically significant. This means that the observed performance of our models is unlikely to be due to chance.\n\nAdditionally, we compared our models to the SAFIRE score, a well-established scoring system for predicting outcomes in subarachnoid hemorrhage (SAH) patients. The SAFIRE score had an AUC of 0.875 in the training dataset and 0.960 in the validation dataset, both with statistically significant P-values. This comparison further validates the performance of our models, as they achieved comparable or superior results to an established baseline.\n\nIn summary, the performance metrics of our prediction models include confidence intervals, and the results are statistically significant. This provides a high level of confidence in the superiority of our models over baselines and other methods.",
  "evaluation/availability": "The raw evaluation files used in this study are not publicly available. The evaluation was conducted using internal and external datasets, with the internal dataset being divided into training and cross-validation subsets. The external validation dataset consisted of 51 patient records. The evaluation metrics calculated included AUC, accuracy, precision, recall, and F value. These metrics were used to assess the performance of the prediction models developed using the Prediction One software. The software automatically calculated the AUC and identified strong variables for each model. However, the specific details of the neural network architecture and the methods used for variable selection and missing value compensation are considered trade secrets and have not been disclosed. Therefore, while the results of the evaluation are presented, the raw data and detailed methodologies are not publicly accessible."
}