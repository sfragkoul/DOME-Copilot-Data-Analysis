{
  "publication/title": "[Identifying Molecular Subtypes of Whole-Slide Image in Colorectal Cancer via Deep Learning].",
  "publication/authors": "Liao J, Feng XB, Wang YH, Guo LC",
  "publication/journal": "Sichuan da xue xue bao. Yi xue ban = Journal of Sichuan University. Medical science edition",
  "publication/year": "2021",
  "publication/pmid": "34323050",
  "publication/pmcid": "PMC10409381",
  "publication/doi": "10.12182/20210760501",
  "publication/tags": "- Colorectal cancer\n- Deep learning\n- Pathology\n- Image analysis\n- Convolutional neural networks\n- Molecular subtypes\n- Medical imaging\n- Machine learning\n- Cancer diagnosis\n- Digital pathology\n- Inception v3\n- ResNet50\n- Patch-based analysis\n- Model evaluation\n- Histopathology\n- Classification\n- Precision\n- Recall\n- F1-score\n- Accuracy",
  "dataset/provenance": "The dataset used in this study was sourced from a publicly available database. It includes a total of 812 whole slide images (WSIs) from 422 colorectal cancer (CRC) patients. These images were divided into a training set, which comprises 75% of the data, and a test set, which includes the remaining 25%. To ensure no overlap, each patient's slides were assigned to either the training or test set, but not both. The dataset is stored in a specific storage system.\n\nThe dataset is composed of four molecular subtypes of CRC: microsatellite instability immune type (CMS1), canonical type (CMS2), metabolic type (CMS3), and mesenchymal type (CMS4). The distribution of these subtypes in the training and test sets is as follows:\n\n- CMS1: 99 in the training set, 36 in the test set\n- CMS2: 266 in the training set, 88 in the test set\n- CMS3: 82 in the training set, 26 in the test set\n- CMS4: 162 in the training set, 53 in the test set\n\nThe labeling process involved the use of a multifunctional annotation tool. Two experienced pathologists from a medical institution annotated the 812 CRC samples based on criteria such as stromal invasion, lymph node metastasis, and distant metastasis. A third pathologist reviewed the annotations to ensure accuracy. This meticulous labeling process resulted in high-quality annotations for the dataset.\n\nThe dataset has been used in this specific study to train and test models for molecular subtyping of colorectal cancer using deep learning techniques. The training set consists of 609 WSIs from 312 patients, while the test set includes 203 WSIs from 110 patients. The models were trained on these annotated images to predict the molecular subtypes of CRC.",
  "dataset/splits": "The dataset consists of 812 whole slide images (WSIs) from 422 colorectal cancer (CRC) patients. These images are divided into two main splits: a training set and a test set. The training set comprises 75% of the data, totaling 609 WSIs from 312 patients. The test set includes the remaining 25%, amounting to 203 WSIs from 110 patients. This division ensures that no single patient's data appears in both the training and test sets, preventing data leakage and ensuring robust model evaluation.\n\nThe dataset is further categorized into four molecular subtypes of colorectal cancer: CMS1 (microsatellite instability immune), CMS2 (canonical), CMS3 (metabolic), and CMS4 (mesenchymal). The distribution of these subtypes in the training and test sets is as follows:\n\n* Training set: 99 CMS1, 266 CMS2, 82 CMS3, and 162 CMS4.\n* Test set: 36 CMS1, 88 CMS2, 26 CMS3, and 53 CMS4.\n\nThis distribution reflects the overall dataset composition, with CMS2 being the most prevalent subtype and CMS3 the least. The dataset was annotated by experienced pathologists from the First Affiliated Hospital of Soochow University, ensuring high-quality labels for model training and evaluation. The annotations were verified by another senior pathologist to maintain accuracy and consistency.",
  "dataset/redundancy": "The dataset used in this study consists of 812 whole slide images (WSIs) from 422 colorectal cancer (CRC) patients. These images were obtained from a public database and were divided into a training set and a test set. The training set comprises 75% of the data, totaling 609 WSIs from 312 patients, while the test set includes the remaining 25%, consisting of 203 WSIs from 110 patients. This split ensures that the training and test sets are independent, with no overlap of patient data between the two sets. This independence is crucial for evaluating the model's performance on unseen data.\n\nThe distribution of the four molecular subtypes of colorectal cancer (CMS1, CMS2, CMS3, and CMS4) in the training and test sets is as follows: CMS1 has 99 and 36 samples, CMS2 has 266 and 88 samples, CMS3 has 82 and 26 samples, and CMS4 has 162 and 53 samples, respectively. This distribution reflects the actual prevalence of these subtypes in the dataset, providing a realistic representation for model training and evaluation.\n\nThe labeling of the samples was conducted using a multi-functional annotation tool. Two experienced pathologists from a hospital annotated the 812 CRC samples based on specific criteria, and a third pathologist reviewed the annotations to ensure accuracy. This rigorous annotation process ensures the reliability of the dataset.\n\nThe dataset's split and distribution are designed to mimic real-world scenarios, where the model needs to generalize from a training set to a test set without any data leakage. This approach aligns with best practices in machine learning, ensuring that the model's performance is a true reflection of its ability to handle new, unseen data.",
  "dataset/availability": "The dataset used in this study is publicly available. It includes 812 whole slide images (WSIs) from 422 colorectal cancer (CRC) patients, which were obtained from a specific database. The dataset is divided into a training set comprising 75% of the data and a test set comprising the remaining 25%. To ensure that there is no overlap between the training and test sets, the images from the same patient are only included in one of the sets.\n\nThe dataset is stored in the www.paiwsit.com storage system. The images are labeled by medical professionals from a hospital, with detailed annotations based on criteria such as matrix infiltration, lymph node metastasis, and distant metastasis. The labeling process involves multiple experts to ensure high-quality annotations.\n\nThe dataset includes four subtypes of CRC: microsatellite instability immune type (CMS1), classic type (CMS2), metabolic type (CMS3), and mesenchymal type (CMS4). The distribution of these subtypes in the training and test sets is provided in a table, ensuring transparency and reproducibility.\n\nThe dataset is made available to the public, allowing other researchers to access and use it for similar studies. The specific licensing details for the dataset are not mentioned, but it is implied that the data is accessible through the storage system. The enforcement of the data split is managed by ensuring that images from the same patient are not included in both the training and test sets, maintaining the integrity of the evaluation process.",
  "optimization/algorithm": "The optimization algorithms used in this study are Adam and Adadelta. These are well-established optimization algorithms commonly used in training deep learning models. They belong to the class of adaptive learning rate optimization algorithms, which adjust the learning rate during training to improve convergence and performance.\n\nNeither Adam nor Adadelta are new algorithms; they have been widely adopted in the machine learning community for several years. Adam, in particular, is known for its efficiency and effectiveness in handling sparse gradients on noisy problems. It combines the advantages of two other extensions of stochastic gradient descent, namely AdaGrad and RMSProp.\n\nThe choice of these optimization algorithms is driven by their proven track record in reducing the training set's cross-entropy loss function value, which is crucial for improving the model's performance. The decision to use these specific algorithms was likely influenced by their ability to handle the complexities of the data and the model architectures employed in this research.\n\nThe focus of this publication is on the application of these optimization algorithms within the context of predicting molecular subtypes of colorectal cancer using convolutional neural networks. The results demonstrate their effectiveness in achieving high accuracy on the training set and reasonable performance on the test set, which is the primary goal of the study.",
  "optimization/meta": "The model described in this publication does not use data from other machine-learning algorithms as input. It is not a meta-predictor. Instead, it directly utilizes convolutional neural networks (CNNs) to analyze whole slide images (WSIs) of colorectal cancer. The primary models evaluated include VGG16, VGG19+Dropout, VGG24+Dropout, VGG24+BN+Dropout, Inception v3, Resnet18, Resnet34, and Resnet50.\n\nThese models are trained and evaluated independently on the same dataset of colorectal cancer WSIs. The training process involves techniques such as Dropout, L2 regularization, and batch normalization to prevent overfitting. Optimization methods like Adam or Adadelta are used to minimize the cross-entropy loss function during training.\n\nThe evaluation metrics for these models include accuracy, precision, recall, and F1-score, which are calculated based on the performance of the models on both training and test sets. The results indicate that Inception v3 and ResNet50 generally perform better than the other models in predicting the molecular subtypes of colorectal cancer from WSIs.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to handle the large size and complexity of whole slide images (WSIs) of colorectal cancer. Initially, the WSIs, which typically exceed 10,000 pixels in width, were compressed and reduced in size to manageable dimensions. The images were then converted from the RGB color space to the HSV color space, focusing on the saturation channel, which was subsequently binarized and processed to retain only the effective tissue regions. This step was crucial for filtering out non-tissue areas and conserving computational resources.\n\nFollowing this, the images underwent a patch extraction process. Using a 64-pixel stride, overlapping patches of 256x256 pixels were generated from the full-size WSIs. These patches were selected based on a threshold where at least 50% of the patch area contained annotated tissue. This method ensured a high-quality dataset for training, with millions of patches generated to balance the data across different molecular subtypes of colorectal cancer.\n\nTo address the issue of class imbalance and overfitting, data augmentation techniques such as horizontal and vertical flips were applied to the training samples. This augmentation helped in creating a more balanced and robust dataset, essential for training deep learning models effectively.\n\nThe preprocessing steps were designed to optimize the input data for convolutional neural networks (CNNs), ensuring that the models could learn meaningful features from the WSIs. The use of techniques like dropout, L2 regularization, and batch normalization further aided in preventing overfitting and improving the generalization of the models. The final encoded data consisted of high-quality, balanced patches ready for input into various CNN architectures, including VGG, Inception, and ResNet models, for training and evaluation.",
  "optimization/parameters": "In our study, we employed several convolutional neural network (CNN) architectures, each with its own set of parameters. The specific number of parameters varied depending on the model used. For instance, the Inception v3 model, known for its efficient use of parameters, had a structure designed to reduce the number of parameters and mitigate overfitting. This was achieved by splitting 2D convolutions into two 1D convolutions, which helped in lowering the parameter count.\n\nThe ResNet50 model, another key architecture in our research, utilized a residual learning framework. This framework allowed for the direct transmission of input information to the output, preserving the integrity of the data and easing the training of deep networks. The ResNet50 architecture, as detailed in our tables, included multiple convolutional layers with varying kernel sizes and strides, contributing to its overall parameter count.\n\nThe selection of these models and their parameters was guided by the need to balance model complexity and performance. We aimed to ensure that the models were deep enough to capture the intricate patterns in the data but not so complex that they would overfit. Techniques such as Dropout, L2 regularization, and batch normalization were employed to further prevent overfitting and enhance the models' generalization capabilities.\n\nDuring training, we used optimizers like Adam and Adadelta to minimize the cross-entropy loss on the training set. The batch size was set to values like 128 or 32, depending on the model, and the learning rate was fixed at 0.01. These hyperparameters were chosen based on empirical results and common practices in the field to ensure efficient and effective training.\n\nIn summary, the number of parameters in our models was determined by the architecture chosen, and the selection of these architectures was based on their proven performance in similar tasks. The use of regularization techniques and careful tuning of hyperparameters further ensured that our models were well-suited for the task of predicting molecular subtypes of colorectal cancer from whole slide images.",
  "optimization/features": "The input features for the models used in this study are derived from whole slide images (WSIs) of colorectal cancer tissue samples. These images are preprocessed to extract relevant features. The preprocessing involves several steps:\n\nFirst, the WSIs are downsampled to create thumbnail images. The color space of these images is converted from RGB to HSV, and the saturation channel is retained and binarized. This process helps in identifying the effective tissue regions, which typically constitute about 30% to 40% of the total area of a WSI. Non-tissue areas are then filtered out.\n\nNext, patches of size 256x256 pixels are extracted from the WSIs using a sliding window approach with a step size of 64 pixels. These patches are further processed to ensure that at least 50% of each patch contains annotated tissue regions. This results in a large number of training samples, often reaching the millions.\n\nTo address the issue of class imbalance among the different colorectal cancer molecular subtypes (CMS1-4), data augmentation techniques such as horizontal and vertical flips are applied to the training set. This helps in balancing the dataset and improving the model's robustness.\n\nThe final input features for the convolutional neural networks (CNNs) are these 256x256 pixel RGB patches. No explicit feature selection is performed beyond the preprocessing steps described. The feature extraction and selection are inherently handled by the CNN architectures, which include various layers designed to capture relevant patterns and features from the input images.\n\nThe models used in this study include VGG16, VGG19+Dropout, VGG24+Dropout, VGG24+BN+Dropout, Inception v3, Resnet18, Resnet34, and Resnet50. Each of these models has its own architecture and training strategy, but they all utilize the same preprocessed input features. The training strategies involve techniques such as dropout, batch normalization, and regularization to prevent overfitting. Optimization methods like Adam and Adadelta are used to minimize the cross-entropy loss during training.",
  "optimization/fitting": "In our study, we employed several strategies to address potential overfitting and underfitting issues. Overfitting occurs when a model learns the training data too well, including its noise and outliers, leading to poor generalization on new data. Conversely, underfitting happens when a model is too simple to capture the underlying patterns in the data.\n\nTo mitigate overfitting, we utilized techniques such as Dropout, L2 regularization, and batch normalization. Dropout randomly sets a fraction of input units to zero at each update during training time, which helps prevent over-reliance on specific neurons. L2 regularization adds a penalty term to the loss function based on the magnitude of the weights, encouraging the model to keep weights small and reducing the risk of overfitting. Batch normalization normalizes the inputs of each layer, which stabilizes and accelerates training, and also acts as a regularizer.\n\nAdditionally, we used data augmentation techniques, such as horizontal and vertical flips, to increase the diversity of our training dataset. This helps the model generalize better by exposing it to a wider range of variations in the input data.\n\nTo address the potential issue of underfitting, we employed deep convolutional neural networks (CNNs) with architectures known for their effectiveness in image classification tasks, such as Inception v3 and ResNet50. These models have a large number of parameters, allowing them to capture complex patterns in the data. We also used the Adam or Adadelta optimization algorithms, which adapt the learning rate for each parameter, helping to converge faster and more accurately.\n\nThe number of parameters in our models is indeed much larger than the number of training points. However, the use of regularization techniques, data augmentation, and advanced optimization algorithms helps to ensure that the models generalize well to new data, rather than simply memorizing the training set.\n\nWe evaluated the performance of our models on a separate test set, which was not used during training. This allowed us to assess the models' ability to generalize to new, unseen data. The results showed that our models achieved reasonable accuracy on the test set, indicating that overfitting was effectively controlled.\n\nIn summary, we took several steps to prevent overfitting and underfitting, including the use of regularization techniques, data augmentation, advanced optimization algorithms, and thorough evaluation on a separate test set. These measures helped ensure that our models generalized well to new data.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our models. One of the primary methods used was Dropout, which involves randomly setting a fraction of input units to zero at each update during training time. This helps to prevent units from co-adapting too much, thereby improving the model's generalization ability. Different models utilized varying Dropout rates, such as 0.3 and 0.5, depending on the specific architecture and requirements.\n\nAdditionally, L2 regularization was applied to penalize large weights, which helps in reducing the model's complexity and preventing overfitting. This technique adds a penalty term to the loss function, encouraging the model to keep the weights small.\n\nBatch normalization was also integrated into some of our models. This technique normalizes the inputs of each layer, which helps in stabilizing and accelerating the training process. By reducing internal covariate shift, batch normalization allows for higher learning rates and acts as a slight regularizer, further aiding in the prevention of overfitting.\n\nData augmentation techniques, such as horizontal and vertical flips, were employed to increase the diversity of the training dataset. This approach helps in making the model more robust and less likely to overfit to the specific patterns in the training data.\n\nOverall, the combination of Dropout, L2 regularization, batch normalization, and data augmentation played a crucial role in mitigating overfitting and enhancing the performance of our models on the test dataset.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are detailed within the publication. Specifically, the training strategies for various convolutional neural networks, including batch sizes, epochs, optimizers, and dropout rates, are outlined in a dedicated table. For instance, models like VGG16, VGG19+Dropout, and ResNet50 were trained with specific batch sizes and optimizers such as Adam and Adadelta. The exact configurations for each model are clearly listed, providing transparency on how the experiments were conducted.\n\nThe model files and optimization parameters are not explicitly provided in the publication. However, the architectures of key models like Inception v3 and ResNet50 are described in detail, including layer configurations and output sizes. This information allows for the replication of the models used in the study.\n\nRegarding the availability and licensing of the data and models, the publication does not specify the exact terms under which the data or model files might be shared. Typically, such details would be provided in supplementary materials or through direct contact with the authors. For precise information on data access and licensing, readers are encouraged to refer to the supplementary materials or contact the corresponding author.\n\nIn summary, while the hyper-parameter configurations and optimization schedules are thoroughly documented, the model files and specific optimization parameters are not directly available in the publication. For access to these resources, further communication with the authors may be necessary.",
  "model/interpretability": "The model employed in this study is not entirely a black box, as efforts have been made to enhance its interpretability. One of the key techniques used to achieve this is the generation of heat maps. These heat maps visualize the areas of interest within the input images that the model focuses on to make its predictions. For instance, the heat maps for different subtypes of colorectal cancer, such as CMS2 and CMS4, highlight specific regions in the original images that are crucial for classification. These visualizations help in understanding which parts of the image the model considers important, thereby providing some transparency into the decision-making process.\n\nAdditionally, the model's architecture, such as ResNet50, includes mechanisms like residual learning, which help in preserving the integrity of the input information as it passes through the network. This architectural choice aids in making the model's internal workings more interpretable, as it allows for a clearer understanding of how information flows through the layers.\n\nThe use of metrics like precision, recall, and F1-score for evaluating the model's performance on different subtypes also contributes to interpretability. These metrics provide a detailed breakdown of the model's performance, indicating how well it identifies true positives, true negatives, false positives, and false negatives. This detailed evaluation helps in understanding the strengths and weaknesses of the model, making it less of a black box.\n\nFurthermore, the model's training process involves techniques like dropout, L2 regularization, and batch normalization, which are designed to prevent overfitting and improve generalization. These techniques, while primarily aimed at enhancing performance, also contribute to interpretability by ensuring that the model does not rely too heavily on any single feature or pattern in the data.\n\nIn summary, while the model is complex and involves deep learning techniques, several measures have been taken to make it more interpretable. The use of heat maps, detailed performance metrics, and transparent architectural choices all contribute to reducing the black-box nature of the model.",
  "model/output": "The model is designed for classification tasks, specifically for predicting molecular subtypes of colorectal cancer from whole slide images (WSIs). The output of the model is a probability distribution over the different molecular subtypes, which are then used to generate heatmaps indicating the predicted subtype for each patch of the WSI.\n\nThe model evaluates its performance using several metrics tailored for classification problems. These include accuracy, precision, recall, and F1-score. Accuracy measures the proportion of correctly predicted patches out of the total number of patches. Precision assesses the proportion of true positive predictions among all positive predictions. Recall, or sensitivity, evaluates the proportion of true positive predictions among all actual positives. The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both concerns.\n\nFor multi-class classification, the model uses micro-averaging and macro-averaging methods to compute the ROC curve. Micro-averaging calculates metrics globally by considering all true positives, false positives, and false negatives, while macro-averaging computes metrics for each class separately and then takes their average.\n\nThe model's architecture, such as ResNet50, includes layers designed to handle high-dimensional image data effectively. These layers process the input images through convolutional operations, pooling, and fully connected layers, ultimately producing a softmax output that classifies the input into one of the predefined molecular subtypes.\n\nIn summary, the model's output is a classification of molecular subtypes, with performance evaluated using standard classification metrics and techniques suitable for multi-class problems.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our study, we employed several evaluation methods to assess the performance of our models in predicting molecular subtypes of colorectal cancer from whole slide images.\n\nFor the evaluation of our models, we used a combination of accuracy, precision, recall, and F1-score metrics. Accuracy was calculated as the proportion of correctly predicted patches out of the total number of patches. Precision was determined by the ratio of true positive predictions to the total predicted positives. Recall, also known as sensitivity, was the ratio of true positive predictions to the actual positives. The F1-score, which is the harmonic mean of precision and recall, provided a single metric that balances both concerns.\n\nFor binary classification problems, we utilized the Receiver Operating Characteristic (ROC) curve to evaluate the model's predictive performance. The ROC curve plots the true positive rate against the false positive rate at various threshold settings. The Area Under the ROC Curve (AUC) was used as a summary measure of the model's performance.\n\nFor multi-class classification problems, such as our four-class problem, we calculated precision, recall, and F1-score for each class separately. We then reported the average of these metrics across all classes. This approach, known as macro-averaging, treats all classes equally and is useful when the class distribution is imbalanced.\n\nAdditionally, we used micro-averaging, which calculates the metrics globally by considering the contributions of all classes. This method is useful when the class distribution is balanced.\n\nWe also visualized the model's predictions using heatmaps. These heatmaps provided a visual representation of the model's confidence in predicting each molecular subtype across the entire slide. This allowed us to assess the model's performance qualitatively and identify areas where the model performed well or poorly.\n\nIn summary, our evaluation methods included a combination of quantitative metrics and qualitative visualizations to provide a comprehensive assessment of our models' performance.",
  "evaluation/measure": "In the evaluation of our models, several key performance metrics were reported to provide a comprehensive assessment of their predictive capabilities. These metrics include accuracy, precision, recall, and F1-score, which are commonly used in the literature for evaluating classification models.\n\nAccuracy measures the proportion of correctly predicted patches out of the total number of patches. It provides an overall sense of how well the model is performing. However, accuracy alone can be misleading, especially in cases of imbalanced datasets, which is why additional metrics are also considered.\n\nPrecision, also known as the positive predictive value, indicates the proportion of true positive predictions among all positive predictions made by the model. It is crucial for understanding the reliability of the model's positive predictions.\n\nRecall, or sensitivity, measures the proportion of true positive predictions out of all actual positives. It is important for assessing the model's ability to identify all relevant instances.\n\nThe F1-score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. It is particularly useful when there is an uneven class distribution.\n\nFor our models, these metrics were calculated at both the patch level and the slide level. At the patch level, we evaluated models like VGG16, VGG19+Dropout, VGG24+Dropout, VGG24+BN+Dropout, Inception v3, Resnet18, Resnet34, and Resnet50. The results showed that Inception v3 and ResNet50 generally performed better in terms of accuracy, precision, recall, and F1-score compared to the other models.\n\nAt the slide level, we focused on the overall accuracy of the models in predicting the molecular subtypes of colorectal cancer. The ResNet50 model achieved an overall accuracy of 51.72%, with notable performance in predicting the CMS2 subtype, reaching an accuracy of 75.00%. This indicates that while the models show promise, there is still room for improvement, particularly in handling the variability and complexity of colorectal cancer subtypes.\n\nThese performance metrics are representative of the standards used in the literature for evaluating similar models. They provide a clear and comprehensive view of the models' strengths and areas for improvement, guiding further research and development in this field.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of various convolutional neural network (CNN) models to evaluate their performance in predicting molecular subtypes of colorectal cancer from whole slide images. The models compared included VGG16, VGG19+Dropout, VGG24+Dropout, VGG24+BN+Dropout, Inception v3, Resnet18, Resnet34, and Resnet50. Each model was trained using specific strategies, including different batch sizes, epochs, optimizers, and dropout rates, as detailed in our training strategies table.\n\nTo assess the performance of these models, we used several evaluation metrics, including accuracy, precision, recall, and F1-score. These metrics were calculated for both the training and test sets, providing a thorough evaluation of each model's predictive capabilities. The results indicated that Inception v3 and Resnet50 outperformed the other models in terms of accuracy and other evaluation metrics.\n\nIn addition to comparing different CNN architectures, we also employed techniques such as dropout, regularization, and batch normalization to prevent overfitting. Optimization methods like Adam and Adadelta were used to minimize the cross-entropy loss on the training set, ensuring robust model performance.\n\nFor the evaluation of the models, we focused on both patch-level and slide-level predictions. At the patch level, we calculated the accuracy, precision, recall, and F1-score for each model. At the slide level, we generated heatmaps to visualize the predictions and calculated the overall accuracy for each model. The Resnet50 model, in particular, showed promising results with a high accuracy on the training set and competitive performance on the test set.\n\nOverall, our comparison of different CNN models and evaluation metrics provided valuable insights into the strengths and weaknesses of each approach. The Resnet50 model emerged as the most effective for predicting molecular subtypes of colorectal cancer, demonstrating its potential for clinical applications.",
  "evaluation/confidence": "Evaluation Confidence\n\nThe evaluation of the models involved several performance metrics, including accuracy, precision, recall, and F1-score. These metrics were calculated for both training and test sets, providing a comprehensive view of the models' performance.\n\nAccuracy was used as the primary evaluation metric for the patch-level and slide-level models. The accuracy values for the training and test sets were reported for various models, such as VGG16, VGG19+Dropout, VGG24+Dropout, VGG24+BN+Dropout, Inception v3, Resnet18, Resnet34, and Resnet50. For instance, the Inception v3 model achieved an accuracy of 0.87 on the training set and 0.53 on the test set, while the ResNet50 model achieved an accuracy of 0.89 on the training set and 0.42 on the test set.\n\nPrecision, recall, and F1-score were also calculated for the test set to evaluate the models' performance in predicting the molecular subtypes of colorectal cancer. These metrics provide insights into the models' ability to correctly identify positive cases (precision), the proportion of actual positives that were correctly identified (recall), and the harmonic mean of precision and recall (F1-score).\n\nThe results indicate that the Inception v3 and ResNet50 models performed better than other models in predicting the molecular subtypes of colorectal cancer. However, the performance metrics do not include confidence intervals, which would provide a measure of the uncertainty associated with the estimates. Without confidence intervals, it is challenging to assess the statistical significance of the results and claim that one method is superior to others and baselines with certainty.\n\nAdditionally, the evaluation did not include statistical tests to compare the performance of different models. Such tests would help determine whether the observed differences in performance are statistically significant or due to random variation. Therefore, while the results suggest that certain models perform better, the lack of statistical analysis limits the confidence in these findings.",
  "evaluation/availability": "Not enough information is available."
}