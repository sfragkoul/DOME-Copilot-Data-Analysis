{
  "publication/title": "Development and use of machine learning models for prediction of male sling success A proof-of-concept institutional evaluation.",
  "publication/authors": "Kim JK, McCammon KA, Kim KJ, Rickard M, Lorenzo AJ, Chua ME",
  "publication/journal": "Canadian Urological Association journal = Journal de l'Association des urologues du Canada",
  "publication/year": "2023",
  "publication/pmid": "37494315",
  "publication/pmcid": "PMC10581726",
  "publication/doi": "10.5489/cuaj.8265",
  "publication/tags": "- Machine Learning\n- Male Sling\n- Urinary Incontinence\n- Predictive Modeling\n- K-Nearest Neighbor\n- Random Forest\n- Ensemble Learning\n- Clinical Outcomes\n- Transobturator Male Sling\n- Surgical Success Prediction",
  "dataset/provenance": "The dataset used in this study was sourced from a prospectively maintained database at our institution. This database includes all male patients who underwent transobturator male sling insertion between August 2006 and June 2012, performed by a single surgeon. The dataset consists of 181 patients, with 153 patients included in the training set and 28 patients in the testing set. All redo cases and patients with missing data were excluded from the analysis.\n\nThe data collected includes various preoperative clinical variables such as age, smoking status, diagnosis of diabetes, race, height, weight, prior prostatectomy, prior pelvic radiation, prior stress urinary incontinence (SUI) management, bladder neck contractures, type of incontinence, severity of urgency incontinence (if present), potential need for concomitant procedures, number of preoperative pads, and etiology of incontinence. These variables were carefully selected and standardized to ensure they contributed equally to the models.\n\nThe dataset has been internally validated through a random counter-verification of 15% of the total extracted data. This validation process helps ensure the accuracy and reliability of the data used in our analysis. The same database was previously used in a study by Chua et al, which reported that long-term outcomes of preoperative moderate to severe SUI were the only independent predictors for failure to achieve cure in long-term follow-up. This study supplements that knowledge by incorporating additional clinical characteristics into the algorithm training, allowing for a more comprehensive prediction of surgical outcomes.",
  "dataset/splits": "Two data splits were performed. The first split was an 85:15 train-test split, resulting in 153 patients in the training set and 28 patients in the testing set. The second split was a random train-test split, which also resulted in 153 patients in the training set and 28 patients in the testing set. The distribution of data points in each split was not explicitly detailed, but the baseline characteristics of patients in each training and testing set are summarized in a table.",
  "dataset/redundancy": "The dataset used in this study consisted of 181 patients who underwent transobturator male sling surgery. To ensure the robustness of our machine learning models, the dataset was split into training and testing sets. The split was performed randomly, resulting in 153 patients in the training set and 28 patients in the testing set. This random split helps to ensure that the training and testing sets are independent, reducing the risk of data leakage and overfitting.\n\nTo enforce the independence of the training and testing sets, we used a random splitting method. This method ensures that each patient's data is included in only one of the sets, maintaining the integrity of the evaluation process. The baseline characteristics of the patients in both the training and testing sets were summarized to verify that the distributions were comparable, which is crucial for the validity of our model's performance metrics.\n\nThe distribution of the dataset in this study is comparable to other machine learning datasets in the medical field, particularly those involving surgical outcomes. The random split and the subsequent validation steps ensure that our models are trained and tested on representative samples, enhancing the generalizability of our findings. This approach is consistent with best practices in machine learning, where the goal is to create models that can perform well on unseen data.",
  "dataset/availability": "The data used in this study is not publicly available. The analysis was conducted using a retrospective assessment of a prospectively maintained database from a single institution. This database included all male patients who underwent transobturator male sling insertion between August 2006 and June 2012 by a single surgeon. The data was internally validated through a random counter-verification of 15% of the total extracted data. However, the specific details of the data splits used for training and testing are not provided in a public forum. The study does not mention the release of the dataset or any associated license for public use. Therefore, the data remains proprietary to the institution and is not accessible to the general public.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and widely recognized in the field. These include logistic regression, K-nearest neighbor (KNN), naive Bayes, decision tree, and random forest. These algorithms are part of the supervised learning paradigm, which is commonly used for classification tasks.\n\nThe KNN algorithm, in particular, was identified as the best-performing model among the five initially assessed. To enhance its performance, an ensemble learning method known as bagging was applied. Bagging involves creating multiple subsets of the training data, training a KNN model on each subset, and then combining the predictions to produce a more accurate and robust outcome. This approach helps to reduce the variance and improve the generalization of the model.\n\nThe choice of these algorithms was driven by their effectiveness in handling small datasets and their ability to provide interpretable results. The KNN algorithm, for instance, is known for its simplicity and effectiveness in capturing local patterns in the data. However, it can be computationally intensive and may not scale well with larger datasets. To mitigate this, hyperparameter tuning was performed to optimize the number of neighbors (k) used in the KNN algorithm, which was set to 23. This value was determined through a grid search, ensuring that the model balanced between bias and variance.\n\nThe use of these established algorithms in a medical context is not uncommon. They have been extensively studied and applied in various domains, including healthcare, where interpretability and reliability are crucial. The focus of this study was not on developing a new machine-learning algorithm but rather on applying existing methods to predict the outcomes of a specific medical procedure\u2014the transobturator male sling. The results demonstrate the feasibility of using machine learning to aid in clinical decision-making, particularly in predicting the success of surgical interventions.\n\nThe algorithms were implemented using Python, a popular programming language for machine learning and data analysis. The models were developed and evaluated using standard metrics such as sensitivity, specificity, area under the receiver operating characteristic curve (AUROC), area under the precision-recall curve (AUPRC), and F1-score. These metrics provide a comprehensive assessment of the models' performance and reliability.\n\nIn summary, the machine-learning algorithms used in this study are well-established and have been applied effectively to predict the outcomes of the transobturator male sling procedure. The use of ensemble learning and hyperparameter tuning further enhanced the performance and robustness of the models, making them suitable for clinical application.",
  "optimization/meta": "The model developed in this study does not function as a meta-predictor. Instead, it relies on a single machine-learning algorithm, specifically the K-nearest neighbor (KNN) method, which was enhanced using ensemble learning techniques such as bagging. The KNN model was chosen for its performance on the dataset, particularly its effectiveness with smaller sample sizes.\n\nThe ensemble KNN model was created using the bagging method, which involves training multiple instances of the KNN algorithm on different subsets of the data and then combining their predictions. This approach helps to reduce noise and variance, improving the model's predictive performance.\n\nThe training data for the KNN model was derived from a dataset of 181 patients, which was split into training and testing sets. The training set consisted of 153 patients, while the testing set included 28 patients. This split ensures that the training data is independent of the testing data, allowing for an unbiased evaluation of the model's performance.\n\nThe model's performance was assessed using various metrics, including the area under the receiver operating characteristic curve (AUROC), the area under the precision-recall curve (AUPRC), and the F-1 score. The calibrated bagging KNN model demonstrated the best performance among all evaluated models, with an AUROC of 0.821, an AUPRC of 0.921, and an F-1 score of 0.848.\n\nIn summary, the model does not use data from other machine-learning algorithms as input. It is based solely on the KNN algorithm, enhanced through bagging, and the training data is independent, ensuring reliable performance evaluation.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for ensuring the machine-learning models performed optimally. Initially, the dataset included various preoperative clinical variables such as age, smoking status, diabetes diagnosis, race, height, weight, prior prostatectomy, prior pelvic radiation, prior stress urinary incontinence (SUI) management, bladder neck contractures, type of incontinence, severity of urgency incontinence, potential need for concomitant procedures, number of preoperative pads, and etiology of incontinence.\n\nTo prepare the data, we first conducted a random counter-verification of 15% of the total extracted data to ensure internal validation. This step helped in confirming the accuracy and reliability of the collected data.\n\nNext, we created plot densities of these variables to assess their impact on outcomes. Variables that showed minimal discrimination for 'cure' based on the assessed variable, such as diagnosis of diabetes and concomitant procedures, were removed to reduce noise and improve model performance.\n\nContinuous variables were standardized and scaled to avoid biases due to different measurement scales. This step ensured that each feature contributed equally to the models, preventing any single variable from dominating the predictions due to its scale.\n\nThe dataset was then split into training and testing sets using an 85:15 ratio. This split allowed for 85% of the data to be used for model training and 15% for performance evaluation. The split was done randomly to ensure that the models were evaluated on unseen data, providing a more accurate assessment of their generalizability.\n\nGrid search was performed to optimize and tune the hyperparameters of the K-nearest neighbor (Knn) and random forest models. This process involved systematically working through multiple combinations of hyperparameter values to determine the best configuration for each model.\n\nFor the Knn model, the best hyperparameters were found to be n-neighbor of 23 with uniform weight. For the random forest classifier, the optimal hyperparameters included an n-estimator of 57.\n\nThese preprocessing and encoding steps were essential in preparing the data for the machine-learning models, ensuring that they could effectively learn from the data and make accurate predictions.",
  "optimization/parameters": "In our study, we initially considered a comprehensive set of preoperative clinical variables, including age, smoking status, diagnosis of diabetes, race, height, weight, prior prostatectomy, prior pelvic radiation, prior stress urinary incontinence (SUI) management, bladder neck contractures, type of incontinence, severity of urgency incontinence (if present), potential need for concomitant procedures, number of preoperative pads, and etiology of incontinence.\n\nTo refine our model, we employed plot densities to assess the impact of each variable on the outcomes. This process helped us identify that diagnosis of diabetes and concomitant procedures were less significant contributors to the model's predictive power. Consequently, these variables were removed from the feature selection process.\n\nThe final model utilized a subset of the initial variables, focusing on those that demonstrated a more substantial influence on the prediction of procedural success. This approach ensured that our model was streamlined and more effective, avoiding unnecessary complexity and potential noise from less relevant features.",
  "optimization/features": "The study initially considered a comprehensive set of preoperative clinical variables as potential input features. These variables included age, smoking status, diagnosis of diabetes, race, height, weight, prior prostatectomy, prior pelvic radiation, prior stress urinary incontinence (SUI) management, bladder neck contractures, type of incontinence, severity of urgency incontinence (if present), potential need for concomitant procedures, number of preoperative pads, and etiology of incontinence.\n\nTo enhance the model's performance and reduce noise, feature selection was performed. This process involved creating plot densities of the variables to assess their impact on outcomes. Variables that showed minimal discrimination for 'cure' based on the assessed variable were removed. Specifically, diagnosis of diabetes and concomitant procedures were identified as less significant contributors to the model and were excluded from the final feature set.\n\nThe final model utilized a refined set of features, focusing on those that demonstrated greater importance in predicting the success of the male sling procedure. The most significant features identified through permutation importance included the preoperative number of pads used, weight, height, severity of incontinence, and type of incontinence. These features were determined to have a relatively greater importance compared to others, indicating their crucial role in the model's predictive accuracy.\n\nFeature selection was conducted using the training set only, ensuring that the model's performance on the testing set remained unbiased and reflective of real-world applicability. This approach helped in building a more robust and reliable model for predicting the outcomes of transobturator male sling surgery.",
  "optimization/fitting": "The study involved a dataset of 181 patients, which was split into training and testing sets with 153 and 28 patients, respectively. This split ensures that the number of parameters is not excessively larger than the number of training points, which helps in mitigating overfitting risks.\n\nTo further address overfitting, several techniques were employed. Grid search was used to optimize hyperparameters, which is crucial for tuning models like random forest and K-nearest neighbor (Knn). For the random forest classifier, the best hyperparameter was found to be 57 estimators, while for Knn, the optimal number of neighbors was 23 with uniform weight. These optimizations help in finding the best model configuration that generalizes well to unseen data.\n\nAdditionally, ensemble learning through the bagging method was applied to the Knn model. Bagging involves training multiple models on different subsets of the data and combining their predictions, which reduces variance and improves the model's robustness. This approach is particularly effective in small datasets, as it helps in reducing noise and variance.\n\nTo ensure the model's reliability, validation curves were constructed. Initially, the bagging Knn model showed poor predictability at low predicted probabilities and a tendency to over-forecast when probabilities were below 0.5 and under-forecast when probabilities were above 0.5. To address this, the model was calibrated using the sigmoid method, which improved the calibration and reduced the likelihood of over-forecasting.\n\nFeature selection was also performed to reduce noise. Variables that did not significantly contribute to the model, such as diagnosis of diabetes and concomitant procedures, were removed. This step helps in focusing on the most relevant features, thereby reducing the risk of overfitting.\n\nMoreover, the model's performance was assessed using multiple metrics, including sensitivity, specificity, area under the receiver operating characteristic curve (AUROC), area under the precision-recall curve (AUPrC), and F1-score. These metrics provide a comprehensive evaluation of the model's performance and help in identifying any signs of underfitting or overfitting.\n\nIn summary, the study employed several strategies to address overfitting and underfitting, including hyperparameter tuning, ensemble learning, model calibration, and feature selection. These techniques ensure that the model is robust and generalizes well to new data.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and enhance the robustness of our models. One of the primary methods used was hyperparameter tuning. For instance, we utilized grid search to identify the optimal hyperparameters for our K-nearest neighbor (Knn) and random forest models. This process helped in selecting the best parameters that minimized overfitting by balancing the model's complexity and performance.\n\nAdditionally, we implemented ensemble learning through the bagging method. Bagging involves training multiple instances of the Knn model on different subsets of the data, which helps to reduce variance and improve the model's generalization ability. By aggregating the predictions from these multiple models, we were able to create a more stable and reliable prediction.\n\nFeature selection was another crucial step in our optimization process. We analyzed the plot densities of various preoperative clinical variables to determine their significance in predicting outcomes. Variables that did not significantly contribute to the model, such as the diagnosis of diabetes and concomitant procedures, were removed. This reduction in the number of features helped to decrease noise and improve the model's performance.\n\nFurthermore, we calibrated our bagged Knn model using the sigmoid method. Calibration is essential for ensuring that the predicted probabilities are well-aligned with the actual outcomes. This step helped in mitigating the tendency of the model to over-forecast or under-forecast the likelihood of cure, thereby enhancing its reliability.\n\nOverall, these techniques collectively contributed to the development of a more accurate and generalizable model, reducing the risk of overfitting and improving the predictive performance.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters for the models developed in this study are reported in the results section. Specifically, the best hyperparameters for the random forest classifier were identified as an n-estimator of 57, and for the K-nearest neighbor (Knn) model, the best hyperparameters were an n-neighbor of 23 with uniform weight. These details are provided to ensure reproducibility and transparency in the model development process.\n\nThe optimization schedule involved using grid search to determine the best hyperparameters for the Knn and random forest models. This method systematically works through multiple combinations of parameter tunes to determine the optimal configuration.\n\nThe model files and specific optimization parameters are not directly available in the text, but the process and results of the optimization are detailed. For further specifics, such as the exact model files or additional optimization parameters, supplementary materials or appendices may be referenced, which are available at the specified URL (cuaj.ca).\n\nRegarding the availability and license of these configurations, the study does not explicitly mention a specific license for the reported configurations. However, the results and methods are published in a peer-reviewed journal, which typically implies that the findings and methodologies are available for academic and research purposes under standard scholarly communication practices. For precise licensing details, one would need to refer to the journal's policies or contact the authors directly.",
  "model/interpretability": "The model developed in this study is not entirely a black box. While machine learning models, particularly those based on algorithms like K-nearest neighbors (KNN) and random forests, can sometimes be seen as complex and opaque, efforts were made to enhance interpretability.\n\nTo achieve this, permutation importance was used to identify the top features contributing to the model. This method helps in understanding which variables are most influential in predicting the success of the male sling procedure. The preoperative number of pads used was found to be the most important feature. Additionally, variables such as weight, height, severity of incontinence, and type of incontinence were also significant contributors. These findings align with clinical knowledge, where factors like the severity of incontinence and patient characteristics are known to impact surgical outcomes.\n\nBy using permutation importance, the model's predictions can be traced back to specific clinical variables, making it more transparent. This approach allows healthcare practitioners to understand the rationale behind the model's predictions, thereby increasing trust and facilitating better clinical decision-making. The model's interpretability is further supported by the use of validation curves and calibration methods, which help in assessing the model's reliability and accuracy.",
  "model/output": "The model developed is a classification model. It is designed to predict the success of male sling procedures, specifically whether patients achieve a 'cure' status, defined as complete dryness or zero pads used. The model evaluates various preoperative clinical variables to make these predictions.\n\nSeveral classifier models were assessed, including logistic regression, K-nearest neighbor (KNN), naive Bayes, decision tree, and random forest. Among these, the KNN model demonstrated the highest balanced performance, with an area under the receiver operating characteristic curve (AUROC) of 0.759, an area under the precision-recall curve (AUPRC) of 0.916, and an F-1 score of 0.833. To enhance the model's reliability, an ensemble KNN model was developed using the bagging method. This bagging KNN model achieved an AUROC of 0.791, an AUPRC of 0.919, and an F-1 score of 0.812.\n\nFurther calibration of the bagging KNN model using the sigmoid method improved its performance, resulting in an AUROC of 0.821, an AUPRC of 0.921, and an F-1 score of 0.848. This calibrated model showed good concordance with a theoretical perfectly calibrated model, indicating its potential utility in clinical practice.\n\nThe model's performance was evaluated using sensitivity, specificity, AUROC, AUPRC, and F-1 score. Validation curves were constructed to assess model reliability, and the model was calibrated to improve its predictive accuracy. The most significant contributing variables in the model were identified as height, weight, severity of incontinence, and type of incontinence, which are known to be predictive of male sling success or stress urinary incontinence (SUI) outcomes.\n\nThe model's predictive potential suggests a role for machine learning in clinical practice for male SUI. However, its immediate use and generalization are limited by the small study cohort and single-surgeon series of patients. Prospective data collection and involvement of other institutions are planned to improve the model's predictive performance and generalizability.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the machine learning models involved several key steps to ensure robustness and reliability. Initially, the dataset was split into training and testing sets using an 85:15 ratio, with the training set comprising 153 patients and the testing set comprising 28 patients. This split was done randomly to ensure that the models were evaluated on unseen data.\n\nFive different classifier models were developed and assessed: logistic regression, K-nearest neighbor (Knn), naive Bayes, decision tree, and random forest. The performance of these models was evaluated using several metrics, including sensitivity, specificity, area under the receiver operating characteristic curve (AUroC), area under the precision-recall curve (AUPrC), and F1-score. These metrics provided a comprehensive view of the models' ability to correctly classify patients as achieving 'cure' or not.\n\nGrid search was employed to optimize the hyperparameters of the Knn and random forest models. For the random forest classifier, the best hyperparameter was found to be an n-estimator of 57. For the Knn model, the optimal hyperparameters were an n-neighbor of 23 with uniform weight. These optimized parameters were crucial in enhancing the models' performance.\n\nThe Knn model demonstrated the highest balanced performance among the five models, with an AUroC of 0.759, AUPrC of 0.916, and F1-score of 0.833. To further improve the model's performance, an ensemble learning method using bagging was applied. The bagging Knn model achieved an AUroC of 0.791, AUPrC of 0.919, and F1-score of 0.812. This ensemble approach helped to reduce noise and variance, leading to more accurate predictions.\n\nValidation curves were constructed to assess the reliability of the bagging Knn model. Initially, the model showed poor predictability when the predicted probability of cure was low and tended to over-forecast when the predicted and true probabilities were less than 0.5, and under-forecast when they were greater than 0.5. To address these issues, the bagged Knn model was calibrated using the sigmoid method. Post-calibration, the validation curve indicated a closer alignment to perfect calibration, reducing the likelihood of over-forecasting and improving the model's overall predictability.\n\nThe calibrated bagging Knn model was further evaluated using validation curves, which showed good concordance with a theoretical perfectly calibrated model. This calibration process was essential in enhancing the model's reliability and accuracy in predicting the likelihood of cure for patients undergoing the transobturator male sling procedure.",
  "evaluation/measure": "To evaluate the performance of our machine learning models, we utilized several key metrics that are widely recognized in the literature for their effectiveness in assessing model performance. These metrics include sensitivity, specificity, the area under the receiver operating characteristic curve (AUROC), the area under the precision-recall curve (AUPRC), and the F1-score.\n\nSensitivity, also known as recall, measures the proportion of actual positives that are correctly identified by the model. Specificity, on the other hand, measures the proportion of actual negatives that are correctly identified. These metrics are crucial for understanding how well the model can distinguish between positive and negative outcomes.\n\nThe AUROC provides a comprehensive view of the model's ability to discriminate between classes across all threshold levels. It is particularly useful for evaluating models in imbalanced datasets, where the classes are not equally represented. Similarly, the AUPRC focuses on the precision-recall trade-off, which is essential for understanding the model's performance in scenarios where the positive class is rare.\n\nThe F1-score is the harmonic mean of precision and recall, offering a single metric that balances both concerns. It is especially useful when the classes are imbalanced, as it provides a more nuanced view of the model's performance compared to accuracy alone.\n\nThese metrics collectively provide a robust evaluation framework, ensuring that our models are assessed from multiple angles. The choice of these metrics aligns with established practices in the field, making our evaluation both representative and comprehensive. This approach allows us to confidently report the performance of our models and compare them with other studies in the literature.",
  "evaluation/comparison": "In our study, we developed and evaluated several machine learning models to predict the success of transobturator male sling surgery. We compared the performance of five different classifier models: logistic regression, K-nearest neighbor (Knn), naive Bayes, decision tree, and random forest. Each model was trained and tested using the same dataset, which was split randomly into training and testing sets.\n\nTo ensure a fair comparison, we used standard performance metrics, including sensitivity, specificity, area under the receiver operating characteristic curve (AUROC), area under the precision-recall curve (AUPrC), and F1-score. These metrics provided a comprehensive evaluation of each model's ability to predict surgical outcomes accurately.\n\nThe K-nearest neighbor model demonstrated the highest balanced performance among the five models, with an AUROC of 0.759, AUPrC of 0.916, and F1-score of 0.833. To further enhance the performance, we employed an ensemble learning method, specifically the bagging technique, which improved the model's reliability and accuracy. The bagging K-nearest neighbor model achieved an AUROC of 0.791, AUPrC of 0.919, and F1-score of 0.812.\n\nAdditionally, we calibrated the bagging K-nearest neighbor model using the sigmoid method to improve its predictability, particularly when the predicted probability of cure was low. This calibration process resulted in a model that was less likely to over-forecast success and provided a more accurate prediction of surgical outcomes.\n\nWhile we did not compare our models to publicly available methods on benchmark datasets, our approach included a thorough comparison to simpler baselines, such as logistic regression and decision trees. This comparison allowed us to assess the relative performance of more complex models, like random forest and K-nearest neighbor, in the context of our specific dataset and clinical problem.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}