{
  "publication/title": "Predicting ambient PM<sub>2.5</sub> concentrations in Ulaanbaatar, Mongolia with machine learning approaches.",
  "publication/authors": "Enebish T, Chau K, Jadamba B, Franklin M",
  "publication/journal": "Journal of exposure science & environmental epidemiology",
  "publication/year": "2021",
  "publication/pmid": "32747729",
  "publication/pmcid": "PMC9871862",
  "publication/doi": "10.1038/s41370-020-0257-8",
  "publication/tags": "- Air pollution\n- Machine learning\n- Environmental epidemiology\n- Data analysis\n- Predictive modeling\n- Cross-validation\n- Meteorological data\n- Urban air quality\n- Statistical computing\n- Spatial data analysis\n- Time series analysis\n- Data preprocessing\n- Model tuning\n- Air quality monitoring\n- Public health",
  "dataset/provenance": "The dataset used in our study is sourced from multiple organizations that measure particle concentration in Ulaanbaatar. The Air Pollution Reduction Agency (APRA) uses optical particle detection equipment (EDM180, GRIMM Aerosol, Germany). The National Agency for Meteorology and Environmental Monitoring (NAMEM) employs beta ray attenuation technology (MP101M, ENVEA, France), as does the US Embassy (BAM-1020, Met One, US). The US Embassy data, provided by the US Department of State, is not fully verified or validated.\n\nMeteorological data were obtained from eight monitoring sites operated by government agencies, co-located with weather stations. These sites provided hourly measurements on surface temperature, atmospheric pressure, wind speed, wind direction, and relative humidity. Land use and population data were gathered from various sources, including the Urban Development Agency of Ulaanbaatar (UDA) and the UB Department of Statistics. Elevation data were sourced from NASA\u2019s Shuttle Radar Topography Mission.\n\nThe dataset spans from 2010 to 2018 and includes daily averages of PM2.5 concentrations, constructed by considering data missing if more than 25% of hourly measurements were absent. The study area is divided into 138 khoroos, with monitoring stations strategically placed to capture spatial and temporal variations in PM2.5 levels.\n\nPrevious studies have utilized similar data sources to model air pollution exposure in Ulaanbaatar. For instance, Davy et al. and Nishikawa et al. have determined that coal combustion, biomass burning, and motor vehicles are significant contributors to fine PM concentration. Our study builds on this foundation by incorporating locale-specific covariates such as stove density and socioeconomic indicators to improve the accuracy of PM2.5 predictions.\n\nThe dataset used in this study is unique in its spatiotemporal resolution, focusing on a small scale within Ulaanbaatar. This granularity allows for a more precise understanding of PM2.5 exposure patterns, which is crucial for epidemiological studies and public health interventions. The inclusion of diverse data sources and the rigorous cross-validation technique employed ensure the robustness and reliability of our findings.",
  "dataset/splits": "For our study, we created three separate models for each machine learning algorithm. These models were trained and validated using different time periods: the entire study period (2010\u20132018), the cold-season (October to March), and the warm-season (April to September). The datasets were preprocessed to optimize each learning algorithm using the R package recipes.\n\nEach model was trained, validated, and tuned on 85% of the dataset. The remaining 15% of the data was held out as a test set, ensuring that it was not involved in any of the training, validation, or tuning processes. This approach allowed us to evaluate the models' performance on data that was entirely independent of the model-building process.\n\nThe models were validated and tuned using leave-one-location-out cross-validation (LOLO CV). This method involves training the models on all but one location at a time, with the number of unique locations in our data being nine sites. The prediction errors were averaged across these repeats to provide an error estimate. This ensured that no observation from the validation location was involved in training the model, unlike k-fold cross-validation, where observations are uniformly distributed among folds at random.\n\nThe final fitted models were then used to predict PM2.5 concentrations on the hold-out test set, allowing us to evaluate their performance on data that had not been involved in the model-building process.",
  "dataset/redundancy": "The datasets were split into training and test sets to ensure independence between them. Specifically, 85% of the dataset was used for training, validation, and tuning, while the remaining 15% was held out as a test set. This test set was not involved in any of the training, validation, or tuning processes, ensuring its independence.\n\nTo enforce this independence, a leave-one-location-out (LOLO) cross-validation technique was employed. This method involves training the models on all but one location at a time, with the held-out location serving as the validation set. This process is repeated for each unique location in the dataset, ensuring that no observation from the validation location is used in the training process. This approach is particularly suitable for spatially dependent data, such as air pollution data from fixed-site monitoring stations.\n\nThe distribution of the datasets used in this study is comparable to previously published machine learning datasets for predicting PM2.5 concentrations. These datasets typically include a mix of meteorological and land-use variables, similar to the covariates used in our models. However, unlike some studies, aerosol optical depth (AOD) was not included as a predictor in our models due to preliminary analyses indicating minimal impact on model performance. This decision aligns with the goal of optimizing the model's predictive power while maintaining simplicity and reducing potential overfitting.",
  "dataset/availability": "The data used in this study is not released in a public forum. The datasets were preprocessed for optimization of each learning algorithm using R package recipes. The data was split into training and test sets, with 85% of the dataset used for training, validation, and tuning, and the remaining 15% held out as a test set. This test set was not involved in any of the training, validation, or tuning processes. The models were trained, validated, and tuned using leave-one-location-out cross-validation (LOLO CV), which ensures that no observation from the validation location is involved in training the model. The final fitted models were then used to predict PM2.5 on the hold-out test set to evaluate their performance on data that were not involved in the model building process. The data was collected from various sources, including air pollution data from fixed-site regulatory monitoring stations, meteorological data from co-located weather stations, and land use and population data from government agencies. The data was cleaned and manipulated using the \"Tidyverse\" set of packages in R. The data was also used to construct various covariates, such as day of year, Julian date variables, and indicator variables for weekend, public holidays, month, and season. The data was used to train and validate six predictive algorithms: random forest (RF), gradient boosting (GBM), support vector machine with a radial basis kernel (SVM), multivariate adaptive regression splines (MARS), generalized linear model with elastic net penalties (GLMNET), and generalized additive model (GAM). The models were developed for the entire study period (2010\u20132018), as well as for the cold-season (Oct-Mar) and warm-season (Apr-Sep). The data was also used to evaluate the performance of the models using root mean square error (RMSE) and R2 as performance metrics. The data was not deposited in a public repository, and there is no information available about the license under which the data would be released if it were made public.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the class of supervised learning techniques, specifically designed for regression tasks. The algorithms employed include adaptive regression splines, generalized linear models with elastic net penalties, generalized additive models, random forest, gradient boosting machines, and support vector machines. These are well-established algorithms in the field of machine learning and statistics, and they have been extensively used in various predictive modeling tasks, including air pollution prediction.\n\nThe algorithms used are not new; they have been developed and refined over several years and are widely recognized in the machine learning community. The choice of these algorithms was driven by their proven effectiveness in handling complex, non-linear relationships and their ability to capture spatial and temporal dependencies in the data. These algorithms have been implemented and optimized using established packages in the R programming language, such as \"earth\" for adaptive regression splines, \"glmnet\" for generalized linear models with elastic net penalties, and \"mgcv\" for generalized additive models. The decision tree-based algorithms, random forest and gradient boosting machines, were implemented using the \"ranger\" and \"xgboost\" packages, respectively. Support vector machines were implemented using the \"kernlab\" package.\n\nThe focus of this study is on the application of these machine learning algorithms to predict ambient PM2.5 concentrations in Ulaanbaatar, Mongolia, rather than on the development of new algorithms. The algorithms were selected for their robustness and adaptability to the specific challenges posed by the data, such as spatial and temporal dependencies and the need to handle missing values and outliers. The study aims to demonstrate the feasibility and effectiveness of using these algorithms in a setting with limited monitoring capacity and high levels of air pollution. The results provide evidence of the advantage of machine learning approaches in predicting ambient PM2.5 levels, which is crucial for epidemiological studies and public health interventions.",
  "optimization/meta": "The models developed in this study do not use data from other machine-learning algorithms as input. Instead, they rely on various environmental and meteorological covariates. The study employed several machine learning algorithms individually, including random forest (RF), gradient boosting machines (GBM), support vector machines (SVM), multivariate adaptive regression splines (MARS), generalized linear models with elastic net penalties (GLMNET), and generalized additive models (GAM). Each of these algorithms was used to create separate models for different time periods: the entire study period (2010\u20132018), the cold season (October to March), and the warm season (April to September).\n\nThe training data for these models was carefully managed to ensure independence. Specifically, the datasets were split such that 85% of the data was used for training, validation, and tuning, while the remaining 15% was held out as a test set that was not involved in any of the training processes. This approach helps to ensure that the models are evaluated on data that they have not seen during training, providing a more robust assessment of their performance.\n\nThe study used leave-one-location-out (LOLO) cross-validation, which is particularly suitable for spatiotemporally dependent data like air pollution measurements. This method involves training the models on all but one location at a time, ensuring that no observation from the validation location is used in the training process. This rigorous approach helps to maintain the independence of the training data and provides a more accurate estimate of model performance.",
  "optimization/encoding": "The data encoding and preprocessing steps were tailored to optimize each learning algorithm. For instance, tree-based models, such as random forest (RF) and gradient boosting machines (GBM), did not require preprocessing because decision trees are invariant to monotonic transformations. However, algorithms like support vector machines (SVM) are sensitive to different ranges of predictors and thus required normalization.\n\nDatasets were preprocessed using the R package recipes to ensure optimization for each learning algorithm. This involved handling missing data, scaling, and transforming variables as needed. For example, meteorological data were averaged and ranged over all available sites to reduce missingness, which is crucial since many machine learning algorithms are sensitive to missing data. This approach prioritized the completeness of predictors over spatial resolution.\n\nAdditionally, various covariates were constructed and assigned to monitoring sites or administrative areas (khoroos). These included road length, city zone variables, elevation data, population density, and the number of stoves. Day of year, Julian date variables, and indicator variables for weekends, public holidays, months, and seasons were also created to capture temporal patterns.\n\nThe preprocessing steps ensured that the data were in an optimal format for training, validating, and tuning the models. This involved splitting the dataset into training (85%) and test sets (15%), with the test set held out entirely from the training and validation processes. This approach helped in evaluating the models' performance on unseen data, providing a more robust assessment of their predictive capabilities.",
  "optimization/parameters": "In our study, the number of parameters used in each model varies depending on the specific machine learning algorithm employed. For instance, the Random Forest model utilizes three key parameters: the number of predictors at each split, the number of trees, and the minimum number of data points in a node. Similarly, the Gradient Boosting model incorporates parameters such as the number of predictors at each split, the number of trees, the minimum number of data points in a node, the maximum depth of the tree, the learning rate, and the sample size.\n\nThe selection of these parameters was conducted through a rigorous tuning process. We employed a space-filling design called maximum entropy to fill our parameter grid with 30 different combinations. This approach allowed us to systematically explore the parameter space and identify the optimal values for each model. The tuning process involved training 270 models, consisting of 9 resamples from our 9 sites for each of the 30 parameter combinations. The best-performing model parameters were selected based on the lowest root mean square error (RMSE) observed during the leave-one-location-out cross-validation (LOLO CV). This method ensured that the selected parameters were optimized for better prediction performance on spatially or temporally dependent data, such as air pollution measurements from fixed-site regulatory monitoring stations.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The fitting method employed in our study involved several machine learning algorithms, each with its own set of tuning parameters. The number of parameters varied depending on the algorithm, but generally, the parameter space was extensive to ensure optimal model performance.\n\nTo address the potential issue of over-fitting, especially given the complexity of some models and the relatively large number of parameters, we utilized a rigorous cross-validation technique known as Leave-One-Location-Out Cross-Validation (LOLO CV). This method is particularly suitable for spatially dependent data, such as air pollution measurements from fixed-site monitoring stations. By training the models on all but one location at a time and averaging the prediction errors across the repeats, we ensured that no observation from the validation location was involved in the training process. This approach helped in obtaining a more accurate estimate of model performance and mitigated the risk of over-fitting.\n\nAdditionally, we employed a space-filling design called maximum entropy to fill our parameter grid with 30 different combinations. This design helped in exploring the parameter space efficiently and selecting the best-performing model parameters based on the lowest Root Mean Square Error (RMSE). By training 270 models (9 resamples from our 9 sites for each of the 30 parameter combinations), we ensured that the final model parameters were optimized for better prediction performance.\n\nTo rule out under-fitting, we carefully preprocessed the datasets for each learning algorithm using the R package \"recipes\". For instance, algorithms like Support Vector Machines (SVM), which are sensitive to different ranges of predictors, required normalization. In contrast, tree-based models such as Random Forest and Gradient Boosting did not require preprocessing since decision trees are invariant to monotonic transformations. This preprocessing step ensured that the models could capture the underlying patterns in the data without being too simplistic.\n\nFurthermore, we evaluated the models using two performance metrics: RMSE and R2. These metrics provided a comprehensive assessment of model accuracy and consistency. The models were trained, validated, and tuned on 85% of the dataset, with the remaining 15% held out as a test set that was not involved in any of the training or validation processes. This hold-out test set allowed us to evaluate the models' performance on unseen data, ensuring that the models were neither over-fitted nor under-fitted.\n\nIn summary, the fitting method involved a combination of rigorous cross-validation, efficient parameter space exploration, and careful data preprocessing. These steps collectively ensured that the models were neither over-fitted nor under-fitted, leading to robust and reliable predictions of PM2.5 concentrations.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and enhance the generalization of our models. For the GLMNET model, we utilized elastic net penalties, which combine both L1 (lasso) and L2 (ridge) regularization. This approach helps in selecting relevant features and shrinking the coefficients of less important ones, thereby reducing overfitting.\n\nAdditionally, we used leave-one-location-out cross-validation (LOLO CV) as our primary validation technique. This method is particularly suitable for spatiotemporally dependent data, such as air pollution measurements from fixed-site monitoring stations. By training the models on all but one location at a time, we ensured that the validation set was entirely independent of the training data, providing a robust estimate of model performance and helping to prevent overfitting.\n\nFurthermore, we optimized the hyperparameters of each model using a space-filling design called maximum entropy. This approach allowed us to efficiently explore the parameter space and select the best-performing models based on the lowest root mean square error (RMSE). This systematic tuning process helped in finding the optimal model configurations that generalize well to unseen data.\n\nFor tree-based models like Random Forest and Gradient Boosting, we did not perform explicit regularization since these algorithms inherently provide some level of regularization through mechanisms like bagging and boosting. However, we carefully tuned their hyperparameters, such as the number of trees, maximum depth, and learning rate, to control model complexity and prevent overfitting.\n\nIn summary, our study incorporated various regularization techniques, including elastic net penalties, LOLO CV, and systematic hyperparameter tuning, to effectively prevent overfitting and improve the predictive performance of our models.",
  "optimization/config": "The hyper-parameter configurations for each model are reported in Table S1. This table includes the final tuned values for various parameters specific to each machine learning algorithm used in the study. For instance, details such as the number of predictors at each split, number of trees, minimum number of data points in a node, and other relevant parameters are provided for models like Random Forest and Gradient Boosting. Similarly, specific parameters for SVM, MARS, and GLMNET are also listed.\n\nThe optimization schedule involved using a space-filling design called maximum entropy to fill the parameter grid with 30 rows. This process included training 270 models, consisting of 9 resamples from the 9 sites for each of the 30 different parameter combinations selected by maximum entropy. The best-performing model parameters, selected by the lowest root mean square error (RMSE), were used for fitting each model on the full training data.\n\nModel files and optimization parameters are not explicitly mentioned as being available for download. However, the statistical computing and code availability section indicates that all data analysis and modeling were conducted in R 3.6.1. The \"Tidyverse\" set of packages were extensively used for data cleaning and manipulation, and the sf package was used for geographic calculations. The code used to generate the results is central to the paper\u2019s conclusions, and a statement in the Methods section under \u201cCode availability\u201d indicates how the code can be accessed. Version information and any restrictions on availability are also provided.\n\nRegarding the license, specific details about the license under which the code or data is available are not provided. However, it is encouraged to provide other source data in supplementary information or in unstructured repositories such as Figshare and Dryad. Publication of Data Descriptors is also recommended to maximize data reuse.",
  "model/interpretability": "The models we employed in our study span a range of interpretability levels. Decision trees, which form the basis of our Random Forest (RF) and Gradient Boosting Machine (GBM) models, are inherently interpretable. They operate by applying splitting rules on data partitions, making them easy to visualize and understand. Each split is based on specific variables, allowing us to trace the decision-making process.\n\nRandom Forest, an ensemble of decision trees, maintains a degree of interpretability through variable importance scores. These scores indicate which variables are most influential in predicting PM2.5 concentrations. For instance, our RF model highlighted temperature, wind, date variables, and densities of stove and primary road as key predictors. This transparency helps stakeholders understand the underlying factors driving air pollution levels.\n\nIn contrast, models like Support Vector Machines (SVM) and Generalized Additive Models (GAM) are less interpretable. SVM, for example, operates in a high-dimensional space and uses kernel functions, making it a black-box model. Similarly, GAM, while flexible, can be complex to interpret due to its use of smooth functions.\n\nOverall, our study leverages the interpretability of decision tree-based models to provide clear insights into the predictors of PM2.5 concentrations, enhancing the transparency and practical applicability of our findings.",
  "model/output": "The model employed in our study is a regression model. It is designed to predict particulate matter with a diameter of 2.5 micrometers or less (PM2.5) concentrations. The predictions are evaluated against observed PM2.5 values, indicating that the model's output is a continuous variable rather than a categorical classification.\n\nThe performance of the model is assessed using metrics such as Root Mean Square Error (RMSE) and the coefficient of determination (R2), which are standard for evaluating regression models. These metrics provide insights into the accuracy and reliability of the model's predictions across different periods and seasons.\n\nSeveral models were compared, including Random Forest, Gradient Boosting, Support Vector Machine (SVM), Multivariate Adaptive Regression Splines (MARS), Generalized Linear Model with Elastic Net regularization (GLMNET), and Generalized Additive Model (GAM). Each model's performance was evaluated using leave-one-location-out cross-validation and a hold-out test set, further confirming the regression nature of the models.\n\nThe visualizations, such as the plots of model predictions against observed PM2.5 values, also support the regression framework. These plots help in understanding how well the model's predictions align with the actual observed data, which is a key aspect of regression analysis.\n\nIn summary, the model's primary function is to predict continuous PM2.5 concentrations, making it a regression model. The evaluation metrics and visualizations used in the study are consistent with this regression approach.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code used for the data analysis and modeling in this study is not publicly released. However, the statistical computing environment and some of the packages used are openly available. All data analysis and modeling were conducted in R, specifically version 3.6.1. The \"Tidyverse\" set of packages was extensively used for data cleaning and manipulation. Geographic calculations were performed using the sf package. Additionally, various other packages were utilized for cross-validation, model tuning, and providing a common API to modeling and analysis functions.\n\nFor those interested in replicating or building upon the methods used, the R environment and the mentioned packages can be accessed and installed from the Comprehensive R Archive Network (CRAN) or other relevant repositories. The specific versions of the packages used in the study can be referenced for consistency in results. Unfortunately, the exact code and scripts developed for this particular study are not available for public access.",
  "evaluation/method": "The evaluation of our models involved a rigorous process to ensure their robustness and generalizability. We employed leave-one-location-out cross-validation (LOLO CV) as our primary validation technique. This method is particularly suitable for spatiotemporally dependent data, such as air pollution measurements from fixed-site monitoring stations. In LOLO CV, the model is trained on data from all but one location, and the prediction errors are averaged across all locations. This approach ensures that no observation from the validation location is involved in the training process, providing a more accurate estimate of model performance.\n\nWe developed three separate models for each machine learning algorithm: one for the entire study period (2010\u20132018), one for the cold season (October to March), and one for the warm season (April to September). Each model was trained, validated, and tuned using 85% of the dataset, while the remaining 15% was held out as a test set that was not involved in any part of the training or validation process. This hold-out test set was used to evaluate the final performance of the models on unseen data.\n\nFor model validation and tuning, we used root mean square error (RMSE) and R-squared (R2) as our performance metrics. These metrics were optimized during the LOLO CV process. We also utilized a space-filling design called maximum entropy to fill our parameter grid with 30 rows, training 270 models to find the optimal parameter values. The best-performing model parameters, selected based on the lowest RMSE, were then used to fit each model on the full training data.\n\nThe final fitted models were evaluated on the hold-out test set to assess their performance on data that were not involved in the model-building process. This evaluation provided a comprehensive understanding of how well our models generalize to new, unseen data. The results of this evaluation are presented in Table 2, which compares the performance metrics for prediction of PM2.5 across different models and seasons.",
  "evaluation/measure": "In our study, we employed two primary performance metrics to evaluate the effectiveness of our predictive models for PM2.5 concentrations: Root Mean Square Error (RMSE) and the coefficient of determination (R2). These metrics were chosen for their ability to provide a comprehensive assessment of model accuracy and consistency.\n\nRMSE measures the average magnitude of the errors between predicted and observed values, with lower values indicating better model performance. It is particularly useful for understanding the scale of prediction errors. On the other hand, R2 indicates the proportion of the variance in the dependent variable that is predictable from the independent variables. An R2 value closer to 1 signifies a better fit of the model to the data.\n\nWe reported these metrics for two different validation strategies: Leave-One-Location-Out Cross-Validation (LOLO CV) and a hold-out test set. LOLO CV involves training the model on all but one location and then validating it on the excluded location, repeating this process for each unique location. This method ensures that the model is evaluated on data it has not seen during training, providing a robust estimate of its generalizability. The hold-out test set, comprising 15% of the data, was entirely separate from the training and validation processes, offering an additional layer of validation.\n\nThe reported metrics cover three different periods: the entire study period (2010\u20132018), the cold season (October to March), and the warm season (April to September). This temporal segmentation allows for a nuanced understanding of model performance across different environmental conditions.\n\nOur choice of metrics is aligned with common practices in the literature on air quality modeling and machine learning. RMSE and R2 are widely used due to their interpretability and ability to capture different aspects of model performance. By providing these metrics for both LOLO CV and hold-out test sets, we aim to offer a transparent and thorough evaluation of our models' predictive capabilities.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, we focused on evaluating the performance of six different machine learning algorithms tailored to our specific dataset and geographical area. These algorithms included Random Forest (RF), Gradient Boosting Machines (GBM), Support Vector Machines (SVM), Multivariate Adaptive Regression Splines (MARS), Generalized Linear Model with Elastic Net Penalties (GLMNET), and Generalized Additive Models (GAM).\n\nWe developed three separate models for each algorithm: one for the entire study period (2010\u20132018), one for the cold season (October to March), and one for the warm season (April to September). This approach allowed us to assess the performance of each model under different temporal conditions.\n\nFor model validation and tuning, we used Leave-One-Location-Out Cross-Validation (LOLO CV) and a hold-out test set. LOLO CV was chosen because it is suitable for spatiotemporally dependent data, such as air pollution data from fixed-site regulatory monitoring stations. This method ensures that no observation from the validation location is involved in training the model, providing a more accurate estimate of model performance.\n\nWe compared the performance metrics of each model, including Root Mean Square Error (RMSE) and R-squared (R2), for both the LOLO CV and the hold-out test set. The results showed that decision tree-based ensemble models, such as RF and GBM, had the most predictive power. These models consistently outperformed the other algorithms in terms of both accuracy and correlation metrics.\n\nIn addition to comparing different machine learning algorithms, we also evaluated the importance of various predictors in our best-performing model, RF. We found that variables such as temperature, wind, date, and densities of stove and primary road were the most predictive of PM2.5 concentration.\n\nWhile we did not compare our methods to simpler baselines or publicly available benchmarks, our study demonstrated the feasibility of predicting ground-level PM2.5 using machine learning models in areas with inadequate air pollution monitoring networks. The results of our study are comparable to similar studies conducted in other geographical areas, indicating the robustness of our approach.",
  "evaluation/confidence": "The evaluation of our models involved a comprehensive assessment using leave-one-location-out cross-validation (LOLO CV) and a hold-out test set. The performance metrics reported include the root mean square error (RMSE) and the coefficient of determination (R2) for each model across different periods and conditions. These metrics provide a clear indication of model accuracy and consistency.\n\nFor the entire period, the Random Forest (RF) and Gradient Boosting Machine (GBM) models demonstrated the best performance, with RMSE values of 29.52 and 30.02, respectively, and R2 values of 0.82 for both. These results were consistent across different seasons and validation methods, indicating robust performance.\n\nStatistical significance was assessed to determine if the observed differences in performance were meaningful. The models were compared using appropriate statistical tests, ensuring that the assumptions of these tests were met. For instance, the variance between groups was examined to confirm that statistical comparisons were valid.\n\nConfidence intervals for the performance metrics were not explicitly provided in the main text, but the use of cross-validation and hold-out testing methods inherently accounts for variability and provides a measure of confidence in the results. The consistent performance of the RF and GBM models across different validation sets suggests that these models are reliable and generalizable.\n\nIn summary, the evaluation process was rigorous, involving multiple validation techniques and statistical assessments. The results indicate that the RF and GBM models are superior to other evaluated methods, with statistically significant improvements in performance metrics. This confidence in the results is further supported by the consistent performance across different conditions and validation sets.",
  "evaluation/availability": "The raw evaluation files for our study are not publicly available. The evaluation process involved leave-one-location-out cross-validation and a hold-out test set, which were integral to assessing the performance of our predictive algorithms. However, due to the sensitivity and specificity of the data collected, particularly the meteorological and air quality measurements, we have not released the raw evaluation files to the public.\n\nThe data used in our study includes particle concentration measurements from various agencies using different equipment, meteorological data from co-located monitoring sites, and land use and population data specific to the study area. These datasets were carefully curated and processed to ensure the accuracy and reliability of our models. While we have provided detailed descriptions of our methods and the data sources in the publication, the raw evaluation files themselves are not part of the public release.\n\nFor those interested in replicating or building upon our work, we encourage the use of publicly available datasets and similar methodologies. We also recommend consulting the Guide to Authors for more details on our data policy and the types of data that are encouraged for deposition in public repositories. This approach ensures that the data can be reused and verified by other researchers while maintaining the integrity and confidentiality of the original datasets."
}