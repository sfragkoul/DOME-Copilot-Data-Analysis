{
  "publication/title": "Modelling compound cytotoxicity using conformal prediction and PubChem HTS data.",
  "publication/authors": "Svensson F, Norinder U, Bender A",
  "publication/journal": "Toxicology research",
  "publication/year": "2017",
  "publication/pmid": "30090478",
  "publication/pmcid": "PMC6061930",
  "publication/doi": "10.1039/c6tx00252h",
  "publication/tags": "- Conformal Prediction\n- Predictive Modeling\n- Toxicology\n- Machine Learning\n- Random Forests\n- Chemical Diversity\n- PubChem\n- Drug Discovery\n- Classification\n- Imbalanced Data Sets\n- Chemoinformatics\n- Predictive Toxicology\n- Molecular Descriptors\n- Data Validation\n- Computational Toxicology",
  "dataset/provenance": "The dataset used in this study was sourced from the PubChem BioAssay database. This publicly available repository contains chemical compounds and associated assay data. The specific datasets queried were those related to cytotoxicity screening, each containing more than 20,000 tested compounds. The selected datasets were downloaded and processed to neutralize structures and remove salts using CORINA. Standardization was performed using the IMI eTOX project standardizer in combination with the MolVS standardizer for tautomer standardization.\n\nThe collected data includes a total of 441,396 unique PubChem compound identifiers (CIDs). Among these, 16,228 unique CIDs were identified as toxic in at least one assay, with only 3,967 CIDs being toxic in more than one assay. The data is highly imbalanced, with the fraction of toxic compounds ranging from 0.13% to 6.03%, averaging around 0.8%.\n\nThe PubChem data set AID 364 served as an external test set for AID 463, as it was deposited by the same assay provider and run using the same protocol. After processing, the AID 364 dataset contained 3,247 non-toxic and 48 toxic compounds.\n\nPrevious studies have utilized various machine learning approaches for predicting compound cytotoxicity based on in vitro data. These approaches include neural networks, random forests, decision trees, linear regression, and Bayesian learning. Different techniques to handle data imbalance have also been applied, such as under-sampling, over-sampling, and Bayesian learning. The main source for obtaining cytotoxicity data has been PubChem, although some studies have also used internal data from assays carried out at specific organizations.",
  "dataset/splits": "The dataset used in this study was split into training and test sets. For internal validation, 80% of the data was used for training, while the remaining 20% was kept as a fixed test set. This split was applied to various assays, with the specific distribution of data points varying by assay.\n\nFor example, in the case of AID 463, the training set consisted of a majority of the data, while the test set contained the remaining portion. Additionally, AID 364 served as an external test set for AID 463. This external test set was deposited by the same assay provider and run using the same protocol, containing 3247 non-toxic and 48 toxic compounds.\n\nThe performance of the models was evaluated on both the internal test set and the external test set, with the results indicating a slight drop in validity and accuracy for the external data compared to the training data. This approach allowed for a comprehensive assessment of the model's performance across different data splits.",
  "dataset/redundancy": "The datasets were split randomly into training and test sets, with 80% of the data used for training and 20% reserved for testing. To ensure independence between the training and test sets, the process was repeated 100 times, each time storing the predictions on the test set. The median predicted probability for each compound was then calculated and used for class assignment according to the set confidence levels.\n\nAdditionally, further evaluation was conducted by randomly selecting 20% of each dataset as a fixed external test set. One hundred models were trained on the remaining training data for each dataset, with new random splits for the proper training and calibration sets at each iteration. These models were then used to predict the external test sets.\n\nFor the model built on the data from AID 463, AID 364 was applied as an external test set. This approach ensured that the training and test sets were independent and that the models were evaluated on unseen data.\n\nThe distribution of the datasets in this study is comparable to previously published machine learning datasets for predictive toxicology. The datasets were highly imbalanced, with a fraction of toxic compounds spanning from 0.13 to 6.03%, and an average of 0.8%. This imbalance is typical in cytotoxicity screening data, where a large number of compounds are screened to find a few active (or toxic) compounds. The datasets included a total of 441,396 unique PubChem compound identifiers (CIDs), with 16,228 unique CIDs being toxic in at least one assay and only 3,967 CIDs being toxic in more than one assay. This highlights the cell-type specificity of cytotoxic effects and the structural diversity of the toxic compounds.",
  "dataset/availability": "The data used in this study is publicly available from the PubChem BioAssay database. This database is a repository of chemical compounds and associated assay data, which includes various assays for cell viability and cell proliferation inhibition, collectively referred to as cytotoxicity.\n\nThe datasets were manually queried for cytotoxicity screening with more than 20,000 tested compounds. The selected datasets were downloaded and processed to neutralize structures and remove salts using CORINA. Structure standardization was performed using the IMI eTOX project standardizer in combination with the MolVS standardizer for tautomer standardization.\n\nThe data splits used in the study, including the training and test sets, were created by randomly dividing each dataset into an 80% training set and a 20% test set. The training set was further divided into a proper training set (70%) and a calibration set (30%). This process was repeated 100 times to ensure robustness and reliability of the results.\n\nThe data is available for public use, and the specific details regarding the assays and compounds can be found in the PubChem entries for the respective Assay IDs (AIDs). The data processing steps and the methods used for model development are described in detail in the methods section of the publication.\n\nThe data is released under the terms of the PubChem database, which allows for public access and use for research purposes. The specific license details can be found on the PubChem website. The enforcement of data availability and usage is managed by the PubChem database administrators, ensuring that the data is accessible and properly cited in any research that uses it.",
  "optimization/algorithm": "The optimization algorithm employed in this study is based on the random forest machine learning technique. Random forests are an ensemble learning method that operates by constructing multiple decision trees during training and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees. This approach is well-established and widely used in various domains due to its robustness and ability to handle large datasets with high dimensionality.\n\nThe random forest algorithm is not new; it has been extensively studied and applied in numerous research areas. The choice to use random forests in this context is driven by their proven effectiveness in handling complex datasets and their ability to provide reliable predictions. The algorithm's versatility allows it to be integrated into the conformal prediction framework, which is a key aspect of this study. This framework ensures that predictions come with a guaranteed error rate, providing a level of confidence in the results.\n\nThe decision to publish this work in a toxicology journal rather than a machine-learning journal is justified by the focus of the study. The primary objective is to demonstrate the application of conformal prediction in modeling cytotoxicity, which is a critical area in toxicology research. The integration of random forests within this framework is a means to an end, rather than the end itself. The study aims to showcase the practical benefits of conformal prediction in handling imbalanced datasets and guiding further experimental work, which is of significant interest to the toxicology community.",
  "optimization/meta": "The model employed in this study does not function as a meta-predictor. Instead, it relies on a single machine-learning algorithm, specifically Random Forest (RF), as the underlying model for predictions. The Random Forest algorithm was chosen for its robustness and ability to deliver reliable results even without case-specific calibration.\n\nThe Random Forest models were developed using Python, Scikit-learn version 0.17, and the nonconformist package version 1.2.5. Binary classification models were constructed using the Scikit-learn RandomForestClassifier with 500 trees, and all other options were set to their default values. Conformal predictions were performed using the ProbEstClassifierNC and IcpClassifier functions in the nonconformist package, with options enabled for class-conditional conformal predictions.\n\nThe data sets were randomly divided into training (80%) and test sets (20%). The training set was further split into a proper training set and a calibration set using 70% and 30% of the training data, respectively. This process was repeated 100 times to ensure the robustness of the model. The median predicted probability for each compound was then calculated and used for class assignment in accordance with the set confidence levels.\n\nAdditionally, further evaluation was conducted by randomly selecting 20% of each data set as a fixed external test set. One hundred models were trained on the remaining training data for each data set, with new random splits for the proper training and calibration sets at each iteration. These models were then used to predict the external test sets. For the model built on the data from AID 463, AID 364 was applied as an external test set.\n\nThe performance of the models was assessed using various metrics, including accuracy and coverage for both the non-toxic and toxic classes. The results indicated that the internal validation procedure from the aggregated conformal predictors provided accurate estimates of the model performance for new data. The models demonstrated good predictive performance despite the large imbalance between the number of toxic and non-toxic compounds.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the quality and consistency of the input for the machine-learning algorithm. The data was collected from the PubChem BioAssay database, focusing on cytotoxicity screening assays with more than 20,000 tested compounds. The structures of these compounds were neutralized, and salts were removed using CORINA. Structure standardization was performed using the IMI eTOX project standardizer in combination with the MolVS standardizer for tautomer standardization, employing defined SMARTS patterns for these operations.\n\nActivity was assigned to compounds based on the PubChem outcome annotation, and records with missing or conflicting annotations were removed. This ensured that only reliable and consistent data was used for model training and validation.\n\nFor descriptor calculation, 97 different physicochemical descriptors were computed using RDKit. Additionally, Molprint2D fingerprints were generated using Canvas, applying Mol2 atom types and a maximum path length of two. To manage memory usage efficiently in the random forest algorithm, only bits present in at least 0.1% of the molecules were retained. This preprocessing step helped in reducing the dimensionality of the data while preserving essential information.\n\nThe conformal prediction framework, combined with the random forest algorithm, was used to build predictive models. The random forest was chosen for its robustness and ability to deliver reliable results without case-specific calibration. The models were developed using Python, Scikit-learn version 0.17, and the nonconformist package version 1.2.5. Binary classification models were constructed based on the random forest using the Scikit-learn RandomForestClassifier with 500 trees, and all other options were set to default. This approach ensured that the models were both efficient and effective in handling the highly imbalanced cytotoxicity data.",
  "optimization/parameters": "In our study, we utilized a Random Forest (RF) model as the underlying predictor. The RF model inherently manages a large number of parameters due to its ensemble nature, which involves multiple decision trees. Each tree within the forest considers a subset of the input features, and the number of trees in the forest is a key parameter.\n\nWe set the number of trees in the Random Forest to 500, which is a common choice that balances computational efficiency and model performance. All other parameters for the Random Forest were kept at their default settings, as the primary focus of our study was to introduce the framework of conformal prediction and its usefulness for predictive toxicology rather than optimizing the RF model itself.\n\nThe selection of the number of trees was based on empirical evidence from previous studies, which have shown that 500 trees generally provide robust and stable results without the need for case-specific calibration. This choice allows the model to capture complex patterns in the data while maintaining computational feasibility.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "In our study, we employed Random Forests (RF) as the underlying model for our predictors. This choice was driven by the robustness of RF, which has been shown to deliver reliable results even without case-specific calibration. The RF model was implemented using the Scikit-learn library, specifically the RandomForestClassifier with 500 trees, and all other options were set to their default values.\n\nGiven the complexity of the data and the potential for overfitting, particularly with a large number of parameters relative to the training points, we took several precautions. Firstly, the use of RF inherently helps mitigate overfitting due to its ensemble nature, which averages the predictions of multiple decision trees. Additionally, we employed conformal prediction, which provides a framework for making valid predictions according to a user-defined confidence level. This method ensures that the frequency of errors does not exceed the set confidence level, thereby controlling overfitting.\n\nTo further address the issue of overfitting, we divided each dataset into training (80%) and test sets (20%). The training set was further split into a proper training set (70%) and a calibration set (30%). This calibration set is crucial for the performance of conformal prediction in terms of validity. The size of the calibration set was chosen within a recommended range previously investigated for conformal prediction in combination with RF. This process was repeated 100 times, each time storing the predictions on the test set. The median predicted probability for each compound was then calculated and used for class assignment in accordance with the set confidence levels.\n\nMoreover, we performed additional evaluations by randomly selecting 20% of each dataset as a fixed external test set. We trained 100 models on the remaining training data for each dataset, with new random splits for the proper training and calibration sets at each iteration. These models were then used to predict the external test sets. This rigorous validation process helped ensure that our models were not overfitting to the training data.\n\nRegarding underfitting, the use of 500 trees in the RF model and the extensive feature engineering, including the calculation of 97 different physiochemical descriptors and Molprint2D fingerprints, helped capture the complexity of the data. The diversity of the features and the ensemble nature of RF ensured that the model could generalize well to unseen data. Furthermore, the conformal prediction framework provided a mechanism to assess the model's performance and adjust the confidence levels accordingly, ensuring that the model was neither too simplistic nor too complex.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was the application of Random Forests (RF), which is known for its ability to deliver robust results even without case-specific calibration. RF inherently helps in preventing overfitting by averaging multiple decision trees, reducing the variance and improving the generalization of the model.\n\nAdditionally, we utilized conformal prediction, which provides a framework for making predictions with guaranteed validity. This method involves dividing the dataset into training, calibration, and test sets, ensuring that the model's performance is evaluated on unseen data. The calibration set is crucial for the performance of conformal prediction in terms of validity, and we chose its size within the recommended range previously investigated for conformal prediction in combination with RF.\n\nFurthermore, we performed extensive model validation by repeating the training and testing process multiple times with different random splits. This approach helps in assessing the model's performance across various subsets of the data, providing a more reliable estimate of its generalization capability.\n\nWe also conducted additional evaluations by using a fixed external test set, which was not part of the training process. This step is essential for evaluating the model's performance on completely new data, ensuring that it can generalize well beyond the training dataset.\n\nIn summary, our study incorporated Random Forests, conformal prediction, and rigorous validation techniques to prevent overfitting and ensure the reliability and generalizability of our predictive models.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, we utilized the RandomForestClassifier from Scikit-learn with 500 trees, and all other options were set to their default values. The conformal predictions were performed using the ProbEstClassifierNC and IcpClassifier functions from the nonconformist package, version 1.2.5, with options for class conditional conformal predictions enabled.\n\nThe models were developed using Python, Scikit-learn version 0.17, and the nonconformist package. The specific versions of these tools are mentioned to ensure reproducibility. However, the exact model files and optimization schedules are not provided directly in the publication. Instead, the methods and parameters are described in detail to allow other researchers to replicate the experiments.\n\nRegarding the availability and licensing, the tools and libraries used, such as Scikit-learn and the nonconformist package, are open-source and freely available. Researchers can access these tools under their respective open-source licenses, which typically permit use, modification, and distribution. This ensures that the community can build upon our work and apply similar methodologies in their own research.",
  "model/interpretability": "The models developed in this study are not entirely black-box. The use of conformal prediction provides a level of transparency and reliability that is often lacking in traditional machine learning models. Conformal prediction allows for predictions to be made with a guaranteed error rate, which means that the confidence in the predictions can be quantified. This is a significant advantage because it enables decision-makers to understand the reliability of the model's outputs.\n\nOne of the key features of conformal prediction is its ability to handle imbalanced data without the need for additional considerations such as over- or undersampling. This is particularly important in the context of cytotoxicity modeling, where the data often contains a large imbalance between toxic and non-toxic compounds. The models built using conformal prediction with random forest were predictive for both classes, despite this imbalance.\n\nFurthermore, the predictions can serve to guide further experiments. For example, screening of additional compounds in the \"both\" category can increase the separation of the two classes, while screening of compounds from the \"empty\" category can serve to expand the model. This iterative process allows for continuous improvement of the model's performance.\n\nThe underlying machine learning algorithm used in this study was random forest, which is known for its robustness and ability to deliver reliable results even without case-specific calibration. However, the conformal prediction framework allows for any machine learning technique to be applied as long as it is paired with a suitable conformity function. This flexibility means that already validated modeling workflows can be rapidly converted into a conformal prediction framework, underlining the versatility of the method.\n\nIn summary, the models developed in this study are transparent in the sense that they provide quantifiable confidence levels for their predictions. This transparency is achieved through the use of conformal prediction, which combines the reliability of the results with the ability to handle imbalanced data effectively. The models are not black-box; instead, they offer a clear and interpretable framework for making predictions in the context of cytotoxicity modeling.",
  "model/output": "The model developed in this study is a binary classification model. It was built using the RandomForestClassifier from Scikit-learn, with 500 trees and default settings for all other options. The model's primary purpose is to predict whether compounds are toxic or non-toxic against various cell lines. The predictions are made with a certain confidence level, and the model's performance is evaluated based on metrics such as accuracy, coverage, and validity for both the toxic and non-toxic classes. The model's output includes predicted probabilities for each compound, which are then used for class assignment based on the set confidence levels. The predictions are also validated using both internal and external test sets to ensure their reliability and generalizability.",
  "model/duration": "The execution time of the models varied depending on the descriptors used. Models utilizing Molprint2D fingerprints had a significantly higher computational cost due to the large number of features involved. This increased cost was primarily due to the extensive feature set of Molprint2D, which required more processing time compared to other descriptors.\n\nTo mitigate this issue and ensure efficient analysis, additional analyses were conducted using RDKit descriptors. These descriptors provided similar model performance but with a much lower computational burden, making them more practical for further evaluations. The choice of RDKit descriptors was driven by their balance of performance and computational efficiency, allowing for thorough model validation and testing without excessive time constraints.",
  "model/availability": "The source code for the models developed in this study is not publicly released. The models were developed using Python, Scikit-learn version 0.17, and the nonconformist package version 1.2.5. The RandomForestClassifier from Scikit-learn was used with 500 trees and default settings for other options. The nonconformist package, which is available on GitHub, was utilized for implementing the conformal prediction framework. However, the specific scripts and data used to generate the models are not provided.\n\nThe software dependencies and versions used in this study are specified, allowing for reproducibility of the results. The nonconformist package is open-source and can be accessed via its GitHub repository. This package facilitates the implementation of conformal prediction, which is a key component of the modeling approach described. While the exact code and data are not shared, the tools and libraries used are publicly available, enabling others to replicate the methodology.",
  "evaluation/method": "The evaluation method employed in this study involved a combination of internal validation and external test set predictions. For internal validation, each dataset was randomly divided into a training set (80%) and a test set (20%). The training set was further split into a proper training set (70%) and a calibration set (30%). This process was repeated 100 times to ensure robustness, and the median predicted probability for each compound was used for class assignment based on the set confidence levels.\n\nAdditionally, we performed further evaluation by randomly selecting 20% of each dataset as a fixed external test set. We then trained 100 models on the remaining training data for each dataset, with new random splits for the proper training and calibration sets at each iteration. These models were used to predict the external test sets. For the model built on the data from AID 463, we specifically used AID 364 as an external test set, as it was deposited by the same assay provider and run using the same protocol.\n\nThe performance metrics evaluated included validity, accuracy, and coverage for both non-toxic and toxic classes. The results indicated that the internal validation procedure provided accurate estimates of the model performance on new data. The performance on the external test set showed a slight drop compared to the training data, highlighting the robustness of the models in handling real-world scenarios.",
  "evaluation/measure": "The performance of the models was evaluated using several key metrics to ensure a comprehensive assessment. The primary metrics reported include accuracy, coverage, and validity for both non-toxic and toxic classes. Accuracy measures the proportion of correct predictions out of the total predictions made, providing a straightforward indication of the model's performance. Coverage refers to the proportion of instances for which the model makes a prediction, highlighting the model's ability to provide predictions across the dataset. Validity ensures that the frequency of errors does not exceed the set confidence level, which is crucial for maintaining the reliability of the predictions.\n\nIn addition to these metrics, the models were evaluated at different confidence levels, specifically at 70% and 80%. This approach allows for a nuanced understanding of how the model's performance varies with different levels of confidence. The results indicate that the models maintain a balance between accuracy and coverage, which is essential for practical applications.\n\nThe performance metrics reported are representative of those commonly used in the literature for similar studies. For instance, previous models built on PubChem data have reported sensitivity and specificity, which are analogous to the accuracy and coverage metrics used here. This consistency with established practices in the field ensures that the results are comparable and meaningful within the broader context of predictive toxicology.\n\nThe models were also tested on both internal and external datasets. Internal validation involved using a fixed test set, while external validation utilized an additional assay (AID 364) that was conducted by the same depositor using the same protocol. This dual approach provides a robust evaluation of the model's generalizability and performance on new, unseen data. The results show a slight drop in performance for the external dataset, which is expected and indicates the model's ability to handle real-world variability.\n\nOverall, the set of performance metrics used in this study is comprehensive and aligns with established practices in the field. The focus on accuracy, coverage, and validity at multiple confidence levels, along with both internal and external validation, ensures a thorough and reliable evaluation of the models' performance.",
  "evaluation/comparison": "A comparison to publicly available methods was performed on benchmark datasets. Specifically, for the assay AID 364, model performance was compared to previous models that were also based on data from the Jurkat cell line. These previous studies reported varying sensitivities and specificities, highlighting the challenges in generating balanced models with similar predictive power for both toxic and non-toxic classes. For instance, one study reported a sensitivity of 56% and a specificity of 80%, while another showed a sensitivity of 82% and a specificity of 35%. These comparisons underscore the difficulties in achieving consistent and reliable predictions across different datasets and methods.\n\nRegarding simpler baselines, the focus was primarily on evaluating the performance of conformal predictors using different descriptors and comparing their accuracy and coverage. Models using Molprint2D fingerprints and RDKit descriptors were evaluated, showing similar performance in terms of accuracy at various confidence levels. This indicates that the models are not overly sensitive to the choice of descriptor, suggesting robustness in the methodology. However, Molprint2D fingerprints were found to have a higher computational cost due to the large number of features, leading to the decision to use RDKit descriptors for further analyses.\n\nNot applicable.",
  "evaluation/confidence": "The evaluation of our models includes an assessment of confidence levels, which is a key aspect of our conformal prediction framework. This framework allows us to make predictions with a guaranteed error rate, providing a measure of confidence in our results. For instance, at the 80% confidence level, the average accuracy for single class predictions is 78% for the non-toxic class and 80% for the toxic class. The coverage, which indicates the proportion of predictions that fall within the confidence interval, is 83% for the non-toxic class and 87% for the toxic class at the same confidence level.\n\nThe performance metrics do not explicitly state confidence intervals in the traditional sense, but the use of conformal prediction ensures that the predictions are valid with a specified level of confidence. This means that the results are statistically significant in the context of the chosen confidence level. For example, the validity of the predictions for both the toxic and non-toxic classes is maintained across different confidence levels, as shown in the tables provided.\n\nThe models were also evaluated on external data to assess their generalizability. For AID 463, an external test set (AID 364) was used, and the performance metrics showed a slight drop compared to the internal validation. However, the coverage remained practically unchanged for the non-toxic class and increased for the toxic class. This indicates that the models are robust and can perform well on new, unseen data.\n\nIn summary, the conformal prediction framework provides a reliable way to evaluate the performance of our models, ensuring that the predictions are made with a specified level of confidence. The results are statistically significant, and the models demonstrate good performance on both internal validation and external test sets.",
  "evaluation/availability": "Not enough information is available."
}