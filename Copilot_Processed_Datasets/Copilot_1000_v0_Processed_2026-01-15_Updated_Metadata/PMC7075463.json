{
  "publication/title": "Predicting HLA class II antigen presentation through integrated deep learning.",
  "publication/authors": "Chen B, Khodadoust MS, Olsson N, Wagar LE, Fast E, Liu CL, Muftuoglu Y, Sworder BJ, Diehn M, Levy R, Davis MM, Elias JE, Altman RB, Alizadeh AA",
  "publication/journal": "Nature biotechnology",
  "publication/year": "2019",
  "publication/pmid": "31611695",
  "publication/pmcid": "PMC7075463",
  "publication/doi": "10.1038/s41587-019-0280-2",
  "publication/tags": "- HLA class II antigen presentation\n- Deep learning\n- Predictive performance\n- Peptide binding\n- Gene expression\n- Immunopeptidome\n- Machine learning\n- Bioinformatics\n- Computational biology\n- HLA-DR ligands",
  "dataset/provenance": "The dataset used in this study is derived from various sources, primarily focusing on HLA ligands identified through mass spectrometry (MS) analysis. The SysteMHC Atlas was constructed to consolidate HLA-I and HLA-II ligand sequences from diverse studies, providing a comprehensive resource for training and validating predictive models like MARIA. The SysteMHC Atlas includes data from multiple studies, such as those conducted on melanoma tissues, B cell lines, and ovarian carcinoma, which were profiled using specific antibodies like SPV-L3, HB-145, and W6/32.\n\nThe Immune Epitope Database (IEDB) is another significant source, containing quantitative HLA-DR binding affinities for approximately 12,000 non-redundant peptide sequences. Additionally, specific studies have identified over 23,000 and 16,000 non-redundant peptide sequences through HLA-DR immunoprecipitation and MS analysis. These datasets are crucial for training and validating MARIA, as they provide a large and diverse set of naturally presented HLA-II peptides.\n\nThe data used in this study includes HLA ligands from mantle cell lymphomas (MCLs) representing 16 HLA-DR alleles, as well as ligands from the MCL-derived JeKo-1 cell line. The study also utilizes gene expression data from various sources, including RNA-seq results from previous studies on MCL patients and the JeKo-1 cell line. When personalized gene expression profiles are not available, expression profiles are estimated from the corresponding tumor type using median values from relevant datasets like TCGA RNA-seq results.\n\nThe dataset includes a mix of publicly available data and data generated specifically for this study. For example, ligandomes are available from the PRIDE Archive under accession numbers PXD004746 and PXD005704, and data from two K562 ligandomes are provided in supplementary tables. The remaining HLA ligand datasets are publicly available from the provided references, ensuring that the community has access to the data used in this research.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "To ensure the robustness and generalizability of our model, we took several steps to manage dataset redundancy. We used a recurrent neural network (RNN) framework, which is well-suited for handling variable-length sequence data, a common challenge in HLA-II peptide ligands that range from 8 to 26 amino acids.\n\nWe employed a cross-validation strategy where we filtered out any peptides in the validation set that were substrings or highly similar to training peptides. This filtering process was crucial to prevent model overfitting due to similarities between the training and validation sequences. Specifically, during cross-validation, we excluded any peptides in the validation set that were substrings (where A is part of B or B is part of A) of any training peptides. This approach helped to ensure that the model was evaluated on truly independent data, thereby providing a more accurate assessment of its performance.\n\nAdditionally, we used length-matched random human peptides as negative examples, maintaining a positive-to-negative ratio of 1:3 for training and 1:1 for validation. This strategy helped to balance the dataset and improve the model's ability to distinguish between presented and non-presented peptides.\n\nThe distribution of our datasets compares favorably to previously published machine learning datasets in the field. By incorporating empirical in vitro HLA binding measurements, gene expression levels, and MS-based antigen presentation profiling datasets, we aimed to create a more comprehensive and accurate model for predicting HLA-II peptide presentation. This approach allowed us to leverage the strengths of different data types and improve the overall performance of our model.",
  "dataset/availability": "The data used in this study is available through various means. Ligandomes can be accessed from the PRIDE Archive under the accession numbers PXD004746 and PXD005704. Additionally, data from two K562 ligandomes is provided in Supplementary Tables 5 and 6. The remaining HLA ligand datasets are publicly available through the references provided in the study. All other data can be obtained from the corresponding authors upon reasonable request. This ensures that the data is accessible for further research and verification.",
  "optimization/algorithm": "The machine-learning algorithm class used in our work is a recurrent neural network (RNN) framework, a type of deep learning model. This choice was driven by the need to handle the high variability in the length of HLA-II peptide ligands, which can range from 8 to 26 amino acids. RNNs are well-suited for processing sequential data of varying lengths, making them an effective tool for this task.\n\nThe algorithm developed is not entirely new; it builds upon established deep learning techniques. However, its application to predicting HLA-II antigen presentation is novel. The integration of multiple biological features, such as MS-based antigen presentation profiling datasets, traditional HLA binding affinity data, and gene expression levels, represents a significant advancement in the field.\n\nThe decision to publish in a biotechnology journal rather than a machine-learning journal was strategic. The primary focus of our work is on improving the prediction of HLA-II antigen presentation, which has direct implications for immunology and cancer research. While the machine-learning aspects are crucial to our methodology, the biological significance and potential applications of our findings are of paramount importance. Therefore, publishing in a biotechnology journal ensures that our work reaches the relevant scientific community and has the greatest impact on future research and clinical practices.",
  "optimization/meta": "The model MARIA is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, MARIA is a deep learning model that directly learns from mass spectrometry (MS)-identified HLA ligands, in addition to traditional HLA binding affinity data and gene expression levels. The model employs a recurrent neural network (RNN) framework, which is well-suited for handling variable-length sequence data, such as the HLA-II peptide ligands that can range from 8 to 26 amino acids in length.\n\nThe training process for MARIA involves several steps to ensure robustness and prevent overfitting. One key step is filtering out any peptides in the cross-validation set that are substrings or highly similar to a training peptide. This ensures that the model generalizes well to new, unseen data. The performance of MARIA is evaluated using various combinations of biological features, including peptide sequences, gene expression levels, and protease cleavage signatures. This comprehensive approach allows MARIA to predict active HLA-II presentation in vivo more accurately than previous methods that rely solely on in vitro binding affinities.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithm. Each amino acid in a peptide sequence was represented using a 21-number one-hot vector. This vector includes 20 common amino acids plus an additional category for any uncommon amino acids, denoted as 'X'. The one-hot vector is a sparse vector of zeros with a single one indicating the specific amino acid species, following a consistent but arbitrary mapping. For instance, the first position in the vector might represent alanine. This encoding method transforms each peptide sequence into a two-dimensional vector of size (21, L), where L is the length of the peptide.\n\nWe also explored alternative amino acid encodings such as BLOSUM50 and ProtVec, but these did not significantly influence the model's performance. Therefore, we proceeded with the one-hot encoding for its simplicity and effectiveness.\n\nFor gene expression values, we used log10(TPM + 0.001) to prevent neurons connected to gene expression inputs from dominating the optimization gradient. This transformation helps in normalizing the gene expression data and ensures that the model can effectively learn from both high and low expression values.\n\nIn addition to amino acid encoding, we built a neural network to estimate cleavage scores. This network considers six amino acids upstream and six amino acids downstream of the query peptide, totaling 12 amino acids. These flanking regions are encoded using the same one-hot encoding method and processed through hidden layers to output a probability score between 0 and 1. This approach helps in quantifying the likelihood of peptide cleavage, which is essential for predicting peptide presentation.\n\nTo avoid overfitting, especially when applying the model to lymphoma data, we trained the cleavage model on an independent dendritic cell line (MUTZ3) ligandome. This training strategy ensures that the model generalizes well to different datasets and reduces the risk of overfitting to specific training data.\n\nOverall, our data encoding and preprocessing steps were designed to handle the complexity of peptide sequences and gene expression data, ensuring that the machine-learning algorithm could effectively learn and make accurate predictions.",
  "optimization/parameters": "In the MARIA model, the number of parameters used is determined by the architecture of the neural networks employed. For the LSTM networks, we explored configurations with 32, 64, and 128 neurons, with depths ranging from one to four layers. The optimal configuration was found to be an LSTM network with 64 neurons and one layer of depth. This LSTM layer is followed by dense layers with 32 neurons each, utilizing the rectified linear unit (ReLU) activation function. Dropout of 40% is applied to each layer for regularization. The model also includes concatenation of the LSTM and dense layers, followed by two additional dense layers with 32 neurons each. The output layer contains two neurons representing the non-presenting and presenting classes.\n\nThe selection of these parameters was based on extensive experimentation and validation. We used a 9:1 training:validation scheme to evaluate different configurations. The final model architecture was chosen because it provided the best performance in predicting peptide presentation. Regularization techniques such as L1 and L2 were attempted but were not included in the final model due to their lack of influence on performance. The model was trained using the TensorFlow GPU backend to enable parallel calculation of gradients, although the CPU backend is sufficient for general user applications.",
  "optimization/features": "The MARIA model utilizes several input features to predict peptide presentation by the HLA-DR complex. These features include peptide sequences, patient or cell HLA-DR alleles, and corresponding gene names. Additionally, the model estimates HLA peptide-binding affinities and cleavage scores using separate neural network models. Gene expression levels of each input gene are also considered, based on external tissue-matched RNA-seq results. The gene expression values are transformed using log10(TPM + 0.001) to prevent neurons from dominating the optimizing gradient.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the model integrates various biological features that are known to influence HLA-II peptide presentation. These features include peptide sequence, gene expression levels, binding affinities, and cleavage scores. The model was trained using HLA-II ligands identified by MS-based antigen presentation profiling, along with empirical in vitro HLA binding measurements and gene expression levels. The use of these features was guided by biological relevance rather than a statistical feature selection process.\n\nThe training process involved filtering out any peptides in the cross-validation set that were substrings or highly similar to a training peptide to prevent overfitting. This approach ensures that the model generalizes well to new data. The performance of the full model, as well as other models trained on each possible combination of biological features, was evaluated to determine the optimal set of input features.",
  "optimization/fitting": "The MARIA model employs a recurrent neural network (RNN) framework, specifically using Long Short-Term Memory (LSTM) networks, to handle the variable-length sequence data of HLA-II peptides. The LSTM network architecture was explored with different configurations, including 32, 64, and 128 neurons and depths ranging from one to four layers. The optimal configuration was found to be an LSTM network with 64 neurons and one layer of depth.\n\nTo prevent overfitting, several regularization techniques were applied. Dropout of 40% was used in each layer, which helps to reduce overfitting by randomly setting a fraction of input units to zero at each update during training time. Additionally, the model was trained on an independent dendritic cell line (MUTZ3) ligandome to ensure that it generalizes well to lymphoma data. Cross-validation was performed by excluding any peptides in the validation set that were substrings or highly similar to training peptides, further mitigating overfitting risks.\n\nThe model's architecture includes two fully connected hidden layers with 32 neurons each, using the rectified linear unit (ReLU) activation function. The output layer contains two neurons representing the non-presenting and presenting classes. The model was trained using the TensorFlow GPU backend to enable parallel calculation of gradients, which speeds up the training process. For general user applications, the CPU backend is sufficient.\n\nTo address the potential issue of underfitting, the model's performance was evaluated on held-out HLA-II ligand sets from various cell types and MS instruments. The evaluation included peptides of lengths between 8 and 26 amino acids, which is the range handled by the RNN. The model's performance was compared against existing methods, and it demonstrated robust and more accurate HLA-II prediction.\n\nIn summary, the MARIA model's architecture and training procedures were designed to balance complexity and generalization, with regularization techniques and independent validation sets used to rule out overfitting and underfitting. The model's performance was thoroughly evaluated to ensure its reliability and accuracy in predicting HLA-II peptide presentation.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting. We utilized dropout, a regularization method that involves randomly setting a fraction of input units to zero at each update during training time, which helps prevent overfitting. Specifically, we applied a dropout rate of 40% to each layer. Additionally, we explored L1 and L2 regularization techniques, but they were not included in the final model due to their lack of influence on model performance. To further mitigate overfitting, we filtered out any peptides in the cross-validation set that were substrings or highly similar to training peptides. This ensured that the model did not simply memorize the training data but rather learned generalizable patterns.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication and supplementary materials. Specifically, the configurations of the neural networks, including the number of neurons and layers in the LSTM networks, are described. The optimization parameters, such as the use of dropout for regularization and the activation functions employed, are also provided.\n\nModel files and specific optimization parameters are available upon request for academic research. Researchers can access the custom software code and related materials through the MARIA website at https://maria.stanford.edu/. For commercial entities interested in the software, it is advisable to contact Stanford University\u2019s Office of Technology Licensing, referencing docket S19-020.\n\nThe data used for training and validation, including the ligandomes, are accessible from the PRIDE Archive under specific accession numbers. Additionally, some datasets are provided in the supplementary tables accompanying the publication. All other data can be obtained from the corresponding authors upon reasonable request.",
  "model/interpretability": "The MARIA model, while leveraging complex neural network architectures, incorporates several design choices that enhance its interpretability. Unlike traditional black-box models, MARIA provides insights into its decision-making process through various mechanisms.\n\nFirstly, the model's architecture includes separate neural network components for estimating binding affinities and cleavage scores. These components process specific aspects of the input data, such as peptide sequences and flanking regions, making it clearer how different features contribute to the final prediction. For instance, the cleavage score model explicitly considers six amino acids upstream and downstream of the query peptide, providing a transparent view of the sequence context that influences cleavage predictions.\n\nAdditionally, MARIA uses one-hot encoding for amino acids, which is a straightforward and interpretable way to represent peptide sequences. This encoding ensures that each amino acid's contribution to the model's output is explicit and can be traced back to specific positions in the peptide sequence.\n\nThe model also integrates gene expression values, which are transformed using a log10 scale to prevent dominance in the optimization gradient. This transformation is a clear and interpretable step that ensures gene expression levels are appropriately weighted in the model's predictions.\n\nFurthermore, MARIA's output scores can be normalized to percentiles, which provides a human-interpretable measure of how likely a peptide is to be presented by a specific HLA-II complex. This normalization process involves comparing the raw output score to a distribution of scores from random peptides, making it easier to understand the relative likelihood of presentation.\n\nIn summary, while MARIA utilizes advanced neural network techniques, it includes several transparent components and design choices that enhance its interpretability. These features allow users to gain insights into how different factors, such as peptide sequences, flanking regions, and gene expression levels, contribute to the model's predictions.",
  "model/output": "The model, MARIA, is primarily a classification model designed to predict the probability of a given peptide being presented by an HLA-DR complex. It outputs a score between 0 and 1, where the score indicates the likelihood of presentation. This score can be further normalized to a percentile score for better interpretability and comparison across different peptide lengths. The percentile score is generated by comparing the raw output score to a distribution of scores from length-matched random human peptides. Additionally, MARIA includes a regression component for predicting binding affinities, using a pan-allele regression model to estimate binding affinity given a peptide-allele pair. This regression model outputs a single neuron representing the predicted binding affinity in log space.",
  "model/duration": "The execution time for the model varies depending on the hardware used. When processing ten thousand peptides, the model takes approximately 80 seconds with a 2.8 GHz Intel Core Xeon CPU. However, this time can be significantly reduced to about 11.3 seconds when utilizing a single NVIDIA Tesla K80 GPU. This demonstrates the efficiency gains possible with GPU acceleration for large-scale peptide processing tasks.",
  "model/availability": "The source code for the custom software described in our work is available for academic research upon request from the authors. Additionally, researchers can run our algorithm online through a web server accessible at https://maria.stanford.edu/. For commercial entities interested in the software, they should contact Stanford University\u2019s Office of Technology Licensing and reference docket S19-020. The software is designed to be accessible and usable by the research community, ensuring that our findings can be replicated and built upon by others in the field.",
  "evaluation/method": "To evaluate the performance of MARIA, we employed several rigorous methods. We obtained independent HLA-DR ligand sets from various cell types and mass spectrometry instruments. These sets were used to benchmark MARIA against existing binding-based methods. Peptides shorter than 8 amino acids or longer than 26 amino acids were excluded due to the setup of our recurrent neural network (RNN). Additionally, we excluded peptides with certain lengths to accommodate the input limitations of other methods being compared.\n\nFor negative examples, we used length-matched random human peptides with specific positive-to-negative ratios during training and validation. During cross-validation, we ensured that no peptides in the validation set were substrings of any training peptides to prevent data leakage and overfitting.\n\nWe systematically compared MARIA's performance with alternative methods using antigen presentation profiling to identify HLA-DR ligands from a human cell line (K562) expressing single HLA-DR alleles. This allowed us to assess the effects of HLA-II allelic variation and the cell-of-origin on performance. We identified approximately 3,600 non-redundant peptide ligands from these alleles. When allowing substring matching, about 31% of ligands were shared, consistent with the known promiscuity of HLA-II binding and presentation.\n\nWe selected DRB1*01:01 for initial testing due to the abundance of training data for existing binding prediction methods. MARIA outperformed these methods by a significant margin when predicting the presentation of DRB1*01:01 ligands and distinguishing them from length-matched decoys. We also tested MARIA's performance on HLA-II alleles not present in the training data, specifically HLA-DRB1*04:04, and found that MARIA again outperformed other methods.\n\nAdditionally, we explored the influence of neural network structure on prediction performance. We trained a shallow neural network and a deep RNN model using the same dataset of HLA-DR ligands. Both models outperformed existing methods, with deep neural networks showing superior performance compared to single-layer architectures. This supports our hypothesis that directly learning from mass spectrometry-identified HLA ligands substantially boosts prediction power.\n\nWe also extended MARIA to the HLA-DQ locus, training it on HLA-DQ2.2 human peptide ligands identified from previously profiled cell lines. After cross-validation, MARIA was tested on an independent set of held-out human DQ ligands and length-matched decoys, demonstrating its versatility and robustness across different HLA loci.",
  "evaluation/measure": "In our evaluation, we reported several key performance metrics to assess the effectiveness of our models. These metrics include sensitivity, also known as recall, which measures the proportion of actual positive ligands that were correctly identified. Specificity, on the other hand, indicates the proportion of actual negative or decoy ligands that were correctly identified. We also calculated the positive predictive value, or precision, which is the proportion of predicted positive ligands that are actually positive. This can also be expressed in terms of prevalence, recall, and specificity. The negative predictive value was calculated to determine the proportion of predicted negative ligands that are actually negative.\n\nAdditionally, we used ROC-AUC scores, which provide a single scalar value that represents the quality of the model's predictions across all classification thresholds. These scores were calculated based on the area under the sensitivity and 1-specificity curves. To generate precision and recall curves, we calculated pairs of recalls and specificities across a range of cut-offs and then determined precision for each recall using an assumption of positive peptide prevalence.\n\nThese metrics are widely used in the literature for evaluating the performance of predictive models, particularly in the context of bioinformatics and immunology. They provide a comprehensive view of the model's ability to correctly identify positive and negative ligands, as well as its overall predictive accuracy. By reporting these metrics, we aim to provide a clear and representative assessment of our model's performance, allowing for comparisons with other methods in the field.",
  "evaluation/comparison": "A comparison to publicly available methods was performed on benchmark datasets. Specifically, MARIA was benchmarked against six commonly used HLA-II prediction algorithms: NetMHCIIpan, SMM Align, NN Align, Sturniolo/TEPITOPE, Comblib, and IEDB Consensus. These methods were evaluated on independent HLA-DR test sets, including K562 myeloid cells and primary melanoma patient samples. MARIA outperformed these methods significantly, achieving an AUC of 0.89 compared to the second-best method, SMM Align, which had an AUC of 0.64.\n\nAdditionally, a comparison to simpler baselines was conducted. Two neural network models were trained using the same dataset as MARIA: a shallow neural network (SNN) with a single hidden layer and a deep recurrent neural network (RNN) model. Both of these models, which only considered peptide sequences, outperformed NetMHCIIpan on external validation data from K562 cells. This demonstrated that directly learning from mass spectrometry-identified HLA ligands substantially boosts prediction power. Furthermore, when trained on the same data, deep neural networks outperformed single-layer architectures, highlighting the advantage of more complex models in this context.",
  "evaluation/confidence": "The evaluation of our method, MARIA, includes several performance metrics with associated confidence intervals. For instance, precision and recall curves were generated using tenfold cross-validation, and the shaded areas around the mean values represent the 95% confidence intervals. This approach ensures that the variability and reliability of our predictions are well-documented.\n\nStatistical significance is a crucial aspect of our evaluation. We employed the DeLong test to determine significant differences between AUC curves, which is a robust method for comparing the areas under two or more correlated receiver operating characteristic curves. This test was used to compare MARIA's performance against other methods, and we found that MARIA outperformed the second-best method by a significant margin, with P values less than 1 \u00d7 10\u22125. Additionally, we used two-tailed paired t-tests and Mann\u2013Whitney U tests for other statistical comparisons, ensuring that our claims of superiority are backed by strong statistical evidence. For example, when comparing AUC scores with different methods on the same set of patient data, we used two-tailed paired t-tests to ascertain statistical significance. These rigorous statistical analyses provide confidence in the superiority of MARIA over other methods and baselines.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, data from two K562 ligandomes used in the evaluation are provided in supplementary tables. The remaining HLA ligand datasets used for evaluation are publicly available from the provided references. For other data, including the raw evaluation files, researchers can request access from the corresponding authors. The custom software code described in this work is available for academic research upon request from the authors or through a specified website. Commercial entities interested in the software should contact the Office of Technology Licensing at Stanford University."
}