{
  "publication/title": "Enhancing Open-World Bacterial Raman Spectra Identification by Feature Regularization for Improved Resilience against Unknown Classes.",
  "publication/authors": "Balytskyi Y, Kalashnyk N, Hubenko I, Balytska A, McNear K",
  "publication/journal": "Chemical & biomedical imaging",
  "publication/year": "2024",
  "publication/pmid": "39474520",
  "publication/pmcid": "PMC11503672",
  "publication/doi": "10.1021/cbmi.4c00007",
  "publication/tags": "- Raman spectroscopy\n- Machine learning\n- Deep neural networks\n- Pathogen classification\n- Open-world learning\n- Medical imaging\n- Public safety\n- Environmental monitoring\n- ResNet architecture\n- Spectral analysis",
  "dataset/provenance": "The dataset used in our study is the bacteria-ID dataset. This dataset contains 30 pathogen classes, with each class having 2000 spectra for training, 100 spectra for fine-tuning, and 100 spectra for testing. This dataset has been utilized in our previous proof-of-concept work, which was published in a prior publication. The dataset is specifically designed to test machine learning algorithms in open-world learning settings, where the goal is to identify known classes of interest while also detecting samples that were not seen during the training phase. The dataset includes a variety of pathogen groups, each with distinct characteristics, which allows for comprehensive testing of the algorithm's ability to handle both known and unknown samples.",
  "dataset/splits": "The dataset used in this study consists of 30 pathogen classes, with each class containing 2000 spectra for training, 100 spectra for fine-tuning, and 100 spectra for testing. The dataset is split into four parts for testing the machine learning algorithm in open-world learning settings. These parts are labeled as p1, p2, p3, and p4.\n\nThe pathogen group p1 is assigned to the known category, which includes classes of interest that the deep neural network (DNN) prioritizes to identify. This group consists of extremely common and contagious pathogens.\n\nThe p4 group, which includes antibiotic-resistant or susceptible pathogens, is classified as the unknown category. This group is particularly harmful to patients and poses a burden on healthcare systems. Misclassification of these pathogens is problematic, especially if errors occur between a susceptible strain and a resistant strain.\n\nThe p2 and p3 groups are often antibiotic-susceptible but typically found in the body. These groups are tested as both the known and unknown categories in different experimental runs.\n\nIt is crucial to carefully assign the pathogen classes to the background category to ensure the DNN's efficiency. The groups p1 and p2 are closer in their characteristics compared to p1 and p3. Both p1 and p2 consist mainly of streptococcal species associated with respiratory and invasive infections. Although p1 and p3 share some common features, such as the presence of Staphylococcus species, p1 and p2 are generally more similar to each other. Due to the low signal-to-noise ratio of the dataset, it is necessary to keep the known and unknown categories sufficiently distinct to avoid false positives or misclassifications.",
  "dataset/redundancy": "The dataset used in our study is the bacteria-ID dataset, which contains 30 pathogen classes. For each class, there are 2000 spectra used for training, 100 spectra for fine-tuning, and 100 spectra for testing. To evaluate our machine learning algorithm in open-world learning settings, the dataset was split into four parts: p1, p2, p3, and p4.\n\nThe p1 group, consisting of extremely common and contagious pathogens, was assigned to the known category. The p4 group, which includes antibiotic-resistant or susceptible pathogens that are particularly harmful, was classified as the unknown category to test the algorithm's ability to identify \"never before seen\" samples while maintaining high accuracy on known ones. The p2 and p3 groups, often antibiotic-susceptible but typically found in the body, were tested in both the known and background categories in different experimental runs.\n\nTo ensure the independence of training and test sets, the spectra were carefully divided such that no spectra from the test set were included in the training or fine-tuning sets. This division helps in evaluating the model's generalizability and robustness.\n\nThe distribution of the dataset differs from some previously published machine learning datasets in that it specifically addresses the challenges of open-world learning, where the model must handle both known and unknown classes. This is particularly relevant in clinical settings, where new and unseen pathogens may need to be identified accurately.\n\nThe careful assignment of pathogen classes to the background category is crucial for the deep neural network's efficiency. For instance, groups p1 and p2, which consist mainly of streptococcal species, are closer in characteristics compared to p1 and p3. This distinction is important to avoid false positives or misclassifications, especially given the low signal-to-noise ratio of the dataset. The model's performance was significantly worse when p1 and p2 were both assigned to the known category, highlighting the importance of correct category assignment.",
  "dataset/availability": "The data used in our study, including the specific data splits, is not publicly released. The dataset consists of Raman spectra for 30 pathogen classes, with 2000 spectra per class for training, 100 spectra per class for fine-tuning, and 100 spectra per class for testing. The data was split into four parts, p1, p2, p3, and p4, for different experimental runs to test our machine learning algorithm in open-world learning settings.\n\nThe data splits were carefully designed to ensure that the deep neural network (DNN) could effectively distinguish between known classes of interest, background classes, and novel classes. The pathogen groups were assigned to these categories based on their characteristics and the importance of accurate classification, particularly for antibiotic-resistant or susceptible pathogens.\n\nThe dataset and the specific splits used in our experiments are not available in a public forum due to the sensitive nature of the data and the need to maintain the integrity of the experimental conditions. However, the model and the code used to implement our approach are publicly available on GitHub, allowing other researchers to replicate and build upon our work. The repository can be accessed at https://github.com/BalytskyiJaroslaw/PathogensRamanOpenSet.git.",
  "optimization/algorithm": "The machine-learning algorithm class used is deep neural networks (DNNs), specifically a custom ResNet architecture. This architecture is not entirely new, as it builds upon previous work and incorporates established techniques like the squeeze-and-excitation (SE) attention mechanism. The reason it was not published in a machine-learning journal is that the focus of this work is on its application to Raman spectroscopy for identifying pathogenic bacteria, rather than the development of the algorithm itself. The modifications made to the ResNet architecture are tailored to address the specific challenges of this application, such as the low signal-to-noise ratio in the bacteria-ID dataset. The algorithm's performance was evaluated on this dataset, demonstrating its effectiveness in both closed-world and open-world scenarios. The use of the Adam optimizer and specific learning rates during training and fine-tuning further optimizes the algorithm for this particular task.",
  "optimization/meta": "In our study, we employed an ensemble approach to enhance the stability and accuracy of our model's performance. This approach involved conducting 20 separate runs of our model and then grouping these runs into four ensembles, each consisting of five models. The ensembles were used to make predictions, demonstrating a marked increase in accuracy and a reduction in variance compared to individual model runs.\n\nThe individual models within the ensemble were all based on a custom ResNet architecture, which was fine-tuned to mitigate the risk of overfitting. This architecture incorporated a single attention block, specifically the squeeze-and-excitation (SE) mechanism, in the last residual block. The use of this attention mechanism significantly improved the model's performance compared to both standard ResNet and full SE-ResNet architectures.\n\nThe training data for each model run was independent, ensuring that the ensembles benefited from diverse learning experiences. This independence is crucial for the robustness of the ensemble method, as it helps to average out the errors and biases that individual models might have.\n\nIn summary, our meta-predictor leverages an ensemble of ResNet models, each fine-tuned with an SE attention mechanism. The training data for each model is independent, contributing to the overall reliability and accuracy of the ensemble predictions.",
  "optimization/encoding": "The data encoding process involved the use of a bacteria-ID dataset, which includes 30 pathogen classes. Each class contains 2000 spectra for training, 100 spectra for fine-tuning, and 100 spectra for testing. The dataset was split into four parts: p1, p2, p3, and p4. The pathogen group p1 was assigned to the known category, as these pathogens are extremely common and contagious. The p4 group, which includes antibiotic-resistant or susceptible pathogens, was classified as the unknown category to highlight the algorithm's ability to identify \"never before seen\" samples while maintaining high accuracy on known ones. The p2 and p3 groups, which are often antibiotic-susceptible but typically found in the body, were tested as both background and unknown categories in different experimental runs.\n\nTo address the low signal-to-noise ratio in the dataset, it was crucial to keep the known and background categories sufficiently distinct to avoid false positives or misclassifications. The data was pre-processed to mitigate spectral noise and effectively handle the inherent low signal-to-noise ratio present in the bacteria-ID dataset. This involved implementing adjustments to a custom ResNet architecture, which included augmenting the last residual block with a squeeze-and-excitation (SE) attention mechanism. This adjustment significantly improved performance compared to both SE-ResNet and standard ResNet architectures.\n\nDuring training and fine-tuning, the Adam optimizer was used with a batch size of 32 and a validation set comprising 20% of the total dataset. The learning rate was set to 10^-5 for training and 10^-6 for fine-tuning. Additionally, during the fine-tuning stage, only the last three layers of the custom deep neural network were fine-tuned to mitigate the risk of overfitting. This approach ensured that the model could handle noisy data while maintaining relatively high accuracy.",
  "optimization/parameters": "In our model, we utilize four distinct groups of pathogens, denoted as p1, p2, p3, and p4. These groups were selected based on their spectral characteristics and the need to maintain a sufficient distinction between them to avoid false positives or misclassifications. The selection of these groups was crucial for the model's performance, as including highly similar groups like p1 and p2 led to significant degradation in accuracy. Instead, assigning p1 and p3 or combining p1 and p2 with p3 proved to be more efficient. The choice of these groups was informed by experiments that demonstrated the importance of selecting distinct categories to enhance the model's ability to differentiate between known and unknown samples. The model's architecture, which includes a custom ResNet with a squeeze-and-excitation attention mechanism, was designed to handle the low signal-to-noise ratio present in the bacteria-ID dataset. The use of an ensemble of models further improved the accuracy and reduced variance, highlighting the effectiveness of the chosen parameters.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "In our study, we employed a custom ResNet architecture, which is known for its efficiency and effectiveness in various classification tasks. The architecture was designed to handle the specific challenges posed by our dataset, particularly the low signal-to-noise ratio.\n\nTo mitigate the risk of overfitting, we fine-tuned only the last three layers of our deep neural network (DNN). This approach allowed us to leverage the pre-trained weights of the earlier layers while adapting the final layers to our specific dataset. Additionally, we integrated a single attention block into our ResNet architecture, which enhanced performance without leading to overfitting. Excessive attention within the DNN can result in overfitting, so this careful integration was crucial.\n\nWe also utilized an ensemble of models to improve stability and performance. By conducting 20 runs of our model and grouping these into 4 ensembles, each consisting of 5 models, we achieved an average accuracy of 87.8% \u00b1 0.1% for the ensemble, compared to 87.5% \u00b1 0.4% for individual runs. This ensemble approach not only increased accuracy but also reduced variance, demonstrating the robustness of our model.\n\nTo address underfitting, we employed the Adam optimizer with a batch size of 32 and a validation set comprising 20% of the total dataset. During training, we set the learning rate to 10^-5, and for fine-tuning, we used a smaller learning rate of 10^-6. These adjustments ensured that our model could effectively learn from the data without underfitting.\n\nFurthermore, we implemented feature regularization techniques, such as the Entropic Open-Set (EOS) and Objectosphere loss functions. These methods helped to improve the separation between known and unknown categories, reducing false positives and inconclusive results. The EOS loss function aimed to maximize Shannon entropy for unknown samples, while the Objectosphere loss function increased the separation of deep features between known and unknown classes.\n\nOverall, our approach balanced the complexity of the model with the need to avoid both overfitting and underfitting, resulting in a robust and accurate classification system for pathogen Raman spectra.",
  "optimization/regularization": "In our study, we implemented several techniques to prevent overfitting and enhance the robustness of our deep neural network (DNN) model. One key strategy involved fine-tuning only the last three layers of our custom DNN during the fine-tuning stage. This approach helped mitigate the risk of overfitting by limiting the adjustments made to the model's parameters.\n\nAdditionally, we integrated a single attention block into our ResNet architecture, which significantly improved performance. However, we found that excessive attention within the DNN could lead to overfitting and negatively affect performance. Therefore, we carefully balanced the use of attention mechanisms to ensure optimal results.\n\nWe also employed feature regularization techniques, specifically the Entropic Open-Set (EOS) and Objectosphere loss functions. The EOS loss function aimed to maximize the Shannon entropy for the \"ignored\" category, ensuring that the DNN's output was uniformly distributed over the known classes. The Objectosphere loss function, on the other hand, increased the separation between known and unknown classes by adjusting the deep features of the DNN. This regularization process helped minimize the false positive rate for unknown samples and improved the overall reliability of the model's inferences.\n\nFurthermore, we used an ensemble of models to enhance stability and accuracy. By conducting 20 runs of our model and grouping these into ensembles of 5 models each, we achieved an average accuracy of 87.8% \u00b1 0.1% for the ensemble, which was higher than the accuracy of individual model runs. This ensemble approach not only increased accuracy but also reduced variance, demonstrating the effectiveness of our regularization methods.\n\nIn summary, our regularization techniques included fine-tuning specific layers, balancing attention mechanisms, and using advanced loss functions for feature regularization. These methods collectively contributed to a more robust and accurate DNN model for pathogen classification.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our experiments are detailed within the publication. Specifically, we utilized the Adam optimizer with a batch size of 32 and a validation set comprising 20% of the total dataset. During training, the learning rate was set to 10^-5, while for fine-tuning, a smaller learning rate of 10^-6 was employed. These details are provided to ensure reproducibility of our results.\n\nRegarding model files and optimization parameters, the specific configurations are not explicitly made available in a downloadable format within the publication. However, the architectural adjustments and training procedures are thoroughly described, allowing researchers to replicate the setup. The publication does not specify the availability of model files or optimization parameters under a particular license, suggesting that while the methodological details are open, the actual files may not be directly accessible.\n\nFor those interested in implementing similar models, the described adjustments to the ResNet architecture, including the integration of the SE attention mechanism in the last residual block, are crucial. The use of the Objectosphere loss function and OOD detectors like ODIN is also highlighted as a key component of our approach. These details, combined with the provided hyper-parameter settings, should enable researchers to build and optimize comparable models.",
  "model/interpretability": "Our model, based on a custom ResNet architecture, can be considered somewhat transparent due to several design choices that enhance interpretability. Firstly, the integration of attention mechanisms allows the model to highlight important features in the input data, making it easier to understand which parts of the Raman spectra are crucial for classification. This is particularly useful in medical and biological applications where interpretability is vital.\n\nAdditionally, the use of a SoftMax layer to interpret the model's outputs as probabilities provides a clear and interpretable way to understand the model's confidence in its predictions. However, it's important to note that while the SoftMax layer helps in interpreting the model's outputs, it does not fully address the issue of out-of-distribution (OOD) samples. To handle OOD samples, we have implemented specialized techniques such as the Entropic Open-Set and Objectosphere loss functions, which help the model to reliably distinguish and reject pathogens that are not included in its catalog.\n\nFurthermore, the model's architecture, which includes skip connections inspired by ResNet, helps in mitigating the vanishing gradient problem, making the training process more stable and interpretable. The skip connections allow gradients to flow more easily through the network, which can be visualized and analyzed to understand the learning process better.\n\nIn summary, while our model is not entirely transparent, the use of attention mechanisms, SoftMax layer for probability interpretation, and specialized techniques for handling OOD samples contribute to its interpretability. These features make it a more reliable tool for real-world applications, particularly in scenarios where understanding the model's decisions is crucial.",
  "model/output": "The model is a classification model. It is designed to classify Raman spectra of pathogens into specific classes. The output of the model represents the probability of the spectrum belonging to a particular class of pathogens. This probability is derived from the logit values, which are obtained by multiplying the deep features from the second-to-last layer of the deep neural network by the weights. The resulting values are then processed through a softmax function to convert them into probabilities. The class with the highest softmax score is considered the predicted class. Additionally, the model includes mechanisms to handle out-of-distribution samples, ensuring that it can reliably distinguish and reject pathogens that are not included in its catalog. This makes the model suitable for both closed-world and open-world scenarios, enhancing its robustness and accuracy in real-world applications.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code of our model is publicly available. It can be accessed via GitHub at the following repository: https://github.com/BalytskyiJaroslaw/PathogensRamanOpenSet.git. This release includes the necessary components to run the algorithm, allowing other researchers to replicate and build upon our work. The code is provided under a license that permits use, modification, and distribution, fostering collaboration and further advancements in the field.",
  "evaluation/method": "The evaluation of our method involved several key steps and experiments to ensure its robustness and accuracy. We conducted 20 independent runs of our model, each utilizing all 30 pathogen classes. The accuracy of a single run was assessed, and these runs were subsequently grouped into 4 ensembles, each consisting of 5 models. This ensemble approach helped to demonstrate the stability and reliability of our model's performance.\n\nThe average accuracy of an individual model run across all 30 classes was 87.5% \u00b1 0.4%. When these models were combined into ensembles, the accuracy improved to 87.8% \u00b1 0.1%. This increase in accuracy and reduction in variance highlights the effectiveness of using model ensembles.\n\nTo further evaluate our method, we implemented feature regularization techniques, specifically the Entropic Open-Set (EOS) and Objectosphere (Obj.) methods. These techniques were designed to improve the separation between known and unknown categories, thereby reducing false positive rates and inconclusive results. The performance of these methods was visualized in figures comparing false positive rates and conclusive outcomes.\n\nAdditionally, we combined our Open-Set approaches with a one-vs-rest classifier to minimize inconclusive results. This combination allowed us to introduce per-class thresholds, which helped in classifying the deep neural network's output more accurately.\n\nOverall, our evaluation method involved rigorous testing through multiple runs, ensemble modeling, and the implementation of advanced feature regularization techniques. These steps ensured that our method was thoroughly evaluated and demonstrated its superiority in both closed- and open-world applications.",
  "evaluation/measure": "In our evaluation, we focus on several key performance metrics to comprehensively assess the effectiveness of our models. Primarily, we report the average rate of conclusive results, which indicates the proportion of instances where the model can confidently classify a sample into one of the known categories. This metric is crucial for understanding the model's reliability in real-world applications.\n\nWe also report the false positive (FP) rate and error rate, which are essential for evaluating the model's accuracy and precision. By minimizing these rates, we ensure that our model not only correctly identifies known pathogens but also avoids misclassifications. Additionally, we present the inconclusive rate, which measures the frequency of instances where the model cannot make a definitive classification. This is particularly important in open-set scenarios where the model may encounter unknown or novel pathogens.\n\nTo provide a robust evaluation, we conduct multiple runs of our experiments and report the standard deviation over these runs. This approach helps in understanding the stability and consistency of our model's performance. Furthermore, we compare our results with naive thresholding methods to highlight the improvements achieved through our advanced techniques, such as the Entropic Open-Set (EOS) and Objectosphere (Obj.) approaches.\n\nOur reported metrics are representative of the current literature in the field of pathogen classification and open-set learning. By focusing on conclusive, FP, error, and inconclusive rates, we align with standard practices in evaluating model performance in similar domains. This set of metrics allows for a thorough assessment of our model's capabilities and its potential for practical applications in chemical and biomedical imaging.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our proposed methods with both publicly available techniques and simpler baselines to ensure a comprehensive assessment of their performance.\n\nWe evaluated our approach on the bacteria-ID dataset, which includes 30 pathogen classes with a substantial number of spectra for training, fine-tuning, and testing. This dataset allowed us to benchmark our methods against state-of-the-art techniques under both closed-world and open-world conditions.\n\nFor the closed-world scenario, our methods demonstrated better or comparable performance to existing state-of-the-art methods. This indicates that our approach is robust and effective even when all possible classes are known during training.\n\nIn the open-world setting, where the model encounters samples from classes not seen during training, our methods showed significant improvements. We compared our Entropic Open-Set (EOS) and Objectosphere approaches with naive thresholding methods. The results, presented in Table 3 and Figures 7 and 8, highlight that EOS and Objectosphere consistently outperformed naive thresholding in terms of the average rate of conclusive results. For instance, when using specific partitions of the dataset, such as p1 and p3, our methods achieved conclusive results of 55.5 \u00b1 1.4% and 55.4 \u00b1 1.8%, respectively, compared to the naive approach.\n\nAdditionally, we tested the performance of different out-of-distribution (OOD) detectors in conjunction with our architecture. The ODIN detector, in particular, showed significantly better performance compared to the Mahalanobis and OpenMax detectors, making it a valuable supplement for OOD detection.\n\nWe also explored the impact of including different pathogen groups in the background category. Our experiments revealed that including certain groups, like p2, led to performance degradation, emphasizing the importance of carefully selecting the background category to avoid false positives and misclassifications.\n\nOverall, our evaluation demonstrates that our integrated deep neural network architecture, combined with feature regularization and appropriate OOD detectors, outperforms currently available techniques for both closed- and open-world applications.",
  "evaluation/confidence": "The evaluation of our methods includes a detailed analysis of performance metrics with associated confidence intervals. For instance, the average rate of conclusive results for different approaches is presented with error bars representing one standard deviation over four ensembles. This provides a clear indication of the variability and reliability of our results.\n\nStatistical significance is a crucial aspect of our evaluation. We compare the performance of various methods, such as naive thresholding, Entropic Open-Set (EOS), and Objectosphere (Obj.) approaches. The results show that EOS and Obj. methods consistently outperform naive thresholding in terms of the average rate of conclusive outcomes. For example, when using specific combinations of classes, EOS and Obj. achieve higher conclusive rates with lower error rates compared to naive methods.\n\nThe use of per-class thresholds further enhances the performance by reducing false positives and error rates, although it may increase the number of inconclusive outcomes. This approach has been successfully applied in open-world text classification, demonstrating its effectiveness.\n\nAdditionally, the integration of a one-versus-the-rest classifier with Open-Set approaches helps in increasing the number of conclusive results while maintaining zero false positive and error rates. This combination ensures that the model's performance is robust and reliable across different classes.\n\nOverall, the evaluation metrics and statistical analyses provide strong evidence that our methods are superior to baseline approaches, offering more accurate and reliable classification results.",
  "evaluation/availability": "Not enough information is available."
}