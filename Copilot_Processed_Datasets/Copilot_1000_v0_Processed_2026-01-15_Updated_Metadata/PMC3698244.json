{
  "publication/title": "Classification algorithms for predicting sleepiness and sleep apnea severity.",
  "publication/authors": "Eiseman NA, Westover MB, Mietus JE, Thomas RJ, Bianchi MT",
  "publication/journal": "Journal of sleep research",
  "publication/year": "2012",
  "publication/pmid": "21752133",
  "publication/pmcid": "PMC3698244",
  "publication/doi": "10.1111/j.1365-2869.2011.00935.x",
  "publication/tags": "- Sleep Medicine\n- Sleep Apnea\n- Epworth Sleepiness Scale\n- Apnea-Hypopnea Index\n- Classification Algorithms\n- Machine Learning\n- Support Vector Machines\n- Naive Bayes Classifier\n- Electrocardiogram\n- Polysomnography\n- Cardiovascular Coupling\n- Sleep Disorders\n- Predictive Modeling\n- Clinical Features\n- Sleep Research",
  "dataset/provenance": "The dataset used in this study is derived from the Sleep Heart Health Study (SHHS), a large-scale, multi-center longitudinal study. The SHHS includes data from 6,441 participants aged 40 years and older, drawn from several ongoing cohort studies. The primary goal of the SHHS is to determine the cardiovascular consequences of sleep apnea.\n\nFor this particular analysis, a subset of subjects from the SHHS was utilized. Specifically, participants from the Strong Heart Health Study were excluded, reducing the dataset to 5,299 subjects. Further exclusions were made for subjects with inadequate ECG signal data, such as those with excessive ECG signal dropout, atrial fibrillation, ventricular bigeminy, demand ventricular pacing, and biventricular pacing. These conditions could interfere with single-lead ECG analysis. After these exclusions, a total of 4,647 subjects with complete data were analyzed.\n\nThe SHHS dataset includes a variety of clinical features such as age, sex, race, body mass index (BMI), systolic and diastolic blood pressure, coronary artery disease status, diabetes status, and Epworth Sleepiness Scale (ESS) scores. Additionally, polysomnography (PSG) features like the apnea-hypopnea index (AHI), respiratory disturbance index (RDI), total sleep time, sleep efficiency, and various sleep stage percentages were considered. Novel ECG-spectrographic features, which characterize sleep architecture and sleep-disordered breathing, were also included in the analysis.\n\nThe SHHS data has been used in previous studies and by the community to explore various aspects of sleep-disordered breathing and its cardiovascular consequences. The dataset's comprehensive nature and the inclusion of both clinical and PSG features make it a valuable resource for research in this field.",
  "dataset/splits": "The dataset was divided into multiple splits for the purpose of training and validating the classification algorithms. Specifically, a training set was defined, consisting of subjects with complete data. This training set included 4647 subjects out of the total 5299 subjects described.\n\nThe training set was further divided into 20 equal subsets using a method known as K-fold cross-validation, with K set to 20. In this process, 19 of the subsets were used to train the algorithm, while the remaining one subset was used to test the classification. This procedure was repeated 20 times, ensuring that each subset was used once as the test set and multiple times as part of the training set. This approach helps in obtaining robust and reliable validation results.\n\nThe distribution of data points in each subset was maintained to be as similar as possible to the entire training set through a method called stratified sampling. This ensures that the distribution of classes in each subset closely matches the distribution in the entire training set, which is crucial for obtaining accurate and unbiased classification results.",
  "dataset/redundancy": "The dataset used in this study was derived from the Sleep Heart Health Study (SHHS) database. The dataset consisted of 5299 subjects, but only those with complete data were included in the training set, totaling 4647 subjects. The training set was then divided into 20 equal subsets using a method known as K-fold cross-validation, with K set to 20. This method ensures that the training and test sets are independent by rotating the subsets used for training and testing. In each iteration of the cross-validation process, 19 subsets were used to train the algorithm, while the remaining subset was used for testing. This process was repeated 20 times, ensuring that each subset was used once as the test set and multiple times as part of the training set.\n\nTo maintain the independence of the training and test sets, stratified sampling was employed. This technique ensures that the distribution of classes in each subset is as similar as possible to that of the entire training set. This approach helps to prevent any single subset from disproportionately influencing the results and ensures that the model's performance is evaluated on a representative sample of the data.\n\nThe distribution of clinical features in the dataset was analyzed, and it was found that the subjects with missing data had similar distributions to those with complete data. This similarity suggests that removing the subset with missing data is unlikely to confound the results. The dataset included a variety of clinical, polysomnographic (PSG), and ECG-spectrographic features, which were categorized into three types: clinical, PSG, and spectrographic. This categorization reflects the different types of data that might be considered when making clinical predictions.\n\nThe dataset's distribution of values for routine clinical features demonstrated non-Gaussian distributions. This was confirmed by statistical tests, including the Kolmogorov-Smirnov (KS) normality test and the D\u2019Agostino and Pearson normality test. The non-Gaussian distribution of the features is an important consideration when applying machine learning algorithms, as many algorithms assume normally distributed data.\n\nIn summary, the dataset was carefully split and validated using K-fold cross-validation with stratified sampling to ensure the independence and representativeness of the training and test sets. The distribution of the dataset compares favorably with previously published machine learning datasets in terms of its comprehensive feature set and rigorous validation methods.",
  "dataset/availability": "The data used in this study is from the Sleep Heart Health Study (SHHS), a large database of home-based polysomnography. The SHHS data is anonymous and publicly available. The data is not released in a public forum in its entirety, but it is accessible through the National Sleep Research Resource (NSRR), which is a collaborative effort to advance sleep research. The NSRR provides a platform for researchers to access sleep-related data, including the SHHS data, under a data use agreement that ensures the data is used responsibly and ethically. The data use agreement includes provisions for data security, confidentiality, and compliance with relevant regulations. The specific data splits used in this study are not publicly released, as they were created for the purposes of this specific analysis. However, the methods used to create the data splits are described in the paper, allowing other researchers to replicate the analysis if they have access to the SHHS data. The data use agreement enforces the responsible use of the data, including restrictions on data sharing and publication. Researchers must comply with these provisions to access and use the data.",
  "optimization/algorithm": "The machine-learning algorithms used in our study include the naive Bayes classifier, the k-nearest neighbors (k-NN) algorithm, and the support vector machine (SVM) classifier. These are well-established algorithms in the field of machine learning and have been extensively used in various classification tasks.\n\nThe naive Bayes classifier is a probabilistic classifier based on Bayes' theorem with the assumption of independence among features. It is known for its simplicity and effectiveness in many real-world classification problems despite its naive assumption of feature independence.\n\nThe k-NN algorithm is a non-parametric, instance-based learning algorithm that classifies an object based on the majority vote of its k nearest neighbors. The value of k determines the locality of the pattern search in the feature space.\n\nThe SVM classifier is a powerful supervised learning algorithm used for classification and regression tasks. It works by finding the hyperplane that best separates the classes in the feature space. We implemented the SVM with a radial basis function kernel, which is flexible in handling non-linear feature relationships.\n\nNone of these algorithms are new; they are widely recognized and used in the machine learning community. The choice of these algorithms was driven by their proven effectiveness in handling classification problems, particularly in the context of medical and biological data. The focus of our publication is on the application of these algorithms to predict sleepiness and sleep apnea, rather than on the development of new machine-learning algorithms. Therefore, it was appropriate to publish these findings in a journal focused on sleep research rather than a machine-learning journal.",
  "optimization/meta": "The model discussed in the publication does not explicitly use data from other machine-learning algorithms as input. However, it does employ various machine-learning methods to predict the apnea-hypopnea index (AHI) and subjective sleepiness. These methods include naive Bayes classifiers, support vector machines (SVM), and k-nearest neighbors (k-NN).\n\nThe SVM classifier, for instance, considers the distribution of class features in multi-dimensional space to designate a 'hyperplane' that allows the best feature-based separation of the classes of interest. This method uses a radial basis function kernel, which is flexible in considering non-linear feature relationships.\n\nThe naive Bayes classifier assumes that any given feature value for a particular class is independent of the other feature values for that class. This assumption simplifies the computational problem but does not significantly affect classification accuracy in many real-world scenarios.\n\nThe k-NN algorithm, on the other hand, classifies data points based on the majority vote of their k-nearest neighbors. However, its performance for AHI classification was found to be worse compared to other methods.\n\nRegarding the independence of training data, the classification algorithms utilized a validation method known as K-fold cross-validation. This method ensures that the training set is divided into K equal subsets, with K-1 subsets used to train the algorithm and the remaining subset used to apply the learned classification. This process is repeated K times, ensuring that each subset is classified once by the algorithm trained on the remaining data. This approach helps to validate the model's performance and ensures that the training data is independent for each fold.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithms involved several steps. Initially, a comprehensive set of features was considered, including routinely available clinical features such as age, sex, race, BMI, blood pressure, coronary artery disease, diabetes, and ESS. Additionally, PSG features like AHI, RDI, total sleep time, sleep efficiency, and various sleep stage percentages were included. Novel ECG-spectrographic features were also utilized, which characterize sleep architecture and sleep-disordered breathing through the dominant frequency of cardiopulmonary coupling.\n\nThe ECG-spectrogram allowed for the categorization of sleep into states of high-frequency cardiopulmonary coupling, low-frequency coupling, elevated low-frequency coupling, and very low-frequency coupling. These categories are associated with different respiratory patterns and sleep stages.\n\nFor the analysis, subjects with an adequate ECG signal were selected to ensure the reliability of the ECG-spectrographic features. The data was then divided into a training set and a testing set using a K-fold cross-validation method, with K set to 20. This method involved dividing the training set into 20 equal subsets, using 19 subsets for training and the remaining one for testing, and repeating this process 20 times to ensure that each subset was used once as the test set.\n\nThe features were normalized and preprocessed to ensure compatibility with the machine-learning algorithms. The naive Bayes classifier, k-nearest neighbor (k-NN), and support vector machine (SVM) were employed to explore the predictive power of the demographic and PSG features for ESS and the presence of OSA. The SVM classifier used a radial basis function kernel, which is flexible in considering non-linear feature relationships.\n\nThe classification algorithms were implemented using the freely available software RapidMiner. The results were aggregated into a confusion matrix, which showed correctly and incorrectly classified subjects, from which sensitivity, specificity, and predictive values were calculated. This approach ensured that the data was encoded and preprocessed in a manner that allowed for robust and reliable classification performance.",
  "optimization/parameters": "In our study, we utilized a support vector machine (SVM) classifier to predict the apnea\u2013hypopnea index (AHI) class. The SVM algorithm's performance was evaluated using various sets of features: clinical features, electrocardiogram (ECG) features, and a combination of these with non-respiratory polysomnogram (PSG) features.\n\nThe SVM model employed three key parameters: gamma, C, and epsilon. These parameters were initially searched manually across log-units and then refined to more specific values. The final parameters used in the data were gamma set to 0.1, C set to 0.2, and epsilon set to 0.2. These values were determined through a systematic process of manual testing and optimization to achieve the best classification performance.\n\nThe selection of these parameters was crucial for balancing the trade-off between under-fitting and over-fitting. The C parameter, acting as a penalty term, controls the margin of the hyperplane. Smaller C values tend to under-fit the data, increasing errors, while larger C values can lead to over-fitting. The gamma parameter influences the smoothness of the hyperplane boundaries, with higher values allowing more irregular boundaries and increasing the risk of over-fitting. The epsilon parameter represents the insensitivity zone, or tolerance for classification errors, with higher values reducing the accuracy requirement during training and decreasing the number of support vectors.\n\nIn summary, the SVM model used three primary parameters: gamma, C, and epsilon. These parameters were carefully selected and optimized through manual testing to ensure the best possible classification performance for predicting AHI class based on the given features.",
  "optimization/features": "In our study, we considered a total of 27 features as potential inputs for our classification algorithms. These features were categorized into three main groups: clinical features, polysomnogram (PSG) features, and spectrographic features derived from electrocardiogram (ECG) data.\n\nThe clinical features included routinely available data such as age, sex, race, body mass index (BMI), systolic and diastolic blood pressure, presence of coronary artery disease, diabetes, and the Epworth Sleepiness Scale (ESS) score. The PSG features encompassed various sleep metrics like the apnea-hypopnea index (AHI), respiratory disturbance index (RDI), total sleep time, sleep efficiency, percentages of different sleep stages, arousal index, and oxygen saturation levels. The spectrographic features, derived from ECG data, characterized sleep architecture and sleep-disordered breathing through measures like high-frequency cardiopulmonary coupling, low-frequency coupling, elevated low-frequency coupling, and very low-frequency coupling.\n\nFeature selection was not explicitly performed in the traditional sense of reducing the number of features based on their importance or relevance. Instead, we focused on different subsets of these features to evaluate their predictive power. For instance, we assessed the performance of our classifiers using clinical features alone, ECG features alone, and a combination of these with non-respiratory PSG features. This approach allowed us to understand the contribution of each feature category to the classification accuracy without formally selecting a subset of features.\n\nThe training set, which consisted of subjects with complete data, was used to develop and validate our classification models. We employed K-fold cross-validation, with K set to 20, to ensure that our models were robust and generalizable. This method involved dividing the training set into 20 equal subsets, using 19 subsets for training and the remaining one for testing, and repeating this process 20 times. This ensured that each subject participated in both training and testing phases, providing a comprehensive evaluation of the model's performance.",
  "optimization/fitting": "The fitting method employed in this study involved several machine learning algorithms, including k-Nearest Neighbors (k-NN), Support Vector Machines (SVM), and Naive Bayes classifiers. The number of parameters in these models varied, but the primary concern was to ensure that the models did not overfit or underfit the data.\n\nFor the k-NN algorithm, the value of k determines the locality of the pattern search in the feature space. By adjusting k, the algorithm can capture local patterns or clusters in the dataset. As k approaches the number of subjects, the classification tends towards the most prevalent class in the entire set, which helps in avoiding overfitting. However, if k is too small, the model may overfit by capturing noise in the data.\n\nThe SVM classifier used a radial basis function kernel, which is flexible in handling non-linear feature relationships. The parameters C, gamma, and epsilon were manually tested across a range of values. The C parameter acts as a penalty term, where small values can lead to underfitting, and large values can lead to overfitting. The gamma parameter controls the smoothness of the hyperplane boundaries, with higher values allowing more irregular boundaries and increasing the risk of overfitting. The epsilon parameter represents the insensitivity zone, where higher values reduce the accuracy requirement during training and decrease the number of support vectors, helping to avoid overfitting.\n\nTo rule out overfitting, a range of parameter values was tested, and the performance was evaluated using cross-validation. The optimal classification performance involved a number of support vectors equal to the number of subjects, suggesting that class discrimination was challenging. The time required to train the SVM classifier varied significantly, indicating the computational complexity involved in finding the optimal parameters.\n\nFor the Naive Bayes classifier, the independence assumption of features simplifies the computational problem but does not often compromise classification accuracy. The dataset was large enough to perform non-naive Bayes classification, but the results reduced to the independence assumption, indicating that combining features did not improve the classification significantly. This suggests that the features were sufficiently independent, and the model did not overfit the data.\n\nIn summary, overfitting was ruled out by carefully selecting parameters and using cross-validation to evaluate performance. Underfitting was addressed by ensuring that the models had enough capacity to capture the underlying patterns in the data. The use of multiple algorithms and thorough parameter tuning helped in achieving a balanced fit, avoiding both overfitting and underfitting.",
  "optimization/regularization": "In our study, we implemented several techniques to prevent overfitting when using the support vector machine (SVM) classifier. One of the key parameters in the SVM is the C parameter, which acts as a penalty term. By carefully selecting the value of C, we could control the trade-off between achieving a low training error and a low testing error. Smaller values of C tend to underfit the data, while larger values can lead to overfitting. We manually tested a range of C values to find an optimal balance.\n\nAnother important parameter is gamma, which influences the smoothness of the boundaries of the hyperplane. Higher gamma values allow for more irregular boundaries, increasing the risk of overfitting. We tested various gamma values to ensure that the model generalized well to unseen data.\n\nAdditionally, we used the epsilon parameter, which represents the insensitivity zone or tolerance for classification errors. Higher epsilon values reduce the accuracy requirement during training, thereby decreasing the number of support vectors and helping to avoid overfitting. We experimented with different epsilon values to find the best setting for our data.\n\nFurthermore, we employed K-fold cross-validation, specifically with K=20, to validate our model. This technique involves dividing the training set into K subsets, training the model on K-1 subsets, and testing it on the remaining subset. This process is repeated K times, ensuring that each subset is used for testing once. This method helps to assess the model's performance more robustly and reduces the risk of overfitting.\n\nBy carefully tuning these parameters and using cross-validation, we aimed to build a model that generalizes well to new data, thereby minimizing the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in detail within the publication. Specifically, for the support vector machine (SVM) classifier, we manually searched and tested a range of values for the parameters gamma, C, and epsilon. The final values used were gamma 0.1, C 0.2, and epsilon 0.2. These parameters were chosen after a systematic search across log-units and more narrow choices to optimize the classifier's performance in predicting the apnea\u2013hypopnea index (AHI) class.\n\nThe optimization schedule involved a thorough manual testing process to ensure that the chosen parameters provided the best classification results. This process included varying the parameters within specified ranges and evaluating the performance metrics such as sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV).\n\nRegarding the availability of model files and optimization parameters, the specific model files used in our analysis are not publicly available. However, the methods and parameters described in the publication can be replicated using standard machine learning software and libraries. The implementation of the classification algorithms was done using freely available software, such as RapidMiner, which is accessible under its respective license.\n\nFor those interested in replicating our work, the detailed descriptions of the features used, the preprocessing steps, and the optimization procedures are provided in the publication. This information should enable researchers to reproduce the results and further build upon our findings.",
  "model/interpretability": "The models used in our study, namely the naive Bayes classifier and the support vector machine (SVM), offer varying degrees of interpretability. The naive Bayes classifier is relatively transparent, as it is based on probabilistic relationships between features and the target variables. This transparency allows for clear examples of how individual features contribute to the classification outcomes. For instance, the sensitivity and specificity values for detecting abnormal Epworth Sleepiness Scale (ESS) scores using polysomnography (PSG) features were 16.7% and 88.8%, respectively, indicating how well the model distinguishes between normal and abnormal ESS values based on the given features.\n\nIn contrast, the SVM classifier is more of a black-box model. While it can effectively separate classes in multi-dimensional space using a hyperplane, the internal workings of how it assigns weights to features are less intuitive. However, the weights assigned to features in the SVM can still provide some insight into the relative importance of each feature. For example, in our analysis, features like elevated low-frequency coupling (e-LFC), body mass index (BMI), and low-frequency coupling (LFC) were found to be the strongest predictors in the algorithm. This information helps in understanding which features are most influential in the classification process, even if the exact mechanisms are not as transparent as in the naive Bayes classifier.\n\nOverall, while the naive Bayes classifier offers more straightforward interpretability, the SVM provides valuable insights through feature weights, making it a useful tool despite its black-box nature.",
  "model/output": "The model employed in this study is a classification model. It was used to predict two specific outcomes: the Epworth Sleepiness Scale (ESS) and the Apnea-Hypopnea Index (AHI). The ESS was dichotomized into normal (0\u201310) or abnormal (11\u201324), while the AHI was categorized into normal (0\u20135) or abnormal (>5). Various classification algorithms, including k-nearest neighbor, naive Bayes, and support vector machine, were utilized to achieve this. The performance of these algorithms was evaluated using metrics such as sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). The results indicated that while the classification of AHI showed modest accuracy, the prediction of ESS was generally poor. This suggests that the features used, whether clinical, spectrographic, or polysomnographic, had limited predictive power for sleepiness as measured by the ESS, but were somewhat more effective for predicting the severity of sleep apnea as indicated by the AHI.",
  "model/duration": "The execution time for the model varied significantly depending on the algorithm used. The naive Bayes classifier was relatively quick, while the support vector machine (SVM) classifier required considerably more time. For the SVM classifier, the training time ranged from approximately 3 minutes to over 2 hours for a single set of parameters. This variation was observed on a Dell Core 2 Duo laptop, indicating that the computational demands of the SVM classifier were much higher compared to the naive Bayes classifier. The time required for training the SVM classifier highlights the complexity and computational intensity of this method, especially when dealing with a large number of features and subjects.",
  "model/availability": "The software used for implementing the classification algorithms in this study is freely available. The algorithms were executed using RapidMiner, a data science platform that provides a wide range of machine learning tools. RapidMiner can be accessed and downloaded from their official website. The software is licensed under terms that allow for its use in both academic and commercial settings, making it accessible for a broad range of users. This ensures that others can replicate and build upon the methods described in the study.",
  "evaluation/method": "The evaluation method employed for the classification algorithms in this study involved a robust approach to ensure the reliability and generalizability of the results. Specifically, K-fold cross-validation was utilized. This method involves dividing the training set, which consisted of 4647 subjects with complete data out of a total of 5299, into K equal subsets. In this case, K was set to 20. The algorithm was then trained on K-1 subsets and tested on the remaining subset. This process was repeated K times, ensuring that each subset was used once as the test set and multiple times as part of the training set. This approach helps in mitigating overfitting and provides a more accurate estimate of the model's performance.\n\nTo enhance the validation results, stratified sampling was used to create the subsets. This technique ensures that the distribution of classes in each subset is similar to that of the entire training set, thereby maintaining the representativeness of the data.\n\nThe performance of the classification algorithms was assessed using a confusion matrix, which details the number of correctly and incorrectly classified subjects. From this matrix, key metrics such as sensitivity, specificity, and predictive values were calculated. These metrics provide a comprehensive evaluation of the algorithm's ability to correctly identify positive and negative cases.\n\nAdditionally, the study considered the use of mutual information to quantify the relationship between various features and the target variables, ESS and AHI. This information-theoretical approach helps in understanding the dependency between features and the target variables, providing insights into the effectiveness of the features used in the classification algorithms.\n\nIn summary, the evaluation method involved K-fold cross-validation with stratified sampling, the use of a confusion matrix for performance assessment, and mutual information analysis to understand feature dependencies. These methods collectively ensure a thorough and reliable evaluation of the classification algorithms.",
  "evaluation/measure": "In our evaluation, we primarily reported performance metrics derived from confusion matrices, which are standard in the literature for classification tasks. These metrics include sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). Additionally, we calculated positive and negative likelihood ratios (LR+ and LR\u2212) to assess the diagnostic value of our classifiers.\n\nSensitivity, also known as the true positive rate, measures the proportion of actual positives that are correctly identified by the model. Specificity, or the true negative rate, measures the proportion of actual negatives that are correctly identified. PPV indicates the probability that subjects with a positive test result truly have the condition, while NPV indicates the probability that subjects with a negative test result do not have the condition.\n\nLR+ and LR\u2212 provide a more nuanced view of the classifier's performance. LR+ is the ratio of the probability of a positive test result in patients with the condition to the probability of a positive test result in patients without the condition. LR\u2212 is the ratio of the probability of a negative test result in patients with the condition to the probability of a negative test result in patients without the condition. These ratios help to understand how the test results change the probability of the condition.\n\nWe found that these metrics were representative of the literature, as they are commonly used in similar studies. The use of LR+ and LR\u2212 is particularly valuable as it allows for a more detailed assessment of the classifier's performance beyond simple accuracy measures. This set of metrics provides a comprehensive view of the classifier's ability to correctly identify both positive and negative cases, as well as the likelihood ratios that indicate the strength of the test results in changing the probability of the condition.",
  "evaluation/comparison": "In our study, we employed several classification algorithms to predict relevant endpoints from clinical, PSG, and cardiorespiratory/autonomic features. To ensure the robustness of our findings, we compared the performance of different algorithms, including the naive Bayes classifier, the k-nearest neighbors (k-NN) algorithm, and the support vector machine (SVM) classifier.\n\nThe naive Bayes classifier was chosen for its simplicity and efficiency, assuming independence among features. This assumption, while often unrealistic, did not significantly compromise classification accuracy in our dataset. We also implemented a non-naive Bayes classification to verify that the independence assumption did not disadvantage our results. The non-naive method reduced to the naive case for the features we considered, indicating that combining features did not improve classification performance.\n\nWe also tested the k-NN algorithm with various k values (1, 3, 5, and 10). However, the classification performance using k-NN was worse than that of the naive Bayes classifier for both ESS and AHI predictions. This suggests that the features used may not be sufficiently predictive of sleepiness class or that the ESS itself is a poor marker of sleepiness.\n\nAdditionally, we utilized the SVM classifier, which considers the distribution of class features in multi-dimensional space to designate a hyperplane for optimal feature-based separation. The SVM performed similarly to the naive Bayes classifier for AHI classification but poorly for ESS classification. The SVM's flexibility in handling non-linear feature relationships did not significantly enhance prediction accuracy for ESS, indicating that the features may not capture the underlying patterns of sleepiness effectively.\n\nIn summary, our comparison of different classification algorithms revealed that the naive Bayes classifier and SVM performed similarly for AHI prediction, while both struggled with ESS prediction. The k-NN algorithm underperformed in both cases. These comparisons highlight the challenges in predicting subjective measures like the ESS and the relative effectiveness of different classification methods in handling various types of data.",
  "evaluation/confidence": "The evaluation of our methods involved a rigorous statistical approach to ensure the reliability and significance of our results. We employed K-fold cross-validation, specifically with K=20, to validate our classification algorithms. This method involves dividing the training set into 20 equal subsets, using 19 subsets for training and the remaining one for testing, and repeating this process 20 times. This ensures that each subset is used once as the test set, providing a comprehensive evaluation of the model's performance.\n\nThe performance metrics, such as sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV), were calculated from the confusion matrices generated by this cross-validation process. These metrics do not explicitly include confidence intervals in the provided results, but the use of cross-validation inherently provides a measure of variability and robustness in the performance estimates.\n\nStatistical significance was assessed through the likelihood ratios (LR+ and LR-), which indicate how much the test result changes the probability of the condition. For example, in the classification of the Apnea-Hypopnea Index (AHI), the LR+ values were around 2.19-2.6, and LR- values were around 0.55-0.74, suggesting a modest but significant adjustment in disease probability. These values are derived from the sensitivity and specificity, which were found to be statistically significant in distinguishing between normal and abnormal AHI.\n\nThe results demonstrated that while the naive Bayes classifier and support vector machine (SVM) classifier showed improved prediction for AHI compared to the Epworth Sleepiness Scale (ESS), the likelihood ratios were still close to 1, indicating only modest adjustments in disease probability. This suggests that while the methods are statistically significant, they may not provide a dramatic improvement over prior probabilities alone.\n\nIn summary, the performance metrics were evaluated using a robust cross-validation technique, and the results were found to be statistically significant. However, the practical improvement in disease probability adjustment was modest, indicating that while the methods are reliable, there is room for further enhancement.",
  "evaluation/availability": "Not enough information is available."
}