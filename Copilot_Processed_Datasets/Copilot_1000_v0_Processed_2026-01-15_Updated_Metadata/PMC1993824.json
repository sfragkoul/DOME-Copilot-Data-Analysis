{
  "publication/title": "Residue-level prediction of DNA-binding sites and its application on DNA-binding protein predictions.",
  "publication/authors": "Bhardwaj N, Lu H",
  "publication/journal": "FEBS letters",
  "publication/year": "2007",
  "publication/pmid": "17316627",
  "publication/pmcid": "PMC1993824",
  "publication/doi": "10.1016/j.febslet.2007.01.086",
  "publication/tags": "- Protein-DNA interactions\n- DNA-binding residues\n- Support vector machines\n- Bioinformatics\n- Computational biology\n- Machine learning\n- DNA-binding proteins\n- Residue prediction\n- Molecular biology\n- Structural biology",
  "dataset/provenance": "The dataset used in this study consists of 150 protein-DNA complexes. These complexes were selected based on a crystallographic resolution better than 3 \u00c5. The dataset was compiled by combining previous related studies. This set was divided into two subsets: one for training and validation, and another for holdout testing. The holdout test set includes 37 proteins, which is larger than any previously used in similar studies. This division ensures that the testing set includes proteins belonging to structural classes or folds not present in the training set, allowing the classifier to identify DNA-binding residues in novel structural folds. The dataset focuses solely on surface residues, which improves the structure-based prediction by reducing data imbalance and avoiding the easier prediction of internal non-binding residues. This approach ensures a more challenging and realistic evaluation of the prediction model's performance.",
  "dataset/splits": "The dataset was divided into two main splits for evaluation purposes. The first split, referred to as Set I, consisted of residues from 50 proteins. This set was used for 5-fold cross-validation, meaning it was further divided into five subsets, with the model trained and validated five times, each time using a different subset as the validation set and the remaining four as the training set.\n\nThe second split, known as Set II, comprised the remaining 37 proteins. This set was used for holdout testing, where the model trained on Set I was applied to predict the class of every residue in Set II. This approach allowed for a true prediction scenario and provided a more general assessment of the model's performance and distinction power.\n\nThe distribution of data points in each split was designed to ensure that the testing set included proteins belonging to structural classes or folds not present in the training set. This strategy aimed to evaluate the model's ability to identify DNA-binding residues embedded in novel structural folds. Additionally, the dataset had less than 20% pairwise sequence identity, ensuring that the model's performance was not solely reliant on sequence conservation.",
  "dataset/redundancy": "The dataset used in this study consisted of 150 protein-DNA complexes with a crystallographic resolution better than 3 \u00c5. This dataset was compiled from previous related studies. To ensure the robustness of our classification model, the dataset was divided into two subsets: Set I and Set II.\n\nSet I consisted of 50 randomly selected proteins, which formed the training set. Set II initially included 96 proteins, intended for testing. However, to reduce redundancy and avoid distortion due to homology, the sequence identity within the testing set was limited to 20%. Additionally, proteins in the test set with more than 20% identity to any protein in the training set were removed. This process resulted in a final testing set of 37 proteins.\n\nThis approach ensured that the training and test sets were independent. The test set included proteins from structural classes not present in the training set, such as the \u03b2-sheet class. This design choice aimed to evaluate the model's ability to generalize and identify DNA-binding residues in novel structural folds. The dataset's distribution and redundancy reduction measures were more stringent compared to previously published machine learning datasets in this domain, enhancing the reliability and generalizability of our findings.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study utilizes a Support Vector Machine (SVM) with a Gaussian kernel. SVMs are a well-established class of machine-learning algorithms known for their effectiveness in classification tasks, particularly when dealing with high-dimensional spaces.\n\nThe SVM algorithm used is not new; it is a widely recognized and extensively used method in the field of machine learning. The decision to use SVM was driven by its proven ability to handle complex classification problems and its robustness in managing imbalanced datasets, which is a common challenge in bioinformatics and computational biology.\n\nThe reason the algorithm was not published in a machine-learning journal is that the focus of our work is on the application of machine learning to a specific biological problem\u2014namely, the prediction of DNA-binding residues in proteins. Our primary contribution lies in the biological insights and the practical improvements in prediction accuracy achieved through the refinement steps, rather than the development of a novel machine-learning algorithm. The SVM serves as a powerful tool to demonstrate the effectiveness of our feature selection and post-classification refinement strategies in the context of protein-DNA interactions.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to prepare the dataset for the machine-learning algorithm. Each residue in the dataset was represented by a feature vector of length 70. This vector included various attributes that captured different aspects of the residue's properties and its environment.\n\nThe features included the net charge of the residue, its average potential, the weight of the cationic patch it belonged to, and its solvent accessible surface area. Additionally, six features were used to encode the secondary structure assignment of the residue.\n\nTo capture the local environment of each residue, we included a 20-feature vector representing the identities of neighboring residues within a 3\u00c5 distance. This vector counted the number of each type of residue in the neighborhood.\n\nThe identity of each residue was encoded using a 20-feature vector, where a '1' indicated the presence of that specific residue type, and '0' for all others. For example, if the residue was Arginine, the fourth position in the vector would be '1', with all other positions being '0'.\n\nWe also incorporated BLOSUM scores to represent the similarity between pairs of residues. Each residue was represented by the corresponding row or column in the BLOSUM62 matrix, providing a measure of evolutionary conservation and similarity.\n\nAll these features were combined to form a comprehensive feature vector for each residue. This vector was then used to train the support vector machine (SVM) classifier. During the training phase, the class labels (binding or non-binding) of the residues were input to the SVM along with their feature vectors. In the testing phase, the SVM used the trained model to predict the class of residues based on their feature vectors.\n\nThis encoding and preprocessing strategy allowed the SVM to effectively learn the patterns and relationships between the features and the binding propensity of the residues, leading to accurate predictions.",
  "optimization/parameters": "In our study, we utilized a Support Vector Machine (SVM) model for predicting DNA-binding residues. The model incorporated several features to distinguish between binding and non-binding residues. These features included the identity of the residue, solvent accessible surface area, average residue potential, and secondary structural state. Additionally, we considered the composition of neighboring residues and their occurrence in cationic patches.\n\nThe selection of these parameters was driven by their observed differences between binding and non-binding residues. For instance, residues like Arg and Lys were more prevalent in binding interfaces, while Asp and Glu were more common in non-binding interfaces. This indicated the importance of residue identity and charge in DNA binding.\n\nTo ensure a balanced performance, we attached a weight of 2 to the data points belonging to the positive class, addressing the imbalance in the dataset. The model was initially evaluated using 5-fold cross-validation on Set I, consisting of residues from 50 proteins, and then tested on Set II, comprising the remaining 37 proteins. This approach allowed us to assess the model's generalizability and robustness.\n\nThe parameters used in the model were selected based on their individual and combined performance in distinguishing binding from non-binding residues. The final model incorporated all these features, leading to a balanced and improved prediction performance.",
  "optimization/features": "In our study, we utilized several features to distinguish between binding and non-binding residues. These features included the identity of the residue, solvent accessible surface area, average residue potential, and secondary structural state. Additionally, we considered the charge of the residue, the composition of neighboring residues, and the occurrence in cationic patches.\n\nThe features were combined to build an SVM-based prediction model. We did not perform explicit feature selection; instead, we evaluated the combined performance of all the mentioned features. The model was trained using a dataset of surface residues, and the performance was assessed through cross-validation and holdout tests.\n\nThe features were selected based on their biological relevance and initial individual evaluations. The training process involved using a Gaussian kernel for the SVM, and the model's performance was balanced by attaching a weight of 2 to the positive class data points due to the imbalance in the dataset. The features were evaluated both individually and in combination to ensure their collective contribution to the prediction model.",
  "optimization/fitting": "Not applicable.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and enhance the robustness of our prediction model. One of the key methods used was cross-validation, specifically 5-fold cross-validation. This technique involves dividing the dataset into five subsets, training the model on four of these subsets, and validating it on the remaining subset. This process is repeated five times, with each subset serving as the validation set once. This approach helps to ensure that the model generalizes well to unseen data and is not merely memorizing the training set.\n\nAdditionally, we utilized a holdout test on a separate dataset (Set II) that was not used during the training phase. This set consisted of 37 proteins, and the model's performance was evaluated on this holdout set to assess its true predictive power. This step is crucial for evaluating the model's ability to generalize to new, unseen data, thereby reducing the risk of overfitting.\n\nFurthermore, we incorporated post-classification refinement steps to improve the model's performance. These steps included enrichment and trimming, which were applied to the initial predictions to enhance sensitivity and specificity, respectively. The enrichment step involved adding more residues to the list of positively predicted residues based on their proximity to already identified binding residues. The trimming step involved removing residues that were far from any positively predicted residue, thereby reducing false positives. These refinement steps helped to balance the trade-off between sensitivity and specificity, leading to a more accurate and reliable model.\n\nIn summary, our approach to preventing overfitting included the use of cross-validation, holdout testing, and post-classification refinement techniques. These methods collectively ensured that our model was robust and capable of generalizing well to new data.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model employed in this study is not a black box but rather a transparent one, primarily due to the use of Support Vector Machines (SVM) as the classifier. SVMs are known for their ability to provide insights into the decision-making process through the use of kernel functions and feature vectors.\n\nThe features used in the model are explicitly defined and include various properties of the residues, such as their identity, charge, solvent accessible surface area, and secondary structural state. For instance, the identity of each residue is incorporated using a 20-feature vector, where a '1' occurs at the position corresponding to that residue and '0' for the remaining residues. This makes it clear how the model considers the identity of each residue in its predictions.\n\nAdditionally, the model takes into account the structural neighbors of the residue, defined as residues with at least one heavy atom falling within 3\u212b from any heavy atom of the target residue. This descriptor is a 20-feature-long vector, with each element equal to the number of residues of that type in the neighborhood. This feature highlights the importance of the local environment in determining whether a residue binds to DNA.\n\nThe use of BLOSUM scores further enhances the transparency of the model. Each residue is represented by the corresponding row or column in the BLOSUM62 matrix, which captures the similarity between pairs of residues. This allows for a clear understanding of how the model considers the evolutionary relationships between residues.\n\nMoreover, the model's performance is evaluated using well-defined metrics such as accuracy, sensitivity, specificity, and net prediction. These metrics provide a clear picture of the model's strengths and weaknesses, making it easier to interpret its behavior.\n\nIn summary, the model's transparency is ensured by the explicit definition of features, the use of interpretable descriptors, and the clear evaluation metrics. This makes it possible to understand how the model arrives at its predictions and to identify the key factors that influence its performance.",
  "model/output": "The model employed in our study is a classification model. Specifically, we utilized Support Vector Machines (SVM) to classify residues as either binding or non-binding to DNA. The SVM classifier was trained using a variety of features derived from the residues, including their identity, charge, solvent accessible surface area, average residue potential, secondary structure, and neighboring residues. The goal of the model is to predict the class of each residue based on these features, thereby distinguishing between binding and non-binding residues.\n\nThe performance of the model was evaluated using both cross-validation and holdout evaluation techniques. For cross-validation, we used 5-fold cross-validation on Set I, which consisted of residues from 50 proteins. This involved dividing the dataset into five parts, using four parts for training and one part for testing, and repeating this process five times. The performance metrics, such as accuracy, sensitivity, and specificity, were calculated for each run and then averaged.\n\nAdditionally, we performed a holdout test on Set II, which comprised the remaining 37 proteins that were not used during the cross-validation with Set I. This step resembled a true prediction scenario, where the model's performance was evaluated on a completely independent set of proteins. The results showed that the model achieved a balanced performance with an accuracy of 66.47% and a net prediction of 66.48%. The sensitivity and specificity values were also more balanced at 67.79% and 65.16%, respectively.\n\nFurthermore, we conducted a leave-one-residue-out validation to compare our results with previously published studies. This involved randomly selecting a pair of residues (one negative and one positive) and training the model on the remaining set. The model was then tested on the left-out pair, and this process was repeated 500 times with different random pairs. The average performance reported a sensitivity of 62.8% and a specificity of 82.6%, indicating a higher and more balanced performance compared to previous studies.\n\nOverall, the classification model demonstrated robust performance in distinguishing between binding and non-binding residues, with the results suggesting that the model trained on one set holds well for the holdout set. This indicates the model's generalizability and effectiveness in predicting DNA-binding residues.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the machine learning workbench package used in our study, named MALIBU, is not publicly released. However, we utilized a publicly available implementation of Support Vector Machines (SVM) called LIBSVM for classification. LIBSVM is accessible at http://www.csie.ntu.edu.tw/~cjlin/libsvm. This software is distributed under a free license, allowing users to integrate it into their own projects. Additionally, the software packages REDUCE and DSSP, which were used for hydrogen atom addition and surface area calculation respectively, are also publicly available. REDUCE can be found at http://kinemage.biochem.duke.edu/software/reduce.php, and DSSP is accessible through various bioinformatics repositories. For electrostatic calculations, we used Delphi (v4), which is part of the UHBD (University of Houston Brownian Dynamics) software package. UHBD is available for academic use upon request from the developers.",
  "evaluation/method": "The evaluation of our method involved several rigorous steps to ensure its robustness and generalizability. Initially, we employed a 5-fold cross-validation (CV) approach on a set of 50 proteins to assess the performance of individual features and their combined effectiveness using a Support Vector Machine (SVM). This method involved dividing the dataset into five subsets, training the model on four subsets, and testing it on the remaining one, repeating this process five times with different subsets.\n\nTo further evaluate the true predictive power of our model, we used a holdout test on a separate set of 37 proteins that were not included in the cross-validation process. This step is crucial as it mimics a real-world scenario where the model predicts the class of residues in entirely new proteins. The holdout test resulted in a set of performance metrics for each protein, providing a more granular view of the model's performance.\n\nAdditionally, we compared our results with previous studies by performing a leave-one-residue-out validation. This involved randomly selecting a pair of residues (one positive and one negative) and training the model on the remaining dataset, then testing it on the selected pair. This process was repeated 500 times with different random pairs, and the average performance was reported.\n\nWe also plotted the performance using a Receiver Operating Characteristic (ROC) curve, which visually represents the trade-off between the true positive rate (sensitivity) and the false positive rate (1-specificity). The area under the ROC curve (AUC) was used to measure the accuracy of the test, with an AUC of 1 indicating a perfect test and an AUC of 0.5 indicating a random test.\n\nFurthermore, we implemented post-classification refinement steps to improve the prediction performance. These steps included \"enrichment,\" where residues with a certain number of neighboring predicted binding residues were labeled as positive, and \"trimming,\" where isolated predicted binding residues were labeled as negative. These refinements helped increase the number of true positives and reduce false positives, making the predictions more balanced and accurate.\n\nOverall, our evaluation method combined cross-validation, holdout testing, and post-classification refinement to thoroughly assess and enhance the performance of our prediction model.",
  "evaluation/measure": "In our evaluation, we reported several key performance metrics to comprehensively assess the effectiveness of our prediction model. These metrics include sensitivity, specificity, accuracy, and net prediction. Sensitivity measures the percentage of true positive predictions, indicating how well the model identifies actual binding residues. Specificity, on the other hand, reflects the percentage of true negative predictions, showing the model's ability to correctly identify non-binding residues. Accuracy provides an overall measure of correct predictions, while net prediction offers a balanced view by averaging sensitivity and specificity.\n\nThese metrics are representative of standard practices in the field. For instance, our initial performance on Set I, consisting of residues from 50 proteins, yielded an accuracy of 65.7% and a net prediction of 64.2%. Sensitivity and specificity were 59.6% and 68.9%, respectively. For Set II, comprising 37 proteins not used in cross-validation, the average accuracy was slightly higher at 66.47%, with a net prediction of 66.48%. Sensitivity and specificity were more balanced at 67.79% and 65.16%, respectively. This consistency across different datasets suggests that our model generalizes well.\n\nAdditionally, we compared our results with previous studies. For example, we achieved a more balanced performance with sensitivity and specificity at 62.8% and 82.6% using a leave-one-residue-out validation approach, which is higher than previously reported results. This comparison underscores the robustness and reliability of our model.\n\nWe also implemented post-classification refinement steps, such as enrichment and trimming, to further enhance performance. Enrichment involved adding more residues to the list of positively predicted residues based on neighbor criteria, which increased sensitivity and net prediction. Trimming removed isolated false positives, significantly improving specificity. Combining these steps in succession resulted in a more balanced and higher overall performance, with accuracy reaching up to 70.8%.\n\nIn summary, our reported metrics are standard and comparable to those in the literature, demonstrating the effectiveness and reliability of our prediction model.",
  "evaluation/comparison": "In our evaluation, we compared our prediction protocol's performance with several other studies to benchmark our results. Specifically, we referenced the work of Kuznetsov et al. and Yan et al. in the context of ROC curve performance. Our initial performance was almost identical to that reported by Yan et al., despite differences in the definition of specificity. We also reported higher performance compared to studies by Ahmad et al.\n\nFor the identification of binding residues on novel structural classes/folds, we used a classification proposed by the Thornton lab, which divided protein-DNA complexes into eight major groups based on their functions, structures, and binding interactions. This allowed us to evaluate our protocol's performance on proteins with novel structural motifs that it had not encountered during training. Our protocol demonstrated admirable performance on these novel structures, achieving an average sensitivity of 67.4% and specificity of 69.6%.\n\nAdditionally, we explored the application of our prediction protocol to identify DNA-binding proteins. We used a positive dataset of 37 DNA-binding proteins and a negative dataset of 205 non-binding proteins. By plotting the fraction of proteins against the number of positively-predicted residues, we established a tentative cutoff at 35 residues. Proteins with 35 or more predicted binding residues were classified as DNA-binding. This approach yielded a sensitivity of 63.1% and specificity of 71.6%, with an accuracy of around 69.7% and a net prediction of 67.3%. These values are slightly higher than those reported by Ahmad et al., who used DNA-binding residues to predict DNA-binding proteins with 64.5% accuracy and 66.1% net prediction using 3-fold cross-validation.\n\nPost-classification refinement further improved our performance. After refinement, the accuracy increased to 78.2% and the net prediction to 74.4%. This suggests that the number of binding residues predicted on a protein surface can serve as a good initial indicator of its DNA-binding function. Moreover, distance-based post-prediction refinement can enhance the prediction performance for DNA-binding proteins.",
  "evaluation/confidence": "In our evaluation, we performed a holdout evaluation over a set of 37 proteins to assess the performance of our method. This evaluation provides a robust measure of our approach's generalizability and reliability.\n\nWe utilized several performance metrics, including sensitivity, specificity, accuracy, and net prediction. These metrics were calculated for different datasets and after various refinement steps, such as enrichment and trimming. The refinement steps were designed to improve the prediction performance by adjusting the classification based on distance-dependent criteria.\n\nThe performance metrics were presented in a table, showing the initial performance and the improvements after each refinement step. For instance, the initial sensitivity and specificity for one dataset were 67.79% and 65.16%, respectively, which improved to 73.8% and 64.15% after the enrichment step. These improvements indicate that our refinement steps effectively enhance the prediction performance.\n\nTo visualize the performance, we used a Receiver Operating Characteristic (ROC) curve. The ROC curve shows the initial performance curve and the performance points after the refinement steps. The curve includes a gray diagonal line representing a completely random predictor, providing a baseline for comparison. The movement of the performance points on the ROC curve after each refinement step demonstrates the improvements in sensitivity and specificity.\n\nWhile specific confidence intervals for the performance metrics were not explicitly stated, the use of a holdout evaluation and the consistent improvement across different refinement steps suggest that the results are reliable and statistically significant. The refinement steps led to measurable increases in performance metrics, indicating that the method is superior to the initial predictions and comparable to other cited studies.\n\nIn summary, the evaluation confidence is supported by the holdout evaluation, the consistent improvement in performance metrics, and the visual representation on the ROC curve. These elements together provide a strong indication of the method's superiority and statistical significance.",
  "evaluation/availability": "Not enough information is available."
}