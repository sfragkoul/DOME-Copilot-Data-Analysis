{
  "publication/title": "Predicting COVID-19 Cases From Atmospheric Parameters Using Machine Learning Approach.",
  "publication/authors": "Ogunjo ST, Fuwape IA, Rabiu AB",
  "publication/journal": "GeoHealth",
  "publication/year": "2022",
  "publication/pmid": "35415381",
  "publication/pmcid": "PMC8983058",
  "publication/doi": "10.1029/2021gh000509",
  "publication/tags": "- COVID-19\n- Machine Learning\n- Atmospheric Parameters\n- Prediction Models\n- Pandemic Management\n- Tropical Regions\n- Data-Driven Approaches\n- Environmental Factors\n- Public Health\n- Time-Series Forecasting",
  "dataset/provenance": "The dataset used in this study was sourced from multiple origins. Daily COVID-19 cases for each of the six locations within Nigeria were obtained from the National Centre for Disease Control. Atmospheric data, including temperature and relative humidity, as well as particulate matter data (PM2.5, PM1.0, PM10.0), were collected from an ongoing campaign by the Centre for Atmospheric Research, National Space Research and Development Agency. These data were retrieved using Purple Air sensors, which were provided by the Alliance for Education, Science, Engineering and Design in Africa (AESEDA), Penn State University, USA. The specific data for particulate matter and atmospheric variables were accessed from the Purple Air network.\n\nThe research was conducted during the harmattan season in Nigeria, spanning from November 1, 2020, to March 31, 2021. This period provided a comprehensive set of data points, allowing for a detailed analysis of the relationship between atmospheric conditions and COVID-19 cases.\n\nThe dataset has been utilized in previous studies and by the community to explore various aspects of COVID-19 transmission and prediction. For instance, machine learning algorithms such as Decision Tree, Random Forest, Support Vector Machine, and k-Nearest Neighbor have been employed to forecast COVID-19 cases using atmospheric parameters as predictors. These algorithms have shown varying levels of performance, with Support Vector Machine and k-Nearest Neighbor generally outperforming others in predicting COVID-19 cases based on previous days' data. The use of these datasets has contributed to a better understanding of how atmospheric conditions can influence the spread of the virus, aiding in the development of more effective mitigation strategies.",
  "dataset/splits": "In our study, we utilized a straightforward data splitting approach for our machine learning models. The dataset was divided into two primary splits: a training set and a testing set. Specifically, 80% of the data was allocated to the training set, while the remaining 20% was reserved for the testing set. This split was consistently applied across all four machine learning algorithms considered in our research: Decision Tree, Random Forest, Support Vector Machine, and k-Nearest Neighbor.\n\nThe training set was used to develop and fine-tune the models, allowing them to learn patterns and relationships within the data. The testing set, on the other hand, was employed to evaluate the performance of the trained models, ensuring that they could generalize well to unseen data. This approach helped us to assess the robustness and reliability of our models in predicting COVID-19 cases using various atmospheric parameters and air pollutants as predictors.",
  "dataset/redundancy": "In our study, we employed a standard approach to splitting our datasets, ensuring that the training and test sets were independent. Specifically, we allocated 80% of the data for training purposes and reserved the remaining 20% for testing. This split was designed to provide a robust evaluation of our machine learning models' performance, ensuring that the test set remained unseen during the training phase.\n\nTo enforce the independence of the training and test sets, we used a random sampling technique. This method helps to mitigate the risk of data leakage, where information from the test set might inadvertently influence the training process. By randomly assigning data points to either the training or test set, we ensured that the models were evaluated on data they had not encountered during training.\n\nComparing our dataset distribution to previously published machine learning datasets, we aimed to adhere to common practices in the field. The 80-20 split is widely accepted and has been used in various studies, including those focused on COVID-19 predictions. This approach allows for a fair assessment of model performance and ensures that the results are comparable to other research in the domain.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are Decision Tree, Random Forest, Support Vector Machine, and k-Nearest Neighbor. These are well-established algorithms in the field of machine learning and have been widely applied in various domains, including healthcare and epidemiology.\n\nThe algorithms employed are not new; they have been extensively studied and used in numerous research works. Decision Tree and Random Forest are both tree-based methods, with Random Forest being an ensemble of Decision Trees. Support Vector Machine is a powerful algorithm for classification and regression tasks, while k-Nearest Neighbor is a simple, instance-based learning algorithm.\n\nThe choice of these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to identify patterns and make reliable predictions. These algorithms do not require specific distributions of the underlying data, making them versatile for different types of datasets.\n\nThe study focuses on applying these machine-learning algorithms to predict COVID-19 cases using atmospheric parameters and past infection data. The algorithms were chosen for their robustness and ability to handle the dynamic nature of COVID-19 data. The results demonstrate the effectiveness of these algorithms in predicting COVID-19 cases, which is crucial for proactive monitoring and control of the pandemic.\n\nThe algorithms were not published in a machine-learning journal because the primary focus of this research is on their application in predicting COVID-19 cases rather than the development of new machine-learning techniques. The study contributes to the field of public health and epidemiology by showcasing the practical use of established machine-learning algorithms in combating the COVID-19 pandemic.",
  "optimization/meta": "The study does not employ a meta-predictor approach. Instead, it utilizes four distinct machine learning algorithms individually: Decision Tree, Random Forest, Support Vector Machine, and k-Nearest Neighbor. Each of these algorithms was trained and tested separately using the same dataset, with 80% of the data reserved for training and 20% for testing. The performance of these algorithms was evaluated based on the root mean square error (RMSE) to determine their effectiveness in predicting COVID-19 cases.\n\nThe Decision Tree algorithm uses a series of decisions based on given conditions to arrive at a conclusion, with the gini index used for splitting. Random Forest, being an extension of Decision Tree, makes decisions from randomly selected subsets of the training data and aggregates these decisions for the final output. Support Vector Machine classifies data points by drawing a boundary line, with support vectors being points close to this boundary. The k-Nearest Neighbor algorithm estimates the distance to the nearest neighbor in the sample space and determines the status of a new sample based on the number of neighbors in its vicinity.\n\nEach of these algorithms was applied independently to predict COVID-19 cases using various predictors such as particulate matter (PM2.5), temperature, and humidity. The performance of these models was compared across different locations, and it was found that k-Nearest Neighbor and Support Vector Machine generally had superior performance in predicting future COVID-19 cases based on past histories. Temperature was identified as the best predictor for the number of COVID-19 cases, followed by relative humidity. Decision Tree models, however, performed poorly in the prediction of COVID-19 cases when considering particulate matter and atmospheric parameters as predictors.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for ensuring the effectiveness of the machine learning algorithms employed. We utilized atmospheric parameters such as particulate matter (PM1.0, PM2.5, PM10.0) and meteorological variables like temperature and humidity, which were obtained from reliable sources. These parameters were chosen due to their potential influence on the spread of COVID-19.\n\nThe data was split into training and testing sets, with 80% of the data used for training and 20% reserved for testing. This split ensured that the models were trained on a substantial amount of data while still having a sufficient testing set to evaluate their performance accurately.\n\nFor the Decision Tree (DT) algorithm, the data was encoded using a series of decision-based conditions. The gini index was employed for splitting the data, which helped in creating a tree structure that effectively classified the data points. The internal nodes represented the available choices at each point in the tree, and the root node was the initial condition that led to the subdivision of the tree into subsets.\n\nIn the k Nearest Neighbor (KNN) algorithm, the data was encoded by creating a space for the training dataset. When a new sample was introduced, the distance to the nearest neighbors in this space was estimated using the kd tree approach. The status of the sample was then determined by the number of neighbors in its vicinity, with a total of 48 neighbors considered in this study.\n\nFor the Support Vector Machine (SVM) algorithm, the data was encoded using a radial basis function as the kernel, along with a polynomial function of degree 3. This approach allowed for the classification of data points by drawing a boundary line, with points close to this boundary acting as support vectors. The classification was performed through the linear combination of these boundaries.\n\nThe Random Forest (RF) algorithm, being an ensemble of decision trees, involved encoding the data by making decisions from randomly selected subsets of the training data. The final output was derived from the aggregation of decisions made by multiple trees, with 40 trees used in this study and the gini measure employed for decision-making.\n\nOverall, the data encoding and preprocessing steps were designed to optimize the performance of the machine learning algorithms, ensuring that they could effectively predict COVID-19 cases using the available atmospheric and meteorological data.",
  "optimization/parameters": "In our study, we utilized several input parameters to predict COVID-19 cases using machine learning algorithms. The primary parameters considered were atmospheric conditions and particulate matter, specifically temperature, humidity, and PM2.5. These parameters were chosen based on their established relationships with COVID-19 transmission and their availability for the study locations.\n\nThe selection of these parameters was driven by previous research indicating that atmospheric conditions and air pollutants play significant roles in the spread of the virus. For instance, temperature and humidity have been shown to influence the transmission rates of respiratory viruses, including COVID-19. Particulate matter, particularly PM2.5, can also affect respiratory health and potentially facilitate the spread of the virus.\n\nIn addition to these atmospheric parameters, we also considered the number of COVID-19 cases from previous days (n-days lag) as inputs to our models. This approach was chosen to account for the latency period of the virus, which can range from 7 to 14 days. By including n-days lag cases, we aimed to provide the models with sufficient information for better prediction and forecasting.\n\nThe specific number of lags (n) varied depending on the location and the model used. For example, in some locations, the models agreed on the number of lags, while in others, different lags were obtained. This variability highlights the importance of tailoring the input parameters to the specific characteristics of each location.\n\nOverall, the selection of input parameters was a critical step in our optimization process. By carefully choosing and tuning these parameters, we were able to improve the performance of our machine learning models in predicting COVID-19 cases.",
  "optimization/features": "In our study, we utilized three primary atmospheric parameters as input features for predicting COVID-19 cases. These features were particulate matter (PM2.5), temperature, and humidity. Additionally, we considered the number of COVID-19 cases from previous days as another set of input features. The specific number of lags (previous days) varied depending on the location and the model used, but generally ranged from 1 to 14 days.\n\nFeature selection was not explicitly performed in the traditional sense, as we focused on using known relevant atmospheric parameters and historical COVID-19 case data. The selection of these features was based on existing literature and their known associations with the spread of respiratory viruses. The choice of lags was determined through model performance evaluation, ensuring that the models used the optimal amount of historical data for accurate predictions.\n\nAll feature selection and lag determination processes were conducted using the training set only, ensuring that the testing set remained unbiased and that the models' performance could be accurately evaluated on unseen data. This approach helped maintain the integrity of our predictive models and ensured that our findings were robust and generalizable.",
  "optimization/fitting": "The fitting method employed in this study involved the use of four machine learning algorithms: Decision Tree, k-Nearest Neighbor (KNN), Support Vector Machine (SVM), and Random Forest. Each algorithm was trained using 80% of the data, with the remaining 20% reserved for testing. The root mean square error (RMSE) was used as the test statistic to evaluate the performance of these models.\n\nThe number of parameters in the models varied, but generally, the complexity was managed to avoid overfitting. For instance, in the KNN algorithm, the number of neighbors was set to 48, and the kd-tree approach was used to estimate the nearest neighbors efficiently. This helped in balancing the model complexity and preventing overfitting. Similarly, in the SVM algorithm, a radial basis function kernel with a polynomial degree of 3 was used, which provided a good trade-off between model flexibility and generalization.\n\nTo rule out overfitting, techniques such as cross-validation were likely employed, although not explicitly mentioned. Cross-validation ensures that the model generalizes well to unseen data by training and validating on different subsets of the data. Additionally, the use of a separate test set (20% of the data) that was not used during training helped in assessing the model's performance on new, unseen data.\n\nUnderfitting was addressed by ensuring that the models were sufficiently complex to capture the underlying patterns in the data. For example, the Decision Tree algorithm used the gini index for splitting, which helps in creating more informative splits. The Random Forest algorithm, which is an ensemble of decision trees, used 40 trees with the gini measure, providing a robust model that is less prone to underfitting.\n\nIn summary, the fitting method involved careful selection of model parameters and the use of techniques to balance model complexity, thereby avoiding both overfitting and underfitting. The evaluation using RMSE on a separate test set further ensured that the models were reliable and generalizable.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in this study exhibit varying degrees of interpretability. Among the algorithms used, Decision Trees (DT) and Random Forests (RF) are particularly notable for their transparency. Decision Trees are inherently interpretable, as they consist of a series of decision rules based on input features. Each internal node represents a decision based on a feature, and the branches represent the outcomes of these decisions, leading to a final prediction at the leaf nodes. This structure allows for a clear understanding of how predictions are made, making it easy to trace the decision path for any given input.\n\nRandom Forests, being an ensemble of Decision Trees, also offer a degree of interpretability. While individual trees in a Random Forest can be examined to understand their decision paths, the overall model's predictions are an aggregate of many trees. This can make the interpretability more complex, but techniques such as feature importance scores can still provide insights into which features are most influential in the model's decisions.\n\nIn contrast, algorithms like Support Vector Machines (SVM) and k-Nearest Neighbors (KNN) are less transparent. SVM involves finding a hyperplane that best separates the data, and while the support vectors (data points closest to the hyperplane) can provide some insight, the overall decision boundary can be complex and difficult to interpret, especially when using non-linear kernels. KNN, on the other hand, makes predictions based on the majority vote of the nearest neighbors in the feature space. While the nearest neighbors can be examined for individual predictions, the overall model does not provide a clear set of rules or decision paths.\n\nIn summary, while Decision Trees and Random Forests offer a high degree of interpretability, allowing for clear insights into the decision-making process, SVM and KNN are more opaque, making it challenging to understand the underlying logic of their predictions.",
  "model/output": "The model employed in this study is a regression model. The primary objective was to predict the number of COVID-19 cases based on various atmospheric parameters and past infection data. The performance of the models was evaluated using the root mean square error (RMSE), which is a common metric for regression tasks. This metric helps in understanding the accuracy of the predictions by measuring the average magnitude of the errors between predicted and actual values.\n\nFour different machine learning algorithms were utilized: Decision Tree (DT), Random Forest (RF), Support Vector Machine (SVM), and k-Nearest Neighbor (KNN). Each of these algorithms was trained on 80% of the data and tested on the remaining 20%. The models were assessed across multiple locations, including Abuja, Delta, Edo, Osun, Kebbi, and Kano States, to determine their predictive capabilities.\n\nThe study found that the models performed differently depending on the predictors used (PM2.5, temperature, and humidity) and the lag periods considered. For instance, in Abuja, temperature was identified as the best predictor with a minimal error of about 3 cases. In Delta State, the models showed varying lags when PM2.5 was used as the sole predictor, with SVM and DT exhibiting the lowest and highest RMSE values, respectively. Similarly, in Edo State, the models required different lag periods for effective predictions, with SVM achieving the best results with an error of approximately 24 cases.\n\nOverall, the models demonstrated varying levels of accuracy and reliability across different locations and predictors. The findings suggest that machine learning algorithms can be effectively used to predict COVID-19 cases, providing valuable insights for proactive monitoring and control measures.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our study, the evaluation of the machine learning algorithms was conducted using a split-sample approach, where 80% of the data was allocated for training the models, and the remaining 20% was reserved for testing their performance. This method ensures that the models are trained on a substantial portion of the data while being evaluated on unseen data, which helps in assessing their generalization capabilities.\n\nThe primary metric used for evaluation was the root mean square error (RMSE). RMSE was chosen due to its ability to penalize larger errors more heavily, providing a more accurate reflection of the model's predictive performance. Additionally, RMSE shares the same units as the dependent variable, making it an intuitive measure for interpreting the model's accuracy.\n\nThe evaluation process involved predicting COVID-19 cases using various atmospheric parameters as predictors at different lags. This approach allowed us to determine the optimal number of lags required for each model to achieve the best predictive performance. The models were evaluated across multiple locations, including Kebbi, Kano, Abuja, Delta, Edo, and Osun States, to ensure the robustness and generalizability of our findings.\n\nFurthermore, we compared the performance of four machine learning algorithms: Decision Tree (DT), k-Nearest Neighbor (KNN), Support Vector Machine (SVM), and Random Forest (RF). The evaluation revealed that the performance of these models varied depending on the location and the specific atmospheric parameter used as a predictor. For instance, KNN and SVM often showed identical performances in several locations, while RF demonstrated superior performance in others. This comprehensive evaluation provides insights into the strengths and weaknesses of each algorithm in predicting COVID-19 cases under different conditions.",
  "evaluation/measure": "In our study, we employed the root mean square error (RMSE) as the primary performance metric to evaluate the effectiveness of our machine learning models. RMSE is a widely used metric in the literature for regression tasks due to its ability to penalize larger errors more heavily, providing a more accurate representation of model performance. This metric is particularly useful in our context as it allows us to measure the differences between predicted and actual COVID-19 case numbers in the same units, making the results more interpretable.\n\nThe choice of RMSE is representative of common practices in the field of machine learning and statistical modeling. It is a standard metric used in various studies that involve time-series forecasting and predictive modeling, including those related to epidemiological data. By using RMSE, we ensure that our results are comparable with other studies in the literature, facilitating a more comprehensive understanding of model performance across different research efforts.\n\nIn addition to RMSE, we also considered the number of lags required by each model to achieve optimal performance. This aspect is crucial as it provides insights into the temporal dependencies of the predictors (PM2.5, temperature, and humidity) on COVID-19 case numbers. The agreement or disagreement among models regarding the optimal number of lags further enriches our analysis, highlighting the robustness and reliability of our findings.\n\nOverall, the use of RMSE as the primary performance metric, coupled with the analysis of lag times, provides a thorough and representative evaluation of our models. This approach aligns with established practices in the literature, ensuring that our study contributes meaningfully to the existing body of knowledge on predictive modeling for COVID-19.",
  "evaluation/comparison": "In our study, we compared the performance of four machine learning algorithms: Decision Tree, k-Nearest Neighbor, Support Vector Machine, and Random Forest. These algorithms were evaluated based on their ability to predict COVID-19 cases using various predictors, including past COVID-19 cases and atmospheric parameters such as particulate matter (PM2.5), temperature, and humidity.\n\nThe comparison was conducted across multiple locations, and the performance of each algorithm was assessed using the root mean square error (RMSE) as the test statistic. This metric was chosen for its ability to compensate for large errors and its consistency with the units of the dependent variable.\n\nWe found that k-Nearest Neighbor and Support Vector Machine generally showed superior performance in predicting future COVID-19 cases based on past histories. These algorithms demonstrated identical performances in three of the locations considered and showed superior performance in two other locations. Decision Tree, on the other hand, was found to perform poorly in the prediction of COVID-19 cases when considering particulate matter and atmospheric parameters as predictors.\n\nRandom Forest, while not the best-performing algorithm in all scenarios, showed the greatest performance at zero lag with superior results in four locations. This suggests that Random Forest can be effective in short-term predictions.\n\nIn terms of atmospheric predictors, temperature was found to be the best predictor for the number of COVID-19 cases, followed by relative humidity. This indicates that atmospheric conditions play a significant role in the spread of the virus and can be effectively used in predictive models.\n\nOverall, our comparison of these machine learning algorithms provides valuable insights into their relative strengths and weaknesses in the context of COVID-19 prediction. This information can guide policymakers in selecting the most appropriate models for proactive monitoring and control of the pandemic.",
  "evaluation/confidence": "Evaluation Confidence\n\nThe performance metrics used in this study include the root mean square error (RMSE), which provides a measure of the differences between predicted and observed values. The RMSE values were calculated for different machine learning algorithms across various locations, allowing for a comparative analysis of their predictive performance.\n\nConfidence intervals for the performance metrics were not explicitly provided in the study. However, the consistent reporting of RMSE values across different models and locations offers a robust basis for evaluating the reliability and generalizability of the findings. The RMSE values were observed to vary across different models and locations, indicating the sensitivity of the algorithms to the specific conditions of each region.\n\nStatistical significance was not explicitly tested for the superiority of one method over others. However, the clear differences in RMSE values among the models in various locations suggest that certain algorithms perform better under specific conditions. For instance, the Random Forest (RF) model showed superior performance in Abuja, while the k Nearest Neighbor (KNN) model excelled in Kebbi. These differences imply that the choice of model can significantly impact predictive accuracy, and the observed variations are likely to be statistically meaningful.\n\nThe study's findings are supported by the consistent performance of certain models across multiple locations. For example, KNN and Support Vector Machine (SVM) showed identical performances in three of the locations considered, indicating a reliable pattern of performance. Similarly, the RF model demonstrated superior results in four locations, further reinforcing its effectiveness.\n\nIn summary, while confidence intervals and formal statistical tests were not reported, the consistent and varied performance of the models across different locations provides a strong indication of their relative effectiveness. The differences in RMSE values suggest that the observed performance metrics are statistically significant, supporting the claim that certain models are superior under specific conditions.",
  "evaluation/availability": "Not enough information is available."
}