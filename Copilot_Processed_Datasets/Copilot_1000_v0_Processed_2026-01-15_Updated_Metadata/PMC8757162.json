{
  "publication/title": "Validation of an Artificial Intelligence Algorithm for Diagnostic Prediction of Coronary Disease: Comparison with a Traditional Statistical Model.",
  "publication/authors": "Correia L, Lopes D, Porto JV, Lacerda YF, Correia VCA, Bagano GO, Pontes BSB, Melo MHV, Silva TEA, Meireles AC",
  "publication/journal": "Arquivos brasileiros de cardiologia",
  "publication/year": "2021",
  "publication/pmid": "35613162",
  "publication/pmcid": "PMC8757162",
  "publication/doi": "10.36660/abc.20200302",
  "publication/tags": "- Machine Learning\n- Coronary Artery Disease\n- Predictive Modeling\n- Logistic Regression\n- Cardiovascular Risk Factors\n- Diagnostic Accuracy\n- ROC Curve Analysis\n- Clinical Decision Support\n- Statistical Validation\n- Medical Data Analysis",
  "dataset/provenance": "The dataset used in this study was sourced from a prospective registry of patients presenting with chest pain and suspected coronary artery disease (CAD). The data collection took place from September 2011 to November 2017 at a hospital's coronary care unit. All patients admitted with chest pain and a clinical suspicion of CAD, regardless of their electrocardiogram results or troponin levels, were included in the study. The only exclusion criterion was the patient's refusal to participate.\n\nA total of 962 patients were enrolled in the study. These patients were then divided into two groups: a derivation sample consisting of the first two-thirds of the patients (641 individuals) and a validation sample consisting of the remaining one-third (321 individuals). The derivation sample was used to build the predictive models, while the validation sample was used to test the models' performance.\n\nThe data collected included various clinical and laboratory variables. These variables were categorized into three main groups: medical history and clinical presentation (13 variables), characteristics of chest discomfort (14 variables), and abnormal findings from imaging or laboratory tests at admission (11 variables). The laboratory tests were performed on plasma samples collected at the time of the patients' arrival at the emergency department. Medical history and chest pain characteristics were recorded by trained investigators to ensure standardization and minimize bias.\n\nThe primary outcome predicted by the models was the diagnosis of obstructive CAD, which was confirmed by subsequent tests during the hospital stay. These tests included invasive coronary angiography or provocative tests such as magnetic resonance perfusion imaging, single-photon emission computed tomography, or dobutamine stress echocardiography, as determined by the attending cardiologist. The diagnosis of obstructive CAD was defined as the presence of a stenosis of 70% or more in the coronary arteries.",
  "dataset/splits": "The dataset was divided into two main splits: a derivation set and a validation set. The derivation set consisted of 641 patients, with an average age of 59 years, 58% being male, and 30% having a history of coronary artery disease. This set was used to develop the models, including both the machine learning algorithm and the logistic regression model. The validation set comprised 221 patients, with similar characteristics to the derivation set, including an average age of 59 years, 58% being male, and 22% with a history of coronary artery disease. This set was used to evaluate the performance of the models.\n\nThe derivation set identified 330 patients with obstructive coronary artery disease, representing a prevalence of 52%. In the validation set, 163 patients were identified with obstructive coronary artery disease, a prevalence of 51%. Both sets were analyzed to ensure that the models were accurately calibrated and discriminatory. The derivation set was used to build the models, while the validation set was used to test their accuracy and reliability. The analysis of the derivation set was completed in January 2018 to avoid multiple analyses and ensure the integrity of the results.",
  "dataset/redundancy": "The dataset was divided into two main groups: a derivation sample and a validation sample. The derivation sample consisted of 641 patients, while the validation sample included 221 patients. Both samples were used to develop and validate models for predicting obstructive coronary artery disease (CAD).\n\nThe derivation sample was used to build the models, including both the machine learning algorithm and the logistic regression model. This sample was analyzed to identify key predictors and to develop initial models. The validation sample, on the other hand, was used to test the performance of these models, ensuring that the results were generalizable to new data.\n\nTo ensure independence between the training and test sets, the derivation and validation samples were kept separate throughout the analysis. This independence was crucial for evaluating the models' performance objectively. The derivation sample was used to train the models, while the validation sample was used to assess their discriminatory power and calibration without any overlap or contamination between the two groups.\n\nThe distribution of the samples was designed to reflect real-world scenarios, with a focus on patients presenting with acute chest pain. The derivation sample had a prevalence of 52% for obstructive CAD, while the validation sample had a prevalence of 51%. This similarity in prevalence helped to ensure that the models were tested under comparable conditions.\n\nCompared to previously published machine learning datasets, our approach emphasized the use of prospectively collected, high-quality data. This differed from typical machine learning datasets derived from large electronic health records, which often lack the rigor and standardization of prospective studies. By using a well-defined, prospectively collected dataset, we aimed to ensure that the models were built on reliable and accurate information, enhancing their validity and generalizability.",
  "dataset/availability": "Not applicable.",
  "optimization/algorithm": "The machine-learning algorithm used in this study is not specified by name, but it falls under the category of algorithms capable of translating an infinity of patterns into probabilities. This suggests that it is likely a type of model that can handle complex, non-linear relationships, such as a neural network, random forest, or gradient boosting machine.\n\nThe algorithm is not explicitly stated to be new, but it is applied in a novel context within the medical field, specifically for predicting coronary disease in patients with acute chest pain. The focus of this study is on the application and comparison of machine learning techniques to traditional statistical models in a medical setting, rather than the development of a new algorithm.\n\nThe reason the algorithm was not published in a machine-learning journal is that the primary focus of this research is on the medical application and validation of the model, rather than the innovation of the machine-learning technique itself. The study aims to contribute to the field of medicine by comparing the performance of machine learning algorithms with traditional statistical models in predicting coronary disease. The findings are published in a cardiology journal, reflecting the medical focus of the research.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, the data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the data was suitable for model training. Initially, we collected a comprehensive set of variables from a consecutive sample of 962 patients admitted with chest pain. These variables included demographic information, clinical parameters, medical history, and laboratory results.\n\nFor the machine-learning algorithm, all candidate predictors were included, unlike the logistic regression model, which was composed of only significant variables at the 5% significance level. This approach allowed the machine-learning model to capture a wide range of patterns and interactions within the data.\n\nThe variables were encoded numerically, with categorical variables being converted into a format that the algorithm could process. Continuous variables were standardized to ensure that they contributed equally to the model, regardless of their original scale. Missing values were handled through imputation techniques to maintain the integrity of the dataset.\n\nFeature selection was performed to identify the most relevant variables for predicting coronary disease. This involved evaluating the importance of each variable based on parameters such as node purity and the percentage increase in error associated with each variable. The final model included variables that demonstrated significant predictive power.\n\nAdditionally, the data was split into training and testing sets. The training set, consisting of the first two-thirds of the patients, was used to build the machine-learning model. The remaining third of the patients formed the test set, which was used to evaluate the model's performance. This split ensured that the model's accuracy and generalizability could be assessed on unseen data.\n\nIn summary, the data encoding and preprocessing involved numerical encoding of variables, standardization of continuous data, imputation of missing values, and feature selection. These steps were crucial in preparing the data for the machine-learning algorithm and ensuring the model's robustness and accuracy.",
  "optimization/parameters": "In our study, we utilized a comprehensive set of parameters to develop both the machine learning and logistic regression models for predicting coronary artery disease (CAD). The machine learning model incorporated all 55 variables related to medical history, clinical presentation, characteristics of chest pain, and laboratory test results. This approach allowed us to leverage the full spectrum of available data without imposing strict limitations on the number of variables.\n\nFor the logistic regression model, we initially considered 13 variables related to medical history and clinical presentation. Through a stepwise selection process, we identified seven variables that were significantly associated with obstructive CAD: age, male sex, acute left ventricular dysfunction, history of CAD, diabetes, smoking, and exercise-induced symptoms. These variables were included in an intermediate logistic regression model. However, history of CAD lost significance in this model, leaving six significant variables.\n\nWe also examined 14 variables related to chest pain characteristics, finding six that were positively associated with CAD: oppressive pain, radiation to the left arm, severe intensity, duration in minutes, relief with nitrates, and similarity to previous infarction. Three variables were negatively associated with CAD: worsening with compression, arm movement, and deep inspiration. When these nine variables were added to the logistic regression model, only three remained significant: worsening with compression, deep inspiration, and severe intensity.\n\nAdditionally, we evaluated 11 laboratory test results, identifying seven that were positively associated with CAD: ischemia on electrocardiogram, positive troponin, creatinine, glucose, NT-pro-BNP, C-reactive protein, and leukocyte count. In the logistic regression model, only ischemia on electrocardiogram and positive troponin remained significant.\n\nThe final logistic regression model included 11 significant variables from the intermediate models. After further analysis, nine variables were found to be significant predictors of CAD: age, male sex, ischemia on electrocardiogram, positive troponin, left ventricular dysfunction, exercise induction, smoking, diabetes, and worsening with deep inspiration (as a protective factor).\n\nIn summary, the machine learning model used all 55 available variables, while the logistic regression model was refined to include nine significant predictors. This approach ensured that our models were robust and based on the most relevant clinical and laboratory data.",
  "optimization/features": "In the optimization process, a total of 55 features were used as input for the machine learning model. These features encompassed various aspects, including medical history, clinical presentation, characteristics of chest pain, and laboratory test results.\n\nFeature selection was not explicitly performed for the machine learning model, as all 55 candidate predictors were included. This approach aligns with the flexibility of machine learning algorithms, which do not impose strict requirements on the number of variables or their distribution.\n\nFor the logistic regression model, however, feature selection was conducted. Initially, multiple variables were considered, but through a stepwise process, only significant variables at the 5% significance level were retained in the final model. This resulted in a logistic regression model composed of nine independent predictors.\n\nThe feature selection for the logistic regression model was performed using the training set only, ensuring that the validation set remained independent for evaluating the model's performance. This approach helps to prevent overfitting and provides a more reliable assessment of the model's generalizability.",
  "optimization/fitting": "The fitting method employed in this study involved both machine learning and logistic regression techniques. For the machine learning model, all 55 parameters were included without any elimination, which means the number of parameters was indeed larger than the number of training points. To address potential overfitting, the model utilized the analysis discriminat\u00f3ria de Fisher to create dendrograms, which were combined repeatedly until the error rate indicated optimal performance. This process helped in identifying the most relevant patterns and reducing the risk of overfitting.\n\nAdditionally, the model's performance was validated using a separate validation sample, ensuring that the results were generalizable and not merely a product of overfitting to the training data. The validation process included comparing the area under the ROC curve (AUC) between the machine learning model and the logistic regression model, which showed similar discriminatory power.\n\nFor the logistic regression model, a stepwise method was used to select variables, ensuring that only significant predictors were included. This approach helped in avoiding underfitting by including relevant variables and overfitting by excluding non-significant ones. The final model was constructed with variables that had a significance level of 5%, ensuring a balance between model complexity and performance.\n\nIn summary, the study employed rigorous validation techniques and model selection processes to mitigate both overfitting and underfitting, ensuring robust and reliable results.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting, ensuring the robustness of our models. For the machine learning model, we utilized the analysis discriminat\u00f3ria de Fisher to create dendrogramas, which were repeatedly combined until the error rate indicated optimal performance. This method inherently helps in selecting the most relevant features and reducing the complexity of the model, thus mitigating overfitting.\n\nAdditionally, we performed extensive validation procedures. The sample was divided into derivation and validation sets, with the derivation set used to build the algorithm and the validation set to test its performance. This split ensures that the model's performance is evaluated on unseen data, providing a more reliable estimate of its generalizability.\n\nFor the regression log\u00edstica model, we used the stepwise method for variable selection, which includes both forward and backward selection processes. This approach helps in identifying the most significant predictors while excluding those that do not contribute substantially to the model, thereby reducing the risk of overfitting.\n\nFurthermore, we applied the teste de DeLong to compare the areas under the ROC curves between the models, ensuring that the discrimination performance was rigorously evaluated. The teste de Hosmer-Lemeshow was used to assess the calibration of the models, providing an additional layer of validation.\n\nIn summary, our approach included feature selection, model validation through data splitting, and rigorous statistical testing to prevent overfitting and ensure the reliability of our predictive models.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are not explicitly detailed in the publication. The focus was primarily on comparing the performance of machine learning models versus logistic regression for predicting coronary artery disease. The machine learning model was developed using all available variables without preselection, and its performance was evaluated based on node purity and error increase percentages. However, specific details about the hyper-parameter tuning process, optimization schedule, or model files are not provided.\n\nThe study does not mention the availability of model files or optimization parameters for public access. Therefore, it is not possible to report where these configurations can be accessed or under what license they might be available. The emphasis was on the comparative analysis and validation of the models rather than on making the specific configurations publicly accessible.",
  "model/interpretability": "The model developed in this study leverages machine learning techniques, which are often considered black-box models due to their complexity and the intricate relationships they can capture. This means that the internal workings of the model are not easily interpretable, and the specific reasons behind its predictions can be difficult to discern.\n\nHowever, the performance of each variable within the machine learning model is presented in a detailed manner. This includes parameters such as the purity of the nodes and the percentage increase in error associated with each variable. These metrics provide some level of transparency by indicating the importance and impact of each variable on the model's predictions.\n\nFor instance, variables like age, sex, and certain medical conditions have significant weights in defining the probability of outcomes. This information can help clinicians understand which factors are most influential in the model's decisions, even if the exact mechanisms remain opaque.\n\nIn contrast, the logistic regression model used in the study is more transparent. It explicitly shows the relationship between each variable and the outcome through coefficients and odds ratios. This allows for a clearer interpretation of how changes in specific variables affect the predicted probability of the outcome.\n\nOverall, while the machine learning model offers powerful predictive capabilities, it lacks the same level of interpretability as the logistic regression model. The detailed performance metrics of individual variables in the machine learning model provide some insights, but the model itself remains largely a black box.",
  "model/output": "The model developed in this study includes both machine learning and logistic regression approaches for predicting disease outcomes. The logistic regression model is specifically designed for classification, aiming to predict the presence or absence of obstructive coronary artery disease (DAC) based on various clinical and laboratory variables. This model provides odds ratios and regression coefficients for each significant predictor, such as age, sex, ischemia on electrocardiogram, troponin levels, and others.\n\nThe machine learning model, on the other hand, incorporates a broader set of variables related to medical history, clinical presentation, characteristics of chest pain, and laboratory tests. It evaluates the performance of each variable using parameters like node purity and error increase percentage. Both models were validated using similar metrics, including the area under the receiver operating characteristic (ROC) curve, which measures their discriminative ability. The area under the ROC curve for the machine learning model was 0.81 (95% CI = 0.77 \u2013 0.86), comparable to the logistic regression model's area of 0.82 (95% CI = 0.78 \u2013 0.87). This indicates that both models have similar discriminative power in predicting the outcomes.\n\nIn terms of calibration, which assesses how well the predicted probabilities match the observed outcomes, the logistic regression model showed a better fit. The Hosmer-Lemeshow test results indicated a lower significance level for the difference between predicted and observed values in the logistic regression model (chi-square = 6.2, p = 0.62) compared to the machine learning model (chi-square = 12.9, p = 0.11). Additionally, the regression line between predicted probabilities and observed event incidence for the logistic regression model had an intercept of 0.010 and a slope of 1.004, with a high correlation coefficient (r = 0.981). For the machine learning model, the intercept was -0.119 and the slope was 1.228, with a slightly lower correlation coefficient (r = 0.953).\n\nOverall, while both models demonstrate strong predictive capabilities, the logistic regression model appears to have a slight edge in terms of calibration. The study concludes that a precise machine learning model can be developed from a relatively simple and moderate-sized patient sample, but it does not outperform the traditional logistic regression model in this specific context.",
  "model/duration": "The execution time for the models was not explicitly detailed in the publication. However, it is worth noting that the analysis of the data was completed in January 2018, suggesting that the model development and validation processes were conducted within a reasonable timeframe. The study involved a sample of 962 patients, with the training sample consisting of the first two-thirds of patients and the test sample consisting of the remaining third. The models included a machine learning algorithm and a traditional logistic regression model. Both models were evaluated for their predictive performance in terms of discrimination and calibration. The area under the ROC curve for the machine learning algorithm was 0.81, similar to that of the logistic model, which was 0.82. The statistical significance of the difference between the models was assessed, with a p-value of 0.68, indicating no significant difference in performance between the two approaches. The study also involved various statistical tests, such as the DeLong test for comparing ROC curves and the Hosmer-Lemeshow test for calibration, which would have contributed to the overall execution time. However, specific details on the computational time required for each step of the model development and validation process are not provided.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method involved constructing two probabilistic models for predicting coronary disease using the first two-thirds of a sample of 962 patients admitted with chest pain. The models included a machine learning algorithm and a traditional logistic regression model. The performance of these predictive strategies was then evaluated using the remaining third of the patients.\n\nThe area under the ROC curve (AUC) was used to test discrimination between the models. The AUC for the machine learning algorithm was 0.81 (95% CI = 0.77 - 0.86), which was similar to the AUC of 0.82 (95% CI = 0.77 - 0.87) obtained with the logistic model. The p-value for the comparison was 0.68, indicating no significant difference in discrimination between the two models.\n\nCalibration was assessed using the Hosmer-Lemeshow test and by calculating the intercept and slope of the regression line between predicted probabilities and observed event rates. The logistic model showed a better calibration with a lower significance level for the difference between predicted and observed values (chi-square = 6.2, p = 0.62) compared to the machine learning model (chi-square = 12.9, p = 0.11). Additionally, the logistic model had an intercept of 0.010 (95% CI = -0.083 \u2013 0.103) and a slope of 1.004 (95% CI = 0.840 \u2013 1.168), indicating a well-calibrated model. The machine learning model had an intercept of -0.119 (95% CI = -0.296 \u2013 0.059) and a slope of 1.228 (95% CI = 0.909 \u2013 1.547).\n\nThe evaluation also involved verifying several assumptions before performing linear regression, including linear relationship, independence of observations, normality of residuals, and homoscedasticity of residuals. The statistical significance was defined as p < 0.05, and the software used for data analysis was SPSS.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the models' effectiveness in predicting coronary artery disease (CAD) in patients with acute chest pain. The primary metrics reported include the area under the Receiver Operating Characteristic (ROC) curve and calibration measures.\n\nThe area under the ROC curve (AUC) is a widely used metric in the literature for assessing the discriminative power of a model. It provides a single scalar value that summarizes the model's ability to distinguish between positive and negative cases. In our evaluation, the AUC for the machine learning model was 0.81 (95% CI = 0.77 \u2013 0.86), which was very similar to the AUC of 0.82 (95% CI = 0.78 \u2013 0.87) obtained by the logistic regression model. This similarity was statistically confirmed with a p-value of 0.68, indicating no significant difference between the two models in terms of discrimination.\n\nIn addition to discrimination, we also evaluated the calibration of the models. Calibration refers to the agreement between the predicted probabilities and the actual outcomes. We used the Hosmer-Lemeshow test to assess calibration, which compares the observed and expected frequencies of events across deciles of predicted probabilities. Both models passed this test, but the logistic regression model showed a lower chi-square value (6.2, p = 0.62) compared to the machine learning model (12.9, p = 0.11), suggesting better calibration.\n\nFurthermore, we performed linear regression between the predicted probabilities and the observed incidence of events. For the logistic regression model, the intercept was 0.010 (95% CI = -0.083 \u2013 0.103) and the slope was 1.004 (95% CI = 0.840 \u2013 1.168), with a correlation coefficient (r) of 0.981. For the machine learning model, the intercept was -0.119 (95% CI = -0.296 \u2013 0.059) and the slope was 1.228 (95% CI = 0.909 \u2013 1.547), with a correlation coefficient (r) of 0.953. These results indicate that the logistic regression model had a slope closer to 1 and a higher correlation coefficient, further supporting its better calibration.\n\nThe set of metrics used in our study is representative of those commonly reported in the literature for evaluating predictive models in medical research. The AUC is a standard metric for assessing discrimination, while calibration measures, such as the Hosmer-Lemeshow test and regression calibration plots, are essential for evaluating the reliability of predicted probabilities. By including both discrimination and calibration metrics, we provide a comprehensive evaluation of the models' performance.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison between machine learning algorithms and traditional statistical models, specifically logistic regression, to evaluate their performance in predicting coronary artery disease in patients presenting with acute chest pain.\n\nWe did not perform a comparison to publicly available methods on benchmark datasets. Instead, we focused on a direct comparison between machine learning and logistic regression using our own dataset. This dataset consisted of 962 patients admitted with chest pain, with the first two-thirds used for model training and the remaining third for testing.\n\nThe machine learning algorithm considered all candidate predictors, while the logistic regression model included only significant variables at the 5% significance level. This approach allowed us to assess the performance of both methods under comparable conditions.\n\nFor the evaluation, we used the area under the Receiver Operating Characteristic (ROC) curve to test discrimination, with comparisons between models made using the DeLong test. Calibration was assessed using the Hosmer-Lemeshow test and by calculating the slope and intercept of the regression line between predicted probabilities and observed event incidence.\n\nThe results showed that the area under the ROC curve for the machine learning algorithm was 0.81 (95% CI = 0.77 - 0.86), which was similar to that of the logistic regression model at 0.82 (95% CI = 0.77 - 0.87), with a p-value of 0.68. This indicates that there was no significant difference in discriminatory power between the two models.\n\nIn terms of calibration, both models passed the Hosmer-Lemeshow test, but the logistic regression model showed a lower significance level for the difference between predicted and observed values (chi-square = 6.2; p = 0.62) compared to the machine learning model (chi-square = 12.9; p = 0.11). This suggests that the logistic regression model had better calibration.\n\nOverall, our study suggests that while machine learning can construct accurate predictive models, it does not necessarily outperform traditional statistical methods like logistic regression in all scenarios. The choice of model may depend on the specific context and the quality of the data available.",
  "evaluation/confidence": "The evaluation of our models included several performance metrics, each accompanied by confidence intervals to provide a range within which the true value is likely to lie. For instance, the area under the ROC curve (AUC) for both the machine learning algorithm and the logistic regression model was reported with 95% confidence intervals. The AUC for the machine learning algorithm was 0.81 (95% CI = 0.77 - 0.86), while for the logistic regression model, it was 0.82 (95% CI = 0.78 - 0.87). These intervals help in understanding the precision of our estimates.\n\nStatistical significance was also a key consideration in our evaluation. We used the DeLong test to compare the AUCs of the two models, and the p-value obtained was 0.68, indicating that there is no statistically significant difference between the two models in terms of discrimination. Additionally, the calibration of the models was assessed using the Hosmer-Lemeshow test, which showed that both models were well-calibrated. The logistic regression model had a chi-square value of 6.2 with a p-value of 0.62, while the machine learning model had a chi-square value of 12.9 with a p-value of 0.11. These results suggest that neither model is significantly better than the other in terms of calibration.\n\nFurthermore, we performed linear regression to assess the relationship between predicted probabilities and observed outcomes. For the logistic regression model, the intercept was 0.010 (95% CI = -0.083 \u2013 0.103) and the slope was 1.004 (95% CI = 0.840 \u2013 1.168), with a correlation coefficient (r) of 0.981. For the machine learning model, the intercept was -0.119 (95% CI = -0.296 \u2013 0.059) and the slope was 1.228 (95% CI = 0.909 \u2013 1.547), with a correlation coefficient (r) of 0.953. These results indicate that both models are well-calibrated, but the logistic regression model shows a slightly better fit.\n\nIn summary, while both models performed similarly in terms of discrimination and calibration, the statistical analyses did not provide sufficient evidence to claim that one method is superior to the other. The confidence intervals and p-values support the conclusion that the machine learning algorithm is not significantly better than the traditional logistic regression model in this context.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study was conducted using specific datasets derived from patient records, which are not released to the public due to privacy and ethical considerations. The analysis and results presented in the publication are based on these internal datasets. The study adheres to ethical guidelines and regulations, ensuring that patient data is handled confidentially and securely. Therefore, access to the raw evaluation files is restricted to the research team and authorized personnel involved in the study."
}