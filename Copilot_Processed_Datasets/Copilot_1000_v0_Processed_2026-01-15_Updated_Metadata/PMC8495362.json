{
  "publication/title": "Testing soft tissue radiodensity parameters interplay with age and self-reported physical activity.",
  "publication/authors": "Recenti M, Ricciardi C, Edmunds K, Jacob D, Gambacorta M, Gargiulo P",
  "publication/journal": "European journal of translational myology",
  "publication/year": "2021",
  "publication/pmid": "34251162",
  "publication/pmcid": "PMC8495362",
  "publication/doi": "10.4081/ejtm.2021.9929",
  "publication/tags": "- CT Scan\n- lifestyle\n- physical activity\n- machine learning\n- soft tissues\n- aging\n- sarcopenia\n- muscle mass\n- radiodensity\n- health assessment",
  "dataset/provenance": "The dataset used in this study is derived from the AGES-Reykjavik study, specifically the initial cross-sectional subset known as AGES-I. This database includes data from 3,157 healthy elderly subjects aged between 66 and 93 years, with an average age of 74.87 years. These participants underwent a series of biometric measurements, including CT scans, and provided additional information such as age and physical activity levels.\n\nThe ethical approval for this work was granted by the Icelandic National Bioethics Committee, and informed consent was obtained from all participants. Due to missing surveys, the total sample size for physical activity analyses was 3,137 subjects. The dataset was divided into three age groups: 66-72 years, 73-79 years, and 80-83 years, with 1,176, 1,371, and 610 subjects respectively. Additionally, the dataset was categorized into three physical activity classes: \"Frequent,\" \"Occasional,\" and \"Rare - No Activity,\" based on self-reported physical activity frequency.\n\nThe AGES-Reykjavik dataset has been utilized in various studies, including those focusing on the relationship between soft tissue radiodensity and factors such as age and physical activity. The dataset's comprehensive nature and the detailed biometric measurements make it a valuable resource for research in gerontology and related fields. The AGES-Reykjavik dataset is not publicly available, but requests for data access can be directed to the AGES-Reykjavik Study Executive Committee.",
  "dataset/splits": "The dataset was divided into three subgroups according to age. The mean and standard deviations of age for each group are as follows: Group 1: 70.06 \u00b1 1.61 years; Group 2: 75.66 \u00b1 1.86 years; and Group 3: 82.35 \u00b1 2.39 years. The number of subjects in each age group for the age regression analysis were: 1,176 subjects for the age group 66-72 years, 1,371 subjects for the age group 73-79 years, and 610 subjects for the age group 80-93 years.\n\nAdditionally, subjects were divided into three activity level classes for the physical activity analysis: \u201cRare - No Activity\u201d, \u201cOccasional\u201d, and \u201cFrequent\u201d. The specific number of subjects in each activity level class is not provided.\n\nThe dataset was also divided into healthy and not healthy groups within each age category for lifestyle index prediction. The average values of the NTRA parameters for these groups are provided in tables, but the exact number of subjects in each healthy and not healthy subgroup is not specified.",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "The dataset used in this study, specifically the AGES-Reykjavik dataset, is not publicly available. The data cannot be made accessible to the general public due to privacy and ethical considerations. However, requests for access to these data can be directed to the AGES-Reykjavik Study Executive Committee. Interested parties should contact Ms. Camilla Kristjansdottir at camilla@hjarta.is for further information on how to obtain access. This controlled access ensures that the data is used responsibly and in accordance with ethical guidelines, maintaining the privacy and confidentiality of the participants involved in the study.",
  "optimization/algorithm": "The machine-learning algorithms employed in this study belong to the tree-based class. Specifically, random forests (RF), AdaBoosting (ADA-B), and gradient boosting (GB) were utilized. These algorithms are well-established in the field of machine learning and have been widely applied in various biomedical studies.\n\nThe algorithms used are not new; they have been extensively studied and applied in numerous research works. The choice to use these algorithms was driven by their proven effectiveness in previous applications, particularly in the context of nonlinear trimodal regression analysis (NTRA). The decision to use tree-based algorithms was also influenced by their ability to handle complex datasets and provide robust results.\n\nThe study focused on the application of these algorithms to a specific biomedical problem, rather than the development of new machine-learning techniques. Therefore, it was published in a journal that aligns with the study's primary focus on translational myology, rather than a machine-learning journal. The emphasis was on demonstrating the utility of these algorithms in analyzing soft tissue radiodensity related to age and physical activity, rather than innovating in the field of machine learning.",
  "optimization/meta": "The model employed in this study does not function as a meta-predictor. Instead, it utilizes specific machine learning algorithms directly on the data derived from computed tomography scans. The algorithms used include random forests (RF), AdaBoosting (ADA-B), and gradient boosting (GB). These algorithms were combined with 10-fold cross-validation to ensure robust performance evaluation. Additionally, the Synthetic Minority Oversampling Technique (SMOTE) was implemented to enhance the quality of the classification by oversampling minority classes.\n\nThe study focuses on the use of nonlinear trimodal regression analysis (NTRA) to quantify sarcopenic muscle degeneration and other related parameters. The NTRA parameters were extracted from the radiodensitometric distributions of soft tissues, including muscle, fat, and connective tissue. These parameters were then used as inputs for the machine learning algorithms to predict various health outcomes, such as age, physical activity levels, and the presence of certain conditions like diabetes and hypertension.\n\nThe training data for these algorithms was derived from a well-defined dataset, ensuring that the data used for training was independent. The dataset was divided into subgroups based on age, and statistical analyses were performed to assess the significance of the NTRA parameters across different age groups. This approach ensured that the model's predictions were based on reliable and independent data, enhancing the overall accuracy and robustness of the results.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, the dataset was divided into three subgroups based on age, and a one-way Kruskal-Wallis test was performed to assess the non-normality of the Nonlinear Trimodal Regression Analysis (NTRA) parameters. This statistical analysis helped in understanding the distribution and variability of the data across different age groups.\n\nFor the machine learning analyses, the Knime analytics platform was employed to perform both regression and classification tasks. Three popular tree-based algorithms were utilized: random forests (RF), AdaBoosting (ADA-B), and gradient boosting (GB). These algorithms were combined with 10-fold cross-validation to ensure the robustness and generalizability of the models.\n\nTo handle class imbalances, the Synthetic Minority Oversampling Technique (SMOTE) was implemented. SMOTE oversamples the minority classes, thereby improving the quality of the classification models. This technique is particularly useful in scenarios where certain classes are underrepresented, which can lead to biased models if not addressed.\n\nThe NTRA parameters were used as initial features for the machine learning models. These parameters were derived from the trimodal quasi-probability density function, which defines the distribution of Hounsfield Units (HU) for muscle, fat, and connective tissue. The parameters included distribution amplitude, location in HU, distribution width, and skewness term, which were fitted to the data using a generalized reduced gradient algorithm.\n\nThe dataset was also divided into three activity level classes: \"Rare - No Activity,\" \"Occasional,\" and \"Frequent.\" A similar statistical analysis workflow was applied to investigate the NTRA parameters according to physical activity levels. This involved performing a one-way Kruskal-Wallis test followed by a posthoc test to compute differences in each parameter across the activity level groups.\n\nIn summary, the data encoding and preprocessing involved statistical analysis to understand data distribution, the use of tree-based machine learning algorithms with cross-validation, and the application of SMOTE to handle class imbalances. The NTRA parameters served as the primary features for the machine learning models, and the dataset was carefully divided into age and activity level groups for comprehensive analysis.",
  "optimization/parameters": "In our study, we utilized a total of 11 subject-specific NTRA parameters. These parameters were derived from the fitting of HU distribution curves using a generalized reduced gradient algorithm, which minimized the standard error at each radio-absorption bin. The selection of these parameters was based on a trimodal quasi-probability density function that characterizes different types of soft tissues, including muscle, fat, and loose connective tissue and atrophic muscle. This function is defined by several key components: the distribution amplitude (N), the location in Hounsfield Units (\u03bc), the distribution width (\u03c3), and the skewness term (\u03b1), which accounts for asymmetries in the distribution. The skewness term is not considered for the central connective tissue distribution. This approach allows for a detailed and subject-specific analysis of soft tissue radiodensity, which is crucial for understanding the relationships between age, physical activity, and health status.",
  "optimization/features": "In our study, we utilized 11 specific features derived from Nonlinear Trimodal Regression Analysis (NTRA) as input for our machine learning models. These features were carefully selected to represent the radiodensity characteristics of muscle, fat, and connective tissues.\n\nFeature selection was not explicitly performed as a separate step. Instead, we focused on extracting meaningful parameters from the NTRA that were relevant to the biological context of our study. These parameters included amplitude, location, width, and skewness for each tissue type, which were deemed essential for capturing the variations in soft tissue radiodensity related to age and physical activity.\n\nThe selection of these features was based on domain knowledge and preliminary analyses, ensuring that they were biologically relevant and statistically significant. The features were consistent across all datasets used in the study, including those for age classification, physical activity classification, and lifestyle index prediction. This approach helped in maintaining the integrity and consistency of the input features across different analyses.",
  "optimization/fitting": "The fitting method employed in this study involves a trimodal quasi-probability density function to model the radiodensity distributions of muscle, fat, and connective tissue. This approach results in the extraction of 11 subject-specific parameters for each individual. The number of parameters is not excessively large compared to the number of training points, as the dataset includes a substantial number of subjects across different age groups and physical activity levels.\n\nTo address potential overfitting, a generalized reduced gradient algorithm was used to minimize standard errors at each radio-absorption bin. This method ensures that the model parameters are optimized in a way that generalizes well to the data, rather than fitting noise. Additionally, the use of 10-fold cross-validation with tree-based machine learning algorithms, such as random forests and gradient boosting, further helps in mitigating overfitting by providing a robust estimate of model performance.\n\nUnderfitting was addressed by ensuring that the model complexity was sufficient to capture the underlying patterns in the data. The trimodal distribution function, with its parameters for amplitude, location, and width, provides a flexible framework that can adapt to the variability in radiodensity distributions across different tissues and subjects. The statistical significance of the differences observed in the parameters across age groups and physical activity levels indicates that the model is capable of capturing meaningful variations in the data.\n\nOverall, the fitting method and the subsequent machine learning analyses were designed to balance model complexity and generalization, ensuring that the results are both accurate and reliable.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting, ensuring the robustness and generalizability of our machine learning models. One key method used was cross-validation, specifically 10-fold cross-validation. This technique involves dividing the dataset into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. Cross-validation helps to assess the model's performance more reliably and reduces the risk of overfitting by ensuring that the model generalizes well to unseen data.\n\nAdditionally, we utilized the Synthetic Minority Oversampling Technique (SMOTE). SMOTE is an oversampling method that addresses class imbalance by generating synthetic samples for the minority class. This technique helps to improve the classification performance, particularly for minority classes, and reduces the risk of the model becoming biased towards the majority class. By balancing the dataset, SMOTE contributes to a more robust and generalizable model.\n\nFurthermore, we employed tree-based algorithms such as Random Forests (RF) and Gradient Boosting (GB). These algorithms inherently include mechanisms to prevent overfitting. Random Forests, for instance, use an ensemble of decision trees, each trained on a different subset of the data, which helps to reduce variance and prevent overfitting. Gradient Boosting, on the other hand, builds trees sequentially, each correcting the errors of the previous ones, and includes regularization parameters to control the complexity of the model and prevent overfitting.\n\nIn summary, our study incorporated cross-validation, SMOTE, and tree-based algorithms to effectively prevent overfitting and ensure the reliability and generalizability of our machine learning models.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, we employed three popular tree-based algorithms: random forests (RF), AdaBoosting (ADA-B), and gradient boosting (GB). These algorithms were combined with 10-fold cross-validation to ensure robust performance evaluation. Additionally, the Synthetic Minority Oversampling Technique (SMOTE) was implemented to enhance the quality of classification by oversampling minority classes.\n\nThe key evaluation metrics considered for regression analysis included the Coefficient of Determination (R\u00b2), Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE). For classification analysis, we focused on Accuracy, Sensitivity, and Specificity.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly provided in the publication. The study primarily focuses on the methodological approach and the results obtained from the analysis rather than the specific model files or detailed optimization schedules. Therefore, while the hyper-parameter configurations and evaluation metrics are reported, the model files and optimization parameters are not made available.\n\nNot applicable",
  "model/interpretability": "The model employed in this study is not a blackbox. It utilizes a combination of nonlinear trimodal regression analysis (NTRA) and machine learning algorithms, which together provide a transparent and interpretable approach.\n\nNTRA involves fitting a trimodal quasi-probability density function to the Hounsfield Unit (HU) distribution of soft tissues. This function is defined by parameters such as distribution amplitude (N), location in HU (\u03bc), distribution width (\u03c3), and skewness (\u03b1). These parameters are subject-specific and are extracted through standard error minimization using a generalized reduced gradient algorithm. This process results in 11 subject-specific NTRA parameters, which can be directly interpreted in the context of muscle, fat, and connective tissue characteristics.\n\nThe machine learning component of the model uses tree-based algorithms such as random forests (RF), AdaBoosting (ADA-B), and gradient boosting (GB). These algorithms are known for their interpretability, as they can provide feature importance scores, which indicate the contribution of each NTRA parameter to the model's predictions. This allows for a clear understanding of how different tissue characteristics influence the outcomes, such as age or health status.\n\nAdditionally, the use of 10-fold cross-validation ensures that the model's performance is robust and generalizable. The evaluation metrics, including Coefficient of Determination (R\u00b2), Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE) for regression, and Accuracy, Sensitivity, and Specificity for classification, provide a comprehensive assessment of the model's performance and reliability.\n\nIn summary, the model's transparency is achieved through the interpretable nature of NTRA parameters and the feature importance scores provided by the machine learning algorithms. This allows for a clear understanding of the relationships between tissue characteristics and the outcomes being predicted.",
  "model/output": "The model employed in this study is both classification and regression, depending on the specific analysis being conducted. For regression tasks, we utilized machine learning algorithms to predict continuous variables, such as age. The regression models were built using tree-based algorithms, specifically Random Forests (RF) and Gradient Boosting (GB), combined with 10-fold cross-validation. These models aimed to capture the variance in the data, with evaluation metrics including the Coefficient of Determination (R\u00b2), Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE).\n\nFor classification tasks, the model was used to categorize subjects into different groups based on age and physical activity levels. The classification models also employed RF and GB algorithms, with the Synthetic Minority Oversampling Technique (SMOTE) to improve the quality of the classification by oversampling minority classes. Key evaluation metrics for classification included Accuracy, Sensitivity, and Specificity. The results indicated that while the regression models showed varying levels of performance across different age groups, the classification models achieved modest accuracy, with the best results reaching around 60% for both age and physical activity classification.\n\nThe study also explored the use of these models to classify subjects into healthy and not healthy groups based on a lifestyle index, achieving high accuracy in this context. Overall, the model's performance was evaluated through a combination of regression and classification metrics, providing insights into its predictive capabilities for different types of analyses.",
  "model/duration": "The execution time for the machine learning models varied depending on the specific algorithm and the complexity of the analysis. The models were implemented using the Knime analytics platform, which facilitated the execution of tree-based algorithms such as random forests (RF), AdaBoosting (ADA-B), and gradient boosting (GB). These algorithms were combined with 10-fold cross-validation to ensure robust performance evaluation.\n\nThe random forest algorithm generally provided the best accuracy results, consistently achieving high sensitivity and specificity. The gradient boosting algorithm also performed well, though slightly less effectively than random forests. AdaBoosting, while included in the analysis, yielded poorer results compared to the other two algorithms and was not reported in detail.\n\nThe execution time for these models was influenced by several factors, including the size of the dataset, the number of features, and the computational resources available. The dataset was divided into three age groups, and the analysis involved a significant number of NTRA parameters, which added to the computational load.\n\nOverall, the models were designed to be efficient, balancing computational time with the need for accurate predictions. The use of 10-fold cross-validation ensured that the models were thoroughly tested, but this process did add to the overall execution time. Despite these considerations, the models were able to provide valuable insights into the classification of subject age and physical activity levels, demonstrating their effectiveness in handling complex datasets.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several key steps and techniques to ensure its robustness and accuracy. Initially, the dataset was divided into three subgroups based on age, and a one-way Kruskal-Wallis test was performed to assess the non-normality of the NTRA parameters. This statistical analysis helped identify significant groupwise differences in the parameters, excluding the location of the connective tissue.\n\nFor the physical activity analysis, subjects were categorized into three activity level classes: \"Rare - No Activity,\" \"Occasional,\" and \"Frequent.\" A similar one-way Kruskal-Wallis test was conducted to investigate the NTRA parameters according to physical activity. This approach allowed for the identification of statistically significant differences across the activity levels.\n\nMachine learning regression models were assembled to classify subject age using NTRA parameters as initial features. The algorithms employed included Random Forests (RF) and Gradient Boosting (GB), as AdaBoosting (ADA-B) results were consistently poor. The age regression was performed by considering three distinct age groups: 66-72 years, 73-79 years, and 80-93 years. The evaluation metrics used included the Coefficient of Determination (R2), Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE). The results indicated that while the R2 values were generally weak, the error metrics provided more indicative results, particularly in predicting the age of individuals.\n\nThe classification of age and physical activity classes did not yield particularly robust results, with accuracy for age classification never exceeding 60%. However, the accuracy for physical activity classification reached 60%, with the RF model yielding the best results. Additionally, the classification of healthy and not healthy groups according to the three age divisions showed the best results in terms of accuracy, consistently reaching over 79%.\n\nThe evaluation also involved the use of the Synthetic Minority Oversampling Technique (SMOTE) to improve the quality of classification by oversampling the minority classes. This technique, combined with 10-fold cross-validation, ensured that the models were thoroughly tested and validated. Overall, the evaluation process provided a comprehensive assessment of the method's performance and its potential applications in predicting age and physical activity levels based on NTRA parameters.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate both regression and classification tasks. For regression analysis, we considered the Coefficient of Determination (R\u00b2), Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE). These metrics are widely accepted in the literature and provide a robust evaluation of the model's predictive accuracy and error distribution.\n\nFor classification tasks, we focused on Accuracy, Sensitivity, and Specificity. These metrics are crucial for understanding the model's ability to correctly identify positive and negative cases, as well as its overall correctness. Accuracy provides a general measure of the model's performance, while Sensitivity and Specificity offer insights into its performance on specific classes.\n\nThe choice of these metrics ensures that our evaluation is thorough and representative of standard practices in the field. By including both regression and classification metrics, we cover a broad range of model performance aspects, making our evaluation comprehensive and reliable. This approach aligns with established methodologies in the literature, ensuring that our results are comparable and meaningful within the scientific community.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, we focused on the diagnostic value of our Nonlinear Trimodal Regression Analysis (NTRA) methodology using longitudinal data. We suggested future work to compare the sensitivity of NTRA parameters to age and physical activity with standard CT analyses, such as total, fat, and lean muscle cross-sectional areas and/or average cross-sectional radiodensity.\n\nRegarding simpler baselines, we employed three popular tree-based algorithms: random forests (RF), AdaBoosting (ADA-B), and gradient boosting (GB). However, we did not explicitly compare these algorithms to simpler baselines. The results from ADA-B were consistently poor compared to RF and GB, leading us to report only the latter two in our analysis. We used key evaluation metrics such as Coefficient of Determination (R2), Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE) for regression, and Accuracy, Sensitivity, and Specificity for classification.\n\nWe acknowledge that future comparative analyses are necessary to fully understand the strengths and weaknesses of our NTRA methodology. This could involve comparing NTRA with other machine learning algorithms and simpler baselines on benchmark datasets to provide a more comprehensive evaluation.",
  "evaluation/confidence": "The evaluation of our study's performance metrics did not include confidence intervals. However, we employed statistical tests to assess the significance of our results. Specifically, we used the Kruskal-Wallis test to determine if there were statistically significant differences among groups based on age and physical activity levels. This test helped us identify which parameters were significantly different across the groups.\n\nFor the machine learning models, we used evaluation metrics such as the Coefficient of Determination (R\u00b2), Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE) for regression tasks. In classification tasks, we focused on Accuracy, Sensitivity, and Specificity. While these metrics provide a clear picture of model performance, they do not inherently include confidence intervals.\n\nThe results from the Kruskal-Wallis test and subsequent post-hoc analyses indicated statistically significant differences in most of the Nonlinear Trimodal Regression Analysis (NTRA) parameters across different age and physical activity groups. This suggests that our method is robust in distinguishing between these groups.\n\nHowever, the machine learning models, particularly for age and physical activity classification, did not yield highly accurate results. The accuracy for age classification never exceeded 60%, and similar results were observed for physical activity classification. This indicates that while our method shows promise, there is room for improvement in the predictive power of the models.\n\nIn summary, while our statistical analyses provide evidence of significant differences in NTRA parameters, the machine learning models' performance metrics do not include confidence intervals, and the results are not sufficiently robust to claim superiority over other methods or baselines. Future work should focus on improving the predictive accuracy of the models and possibly incorporating confidence intervals for a more comprehensive evaluation.",
  "evaluation/availability": "The raw evaluation files used in this study are not publicly available. The dataset originates from the AGES-Reykjavik study, specifically the AGES-I subset, which includes data from 3,157 healthy elderly subjects. Due to ethical and privacy considerations, this dataset cannot be made publicly accessible. However, requests for these data may be directed to the AGES-Reykjavik Study Executive Committee. Interested parties should contact Ms. Camilla Kristjansdottir at camilla@hjarta.is for further information regarding data access."
}