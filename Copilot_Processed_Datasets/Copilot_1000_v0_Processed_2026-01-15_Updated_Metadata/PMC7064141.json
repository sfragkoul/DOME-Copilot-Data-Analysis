{
  "publication/title": "Predictive models of medication non-adherence risks of patients with T2D based on multiple machine learning algorithms.",
  "publication/authors": "Wu XW, Yang HB, Yuan R, Long EW, Tong RS",
  "publication/journal": "BMJ open diabetes research & care",
  "publication/year": "2020",
  "publication/pmid": "32156739",
  "publication/pmcid": "PMC7064141",
  "publication/doi": "10.1136/bmjdrc-2019-001055",
  "publication/tags": "- Clinical Care\n- Education\n- Nutrition\n- Machine Learning\n- Predictive Modeling\n- Medication Adherence\n- Type 2 Diabetes\n- Data Analysis\n- Statistical Methods\n- Healthcare Management",
  "dataset/provenance": "The dataset used in this study was collected from a survey conducted at the outpatient clinic of Sichuan Provincial People\u2019s Hospital from April 2018 to March 2019. The hospital primarily serves patients from Sichuan Province, a populous region in southwestern China. A total of 630 patients were approached, and 401 completed the survey. The data collection process involved both electronic medical records (EMR) and face-to-face questionnaires. Clinical laboratory results were obtained from EMRs, while other information was gathered through patient interviews and, if necessary, confirmed by telephone follow-ups.\n\nThe survey questionnaire was comprehensive, covering five main parts: basic characteristics, diabetes-related information, other clinical information, exercise, diet, and mental state, and adherence details. The adherence status, determined by the medication possession ratio, was the target variable. The dataset included a variety of variables, but some were excluded due to excessive missing values or low correlation with the target variable. Ultimately, 16 variables were used for modeling, including the last HbA1c value, fasting glucose, age, diet adjustment, weight, cost of hypoglycemic drugs, duration of current treatment regimen, body mass index (BMI), working status, and more.\n\nThe dataset was subjected to rigorous data cleaning processes, including handling missing values, outliers, and imbalances. Variables with more than 50% missing values or a certain category's proportion greater than 80% were excluded. Data filling was performed using mean, median, or mode values, depending on the variable type. Outlier values were adjusted to the maximum or minimum of normalized values. Sampling methods, such as oversampling and undersampling, were employed to address the imbalance between good and non-adherence patients.\n\nThe dataset was partitioned twice for two-way cross-validation. Initially, the original data were randomly divided into two subsets in an 8:2 ratio. After data cleaning, the larger subset was further divided into a training set and a testing set in a 7:3 ratio. This partitioning ensured that the models were trained and tested on independent data, enhancing the reliability of the results.",
  "dataset/splits": "There were two main data partitioning processes conducted in this study. The first partition occurred before data cleaning, where the original dataset was randomly divided into two subsets: set 1 and set 2, in an 8:2 ratio. This means set 1 contained 80% of the data, while set 2 contained the remaining 20%. Set 1 was used for model establishment and verification, whereas set 2 served as an independent validation set.\n\nThe second partition took place after data cleaning. In this process, set 1 was further divided into a training set and a testing set in a 7:3 ratio. Consequently, the training set comprised 70% of set 1, and the testing set comprised the remaining 30%. The training set was utilized to build machine learning models, while the testing set was employed to evaluate the models' fitting effectiveness.\n\nThese partitioning strategies ensured a robust evaluation of the models' performance and generalizability.",
  "dataset/redundancy": "In our study, we employed a two-way cross-validation approach to ensure the robustness and independence of our datasets. Initially, the original data was randomly divided into two subsets, named set 1 and set 2, in an 8:2 ratio. This partitioning was conducted before any data cleaning processes. Set 1 was used for model establishment and initial verification, while set 2 served as an independent verification set to assess the model's performance on unseen data.\n\nAfter the initial partitioning, set 1 underwent a second division into a training set and a testing set in a 7:3 ratio. This second partitioning occurred after data cleaning, ensuring that the training set was used to build machine learning models, and the testing set was used to evaluate the models' fitting effectiveness.\n\nTo address the issue of sample imbalance, we utilized oversampling and undersampling techniques. Oversampling involved quadrupling the number of patients with poor compliance, while undersampling reduced the number of patients with good adherence by 50%. These methods were employed to balance the dataset and mitigate the risk of overfitting. However, to ensure the credibility of our results, we designed the data partitioning process twice. This approach produced a non-repeating set of verification samples, set 2, which was partitioned before oversampling. This design helped prevent the risk of overly optimistic estimations of the models' prediction effects, as some records might otherwise be used both to build and validate the models.\n\nThe distribution of our datasets compares favorably to previously published machine learning datasets in the medical field. By ensuring independent training and testing sets through careful partitioning and addressing sample imbalance, we aimed to enhance the generalizability and reliability of our models. The use of set 2 as the best model evaluation indicator further strengthened the credibility of our results, as it provided an unbiased assessment of the models' performance on truly unseen data.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in this study leverages ensemble learning, which is a class of machine-learning algorithms that combine multiple models to improve overall performance. This approach is not new but is widely recognized for its effectiveness in various predictive tasks.\n\nThe ensemble model used in this study was identified as the best algorithm among 30 different machine learning models developed. It utilized oversampling for data balance after data imputing and did not involve data binning. This ensemble model was constructed from five individual models, highlighting the strength of combining multiple algorithms to enhance predictive accuracy.\n\nThe decision to use an ensemble model was driven by its proven ability to handle complex datasets and improve generalization, which is crucial for reliable predictions. The ensemble approach helps mitigate the risk of overfitting by averaging the predictions of multiple models, thereby reducing the variance and increasing the robustness of the final model.\n\nThe ensemble model's performance was evaluated using metrics such as the area under the receiver operating characteristic curve (AUC) and overfitting values. The AUC values indicated the model's ability to distinguish between different classes, while the overfitting values assessed the model's generalization to new, unseen data. The ensemble model demonstrated strong performance in both training and testing datasets, confirming its effectiveness.\n\nGiven the focus of this study on clinical data and predictive modeling for healthcare outcomes, the ensemble approach was chosen for its practical applicability and reliability in real-world scenarios. The results showed that the ensemble model outperformed other individual algorithms, making it the optimal choice for this specific application.",
  "optimization/meta": "The optimization process involved developing and evaluating 30 machine learning-based modeling algorithms for adherence prediction. These algorithms were built using up to 16 variables. The best-performing algorithm, which achieved a mean AUC of 0.866\u00b10.082, was selected after examining various data filling, sampling, binning, and variable screening methods.\n\nThe meta-predictor leverages the outputs of multiple machine learning models to enhance predictive performance. The constituent methods include various algorithms evaluated during the study, such as decision trees, forward and stepwise selection, backward selection, k-nearest neighbors, and others. The specific algorithms used in the meta-predictor are denoted by symbols like $D, $S, $L, $KNN, $XF, and $R, each representing different machine learning techniques.\n\nThe training data for the meta-predictor is designed to be independent. This independence is ensured through a rigorous partitioning process where the data is divided into training and testing sets. The 'Partition' node in the data flow ensures that the training data used to build the individual models is separate from the testing data used to evaluate their performance. This separation helps in maintaining the integrity and independence of the training data, which is crucial for the reliability of the meta-predictor.",
  "optimization/encoding": "In our study, we employed a technique called statistical blindness to ensure the integrity of our analysis. All variable names were encoded as X1 through X44. This encoding was maintained throughout the data analysis process, including model evaluation. Only after the model evaluation processes were completed were the variables unblinded.\n\nData cleaning was a crucial step in our preprocessing pipeline. Variables with more than 50% missing values or where a certain category's proportion exceeded 80% were excluded. To assess the correlations between input variables and the target variable, we used the maximum likelihood ratio method. Variables with a p-value greater than 0.1 were deemed unimportant and were excluded following the likelihood ratio test.\n\nFor data filling, we used different strategies based on the type of variable. Numerical variables were filled with their mean values, ordinal variables with their median, and nominal variables with their mode. Additionally, 18 outlier values were adjusted to the maximum or minimum of normalized values.\n\nGiven the imbalance between good and non-adherence patients, we applied sampling methods to address this issue. Oversampling was used for patients with poor compliance, increasing their representation fourfold. Conversely, undersampling was applied to patients with good adherence, reducing their representation by 50%. This approach helped mitigate the imbalance and ensured that the models were not overly influenced by the majority class.\n\nTwo data partitioning processes were conducted for two-way cross-validation. The first partition occurred before data cleaning, where the original data were randomly divided into two subsets in an 8:2 ratio. The second partition involved dividing the first subset into a training set and a testing set in a 7:3 ratio after data cleaning. The training set was used to build the machine learning models, while the testing set was used to evaluate their performance.",
  "optimization/parameters": "In our study, the number of parameters used in the models varied depending on the specific modeling approach and variable selection method employed. Initially, we started with a total of 16 variables for modeling, which included factors such as the last HbA1c value, fasting glucose, age, diet adjustment, weight, cost of hypoglycemic drugs, duration of the current treatment regimen, body mass index (BMI), working status, the duration since the prior blood glucose test, dyslipidemia, and others.\n\nThe selection of these variables was based on a rigorous feature selection process. Three variables were excluded due to having too many missing values. Four variables were excluded because the proportion of certain categories was too large, and 18 variables were excluded due to low correlation with the target variable.\n\nFor variable screening, we employed different methods including forward, backward, and stepwise selection. These methods helped in identifying the most relevant variables for each model. For instance, in one of the best-performing models, an ensemble model that used oversampling for data balance after data imputing and without data binning, nine variables were used. These variables included age, gender, whether the prior fasting blood glucose was under control, duration of the current treatment regimen, diet adjustment, the daily cost of medications, fasting blood glucose value, hyperlipidemia, and BMI.\n\nThe number of variables used in each model ranged from as few as 3 to as many as 16, depending on the specific combination of imputing, sampling methods, binning, and variable screening techniques applied. This flexibility allowed us to optimize the models for better predictive performance and to mitigate overfitting risks.",
  "optimization/features": "In the optimization process, a total of 16 features were used as input for modeling. Feature selection was indeed performed to refine the dataset. Initially, several variables were excluded due to excessive missing values, an imbalanced proportion of certain categories, or a low correlation with the target variable. This careful selection ensured that only the most relevant and reliable features were included in the final models. The feature selection process was conducted using the training set only, ensuring that the testing set remained unbiased and could be used for a fair evaluation of the models' performance.",
  "optimization/fitting": "In our study, we developed 300 machine learning models using 30 different algorithms, varying factors such as imputation, sampling methods, and variable screening techniques. This approach allowed us to thoroughly explore the model space and identify the most robust predictors.\n\nThe number of variables used in our models ranged from 3 to 16, depending on the specific algorithm and screening method employed. In some cases, the number of variables was indeed larger than the number of samples, particularly when using techniques like oversampling, which increased the sample size. However, we implemented several strategies to mitigate the risk of overfitting.\n\nTo evaluate the overfitting risk, we used two metrics: OF1 and OF2. OF1 was calculated as the ratio of the AUC of the training set to the AUC of the testing set, while OF2 was the ratio of the AUC of the testing set to the AUC of an independent validation set (Set 2). Higher values of OF1 and OF2 indicate a greater risk of overfitting. The overall mean OF1 value was 1.028, which was statistically different from 1, but we believe the overfitting risks were negligible due to the small standard deviation and the Gaussian distribution of the differences. The OF2 value was 1.015, with no significant difference from 1, further supporting the low risk of overfitting.\n\nTo rule out underfitting, we ensured that our models were complex enough to capture the underlying patterns in the data. We used ensemble methods, which combine multiple models to improve predictive performance, and we varied the number of variables and samples to find the optimal balance between bias and variance. Additionally, we used techniques like oversampling and undersampling to address class imbalances, which can lead to underfitting if not properly managed.\n\nIn summary, while the number of parameters in some of our models was larger than the number of training points, we employed several strategies to mitigate the risk of overfitting and ensure that our models were not underfitting. Our evaluation metrics and validation procedures provided confidence in the robustness and generalizability of our findings.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the primary methods used was data partitioning through a two-way cross-validation process. The original dataset was initially split into two subsets, with an 8:2 ratio, named set 1 and set 2. Set 1 was further divided into a training set and a testing set in a 7:3 ratio after data cleaning. This approach helped in evaluating the models' performance on unseen data, thereby reducing the risk of overfitting.\n\nAdditionally, we utilized sampling methods to address the imbalance in the dataset. Both oversampling and undersampling techniques were applied to balance the proportions of good and non-adherence patients. Oversampling involved duplicating records of patients with poor compliance, while undersampling reduced the number of records for patients with good adherence. This balancing act helped in creating more representative models and mitigating the risk of overfitting due to class imbalance.\n\nWe also implemented variable selection methods, including forward, backward, and stepwise logistic regression, to screen significant variables. These methods helped in identifying the most relevant features, thereby simplifying the models and reducing the complexity that could lead to overfitting.\n\nFurthermore, we evaluated the overfitting risk of the models using two metrics: OF1 and OF2. OF1 was calculated as the ratio of the AUC of the training set to the AUC of the testing set of set 1, while OF2 was the ratio of the AUC of the testing set of set 1 to the AUC of set 2. Higher values of these metrics indicate a greater risk of overfitting. Despite statistical differences observed in OF1, the overall mean values were very close to 1, with small standard deviations, suggesting negligible overfitting risks across all models.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model developed in this study is not entirely a black box, as it incorporates several transparent and interpretable components. The ensemble model, which was identified as the best performing algorithm, combines the outputs of five individual models. These individual models include decision trees, logistic regression, and other algorithms that are generally considered interpretable.\n\nDecision trees, for instance, are inherently interpretable as they provide a clear, visual representation of the decision-making process. Each node in the tree represents a decision based on a specific variable, making it easy to trace the path that leads to a particular outcome. Logistic regression is another interpretable model, as it provides coefficients that indicate the strength and direction of the relationship between each predictor variable and the outcome.\n\nThe ensemble model itself, while more complex, can still offer insights into the importance of different variables. By examining the contribution of each individual model within the ensemble, one can gain an understanding of which variables are most influential in the overall prediction. Additionally, the variable selection process, which included methods like forward, backward, and stepwise selection, helps to identify the most relevant predictors.\n\nIn summary, while the ensemble model may appear complex, it is built upon interpretable components, and the variable selection process provides further transparency. This allows for a reasonable level of interpretability, making it possible to understand the key factors driving the model's predictions.",
  "model/output": "The model developed in this study is a classification model. More than a dozen classification algorithms were applied and assessed, including C 5.0, logistic regression, decision list, Bayesian network, discriminant model, KNN algorithm, LSVM, random forest, SVM, Tree- AS, CHAID, Quest, C&R Tree, Neural Net, and an ensemble model. These algorithms were used to predict medication adherence in patients with type 2 diabetes mellitus. The ensemble model summarized the output of the best five models, assessed by the area under the receiver operating characteristic curve (AUC), and generated their outputs according to the voting principle. The final selected model had an excellent predictive capability, with a mean AUC of 0.866\u00b10.082, demonstrating its effectiveness in identifying non-adherence patients. The model was validated using sensitivity, specificity, and overfitting risk, ensuring its robustness and reliability.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a rigorous two-way cross-validation process. Initially, the original data was randomly partitioned into two subsets, named set 1 and set 2, in an 8:2 ratio. This division was performed before any data cleaning to ensure an unbiased evaluation. Set 1 was further divided into a training set and a testing set in a 7:3 ratio after data cleaning. The training set was used to build machine learning models, while the testing set was used to evaluate their fitting effectiveness.\n\nTo assess the predictive performances of the models, the area under the receiver operating characteristic curve (AUC) was calculated for each dataset: the training set of set 1, the testing set of set 1, and set 2. Due to the oversampling process, which could generate duplicate records, the AUC of set 2 was considered the best model evaluation indicator. This approach helped mitigate the risk of overly optimistic estimations of the models' prediction effects.\n\nThe overfitting risks of the models were evaluated using two metrics: OF1 and OF2. OF1 was calculated as the ratio of AUC of the set 1 training set to the AUC of the set 1 testing set. OF2 was calculated as the ratio of AUC of the set 1 testing set to the AUC of set 2. Higher values of OF1 and OF2 indicated more serious overfitting risks. To ensure the robustness of the evaluation, tenfold independent repeated values of these indexes were generated by changing the random seed number in the first data partition process ten times. This comprehensive evaluation method ensured that the models were thoroughly tested for their predictive accuracy and generalizability.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness and robustness of our models. The primary metric used was the Area Under the Curve (AUC) for the Receiver Operating Characteristic curve, which is a widely accepted measure in the literature for evaluating the performance of classification models. We reported AUC for three different sets: the training set (AUC_TR), the testing set (AUC_TE), and an independent set (AUC_Set 2). This approach allowed us to assess the model's performance on both seen and unseen data, providing a comprehensive evaluation.\n\nTo address the risk of overfitting, we introduced two additional metrics: OF1 and OF2. OF1 is calculated as the ratio of AUC on the training set to AUC on the testing set, while OF2 is the ratio of AUC on the testing set to AUC on the independent set. Higher values of OF1 and OF2 indicate a greater risk of overfitting, helping us to identify models that may generalize poorly to new data.\n\nWe also considered the impact of various modeling approaches on these predictive indicators, including imputing methods, sampling techniques, binning strategies, screening methods, model algorithms, and the number of variables and samples. This thorough evaluation ensured that our models were robust and that our findings were not overly dependent on specific methodological choices.\n\nThe use of these metrics is representative of current practices in the field, as AUC is a standard measure for classification tasks, and overfitting assessment is crucial for ensuring model reliability. Our approach provides a balanced view of model performance, considering both accuracy and generalization ability.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of various modeling approaches to evaluate their impact on predictive indicators. We examined different methods for imputing missing data, sampling techniques, binning strategies, variable screening methods, and model selection. Each approach was assessed using multiple performance metrics, including AUC for training, testing, and validation sets, as well as overfitting factors.\n\nWe evaluated 30 different machine learning models, each configured with various combinations of the aforementioned methods. This allowed us to systematically compare the performance of different modeling approaches. For instance, we looked at the effects of imputing missing data versus not imputing, using undersampling or oversampling techniques, applying binning, and employing different variable screening methods like forward, backward, and stepwise selection.\n\nAdditionally, we compared the performance of simpler baseline models against more complex ensemble models. This included models like discriminant analysis, logistic regression, and support vector machines, as well as more advanced ensemble methods. The comparison was not limited to publicly available benchmark datasets but was conducted on our specific dataset, which included variables encoded for statistical blindness.\n\nThe results highlighted significant differences in performance metrics across various approaches. For example, oversampling methods generally led to abnormal distributions in all five indexes, indicating potential overfitting risks. In contrast, certain combinations of imputing, sampling, and model selection methods yielded better performance in terms of AUC and overfitting factors.\n\nOverall, our comparison provided insights into the strengths and weaknesses of different modeling approaches, helping to identify the most effective strategies for predictive modeling in our study.",
  "evaluation/confidence": "The evaluation of our models includes a comprehensive assessment of performance metrics with associated confidence intervals. For instance, the area under the receiver operating characteristic curve (AUC) values for both the training and testing sets are presented with their respective standard deviations, indicating the variability and reliability of these estimates. This approach allows for a more nuanced understanding of model performance beyond simple point estimates.\n\nStatistical significance is a critical aspect of our evaluation. We employ various statistical tests to determine if the observed differences in performance metrics are likely due to chance or if they represent genuine improvements. For example, the Kruskal-Wallis test is used to compare the distributions of AUC values across different modeling approaches, providing a robust measure of statistical significance. Additionally, general linear models for analysis of variance (ANOVA) are utilized to assess the impact of different variables on predictive indicators, further reinforcing the statistical rigor of our findings.\n\nThe variance inflation factor (VIF) is also considered to ensure the stability of our models. A high VIF indicates multicollinearity, which can make the model unstable. In cases where multicollinearity is detected, variables are eliminated, and the model is re-established to maintain robustness.\n\nOverall, the inclusion of confidence intervals and the use of statistical tests ensure that our claims of model superiority are grounded in solid statistical evidence. This meticulous approach to evaluation provides a high level of confidence in the reliability and validity of our results.",
  "evaluation/availability": "Not enough information is available."
}