{
  "publication/title": "Enhancing Top-Down Proteomics Data Analysis by Combining Deconvolution Results through a Machine Learning Strategy.",
  "publication/authors": "McIlwain SJ, Wu Z, Wetzel M, Belongia D, Jin Y, Wenger K, Ong IM, Ge Y",
  "publication/journal": "Journal of the American Society for Mass Spectrometry",
  "publication/year": "2020",
  "publication/pmid": "32223200",
  "publication/pmcid": "PMC7909725",
  "publication/doi": "10.1021/jasms.0c00035",
  "publication/tags": "- Mass spectrometry\n- Deconvolution algorithms\n- Machine learning\n- Random forest\n- Precision-recall curves\n- Expert annotation\n- Cluster filtering\n- Database searching\n- Protein sequences\n- Spectral analysis",
  "dataset/provenance": "The dataset utilized in this study comprises collision-induced dissociation (CID) and electron-capture dissociation (ECD) tandem mass spectrometry (MS/MS) spectra from 15 sarcomeric protein proteoforms. These spectra were previously published and are publicly available. The protein identification, accession numbers, and post-translational modifications are detailed in supplementary Table S2.\n\nThe mass spectrometry proteomics raw data and annotations have been deposited in the ProteomeXchange Consortium via the PRIDE partner repository. The dataset identifier is PXD018043. This repository ensures that the data is accessible to the scientific community for further analysis and validation.\n\nThe MS/MS data were collected using a 12 Tesla solariX Fourier transform ion cyclotron resonance (FTICR) mass spectrometer equipped with an automated chip-based nano-electrospray ionization source. This high-resolution instrument was crucial for obtaining detailed and accurate spectra.\n\nThe dataset includes a variety of proteoforms, providing a comprehensive view of the sarcomeric proteins. The spectra were processed using multiple deconvolution algorithms, including THRASH, MS-Deconv, TopFD, and the SNAP algorithm from Bruker DataAnalysis. These algorithms were employed to extract deconvoluted peaks, which were then analyzed using machine learning strategies to improve proteoform identification.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The mass spectrometry proteomics raw data and annotations have been deposited to the ProteomeXchange Consortium via the PRIDE partner repository. The dataset identifier is PXD018043. This ensures that the data is publicly available and can be accessed by other researchers for verification and further studies. The deposition to PRIDE, a well-established repository, enforces standard practices for data sharing and accessibility, making it easier for the scientific community to utilize the data. The data is made available under the terms that allow for its use in research, subject to proper citation and acknowledgment of the original work. This approach promotes transparency and reproducibility in scientific research.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the random forest model. This is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n\nThe random forest algorithm is not new; it has been widely used and studied in various fields due to its robustness and ability to handle large datasets. It is particularly effective in classification tasks, which made it suitable for our needs.\n\nThe reason it was not published in a machine-learning journal is that our primary focus was on applying this well-established algorithm to a specific problem in mass spectrometry data analysis. Our work centered on demonstrating how random forests could improve the accuracy of peak clustering and deconvolution in mass spectrometry, rather than developing a new machine-learning algorithm. The application and validation of existing algorithms in new domains are crucial for advancing scientific research, and this is why we chose to publish our findings in a journal focused on mass spectrometry.",
  "optimization/meta": "The model described in this publication employs a meta-predictor approach, specifically utilizing a random forest machine learning algorithm. This meta-predictor integrates the results from multiple individual deconvolution algorithms, including THRASH, MS-Deconv, TopFD, and SNAP. The random forest model leverages the votes from these algorithms as features, allowing it to learn the confidence in each algorithm's predictions and determine an optimal score for identifying true positive clusters.\n\nThe random forest model is trained using a probability threshold that optimizes the F1 score on the training spectra, which is then applied to the corresponding test set. This process ensures that the training data is independent of the test data, maintaining the integrity of the model's performance evaluation.\n\nThe use of a majority vote (2 or more votes) from the individual deconvolution algorithms also plays a significant role. This voting method helps to reduce false positives by filtering out peaks that are not consistently identified by multiple algorithms. The random forest model further refines this process by considering the spectral features and characteristics of the clusters, such as average mass, cluster charge, and average intensity, which are crucial for accurate identification.\n\nIn summary, the meta-predictor approach combines the strengths of multiple deconvolution algorithms and a random forest machine learning model to enhance the accuracy and confidence in proteoform identification. The independence of the training data from the test data is clearly maintained, ensuring reliable performance metrics.",
  "optimization/encoding": "For the machine-learning algorithm, data encoding and preprocessing involved several key steps. Initially, each MSAlign file was parsed using a Python script, and the results were concatenated into a single peak list. This list recorded essential features such as the monoisotopic mass, charge, and the source algorithm for each peak.\n\nPeaks with the same charge and similar monoisotopic mass were clustered together, forming groups that represented the same peak. This clustering process utilized hierarchical clustering, with the distance metric based on the log10 transformed monoisotopic mass. This transformation helped to remove the linear dependence of the error with mass, allowing for a constant cutoff to determine the number of clusters. Additionally, a constraint ensured that the charges within each cluster were the same.\n\nFor each cluster, a feature vector was generated using various features. These features included the activation type used to generate spectra, the charge of the peaks, the number of deconvolution algorithms that identified a peak within the cluster, the sum and average intensity of the peaks, and boolean indicators for whether specific algorithms called the peak. Other features included the average mass, standard deviation of the mass, precursor charge, precursor mass, and precursor m/z.\n\nExpert annotations were obtained and verified manually using the MASH software with the embedded enhanced-THRASH algorithm at a 60% fit setting. These annotations were used to label clusters as either expert-matched or unmatched. The identified clusters were annotated by finding those that matched the expert annotations in terms of charge and monoisotopic mass within a specified ppm window.\n\nThe machine learning analysis was performed using the R language. The task involved separating expert-annotated matched clusters from unmatched clusters. Precision-recall curves were estimated using leave-one-spectrum-out cross-validation, where each fold estimated the probabilities of a true annotation for each cluster. This approach ensured that the model was robust and could generalize well to new data.",
  "optimization/parameters": "In our study, we utilized a random forest model, which is an ensemble of decision trees. The number of parameters, p, in the model is not explicitly stated as a fixed number, as random forests can handle a large number of input features. However, we did perform feature selection to identify the most important features.\n\nTo determine the optimal set of features, we employed backward selection with leave-one-spectrum-out cross-validation. This process iteratively removes features to optimize model performance, specifically the median F1 score. Through this method, we found that five features\u2014Charge, Precursor Charge, THRASH 60% Fit, AvgMass, and SumIntensity\u2014were sufficient to achieve performance comparable to a model built with all features. These five features were identified as crucial for the model's performance, and omitting any one of them resulted in a significant decrease in the median F1 score.\n\nAdditionally, we considered the correlation among features. For instance, features like AvgIntensity, Votes, and SumIntensity are correlated by definition. Therefore, removing two of these correlated features was deemed sufficient for the discriminatory models.\n\nThe selection of these parameters was driven by the need to balance model complexity and performance, ensuring that the most relevant features were retained while minimizing the risk of overfitting. This approach allowed us to build a robust model that effectively filters out false positives and improves the identification of true expert annotations.",
  "optimization/features": "In our study, we utilized a comprehensive set of features to train our machine learning models. The total number of features (f) used as input was not explicitly stated, but it is clear that multiple features were considered. These features included various spectral characteristics and cluster features, such as the average mass of the cluster (AvgMass), the cluster charge (Charge), and the average intensity (AvgIntensity). Additionally, features describing the spectrum characteristics, such as activation type, precursor mass, and precursor charge, were also included.\n\nFeature selection was indeed performed to optimize model performance. We employed backward selection, which iteratively removes features to determine their importance. This process was conducted using leave-one-spectrum-out cross-validation and optimized on the median F1 score. Through this method, we identified that five key features\u2014Charge, Precursor Charge, THRASH 60% Fit, AvgMass, and SumIntensity\u2014were sufficient to achieve the same performance as a model built from all the features. Omitting any of these five features resulted in a significant decrease in the median F1 score.\n\nIt is important to note that the feature selection process was performed using the training set only, ensuring that the model's performance on the test set remained unbiased. This approach helped us to identify the most relevant features for our predictive models, enhancing their accuracy and reliability.",
  "optimization/fitting": "The fitting method employed in this study utilized a random forest model, which is an ensemble learning method capable of handling a large number of input features without requiring extensive tuning. The number of features used in the model was substantial, but the random forest's ability to manage high-dimensional data helped mitigate the risk of overfitting.\n\nTo ensure that overfitting was not an issue, several strategies were implemented. Firstly, leave-one-spectrum-out cross-validation was used, which is a rigorous form of cross-validation that helps in assessing the model's performance on unseen data. This method involves training the model on all but one spectrum and testing it on the left-out spectrum, repeating this process for each spectrum in the dataset. Additionally, the model's performance was optimized using the median F1 score, which balances both precision and recall, providing a more robust measure of the model's effectiveness.\n\nUnderfitting was addressed by ensuring that the model had enough complexity to capture the underlying patterns in the data. The random forest model, with its ensemble of decision trees, is inherently capable of capturing complex relationships. Furthermore, the inclusion of multiple deconvolution algorithms and features such as spectrum characteristics and cluster features helped in providing a comprehensive set of inputs to the model, reducing the risk of underfitting.\n\nThe use of a 7-vote ensemble, which includes results from multiple deconvolution algorithms with different fit scores, also contributed to the model's robustness. This approach increased the diversity of the input data, helping the model to generalize better and avoid both overfitting and underfitting. The performance improvements observed with the 7-vote ensemble, as compared to the 4-vote ensemble, further validate the effectiveness of this approach.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method was backward selection, which iteratively removes features to optimize model performance. This process helped us identify the most important features, such as Charge, Precursor Charge, THRASH 60% Fit, AvgMass, and SumIntensity, which were crucial for maintaining high performance. By omitting any of these five features, we observed a significant decrease in the median F1 score, underscoring their importance.\n\nAdditionally, we used leave-one-spectrum-out cross-validation to evaluate the model's performance. This technique ensures that each spectrum is used once as a test set while the model is trained on the remaining spectra. This approach helps in assessing the model's generalization capability and prevents overfitting to specific spectra.\n\nFurthermore, we utilized the random forest algorithm, which inherently provides regularization through the ensemble of decision trees. The random forest model's ability to handle large datasets and exhibit excellent performance in classification tasks was particularly beneficial. The model's feature importance values, such as Mean Decrease Accuracy, helped us understand which features were most significant in reducing model accuracy when permuted, thereby aiding in feature selection and preventing overfitting.\n\nIn summary, our use of backward selection, leave-one-spectrum-out cross-validation, and the random forest algorithm collectively contributed to effective regularization and prevention of overfitting in our models.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, we detailed the choice of the ppm cutoff for clustering, which was determined by evaluating multiple cutoff levels (1 ppm, 2 ppm, 5 ppm, 10 ppm, 20 ppm, 50 ppm, 100 ppm, and 200 ppm). The optimal cutoff was found to be 10 ppm based on several metrics, including the number of clusters, the percent of peaks assigned, cross-validated accuracy from the random forest model, and the F1 score.\n\nThe random forest model parameters, such as the number of trees (ntree) set to 100, are also mentioned. Additionally, the features used in the model, including Charge, Precursor Charge, THRASH 60% Fit, AvgMass, and SumIntensity, were identified through a backward selection process.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly provided in the publication. The methods and results are described in detail, but the actual files or scripts used for the optimization are not shared. The publication focuses on the methodological approach and the outcomes rather than providing direct access to the computational resources used.\n\nFor those interested in replicating or building upon our work, the detailed descriptions of the methods and parameters should serve as a comprehensive guide. However, specific model files and optimization scripts would need to be developed independently based on the information provided.",
  "model/interpretability": "The model employed in this study is not a black box; it offers interpretability through several means. One of the key advantages of using a random forest model is its ability to provide insights into feature importance. This is achieved through metrics such as Mean Decrease Accuracy, which estimates the reduction in the model's accuracy when the values of a particular feature are permuted. Features that significantly reduce the model's accuracy when permuted are considered more important.\n\nFor instance, cluster features like the average mass of the cluster, cluster charge, and average intensity were found to have the most substantial impact on the model's performance. This indicates that the random forest model is learning spectral features such as charge and mass ranges that are crucial for identifying true positive clusters. Additionally, characteristics of the spectrum, such as activation type, precursor mass, and precursor charge, also played a significant role in the model's performance.\n\nThe process of backward selection further enhances the model's interpretability. By iteratively removing features and optimizing the model's performance, it was determined that features like Charge, Precursor Charge, THRASH 60% Fit, AvgMass, and SumIntensity are essential for achieving optimal performance. This method not only identifies the most important features but also shows that omitting any of these five features results in a significant decrease in the model's median F1 score.\n\nMoreover, the random forest model's ability to learn the confidence in each deconvolution algorithm provides an additional layer of interpretability. By incorporating the votes of each deconvolution algorithm, the model can determine an optimal score, thereby improving the overall accuracy and reliability of the predictions. This approach allows for a more nuanced understanding of how different algorithms contribute to the final output, making the model more transparent and interpretable.",
  "model/output": "The model discussed in this publication is a classification model. It is specifically designed to identify cluster peaks in spectra that match expert annotations. The model uses a random forest algorithm to predict whether a cluster peak is a true positive, meaning it has an expert annotation, or not. The performance of the model is evaluated using metrics such as recall, precision, and the F1 score, which are all relevant to classification tasks. Recall measures the percentage of expert annotations found by the algorithm, while precision measures the percentage of cluster peak calls that have true annotations. The F1 score balances between precision and recall, providing a single metric that summarizes the model's performance.\n\nThe model's output is a probability of a true expert assignment for each cluster. This probability is then thresholded to make the final call on whether a cluster peak is a true positive or not. The threshold is selected to maximize the F1 score within the training dataset. The model's performance is visualized using precision-recall curves, which are commonly used in classification tasks to show the trade-off between precision and recall at different threshold levels.\n\nThe model's output is also used to filter out false positive clusters. Two avenues of filtering were explored: a simple voting heuristic that thresholds clusters based upon the number of deconvolution algorithms that called the peaks within the cluster, and applying the machine learning models to assign a probability of a true expert assignment to each cluster. The clusters were then filtered via thresholding upon this probability. The consensus results were output as an MSAlign file, which were processed using database search.\n\nThe model's performance was compared to individual deconvolution algorithms and a simple voting method. The random forest model achieved superior performance, with an average recall of 0.49, a precision score of 0.69, and an F1 score of 0.55. This suggests that the random forest algorithm is effective for identifying clusters which are true expert annotations. The model's ability to extract feature importance values also provides insights into the spectral features that contribute to a true positive cluster.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the machine learning strategy discussed in this publication is not explicitly mentioned as being publicly released. However, the strategy is integrated into a developing software tool called MASH Explorer, which is designed for analyzing high-resolution top-down mass spectrometry data. This tool is described as comprehensive and user-friendly, suggesting it may be made available to users in the future.\n\nFor running the algorithm, the publication mentions the use of specific software tools and programming languages. Python (version 2.7.10) was used to generate clusters, and R (version 3.6.0) was used for machine learning analysis and automating searches with MS-Align+. These details indicate the technical environment in which the algorithm operates, but they do not provide information on a publicly accessible method to run the algorithm independently.\n\nRegarding the availability of executables, web servers, virtual machines, or container instances, there is no information provided in the publication about such resources being publicly released. The focus is primarily on the methodology and the integration of the machine learning strategy into MASH Explorer.\n\nIn summary, while the machine learning strategy and the associated software tools are well-documented, there is no clear indication that the source code or a method to run the algorithm independently is publicly available. The development of MASH Explorer suggests potential future availability, but specific details on access and licensing are not provided.",
  "evaluation/method": "The evaluation of the methods involved several key steps and metrics to ensure robust performance assessment. Precision and recall were the primary metrics used, defined as the ratio of true positives to the sum of true positives and false negatives (recall), and the ratio of true positives to the sum of true positives and false positives (precision). These metrics were crucial for evaluating the performance of different deconvolution methods and machine learning models.\n\nPrecision-recall curves were utilized to visualize the accuracy of the methods, particularly due to the high rate of false positives in the datasets. These curves provided a clear comparison of how well each method performed in terms of recall and precision. The F1 score, which balances precision and recall, was also calculated to provide a single metric for evaluating the overall performance of the methods.\n\nThe random forest model was evaluated using cross-validation, specifically leave-one-spectrum-out cross-validation. This approach involved training the model on a subset of the data and testing it on the remaining spectrum, ensuring that the model's performance was assessed on unseen data. The probability threshold that maximized the F1 score within the training dataset was selected to make the final call on the associated test set.\n\nIn addition to the random forest model, a simple voting heuristic was employed to filter clusters based on the number of deconvolution algorithms that called the peaks within the cluster. This method provided a straightforward way to reduce false positives and improve the accuracy of the peak calls.\n\nThe performance of the individual deconvolution algorithms, the simple voting method, and the random forest machine learning algorithm was summarized and compared. The random forest model demonstrated superior performance, achieving higher precision and recall compared to most of the individual deconvolution methods. The model's ability to handle large datasets and exhibit excellent performance in classification tasks was highlighted, making it a robust choice for the evaluation.\n\nFeature importance values were extracted from the random forest model to identify the most significant features contributing to the model's performance. Metrics such as Mean Decrease Accuracy were used to estimate the reduction in accuracy upon permuting the values of each feature, providing insights into the spectral features that contributed to true positive clusters.\n\nOverall, the evaluation method involved a comprehensive assessment of precision, recall, and F1 score, utilizing cross-validation and feature importance analysis to ensure the robustness and accuracy of the deconvolution methods and machine learning models.",
  "evaluation/measure": "In our evaluation, we focused on several key performance metrics to assess the effectiveness of our deconvolution methods. The primary metrics reported are recall, precision, and the F1 score.\n\nRecall measures the percentage of expert annotations that the algorithm successfully identifies. It is calculated as the ratio of true positives to the sum of true positives and false negatives. Precision, on the other hand, evaluates the accuracy of the cluster peak calls that have true annotations. It is determined by the ratio of true positives to the sum of true positives and false positives. The F1 score balances precision and recall, providing a single metric that reflects both the algorithm's ability to find relevant instances and its accuracy in doing so. It is calculated as the harmonic mean of precision and recall.\n\nThese metrics are widely used in the literature for evaluating the performance of machine learning models, particularly in tasks involving classification and annotation. By reporting recall, precision, and the F1 score, we ensure that our evaluation is comprehensive and comparable to other studies in the field.\n\nAdditionally, we used precision-recall curves to visualize the performance of the methods. These curves provide a detailed view of how precision and recall trade off as the decision threshold varies. A superior performing classification algorithm would have a curve that is higher in both precision and recall, indicating better overall performance.\n\nTo further validate our approach, we compared the aggregate predictive performance of individual deconvolution algorithms, a simple voting method, and a random forest machine learning algorithm. The random forest model, in particular, demonstrated superior performance, achieving an average recall of 0.49, a precision score of 0.69, and an F1 score of 0.55. This comparison highlights the effectiveness of our machine learning approach in identifying clusters that match expert annotations.\n\nIn summary, the performance metrics reported in our evaluation are representative of the standards in the literature. They provide a thorough assessment of the algorithms' ability to accurately identify and annotate clusters in mass spectrometry data.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the performance of various deconvolution methods, including individual algorithms, a simple voting ensemble, and a random forest machine learning approach. To ensure a comprehensive comparison, we utilized precision-recall curves and F1 scores, which are robust metrics for assessing performance, especially in the presence of imbalanced datasets.\n\nWe compared our methods against several publicly available deconvolution algorithms, such as SNAP, TopFD, and THRASH. These algorithms are widely used in the field and served as strong baselines for our evaluations. Additionally, we included a simple voting ensemble method, which aggregates the results of multiple deconvolution algorithms based on majority votes. This approach provided a straightforward baseline that does not rely on complex machine learning models.\n\nThe random forest algorithm demonstrated superior performance compared to the individual deconvolution methods and the simple voting ensemble. It achieved higher precision and recall, as well as a better F1 score, indicating a more balanced trade-off between these two metrics. The random forest's ability to handle large datasets and its robustness in classification tasks contributed to its excellent performance.\n\nFurthermore, we explored the impact of different clustering parameters, such as the ppm cutoff, on the performance of these methods. We found that a 10 ppm cutoff was optimal for our dataset, balancing the number of clusters, the percentage of peaks assigned, and the overall accuracy measured by the F1 score.\n\nIn summary, our evaluation included comparisons with publicly available methods and simpler baselines, providing a thorough assessment of the performance of our deconvolution approaches. The random forest algorithm emerged as the most effective method, outperforming individual deconvolution algorithms and the simple voting ensemble.",
  "evaluation/confidence": "The evaluation of our methods includes a thorough assessment of confidence in the results. We utilized precision-recall curves to visualize the accuracy of the methods, which is particularly useful due to the high rate of false positives within the datasets. These curves provide a clear indication of how well each method performs in terms of recall and precision.\n\nTo ensure the robustness of our findings, we employed cross-validation techniques. For the random forest model, we determined the probability threshold that optimizes the F1 score within the training dataset and applied it to the associated test set. This process helps in assessing the generalizability of our model and ensures that the results are not overfitted to the training data.\n\nWe also calculated aggregate precision and recall scores using a probability threshold cutoff that optimizes the F1 score on the training spectra and applied it to the corresponding test set. This approach provides a reference for the random forest method and demonstrates its superiority over most other methods.\n\nStatistical significance was evaluated by comparing the performance metrics across different deconvolution methods. For instance, the random forest model achieved an average recall of 0.49, a precision score of 0.69, and an F1 score of 0.55. In comparison, the best algorithm by F1 score, THRASH of 60% fit, achieved a recall of 0.76 and precision of 0.18, with an F1 score of 0.30. These comparisons highlight the superior performance of the random forest model.\n\nAdditionally, we performed backward selection with leave-one-spectrum-out cross-validation to determine feature importance. This process involved iteratively removing features to optimize model performance, ensuring that the most significant features were retained. The results showed that certain features, such as Charge, Precursor Charge, THRASH 60% Fit, AvgMass, and SumIntensity, were crucial for achieving the same performance as a model built from all features.\n\nIn summary, our evaluation includes confidence intervals and statistical significance checks to validate the superiority of our methods. The use of cross-validation, precision-recall curves, and feature importance analysis provides a comprehensive assessment of the confidence in our results.",
  "evaluation/availability": "The raw mass spectrometry proteomics data and annotations used in this study have been made publicly available. They can be accessed through the ProteomeXchange Consortium via the PRIDE partner repository. The dataset identifier for this specific study is PXD018043. This repository allows researchers to download and use the data for their own analyses, subject to the terms and conditions specified by the ProteomeXchange Consortium. The data includes collision-induced dissociation (CID) and electron-capture dissociation (ECD) tandem MS (MS/MS) spectra from 15 sarcomeric protein proteoforms, along with the associated protein identification, accession numbers, and post-translational modifications. This public release aims to facilitate further research and validation of the methods and findings presented in this work."
}