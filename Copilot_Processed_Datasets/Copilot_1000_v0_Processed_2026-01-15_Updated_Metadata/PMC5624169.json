{
  "publication/title": "Can a Smartphone Diagnose Parkinson Disease? A Deep Neural Network Method and Telediagnosis System Implementation.",
  "publication/authors": "Zhang YN",
  "publication/journal": "Parkinson's disease",
  "publication/year": "2017",
  "publication/pmid": "29075547",
  "publication/pmcid": "PMC5624169",
  "publication/doi": "10.1155/2017/6209703",
  "publication/tags": "- Parkinson\u2019s Disease\n- Speech Analysis\n- Machine Learning\n- Telemedicine\n- Classification Accuracy\n- Sensitivity\n- Specificity\n- F-score\n- Matthews Correlation Coefficient\n- K-Nearest Neighbors\n- Stacked Autoencoders\n- Deep Neural Networks\n- Telediagnosis\n- Smartphone Diagnostics\n- Browser/Server Architecture",
  "dataset/provenance": "The datasets used in our experiments originate from two distinct sources. The first dataset, referred to as the \"Oxford Dataset,\" was created by Little et al. at the University of Oxford in collaboration with the National Centre for Voice and Speech in Denver, Colorado. This dataset comprises voice measurements from 31 individuals, of whom 23 were diagnosed with Parkinson's Disease (PD). In total, there are 195 samples, with 147 samples from PD patients and 48 from healthy individuals. The dataset includes 22 features, all of which are real-type values with no missing data.\n\nThe second dataset, known as the \"Istanbul Dataset,\" was developed at Istanbul University. This dataset includes voice recordings from 20 PD patients and 20 healthy individuals, with a balanced gender distribution. The participants were asked to produce sustained vowels, numbers, words, and short sentences, resulting in 26 different voice samples per individual. From these recordings, 26 linear and time-frequency-based features were extracted. The dataset is divided into a training set with 1040 recordings and a testing set with 168 recordings. This dataset can be used as an independent test set to validate results obtained from the training set.\n\nBoth datasets have been utilized in previous research. The Oxford Dataset was used by Ma et al. in their study, where they compared their method with 15 other research methods. The Istanbul Dataset was used to validate the results obtained from the training set, as declared by the researchers at Istanbul University. These datasets provide a robust foundation for comparing various classifiers and evaluating their performance in diagnosing Parkinson's Disease.",
  "dataset/splits": "In our study, we utilized two distinct datasets: the Oxford Dataset and the Istanbul Dataset. For the Oxford Dataset, we employed a 50-50% training-testing split, following the methodology used by Polat and Daliri. This approach involved dividing the dataset into two equal parts, each containing 97.5 samples. The dataset comprises a total of 195 samples, with 147 samples from individuals diagnosed with Parkinson's Disease (PD) and 48 samples from healthy individuals. There are no missing values in the dataset, and each feature is a real-type value.\n\nThe Istanbul Dataset, on the other hand, consists of predefined training and testing files. The training data includes recordings from 20 PD patients and 20 healthy individuals, resulting in a total of 1040 recordings. The testing file contains 168 recordings from 28 PD patients who were asked to say the sustained vowels \u201ca\u201d and \u201co\u201d three times. This dataset was used to validate the results obtained from the training set, providing an independent test set for our experiments.",
  "dataset/redundancy": "The datasets used in our experiments were split into training and testing sets. For the Oxford Dataset, we employed a 50-50% training-testing split, following the methods used by Polat and Daliri. This approach tests comparative methods with less training data than traditional 10-fold cross-validation. Each classifier was tested 10 times to ensure robustness and reliability of the results.\n\nThe Istanbul Dataset, on the other hand, consisted of predefined training and testing files provided by Istanbul University. The training data included recordings from 20 people with Parkinson's Disease (PD) and 20 healthy individuals, while the testing data consisted of recordings from 28 PD patients. This setup ensured that the training and testing sets were independent, with no overlap in the subjects used for training and testing.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets for PD diagnosis. The Oxford Dataset included 195 samples, with 147 PD samples and 48 healthy samples, ensuring a comprehensive representation of both classes. The Istanbul Dataset, with its predefined split, provided a rigorous test of the classifiers' ability to generalize from the training data to unseen test data. This independence was enforced by using distinct sets of subjects for training and testing, thereby avoiding any potential bias that could arise from overlapping data.",
  "dataset/availability": "The datasets used in our experiments are publicly available and can be accessed for further research. The \"Istanbul Dataset\" was collected at Istanbul University, where participants were asked to sustain vowels \"a\" and \"o\" three times. This dataset is available for use as an independent test set to validate results obtained on the training set. The training file contains 1040 recordings, and the testing file contains 168 recordings. The dataset includes 26 linear and time-frequency based features extracted from each voice sample.\n\nThe \"Oxford Dataset\" was created by Little et al. of the University of Oxford, in collaboration with the National Centre for Voice and Speech, Denver, Colorado. This dataset consists of voice measurements from 31 people, with 23 diagnosed with Parkinson's Disease (PD). It includes a total of 195 samples, comparing 147 PD and 48 healthy samples. There are no missing values in the dataset, and each feature is a real-type value. The dataset contains 22 features in total.\n\nBoth datasets are available for public use, allowing researchers to replicate and build upon our findings. The specific licensing details and access methods for these datasets can be found in the respective publications and repositories where they are hosted. The availability of these datasets ensures that the research community can verify our results and explore new methodologies for diagnosing Parkinson's Disease using speech analysis.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and include kernel extreme learning machine (KELM), linear support vector machine (LSVM), multilayer perceptrons support vector machine (MSVM), radial basis function support vector machine (RSVM), classification and regression tree (CART), k-nearest neighbors (KNN), linear discriminant analysis (LDA), and naive Bayesian (NB) method. These algorithms are commonly used in the field of machine learning and have been extensively studied and applied in various domains.\n\nThe algorithms employed are not new; they are standard classifiers that have been optimized using grid search methods. The focus of this work is on applying these algorithms to the specific problem of Parkinson's disease diagnosis using speech data, rather than introducing a novel machine-learning algorithm.\n\nThe reason these algorithms were not published in a machine-learning journal is that the primary contribution of this study lies in the application and comparison of these algorithms on a specific dataset for Parkinson's disease diagnosis. The research highlights the performance of these classifiers in the context of speech data from Parkinson's patients, which is a specialized application rather than a contribution to the development of new machine-learning techniques. The study aims to demonstrate the effectiveness of these algorithms in a real-world medical diagnosis scenario, which is more aligned with biomedical engineering or medical informatics journals.",
  "optimization/meta": "The subsection \"Meta-predictor\" within the \"Optimization\" section is not applicable. The provided information does not discuss a meta-predictor model. Instead, it focuses on the performance of various classifiers, both with and without the use of Stacked Autoencoders (SAE), on two datasets: the Istanbul Dataset and the Oxford Dataset. The classifiers mentioned include Kernel Extreme Learning Machine (KELM), Linear Support Vector Machine (LSVM), Multilayer Perceptrons Support Vector Machine (MSVM), Radial Basis Function Support Vector Machine (RSVM), Classification and Regression Tree (CART), K-Nearest Neighbors (KNN), Linear Discriminant Analysis (LDA), and Naive Bayesian (NB). The results are presented in terms of classification accuracy and other performance indexes such as sensitivity, specificity, F-score, and Matthews Correlation Coefficient (MCC). The experiments involve comparing the performance of these classifiers with and without the use of SAE, and the influence of different SAE configurations on classification accuracy.",
  "optimization/encoding": "In our study, we utilized two distinct datasets for training and testing our machine learning models: the Oxford Dataset and the Istanbul Dataset. The Oxford Dataset, collected by Little et al., consists of voice measurements from 31 individuals, with 23 diagnosed with Parkinson's Disease (PD). This dataset comprises 195 samples, including 147 PD samples and 48 healthy samples, with no missing values and all features being real-type values. The Istanbul Dataset, developed by Istanbul University, includes recordings from 20 PD patients and 20 healthy individuals, with multiple types of sound recordings and 26 extracted features per voice sample.\n\nFor the Oxford Dataset, we employed a 50-50% training-testing split, following the methodology of Polat and Daliri. This approach allowed us to test comparative methods with less training data than traditional 10-fold cross-validation. Each classifier was tested 10 times to ensure robustness and reliability of the results.\n\nThe Istanbul Dataset provided fixed training and testing sets, with the training file containing 1040 recordings and the testing file containing 168 recordings. This dataset was used to validate the results obtained from the training set, as declared by the researchers at Istanbul University.\n\nIn terms of data encoding and preprocessing, we extracted 26 linear and time-frequency based features from each voice sample in both datasets. These features were used as input for our machine learning algorithms. For dimensionality reduction and feature learning, we employed stacked autoencoders (SAE) with two hidden layers. The size of the first hidden layer was set to 10, 9, or 8 neurons, while the second hidden layer was set to 8, 7, or 6 neurons. The batch size for SAE training and fine-tuning was 20.\n\nAfter the pretraining stage, the entire SAE was fine-tuned based on a predefined objective. The last hidden layer of the SAE encoder could then be used in conjunction with other applications, such as Support Vector Machines (SVM) for classification tasks. This approach allowed us to improve the classification performance of various classifiers, including KNN, KELM, LSVM, MSVM, RSVM, CART, LDA, and NB, by reducing the dimensionality of the input data and learning more relevant features.",
  "optimization/parameters": "In our study, we utilized a stacked autoencoder (SAE) with two hidden layers for dimensionality reduction and feature learning. The first hidden layer's size was set to either 10, 9, or 8 neurons, while the second hidden layer's size was set to 8, 7, or 6 neurons. This configuration resulted in a total of nine different parameter settings for the SAE, which were systematically evaluated to determine the optimal architecture for our classification tasks.\n\nThe selection of these parameters was based on a subgrid search approach, where we tested various combinations of neurons in the two hidden layers. This method allowed us to explore the impact of different dimensionality reductions on the classification performance. The batch size for training and fine-tuning the SAE was consistently set to 20 across all experiments.\n\nFor the classification tasks, we employed several comparative classifiers, including kernel extreme learning machine (KELM), linear support vector machine (LSVM), multilayer perceptrons support vector machine (MSVM), radial basis function support vector machine (RSVM), classification and regression tree (CART), k-nearest neighbors (KNN), linear discriminant analysis (LDA), and naive Bayesian (NB) method. These classifiers were optimized using a grid search method to ensure robust performance.\n\nIn summary, the model parameters were carefully selected and optimized through a combination of grid search and subgrid search methods, focusing on the architecture of the SAE and the settings of the comparative classifiers. This systematic approach enabled us to identify the most effective configurations for accurately classifying Parkinson's disease based on voice measurements.",
  "optimization/features": "In the study, two distinct datasets were utilized, each with its own set of input features.\n\nFor the Oxford Dataset, a total of 22 features were used as input. These features are time-frequency features extracted from voice measurements. No explicit mention of feature selection being performed on this dataset is provided.\n\nFor the Istanbul Dataset, 26 linear and time-frequency based features were extracted from each voice sample. Similar to the Oxford Dataset, there is no explicit mention of feature selection being performed on this dataset.\n\nIn both cases, the features are real-type values, and there are no missing values in the datasets. The features were used directly for the classification tasks without any indication of feature selection processes.",
  "optimization/fitting": "The fitting method employed in this study involves the use of stacked autoencoders (SAE) for dimensionality reduction and feature learning, followed by classification using various machine learning algorithms. The SAE consists of multiple layers of autoencoders, each with a specific number of neurons in the hidden layers. The sizes of these hidden layers were systematically varied to optimize performance.\n\nThe number of parameters in the SAE is indeed larger than the number of training points, which could potentially lead to overfitting. To mitigate this risk, several strategies were implemented. Firstly, the SAE was trained using a batch size of 20, which helps in generalizing the model by reducing the variance in the training process. Secondly, L2 regularization was applied during the training of the SAE to penalize large weights, thereby preventing the model from becoming too complex and overfitting the training data. Additionally, the model was fine-tuned based on a predefined objective, which further helped in improving generalization.\n\nTo rule out underfitting, the performance of the SAE was evaluated using a 50-50% training-testing split, which is a more stringent test compared to 10-fold cross-validation. Each classifier was tested 10 times, and the results were averaged to ensure robustness. The use of a grid search method for optimizing the parameters of the comparative classifiers also helped in finding the best hyperparameters, reducing the risk of underfitting.\n\nThe comparative results showed that the proposed method achieved better results than other methods, even with a relatively fewer training samples. This indicates that the model is neither overfitting nor underfitting the data. The performance of the SAE and other classifiers was robust to the number of hidden neurons, further confirming the effectiveness of the fitting method.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the primary methods used was the Stacked Autoencoder (SAE) with two hidden layers. The SAE was configured with varying neuron sizes in the hidden layers, specifically 10, 9, or 8 neurons in the first layer and 8, 7, or 6 neurons in the second layer. This dimensionality reduction technique helped in capturing the most relevant features from the input data, thereby reducing the complexity and preventing the model from memorizing the training data.\n\nAdditionally, we utilized a batch size of 20 during the training and fine-tuning steps of the SAE. This batch processing approach helped in stabilizing the training process and reducing the risk of overfitting by ensuring that the model generalizes better to unseen data.\n\nFor the comparative classifiers, we optimized their performance using a grid search method. This systematic approach involved exhaustively searching through a specified subset of hyperparameters to find the optimal combination that maximizes classification accuracy. By doing so, we ensured that each classifier was fine-tuned to perform at its best without overfitting to the training data.\n\nFurthermore, we employed a 50-50% training-testing split method, which is a simpler but effective way to evaluate model performance compared to more complex techniques like 10-fold cross-validation. This split method allowed us to test our comparative approaches with a balanced amount of training data, ensuring that the models were not overfitted to a specific subset of the data.\n\nIn summary, our study incorporated dimensionality reduction through SAE, batch processing, hyperparameter optimization via grid search, and a balanced training-testing split to prevent overfitting and enhance the generalization capability of our models.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are not explicitly detailed in the provided information. However, the performance metrics and results of various classifiers with different configurations of Stacked Autoencoders (SAE) are thoroughly documented. These include specificity, sensitivity, F-score, and Matthews correlation coefficient (MCC) for multiple SAE configurations across different datasets, such as the Oxford and Istanbul datasets.\n\nThe results indicate a comprehensive evaluation of the proposed method's performance, showing that it achieves better results than other methods with relatively fewer training samples. This reduced training dataset is meaningful for real-world applications. The performance of SAE and K-Nearest Neighbors (KNN) is noted to be robust to the number of hidden neurons in deep neural networks.\n\nWhile the specific hyper-parameter configurations and optimization parameters are not directly reported, the detailed performance metrics and comparative results provide valuable insights into the effectiveness of the proposed method. The focus on classification accuracy, sensitivity, and other performance indices suggests a rigorous evaluation process. The advantages of the proposed method include time-saving, convenience, and low cost, although achieving 100% correct classification accuracy on smartphones remains a challenge.\n\nThe future work section highlights potential improvements, such as considering a good enough microphone, speech denoising using multiple microphones, and developing an evolutionary system to handle large testing datasets in real-world scenarios. These points suggest ongoing efforts to refine and optimize the method for better performance and practical applicability.\n\nThe publication does not explicitly mention the availability of model files or optimization parameters under a specific license. However, the detailed performance results and discussions provide a solid foundation for further research and development in this area.",
  "model/interpretability": "The models discussed in this publication primarily focus on classification accuracy and performance indices such as specificity, sensitivity, F-score, and Matthews correlation coefficient (MCC) for diagnosing Parkinson's Disease. These models, which include various classifiers like KELM, L-SVM, M-SVM, R-SVM, CART, KNN, LDA, and NB, are often used in conjunction with Stacked Autoencoders (SAE) to enhance their performance.\n\nThe use of SAE with these classifiers indicates a deep learning approach, which is generally considered a black-box model. Black-box models are known for their complexity and lack of transparency, making it difficult to interpret how they arrive at specific predictions. This is because deep learning models, including SAEs, involve multiple layers of neural networks that process data in ways that are not easily understandable by humans.\n\nHowever, the performance metrics provided, such as specificity, sensitivity, F-score, and MCC, offer some level of interpretability. For instance, specificity measures the true negative rate, indicating how well the model identifies non-disease cases. Sensitivity, on the other hand, measures the true positive rate, showing how well the model identifies disease cases. The F-score is the harmonic mean of precision and recall, providing a balance between the two. The MCC offers a more comprehensive view of the model's performance by considering true and false positives and negatives.\n\nWhile these metrics help in evaluating the model's performance, they do not provide insights into the internal workings of the model. The models discussed are designed to achieve high classification accuracy, which is crucial for diagnostic purposes. The focus on performance indices rather than interpretability suggests that the primary goal is to ensure reliable and accurate diagnoses, even if the decision-making process within the model remains opaque.\n\nIn summary, the models used in this study are largely black-box models, particularly when employing deep learning techniques like SAE. The interpretability is limited to performance metrics, which provide a quantitative assessment of the model's effectiveness but do not shed light on the underlying mechanisms of the model's predictions.",
  "model/output": "The model discussed in this publication is primarily focused on classification tasks. It involves various classifiers, both traditional machine learning algorithms and deep neural networks, to diagnose Parkinson's Disease based on voice samples. The performance of these classifiers is evaluated using metrics such as classification accuracy, sensitivity, F-score, and Matthews correlation coefficient (MCC). The classifiers compared include KELM, L-SVM, M-SVM, R-SVM, CART, KNN, LDA, and NB. The results are presented for different datasets, including the Istanbul and Oxford datasets, with and without the use of Stacked Autoencoders (SAE) for dimensionality reduction. The classification accuracy varies across different classifiers and configurations, with some classifiers showing significant improvement when SAE is applied. For instance, the KNN classifier consistently provides high classification accuracy, while others like Naive Bayesian (NB) show lower performance. The use of SAE generally enhances the classification accuracy of most classifiers, indicating its effectiveness in feature extraction and dimensionality reduction.",
  "model/duration": "The execution time of the proposed system was evaluated in a real-world scenario. Using a smartphone running the Android operating system and the Google Chrome web browser, we connected to a server via a 4G mobile network. The server, installed with the Windows operating system and Internet Information Services, received 28 test speech records (WMA files) from the Istanbul Dataset, totaling 12.1MB of data. The transmission of this data took less than 5 seconds. The server then processed these speech records, running empirical experiments as presented in previous sections. The results of all comparative classifiers were displayed on the smartphone within no more than 2 minutes. This demonstrates the efficiency and practicality of the system in real-time applications.",
  "model/availability": "Not applicable",
  "evaluation/method": "The evaluation of the proposed method involved a comprehensive approach using two distinct datasets: the Istanbul Dataset and the Oxford Dataset. The Istanbul Dataset was collected at Istanbul University, where Parkinson's Disease (PD) patients were asked to sustain vowels \"a\" and \"o\" three times. The Oxford Dataset, created by Little et al. in collaboration with the National Centre for Voice and Speech, consists of voice measurements from 31 individuals, with 23 diagnosed with PD.\n\nFor the evaluation, a 50-50% training-testing split was employed, similar to methods used by Polat and Daliri. This approach tests the classifiers with much less training data compared to traditional 10-fold cross-validation. Each classifier was tested 10 times to ensure robustness and reliability of the results.\n\nThe performance of the classifiers was assessed using several key metrics: accuracy, sensitivity, specificity, F-score, and Matthews Correlation Coefficient (MCC). These metrics provide a holistic view of the classifiers' effectiveness in distinguishing between PD patients and healthy individuals.\n\nThe classifiers evaluated include Kernel Extreme Learning Machine (KELM), Linear Support Vector Machine (LSVM), Multilayer Perceptrons Support Vector Machine (MSVM), Radial Basis Function Support Vector Machine (RSVM), Classification and Regression Tree (CART), K-Nearest Neighbors (KNN), Linear Discriminant Analysis (LDA), and Naive Bayesian (NB) method. The results were presented in tables, showing the maximum, mean, and minimum classification accuracy for each classifier.\n\nAdditionally, the Stacked Autoencoder (SAE) was integrated with these classifiers, and its performance was evaluated with different configurations of hidden layers. The SAE configurations varied in the number of neurons in the two hidden layers, and the results were compared to understand the impact of these variations on classification accuracy.\n\nThe evaluation also highlighted the importance of a quiet environment for achieving satisfactory classification accuracy, as noise can significantly affect the performance of the classifiers. Future work is suggested to focus on improving the robustness of the method in noisy conditions and exploring evolutionary systems for better handling of large testing datasets in real-world applications.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report several key metrics to evaluate the effectiveness of our classifiers. These metrics include specificity, sensitivity, F-score, and Matthews correlation coefficient (MCC). Specificity measures the true negative rate, indicating how well the model identifies negative instances. Sensitivity, or the true positive rate, assesses the model's ability to identify positive instances correctly. The F-score provides a harmonic mean of precision and recall, offering a balanced view of the model's performance. The MCC is a correlation coefficient between the observed and predicted classifications, providing a more comprehensive evaluation of the classifier's performance, especially when dealing with imbalanced datasets.\n\nThese metrics are widely used in the literature and are representative of standard practices in evaluating classification models. They provide a thorough assessment of the classifiers' performance across different aspects, ensuring that our evaluation is both comprehensive and comparable to other studies in the field. The inclusion of these metrics allows for a detailed analysis of the classifiers' strengths and weaknesses, facilitating a more informed discussion on their applicability and potential improvements.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of our proposed method with several publicly available methods using benchmark datasets. Specifically, we utilized two datasets: the \"Istanbul Dataset\" and the \"Oxford Dataset\". The Istanbul Dataset was collected at Istanbul University, where Parkinson's Disease (PD) patients were asked to sustain vowels \"a\" and \"o\" three times. The Oxford Dataset, created by Little et al. in collaboration with the National Centre for Voice and Speech, consists of voice measurements from 31 individuals, with 23 diagnosed with PD.\n\nWe compared our method against a variety of classifiers, including Kernel Extreme Learning Machine (KELM), Linear Support Vector Machine (LSVM), Multilayer Perceptrons Support Vector Machine (MSVM), Radial Basis Function Support Vector Machine (RSVM), Classification and Regression Tree (CART), K-Nearest Neighbors (KNN), Linear Discriminant Analysis (LDA), and Naive Bayesian (NB) method. These classifiers were chosen to represent a range of machine learning techniques, from simple to complex, ensuring a thorough evaluation.\n\nThe performance of these classifiers was assessed using several metrics, including accuracy, sensitivity, specificity, F-score, and Matthews Correlation Coefficient (MCC). These metrics provide a holistic view of the classifiers' performance, considering true positives, true negatives, false positives, and false negatives.\n\nAdditionally, we employed a Stacked Autoencoder (SAE) with two hidden layers in our proposed method. The SAE configurations varied, with layer sizes set as 10, 9, or 8 neurons for the first layer and 8, 7, or 6 neurons for the second layer. The batch size for SAE training and fine-tuning was set to 20. All comparative classifiers were optimized using the grid search method, and we used a 50-50% training-testing split, following the approach of previous studies.\n\nThe results of these comparisons are presented in tables, showing the maximum, mean, and minimum classification accuracies for each classifier and SAE configuration. This detailed comparison allows for a clear understanding of how our proposed method performs relative to established baselines and more complex models.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The datasets used in our experiments, such as the Istanbul Dataset and the Oxford Dataset, were collected from specific institutions and are not released publicly. The Istanbul Dataset was collected at Istanbul University, while the Oxford Dataset was created by Little et al. at the University of Oxford in collaboration with the National Centre for Voice and Speech in Denver, Colorado. These datasets contain voice measurements from individuals, some of whom were diagnosed with Parkinson's Disease (PD). The specific details and features of these datasets are described in our publication, but the actual data files are not provided for public access. Therefore, researchers interested in replicating our experiments would need to obtain these datasets through the respective institutions or collaborators."
}