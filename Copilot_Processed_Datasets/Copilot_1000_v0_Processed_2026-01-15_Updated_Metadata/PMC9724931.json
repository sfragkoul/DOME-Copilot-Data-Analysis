{
  "publication/title": "Artificial intelligence and optical coherence tomography for the automatic characterisation of human atherosclerotic plaques.",
  "publication/authors": "Chu M, Jia H, Guti\u00e9rrez-Chico JL, Maehara A, Ali ZA, Zeng X, He L, Zhao C, Matsumura M, Wu P, Zeng M, Kubo T, Xu B, Chen L, Yu B, Mintz GS, Wijns W, Holm NR, Tu S",
  "publication/journal": "EuroIntervention : journal of EuroPCR in collaboration with the Working Group on Interventional Cardiology of the European Society of Cardiology",
  "publication/year": "2021",
  "publication/pmid": "33528359",
  "publication/pmcid": "PMC9724931",
  "publication/doi": "10.4244/eij-d-20-01355",
  "publication/tags": "- AI\n- OCT\n- Plaque characterisation\n- Deep convolutional model\n- Medical imaging\n- Cardiovascular disease\n- Machine learning\n- Image analysis\n- Interventional cardiology\n- Diagnostic accuracy",
  "dataset/provenance": "The dataset utilized in this study comprises a total of 509 intravascular optical coherence tomography (IVOCT) pullbacks obtained from 391 patients. These pullbacks were sourced from five international centers, ensuring a diverse and representative sample. The dataset was divided into training and testing sets, with 90% of the data (10,517 cross-sections) allocated for training and hyperparameter tuning, and the remaining 10% (1,156 cross-sections) reserved for internal evaluation. The training set was used to develop the deep convolutional model, while the testing set was employed to assess the model's performance internally. The ground truth for the annotations was generated by experienced OCT analysts who labeled nine distinct objects. This dataset has not been previously used in other publications or by the community, making it unique to this study. The supplementary data, including detailed annotation strategies and model architectures, are available online for further reference.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a testing set. The training set comprised 90% of the data, while the testing set comprised the remaining 10%. Specifically, the training set consisted of 10,517 cross-sections, and the testing set consisted of 1,156 cross-sections. Additionally, 10% of the training set was further separated for hyper-parameter optimization. This division ensured that there was no repetition of pullbacks across different datasets, maintaining the integrity of the evaluation process.",
  "dataset/redundancy": "The datasets were split into training and testing sets with a proportion of 9 to 1, ensuring that no pullbacks were repeated across different datasets. This strict avoidance of repetition was enforced to maintain the independence of the training and testing sets. The training set was used for model development, with 10% of it reserved for hyperparameter optimization. The testing set was then used for internal evaluation of the model's performance. This approach ensures that the model's performance is assessed on completely unseen data, providing a more reliable evaluation of its generalization capabilities.\n\nThe distribution of the datasets in this study involved a total of 509 OCT pullbacks from 391 patients, resulting in 10,517 cross-sections for the training set and 1,156 for the testing set. This distribution is designed to provide a robust training process while also ensuring that the testing set is representative of real-world scenarios. The focus on maintaining independence between the training and testing sets aligns with best practices in machine learning to prevent data leakage and overfitting, thereby enhancing the model's reliability and validity.",
  "dataset/availability": "Not enough information is available",
  "optimization/algorithm": "The machine-learning algorithm class used is a deep convolutional neural network (CNN). This specific architecture is a U-shaped encoder-decoder design, which includes a contracting path for high-level feature extraction and an expansion path to produce full-resolution segmentation. Additionally, it incorporates vertical and horizontal feature bridges to preserve detailed spatial information.\n\nThe model is not entirely new, as CNNs are well-established in the field of deep learning, particularly for image processing tasks. However, the specific implementation and its application to intravascular optical coherence tomography (IVOCT) images for plaque characterisation are novel contributions of this study.\n\nThe reason this work was not published in a machine-learning journal is that the primary focus of the study is on the medical application of the algorithm rather than the development of new machine-learning techniques. The research aims to address a specific clinical need\u2014automating and standardising the characterisation of atherosclerotic plaques in IVOCT images. This application-driven approach is more aligned with journals in the field of medical imaging and intervention, where the practical implications and clinical validity of the model are of primary interest.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "For the machine-learning algorithm, the data encoding involved creating pseudo-3D inputs by stacking consecutive intravascular optical coherence tomography (IVOCT) cross-sections as separate color channels. This approach integrated spatial information across multiple slices, enhancing the model's ability to capture detailed spatial features. The model architecture designed for this task was a U-shaped encoder-decoder, which included a contracting path for high-level feature extraction and an expansion path to produce full-resolution segmentation. Additionally, vertical and horizontal feature bridges were incorporated to preserve detailed spatial information throughout the encoding and decoding processes. This encoding strategy facilitated comprehensive plaque characterization by leveraging the spatial context provided by the stacked cross-sections.",
  "optimization/parameters": "The model utilized a U-shaped encoder-decoder architecture, which is a type of convolutional neural network (CNN) designed for image segmentation tasks. This architecture includes a contracting path for high-level feature extraction and an expansion path to produce full-resolution segmentation. Additionally, vertical and horizontal feature bridges were incorporated to preserve detailed spatial information.\n\nThe model was fed with pseudo-3D input by stacking consecutive IVOCT cross-sections as separate color channels. This approach integrates spatial information across multiple slices, enhancing the model's ability to capture complex patterns in the data.\n\nA hybrid loss function, combining multi-class cross-entropy loss and focal Tversky loss, was employed to address the problem of class imbalance. This hybrid loss function helps the model to focus more on the minority classes, improving the overall segmentation performance.\n\nThe training set, which constituted 90% of the annotated pullbacks, was used for model development. Within this training set, 10% of the data was separated for hyper-parameter optimization. This optimization process involved tuning various parameters to achieve the best model performance.\n\nThe specific number of parameters (p) used in the model is not explicitly stated, but the architecture and training strategy suggest a comprehensive approach to optimizing the model's performance. The use of a hybrid loss function and the integration of spatial information through pseudo-3D input are key factors in selecting and optimizing the model parameters.",
  "optimization/features": "The model was designed to handle pseudo-3D input by stacking consecutive intravascular optical coherence tomography (IVOCT) cross-sections as separate color channels. This approach integrates spatial information across multiple cross-sections, effectively creating a multi-channel input feature set. The exact number of features (f) used as input is not explicitly stated, but it involves multiple channels derived from the stacked IVOCT cross-sections.\n\nFeature selection in the traditional sense was not performed. Instead, the model leverages the spatial information from consecutive cross-sections to enhance feature extraction. The design of the U-shaped encoder-decoder architecture, with its contracting and expansion paths, along with vertical and horizontal feature bridges, ensures that detailed spatial information is preserved and utilized effectively.\n\nThe training process involved dividing the annotated pullbacks into training and testing datasets in a 9:1 proportion, with strict avoidance of repetition of pullbacks in different datasets. This division ensures that the model's performance is evaluated on unseen data, maintaining the integrity of the training and testing phases. The training set was also used for hyper-parameter optimization, with 10% of the training data set aside for this purpose. This approach ensures that the model's parameters are tuned using the training data only, without contaminating the testing dataset.",
  "optimization/fitting": "The model employed in this study is a deep convolutional neural network with a U-shaped encoder-decoder architecture. This architecture is designed to handle high-dimensional data efficiently, which helps in managing a large number of parameters relative to the training points.\n\nTo address potential overfitting, several strategies were implemented. Firstly, the model was trained using a hybrid loss function that combines multi-class cross-entropy loss and focal Tversky loss. This approach helps in mitigating class imbalance issues, which are common in medical imaging datasets. Additionally, the training dataset was augmented by stacking consecutive IVOCT cross-sections as separate color channels, integrating spatial information and effectively increasing the diversity of the training data.\n\nFurthermore, the model's performance was rigorously evaluated on a separate testing dataset, which was not used during the training phase. This internal evaluation ensured that the model generalizes well to unseen data. The agreement between predictions and ground truth was quantified using the Dice coefficient, precision, and recall, providing a comprehensive assessment of the model's performance.\n\nTo prevent underfitting, the model architecture includes vertical and horizontal feature bridges that preserve detailed spatial information. This design choice ensures that the model captures both high-level features and fine-grained details, enhancing its ability to accurately segment different plaque types.\n\nThe training process also involved hyperparameter tuning on a subset of the training data, which helped in optimizing the model's performance without overfitting to the training set. The use of a validation set during this tuning process further ensured that the model's performance was robust and generalizable.\n\nIn summary, the combination of a sophisticated architecture, hybrid loss function, data augmentation, and thorough evaluation strategies helped in balancing the model's complexity and ensuring it neither overfits nor underfits the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our deep convolutional model. One key strategy involved the use of a hybrid loss function, which combined multi-class cross-entropy loss with focal Tversky loss. This approach helped address the problem of class imbalance, which is common in medical imaging datasets. By doing so, we ensured that the model did not become biased towards the more prevalent classes, thereby improving its generalization capabilities.\n\nAdditionally, we conducted ablation experiments to validate the design choices of our convolutional neural network (CNN). These experiments involved systematically removing or altering components of the model to observe their impact on performance. This process helped us identify the most effective architecture and hyperparameters, further reducing the risk of overfitting.\n\nDuring the training phase, we also utilized a rigorous data splitting strategy. The annotated pullbacks were randomly divided into training and testing datasets in a 9:1 ratio, ensuring that no pullbacks were repeated across different datasets. This strict separation helped in evaluating the model's performance on unseen data, providing a more accurate assessment of its generalization ability.\n\nFurthermore, we set aside 10% of the training dataset for hyperparameter optimization. This subset was used to fine-tune the model's parameters, ensuring that the final model was optimized for performance without overfitting to the training data.\n\nIn summary, our approach to preventing overfitting included the use of a hybrid loss function, ablation experiments, rigorous data splitting, and hyperparameter optimization. These techniques collectively contributed to the development of a robust and generalizable model for plaque characterization.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule are detailed in the supplementary materials, specifically in Supplementary Appendix 4. The model architecture and design are also thoroughly documented in Supplementary Appendix 2. However, model files and optimization parameters are not explicitly mentioned as being available for download or use. The supplementary materials, including the appendices and figures, are published online and can be accessed via the provided DOI. The licensing details for accessing these supplementary materials are not specified, but they are intended for academic and research purposes.",
  "model/interpretability": "The model developed for plaque characterisation in intravascular optical coherence tomography (IVOCT) is not a blackbox. It is designed with a U-shaped encoder-decoder architecture, which includes a contracting path for high-level feature extraction and an expansion path to produce full-resolution segmentation. This architecture helps in preserving detailed spatial information through vertical and horizontal feature bridges.\n\nThe model's design allows for a degree of interpretability. For instance, the use of pseudo-3D input by stacking consecutive IVOCT cross-sections as separate color channels integrates spatial information, making it easier to understand how the model processes and interprets data. Additionally, the hybrid loss function, which combines multi-class cross-entropy loss and focal Tversky loss, addresses class imbalance issues and provides insights into the model's decision-making process.\n\nThe model's performance on different plaque types and structures, such as fibrous, calcific, and lipidic plaques, as well as inflammatory markers like microvessels and macrophages, demonstrates its ability to provide detailed and interpretable results. The model's segmentation accuracy and the correlation between its predictions and ground truth further support its transparency and reliability.",
  "model/output": "The model is primarily designed for segmentation, which is a type of classification task at the pixel level. It identifies and delineates different types of plaque and tissue components in intravascular optical coherence tomography (IVOCT) images. The model categorizes various plaque regions, such as fibrous, lipidic, and calcific plaques, as well as non-tissue structures like guidewires and side branches. It also segments inflammatory markers, including microvessels, cholesterol crystals, and macrophages.\n\nThe model's output includes detailed segmentation maps that highlight different tissue types and structures within the IVOCT images. These maps are used to quantify parameters like plaque burden, which is calculated as the area between the estimated internal elastic lamina (IEL) contour and the lumen contour, divided by the IEL area and multiplied by 100%. The accuracy of the IEL segmentation is evaluated based on the agreement in plaque burden between the model's predictions and the ground truth.\n\nThe model's performance is assessed using metrics such as the Dice coefficient, precision, and recall. The Dice coefficient measures the overlap between the predicted and ground truth segmentations, providing a score for each category of plaque or tissue type. Precision indicates the purity of positive predictions, while recall reflects the completeness of these predictions relative to the ground truth.\n\nIn addition to segmentation, the model provides quantitative measurements of plaque characteristics, such as plaque area, arc degree, and tissue proportions. These outputs are integrated into the OctPlus software for real-time analysis of IVOCT pullbacks, displaying plaques in both 2D cross-sectional and 3D views. The software's diagnostic accuracy is evaluated based on the overlap between the model's predictions and the consensus labels from core labs, with correct characterisation defined as an overlap of more than 80%.\n\nThe model's design includes a U-shaped encoder-decoder architecture with vertical and horizontal feature bridges to preserve spatial information. It uses a hybrid loss function that combines multi-class cross-entropy loss and focal Tversky loss to address class imbalance issues. The model is trained on a dataset of IVOCT images, with a proportion of 9 to 1 for training and testing, respectively. The training set is further divided, with 10% used for hyper-parameter optimization. The model's performance is validated internally on the testing dataset and externally on a separate dataset provided by international core labs.",
  "model/duration": "The model demonstrated a rapid computational speed during its execution. The median time required for the convolutional neural network (CNN) model to analyze an image pullback, consisting of 271 cross-sections with a pixel size of 704\u00d7704, was approximately 21.4 seconds. This translates to an average speed of about 0.07 seconds per cross-section. These performance metrics were achieved using a laptop equipped with an AMD Ryzen 7 processor and a Geforce RTX 2060 graphics card. This efficiency is particularly notable when compared to the time-consuming process of manual annotation by analysts, which can take several minutes per frame, especially for images with complex plaque compositions or suboptimal quality. The model's ability to quickly integrate spatial continuity information across frames further enhances its diagnostic accuracy and efficiency.",
  "model/availability": "The proposed deep convolutional model has been integrated into the OctPlus software, developed by Pulse Medical Imaging Technology, Shanghai, China. This software enables real-time analysis of intravascular optical coherence tomography (IVOCT) pullbacks, providing both 2D cross-sectional and 3D views of plaques. The software quantifies various parameters, including plaque area, arc degree, and different tissue proportions.\n\nThe source code for the model itself has not been publicly released. However, the software that incorporates the model is available for use. The specific details regarding the licensing and access to the OctPlus software would need to be obtained directly from Pulse Medical Imaging Technology.",
  "evaluation/method": "The evaluation of the method involved both internal and external validation processes. For internal evaluation, the model was tested on a separate dataset that was not used during training. This dataset consisted of 10% of the total annotated pullbacks, ensuring no repetition of pullbacks between the training and testing sets. The performance was assessed using the Dice coefficient, which measures the overlap between the predicted and ground truth segmentations. Additionally, precision and recall were reported to evaluate the purity and completeness of the positive predictions.\n\nFor external validation, the model was tested on a completely independent dataset consisting of 300 IVOCT images from 30 patients, provided by three international core labs. These images were labeled by four experienced OCT readers from the core labs, who were blinded to each other's analyses. The regions were classified based on the agreement among the core labs, with consensus defined as agreement between at least two core labs. The model's performance was evaluated using accuracy, as the pixel-wise evaluation by Dice coefficient was not applicable due to some regions being flagged without complete boundaries. Correct plaque characterization was defined as an overlapping portion of more than 80% between the software's prediction and the core lab consensus in area or arc circumference. A sensitivity analysis was also conducted to assess the impact of varying the overlapping portion from 80% to 90% on diagnostic accuracy.",
  "evaluation/measure": "For the evaluation of our model, we primarily reported the Dice coefficient, precision, and recall for segmentation performance on the testing data set. These metrics are widely used in the literature for evaluating the performance of segmentation models, particularly in medical imaging.\n\nThe Dice coefficient measures the overlap between the predicted segmentation and the ground truth, providing a single value that summarizes the performance. Precision and recall offer more granular insights, with precision indicating the accuracy of the positive predictions and recall measuring the ability of the model to identify all relevant instances.\n\nAdditionally, we used accuracy as the primary metric for the external validation. This was chosen because some plaque regions were flagged with incomplete boundaries, making pixel-wise evaluation by Dice coefficient less applicable. Correct plaque characterization was defined as an overlapping portion greater than 80% between the model's prediction and the core lab consensus in area or arc circumference.\n\nFor quantitative parameters, we assessed the correlation between the model's predictions and the ground truth using the Pearson or Spearman correlation tests, depending on the data distribution. We also performed Bland-Altman analysis and calculated the intraclass correlation coefficient for absolute agreement to evaluate the consistency between the model's predictions and the manual annotations.\n\nThese performance metrics provide a comprehensive evaluation of our model's capabilities, covering both segmentation accuracy and the reliability of quantitative assessments. The choice of metrics aligns with established practices in the field, ensuring that our evaluation is representative and comparable to other studies in the literature.",
  "evaluation/comparison": "Not applicable. The provided information does not detail any comparisons to publicly available methods or simpler baselines on benchmark datasets. The evaluation focuses on the internal and external validation of the proposed deep convolutional model, including its performance on different types of plaques and structures, as well as the agreement between the model predictions and ground truth annotations. The evaluation metrics used include the Dice coefficient, precision, recall, and accuracy, but there is no mention of comparing the model's performance against other existing methods or baselines.",
  "evaluation/confidence": "The evaluation of our model's performance included the use of confidence intervals to estimate the plausible range of values. Specifically, a confidence level of 95% was employed. This approach ensures that the reported performance metrics, such as the Dice coefficients for different plaque types, are robust and provide a reliable indication of the model's capabilities.\n\nStatistical significance was a key consideration in our analysis. We performed various statistical tests, including the Shapiro-Wilk test for normality, Pearson or Spearman correlation tests for evaluating the correlation between ground truth and model predictions, and Bland-Altman analysis for assessing agreement between groups for continuous variables. The intraclass correlation coefficient (ICC) and kappa coefficient were used for categorical variables. A two-sided p-value of \u22640.05 was considered statistically significant, ensuring that our findings are not due to random chance.\n\nThe model's performance was evaluated using a testing dataset that was separate from the training data, ensuring an unbiased assessment. The Dice coefficients for fibrous, calcific, and lipidic plaques were 0.906, 0.848, and 0.772, respectively, indicating strong segmentation capabilities. Additionally, the external validation involved a different dataset provided by three international core labs, further validating the model's generalizability and reliability.\n\nIn summary, the performance metrics are supported by statistical analyses that confirm their significance and reliability. The use of confidence intervals and rigorous statistical testing underscores the robustness of our model's evaluation.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The supplementary data, which includes various appendices and figures related to the model's architecture, training, and evaluation, is published online. However, this data does not include the raw evaluation files. The supplementary data can be accessed at the provided DOI link, but it is not specified whether the raw evaluation files are part of this supplementary data. The external validation was performed using a different dataset than the one used for model development, and the results of this validation are reported in the publication. The evaluation metrics used include the Dice coefficient for internal evaluation and accuracy for external validation. The statistical analysis was performed using SPSS, Version 23.0."
}