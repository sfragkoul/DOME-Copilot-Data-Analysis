{
  "publication/title": "Diagnostic usefulness of deep learning methods for <i>Helicobacter pylori</i> infection using esophagogastroduodenoscopy images.",
  "publication/authors": "Kang D, Lee K, Kim J",
  "publication/journal": "JGH open : an open access journal of gastroenterology and hepatology",
  "publication/year": "2023",
  "publication/pmid": "38162866",
  "publication/pmcid": "PMC10757494",
  "publication/doi": "10.1002/jgh3.12995",
  "publication/tags": "- Artificial intelligence\n- Deep learning\n- Helicobacter pylori\n- Esophagogastroduodenoscopy\n- Diagnostic methods\n- Convolutional neural networks\n- Medical imaging\n- Gastrointestinal diseases\n- Image preprocessing\n- Machine learning in medicine\n- Diagnostic accuracy\n- Endoscopy\n- Gastric mucosa\n- Predictive models\n- Healthcare technology",
  "dataset/provenance": "The dataset used in this study consists of 13,071 esophagogastroduodenoscopy (EGDS) images. These images were collected from various gastric sub-areas and were used to train and validate deep convolutional neural network (DCNN) models for detecting Helicobacter pylori infection. The images were categorized based on different sub-areas of the stomach, including the antrum, cardia and fundus, lower body greater curvature and lesser curvature, and upper body greater curvature and lesser curvature. The dataset was divided into training, validation, and test sets, with 8,894 images used for training, 1,570 images for validation, and 2,607 images for testing. The training set included 4,953 positive and 3,941 negative images, while the validation set had 874 positive and 696 negative images. The test set consisted of 1,452 positive and 1,155 negative images. The images underwent preprocessing to remove black regions containing patient information and to obtain consistent images for input into the DCNN models. Data augmentation techniques were applied to the training images to reduce overfitting and increase the number of training examples. The dataset was not used in any previous paper or by the community before this study.",
  "dataset/splits": "The dataset consisted of 13,071 endoscopic images, which were divided into three splits: training, validation, and test sets. The training set contained 8,894 images, with 4,953 positive cases and 3,941 negative cases. The validation set comprised 1,570 images, with 874 positive cases and 696 negative cases. The test set included the remaining 2,607 images, with 1,452 positive cases and 1,155 negative cases. These splits were used to train, validate, and test the deep convolutional neural network (DCNN) models for diagnosing H. pylori infection.",
  "dataset/redundancy": "The dataset used in this study consisted of 13,071 endoscopic images. To ensure robust evaluation, the dataset was split into three independent sets: training, validation, and test sets. Specifically, 8,894 images were randomly chosen for training, with 4,953 positive and 3,941 negative cases. The validation set comprised 1,570 images, including 874 positive and 696 negative cases. The remaining 2,607 images, consisting of 1,452 positive and 1,155 negative cases, were set aside for testing.\n\nThe independence of the training and test sets was enforced by ensuring that the images in each set were selected independently and did not overlap. This approach helps to prevent data leakage and ensures that the model's performance on the test set is a true reflection of its generalization capability.\n\nComparing this dataset to previously published machine learning datasets, the split ratios and the independence of the sets are consistent with standard practices in the field. The use of a validation set to monitor performance and prevent overfitting is a common strategy, and the size of the test set is sufficient to provide reliable performance metrics. The distribution of positive and negative cases in each set is balanced, which is crucial for training robust models and obtaining unbiased performance evaluations.",
  "dataset/availability": "The data used in this study, including the specific data splits for training, validation, and testing, are not publicly released. The dataset consists of 13,071 EGDS images, which were divided into training (8,894 images), validation (1,570 images), and test (2,607 images) sets. The images were selected and processed to ensure they met the quality standards required for the study, with substandard images and those with specific lesions excluded.\n\nThe decision not to release the data publicly was made to protect patient privacy. The images contained personal information that was removed through preprocessing steps, but releasing the data could still pose risks. Therefore, the data remains confidential and is not available in any public forum.\n\nThe enforcement of data confidentiality was managed through institutional review board (IRB) approval and adherence to ethical guidelines. The study was approved by the Inje University Busan Paik Hospital\u2019s Institutional Review Board, which ensured that all procedures complied with ethical standards and patient privacy was maintained. The IRB approval number is 2021-08-059. Written informed consent was waived by the IRB, further emphasizing the importance of data confidentiality.\n\nIn summary, while the data splits and processing methods are detailed in the study, the actual dataset is not publicly available due to privacy concerns. The confidentiality of the data was enforced through IRB approval and ethical guidelines, ensuring that patient information was protected throughout the research process.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is deep convolutional neural networks (DCNNs). Specifically, five different DCNN architectures were employed: ResNet-101, Xception, Inception-v3, InceptionResnet-v2, and DenseNet-201. These are well-established architectures in the field of computer vision and deep learning.\n\nThe algorithms used are not new; they are widely recognized and have been extensively studied and applied in various domains. The choice of these architectures was likely driven by their proven effectiveness in image classification tasks, which aligns with the goal of diagnosing H. pylori infections using endoscopic images.\n\nThe decision to use these established models rather than proposing a new algorithm can be attributed to several factors. Firstly, these models have demonstrated strong performance in similar tasks, providing a solid foundation for the study. Secondly, using established models allows for a more straightforward comparison with existing literature, facilitating the evaluation of the study's contributions. Lastly, the focus of this research is on the application of deep learning to medical diagnosis, rather than the development of new machine-learning algorithms. Therefore, leveraging proven models enables the researchers to concentrate on the specific medical application and its implications.\n\nThe ensemble model, which combines the output probabilities of the best-performing DCNN models, is also not a novel concept in machine learning. Ensemble methods are commonly used to improve the robustness and accuracy of predictions by aggregating the strengths of multiple models. In this case, the ensemble model was constructed to enhance the diagnostic performance for H. pylori infections.\n\nThe study's primary contribution lies in the application of these deep learning techniques to the diagnosis of H. pylori infections using endoscopic images, rather than in the development of new machine-learning algorithms. The focus is on evaluating the diagnostic performance of these models and understanding their potential in medical practice.",
  "optimization/meta": "In our study, we employed an ensemble model as a meta-predictor to enhance the diagnostic performance for detecting H. pylori infections using esophagogastroduodenoscopy (EGDS) images. This ensemble model integrates the output probabilities from multiple deep convolutional neural network (DCNN) models, specifically ResNet-101, Xception, Inception-v3, InceptionResnet-v2, and DenseNet-201. By averaging these probabilities, the ensemble model aims to leverage the strengths of each individual DCNN model, thereby improving overall accuracy and robustness.\n\nThe ensemble approach does not directly use data from other machine-learning algorithms as input in the traditional sense. Instead, it aggregates the predictions made by different DCNN models, which were trained independently on the same dataset. This method ensures that the training data for each constituent model remains independent, as each model is trained separately before their outputs are combined.\n\nThe ensemble model demonstrated superior performance compared to the individual DCNN models. For instance, when all EGDS images were used for training, validation, and testing, the ensemble model achieved the highest area under the curve (AUC) of 0.867, as well as the best specificity (78.44%), accuracy (80.28%), positive predictive value (82.66%), and negative predictive value (77.37%). The highest sensitivity (83.75%) was obtained by the Inception-v3 model. This indicates that the ensemble model effectively combines the predictive power of the individual DCNN models, leading to more reliable and accurate diagnoses.\n\nIn summary, the ensemble model serves as a meta-predictor that enhances diagnostic performance by integrating the outputs of multiple DCNN models. The training data for each constituent model is independent, ensuring that the ensemble approach leverages the diverse strengths of the individual models without compromising the integrity of the training process.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for preparing the endoscopic images for input into the deep convolutional neural network (DCNN) models. Initially, raw endoscopic images contained black regions with patient information, which were removed to prevent leakage of personal data and to avoid mistraining the models. This involved applying image processing techniques to eliminate unnecessary black regions, ensuring consistent and privacy-protected images.\n\nTo obtain the region of interest (ROI) from each image, only the red channel was used, as endoscopic images are rich in red color. The Canny edge detection algorithm was applied to find edges in the grayscale image. Morphological operations such as dilation and image filling were performed to refine the edge detection. The resulting image was complemented and dilated to remove any remaining personal information. The coordinate information of the four corners of the white area was then extracted, and the ROI image was obtained using these coordinates.\n\nThe images were resized to specific dimensions compatible with the different DCNN models: 224x224 pixels for ResNet-50 and DenseNet-201, and 299x299 pixels for Xception, Inception-v3, and InceptionResnet-v2. Data augmentation techniques were applied to the training images to reduce overfitting and increase the number of training examples. These techniques included resizing with a random scale factor, horizontal and vertical flipping, rotation, and horizontal and vertical shifting. The input images were normalized by channel-wise mean subtraction and division by the standard deviation using ImageNet mean and standard deviation values.\n\nThe validation and test data were not augmented. This preprocessing ensured that the images were in a suitable format for training the DCNN models, enhancing their ability to accurately diagnose H. pylori infection.",
  "optimization/parameters": "The models used in this study are deep convolutional neural networks (DCNNs), specifically ResNet-101, Xception, Inception-v3, InceptionResnet-v2, and DenseNet-201. These models have a large number of parameters, which are typically in the range of millions due to their deep architectures. The exact number of parameters varies depending on the specific model architecture.\n\nThe selection of these models and their parameters was based on their proven performance in various image classification tasks. The learning rates for these models were set to values such as 5e-5, 1e-4, and 5e-4, and the L2 regularization was set to 5e-4 and 1e-3. These hyperparameters were chosen to optimize the models' performance on the given dataset of esophagogastroduodenoscopy (EGDS) images.\n\nAdditionally, an early stopping strategy was employed to prevent overfitting, with a patience of 30 epochs. The mini-batch sizes were determined based on the maximum capacity of the GPU used for training, with sizes of 32 for Xception and Inception-v3, 16 for ResNet-101 and InceptionResnet-v2, and 8 for DenseNet-201.\n\nThe best model was selected based on the area under the curve (AUC) in the validation set, and an ensemble model was created by averaging the output probabilities of the best individual models to improve discrimination power.",
  "optimization/features": "The input features for the models were the preprocessed endoscopic images. Each image was resized and normalized before being fed into the networks. The specific dimensions for the input images varied depending on the model: 224x224 pixels for ResNet-50 and DenseNet-201, and 299x299 pixels for Xception, Inception-v3, and InceptionResnet-v2. Feature selection in the traditional sense was not performed, as the models utilized convolutional neural networks (CNNs) that automatically learn relevant features from the raw image data. The preprocessing steps, including resizing, flipping, rotating, and shifting, were applied to augment the training data and improve the models' generalization capabilities. These augmentations were determined based on the maximum capacity of the GPU and were applied to the training set only, ensuring that the validation and test data remained unchanged.",
  "optimization/fitting": "The fitting method employed in this study utilized five pretrained deep convolutional neural network (DCNN) architectures: ResNet-101, Xception, Inception-v3, InceptionResnet-v2, and DenseNet-201. These networks had already learned to extract informative features from the ImageNet dataset, which significantly reduced the number of parameters that needed to be trained from scratch.\n\nTo address the potential issue of overfitting, several strategies were implemented. Firstly, L2 regularization (weight decay) was applied to penalize large weights, which helps in generalizing the results well. The regularization hyperparameter was set to 5e-4 and 1e-3 for all DCNN models. Additionally, an early stopping strategy was adopted to monitor the validation loss with a patience of 30 epochs. This means that if the validation loss did not improve for 30 consecutive epochs, the training process was halted to prevent overfitting. The mini-batch sizes were carefully chosen based on the maximum capacity of the GPU, ensuring efficient use of computational resources without compromising the model's performance.\n\nData augmentation techniques were also employed to reduce overfitting and increase the number of training examples. These techniques included resizing the input image using a scale factor selected randomly from the range (0.8, 1.2), flipping with 50% probability in horizontal and vertical axes, rotating by an angle selected randomly from the range (-20, 20) degrees, and shifting horizontally and vertically by a distance selected randomly from the range (-30, 30) pixels. These augmentations helped in making the model more robust to variations in the input data.\n\nTo ensure that the models were not underfitting, the learning rates were carefully tuned. Initial learning rates of 5e-4, 1e-4, and 5e-5 were used to fine-tune the pretrained networks. The Adam optimizer (\u03b21 = 0.9, \u03b22 = 0.999) was employed for this purpose, which is known for its efficiency and adaptability in handling sparse gradients.\n\nThe ensemble model, which constructs different DCNN models and averages their output probabilities, was used to improve the discrimination power. This approach helped in mitigating the risk of underfitting by leveraging the strengths of multiple models.\n\nIn summary, the fitting method involved a combination of regularization techniques, early stopping, data augmentation, and careful tuning of hyperparameters to balance between overfitting and underfitting. The use of pretrained models and an ensemble approach further enhanced the model's performance and generalization capabilities.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the generalization of our deep convolutional neural network (DCNN) models. One of the key methods used was L2 regularization, also known as weight decay. This technique penalizes large weights in the model, which helps to reduce overfitting by discouraging the model from becoming too complex. The regularization hyperparameter was set to values of 5e-4 and 1e-3 for all DCNN models.\n\nAdditionally, we implemented an early stopping strategy. This involved monitoring the validation loss and setting a patience of 30 epochs. If the validation loss did not improve for 30 consecutive epochs, the training process was halted. This approach ensures that the model does not continue to train beyond the point where it starts to overfit the training data.\n\nData augmentation was another crucial technique used to prevent overfitting. We applied various augmentation methods to the training images, including resizing, flipping, rotating, and shifting. These transformations increased the diversity of the training dataset, making the model more robust and less likely to overfit to the specific characteristics of the training images.\n\nFurthermore, we used an ensemble model to improve the discrimination power of our DCNNs. The ensemble model was constructed by averaging the output probabilities of the best-performing DCNN models. This approach leverages the strengths of multiple models, reducing the risk of overfitting and enhancing the overall performance.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, the regularization parameters were set to 5e-4 and 1e-3 for all deep convolutional neural network (DCNN) models. An early stopping strategy was employed to monitor validation loss with a patience of 30 epochs to prevent overfitting. The mini-batch sizes varied depending on the model: 32 for Xception and Inception-v3, 16 for ResNet-101 and InceptionResnet-v2, and 8 for DenseNet-201. These sizes were determined based on the maximum capacity of the GPU used.\n\nThe learning rates for fine-tuning the pretrained networks were set to 5e-5, 1e-4, and 5e-4. The Adam optimizer was used with \u03b21 = 0.9 and \u03b22 = 0.999. The best DCNN model was selected based on the highest area under the curve (AUC) in the validation set. Additionally, an ensemble model was constructed by averaging the output probabilities of the best DCNN models to improve discrimination power.\n\nRegarding the availability of model files and optimization parameters, the specific model files are not directly provided in the publication. However, the configurations and parameters used are thoroughly described, allowing for replication of the experiments. The publication does not specify a particular license for the use of these configurations, but standard academic practices for citation and acknowledgment should be followed.\n\nNot applicable",
  "model/interpretability": "The models employed in our study are primarily deep convolutional neural networks (DCNNs), which are often considered black-box models due to their complex, multi-layered architectures. However, to enhance interpretability, we utilized the Gradient-weighted Class Activation Mapping (Grad-CAM) technique. This method provides visual explanations of the areas in the images that influence the model's classification decisions.\n\nGrad-CAM works by utilizing the gradient of the classification score with respect to the convolutional features determined by the network. This process highlights the regions in the image that are most critical for the model's predictions. The resulting visual explanation is presented as a heatmap, where red typically represents high values (areas of high importance) and blue represents low values (areas of low importance).\n\nFor instance, in our study, Grad-CAM was applied to images classified by the InceptionResnet-v2 model. The heatmaps generated by Grad-CAM showed the areas in the endoscopic images that were most likely to be infected with H. pylori. This visual tool helps in understanding why the model made certain decisions, whether correct or incorrect. For example, the heatmaps revealed the basis for the model's correct diagnoses and also highlighted the regions that led to misdiagnoses.\n\nBy overlaying these heatmaps on the original images, we could visually inspect the model's focus areas. This approach not only aids in validating the model's decisions but also provides insights into the model's learning patterns, making it more transparent and interpretable.",
  "model/output": "The model is a classification model designed to diagnose Helicobacter pylori infections using esophagogastroduodenoscopy (EGDS) images. It employs deep convolutional neural networks (DCNNs) to classify images into positive (infected) or negative (uninfected) categories. The output of the model is a probability indicating the likelihood of an image being positive for H. pylori infection. An ensemble model, which averages the output probabilities of the best-performing DCNN models, is used to improve the discrimination power and overall diagnostic performance. The model's performance is evaluated using metrics such as accuracy, sensitivity, specificity, positive predictive value, and negative predictive value. The area under the receiver operating characteristic curve (AUC) is also calculated to assess the model's diagnostic capacity. The best cutoff point for each DCNN model to discriminate H. pylori infection is determined using the maximal Youden index. The model's outputs are visualized using gradient-weighted class activation mapping (Grad-CAM) to provide insights into the areas of the images that influence the classification decisions.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the deep convolutional neural network (DCNN) models involved several key steps and metrics to ensure robust and reliable performance. The dataset consisted of 13,071 esophagogastroduodenoscopy (EGDS) images, which were split into training, validation, and test sets. Specifically, 8,894 images were used for training, 1,570 for validation, and 2,607 for testing. This split ensured that the models were trained on a substantial amount of data while also having separate sets for validation and testing to prevent overfitting and to evaluate performance on unseen data.\n\nTo enhance the models' generalization and reduce overfitting, data augmentation techniques were applied to the training images. These techniques included resizing, flipping, rotating, and shifting the images. The input images were then resized to specific dimensions depending on the model: 224x224 pixels for ResNet-50 and DenseNet-201, and 299x299 pixels for Xception, Inception-v3, and InceptionResnet-v2. Normalization was performed using channel-wise mean subtraction and division by the standard deviation, based on ImageNet values.\n\nThe evaluation metrics used included accuracy, sensitivity, specificity, positive predictive value, and negative predictive value. These metrics were calculated to provide a comprehensive assessment of the models' diagnostic performance. The area under the receiver operating characteristic (ROC) curve (AUC) was also computed to evaluate the models' ability to discriminate between positive and negative cases. The best cutoff point for each model was determined using the maximal Youden index.\n\nAn ensemble model was created by averaging the output probabilities of the best-performing DCNN models. This ensemble approach aimed to improve the discrimination power and overall performance. The diagnostic performances of the individual DCNN models and the ensemble model were compared using the DeLong test for AUCs and the generalized estimating equation (GEE) method for other metrics.\n\nThe results showed that the ensemble model achieved the best performance across most metrics, including AUC, specificity, accuracy, positive predictive value, and negative predictive value. The Inception-v3 model had the highest sensitivity. The diagnostic performances were also evaluated for different sub-anatomical categories of the stomach, with the ensemble model generally performing best across these regions.\n\nIn summary, the evaluation method involved a rigorous split of the dataset, extensive data augmentation, normalization, and the use of multiple performance metrics. The ensemble model demonstrated superior performance, highlighting the effectiveness of combining multiple DCNN models.",
  "evaluation/measure": "In our study, we evaluated the diagnostic capacity of deep convolutional neural network (DCNN) models using several quantitative performance measures. These metrics are widely recognized in the literature for assessing the effectiveness of classification models, particularly in medical imaging.\n\nThe primary metrics we reported include:\n\n* **Accuracy**: This measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. It provides an overall sense of how well the model performs across all classes.\n\n* **Sensitivity (Recall)**: This metric, also known as the true positive rate, indicates the proportion of actual positives that are correctly identified by the model. High sensitivity is crucial for ensuring that the model can detect most of the positive cases.\n\n* **Specificity**: This metric, also known as the true negative rate, represents the proportion of actual negatives that are correctly identified by the model. High specificity is important for minimizing false positives.\n\n* **Positive Predictive Value (PPV)**: This metric indicates the proportion of positive identifications that are actually correct. It is essential for understanding the reliability of positive predictions made by the model.\n\n* **Negative Predictive Value (NPV)**: This metric represents the proportion of negative identifications that are actually correct. It is important for understanding the reliability of negative predictions made by the model.\n\nAdditionally, we used the **Area Under the Curve (AUC)** of the Receiver Operating Characteristic (ROC) curve. The AUC provides a single scalar value that summarizes the performance of the model across all classification thresholds. A higher AUC indicates better model performance.\n\nThese metrics collectively provide a comprehensive evaluation of the model's diagnostic performance. They allow us to assess not only the overall accuracy but also the model's ability to correctly identify positive and negative cases, as well as the reliability of its predictions. This set of metrics is representative of standard practices in the literature, ensuring that our evaluation is robust and comparable to other studies in the field.",
  "evaluation/comparison": "A comparison of the diagnostic performances of five deep convolutional neural network (DCNN) models and an ensemble model was conducted using esophagogastroduodenoscopy (EGDS) images. The models evaluated included ResNet-101, Xception, Inception-v3, InceptionResnet-v2, and DenseNet-201. The ensemble model, which averaged the output probabilities of the best DCNN models, achieved the highest area under the curve (AUC) of 0.867, indicating superior performance in diagnosing H. pylori infection compared to the individual DCNN models.\n\nThe diagnostic metrics evaluated included sensitivity, specificity, accuracy, positive predictive value, and negative predictive value. The ensemble model also demonstrated the best specificity (78.44%), accuracy (80.28%), positive predictive value (82.66%), and negative predictive value (77.37%). The Inception-v3 model had the highest sensitivity (83.75%).\n\nTo compare the models, the nonparametric DeLong test was applied to the AUCs, revealing significant differences among the models. Pairwise comparisons were made to calculate P-values for each pair of models, and the generalized estimating equation (GEE) method was used to compare the diagnostic metrics between the DCNN and ensemble models.\n\nThe study also analyzed the performance of the models on sub-anatomical categories of the stomach. The ensemble model achieved the best AUC for most regions, except for the angle region, where DenseNet-201 performed best. The AUCs for the antrum, cardia and fundus, lower body GC and LC, and upper body GC and LC regions were 0.842, 0.826, 0.718, and 0.858, respectively, when using the ensemble model. These results suggest that the ensemble model provides a more robust and accurate diagnosis of H. pylori infection across different anatomical regions of the stomach.\n\nNot applicable.",
  "evaluation/confidence": "The evaluation of the models included several performance metrics, each accompanied by confidence intervals. These metrics included the area under the curve (AUC), sensitivity, specificity, accuracy, positive predictive value, and negative predictive value. The confidence intervals provide a range within which the true value of these metrics is likely to fall, offering a measure of the uncertainty associated with the estimates.\n\nStatistical significance was assessed using the DeLong test for comparing AUCs and the generalized estimating equation (GEE) method for comparing other performance metrics. The P-values obtained from these tests indicated significant differences between the models. For instance, the P-values for the overall differences between each deep convolutional neural network (DCNN) model, including the ensemble model, were all less than 0.0001. This suggests that the observed differences in performance metrics are unlikely to have occurred by chance, providing strong evidence that the ensemble model, in particular, outperforms the individual DCNN models.\n\nThe use of these statistical methods ensures that the claims of superiority are robust and not merely due to random variation. The ensemble model, which achieved the best AUC, specificity, accuracy, positive predictive value, and negative predictive value, was shown to be significantly better than the individual DCNN models. This comprehensive evaluation approach enhances the confidence in the results and the conclusions drawn from the study.",
  "evaluation/availability": "The raw evaluation files, specifically the EGDS images used for training, validation, and testing, are not publicly available. The study involved a dataset of 13,071 EGDS images, which were divided into training, validation, and test sets. However, these images were not released publicly due to privacy concerns and the need to protect patient information. The images underwent preprocessing to remove black regions containing patient information, ensuring that personal data was not exposed.\n\nThe evaluation process included various performance measures such as accuracy, sensitivity, specificity, positive predictive value, and negative predictive value. These measures were calculated based on the performance of different deep convolutional neural network (DCNN) models and an ensemble model. The results, including the area under the curve (AUC) and other performance metrics, are presented in the publication. However, the actual images and raw data used for these evaluations are not accessible to the public.\n\nThe study was conducted in compliance with ethical guidelines, and approval was obtained from the Institutional Review Board (IRB) of Inje University Busan Paik Hospital. The IRB waived the requirement for written informed consent, but this does not imply that the data can be freely shared. The focus was on ensuring the privacy and security of the patients whose images were used in the study."
}