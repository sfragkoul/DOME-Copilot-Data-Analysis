{
  "publication/title": "Classification of Lapses in Smokers Attempting to Stop: A Supervised Machine Learning Approach Using Data From a Popular Smoking Cessation Smartphone App.",
  "publication/authors": "Perski O, Li K, Pontikos N, Simons D, Goldstein SP, Naughton F, Brown J",
  "publication/journal": "Nicotine & tobacco research : official journal of the Society for Research on Nicotine and Tobacco",
  "publication/year": "2023",
  "publication/pmid": "36971111",
  "publication/pmcid": "PMC10256890",
  "publication/doi": "10.1093/ntr/ntad051",
  "publication/tags": "- Smoking cessation\n- Machine learning\n- Lapse prediction\n- Mobile health\n- Supervised learning\n- Random Forest\n- Support Vector Machine\n- Logistic Regression\n- XGBoost\n- Behavioral data analysis\n- Craving severity\n- Real-time support\n- Data-driven interventions\n- Predictive modeling\n- Tobacco use",
  "dataset/provenance": "The dataset used in this study was sourced from the \"pro\" version of the Smoke Free app, a popular smoking cessation app available in commercial app stores. The data was collected from participants who had voluntarily downloaded the app and agreed to have their data analyzed. The study focused on a subset of participants who had made at least 20 craving feature entries within three months of app registration. This resulted in an analytic sample of 791 participants, who provided a total of 37,002 data entries. These entries included information about craving severity, mood, activity, social context, and lapse incidence.\n\nThe data structure represents real-world conditions, providing a realistic estimate of the amount and nature of data that can be expected in an unprompted study design. This dataset has not been used in previous papers by the community, as it is derived from a specific study focusing on the development and evaluation of machine learning algorithms for classifying lapses in smokers attempting to quit. The data was collected as part of a longitudinal, observational study with unprompted, self-initiated repeated measures of lapse incidence nested within participants.",
  "dataset/splits": "The dataset was split using a k-fold cross-validation approach, with k set to 10. This means that the data was divided into 10 different splits, or folds. For each iteration, or fold, the algorithms were trained on 80% of the data and tested on the remaining 20%. This process was repeated 10 times, with each fold serving as the test set once and as part of the training set nine times. The aim of this approach was to minimize the out-of-sample error, which is the error rate on data that was not used during the training phase. This helps to ensure that the algorithms can accurately classify outcome values in previously unseen samples. The distribution of data points in each split was designed to balance the in-sample and out-of-sample errors, with a widely used rule of thumb being to set aside 20% of the dataset for testing. This approach helps to provide a robust estimate of the algorithm's performance and generalizability to new data.",
  "dataset/redundancy": "The datasets were split using a k-fold cross-validation approach, with k set to 10. This means that for each iteration, or fold, the algorithms were trained on 80% of the data and tested on the remaining 20%. This process was repeated 10 times, with each fold serving as the test set once. The aim was to minimize the out-of-sample error, which is the error rate on data not used during the training phase. This approach ensures that the training and test sets are independent, as the test set is not used during the learning phase. The distribution of the data in the training and test sets is designed to balance the in-sample and out-of-sample errors, with a widely used rule of thumb being to set aside 20% of the dataset for testing. This method is commonly used in machine learning to ensure robust and generalizable model performance. The specific details of the train and test splits across different objectives can be found in the supplementary materials.",
  "dataset/availability": "The data used in this study were collected from users of the Smoke Free app, which is available in commercial app stores. The study protocol and exploratory analysis plan were preregistered on the Open Science Framework. However, the specific dataset used for this study is not publicly released. The data were collected from participants who voluntarily downloaded the app and agreed to have their data analyzed. Ethical approval for the study was obtained from the University College London's Research Ethics Committee. The data were used to develop and evaluate machine learning algorithms for classifying lapses in smokers attempting to stop. The algorithms were trained and tested using k-fold cross-validation, with k set to 10. For each iteration, algorithms were trained on 80% and tested on the remaining 20% of the data. The study aimed to minimize the out-of-sample error to ensure the algorithms could accurately classify outcome values in a previously unseen sample. The performance of the algorithms was evaluated using metrics such as accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (AUC). The best-performing group-level algorithm achieved an AUC of 0.969. Individual-level and hybrid algorithms were also developed and evaluated, with varying levels of performance. The study concluded that while a high-performing group-level algorithm was feasible, its performance was variable when applied to new, unseen individuals. Individual-level and hybrid algorithms showed improved performance but could only be constructed for a minority of participants due to the lack of variability in the outcome measure.",
  "optimization/algorithm": "The machine-learning algorithms used in this study were supervised learning algorithms, specifically focusing on classification tasks. The algorithms employed included Random Forest (RF), Support Vector Machine (SVM), Penalized Logistic Regression, and Extreme Gradient Boosting (XGBoost). These are well-established algorithms in the field of machine learning and are commonly used for various predictive modeling tasks.\n\nThe algorithms used are not new; they are standard and widely recognized in the machine learning community. The choice of these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to provide robust performance metrics. The study aimed to compare the performance of these algorithms in distinguishing lapse from non-lapse events in smokers attempting to quit.\n\nThe decision to use these established algorithms rather than novel ones was likely influenced by the need for reliability and comparability. These algorithms have been extensively validated and are known for their ability to handle large datasets with multiple variables. Additionally, using well-known algorithms allows for easier replication and validation of the results by other researchers in the field.\n\nThe study's focus was on applying these algorithms to a specific healthcare context, rather than developing new machine-learning techniques. Therefore, publishing in a machine-learning journal was not the primary objective. The findings and methodologies are more aligned with the goals of healthcare research, where the practical application and impact on public health are of utmost importance.",
  "optimization/meta": "The study did not employ a meta-predictor approach. Instead, it focused on training and testing various individual machine learning algorithms to distinguish lapse from non-lapse events in smokers attempting to quit. The algorithms used included Random Forest (RF), Support Vector Machine (SVM), Penalized Logistic Regression, and Extreme Gradient Boosting (XGBoost). These algorithms were selected based on their relatively low computational demands, availability of off-the-shelf R packages, and good interpretability.\n\nThe algorithms were trained and tested using k-fold cross-validation with k set to 10. For each iteration, algorithms were trained on 80% of the data and tested on the remaining 20%. This approach aimed to minimize out-of-sample error and balance the in-sample and out-of-sample errors.\n\nThe study did not combine predictions from multiple machine learning algorithms to create a meta-predictor. Instead, it evaluated the performance of each algorithm individually and compared their results. The performance metrics included accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (AUC). The study also conducted sensitivity analyses to ensure the robustness of the predictor variables across different mathematical formalisms.\n\nIn summary, the study did not use data from other machine-learning algorithms as input for a meta-predictor. The algorithms were trained and tested independently, and their performance was evaluated separately. The study did not address the independence of training data in the context of a meta-predictor approach.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithms involved several steps to ensure the data was suitable for training and testing. Time-invariant variables, such as time to first cigarette after waking up and cigarettes smoked per day, were entered once per participant upon app download. Time-varying variables, which included craving severity, feeling states, activities, social context, and temporal factors, were captured at each craving entry. These variables were recorded based on the participant's self-initiated craving entries, with retrospective reporting minimized by excluding entries where the time difference between the entry and the craving event was more than 24 hours.\n\nCraving severity was measured on an 11-point Likert scale, while feeling states, activities, and social context were binary variables indicating the presence or absence of specific conditions. Temporal variables included the time of day, day of the week, cumulative time from the quit date, and the time since the previous craving entry. The dataset was imbalanced, so random up- or down-sampling with replacement was used to address this issue before algorithm training and testing. This preprocessing ensured that the algorithms could effectively learn from the data and make accurate predictions.",
  "optimization/parameters": "The model utilized various parameters, both time-invariant and time-varying, to predict outcomes. Time-invariant parameters included the time to the first cigarette after waking up and the number of cigarettes smoked per day. These were recorded once per participant upon app download.\n\nTime-varying parameters were captured each time a participant self-initiated a craving feature entry. These included craving severity, feeling states (such as annoyance, anxiety, boredom, sadness, happiness, hunger, and loneliness), activities (like being at a bar, chatting, drinking alcohol, working, etc.), social context (being alone, with family, with friends, etc.), and temporal factors (time of day, day of the week, time since the previous craving entry, and whether the previous entry was a lapse).\n\nThe selection of these parameters was guided by their relevance to the craving and lapse behavior of participants. The time-varying nature of many parameters allowed for a dynamic modeling approach, capturing the fluctuating states and contexts that influence cravings and lapses. The specific number of parameters used in the model varied depending on the algorithm and the individual participant's data, as different algorithms utilize different equations and terms to estimate model parameters.",
  "optimization/features": "The study utilized a combination of time-invariant and time-varying explanatory variables as input features. Time-invariant variables included the time to the first cigarette after waking up and the number of cigarettes smoked per day, both recorded once per participant upon app download. Time-varying variables encompassed a wide range of factors captured during each craving feature entry. These included craving severity, various feeling states (such as annoyance, anxiety, boredom, sadness, happiness, hunger, and loneliness), activities (like being at a bar, chatting, drinking coffee, driving, etc.), social context (being alone, with colleagues, family, friends, etc.), and temporal factors (time of day, day of the week, time since the previous craving entry, and whether the previous entry was a lapse).\n\nFeature selection was not explicitly mentioned as a separate step in the methodology. However, the variables included in the models were carefully chosen based on their relevance to the study's objectives and the availability of data. The time-varying variables were designed to capture the context in which cravings occurred, ensuring that the most relevant factors were considered. The robustness of these variables was assessed through sensitivity and stability analyses, which helped in identifying the most important predictors for lapse incidence.\n\nThe training and testing of algorithms were performed using k-fold cross-validation with k set to 10, ensuring that the models were evaluated on different subsets of the data. This approach helped in minimizing overfitting and ensuring that the models generalized well to unseen data. The performance of the algorithms was evaluated using metrics such as accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (AUC), providing a comprehensive assessment of their predictive power.",
  "optimization/fitting": "The fitting method employed in this study utilized k-fold cross-validation with k set to 10. This approach ensured that each iteration, or fold, involved training the algorithms on 80% of the data and testing on the remaining 20%. This method helps to mitigate overfitting by ensuring that the model is evaluated on multiple subsets of the data, reducing the risk of the model becoming too tailored to the training data.\n\nTo further address overfitting, sensitivity analyses were conducted. These analyses involved excluding certain predictor variables, such as the \"prior event lapse,\" and evaluating the performance of the algorithms. The results showed that the best-performing algorithms, such as the Random Forest (RF) and Extreme Gradient Boosting (XGBoost), remained robust even when key predictors were excluded. This indicates that the models are not overly reliant on any single variable, reducing the likelihood of overfitting.\n\nUnderfitting was addressed by ensuring that the models were complex enough to capture the underlying patterns in the data. The use of advanced algorithms like RF, XGBoost, Penalized Logistic Regression, and Support Vector Machines (SVM) provided the necessary flexibility to model the relationships between predictors and outcomes accurately. Additionally, the performance metrics, such as the Area Under the Receiver Operating Characteristic Curve (AUC), were carefully evaluated to ensure that the models were not too simplistic.\n\nThe dataset itself was sufficiently large, with 37,002 craving entries from 791 participants, providing a robust foundation for training and testing the algorithms. This large sample size helped to ensure that the models were neither underfitted nor overfitted, as there were enough data points to capture the necessary patterns without becoming too specific to the training data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our algorithms. One key method was k-fold cross-validation, specifically with k set to 10. This approach involved splitting the dataset into 10 folds, where the model was trained on 80% of the data and tested on the remaining 20% in each iteration. This process was repeated 10 times, with each fold serving as the test set once, ensuring that the model's performance was evaluated on different subsets of the data.\n\nAdditionally, we used regularization techniques inherent in some of our algorithms. For instance, Penalized Logistic Regression incorporates regularization to prevent overfitting by adding a penalty term to the loss function, which helps to shrink the coefficients of less important features. Similarly, algorithms like Random Forest (RF) and Extreme Gradient Boosting (XGBoost) inherently provide regularization through mechanisms like tree pruning and controlling the complexity of the individual trees.\n\nWe also conducted sensitivity analyses to assess the robustness of our algorithms. These analyses involved varying the inclusion criteria for participants, such as changing the cutoff for the number of craving entries required for inclusion. For example, we compared the performance of our algorithms when including participants with at least 10 craving entries versus those with at least 20 craving entries. The results showed that the performance of our best-performing algorithms, such as RF and XGBoost, remained largely robust across these different criteria.\n\nFurthermore, we addressed the issue of imbalanced datasets by using random up- or down-sampling with replacement prior to algorithm training and testing. This technique helped to balance the number of lapse and non-lapse events, ensuring that our models were not biased towards the majority class.\n\nIn summary, our study utilized k-fold cross-validation, regularization techniques within specific algorithms, sensitivity analyses, and data balancing methods to prevent overfitting and ensure the reliability of our findings.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the supplementary materials. Specifically, details about the parameter values for the best-performing group-level algorithms are provided in Table S1. The variable importance for these algorithms is also illustrated in Figure S4. These supplementary materials are available alongside the main publication.\n\nThe optimization schedule and model files are not explicitly detailed in the main text or supplementary materials. However, the methods section provides a comprehensive overview of the algorithms used, including Random Forest (RF), Support Vector Machine (SVM), Penalized Logistic Regression, and Extreme Gradient Boosting (XGBoost). The study utilized k-fold cross-validation with k set to 10, where algorithms were trained on 80% and tested on the remaining 20% of the data for each iteration.\n\nRegarding the availability and licensing of the reported configurations and parameters, the supplementary materials are typically accessible to readers of the publication. However, specific details about the licensing of these materials are not provided in the given context. It is standard practice for academic publications to make supplementary materials freely available to readers, but the exact licensing terms would need to be verified through the publisher's policies.",
  "model/interpretability": "The models employed in this study are not entirely black-box, as efforts were made to ensure interpretability. The algorithms used, such as Random Forest (RF), Support Vector Machine (SVM), Penalized Logistic Regression, and Extreme Gradient Boosting (XGBoost), were selected for their relatively good interpretability compared to more complex approaches like deep learning. These algorithms provide insights into the importance of various predictor variables.\n\nFor instance, in the best-performing group-level RF algorithm, key predictor variables included whether the immediately preceding event was a lapse, craving severity, cigarettes smoked per day, time to first cigarette, and whether the person was alone. These variables were identified as influential in predicting lapses, offering a clear understanding of the factors driving the model's decisions.\n\nAdditionally, sensitivity and stability analyses, such as partial dependence plots, were recommended to ensure that predictor variables remain robust across different algorithm types. This approach helps in understanding how changes in predictor variables affect the model's output, thereby enhancing transparency.\n\nHowever, it is important to note that while these models offer some level of interpretability, they are not entirely transparent. The complexity of the algorithms means that the exact relationships between predictors and outcomes can still be somewhat opaque. Therefore, while the models provide valuable insights, they do not offer a complete, straightforward explanation of their decision-making processes.",
  "model/output": "The model employed in this study is a classification model. The primary objective was to predict binary outcomes, specifically whether a craving entry would result in a lapse (smoking) or not. This is evident from the use of metrics such as accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (AUC), which are all commonly used to evaluate the performance of classification models. The algorithms tested, including Random Forest (RF), Support Vector Machine (SVM), Penalized Logistic Regression, and Extreme Gradient Boosting (XGBoost), were all configured in classification mode. The goal was to minimize out-of-sample error, which is a typical aim in classification tasks to ensure the model can accurately classify outcome values in previously unseen samples.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a rigorous approach to ensure the robustness and generalizability of the algorithms. The primary technique used was k-fold cross-validation, specifically with k set to 10. This method involved splitting the dataset into 10 folds, where the algorithms were trained on 80% of the data and tested on the remaining 20% in each iteration. This process was repeated 10 times, with each fold serving as the test set once, ensuring that every data point was used for both training and testing.\n\nFor the hybrid group- and individual-level algorithms, a slightly different approach was taken. In this case, 20% of each individual\u2019s data was included in the training set, allowing the algorithms to leverage both group-level patterns and individual-specific data.\n\nThe performance of the algorithms was evaluated using several metrics. Initially, predicted and observed outcomes were compared to estimate algorithm accuracy, sensitivity, and specificity. Accuracy refers to the proportion of true positives and true negatives, sensitivity is the true positive rate, and specificity is the true negative rate. These metrics provide a comprehensive view of how well the algorithms can classify lapses.\n\nAdditionally, the area under the receiver operating characteristic curve (AUC) was calculated to evaluate algorithm performance. The AUC captures the trade-off between sensitivity and specificity, providing a single measure that summarizes overall algorithm performance. AUC estimates with confidence intervals that include 0.50 were considered unacceptable, as this value represents chance performance for a binary outcome.\n\nFor group-level algorithms, the AUC was the primary focus due to its ability to summarize performance effectively. For individual-level algorithms, estimates were compared against prespecified thresholds for acceptable accuracy (0.70), sensitivity (0.70), and specificity (0.50). The lower specificity threshold was chosen because missing a true positive (lapse) is more costly for a future Just-In-Time Adaptive Intervention (JITAI) than missing a true negative (non-lapse), as the former could set the individual on a trajectory towards full relapse.\n\nSensitivity analyses were also conducted to examine the robustness of the algorithms. These analyses involved varying the inclusion criteria for the analytic sample and excluding certain predictor variables to assess their impact on algorithm performance. The results of these analyses further validated the robustness of the best-performing algorithms.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our machine learning algorithms. The primary metric reported is the Area Under the Receiver Operating Characteristic Curve (AUC), which provides a single measure that balances sensitivity and specificity. This is particularly useful for summarizing the overall performance of group-level algorithms. The AUC estimates were accompanied by 95% confidence intervals (CIs) to assess the reliability of these estimates. An AUC estimate with a CI that includes 0.50 was considered unacceptable, as this value represents chance performance for a binary outcome.\n\nIn addition to the AUC, we also reported accuracy, sensitivity, and specificity. Accuracy refers to the proportion of true positives and true negatives correctly identified by the algorithm. Sensitivity, or the true positive rate, indicates the algorithm's ability to correctly identify lapse events. Specificity, or the true negative rate, measures the algorithm's ability to correctly identify non-lapse events. For individual-level algorithms, we compared these estimates against prespecified thresholds for acceptable performance: accuracy and sensitivity of 0.70, and specificity of 0.50. These thresholds were chosen based on the clinical significance of missing a true positive (lapse) event, which could potentially lead to full relapse.\n\nThe set of metrics used in this study is representative of standard practices in the literature for evaluating machine learning algorithms, particularly in the context of binary classification problems. The AUC is widely recognized as a robust metric for evaluating model performance, as it provides a comprehensive assessment of the trade-off between sensitivity and specificity. The inclusion of accuracy, sensitivity, and specificity ensures that the performance of the algorithms is evaluated from multiple perspectives, providing a more nuanced understanding of their strengths and limitations. This approach aligns with established guidelines for reporting performance metrics in machine learning studies, ensuring that our results are comparable with other research in the field.",
  "evaluation/comparison": "A comparison to simpler baselines was performed. The study evaluated the performance of various algorithms, including Random Forest (RF), Support Vector Machine (SVM), Penalized Logistic Regression, and Extreme Gradient Boosting (XGBoost). These algorithms were compared based on their ability to classify lapses, with performance metrics such as accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (AUC) being used for evaluation.\n\nThe study aimed to identify the best-performing group-level algorithm and evaluate its ability to classify lapses for out-of-sample observations. Additionally, individual-level algorithms were developed and evaluated for their performance in classifying lapses for out-of-sample individual observations. A hybrid algorithm, combining group- and individual-level approaches, was also assessed for its performance.\n\nThe comparison involved using k-fold cross-validation, with k set to 10, to ensure robust evaluation. For each iteration, algorithms were trained on 80% of the data and tested on the remaining 20%. This approach helped in minimizing the out-of-sample error and balancing the in-sample and out-of-sample errors.\n\nThe study also conducted sensitivity analyses to examine the robustness of the algorithms. For instance, the performance of the algorithms was evaluated when excluding certain predictor variables or when using different cutoffs for inclusion in the analytic sample. These analyses provided insights into the stability and reliability of the algorithms under varying conditions.\n\nOverall, the comparison to simpler baselines and the use of rigorous evaluation methods ensured that the study's findings were reliable and generalizable. The performance metrics and sensitivity analyses highlighted the strengths and weaknesses of the different algorithms, providing a comprehensive evaluation of their effectiveness in classifying lapses.",
  "evaluation/confidence": "The evaluation of the algorithms included the calculation of confidence intervals for the performance metrics. Specifically, the area under the receiver operating characteristic curve (AUC) estimates were accompanied by 95% confidence intervals (CIs). These intervals provide a range within which the true AUC value is expected to lie with 95% confidence, offering a measure of the reliability of the AUC estimates.\n\nThe results indicated that the best-performing group-level algorithm was a Random Forest (RF) algorithm with an AUC of 0.969 and a 95% CI of 0.961 to 0.978. This was closely followed by an Extreme Gradient Boosting (XGBoost) algorithm with an AUC of 0.966 and a 95% CI of 0.958 to 0.975. Other algorithms, such as Penalized Logistic Regression and Support Vector Machine (SVM), also showed strong performance with AUCs of 0.952 and 0.947, respectively, and their corresponding confidence intervals.\n\nFor individual-level algorithms, the performance metrics were compared against prespecified thresholds for acceptable accuracy (0.70), sensitivity (0.70), and specificity (0.50). The median AUC for participants\u2019 best-performing algorithms was 0.938, with a range from 0.518 to 1.000. This indicates that the individual-level algorithms generally performed well, with a significant proportion of participants achieving high AUC values.\n\nThe statistical significance of the results was assessed through the use of confidence intervals. AUC estimates with CIs that include 0.50 were considered unacceptable, as this value represents chance performance for a binary outcome. The confidence intervals for the top-performing algorithms did not include 0.50, suggesting that the algorithms' performance was statistically significant and superior to random guessing.\n\nIn summary, the performance metrics included confidence intervals, and the results were statistically significant. This provides a strong basis for claiming that the methods are superior to others and baselines.",
  "evaluation/availability": "Not enough information is available."
}