{
  "publication/title": "Microbiome Data Enhances Predictive Models of Lung Function in People With Cystic Fibrosis.",
  "publication/authors": "Zhao CY, Hao Y, Wang Y, Varga JJ, Stecenko AA, Goldberg JB, Brown SP",
  "publication/journal": "The Journal of infectious diseases",
  "publication/year": "2021",
  "publication/pmid": "33330902",
  "publication/pmcid": "PMC8355460",
  "publication/doi": "10.1093/infdis/jiaa655",
  "publication/tags": "- Cystic Fibrosis\n- Microbiome\n- Lung Function\n- 16S rRNA Sequencing\n- Machine Learning\n- ElasticNet\n- Sputum Analysis\n- Pulmonary Function\n- Microbiology\n- Clinical Microbiology\n- Pathogen Detection\n- Nonpathogen Taxa\n- Predictive Modeling\n- Bioinformatics\n- Respiratory Health\n- Chronic Infections\n- Microbiome Composition\n- Patient Metadata\n- Shannon Diversity\n- Bray-Curtis Distances",
  "dataset/provenance": "The dataset used in this study was derived from sputum samples collected from cystic fibrosis (CF) patients attending the Children\u2019s Healthcare of Atlanta and Emory University CF Care Center between January 2015 and August 2016. The study involved 77 CF patients, from whom expectorated sputum samples were obtained. Deidentified patient information, including age, sex, height, body mass index (BMI), cystic fibrosis transmembrane conductance regulator (CFTR) genotype, degree of glucose control (HbA1c), and percent predicted forced expiratory volume in 1 second (ppFEV1), was also collected.\n\nThe sputum samples were processed and analyzed using 16S rRNA gene sequencing to characterize the microbiome composition. The 16S V4 region was amplified and sequenced using Illumina MiSeq, yielding an average of 137,708 sequences per sample. The sequences were quality-filtered and analyzed using the QIIME2 debur plugin. Taxonomic assignments were made against the SILVA and Greengenes 16S reference databases.\n\nTo address the dimensionality problem due to high between-feature correlations and a relatively small number of independent patient observations, the microbiome analysis was restricted to the top 23 genera, which encompassed 97% of the total sequenced reads. Additional summary statistics, such as the percentage of pathogens, oral taxa, and Shannon diversity, were calculated. The 16S data were center-log transformed to mitigate compositional effects, and total bacterial load was incorporated as a predictor. The final dataset, referred to as the All Features dataset, combined metadata and 16S data.\n\nThe study aimed to assess whether nonpathogenic taxa contain informative biomarkers for predicting lung function in CF patients. The dataset was split into training and testing samples, with ElasticNet used to train predictive models while performing feature selection. The performance of models trained on different feature subsets, including CF pathogens, all 16S data, metadata, and metadata plus pathogens, was compared. The results indicated that the addition of nonpathogen data improved the prediction of patient lung function compared to established pathogen data alone.",
  "dataset/splits": "We employed a simple 70:30 train-test holdout split for our dataset, resulting in two primary data splits. The training set consisted of 53 samples, while the test set comprised 24 samples. This split was used to train and evaluate our machine learning models.\n\nIn addition to the train-test split, we generated four additional feature subsets to assess the contribution of different types of data to model performance. These subsets included:\n\n1. **CF Pathogens**: This subset focused solely on the canonical cystic fibrosis (CF) pathogens.\n2. **All 16S Data**: This subset included all 16S sequencing data, encompassing both pathogen and nonpathogen taxa.\n3. **Metadata**: This subset contained only patient metadata, such as age, body mass index (BMI), and other clinical information.\n4. **Metadata + Pathogens**: This subset combined patient metadata with pathogen quantitation data.\n\nThese subsets allowed us to compare the performance of models trained on different combinations of data, providing insights into the relative importance of each data type in predicting lung function.",
  "dataset/redundancy": "The datasets were split into training and testing sets to assess the performance of machine learning models. Specifically, 53 samples were used for training, while 24 samples were set aside for testing. This split was done randomly to ensure that the training and test sets were independent. The independence of the sets was enforced by withholding the test set samples entirely during the training process, ensuring that the models were not exposed to these samples until the evaluation phase.\n\nThe distribution of the datasets used in this study is notable for its relatively small size compared to some previously published machine learning datasets. With only 77 patient samples in total, the dataset is constrained by the number of independent patient observations available. This limitation is further compounded by the high dimensionality of the data, which includes 86 initial predictors, such as bacterial taxa and patient metadata. To mitigate the dimensionality problem, the analysis was restricted to the top 23 genera, which encompassed 97% of the total sequenced reads. Additionally, summary statistics like the percentage of pathogens, oral taxa, and Shannon diversity were calculated to reduce the complexity of the data.\n\nThe use of leave-one-out cross-validation and 1000-fold bootstrap resampling further ensured the robustness and generalizability of the models. These techniques helped to assess overfitting and provided a comprehensive evaluation of model performance across different feature subsets. The models trained on the full sample set consistently showed lower prediction errors compared to those trained on subsets, highlighting the importance of including all available data for accurate predictions.",
  "dataset/availability": "The data generated and analyzed during the study has been made publicly available. The nucleotide sequences have been uploaded to the BioProject database under the accession number PRJNA666192. This database is a public repository for genomic data, ensuring that the data is accessible to the scientific community for further research and validation.\n\nThe data release was enforced by adhering to the guidelines and policies of the BioProject database, which require that all submitted data be made publicly available. This ensures that the data can be accessed and used by other researchers, promoting transparency and reproducibility in scientific research.\n\nThe data splits used in the study, such as the training and testing sets for the machine learning models, are not explicitly detailed in the public forum. However, the methods used to split the data, including the use of a 70:30 train-test holdout and leave-one-out cross-validation, are described in the publication. This allows other researchers to replicate the data splits if needed.\n\nThe data is released under the terms of the BioProject database, which typically allows for open access and use of the data for research purposes, subject to proper citation and acknowledgment of the original study. This ensures that the data can be used ethically and responsibly by the scientific community.",
  "optimization/algorithm": "The machine-learning algorithm class used is regularized linear models, specifically ElasticNet. ElasticNet is not a new algorithm; it is a well-established method that combines the penalties of Lasso (L1) and Ridge (L2) regression. This combination helps in selecting informative features while limiting overfitting by penalizing nonzero coefficients.\n\nThe reason ElasticNet was not published in a machine-learning journal is that it has been extensively studied and validated in the literature since its introduction. It is a widely used technique in various fields, including bioinformatics, for its ability to handle high-dimensional data and perform feature selection effectively. Given its established reputation and widespread use, there was no need to publish it in a machine-learning journal within the context of this study. Instead, the focus was on applying ElasticNet to a specific dataset to predict lung function in cystic fibrosis patients, integrating microbiome and patient metadata.",
  "optimization/meta": "The models employed in this study do not use data from other machine-learning algorithms as input. Instead, they utilize a combination of patient metadata and microbiome data. The machine learning approach used is ElasticNet, which is a regularized regression method that combines the penalties of Lasso (L1) and Ridge (L2) regression. This method was chosen for its ability to perform both variable selection and regularization to enhance the prediction accuracy and interpretability of the model.\n\nThe ElasticNet models were trained using various input datasets, including patient metadata, 16S pathogen quantitation, and other 16S taxa. The models were evaluated using different strategies, such as 1000-fold bootstrapping and leave-one-out cross-validation, to generate prediction error ranges across feature subsets. These strategies help in assessing the robustness and generalizability of the models.\n\nTo ensure the independence of training data, the samples were split into training and testing sets. Specifically, 53 samples were used for training, and 24 samples were withheld as a test set. This split helps in evaluating the model's performance on unseen data and prevents overfitting. Additionally, leave-one-out cross-validation was employed to further validate the model's performance by assessing prediction errors across different folds.\n\nThe models were also standardized to allow for cross-feature comparability, ensuring that each feature contributes equally to the prediction. This standardization process involves transforming the data to have a mean of zero and a standard deviation of one.\n\nIn summary, the models used in this study are not meta-predictors but rather standalone ElasticNet regression models trained on integrated datasets of patient metadata and microbiome data. The training data independence is maintained through careful splitting of samples into training and testing sets, along with the use of cross-validation techniques.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the data was suitable for training robust models. Initially, the microbiome analysis was restricted to the top 23 genera, which encompassed 97% of the total sequenced reads. This reduction helped mitigate the dimensionality problem caused by the high number of initial predictors relative to the number of patient observations.\n\nThree additional summary statistics were calculated: percentage of pathogens, percentage of oral taxa, and Shannon diversity. These statistics provided a more comprehensive view of the microbiome composition. The clustering analysis showed reasonable agreement between clinical microbiology detection and rDNA sequencing, leading to the exclusion of binary detection results in favor of 16S quantitation.\n\nTo address the compositionality of 16S data, total bacterial load was incorporated as a predictor. This was measured using a universal 16S primer quantitative polymerase chain reaction (qPCR). A centered log-ratio transform was applied to the genus-level relative abundance data to stabilize variance and reduce spurious correlations. This transformed data was then standardized to have a mean of zero and unit variance, ensuring that all features contributed equally to the model.\n\nThe final combination of metadata and 16S data was referred to as the All Features dataset. This dataset included various patient metadata such as age, BMI, and clinical microbiology results, along with the processed microbiome data. The data was split into training and testing sets, with 53 samples used for training and 24 samples reserved for testing. This split allowed for the evaluation of model performance on unseen data.\n\nAdditionally, within-feature shuffling was used to create a noninformative control dataset from the All Features set. This shuffling preserved the mean zero, unit variance within-feature structure while scrambling between-feature correlations, providing a baseline for comparing model performance.",
  "optimization/parameters": "In our study, we initially faced a dimensionality problem due to the high number of available predictors (86 total, including 59 bacterial taxa) compared to the number of independent patient observations (n = 77). To mitigate this, we restricted our microbiome analysis to the top 23 genera, which encompassed 97% of the total sequenced reads. Additionally, we calculated three summary statistics: percentage of pathogens, percentage of oral taxa, and Shannon diversity. We also incorporated total bacterial load as a predictor to address the compositionality of 16S data. Furthermore, we used a centered log-ratio transform on our genus-level relative abundance data before standardizing to mean zero and unit variance. This final combination of metadata and 16S data is referred to as our All Features dataset.\n\nWe employed ElasticNet, a regularized linear modeling technique that combines the penalties of LASSO (L1) and Ridge (L2) regression, to fit our models. This method helps in feature selection and limits overfitting by penalizing nonzero coefficients. The number of parameters (p) used in the model was determined through this feature selection process, where ElasticNet automatically selects the most relevant features from the input dataset.\n\nTo assess the robustness of our models, we used 1000-fold bootstrapping and leave-one-out cross-validation (LOOCV). These techniques helped us to generate prediction error ranges across different feature subsets and to identify key metadata and taxa that were robustly selected across the ensemble of models. The final models were trained on standardized input datasets, ensuring cross-feature comparability.",
  "optimization/features": "In the optimization process, we utilized a comprehensive set of input features to train our machine learning models. Initially, we had 86 total predictors, including 59 bacterial taxa. To mitigate the dimensionality problem, we restricted our microbiome analysis to the top 23 genera, which encompassed 97% of the total sequenced reads. Additionally, we calculated three summary statistics: percentage of pathogens, percentage of oral taxa, and Shannon diversity. We also incorporated total bacterial load, measured by universal 16S primer quantitative polymerase chain reaction (qPCR), to address the compositionality of 16S data. Furthermore, we used a centered log-ratio transform on our genus-level relative abundance data before standardizing to mean zero and unit variance inputs. This final combination of metadata and 16S data is referred to as our All Features dataset.\n\nFeature selection was performed using ElasticNet, a regularized linear model that employs a weighted average of L1 (LASSO) and L2 (ridge regression) penalties. This method helps to limit overfitting by penalizing nonzero coefficients. The feature selection process was conducted using the training set only, ensuring that the selected features were not influenced by the test set. This approach allowed us to identify key metadata and taxa that were robustly selected across the ensemble of models, enhancing the generalizability and robustness of our predictions.",
  "optimization/fitting": "The number of parameters initially considered was indeed much larger than the number of training points. We started with 86 predictors, including 59 bacterial taxa, for 77 patient observations. To mitigate this dimensionality problem, we restricted our microbiome analysis to the top 23 genera, which encompassed 97% of the total sequenced reads. Additionally, we calculated three summary statistics: % pathogen, % oral taxa, and Shannon diversity. We also incorporated total bacterial load as a predictor to address the compositionality of 16S data.\n\nTo rule out overfitting, we employed several strategies. We used ElasticNet, which combines L1 and L2 penalties to limit overfitting by penalizing nonzero coefficients. We also performed 1000-fold bootstrap resampling to fit an ensemble of models, ensuring that robust features selected by the baseline model were also selected by a large portion of the bootstrapped ensemble. Furthermore, we used leave-one-out cross-validation to assess overfitting by comparing the prediction error across folds against the test set error. The results showed no significant differences between cross-validated model errors across our training sets, suggesting that our models were not overfitting.\n\nTo address underfitting, we ensured that our models were complex enough to capture the underlying patterns in the data. We included a variety of features, such as patient metadata (age, BMI, etc.), microbiome composition, and clinical microbiology results. Additionally, we compared the performance of models trained on different feature subsets, including CF Pathogens, All 16S Data, Metadata, and Metadata + Pathogens. The models trained on all of the data showed lower error compared to other feature subsets, indicating that our models were not underfitting. Moreover, all models using patient metadata or microbiome data outperformed the negative control, further supporting the adequacy of our model complexity.",
  "optimization/regularization": "To prevent overfitting in our machine learning models, we employed several techniques. We used ElasticNet, a regularized linear regression model that combines L1 (LASSO) and L2 (ridge regression) penalties. This method helps to limit overfitting by penalizing nonzero coefficients, effectively performing feature selection and regularization simultaneously.\n\nAdditionally, we utilized 1000-fold bootstrapping to generate an ensemble of models. This approach allowed us to obtain distributions for each model coefficient and identify key metadata and taxa that were robustly selected across the ensemble. By doing so, we ensured that the features selected were consistent and not due to random noise in the data.\n\nWe also assessed model generalizability using leave-one-out cross-validation. This method involves training the model on all but one sample and then testing it on the left-out sample. By repeating this process for each sample, we could compare the prediction error across folds against the test set error, providing a robust measure of model performance and generalizability.\n\nFurthermore, we standardized all input features to have a mean of zero and a standard deviation of one. This preprocessing step allowed for better between-feature interpretability and helped in mitigating the effects of features with different scales, which can sometimes lead to overfitting.\n\nLastly, we created a noninformative negative control dataset by shuffling the features within the full dataset. This control helped us to compare the performance of our models against a baseline that had no meaningful structure, ensuring that the improvements observed were due to the actual informative content in the data rather than artifacts of the modeling process.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in this study are not black-box but rather transparent, as they utilize regularized linear models, specifically ElasticNet. This approach allows for interpretability by providing coefficient ranges that indicate the importance and direction of each feature in predicting lung function.\n\nFor instance, both train/test and bootstrapped models consistently selected Pseudomonas and Achromobacter as negative predictors of lung function. This means that the presence of these pathogens is associated with worse lung function outcomes. Conversely, taxa such as Haemophilus, Fusobacterium, and Rothia, along with metadata features like age, BMI, and diabetic status, were identified as positive predictors. This indicates that higher abundances of these taxa or certain metadata values are associated with better lung function.\n\nThe models also highlighted the significance of nonpathogen taxa, showing that their inclusion significantly improved prediction accuracy. This suggests that these taxa provide valuable predictive information, potentially serving as biomarkers or even indicating underlying health mechanisms.\n\nAdditionally, the models demonstrated consistency in feature selection across different training sets, reinforcing the reliability of the identified predictors. The use of leave-one-out cross-validation further ensured that the models were not overfitting, providing robust and generalizable results.",
  "model/output": "The model employed in our study is a regression model. Specifically, we used ElasticNet, a type of regularized linear regression, to predict lung function, measured by ppFEV1. This approach allows us to handle high-dimensional data by performing feature selection and regularization, which helps in mitigating overfitting. The model was trained using various input datasets, including patient metadata, microbiome composition, and clinical microbiology results. We assessed model performance using mean squared error (MSE) and employed techniques such as 1000-fold bootstrapping and leave-one-out cross-validation to ensure robustness and generalizability. The model's output provides predictions of lung function based on the input features, with consistent selection of key predictors across different training sets.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the machine learning models involved several strategies to ensure robustness and generalizability. We employed leave-one-out cross-validation to assess overfitting, comparing the prediction error across folds against the test set error. This method helps in understanding how well the model performs on unseen data. Additionally, we used 1000-fold bootstrap resampling to fit both a baseline and an ensemble of models. This approach allowed us to identify robust features that were consistently selected across multiple model iterations. By standardizing all features to have a mean of zero and unit variance, we ensured cross-feature comparability. Furthermore, we generated a noninformative control dataset by shuffling within-feature data, which helped in preserving the mean zero, unit variance structure while scrambling between-feature correlations. This control dataset was used to compare against our models to ensure that the predictive power was not due to random chance. The results of these evaluations showed that models using patient metadata or microbiome data outperformed the negative control, indicating the effectiveness of our feature selection and model training processes.",
  "evaluation/measure": "In our study, we primarily focused on mean squared error (MSE) as our key performance metric to evaluate the predictive accuracy of our machine learning models. This metric was chosen because it provides a clear indication of the average squared difference between predicted and actual values, which is crucial for assessing the performance of regression models aimed at predicting continuous outcomes like lung function.\n\nWe employed two main strategies to generate prediction error ranges: 1000-fold bootstrapping and leave-one-out cross-validation (LOOCV). These methods allowed us to assess the robustness and generalizability of our models. By using bootstrapping, we could generate a distribution of MSE values, providing a more comprehensive understanding of model performance variability. LOOCV, on the other hand, ensured that each data point was used once as a validation set, offering a rigorous test of model performance.\n\nOur models were trained using various input datasets, including patient metadata, microbiome data, and combinations thereof. We found that models trained on all available data consistently showed lower error compared to those trained on subsets of features. This indicates that integrating multiple data sources enhances predictive accuracy.\n\nAdditionally, we compared the performance of models trained on all 16S data versus those using only 16S pathogen quantitation. The results showed that models trained on all 16S data significantly outperformed those using only pathogen quantitation, highlighting the importance of including nonpathogen taxa in predictive models.\n\nTo ensure that our models were not overfitting, we obtained ranges of model errors using LOOCV and found no significant differences between cross-validated model errors across our training sets. This suggests that our models were robust and not overly reliant on specific data points.\n\nIn summary, our use of MSE as the primary performance metric, along with bootstrapping and LOOCV for error estimation, provides a thorough evaluation of model performance. These metrics are representative of standard practices in the literature for assessing the predictive accuracy of regression models in biomedical research.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, we focused on evaluating the performance of our machine learning models using various feature subsets and validation techniques.\n\nWe did, however, compare our models to simpler baselines to assess their performance. Specifically, we generated a noninformative control dataset from the All Features set using within-feature shuffling. This control dataset preserved the mean zero, unit variance within-feature structure but scrambled between-feature correlations. All models using patient metadata or microbiome data outperformed this negative control, indicating that our models captured meaningful patterns in the data.\n\nAdditionally, we compared models trained on different feature subsets, including metadata-only, microbiome-only, and combined metadata and microbiome data. This comparison allowed us to assess the contribution of each data type to the predictive performance of the models. We found that the combined model achieved greater performance, suggesting that integrating both metadata and microbiome data provides a more comprehensive predictive framework.\n\nWe also evaluated model generalizability using leave-one-out cross-validation and compared the prediction error across folds against the test set error. This approach helped us to assess overfitting and ensure that our models were robust and generalizable to new data.",
  "evaluation/confidence": "The evaluation of our models involved rigorous statistical methods to ensure the confidence and significance of our results. We employed 1000-fold bootstrapping and leave-one-out cross-validation (LOOCV) to generate prediction error ranges across various feature subsets. This approach allowed us to assess the robustness and generalizability of our models.\n\nThe mean squared error (MSE) was used as the primary performance metric, and we reported the ranges of MSE across different models. This provided a clear indication of the variability and reliability of our predictions. The use of bootstrapping helped in understanding the distribution of model coefficients, identifying key features that were consistently selected across multiple iterations.\n\nStatistical significance was a crucial aspect of our evaluation. We found that models trained on all 16S data significantly outperformed those using only 16S pathogen quantitation (P < .01, t test). This indicates that the inclusion of nonpathogenic taxa significantly improves model performance. Additionally, the combined model, which included both metadata and microbiome data, achieved greater performance than models using either data type alone. This suggests a synergistic effect when integrating multiple data sources.\n\nThe consistency in feature selection between our baseline and bootstrapped models further supports the reliability of our findings. Features such as Pseudomonas, Achromobacter, age, and diabetic status were consistently selected as negative predictors, while Haemophilus, Fusobacterium, Rothia, oral taxa abundance, and BMI were positive predictors. This consistency across different modeling strategies enhances our confidence in the identified predictors.\n\nMoreover, we did not find significant differences between cross-validated model errors across our training sets, suggesting that our models were not overfitting despite the difference in the number of available predictors. This is a critical point in ensuring that our models are robust and can generalize well to new, unseen data.\n\nIn summary, the evaluation of our models included comprehensive statistical analyses that provided confidence intervals for performance metrics and demonstrated statistical significance in model comparisons. These rigorous evaluations support the claim that our method is superior to others and baselines, particularly in the context of predicting lung function in cystic fibrosis patients.",
  "evaluation/availability": "Not enough information is available."
}