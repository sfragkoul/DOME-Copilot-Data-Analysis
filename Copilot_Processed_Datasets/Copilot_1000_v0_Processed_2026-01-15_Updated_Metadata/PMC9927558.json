{
  "publication/title": "G-RANK: an equivariant graph neural network for the scoring of protein-protein docking models.",
  "publication/authors": "Kim HY, Kim S, Park WY, Kim D",
  "publication/journal": "Bioinformatics advances",
  "publication/year": "2023",
  "publication/pmid": "36818727",
  "publication/pmcid": "PMC9927558",
  "publication/doi": "10.1093/bioadv/vbad011",
  "publication/tags": "- Structural bioinformatics\n- Protein-protein interactions\n- Protein complex structure prediction\n- Computational docking\n- Scoring functions\n- Machine learning\n- Graph neural networks\n- Equivariant graph neural networks\n- Protein design\n- Drug discovery",
  "dataset/provenance": "The datasets used in our study are sourced from the DeepRank project, which are publicly available at https://data.sbgrid.org/dataset/843/. We utilized two main datasets: the BM5 cross-validation dataset and the CAPRI score set.\n\nThe BM5 cross-validation dataset was originally created by the authors of DeepRank, who conducted docking using HADDOCK version 2.2 on the docking benchmark dataset version 5 (BM5). This dataset consists of 142 non-redundant complexes, excluding antibody\u2013antigen complexes and those involving more than two chains. The dataset was used for 10-fold cross-validation, where 15 complexes are left out as the test set, and the remaining complexes are split into training and validation sets. This dataset has been previously used by the authors of DeepRank-GNN, ensuring a fair comparison.\n\nThe CAPRI score set is an independent test dataset consisting of more than 16,000 docking models for 13 protein\u2013protein complexes. These models were generated using various docking methods by over 40 research groups. After excluding one complex with no positive samples, the dataset includes 12 complexes. This dataset has also been used by the DeepRank authors and is widely recognized in the community for evaluating docking predictions.\n\nIn summary, both datasets are well-established in the field and have been used in previous studies, providing a robust foundation for our model evaluations.",
  "dataset/splits": "In our study, we utilized two primary datasets for training and evaluating our model: the BM5 cross-validation dataset and the independent test dataset.\n\nThe BM5 cross-validation dataset was derived from the docking benchmark dataset version 5 (BM5). It consists of 142 non-redundant protein-protein complexes, excluding antibody\u2013antigen complexes and those involving more than two chains. This dataset was used for 10-fold cross-validation. In each fold, 15 complexes were left out as the test set, while 102 and 25 complexes were assigned to the training and validation sets, respectively. This results in a distribution where each fold has a different set of 15 complexes as the test set, ensuring a comprehensive evaluation of the model's performance.\n\nThe independent test dataset, known as the CAPRI score set, consists of more than 16,000 docking models for 13 protein\u2013protein complexes. After excluding one complex containing no positive samples, 12 complexes remained. This dataset was used to assess the model's performance on an independent set of data, providing a robust evaluation of its generalization capabilities.\n\nIn summary, the BM5 dataset is split into 10 folds for cross-validation, with each fold containing 102 training complexes, 25 validation complexes, and 15 test complexes. The CAPRI score set serves as an independent test dataset with 12 complexes.",
  "dataset/redundancy": "The datasets used in our study were carefully curated to ensure independence between training and test sets, which is crucial for a fair evaluation of model performance.\n\nFor the BM5 cross-validation dataset, we utilized the same dataset as DeepRank-GNN to ensure a fair comparison. This dataset was originally created by the authors of DeepRank, who conducted docking using HADDOCK version 2.2 on the docking benchmark dataset version 5 (BM5). From the 232 non-redundant complexes obtained, antibody\u2013antigen complexes and those involving more than two chains were excluded, resulting in 142 complexes. This dataset was split into 10 folds for cross-validation. In each fold, 15 complexes were left out as the test set, while 102 and 25 complexes were assigned to the training and validation sets, respectively. This splitting ensures that the training and test sets are independent, as no complex appears in both sets within the same fold.\n\nFor the independent test dataset, we used the CAPRI score set, which consists of more than 16,000 docking models for 13 protein\u2013protein complexes. This dataset was generated using various docking methods by over 40 research groups. We excluded one complex containing no positive samples, resulting in 12 complexes for evaluation. This dataset is independent of the BM5 dataset, ensuring that the models are evaluated on unseen data.\n\nThe distribution of our datasets compares favorably to previously published machine learning datasets in the field of protein-protein docking. The BM5 dataset, with its 10-fold cross-validation, provides a robust framework for training and evaluating models. The CAPRI score set, with its diverse set of docking models generated by multiple methods, offers a challenging and independent test bed for assessing model performance. This approach ensures that our models are evaluated on data that is both independent and representative of the complexity found in real-world protein-protein docking scenarios.",
  "dataset/availability": "The data used in our study is publicly available and was made accessible by the authors of DeepRank. It can be accessed at the following URL: https://data.sbgrid.org/dataset/843/. The datasets include the BM5 cross-validation dataset and the independent test dataset known as the CAPRI score set. These datasets were used to evaluate the performance of our model, G-RANK, and to ensure a fair comparison with other methods such as DeepRank-GNN.\n\nThe BM5 cross-validation dataset consists of 142 non-redundant protein-protein complexes, excluding antibody-antigen complexes and those involving more than two chains. This dataset was used for 10-fold cross-validation, where 15 complexes were left out as the test set for each fold, with 102 and 25 complexes assigned to the training and validation sets, respectively.\n\nThe CAPRI score set includes more than 16,000 docking models for 13 protein-protein complexes, generated by various docking methods from over 40 research groups. One complex containing no positive samples was excluded, resulting in 12 complexes for evaluation.\n\nThe data is available for public use, allowing other researchers to replicate and build upon our findings. The specific details on the dataset generation and composition can be found in the supplementary materials provided with the original DeepRank publication.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages a type of neural network known as an equivariant graph neural network (EGNN). Specifically, we utilized a subtype called the geometric vector perceptron\u2013graph neural network (GVP-GNN). This class of algorithms is relatively new and has shown promise in various 3D molecular structure modeling tasks.\n\nThe choice to use GVP-GNN in our work was driven by its unique properties, particularly its equivariance to transformations such as translation, rotation, and reflection in 3D Euclidean space. This equivariance is crucial for recognizing proteins in different positions and orientations, making it highly suitable for the scoring of protein-protein docking models.\n\nRegarding the publication venue, our focus was on the application of this neural network architecture to structural bioinformatics, specifically in the context of protein-protein docking. While the GVP-GNN architecture itself is innovative, its application to this particular problem domain is what we aimed to highlight. Therefore, publishing in a bioinformatics journal allowed us to reach the relevant audience and demonstrate the practical benefits of this approach in a field where it can have significant impact.",
  "optimization/meta": "The model described in this publication is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it is a standalone method called G-RANK, which is based on a geometric vector perceptron\u2013graph neural network (GVP-GNN). This type of neural network is a subtype of equivariant graph neural networks (EGNNs), which are designed to be equivariant to transformations such as translation, rotation, and reflection in 3D Euclidean space. This property is particularly useful for recognizing proteins in different positions and orientations.\n\nThe model was trained using a specific dataset, the BM5 10-fold cross-validation dataset, and evaluated on an independent test dataset known as the CAPRI score set. The training process involved using a batch size of 64, a learning rate of 0.0001, and the Adam optimizer with mean squared error as the loss function. The model was trained for 50 epochs, with the best-performing model (based on validation loss) being retained for final evaluation. This approach ensures that the training data is independent and that the model's performance can be fairly assessed on unseen data.",
  "optimization/encoding": "In our study, the data encoding and preprocessing were carefully designed to facilitate the effective training of our GVP-GNN-based model, G-RANK. The graph representation of protein-protein complexes was constructed solely on the interface of the complexes, rather than the entire structure. This interface was determined using pdb2sql with a cutoff distance of 8.5 \u00c5. This approach allowed us to focus on the critical regions of interaction, enhancing the model's ability to discern relevant features.\n\nThe atoms considered in the model inputs were limited to five types: carbon (C), nitrogen (N), oxygen (O), sulfur (S), and hydrogen (H). This selection was based on their significance in protein structures and interactions. The Protein Data Bank (PDB) files were converted into the LMDB data format using the ATOM3D package. This conversion was essential for preparing the data in a format compatible with our machine-learning pipeline.\n\nThe initial node embeddings were represented using one-hot encoding of the atom types. This encoding method ensures that each atom type is uniquely identifiable, providing a clear and distinct input for the model. The edge embeddings were computed from the atom coordinates and consisted of two components: the Gaussian radial basis function encoding of the edge length and the unit vector in the edge direction. These embeddings capture the spatial relationships and orientations between atoms, which are crucial for understanding the 3D structure of protein complexes.\n\nThe preprocessing steps ensured that the data was in a suitable format for the GVP-GNN architecture. The node and edge embeddings were passed through initial GVP layers before being fed into the GVPConvLayer blocks. This preprocessing allowed the model to effectively learn from the encoded data, leading to improved performance in scoring protein-protein docking models.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "The input features for our model are derived from the interface of protein-protein complexes, focusing on specific atoms. The atoms included in the model inputs are Carbon (C), Nitrogen (N), Oxygen (O), Sulfur (S), and Hydrogen (H). These atoms are selected based on their relevance to the protein-protein interactions being modeled.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the selection of these atoms was guided by domain knowledge and the specific requirements of the geometric vector perceptron (GVP) layers used in our model. The interface was determined using pdb2sql with a cutoff of 8.5 \u00c5, ensuring that only relevant contact atoms were included. Models for which no contact atoms were found were excluded from the analysis.\n\nThe process of determining the interface and selecting the relevant atoms was conducted independently of the training set. This approach ensures that the model's performance is not biased by the training data, maintaining the integrity of the validation and test sets. The ATOM3D package was used to convert the PDB files into the LMDB data format required for model inputs, facilitating the integration of these features into our training pipeline.",
  "optimization/fitting": "The fitting method employed for the models involved several key strategies to address potential overfitting and underfitting issues.\n\nThe models were trained using a batch size of 64 and a learning rate of 0.0001, which are standard choices that help in stabilizing the training process and preventing overfitting. The models were trained for 50 epochs, and the model with the lowest validation loss was retained for final evaluation. This approach ensures that the model generalizes well to unseen data by selecting the best-performing model based on validation performance.\n\nTo further mitigate overfitting, techniques such as dropout and layer normalization were incorporated into the model architecture. Dropout randomly sets a fraction of input units to zero at each update during training time, which helps prevent overfitting. Layer normalization normalizes the inputs across the features, stabilizing the learning process and improving generalization.\n\nAdditionally, the use of an ensemble method, where predictions from all models across 10 folds were averaged, helped in reducing the variance and improving the robustness of the predictions. This ensemble approach is effective in enhancing the model's performance and generalization capabilities.\n\nThe models were evaluated using multiple assessment metrics, including ROC-AUC and PR-AUC, which provide a comprehensive view of the model's performance. The hit rate and success rate metrics were also used to assess the model's ability to identify near-native models among a large pool of candidates. These metrics ensure that the model is not only performing well on average but also excelling in critical tasks.\n\nOverall, the combination of these strategies\u2014including careful selection of hyperparameters, use of regularization techniques, and ensemble methods\u2014ensured that the models were neither overfitting nor underfitting the data. The consistent performance across different datasets and metrics further validates the effectiveness of the fitting method.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One key approach was the use of dropout layers within our neural network architecture. Dropout randomly sets a fraction of input units to zero at each update during training time, which helps prevent overfitting by ensuring that the model does not become too reliant on any single neuron.\n\nAdditionally, we utilized an ensemble method to average the predictions of all models from the 10 folds of our cross-validation dataset. This ensemble approach helps to reduce the variance and improve the generalization performance of the model by combining the strengths of multiple models.\n\nWe also saved the model at each epoch and retained the model with the lowest validation loss for final evaluation on the test set. This practice ensures that we select the model that performs best on the validation data, which is a separate subset of the data not used for training. This helps in mitigating overfitting by focusing on the model's performance on unseen data.\n\nFurthermore, we used a relatively large dataset for training and validation, which helps the model to learn more generalizable features rather than memorizing the training data. The BM5 dataset, consisting of 142 complexes, and the CAPRI score set, consisting of 12 complexes, provided a comprehensive training and validation environment.\n\nBy implementing these techniques, we aimed to enhance the model's ability to generalize to new, unseen data and reduce the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule are reported in the publication. The models were trained using a batch size of 64 and a learning rate of 0.0001. The training process involved 50 epochs, with the model being saved at each epoch. The model with the lowest validation loss was retained for final evaluation on the test set. The loss function used was mean squared error, and the Adam optimizer was employed for optimization.\n\nThe model architecture is based on the GVP-GNN model, and details about the architecture can be found in the referenced work by Jing et al. The datasets used for training and evaluation, including the BM5 cross-validation dataset and the CAPRI score set, are available at https://data.sbgrid.org/dataset/843/. These datasets were made available by the authors of DeepRank.\n\nThe specific model files and optimization parameters are not explicitly detailed in the publication, but the overall approach and configurations are described. The datasets and some of the methodological details are accessible, ensuring reproducibility to some extent. However, the exact model files and intermediate optimization parameters are not provided directly.",
  "model/interpretability": "The G-RANK model, which is based on the geometric vector perceptron\u2013graph neural network (GVP-GNN), is inherently more interpretable than traditional black-box models due to its equivariant properties. This type of neural network is designed to be equivariant to translations, rotations, and reflections in 3D Euclidean space. This means that the model's predictions remain consistent regardless of the orientation or position of the input protein structures. This property is crucial for understanding protein interactions, as proteins can exist in various orientations and positions.\n\nThe architecture of G-RANK includes several layers that process node and edge embeddings, which represent the atoms and their interactions within the protein complex. The feed-forward layer updates these node embeddings in a point-wise manner, and these updated embeddings are repeatedly used as inputs for subsequent layers. This iterative process allows the model to capture complex patterns and relationships within the protein structures.\n\nAfter five repetitions of the GVPConvLayer block, the final node embeddings are passed through a GVP layer and then reduced to scalars. These embeddings are averaged across all nodes using a mean pooling layer, and finally, they pass through two dense layers to produce a single scalar value. This scalar value represents the predicted quality of the docking model, specifically the f-nat value, which indicates the fraction of native contacts.\n\nThe use of equivariant layers and the iterative updating of node embeddings provide some level of transparency in how the model arrives at its predictions. However, the exact interpretation of the embeddings and their contributions to the final prediction can still be challenging due to the complexity of the neural network architecture. Nonetheless, the equivariant properties of G-RANK make it more interpretable compared to other black-box models, as it ensures that the model's predictions are consistent across different orientations and positions of the input proteins.",
  "model/output": "The model, G-RANK, is designed for regression rather than classification. It predicts the f-nat values of docking models, which range from 0 to 1. Higher f-nat values indicate higher model quality. The model outputs a single scalar value for each docking model, representing its predicted f-nat value. This scalar output is derived from the final node embeddings, which are averaged and passed through dense layers. The model's architecture, based on GVP-GNN, processes both scalar and vector features, enabling it to handle the complexities of 3D molecular structures effectively. The regression nature of the model allows it to provide a continuous measure of model quality, which is crucial for ranking protein\u2013protein docking models accurately.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for G-RANK is publicly available. It can be accessed via GitHub at the repository link https://github.com/ha01994/grank. This allows users to download, modify, and run the algorithm according to their needs. The availability of the source code facilitates reproducibility and further development by the scientific community.",
  "evaluation/method": "The evaluation of the method involved several steps and datasets to ensure a comprehensive assessment of its performance. Initially, the model was trained using a 10-fold cross-validation approach on the BM5 dataset. This dataset, consisting of 142 non-redundant complexes, was divided into training, validation, and test sets, with 15 complexes left out as the test set for each fold. The model's performance was evaluated using metrics such as ROC-AUC and PR-AUC, which provided insights into its ability to distinguish between positive and negative models.\n\nAdditionally, an ensemble method was employed to compare the model's performance with that of DeepRank-GNN. This involved averaging the predictions from all models across the 10 folds for final evaluation on the left-out test set. The test set consisted of 15 complexes with a significant number of positive and negative models, allowing for a robust assessment of the model's predictive capabilities.\n\nFurthermore, the model was evaluated on an independent test dataset known as the CAPRI score set. This dataset included more than 16,000 docking models for 12 protein-protein complexes, generated using various docking methods by multiple research groups. The evaluation on this dataset provided an additional layer of validation, ensuring that the model's performance was consistent across different datasets and docking methods.\n\nThe assessment metrics used included the area under the receiver operating characteristic curve (ROC-AUC), the precision-recall area under the curve (PR-AUC), the hit rate, and the success rate. The hit rate was defined as the number of hits (positive models) among the top-k ranked models predicted by the model, divided by the total number of positive models for a given complex. The success rate was calculated as the number of docking cases with at least one near-native model among the top-k predicted models, divided by the total number of cases.\n\nOverall, the evaluation method involved a combination of cross-validation, independent dataset testing, and the use of multiple assessment metrics to provide a thorough and reliable evaluation of the model's performance.",
  "evaluation/measure": "In our evaluation, we employed several performance metrics to comprehensively assess the effectiveness of our model. The primary metrics reported include the area under the receiver operating characteristic curve (ROC-AUC) and the precision-recall area under the curve (PR-AUC). These metrics are widely used in the literature and provide a robust evaluation of model performance across different thresholds.\n\nAdditionally, we utilized per-complex assessment metrics such as the hit rate and success rate. The hit rate is defined as the proportion of positive models among the top-k ranked predictions, offering insights into the model's ability to identify correct predictions within a specified rank. The success rate, on the other hand, measures the percentage of docking cases where at least one near-native model is found within the top-k ranked predictions. These metrics are crucial for understanding the practical utility of the model in real-world applications.\n\nThe use of these metrics ensures that our evaluation is representative and comparable to other studies in the field. ROC-AUC and PR-AUC are standard metrics that allow for a fair comparison with existing methods, while the hit rate and success rate provide a more nuanced understanding of the model's performance in specific scenarios. Together, these metrics offer a comprehensive view of the model's strengths and areas for improvement.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of our model, G-RANK, with several publicly available methods on benchmark datasets. Specifically, we evaluated G-RANK on the BM5 test set and the CAPRI score set. For the BM5 test set, we compared G-RANK with DeepRank-GNN using an ensemble method, which involved averaging the predictions of all models from 10 folds for final evaluation on the left-out test set. This approach ensured a fair comparison, as DeepRank-GNN predictions were also obtained using the same ensemble method.\n\nOn the CAPRI score set, we compared G-RANK with multiple methods, including DeepRank-GNN, GOAP, GNN-DOVE, HADDOCK, and iScore. The results demonstrated that G-RANK achieved higher distributions of per-complex ROC-AUC and PR-AUC values compared to the other methods. Additionally, G-RANK showed higher median hit rates and success rates, particularly for smaller values of k, indicating its superior performance in identifying near-native models.\n\nRegarding simpler baselines, the comparison was not explicitly mentioned in the provided context. However, the evaluation against established methods like DeepRank-GNN, GOAP, and others serves as a robust benchmark for assessing the performance of G-RANK. These methods represent state-of-the-art approaches in the field, providing a strong basis for evaluating the effectiveness and efficiency of G-RANK.",
  "evaluation/confidence": "The evaluation of our method, G-RANK, includes several performance metrics such as ROC-AUC, PR-AUC, hit rates, and success rates. These metrics are presented with distributions and medians, which inherently provide a sense of variability and confidence intervals. For instance, the distributions of per-complex ROC-AUC and PR-AUC values are shown in boxplots, which display the median, quartiles, and potential outliers, giving a clear picture of the spread and central tendency of the data.\n\nIn terms of hit rates, the performance is shown for various values of k, ranging from 0 to 1000, with median hit rates and 25\u201375% quantile intervals provided. This detailed presentation allows for an assessment of the consistency and reliability of the hit rates across different thresholds.\n\nSuccess rates are also evaluated for specific values of k (1, 10, 25, 50, 100), and G-RANK demonstrates the highest success rate among all predictors for values of k = 1, 10, 25, and 50. This indicates a statistically significant superiority of G-RANK in these specific cases.\n\nAdditionally, the use of ensemble methods for model evaluation ensures that the results are robust and not dependent on a single training fold. The ensemble method averages the predictions of all models from the 10 folds, providing a more stable and reliable estimate of performance.\n\nOverall, the evaluation metrics and methods used provide a comprehensive and statistically significant assessment of G-RANK's performance, demonstrating its superiority over other methods and baselines in various scenarios.",
  "evaluation/availability": "The raw evaluation files used in our study are not directly available. However, the datasets used for evaluation, such as the BM5 cross-validation dataset and the CAPRI score set, are publicly accessible. These datasets were made available by the authors of DeepRank and can be found at https://data.sbgrid.org/dataset/843/. The datasets include docking models for various protein-protein complexes, which were generated using different docking methods. The specific details on the size and composition of these datasets can be found in the supplementary materials provided with the original DeepRank publication.\n\nThe evaluation metrics and results, including ROC-AUC, PR-AUC, hit rates, and success rates, are thoroughly documented in the main text and supplementary figures. These metrics provide a comprehensive assessment of the model's performance on the test sets. While the raw evaluation files themselves are not provided, the methods and datasets used for evaluation are fully described, allowing for reproducibility and further analysis by other researchers."
}