{
  "publication/title": "Establishment of a diagnostic model of coronary heart disease in elderly patients with diabetes mellitus based on machine learning algorithms.",
  "publication/authors": "Xu H, Cao WZ, Bai YY, Dong J, Che HB, Bai P, Wang JD, Cao F, Fan L",
  "publication/journal": "Journal of geriatric cardiology : JGC",
  "publication/year": "2022",
  "publication/pmid": "35845157",
  "publication/pmcid": "PMC9248279",
  "publication/doi": "10.11909/j.issn.1671-5411.2022.06.006",
  "publication/tags": "- Coronary Heart Disease\n- Diabetes Mellitus\n- Machine Learning\n- Predictive Modeling\n- Elderly Patients\n- XGBoost\n- Random Forest\n- Decision Tree\n- Adaptive Boosting\n- Logistic Regression\n- Medical Big Data\n- Cardiovascular Disease\n- Comorbidity\n- Predictive Analytics\n- Healthcare Data Analysis",
  "dataset/provenance": "The dataset used in this study was sourced from the Medical Big Data Research Centre of Chinese PLA General Hospital in Beijing, China. It comprises hospitalization information for tens of thousands of elderly patients with diabetes mellitus (DM) collected over the past ten years. Specifically, a cohort of 23,167 elderly patients with DM was enrolled, including 10,533 patients in the research group and 12,634 patients in the control group. The dataset primarily includes demographic characteristics, laboratory examinations, and history of complications. This extensive dataset was utilized to build and validate machine learning models for diagnosing coronary heart disease (CHD) in elderly patients with DM. The large sample size and the comprehensive nature of the data contribute to the robustness and generalizability of the findings.",
  "dataset/splits": "In our study, we employed a random split validation approach to develop our machine learning models. We divided our dataset into two primary splits: a training dataset and a testing dataset. The training dataset comprised 80% of the samples from both the research group and the control group. Specifically, this included 8,426 patients from the research group and 10,107 patients from the control group, totaling 18,533 patients. The remaining 20% of the samples were reserved for the testing dataset, which consisted of 2,107 patients from the research group and 2,527 patients from the control group, totaling 4,634 patients.\n\nAdditionally, we evaluated the performance of our models on a newly recruited independent set, which included 3,116 patients with coronary heart disease (CHD) and 4,331 patients without CHD. This independent set was used to further validate the generalizability and robustness of our models.",
  "dataset/redundancy": "The dataset used in this study consisted of hospitalization information for tens of thousands of elderly patients with diabetes mellitus (DM) collected over the past ten years. The total cohort included 23,167 patients, with 10,533 in the research group (patients with coronary heart disease (CHD)) and 12,634 in the control group (patients without CHD).\n\nTo develop machine learning (ML) models, the dataset was split using random split validation. Specifically, 80% of the samples from both the research group and the control group were allocated to the training dataset. The remaining 20% of the samples from each group were reserved for the testing dataset. This split ensured that the training and testing sets were independent, which is crucial for evaluating the generalizability of the models.\n\nThe distribution of the dataset in this study is notably larger compared to many previously published ML datasets for CHD detection. For instance, a review published in 2019 examined 149 research articles related to ML-based CHD detection, with sample sizes ranging from 20 to 240,000 and a median sample size of approximately 350. In contrast, our dataset included 28,059 elderly patients with DM, providing a more robust foundation for model development and validation.\n\nTo address dataset redundancy, recursive feature elimination (RFE) was employed. RFE is a method for feature selection that combines with various ML models to eliminate redundant information, thereby identifying the most influential features. In this study, RFE was used to rank the importance of all 67 feature variables, and the top 15 feature variables were selected to construct the models. This process helped to reduce dependencies and collinearity within the models, ensuring that the selected features were relevant and non-redundant.\n\nThe feature categories used in this study primarily included demographic characteristics and laboratory examinations. These categories were chosen because they are relatively easy to collect compared to other types of features, such as genetic information or detailed medical imaging data. The top 15 feature variables selected by RFE included pro-B-type natriuretic peptide, hemoglobin A1c, troponin T, high-density lipoprotein cholesterol, total bile acid, D-dimer, and glycated serum protein, among others. These variables were found to be the most influential in predicting CHD in elderly patients with DM.\n\nIn summary, the dataset was split into independent training and testing sets using random split validation. The large sample size and the use of RFE for feature selection helped to ensure that the models were developed on a robust and non-redundant dataset, enhancing their predictive performance and generalizability.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm used in this study falls under the class of ensemble learning methods. Specifically, five different machine-learning algorithms were employed: decision tree (DT), random forest (RF), adaptive boosting (Adaboost), logistic regression (LR), and extreme gradient boosting (XGBoost). These algorithms are well-established in the field of machine learning and are not new.\n\nThe choice of algorithms was driven by their proven effectiveness in handling complex datasets and their ability to improve predictive performance through ensemble techniques. Random forest, for instance, combines multiple decision trees to reduce overfitting and enhance generalization. Adaptive boosting integrates weak classifiers to form a strong classifier, while XGBoost adds regularization to control model complexity and improve performance.\n\nThe decision to use these algorithms in a medical research context, rather than a machine-learning journal, is likely due to the specific application and dataset. The study focuses on diagnosing coronary heart disease (CHD) in elderly patients with diabetes mellitus (DM), utilizing a large cohort of clinical data. The primary goal is to demonstrate the practical utility of these machine-learning techniques in a real-world medical setting, rather than to introduce novel algorithms. The performance of these models was evaluated using standard metrics such as sensitivity, specificity, accuracy, precision, F1-score, and area under the curve (AUC), which are crucial for assessing their diagnostic capabilities in clinical practice.",
  "optimization/meta": "The models developed in this study do not use data from other machine-learning algorithms as input. Instead, they utilize a variety of feature categories, including demographic and laboratory examinations, to detect coronary heart disease (CHD). The feature selection process involved recursive feature elimination (RFE) to rank the importance of all 67 feature variables, with the top 15 being selected to construct the models.\n\nFive commonly used machine learning algorithms were employed to train the models: extreme gradient boosting (XGBoost), random forest (RF), decision tree (DT), adaptive boosting (Adaboost), and logistic regression (LR). Each of these algorithms was trained independently on the same dataset, which was split into training and testing subsets using random split validation. The training dataset consisted of 80% of the research group samples and 80% of the control group samples, while the remaining 20% was reserved for testing.\n\nThe performance of these models was evaluated using standard metrics such as precision, accuracy, F1-score, sensitivity, specificity, and the area under the curve (AUC). Among the five models, XGBoost demonstrated the best performance with the highest AUC, indicating its superior predictive capability. The other models, including RF, DT, Adaboost, and LR, also showed varying levels of performance, with RF and Adaboost exhibiting strong prediction abilities as well.\n\nIt is important to note that the training data for each model was independent, ensuring that the models were not influenced by each other's outputs. This independence is crucial for maintaining the integrity and reliability of the model evaluations. The use of different algorithms allowed for a comprehensive comparison, highlighting the strengths and weaknesses of each approach in predicting CHD in elderly patients with diabetes mellitus (DM).",
  "optimization/encoding": "For the machine-learning algorithms, all demographic characteristics, laboratory examinations, history of complications, and other clinical data were utilized to establish prediction models. Missing data was handled using the nonparametric filling missForest algorithm. Any variable with more than 30% of missing data was removed to ensure data quality. Recursive feature elimination (RFE) was employed to rank the importance of all 67 feature variables, and the top 15 feature variables were selected to construct the models. This process helped to eliminate redundant information and identify the most influential features for each model. The importance of each variable in each model was also evaluated to ensure that the selected features were relevant and contributed significantly to the model's performance.",
  "optimization/parameters": "In our study, we utilized a total of 15 feature variables as input parameters for our models. These features were selected using Recursive Feature Elimination (RFE), a method that recursively removes the least significant features and builds models on those features that remain. This process continues until the desired number of features is reached. The top 15 features identified through this method included pro-B-type natriuretic peptide, hemoglobin A1c, troponin T, high-density lipoprotein cholesterol, total bile acid, D-dimer, glycated serum protein, and others. This selection process ensured that we focused on the most relevant variables, enhancing the model's predictive performance while reducing overfitting and computational complexity.",
  "optimization/features": "In our study, we initially considered 67 feature variables for model development. To enhance the predictive performance and interpretability of our models, we employed recursive feature elimination (RFE) as a feature selection method. RFE was used to rank the importance of all 67 feature variables, and the top 15 feature variables were selected to construct our models. This process was performed using the training set only, ensuring that the feature selection was independent of the testing set. The selected features included a mix of demographic and laboratory examination variables, which were chosen based on their importance scores derived from the RFE process. This approach helped us to eliminate redundant information and identify the most influential features for each model.",
  "optimization/fitting": "In our study, we employed several machine learning algorithms to develop predictive models for coronary heart disease (CHD) in elderly patients with diabetes mellitus (DM). The algorithms used included Random Forest (RF), Adaptive Boosting (Adaboost), Extreme Gradient Boosting (XGBoost), Decision Tree (DT), and Logistic Regression (LR). Each of these models has its own strengths and potential weaknesses in terms of overfitting and underfitting.\n\nThe number of parameters in our models, particularly in ensemble methods like RF and XGBoost, can indeed be quite large. To mitigate the risk of overfitting, we implemented several strategies. For instance, XGBoost includes a regularization term in its objective function to control model complexity, making the learned model simpler and less prone to overfitting. Additionally, XGBoost supports column sampling, which not only reduces overfitting but also decreases computational workload. Random Forest, on the other hand, performs random sampling and has a strong generalization ability, but it can be prone to overfitting on noisy datasets. To address this, we ensured that our feature selection process was rigorous, using Recursive Feature Elimination (RFE) to identify the most influential features.\n\nTo rule out underfitting, we carefully selected and tuned the hyperparameters of each model. For example, we used techniques like cross-validation to ensure that our models were neither too simple nor too complex. The performance metrics, such as accuracy, precision, F1-score, sensitivity, and specificity, were evaluated on both training and testing datasets to ensure that the models generalized well to unseen data. The use of a validation set also helped in assessing the model's performance and ensuring that it was not underfitting.\n\nIn summary, we took multiple steps to balance the trade-off between overfitting and underfitting. By using regularization techniques, feature selection methods, and thorough validation processes, we aimed to develop robust models that could effectively predict CHD in elderly patients with DM.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our models. One of the key methods used was regularization, particularly in the XGBoost model. XGBoost incorporates a regularization term in its objective function, which helps to control the complexity of the model. This regularization process makes the learned model simpler and reduces the risk of overfitting.\n\nAdditionally, the random forest algorithm, which is an ensemble learning method, was utilized. Random forests perform random sampling and feature selection, which helps to reduce variance and improve the generalization ability of the model. However, it is important to note that on datasets with significant noise, random forests can still be prone to overfitting.\n\nAdaptive boosting (Adaboost) was another algorithm employed in our study. While Adaboost is known for its high accuracy, it can struggle with unbalanced data, which may lead to a decrease in classification accuracy. To mitigate this, careful consideration was given to the weight of each classifier in the ensemble.\n\nFurthermore, feature selection techniques, such as recursive feature elimination (RFE), were used to rank the importance of feature variables. This process helped to eliminate dependencies and collinearity that may exist in the model, thereby reducing the risk of overfitting.\n\nIn summary, a combination of regularization techniques, ensemble learning methods, and feature selection processes were implemented to prevent overfitting and enhance the performance and generalizability of our models.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in this study, including Random Forest (RF), Adaptive Boosting (Adaboost), and Extreme Gradient Boosting (XGBoost), are known for their predictive power but are often considered \"black box\" models. This means that while they can accurately predict outcomes, the internal workings and the specific interactions between variables that lead to these predictions are not easily interpretable.\n\nThese models minimize error and improve prediction accuracy by identifying latent variables that are not easily observed. However, their complexity makes it challenging to explain how risk factor variables interact and their independent effects on outcomes. This lack of transparency is a common trade-off when using advanced machine learning algorithms for predictive tasks.\n\nIn contrast, simpler models like Decision Trees (DT) and Logistic Regression (LR) offer more interpretability. Decision Trees, for instance, provide a visual representation of the decision-making process, showing how different features contribute to the final prediction. Logistic Regression, being a linear model, allows for straightforward interpretation of coefficients, indicating the direction and magnitude of the effect of each variable on the outcome.\n\nDespite the black-box nature of the RF, Adaboost, and XGBoost models, their performance metrics, such as high Area Under the Curve (AUC) and accuracy, demonstrate their effectiveness in predicting coronary heart disease (CHD) in elderly patients with diabetes mellitus (DM). Future work may focus on developing more interpretable models or techniques to explain the predictions of these complex models, thereby balancing predictive power with transparency.",
  "model/output": "The model is a classification model. It was designed to predict outcomes based on various input features, with the goal of distinguishing between different classes. Specifically, the model was used to identify coronary heart disease (CHD) in elderly patients with diabetes mellitus (DM). The performance of the model was evaluated using metrics such as sensitivity, specificity, accuracy, precision, F1-score, and the area under the curve (AUC) of the receiver operating characteristic (ROC) curves. These metrics are commonly used to assess the effectiveness of classification models. The model's output provides probabilities or class labels indicating the presence or absence of CHD, making it a classification model rather than a regression model.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the models involved several key metrics and methods to ensure robust performance assessment. The primary evaluation metric used was the confusion matrix, which provided a comprehensive view of the model's performance by detailing true positives, true negatives, false positives, and false negatives.\n\nSeveral evaluation indicators were considered, including sensitivity, specificity, accuracy, precision, F1-score, receiver operating characteristic (ROC) curves, and the area under the curve (AUC). These metrics collectively offered a detailed understanding of the models' strengths and weaknesses.\n\nTo compare the performance of different models, the AUC was a crucial metric. The DeLong method was employed to statistically compare the AUCs of various models, ensuring that the differences in performance were significant. This method was implemented using MedCalc software.\n\nThe models were evaluated on both a test set and a newly recruited independent dataset. For the test set, the classification precision of the models varied, with XGBoost achieving the highest precision at 0.778. The AUC for the test set also highlighted XGBoost as the top-performing model with an AUC of 0.851. This was followed by the random forest model with an AUC of 0.845, and other models like decision tree, adaptive boosting, and logistic regression showing slightly lower performance.\n\nAdditionally, the models were applied to a newly recruited independent set to validate their generalizability. The clinical characteristics of this new dataset were analyzed, and the models' performance was assessed similarly, ensuring that the findings were consistent across different patient cohorts. This approach provided a thorough evaluation of the models' robustness and reliability in real-world scenarios.",
  "evaluation/measure": "In our study, we evaluated the performance of five different machine learning models using a comprehensive set of metrics to ensure a thorough assessment. The primary metrics reported include the Area Under the Curve (AUC) with its 95% Confidence Interval (CI), precision, accuracy, F1-score, sensitivity, and specificity. These metrics provide a well-rounded view of each model's performance, covering aspects such as the model's ability to distinguish between positive and negative classes (AUC), the correctness of positive predictions (precision), the overall correctness of predictions (accuracy), the balance between precision and recall (F1-score), the true positive rate (sensitivity), and the true negative rate (specificity).\n\nThe AUC is particularly important as it summarizes the model's performance across all classification thresholds, providing a single value that reflects the model's ability to discriminate between the classes. The 95% CI for the AUC gives an indication of the reliability of this estimate. Precision, accuracy, and F1-score offer insights into the model's performance in terms of correct predictions, while sensitivity and specificity focus on the model's performance in identifying positive and negative cases, respectively.\n\nThis set of metrics is representative of standard practices in the literature, ensuring that our evaluation is comparable to other studies in the field. By including a diverse range of metrics, we aim to provide a comprehensive understanding of each model's strengths and weaknesses, facilitating a more informed selection of the optimal model for our specific application.",
  "evaluation/comparison": "In our study, we evaluated the performance of five different machine learning models to diagnose coronary heart disease (CHD) in elderly patients with diabetes mellitus (DM). The models compared included XGBoost, Random Forest (RF), Decision Tree (DT), Adaptive Boosting (Adaboost), and Logistic Regression (LR). Each model was assessed using standard evaluation metrics such as sensitivity, specificity, accuracy, precision, F1-score, and the area under the curve (AUC) on both testing and validation sets.\n\nThe performance of these models was compared using a confusion matrix, which provided a comprehensive view of true positives, true negatives, false positives, and false negatives. This allowed us to calculate the aforementioned evaluation metrics and determine the strengths and weaknesses of each model.\n\nAmong the models, XGBoost, RF, and Adaboost demonstrated superior prediction performance. XGBoost, in particular, showed excellent generalization ability and robustness, making it the best-performing model in distinguishing patients with CHD. It incorporated regularization to control model complexity, reducing the risk of overfitting and improving prediction accuracy.\n\nRF, known for its ability to handle large datasets and reduce overfitting through random sampling, also performed well. However, it was noted that RF could be prone to overfitting on noisy datasets. Adaboost, which integrates weak classifiers to form a strong classifier, exhibited high accuracy but struggled with unbalanced data, leading to decreased classification accuracy in such cases.\n\nIn contrast, simpler baselines like DT and LR showed lower performance metrics. DT, while straightforward, often resulted in overfitting due to its tendency to create complex trees. LR, a probabilistic model, had the lowest AUC and other performance metrics, indicating it was less effective for this specific diagnostic task.\n\nOverall, the comparison highlighted the advantages of ensemble methods like XGBoost and RF over simpler baselines, demonstrating their effectiveness in handling complex datasets and improving diagnostic accuracy for CHD in elderly DM patients.",
  "evaluation/confidence": "The performance metrics for the models evaluated in this study include confidence intervals, specifically for the Area Under the Curve (AUC). These intervals provide a range within which the true AUC value is expected to lie with a certain level of confidence, typically 95%. This allows for a more nuanced understanding of the model's performance beyond a single point estimate.\n\nStatistical significance was assessed using the DeLong method to compare the AUCs of different models. This method is specifically designed for comparing correlated receiver operating characteristic (ROC) curves, which is appropriate given that the models were evaluated on the same datasets. A two-sided P-value of less than 0.05 was considered statistically significant, indicating that the differences observed in model performance are unlikely to be due to random chance.\n\nThe results demonstrate that the XGBoost model, for instance, has a statistically significant higher AUC compared to other models, suggesting that it is superior in terms of discriminative ability. This statistical rigor ensures that the claims of model superiority are robust and not merely artifacts of random variation.",
  "evaluation/availability": "Not enough information is available."
}