{
  "publication/title": "A Multibranch of Convolutional Neural Network Models for Electroencephalogram-Based Motor Imagery Classification.",
  "publication/authors": "Altuwaijri GA, Muhammad G",
  "publication/journal": "Biosensors",
  "publication/year": "2022",
  "publication/pmid": "35049650",
  "publication/pmcid": "PMC8773854",
  "publication/doi": "10.3390/bios12010022",
  "publication/tags": "- EEG\n- Motor Imagery\n- Brain-Computer Interface\n- Convolutional Neural Networks\n- Deep Learning\n- Classification\n- Cross-Entropy\n- Adam Optimizer\n- Within-Subject Strategy\n- Performance Metrics\n- Multibranch CNN\n- EEG-MI\n- BCI-IV 2a Dataset\n- HGD Dataset\n- Global Hyperparameters",
  "dataset/provenance": "In our study, we utilized two distinct datasets to evaluate our models. The first dataset is the BCI Competition IV dataset 2a, which is publicly available and can be accessed via the provided link. This dataset comprises recordings from nine subjects, each equipped with 22 EEG electrodes sampling at a rate of 250 Hz. The data was filtered using a bandpass filter ranging from 0.5 to 100 Hz. Additionally, three electrooculography (EOG) channels were used to monitor eye movements. The dataset includes four classes of imagined movements: left hand, right hand, feet, and tongue. Each subject participated in two sessions recorded on different days, with the first session dedicated to training and the second to testing. Each session originally contained 288 trials, but trials contaminated with artifacts were excluded.\n\nThe second dataset employed is the High Gamma Dataset (HGD), which is also publicly accessible. This dataset consists of approximately 1040 trials, with 880 trials designated for training and 160 for testing. It includes the same four classes as the BCI IV 2a dataset: left hand, right hand, feet, and rest. The HGD was collected from 14 volunteers using 128 channels at a sampling frequency of 500 Hz. Each trial has a duration of 4 seconds, with inter-trial intervals ranging from 3 to 4 seconds. The dataset was initially sampled at 500 Hz but was resampled to 250 Hz for our analysis, and the number of channels was reduced to 44 to focus on relevant data. Both datasets have been used in previous research and by the community to advance the field of motor imagery classification in EEG-based brain-computer interfaces.",
  "dataset/splits": "In our study, we utilized two datasets: BCI Competition IV 2a and the High Gamma Dataset (HGD). For the BCI Competition IV 2a dataset, data was collected from nine subjects using 22 EEG electrodes at a sampling rate of 250 Hz. Each subject participated in two sessions recorded on different days. The first session, consisting of 288 trials, was used for training purposes, while the second session, also with 288 trials, was used for testing. Trials polluted with artifacts were excluded from each session.\n\nThe High Gamma Dataset (HGD) comprises approximately 1040 trials, with 880 trials designated for training and 160 trials for testing. This dataset includes four classes: left hand, right hand, feet, and rest. The data was acquired from 14 volunteers using 128 channels at a sampling frequency of 500 Hz, which was later resampled to 250 Hz. Each trial had a duration of 4 seconds, with inter-trial intervals ranging from 3 to 4 seconds.\n\nIn summary, the BCI Competition IV 2a dataset has two splits: a training split with 288 trials and a testing split with 288 trials. The HGD also has two splits: a training split with 880 trials and a testing split with 160 trials.",
  "dataset/redundancy": "In our study, we utilized two benchmark datasets: BCI Competition IV-2a and the High Gamma Dataset (HGD). Both datasets were split into training and testing sets to ensure independence between the two.\n\nFor the BCI Competition IV-2a dataset, two sessions were recorded from each of the nine subjects on different days. The first session was exclusively used for training, while the second session was reserved for testing. This approach ensures that the training and test sets are independent, as they come from different recording sessions. Each session initially contained 288 trials, but trials polluted with artifacts were excluded.\n\nThe HGD comprises approximately 1040 trials, with 880 trials designated for training and 160 trials for testing. This dataset was collected from 14 volunteers and includes four classes of imagined movements: left hand, right hand, feet, and rest. The trials were recorded in an EEG lab designed to collect high-frequency, movement-related EEG components. The distribution of trials across classes and subjects was carefully managed to ensure a balanced and representative dataset.\n\nThe splitting of datasets into independent training and testing sets is crucial for evaluating the generalization performance of our models. By ensuring that the training and test sets are from different sessions or recording days, we mitigate the risk of data leakage and overfitting. This approach aligns with best practices in machine learning, where the independence of training and test sets is essential for reliable model evaluation.\n\nIn comparison to previously published machine learning datasets, our approach to dataset splitting is consistent with standard practices. The independence of training and test sets is a fundamental requirement for robust model evaluation, and our methodology adheres to these principles. The balanced distribution of trials across classes and subjects further ensures that our models are trained and tested on representative data, enhancing the reliability of our results.",
  "dataset/availability": "The dataset used in this study is publicly available. It was collected from nine subjects using 22 EEG electrodes at a sampling rate of 250 Hz. The data was filtered using a bandpass filter between 0.5 and 100 Hz. Additionally, three extra electrooculography (EOG) channels were employed to collect data on eye movement. The dataset contains four classes of imagined movements, each representing a different bodily part: left hand, right hand, feet, and tongue. Two sessions were recorded on different days from each subject. The first session was used for training purposes, whereas the second was used for testing purposes. Trials polluted with artifacts were excluded from each session\u2019s 288 trials.\n\nThe dataset is available for public use, allowing researchers to replicate and build upon the findings presented in this study. The specific details about the license and the forum where the dataset is released are not provided here, but it is accessible to the research community. The availability of this dataset ensures that the methods and results can be verified and extended by other researchers in the field.",
  "optimization/algorithm": "The optimization algorithm used in our models is a cross-entropy optimizer, which is a well-established method for adjusting the properties of neural networks, such as weights and learning rates, to minimize loss functions. This optimizer is not new; it is commonly used in various machine learning applications, including those involving neural networks.\n\nThe choice of optimizer is crucial for the performance of the model. In our case, we compared two commonly used optimization algorithms: stochastic gradient descent (SGD) and adaptive moment estimation (Adam). Through our experiments, we found that Adam provided the best results for our specific application in MI-EEG classification.\n\nThe reason this work was not published in a machine-learning journal is that the focus of our research is on the application of these optimization techniques to the specific problem of MI-EEG classification, rather than on the development of new optimization algorithms. Our contributions lie in the development of an end-to-end classification model using deep learning that can handle the subject-specific nature of MI-EEG signals, and in the investigation of suitable kernel sizes for feature extraction. The optimization algorithm is a component of this broader framework, but it is not the primary innovation presented in this work.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were designed to prepare the EEG signals for effective classification using convolutional neural networks (CNNs). We utilized two datasets: the BCI Competition IV dataset 2a and the High Gamma Dataset (HGD).\n\nFor the BCI Competition IV dataset 2a, the raw EEG data was recorded from 22 electrodes at a sampling frequency of 250 Hz. From each trial, we extracted a time frame starting 0.5 seconds before the pre-cue until the end of the trial, resulting in a total length of 4.5 seconds per trial. This extraction yielded 1125 samples per trial. The data from each channel was standardized, and no further preprocessing, such as bandpass filtering, was applied. Each trial was then shaped into a dimensioned matrix of size (22,1125).\n\nThe HGD, initially sampled at 500 Hz, was resampled to 250 Hz to match the sampling rate of the BCI IV 2a dataset. The number of channels was reduced from 128 to 44 to remove unnecessary data. We extracted a 4.5-second time frame from each trial, resulting in 1125 samples. The trials were then shaped into matrices of size (44,1125). Similar to the BCI IV 2a dataset, each channel was standardized, and no additional filters were used.\n\nThe preprocessing steps were minimal, focusing on extracting the relevant motor imagery signal time frame and standardizing the data. This approach ensured that the raw EEG signals were preserved for the CNN to learn both temporal and spatial features effectively. The data was represented as 2D arrays, where the rows corresponded to the number of electrodes, and the columns represented the number of time steps. This encoding allowed the CNN to analyze the influence of nearby neurons using convolutional filters, capturing the local connectivity and invariance to location and local transitions in the cerebral cortex.",
  "optimization/parameters": "The proposed models, MBEEGNet and MBShallowConvNet, have distinct parameter counts. MBEEGNet has approximately 8.908 \u00d7 10^3 parameters, while MBShallowConvNet has around 147.22 \u00d7 10^3 parameters. These parameter counts were determined through a process of defining the three best kernel sizes for the first convolutional layer and tuning the hyperparameters using basic models of EEGNet and ShallowConvNet.\n\nThe selection of parameters was guided by the need to balance computational complexity and model performance. The MBEEGNet and MBShallowConvNet models were designed to have fewer parameters compared to other multiscale models like TS-SEFFNet and CP-MixedNet, which have 282 \u00d7 10^3 and 836 \u00d7 10^3 parameters, respectively. This reduction in parameters aims to enhance the models' efficiency without sacrificing accuracy.\n\nThe models were trained using a within-subject strategy, where one session was used for training and another for testing. Global parameters were employed for all subjects, and the training process involved saving the best model weights based on accuracy at the end of each epoch. The training settings included a batch size of 64, a learning rate of 0.0009, and 1000 epochs, with a cross-entropy error function and an Adam optimizer.\n\nThe choice of parameters was also influenced by the need to generalize the models for practical applications. The fixed parameters in the proposed methods ensure that the models are not subject-specific, making them more suitable for a broader range of users. This approach contrasts with variable networks, which are more subject-specific and may not perform well for all individuals.",
  "optimization/features": "The input features for our models are derived from raw EEG signals, which are represented as 2D arrays. The rows of this array correspond to the number of electrodes (E), and the columns represent the number of time steps (S). Therefore, the input signal X is defined as X = [E S], where E refers to the number of EEG channels, and S to the length of the EEG signal input.\n\nNo explicit feature selection was performed on the raw EEG data. Instead, the models are designed to learn relevant features directly from the raw signals. The multibranch architecture allows the models to capture different temporal and spatial features using various kernel sizes, making the feature extraction process more robust and subject-specific.\n\nThe models utilize the entire raw EEG signal without significant preprocessing, ensuring that all available information is considered during training. This approach leverages the capabilities of convolutional neural networks (CNNs) to automatically extract and learn relevant features from the input data. The use of multiple branches with different kernel sizes enables the models to capture a wide range of features, enhancing their performance in classifying motor imagery (MI) tasks.",
  "optimization/fitting": "In our study, we employed a within-subject strategy for training and testing our models, which involved using sessions recorded for the same individual with different data. This approach has shown good accuracy and is less problematic than the cross-subject strategy due to the dynamic nature of each subject\u2019s brain waves.\n\nTo address the potential issue of overfitting, we utilized several techniques. First, we employed dropout layers in our multibranch models, which help to reduce overfitting by randomly turning off some neurons during training. The dropout rates varied across the branches: 0 for the first branch, 0.1 for the second branch, and 0.2 for the third branch. Additionally, we used batch normalization, which accelerates training, makes learning easier, allows the use of higher learning rates, and regularizes the model. We also implemented a callback during the training phase to save the best model weights based on the current best accuracy, ensuring that we loaded the best-saved model during the test phase.\n\nRegarding the number of parameters, our multibranch models, MBEEGNet and MBShallowConvNet, have approximately 8.908 \u00d7 10^3 and 147.22 \u00d7 10^3 parameters, respectively. These numbers are relatively small compared to other multiscale models, such as TS-SEFFNet and CP-MixedNet, which have 282 \u00d7 10^3 and 836 \u00d7 10^3 parameters, respectively. This indicates that our models are efficient in terms of computational complexity.\n\nTo ensure that our models were not underfitting, we trained them for a sufficient number of epochs (1000 epochs) with a batch size of 64 and a learning rate of 0.0009. We used a cross-entropy error function and the Adam optimizer, which is known for its effectiveness in minimizing loss functions. The use of global parameters for all subjects also helped in generalizing the model, making it more applicable to practical scenarios.\n\nIn summary, we addressed overfitting through dropout layers, batch normalization, and model saving callbacks. Underfitting was mitigated by extensive training and the use of effective optimization techniques. The relatively small number of parameters in our models further supports their efficiency and generalizability.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and improve the generalization of our models. One of the key methods used was batch normalization. This technique normalizes the inputs of each layer, which helps to stabilize and accelerate the training process. It also acts as a regularizer, reducing the risk of overfitting by making the model more robust to the variations in the data.\n\nAdditionally, we utilized dropout layers in our neural network architecture. Dropout involves randomly setting a fraction of the input units to zero at each update during training time, which helps to prevent overfitting by ensuring that the model does not become too reliant on any single neuron. The dropout probability was carefully tuned for each branch of our multibranch models to achieve the best performance.\n\nAnother important aspect of our approach was the use of a within-subject training strategy. This method involves training and testing the model on sessions recorded for the same individual with different data. This strategy has shown good accuracy and helps to mitigate the issues related to the dynamic nature of each subject\u2019s brain waves, which can be problematic in cross-subject approaches.\n\nFurthermore, we employed early stopping during the training phase. This technique involves monitoring the model's performance on a validation set and stopping the training process when the performance stops improving. This helps to prevent the model from overfitting to the training data by ensuring that it generalizes well to unseen data.\n\nOverall, these regularization techniques played a crucial role in enhancing the performance and robustness of our models, ensuring that they could generalize well to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are thoroughly detailed within the publication. Specifically, the global hyper-parameters for the MBEEGNet model are outlined in a dedicated table, providing clear values for kernel sizes, number of temporal filters, and dropout rates across different branches. The training procedure, including the use of a callback to save the best model weights, batch size, learning rate, number of epochs, error function, and optimizer, is also explicitly described. This information ensures reproducibility of the experiments.\n\nThe model architectures for MBEEGNet and MBShallowConvNet are illustrated in figures within the paper, offering a visual representation of the network designs. Additionally, the performance metrics used to evaluate the models, such as accuracy, precision, recall, F1 score, and Cohen\u2019s kappa score, are defined and discussed.\n\nRegarding the availability of model files and optimization parameters, the experiments were implemented using Google\u2019s Colab environment with the TensorFlow deep learning library and Keras API. While the specific model files are not directly provided in the publication, the detailed descriptions of the architectures, hyper-parameters, and training procedures allow for the reconstruction and implementation of the models by interested researchers.\n\nThe publication does not explicitly mention the licensing terms for the use of the provided information or models. However, academic publications typically adhere to open-access principles, allowing for the use and reproduction of the described methods and results for further research and development. For specific licensing details, it would be advisable to refer to the journal's policies or contact the authors directly.",
  "model/interpretability": "The models presented in this work, specifically the multibranch EEGNet (MBEEGNet) and multibranch ShallowConvNet (MBShallowConvNet), are not entirely black-box systems. While deep learning models are often criticized for their lack of interpretability, the architecture and design choices in these models provide some level of transparency.\n\nThe use of convolutional neural networks (CNNs) allows for the extraction of temporal and spatial features from the EEG signals. The first convolutional layer is designed to learn temporal features according to the temporal hierarchies of local and global modulations. The second convolutional layer focuses on spatial features using spatially global unmixing filters. This separation of temporal and spatial feature extraction provides some insight into what the model is learning.\n\nAdditionally, the multibranch architecture concatenates features from multiple branches before classification. This design choice allows for the exploration of different kernel sizes and their impact on feature extraction. The confusion matrices and performance metrics, such as precision, recall, F1 score, and kappa score, provide a clear evaluation of the model's performance across different subjects and classes. These metrics help in understanding how well the model is generalizing and where it might be making errors.\n\nThe use of fixed parameters in the proposed models also contributes to their interpretability. Unlike subject-specific models, which can vary significantly from one individual to another, the fixed parameters ensure that the model's behavior is consistent across different subjects. This consistency makes it easier to understand and interpret the model's decisions.\n\nIn summary, while the models are complex and involve deep learning techniques, the architecture and design choices provide some level of transparency. The separation of temporal and spatial feature extraction, the use of a multibranch architecture, and the evaluation metrics all contribute to a better understanding of how the models are making their predictions.",
  "model/output": "The model is designed for classification. Specifically, it focuses on classifying motor imagery (MI) electroencephalogram (EEG) signals. The output of the classification system is derived from the last layer, which is a softmax layer. This layer produces a vector of probabilities for each possible class or outcome. The sum of these probabilities across all classes equals one. The softmax function is defined such that it takes an input vector and outputs a probability distribution over the classes. This allows the model to predict the most likely class for a given input signal.\n\nThe cost function used in the model is categorical cross-entropy, which measures the difference between the predicted probabilities and the true labels. The goal during training is to minimize this loss, aiming for a perfect model where the cross-entropy loss is zero. This indicates that the model's predictions perfectly match the true labels.\n\nThe model employs a multibranch architecture, combining features from multiple branches of a basic model before classification. This approach helps in capturing both temporal and spatial features from the EEG signals, which are crucial for accurate classification. The use of techniques like batch normalization, exponential linear units (ELUs), and optimization algorithms such as Adam further enhances the model's performance by accelerating training, improving learning, and reducing overfitting.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the proposed models involved a within-subject strategy, which is known for achieving good accuracy. This strategy involves training and testing the model on sessions recorded for the same individual with different data. Specifically, one session was utilized for training, and the other was used for testing. This approach was applied to two benchmark datasets: BCI Competition IV-2a and HGD.\n\nDuring the training phase, a callback was used at the end of each epoch to save the best model weights based on the current best accuracy. The best-saved model was then loaded during the test phase. The models were trained with a batch size of 64 and a learning rate of 0.0009 for 1000 epochs. A cross-entropy error function and an Adam optimizer were employed for training.\n\nThe performance of the models was evaluated using several metrics: accuracy, precision, recall, F1 score, and Cohen\u2019s kappa score. Accuracy was calculated as the ratio of true positives and true negatives to the total number of predictions. Precision and recall were derived from the true positive, false positive, and false negative rates. The F1 score was computed as the harmonic mean of precision and recall. Cohen\u2019s kappa score was used to measure the agreement between the predicted and actual classifications, adjusted for the agreement occurring by chance.\n\nThe experiments were implemented in Google\u2019s Colab environment using the TensorFlow deep learning library with the Keras API. The proposed models, MBEEGNet and MBShallowConvNet, were compared against several state-of-the-art MI-EEG classification approaches, including EEG-TCNet, fixed and variable EEGNet, ShallowConvNet, and Incep-EEGNet. The results demonstrated that the proposed models achieved competitive performance, with MBEEGNet and MBShallowConvNet achieving accuracies of 82.01% and 81.15%, respectively, on the BCI Competition IV-2a dataset.",
  "evaluation/measure": "In our evaluation, we employed a comprehensive set of performance metrics to thoroughly assess the effectiveness of our models. The primary metrics reported include accuracy, precision, recall, F1 score, and Cohen\u2019s kappa score. Accuracy, calculated as the ratio of true positives and true negatives to the total number of predictions, provides a general measure of the model's correctness. Precision, which is the ratio of true positives to the sum of true positives and false positives, indicates the model's ability to correctly identify positive instances. Recall, or sensitivity, measures the proportion of actual positives that are correctly identified by the model. The F1 score, the harmonic mean of precision and recall, offers a balanced view of these two metrics, especially useful when dealing with imbalanced datasets. Cohen\u2019s kappa score adjusts accuracy by accounting for the agreement that could occur by chance, providing a more robust measure of the model's performance.\n\nThese metrics are widely recognized and used in the literature, ensuring that our evaluation is representative and comparable to other studies in the field. By including both overall accuracy and more nuanced metrics like precision, recall, and F1 score, we provide a detailed assessment of our models' strengths and weaknesses. Additionally, the kappa score helps to contextualize the accuracy by considering the possibility of chance agreement, adding another layer of rigor to our evaluation. This comprehensive approach ensures that our models are evaluated thoroughly and fairly, aligning with the standards set by the broader scientific community.",
  "evaluation/comparison": "In the evaluation of our proposed models, we conducted a thorough comparison with publicly available methods on benchmark datasets. Specifically, we tested our models on the BCI Competition IV-2a dataset and the HGD dataset. These datasets are widely recognized in the field of EEG-based motor imagery (MI) classification, providing a robust benchmark for evaluating the performance of different models.\n\nOur proposed models, MBEEGNet and MBShallowConvNet, were compared against several state-of-the-art MI-EEG classification approaches. These include EEG-TCNet, both fixed and variable EEGNet, ShallowConvNet, and IncepEEGNet. The comparison was based on multiple metrics, including classification accuracy, Cohen\u2019s kappa score, precision, recall, and F1-score. Additionally, we considered the number of parameters in each model to analyze computational complexity.\n\nThe results demonstrated that our proposed models achieved competitive performance. MBEEGNet and MBShallowConvNet showed accuracies of 82.01% and 81.15%, respectively, which were higher than many of the compared models. For instance, MBEEGNet outperformed the fixed EEGNet model by at least 9.61% in accuracy and the variable EEGNet model by 2.95%, despite using fixed parameters for all subjects. This indicates that our models are not only effective but also more generalized, making them suitable for practical applications.\n\nFurthermore, we compared our models to simpler baselines to ensure that the improvements were not merely due to increased complexity. The comparison included models like ShallowConvNet, which served as a baseline for evaluating the effectiveness of our multibranch architectures. The results showed that our models provided significant improvements over these baselines, validating the effectiveness of our approach.\n\nIn summary, the evaluation involved a comprehensive comparison with publicly available methods and simpler baselines on benchmark datasets. The results highlighted the superior performance and generalization capabilities of our proposed models, making them a strong contender in the field of EEG-based MI classification.",
  "evaluation/confidence": "In our study, we calculated the standard deviation for both classification accuracy and Cohen's kappa scores, which provides an indication of the variability and confidence in these metrics. This allows for a better understanding of the reliability of our results.\n\nTo assess the statistical significance of our proposed models, we compared them against several state-of-the-art MI-EEG classification approaches using multiple metrics, including classification accuracy, Cohen\u2019s kappa score, precision, recall, and F1-score. The proposed models, MBEEGNet and MBShallowConvNet, demonstrated superior performance in terms of accuracy, achieving 82.01% and 81.15%, respectively. These accuracies were notably higher than those of other models, such as the fixed EEGNet model by at least 9.61% and the variable EEGNet model by 2.95%.\n\nThe improvements observed in our models were consistent across all nine subjects in the EEGNet variable network, indicating that our approach is robust and generalizable. The use of global parameters for all subjects further supports the reliability and consistency of our results.\n\nAdditionally, we evaluated the precision, recall, and F1 scores for each subject, providing a comprehensive view of the models' performance. The average precision, recall, and F1 scores for MBShallowConvNet on the MI BCI IV-2a dataset were 81.14%, 81.58%, and 81.36%, respectively. These metrics, along with the kappa scores, offer a detailed assessment of the models' effectiveness and reliability.\n\nOverall, the statistical significance of our results is supported by the consistent improvements in performance metrics across multiple subjects and comparisons with established baselines. This suggests that our proposed models are superior and can be confidently recommended for MI-EEG classification tasks.",
  "evaluation/availability": "Not enough information is available."
}