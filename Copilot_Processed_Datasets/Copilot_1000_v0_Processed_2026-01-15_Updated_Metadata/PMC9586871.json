{
  "publication/title": "Multimodal integration of radiology, pathology and genomics for prediction of response to PD-(L)1 blockade in patients with non-small cell lung cancer.",
  "publication/authors": "Vanguri RS, Luo J, Aukerman AT, Egger JV, Fong CJ, Horvat N, Pagano A, Araujo-Filho JAB, Geneslaw L, Rizvi H, Sosa R, Boehm KM, Yang SR, Bodd FM, Ventura K, Hollmann TJ, Ginsberg MS, Gao J, , Hellmann MD, Sauter JL, Shah SP",
  "publication/journal": "Nature cancer",
  "publication/year": "2022",
  "publication/pmid": "36038778",
  "publication/pmcid": "PMC9586871",
  "publication/doi": "10.1038/s43018-022-00416-8",
  "publication/tags": "- Scientific Publication\n- Research Article\n- Medical Science\n- Biomedical Studies\n- Health Research\n- Clinical Trials\n- Data Analysis\n- Medical Data\n- Health Outcomes\n- Experimental Design",
  "dataset/provenance": "The dataset used in our study was sourced from a single institution, focusing on patients with stage IV non-small cell lung cancer (NSCLC) who initiated treatment with anti-PD-(L)1 blockade therapy between 2014 and 2019. The inclusion criteria required patients to have a baseline CT scan, baseline PD-L1 immunohistochemistry (IHC) assessment, and next-generation sequencing performed by MSK IMPACT. Patients who received chemotherapy concurrently with immunotherapy were excluded.\n\nThe multimodal cohort consisted of 247 patients, with 54% being female and a median age of 68 years. The majority of patients (88%) had a history of smoking, with a median of 30 pack-years. The histological subtypes of NSCLC included 79% adenocarcinomas, 15% squamous cell carcinomas, 3% large cell carcinomas, and 3% NSCLC, not otherwise specified (NOS). Notably, 68% of patients had received one or more lines of therapy before starting PD-(L)1 blockade, while 32% received it as first-line therapy.\n\nIn addition to the multimodal cohort, two validation cohorts were used: a radiology cohort of 50 patients with baseline CT scans and a pathology cohort of 52 patients with digitized PD-L1-positive NSCLC biopsies. The data was integrated using computational and machine-learning methods, facilitated by the MSK MIND (Multimodal Integration of Data) initiative. This infrastructure supported the ingestion, integration, and analysis of the multimodal dataset, ensuring that all data, metadata, and annotations were properly managed and utilized for the study.\n\nThe dataset has not been used in previous papers by the community, as it was specifically assembled for this study. The focus on a single institution ensured consistent data quality, although future work may explore the inclusion of external data to enhance model generalizability.",
  "dataset/splits": "In our study, we employed a tenfold cross-validation approach for dataset splits. This method involved dividing the multimodal cohort into ten different folds. Each fold was used once as a validation set while the remaining nine folds served as the training set. This process was repeated ten times, ensuring that each data point was used in both training and validation sets.\n\nFor each model and fold result, a subset of 247 patients was included, depending on the available modalities and the specific train-test split. The distribution of data points in each split was consistent across all folds, maintaining a balanced representation of the dataset in both training and validation phases.\n\nAdditionally, we performed a subsampling analysis where the data was randomly sampled 100 times. In each sampling iteration, 90% of the data was used for training, and the remaining 10% was used for testing. This subsampling approach further evaluated the sensitivity of model fitting to the data used for training.\n\nIn summary, our dataset splits included tenfold cross-validation with consistent subsets of 247 patients per fold and a subsampling analysis with 90% training and 10% testing splits.",
  "dataset/redundancy": "The datasets were split using a tenfold cross-validation approach, where patients were randomly sorted into the folds. This method ensures that the training and test sets are independent in each iteration. To enforce this independence, all feature selection and feature rescaling were performed exclusively on the training fold in each iteration. Additionally, the binary cutoff used to determine the predicted response was established using only the training data. Hyperparameters were determined from the first fold and consistently applied across all models.\n\nTo further evaluate the sensitivity of model fitting to the training data, a subsampling analysis was conducted. In this analysis, the data was randomly sampled 100 times, with 90% used for training and the remaining 10% for testing. This approach helps to assess the robustness and generalizability of the models.\n\nThe distribution of the datasets used in this study compares favorably to previously published machine learning datasets in the field. The multimodal cohort was selected based on the existing number of patients who met all inclusion criteria, ensuring a comprehensive and representative sample. The use of tenfold cross-validation and subsampling analyses provides a rigorous framework for evaluating model performance and ensuring that the findings are reliable and reproducible.",
  "dataset/availability": "The data used in this study is not publicly released in a forum. However, we have made efforts to ensure reproducibility and further validation of our findings. All code developed for this research has been released as open-source software. Additionally, our research-ready datasets are provided to enable others to reproduce our results, validate the findings at other institutions, and ideally extend and refine our methods. This approach allows for transparency and encourages further research and development in the field.\n\nThe datasets include various modalities such as CT scans, digitized PD-L1 IHC slides, and genomic alterations. These datasets were carefully curated and annotated to ensure high-quality training data. The CT scans were performed within a single institution to maintain consistent training data quality. Similarly, digitized PD-L1 IHC slides were chosen from a single center to account for differences in staining quality among different laboratories. The genomic data were obtained using targeted next-generation sequencing panels, with considerations for the variations in coverage and germline filtering techniques across different institutions.\n\nTo mitigate the challenges associated with data integration and heterogeneity, we have made our datasets available for further study. This includes the use of internal single modality validation cohorts for CT scans and histopathology slides, which were used as full hold-out sets to validate the findings from the multimodal cohort. The models demonstrated significant and robust performance in the multimodal cohort and showed stable performance in the radiology and pathology cohorts.\n\nIn the future, the assembly of large, well-annotated, multi-institutional training datasets will be crucial for developing robust multimodal classifiers. These classifiers can serve as powerful biomarkers and decision aids in routine clinical care, helping to quickly and precisely distinguish responders and nonresponders to immunotherapy. The release of our datasets and code as open-source software is a step towards achieving this goal, fostering collaboration and advancements in the field.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is logistic regression (LR), which is a well-established method in the field. However, we have developed a novel approach called the DyAM model, which extends the traditional multiple-instance learning (MIL) paradigm. This model is designed to handle heterogeneous instances and modalities, making it particularly suitable for multimodal data.\n\nThe DyAM model is not a completely new machine-learning algorithm in the sense of introducing a novel optimization technique or loss function. Instead, it represents an innovative application and extension of existing methods to address specific challenges in multimodal data analysis. The core idea involves dynamically determining weights for different modalities and using attention mechanisms to learn the relevance of each modality to the treatment response.\n\nThe reason this work was published in a cancer research journal rather than a machine-learning journal is that the primary focus of our study is on its application in cancer research. The DyAM model was developed to improve the prediction of treatment response in cancer patients by integrating multiple types of data, such as genomic alterations, radiological scans, and immunohistochemical features. The innovation lies in the application of these techniques to a complex, real-world problem in oncology, rather than in the development of a new machine-learning algorithm per se. Therefore, the journal \"Nature Cancer\" was the most appropriate venue for publishing our findings, as it aligns with the study's primary focus on cancer research and its potential clinical implications.",
  "optimization/meta": "The model employed in this study is a meta-predictor that integrates multiple modalities to enhance prediction performance. It leverages data from various machine-learning algorithms as input, specifically logistic regression (LR) models applied to different data types. These include clinical features, radiomics features from CT scans, pathology features from PD-L1 immunohistochemistry, and genomic alterations.\n\nThe meta-predictor, referred to as the DyAM model, utilizes a dynamic attention mechanism to weigh the contributions of each modality. This approach allows the model to learn which input modes are most relevant to treatment response. The final output score is a weighted sum of individually optimized LR models applied for each modality, treating each mode specifically while optimizing all parameters in concert.\n\nThe DyAM model addresses the heterogeneity of instances in the multimodal case, where different types of data (such as radiomics, pathology, and genomics) are integrated. Each input mode has its own set of trainable parameters, including risk parameters and attention parameters, which compete with other input modes. This dynamic pooling scheme ensures that the model can handle missing data by setting the attention of any missing modality to zero.\n\nThe training data for the DyAM model is designed to be independent. The multimodal cohort was selected based on the existing number of patients who met all inclusion criteria, and no formal sample size calculations were performed in advance. Group comparisons were performed using two-sided Mann\u2013Whitney U-tests, and tenfold cross-validation was employed to ensure that the training and validation data were independent. Additionally, a subsampling analysis was conducted to evaluate the sensitivity of model fitting to the data used for training, further ensuring the independence of the training data.",
  "optimization/encoding": "For the machine-learning algorithm, data encoding and preprocessing involved several key steps to ensure the integration of diverse modalities. The data utilized included one-dimensional feature vectors derived from PD-L1 immunohistochemistry (IHC) performed on diagnostic tissue, segmented radiological CT scans delineated by lesion site, and genomic alterations.\n\nThe CT scans were segmented by thoracic radiologists to identify malignant lesions, ensuring consistent training data quality. Digitized PD-L1 IHC slides were annotated by thoracic pathologists, and genomic features were limited to those with known associations with NSCLC and immunotherapy outcomes. This curation process involved domain-specific experts to ensure the relevance and accuracy of the features.\n\nTo handle the heterogeneity of the data modalities, a dynamic pooling scheme was employed. Each input mode had its own set of trainable parameters: one to map the input vector to the label (risk parameters) and another to map the input vector to an attention score (attention parameters). This approach allowed the model to learn which input modes were most relevant to treatment response.\n\nThe model was designed to be robust to missing data by incorporating a masking function that set any missing mode\u2019s attention to zero. This was particularly important for handling cases with non-segmentable disease in CT scans or PD-L1-negative tumors.\n\nThe preprocessing steps also included feature selection and rescaling, which were performed on the training fold in each iteration of the cross-validation analysis. This ensured that the model was trained on the most relevant features and that the data was normalized appropriately.\n\nIn summary, the data encoding and preprocessing involved careful curation by domain experts, dynamic pooling to handle heterogeneous data, and robust handling of missing data through attention masking. These steps were crucial for integrating diverse modalities and ensuring the model's effectiveness in predicting treatment response.",
  "optimization/parameters": "In our study, the model utilized a dynamic attention mechanism with multiple modalities, each having its own set of trainable parameters. Specifically, for each modality, there are parameters to map the input vector to the label (risk parameters) and to map the input vector to an attention score (attention parameters). These parameters are dynamically determined during the training process.\n\nThe hidden size of the model was set to 32, which influences the number of parameters in the neural network layers. The model was developed and fit using PyTorch version 1.8.0, with balanced class weights and binary cross-entropy loss. The learning rate was set to 0.005 for one configuration and 0.01 for another, with L2 regularization strengths of 0.005 and 0.001, respectively. The Adam optimizer was used with 250 steps in one configuration and 125 steps in another.\n\nThe selection of these parameters was based on empirical performance and standard practices in the field. The hidden size, learning rate, and regularization strength were chosen to balance model complexity and generalization performance. The number of steps was determined to ensure sufficient training while avoiding overfitting. The use of balanced class weights and binary cross-entropy loss was chosen to handle the binary classification problem effectively.",
  "optimization/features": "In our study, we utilized a comprehensive set of features derived from various modalities to train our models. The exact number of features (f) used as input varied depending on the specific model and the feature selection process. For instance, in the LR Clinical model, we focused on clinical features, while in the LR Rad-PC and LR Rad-LN models, we concentrated on radiomics features extracted from different types of lesions.\n\nFeature selection was indeed performed to ensure that only the most relevant and robust features were included in our models. This process involved several steps, including the removal of non-robust features and the rejection of outlier-prone features. We employed L1 regularization as part of our feature selection strategy to further refine the feature set.\n\nTo maintain the integrity of our analysis, feature selection was conducted using only the training set. This approach ensured that the selection process did not introduce any bias from the validation or test sets. Specifically, during the tenfold cross-validation, feature selection and rescaling were performed exclusively on the training fold in each iteration. This method helped to validate the generalizability of our models and to prevent data leakage.\n\nAdditionally, we performed a subsampling analysis where the data was randomly sampled 100 times using 90% for training and the remaining 10% for testing. This further ensured that our feature selection process was robust and that the selected features were truly informative.",
  "optimization/fitting": "The model fitting process involved several strategies to address both overfitting and underfitting. The number of parameters in the model was indeed larger than the number of training points, which is a common scenario in complex machine learning models, especially those involving multiple modalities.\n\nTo mitigate overfitting, we employed several regularization techniques. L2 regularization was applied with strengths of 0.005 and 0.001 for different models, which helps to penalize large weights and encourages simpler models. Additionally, we used tenfold cross-validation, where the model was trained and validated on different folds of the data. This ensures that the model generalizes well to unseen data. We also performed a subsampling analysis, where the data was randomly sampled 100 times using 90% for training and 10% for testing. This further helps in assessing the model's performance and stability across different subsets of the data.\n\nTo address underfitting, we utilized dynamic attention mechanisms that allow the model to learn which input modalities are most relevant to the treatment response. This ensures that the model can focus on the most informative features, thereby improving its predictive performance. Moreover, the use of balanced class weights in the loss function helps in handling imbalanced datasets, ensuring that the model does not underfit the minority class.\n\nThe model was developed and fit using PyTorch v.1.8.0, with specific hyperparameters such as a learning rate of 0.005 and 0.01 for different models, and a binary cross-entropy loss function. These hyperparameters were established from the first fold and applied consistently for each model, ensuring a fair and systematic training process.\n\nIn summary, the combination of regularization techniques, cross-validation, subsampling, dynamic attention mechanisms, and balanced class weights helped in addressing both overfitting and underfitting, ensuring a robust and generalizable model.",
  "optimization/regularization": "In our study, we employed regularization techniques to prevent overfitting. Specifically, we used L2 regularization, also known as weight decay, in our models. This technique helps to penalize large weights, encouraging the model to find a simpler solution that generalizes better to unseen data. For the model developed using PyTorch v.1.8.0 with a hidden size of 32, we set the L2 regularization strength to 0.005. In another instance, when utilizing the DyAM model, the L2 regularization strength was set to 0.001. These regularization parameters were chosen to balance between fitting the training data well and avoiding overfitting to the specific dataset.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are fully reported. Specifically, for the single-lesion case, the model was developed using PyTorch version 1.8.0 with a hidden size of 32, balanced class weights, binary cross-entropy loss, a learning rate of 0.005, 250 steps, and L2 regularization of 0.005. The Adam optimizer was employed for training.\n\nFor the multimodal dynamic attention model (DyAM), the configuration included balanced class weights, binary cross-entropy loss, a learning rate of 0.01, 125 steps, and L2 regularization strength of 0.001. The Adam optimizer was also used here. When only one input modality was present, the attention gate was disabled to reduce complexity.\n\nThe feature processing and engineering code, which includes details on how these configurations were implemented, is available as part of the Luna project on GitHub. This code is open-source and can be accessed by the community for further exploration and use. The data used in this study is publicly available at Synapse, and the genomic features of the cohort can be explored on cBioportal. Additionally, source data files are provided with the publication.\n\nThe models were trained in an end-to-end manner with corresponding cross-validation across folds and subsampling. All statistical tests were two-sided with a significance level of 2.5% for each tail. The performance metrics, including AUC, HR, and PFS ratio, were recomputed at each step of the log10 scale from \u22122 to 2. The final output score is a weighted sum of individually optimized logistic regression models applied for each modality, treating each mode specifically while optimizing all parameters in concert.",
  "model/interpretability": "The model developed in this study is not a black box but rather incorporates mechanisms that enhance its interpretability. One key aspect is the use of attention weights, which are trainable parameters that allow the model to learn the relevance of different input modalities to the treatment response. These attention weights can be analyzed to understand which modalities are most influential in the model's predictions.\n\nFor instance, by examining the normalized attention weights and risk scores per modality, one can assess how changes in a modality's attention impact the model's performance. This is done by reweighting the attention layer and observing the effects on performance metrics such as AUC, hazard ratio (HR), and progression-free survival (PFS) ratio. This process involves multiplying the attention vector by a scalar multiple of a unit vector corresponding to a specific modality, taking the dot product with the risk vector, and re-normalizing the score. This approach provides a clear way to see how the model's predictions change when the importance of a particular modality is increased or decreased.\n\nAdditionally, the model includes a masking function that sets the attention of any missing modality to zero. This feature addresses the issue of missing data and ensures that the model's predictions are not unduly influenced by the absence of certain modalities. The attention analysis can be performed schematically, allowing for a detailed examination of how different modalities contribute to the final output score.\n\nIn summary, the model's use of attention weights and the ability to reweight these weights for interpretability make it more transparent than typical black-box models. This transparency is crucial for understanding the model's decision-making process and for validating its predictions in a clinical context.",
  "model/output": "The model's output is designed to predict treatment response, which is a classification task. It uses a weighted sum of logistic regression (LR) models applied to each lesion or modality. The weights are dynamically determined, allowing the model to learn the relevance of each input mode to the treatment response. This approach treats each modality specifically while optimizing all parameters together. The final output score is a weighted sum of individually optimized LR models, indicating a classification approach rather than regression. The model was developed using binary cross-entropy loss, which is typically used for binary classification problems. Additionally, performance metrics such as AUC, precision, recall, and F1-score, which are commonly used to evaluate classification models, were reported. The model's output is calibrated to an optimal threshold using the Youden index, further supporting its classification nature.",
  "model/duration": "The model was trained using a specific number of steps for optimization. For the DyAM model, the training process involved 125 steps. For other models, such as the logistic regression models applied to different modalities, the training process involved 250 steps. The exact execution time can vary depending on the computational resources used, but these steps provide a measure of the training iterations performed.\n\nThe models were developed and fit using PyTorch version 1.8.0, with configurations that included balanced class weights, binary cross-entropy loss, and L2 regularization. The learning rates varied: 0.01 for the DyAM model and 0.005 for the other models. The Adam optimizer was used for training, which is known for its efficiency in handling sparse gradients and adaptive learning rates.\n\nThe training process was conducted in an end-to-end manner with corresponding cross-validation across folds and subsampling. This approach ensures that the model's performance is robust and generalizable to new data. The use of cross-validation and subsampling helps in assessing the model's sensitivity to the training data and in providing reliable performance metrics.\n\nIn summary, the model training involved a specified number of steps, with configurations tailored to optimize performance. The exact execution time would depend on the hardware and software environment in which the training was conducted.",
  "model/availability": "The source code for feature processing and engineering used in our study is publicly available. It can be accessed as part of the Luna project on GitHub. The code is released under a license that permits use, sharing, adaptation, distribution, and reproduction, provided that appropriate credit is given to the original authors and the source is cited. The specific license details can be found on the GitHub repository.\n\nNot applicable",
  "evaluation/method": "The evaluation of our method involved several rigorous statistical and validation techniques to ensure the robustness and generalizability of our findings. We performed tenfold cross-validation, where the data was randomly sorted into folds, and feature selection and rescaling were conducted on the training fold in each iteration. This approach helped in assessing the model's performance across different subsets of the data.\n\nTo further evaluate the sensitivity of model fitting to the training data, we conducted a subsampling analysis. In this analysis, the data was randomly sampled 100 times, with 90% used for training and 10% for testing. This method provided insights into how well the model generalizes to unseen data.\n\nWe used the Area Under the Curve (AUC) as the primary measure of performance for predicting response versus nonresponse. Additionally, we reported the F1-score, precision, and recall for all models. The binary cutoff for determining the predicted response was established using only the training data.\n\nHyperparameters were determined from the first fold and consistently applied across all models. Model outputs were calibrated to an optimal threshold using the Youden index and rescaled to unit variance. The 95% confidence intervals on the AUCs were calculated using DeLong's method for computing the covariance of unadjusted AUCs.\n\nStatistical significance was assessed using two-sided Mann\u2013Whitney U-tests, with a significance level of 2.5% for each tail. We also performed perturbation testing with 20 iterations to evaluate the significance of the AUCs against the hypothesis of meaningless class labels.\n\nTo explicitly evaluate the sensitivity of model fitting to the data used for training, we also performed a subsampling analysis where the data was randomly sampled 100 times using 90% for training and the remaining 10% for testing. All models were trained in an end-to-end manner with corresponding cross-validation across folds and subsampling.\n\nWe also developed a multivariable Cox proportional hazards model using the Lifelines Python package to compare the performance of this model to existing biomarkers and calculate adjusted hazard ratios. Survival curves were estimated using the Kaplan\u2013Meier method.\n\nIn summary, our evaluation method included comprehensive cross-validation, subsampling analysis, and rigorous statistical testing to ensure the reliability and validity of our model's performance.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report a comprehensive set of metrics to evaluate the performance of our models. These metrics include the Area Under the Curve (AUC), F1-score, precision, recall, and accuracy. The AUC is used as a primary measure of performance for biomarkers in predicting response versus nonresponse. The F1-score, precision, and recall are also reported to provide a detailed view of the models' performance, particularly in handling imbalanced datasets. These metrics are calculated across the entire multimodal cohort by merging classifier scores obtained in the validation folds of tenfold cross-validation.\n\nThe reported metrics are representative of standard practices in the literature. AUC is widely used for evaluating the performance of binary classifiers, especially in medical and biological studies. The F1-score, precision, and recall are crucial for understanding the trade-offs between false positives and false negatives, which is particularly important in clinical settings where the costs of misclassification can be high. Accuracy is also included to provide a simple, intuitive measure of overall performance.\n\nAdditionally, we performed tenfold cross-validation to ensure the robustness of our results. In each iteration, feature selection and rescaling were performed on the training fold, and the binary cutoff for predicted response was determined using only the training data. This approach helps to prevent overfitting and ensures that the models generalize well to unseen data. We also conducted a subsampling analysis where the data was randomly sampled 100 times, using 90% for training and 10% for testing, to further evaluate the sensitivity of model fitting to the training data.\n\nThe reported metrics and methods are consistent with established practices in the field, ensuring that our evaluation is rigorous and comparable to other studies. The use of multiple performance metrics provides a holistic view of model performance, addressing different aspects of classification accuracy and robustness.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, we focused on evaluating the performance of our multimodal models using internal validation techniques. We employed tenfold cross-validation, where scores from the validation folds were combined to form predictions across the entire multimodal cohort. This approach allowed us to assess the generalizability and robustness of our models within the dataset we had.\n\nWe did, however, compare our multimodal models to simpler baselines. For instance, we evaluated models that relied on single modalities, such as radiology, pathology, or genomics alone. These comparisons helped us understand the incremental value that each modality brought to the predictive performance. For example, models that integrated radiology and genomics data generally outperformed those that used only one of these modalities. This indicated that the combination of different data types provided a more comprehensive view of the patient's condition, leading to better predictive accuracy.\n\nAdditionally, we reported various performance metrics, including the F1-score, precision, recall, and AUC, for all models tested. These metrics were calculated on the merged classifier results across the entire cohort, providing a clear picture of how each model performed. The comparison to simpler baselines was crucial in demonstrating the effectiveness of our multimodal approach.",
  "evaluation/confidence": "The evaluation of our model's performance includes several statistical analyses to ensure the robustness and significance of our results. We computed confidence intervals for the Area Under the Curve (AUC) using DeLong's method, which accounts for the covariance of unadjusted AUCs. This approach provides a more accurate estimate of the variability in our performance metrics.\n\nTo assess the statistical significance of our findings, we employed two-sided Mann\u2013Whitney U-tests for group comparisons. Additionally, we performed perturbation testing with 20 iterations to evaluate the significance of the AUCs against the hypothesis of meaningless class labels. This testing method helps to ensure that our results are not due to random chance.\n\nWe also conducted a subsampling analysis where the data was randomly sampled 100 times, using 90% for training and 10% for testing. This process was repeated across all models in an end-to-end manner with corresponding cross-validation. This rigorous approach helps to evaluate the sensitivity of model fitting to the data used for training and provides a more reliable assessment of model performance.\n\nFurthermore, all statistical tests were conducted with a significance level of 2.5% for each tail, ensuring a stringent criterion for determining statistical significance. The use of tenfold cross-validation, where scores from the validation folds were combined to form predictions across the entire multimodal cohort, adds another layer of confidence in our performance metrics.\n\nIn summary, our evaluation includes confidence intervals for performance metrics, rigorous statistical testing, and extensive cross-validation and subsampling analyses. These methods collectively ensure that our claims of superior performance are statistically significant and robust.",
  "evaluation/availability": "All data used in this study is publicly available. The raw evaluation files, along with other relevant data, can be accessed at Synapse through the following link: [Synapse Data](https://www.synapse.org/#!Synapse:syn26642505). Additionally, the genomic features of the cohort can be explored on cBioPortal using this link: [cBioPortal Data](https://www.cbioportal.org/study/summary?id=lung_msk_mind_2020). The source data files are also provided with this publication. The data is made available to facilitate reproducibility and further research."
}