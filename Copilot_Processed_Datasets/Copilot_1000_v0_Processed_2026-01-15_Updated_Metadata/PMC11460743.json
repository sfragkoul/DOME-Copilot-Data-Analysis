{
  "publication/title": "Volumetric and textural analysis of PET/CT in patients with diffuse large B-cell lymphoma highlights the importance of novel MTVrate feature.",
  "publication/authors": "Czibor S, Csatl\u00f3s Z, F\u00e1bi\u00e1n K, Piroska M, Gy\u00f6rke T",
  "publication/journal": "Nuclear medicine communications",
  "publication/year": "2024",
  "publication/pmid": "39102514",
  "publication/pmcid": "PMC11460743",
  "publication/doi": "10.1097/mnm.0000000000001884",
  "publication/tags": "- Volumetric analysis\n- Textural analysis\n- PET/CT\n- DLBCL\n- Prognostic model\n- Machine learning\n- Logistic regression\n- Elastic net regularization\n- Cross-validation\n- ROC analysis\n- Clinical data\n- Volumetric data\n- Textural features\n- Intraclass correlation coefficients\n- Survival analysis\n- SUVmax\n- Gray-level co-occurrence matrix\n- MTV\n- MTVrate\n- Prognostic biomarkers",
  "dataset/provenance": "The dataset used in this study was sourced from baseline PET/CT scans of patients with diffuse large B-cell lymphoma (DLBCL) performed at the Department of Nuclear Medicine of Semmelweis University\u2019s Medical Imaging Centre. The final cohort encompassed 50 patients, all of whom received rituximab-based immunochemotherapy with curative intent, primarily the R-CHOP regimen. The inclusion criteria required patients to have at least 2 years of medical follow-up data or documented disease progression within that timeframe.\n\nThe PET/CT scans were conducted using a hybrid PET/CT system, with patients receiving 2.5\u20133.0 MBq/body weight kilogram of FDG intravenously after fasting for more than 6 hours. Imaging was performed 60 minutes post-injection, and data was reconstructed using both a Bayesian penalized likelihood-based algorithm (Q.Clear) and a conventional ordered subset expectation maximization (OSEM) algorithm.\n\nThe dataset included clinical, volumetric, and textural parameters extracted from the PET/CT scans. Volumetric parameters such as metabolic tumor volume (MTV) and total lesion glycolysis (TLG) were calculated, along with a newly introduced parameter, MTVrate, defined as the quotient of the largest lesion\u2019s volume and total body MTV. Textural features, including first-, second-, and higher-order parameters, were also extracted from the largest lymphoma lesion's volume of interest (VOI).\n\nThis dataset has not been previously used in other published studies by the community, as this study represents a novel investigation into the prognostic value of these parameters in DLBCL patients. The analysis involved both individual assessment of various biomarkers and a combined analysis using a machine learning algorithm to enhance prognostic accuracy.",
  "dataset/splits": "The dataset was split into training and test sets using repeated cross-validation in three cycles. In each cycle, the patient population was divided into 70% for training and 30% for testing. After the first and second rounds, the data points were reassigned to create new training and test sets. This process ensured that the model was trained and validated on different subsets of the data, helping to avoid overfitting and to assess the model's performance more robustly.",
  "dataset/redundancy": "In our study, we employed a rigorous approach to split the dataset to ensure the independence of training and test sets, thereby minimizing the risk of data leakage and overfitting. The dataset was divided into training and test sets using a 70%:30% split. This process was repeated in three cycles, with the patient population being reassigned after each round to ensure that different subsets of data were used for training and testing in each cycle.\n\nTo enforce the independence of the training and test sets, we utilized repeated cross-validation. This method involves partitioning the data into multiple folds and iteratively training the model on different combinations of these folds. By doing so, we ensured that each data point was used for both training and testing, but never in the same iteration, thus maintaining the independence of the sets.\n\nThe distribution of our dataset compares favorably with previously published machine learning datasets in the field of medical imaging. Our cohort consisted of 50 patients, which is a reasonable sample size for a retrospective analysis. The demographic and clinical data of the patients were well-balanced, with a mix of male and female participants and a wide age range. This diversity helps in generalizing the findings to a broader population.\n\nMoreover, the inclusion of both clinical and volumetric parameters, along with textural features derived from PET/CT scans, provided a comprehensive set of predictors. This multifaceted approach is in line with contemporary studies that emphasize the importance of integrating various types of data to improve prognostic accuracy. The use of elastic net regularization further helped in selecting the most relevant features, reducing the risk of overfitting and enhancing the model's generalizability.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is logistic regression, which is a well-established and widely used method in the field. To enhance its performance and prevent overfitting, we employed elastic net regularization. This technique combines L1 (Lasso) and L2 (Ridge) regularization functions, which helps in managing the complexity of the model by penalizing the absolute size and squared magnitude of the coefficients, respectively.\n\nThe algorithm itself is not new; it is a standard approach in statistical learning and machine learning. The reason it was not published in a machine-learning journal is that our primary focus was on applying this established method to a specific medical context\u2014namely, the prognostic assessment of diffuse large B-cell lymphoma (DLBCL) using PET/CT data. Our contribution lies in the innovative application of this algorithm to integrate clinical, volumetric, and textural data for improved prognostic modeling in DLBCL, rather than in the development of a new machine-learning algorithm.",
  "optimization/meta": "The model employed in this study is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it is a single machine learning algorithm based on logistic regression with elastic net regularization. This approach was used to build a prognostic model from clinical, volumetric, and textural data.\n\nThe model was trained using repeated cross-validation in three cycles, splitting the patient population into training and test sets with a 70%:30% ratio. This process involved reassigning the patient data after the first and second rounds to ensure that the training data was independent in each cycle. This method helps to avoid overfitting and ensures that the model's performance is robust and generalizable to new data.\n\nThe elastic net regularization technique, which combines L1 (Lasso) and L2 (Ridge) regularization, was utilized to clear redundant parameters and assign relative importance to the remaining ones. This regularization method helps in selecting the most relevant features and improving the model's predictive accuracy.\n\nIn summary, the model is a standalone machine learning algorithm that does not rely on other machine-learning algorithms for input. The training data was independently split in each cycle of cross-validation to ensure the model's reliability and effectiveness.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps. Initially, baseline PET/CT scans of DLBCL patients were evaluated using Mediso InterView Fusion software. Lymphoma lesions were delineated using a fixed standardized uptake value (SUV)-based semi-automatic algorithm, with SUV values greater than 4.0, and manually corrected when necessary. This process established volumes of interest (VOIs).\n\nFrom these VOIs, volumetric parameters such as metabolic tumor volume (MTV) and total lesion glycolysis (TLG) were calculated. MTV was determined as the sum of all lymphoma lesions\u2019 volumes on PET images, while TLG was calculated as the sum of the product of each lesion\u2019s metabolic volume and SUVmean. Additionally, a new parameter, MTVrate, was introduced as the quotient of the largest lesion\u2019s volume and total body MTV.\n\nTextural features were extracted from the VOI of the largest lymphoma lesion. A total of 44 textural features, including first-, second-, and higher-order features, were considered. Patients with a volume of the largest lymphoma lesion under 30 ml were excluded to avoid dependence on volume when calculating second-order entropy.\n\nThe data was then preprocessed for the machine-learning algorithm. Elastic net regularization, a combination of L1 (Lasso) and L2 (Ridge) regularization functions, was utilized to avoid overfitting. Repeated cross-validation was employed to train the model in three cycles, splitting the patient population into 70% training and 30% test sets, with reassignment after each round. This process helped in clearing redundant parameters and assigning relative importance to the remaining ones.\n\nStatistical analysis was performed using R (version 4.2.3) and SigmaPlot (version 11.0). Receiver operating characteristics (ROC) analyses were conducted to assess the prognostic performance of clinical and PET-based values and to define optimal cutoff points. These cutoff points were used to determine low- and high-risk groups for each biomarker, and the progression-free survival (PFS) of these groups was evaluated by log-rank analysis.",
  "optimization/parameters": "In our study, the model utilized a combination of clinical, volumetric, and textural parameters. The specific number of parameters (p) used in the final model was determined through a process that involved elastic net regularization, which is a combination of L1 (Lasso) and L2 (Ridge) regularization functions. This approach helped to clear redundant parameters and assign relative importance to the remaining ones.\n\nThe selection of parameters was guided by their prognostic value, as assessed through individual analyses and their performance in the machine learning model. For instance, among the volumetric features, both MTV (Metabolic Tumor Volume) and MTVrate (the quotient of the volume of the greatest lesion and the total body MTV) were retained as important predictors of 24-months progression-free survival (PFS). From the textural features, contrast, long-zone low gray-level emphasis (LZLGE), zone percentage, and skewness were attributed the highest relative importance. Clinical parameters such as LDH levels, spleen involvement, and patient age also contributed to the model, albeit with lesser importance.\n\nThe final model, which integrated these selected parameters, demonstrated a high prognostic ability with an AUC of 0.83 on ROC analysis. The process of parameter selection ensured that only the most relevant and non-redundant features were included, thereby enhancing the model's predictive performance.",
  "optimization/features": "The study utilized a combination of clinical, volumetric, and textural features as inputs for the machine learning model. The total number of features (f) used as input was 44 textural features, along with clinical and volumetric data.\n\nFeature selection was performed using elastic net regularization, which is a combination of L1 (Lasso) and L2 (Ridge) regularization functions. This method helps in selecting the most relevant features by shrinking the coefficients of less important features to zero, thereby reducing overfitting.\n\nThe feature selection process was conducted within the framework of repeated cross-validation. The patient population was split into training and test sets in a 70%:30% ratio, with reassigning after the first and second rounds. This ensured that the feature selection was performed using the training set only, maintaining the integrity of the test set for unbiased evaluation.",
  "optimization/fitting": "The study involved a machine learning algorithm to build a prognostic model using clinical, volumetric, and textural data. The dataset consisted of 50 patients, which is relatively small compared to the number of parameters investigated. To address the risk of overfitting, elastic net regularization was employed. This technique combines L1 (Lasso) and L2 (Ridge) regularization functions, which helps in reducing the complexity of the model by penalizing large coefficients and encouraging sparsity. This approach effectively clears redundant parameters and assigns relative importance to the remaining ones.\n\nTo further mitigate overfitting, repeated cross-validation was used. The patient population was split into training and test sets in a 70%:30% ratio, with reassigning after the first and second rounds. This method ensures that the model is trained and validated on different subsets of the data, providing a more robust evaluation of its performance.\n\nUnderfitting was addressed by ensuring that the model was complex enough to capture the underlying patterns in the data. The use of elastic net regularization allowed the model to retain important features while discarding noise, thereby balancing the trade-off between bias and variance. The model's performance was evaluated using ROC analysis, which showed an AUC of 0.83, indicating good prognostic ability. This suggests that the model is neither too simple to capture the data's complexity nor too complex to overfit the training data.",
  "optimization/regularization": "To prevent overfitting in our prognostic model, we employed elastic net regularization. This technique combines the strengths of both L1 (Lasso) and L2 (Ridge) regularization methods. L1 regularization promotes sparsity by driving some coefficients to zero, effectively performing feature selection. On the other hand, L2 regularization shrinks the coefficients but does not eliminate them, which helps in reducing the complexity of the model. By using elastic net, we were able to balance between these two approaches, ensuring that our model generalizes well to unseen data while also selecting the most relevant features. This regularization method was crucial in building a robust prognostic model from the clinical, volumetric, and textural data available.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the methodology section. Specifically, we utilized elastic net regularization, which combines L1 (Lasso) and L2 (Ridge) regularization functions, to build our prognostic model. This approach was implemented to avoid overfitting. The model was trained using repeated cross-validation in three cycles, splitting the patient population into 70% training and 30% test sets, with reassigning after the first and second rounds.\n\nThe model files and optimization parameters are not explicitly provided in the publication. However, the statistical analysis was performed using R (version 4.2.3) and SigmaPlot (version 11.0), which are widely available software tools. The specific details of the model, including the relative importance of parameters and the ROC analysis, are described in the results section.\n\nRegarding the availability and licensing of the tools used, R is an open-source software available under the GNU General Public License, and SigmaPlot is a commercial software with its own licensing terms. The exact model files and optimization parameters are not publicly available, but the methods and tools used are well-documented and can be replicated using the information provided.",
  "model/interpretability": "The model employed in our study is not a blackbox, but rather a transparent one, as it utilizes a machine learning algorithm based on logistic regression with elastic net regularization. This approach allows for the identification and ranking of the relative importance of different parameters within the model.\n\nThe model's transparency is evident in several ways. Firstly, the use of elastic net regularization helps in selecting the most relevant features and discarding the redundant ones, making the model more interpretable. This regularization technique combines the penalties of L1 (Lasso) and L2 (Ridge) regularization, which helps in both feature selection and preventing overfitting.\n\nSecondly, the model's training process involved repeated cross-validation, splitting the patient population into training and test sets in three cycles. This method ensures that the model's performance is robust and generalizable, rather than being tailored to a specific subset of data.\n\nMoreover, the model's prognostic value was determined through ROC analysis, which provides a clear measure of the model's ability to distinguish between different patient outcomes. The AUC of 0.83 indicates a high level of prognostic accuracy.\n\nThe model's transparency is further illustrated by the identification of specific parameters that contribute significantly to its prognostic power. For instance, textural features such as contrast, long-zone low gray-level emphasis (LZLGE), zone percentage, and skewness were found to be highly important. Volumetric features like MTV and MTVrate also remained significant predictors of 24-months progression-free survival (PFS). Additionally, clinical parameters such as LDH levels, spleen involvement, and patient age had some lesser importance in the model.\n\nThe relative importance of these parameters within the model is visually represented in a figure, which shows the contribution of each feature to the model's predictions. This visualization aids in understanding which factors are most influential in determining patient outcomes.\n\nIn summary, the model's transparency is achieved through the use of interpretable machine learning techniques, robust training methods, and clear visualization of feature importance. This makes it possible to understand the underlying factors that drive the model's prognostic predictions.",
  "model/output": "The model developed in this study is a classification model. It was built using logistic regression, which is a type of supervised learning algorithm used for binary classification problems. The model was designed to predict the prognosis of patients with diffuse large B-cell lymphoma (DLBCL), specifically their progression-free survival (PFS) at 24 months. The output of the model is a binary classification indicating whether a patient is likely to have a good or poor prognosis.\n\nThe model's performance was evaluated using the area under the receiver operating characteristic curve (AUC-ROC), which is a common metric for assessing the performance of classification models. The AUC-ROC value of 0.83 indicates that the model has a good ability to discriminate between patients with different prognoses.\n\nThe model's predictions are based on a combination of clinical, volumetric, and textural features extracted from PET/CT scans. These features were selected using elastic net regularization, which helps to prevent overfitting and improve the model's generalization to new data. The relative importance of each feature in the model was determined using repeated cross-validation, which involves splitting the data into training and test sets multiple times and averaging the results.\n\nThe model's output can be used to identify patients who are at high risk of poor prognosis and may benefit from more aggressive treatment or closer monitoring. However, it is important to note that the model's predictions are probabilistic and should be used in conjunction with other clinical information and expert judgment.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to ensure the robustness and generalizability of the prognostic model. A machine learning algorithm was utilized to build the prognostic model from clinical, volumetric, and textural data. To mitigate the risk of overfitting, elastic net regularization was applied, which combines L1 (Lasso) and L2 (Ridge) regularization functions. This technique helps in selecting the most relevant features and preventing the model from becoming too complex.\n\nThe model was trained using repeated cross-validation, specifically in three cycles. In each cycle, the patient population was split into training and test sets in a 70%:30% ratio. This process was repeated with reassigning after the first and second rounds to ensure that the model's performance was consistent across different subsets of the data.\n\nAfter preprocessing the data, the model was trained and evaluated. The relative importance of the remaining parameters was determined, and redundant parameters were cleared. The final step involved performing ROC analysis to assess the model's prognostic value. The area under the curve (AUC) was used as the primary metric to evaluate the model's performance.\n\nStatistical analysis was conducted using R (version 4.2.3) and SigmaPlot (version 11.0). The final cohort consisted of 50 patients, and various clinical and volumetric parameters were analyzed. The Ann-Arbor stage and phenotypic status of the DLBCL were found to have significant impacts on survival. The newly introduced parameter, MTVrate, showed the highest individual prognostic value with an AUC of 0.74. The combined analysis using machine learning, which included clinical, volumetric, and textural features, resulted in an AUC of 0.83 on ROC analysis, indicating a high prognostic ability.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the prognostic value of our models. The primary metric reported is the Area Under the Curve (AUC) from the Receiver Operating Characteristic (ROC) analysis. This metric was used to assess the individual prognostic value of various clinical, volumetric, and textural parameters, as well as the overall performance of our combined machine learning model.\n\nFor individual parameters, we reported the AUC values for metrics such as Ann-Arbor stage, lactate-dehydrogenase (LDH) levels, Metabolic Tumor Volume (MTV), Total Lesion Glycolysis (TLG), and the newly introduced MTVrate. Among these, MTVrate showed the highest individual prognostic value with an AUC of 0.74. Additionally, we performed log-rank analysis to evaluate the significance of these parameters in stratifying patient groups with different Progression-Free Survival (PFS).\n\nIn the combined analysis using a machine learning model, we reported an AUC of 0.83 on ROC analysis, indicating a high prognostic ability. This model incorporated clinical, volumetric, and textural features, with textural features such as contrast, long-zone low gray-level emphasis (LZLGE), zone percentage, and skewness being attributed the highest relative importance. Volumetric features like MTV and MTVrate, along with clinical parameters such as LDH levels, spleen involvement, and patient age, also contributed to the model's prognostic value.\n\nThe set of metrics used in our study is representative of those commonly reported in the literature for similar prognostic models in diffuse large B-cell lymphoma (DLBCL). The use of AUC from ROC analysis is a standard practice for evaluating the performance of prognostic models, and our inclusion of additional metrics like log-rank analysis provides a comprehensive assessment of the model's ability to stratify patient outcomes. The high AUC value of our combined model is comparable to other studies in the field, demonstrating the effectiveness of our approach.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. However, we did compare our findings with those of other studies that used similar methodologies. For instance, we referenced the work of Aide et al. and Coskun et al., who also utilized textural and volumetric features from PET/CT scans to predict outcomes in DLBCL patients. Their studies, like ours, employed machine learning techniques to build prognostic models. Notably, despite differences in data collection procedures, our model's prediction accuracy was similar to theirs, suggesting robustness in our approach.\n\nAdditionally, we compared individual parameters such as SUVmax and gray-level co-occurrence matrix (GLCM) dissimilarity, which were found to be independent predictors of 24-months progression-free survival (PFS) in Coskun et al.'s study. Our model also highlighted the importance of textural features like contrast and skewness, along with volumetric features such as MTV and MTVrate.\n\nWhile we did not use simpler baselines for direct comparison, our model's performance was evaluated against individual clinical, volumetric, and textural biomarkers. This evaluation showed that while individual parameters had limited prognostic value, the combined machine learning-based model demonstrated high efficacy, achieving an AUC of 0.83 on ROC analysis. This indicates that our approach, which integrates multiple types of data, provides a more comprehensive and accurate prognostic tool compared to simpler, single-parameter analyses.",
  "evaluation/confidence": "The evaluation of our study's performance metrics included confidence intervals for several key parameters. For instance, intraclass correlation coefficients (ICCs) for various textural features were provided with 95% confidence intervals, indicating the reliability of these features across different PET reconstructions. This information is crucial for understanding the consistency and reproducibility of our findings.\n\nStatistical significance was assessed for various prognostic factors. For example, the Ann-Arbor stage did not show a significant difference in progression-free survival (PFS) with a P-value of 0.185. However, the phenotypic status of DLBCL had a significant impact on survival, with germinal center B-cell (GC) phenotype patients showing a higher 24-month PFS compared to non-GC patients (P = 0.038). Similarly, lactate-dehydrogenase (LDH) levels, metabolic tumor volume (MTV), and total lesion glycolysis (TLG) were evaluated for their prognostic value, with MTV and MTVrate showing significant differences in PFS when patients were divided into low- and high-risk groups.\n\nThe machine learning model, which combined clinical, volumetric, and textural data, demonstrated high prognostic ability with an area under the curve (AUC) of 0.83 in ROC analysis. This model included textural features such as contrast, long-zone low gray-level emphasis, zone percentage, and skewness, as well as volumetric features like MTV and MTVrate. The model's performance was validated through repeated cross-validation, ensuring robustness and generalizability.\n\nIn summary, the performance metrics in our study are supported by confidence intervals and statistical significance tests, providing a strong basis for claiming the superiority of our method over others and baselines. The use of machine learning further enhances the prognostic accuracy, making our findings reliable and applicable in clinical settings.",
  "evaluation/availability": "Not enough information is available."
}